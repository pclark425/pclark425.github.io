<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6671 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6671</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6671</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-84c149537dee8ef3a7ff54b6bb01e9a4c8eaa17c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/84c149537dee8ef3a7ff54b6bb01e9a4c8eaa17c" target="_blank">Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces novel efficiency metrics from both outcome and process perspectives to evaluate the rational use of computational resources by o1-like models, and proposes strategies to mitigate overthinking, streamlining reasoning processes without compromising accuracy.</p>
                <p><strong>Paper Abstract:</strong> The remarkable performance of models like the OpenAI o1 can be attributed to their ability to emulate human-like long-time thinking during inference. These models employ extended chain-of-thought (CoT) processes, exploring multiple strategies to enhance problem-solving capabilities. However, a critical question remains: How to intelligently and efficiently scale computational resources during testing. This paper presents the first comprehensive study on the prevalent issue of overthinking in these models, where excessive computational resources are allocated for simple problems with minimal benefit. We introduce novel efficiency metrics from both outcome and process perspectives to evaluate the rational use of computational resources by o1-like models. Using a self-training paradigm, we propose strategies to mitigate overthinking, streamlining reasoning processes without compromising accuracy. Experimental results show that our approach successfully reduces computational overhead while preserving model performance across a range of testsets with varying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6671.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6671.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwQ-32B-Preview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen QwQ 32B Preview (o1-like)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An o1-like large language model that exposes long chain-of-thought (CoT) style visible thinking during inference and produces multiple solution rounds for math problems; used as the primary testbed in this paper for studying 'overthinking'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwQ-32B-Preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper; described as an o1-like model with visible long CoT behavior. This study uses PRM12K as self-training data for post-training experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ASDIV, GSM8K, MATH500, GPQA, AIME24</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Math word problems including simple arithmetic (e.g., 2+3), multi-step grade-school and competition math</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language math word problems (open-ended answers); single-question prompts (examples include 'What is the answer of 2 plus 3?')</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>ASDIV (elementary grade 1-6); GSM8K (grade-school); MATH500 (competition problems level 1-5); AIME (high difficulty)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Long chain-of-thought (o1-style visible CoT). Generation used sampling (10 samples per instance, temperature=1.0) and greedy decoding as baselines; analyses used solution-splitting via Llama-3.3-70B and clustering via GPT-4o. Post-training / mitigation used SFT, DPO, RPO, SimPO and response simplification heuristics (FCS, FCS+Reflection, GDS).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (exact-answer), # solutions per response, # tokens generated per response, outcome efficiency (ξ_O), process efficiency (ξ_P)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ASDIV: 96.9% accuracy; avg #solutions 3.5; avg tokens 741.8; outcome efficiency 41.9%; process efficiency 66.5%. GSM8K: 94.8% accuracy; #solutions 3.1; tokens 772.8; outcome 50.7%; process 67.6%. MATH500: 93.0% accuracy; #solutions 3.2; tokens 2407.9; outcome 52.3%; process 71.2%. GPQA: 59.6% acc; #solutions 2.2; tokens 3228.4; outcome 51.4%; process 84.3%. AIME24: 46.7% acc; #solutions 2.6; tokens 9480.9; outcome 38.4%; process 84.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No low-level mechanistic probing (no attention/activation/logit-lens analysis); empirical analyses include (1) splitting generated text into discrete 'solutions' using Llama-3.3-70B, (2) clustering solutions into strategy groups with GPT-4o, (3) measuring the 'first-correct' distribution (finding >92% of correct answers appear in the first solution round), and (4) computing distinctness ratios per solution index showing decreasing novelty in later solutions. The paper therefore analyzes process- and outcome-level behavior rather than neuron-level mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Primary failure mode is 'overthinking': producing many redundant solution rounds and long token outputs that rarely improve accuracy (later solutions usually duplicate earlier reasoning), low diversity in later solutions, disproportionate compute on trivially simple problems (e.g., 13 solutions for '2+3'), and inefficient token usage. Overthinking is more frequent on easier problems and wastes inference compute; simplification can hurt performance on hard problems if over-applied.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Characterized behaviorally rather than by parameter scaling: o1-like behavior (long CoT) yields more solution rounds and tokens compared to conventional models; more solution rounds are produced for easier problems (counterintuitively), while token count increases with problem difficulty. Post-training mitigation (SimPO and simplification heuristics) reduces token usage (SimPO reduced tokens by 22.3% on MATH500) while largely preserving accuracy; pure shortest-response SFT and extreme FCS can substantially reduce tokens but sometimes reduce accuracy on hard benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6671.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6671.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1 (o1-like)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An o1-like LLM (DeepSeek family) that exhibits visible multi-round CoT when solving math problems; evaluated alongside QwQ-32B-Preview to study overthinking patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper; described as an o1-like model with explicit multi-round CoT behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ASDIV, GSM8K, MATH500</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Math word problems and arithmetic (multi-step reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language math word problems with free-response answers</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>ASDIV (easy), GSM8K (grade-school), MATH500 (competition levels)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Long chain-of-thought (o1-style) visible reasoning; sampled/generative decoding producing multiple solutions per prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy, # solutions per response, # tokens per response, ξ_O (outcome efficiency), ξ_P (process efficiency)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ASDIV: 97.1% accuracy; avg #solutions 4.5; tokens 845.0; outcome 45.9%; process 64.3%. GSM8K: 96.4% accuracy; #solutions 4.3; tokens 1056.3; outcome 48.9%; process 62.0%. MATH500: 96.4% accuracy; #solutions 4.3; tokens 2704.3; outcome 51.0%; process 66.2%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Same empirical analyses as for QwQ: solution-count distributions, first-correct distribution (initial rounds often correct), and distinctness ratio showing reduced novelty in later rounds. No neuron- or attention-level mechanistic analysis presented.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Overthinking (excessive redundant rounds and tokens), later solutions contribute minimally to correctness, repetition of reasoning strategies across rounds, greater inefficiency compared to conventional single-solution models.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Behavioral: DeepSeek-R1 tends to generate even more solution rounds than QwQ (avg more rounds across datasets) and correspondingly longer outputs, leading to similar inefficiency patterns; no parameter-size scaling analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6671.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6671.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conventional instruction-tuned LLM that typically returns a single solution (no long visible CoT) and serves as a baseline for efficiency comparisons in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not detailed in this paper (standard instruction-tuned Llama family model); used as a conventional single-solution baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ASDIV, GSM8K, MATH500</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Math word problems / arithmetic (multi-step and single-step)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language math word problems (one-shot prompts/instruction-style)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>ASDIV (easy) to MATH500 (hard)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-following single-answer generation (no multi-round visible CoT in evaluated outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy, # solutions, # tokens, ξ_O, ξ_P (process efficiency for single-solution models equals accuracy and 1.0 respectively)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ASDIV: 95.6% accuracy; #solutions 1.0; avg tokens 166.4; ξ_O 95.6%; ξ_P 100.0%. GSM8K: 92.6% acc; #sol 1.0; tokens 220.3; ξ_O 92.6%; ξ_P 100.0%. MATH500: 75.4% acc; #sol 1.0; tokens 553.4; ξ_O 75.4%; ξ_P 100.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Used as processing/splitting tool for other analyses (paper used Llama-3.3-70B to separate solutions in o1 outputs); no internal mechanistic analysis of arithmetic processing presented.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Standard baseline failure modes for harder competition math (drop in accuracy at MATH500), but not prone to the 'overthinking' redundancy issues observed in o1-like models.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Serves as a conventional baseline — single-solution behavior yields high process efficiency (ξ_P=1.0) even when accuracy is lower on hardest benchmarks; no additional size-scaling analysis presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6671.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6671.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-Math-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen 2.5 Math 72B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Math-specialized instruction-tuned large LLM used as a conventional single-solution baseline in the study; produces a single solution per prompt and attains high accuracy on easier benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-Math-72B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (decoder-only LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified here; model name implies math-specialized instruction tuning but this paper does not detail the pretraining or fine-tuning corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ASDIV, GSM8K, MATH500, GPQA, AIME24</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Math word problems and arithmetic (single-step and multi-step)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language math word problems (instructed prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>ASDIV (easy) to AIME (hard)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Instruction-following single-solution generation (no multi-round visible CoT in outputs used here)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy, #solutions, #tokens, ξ_O, ξ_P</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ASDIV: 96.3% accuracy; #sol 1.0; tokens 213.0; ξ_O 96.3%; ξ_P 100.0%. GSM8K: 95.8% acc; tokens 317.4. MATH500: 86.8% acc; tokens 593.1. GPQA: 46.5% acc; tokens 811.7. AIME24: 23.3% acc; tokens 1204.5.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Used as a conventional baseline; no internal mechanistic probing reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lower accuracy on very hard competition benchmarks (AIME), but does not exhibit multi-round overthinking behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>As a single-solution model, process efficiency remains 1.0; comparative trends in the paper focus on relative inefficiency of o1-like models rather than parameter scaling of Qwen2.5.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6671.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6671.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimPO + FCS+Reflection (mitigation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimPO post-training with First-Correct-Solution + Reflection simplification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mitigation approach combining Simple Preference Optimization (SimPO) fine-tuning and a response-simplification strategy (keep first-correct solution plus a reflection round) to reduce overthinking while preserving accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwQ-32B-Preview + SimPO (FCS+Reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>preference optimization / fine-tuning applied to transformer LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B (applied to QwQ-32B-Preview)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Self-generated training data from PRM12K (10 samples per instance), shortest-sample selection and simplified positive examples (FCS, FCS+Reflection, GDS) used for preference optimization</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ASDIV, GSM8K, MATH500, GPQA, AIME24</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Math word problems (same tasks as base models)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompts; model trained to prefer shorter / non-redundant multi-solution outputs</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>All levels tested (elementary to competition); simplification tuned to preserve performance on hard tasks</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Post-training via SimPO preference optimization on self-generated examples; positive examples constructed by FCS+Reflection (keep earliest correct solution and one reflective solution); generation still uses CoT but with learned length/diversity preferences</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy, #solutions, #tokens, ξ_O, ξ_P</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ASDIV: accuracy 96.8%; #solutions 2.0; tokens 381.6; ξ_O 77.6%; ξ_P 86.0%. GSM8K: 96.0% acc; #solutions 2.0; tokens 416.6; ξ_O 80.2%; ξ_P 87.2%. MATH500: 92.8% acc; #solutions 1.9; tokens 1330.7; ξ_O 80.0%; ξ_P 89.5%. GPQA: 59.1% acc; #solutions 1.7; tokens 2085.7; ξ_O 55.7%; ξ_P 90.4%. AIME24: 43.3% acc; #solutions 1.7; tokens 5154.5; ξ_O 39.8%; ξ_P 92.0%. (Compared to vanilla QwQ-32B-Preview, SimPO + FCS+Reflection substantially reduces tokens and increases both outcome and process efficiencies while largely preserving accuracy.)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Empirical evaluation shows SimPO (with FCS+Reflection) reduces average tokens and number of solution rounds; paper reports comparative tables showing the effect of SFT, DPO, RPO, SimPO and different positive-example strategies (Shortest, FCS, FCS+Reflection, GDS). No mechanistic probing into why preference optimization changes internal arithmetic representations is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Simplification strategies can trade off computation for accuracy on very hard problems: pure FCS (first-correct-only) produced the best efficiency but decreased accuracy on MATH500; FCS+Reflection is chosen as a compromise. Some complex problem types still require deeper multi-round inference and may be harmed by too aggressive simplification.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>SimPO reduced tokens by 22.3% on MATH500 when applied to QwQ-32B-Preview in this study; SFT and other preference methods provided smaller improvements. The paper frames these as post-training behavioral changes rather than traditional parameter/data scaling trends.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to reason with llms <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Let's verify step by step <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Qwq: Reflect deeply on the boundaries of the unknown <em>(Rating: 1)</em></li>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6671",
    "paper_id": "paper-84c149537dee8ef3a7ff54b6bb01e9a4c8eaa17c",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "QwQ-32B-Preview",
            "name_full": "Qwen QwQ 32B Preview (o1-like)",
            "brief_description": "An o1-like large language model that exposes long chain-of-thought (CoT) style visible thinking during inference and produces multiple solution rounds for math problems; used as the primary testbed in this paper for studying 'overthinking'.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "QwQ-32B-Preview",
            "model_family": "transformer (decoder-only LLM)",
            "model_size": "32B",
            "training_data_description": "Not specified in this paper; described as an o1-like model with visible long CoT behavior. This study uses PRM12K as self-training data for post-training experiments.",
            "benchmark_name": "ASDIV, GSM8K, MATH500, GPQA, AIME24",
            "task_type": "Math word problems including simple arithmetic (e.g., 2+3), multi-step grade-school and competition math",
            "problem_format": "Natural-language math word problems (open-ended answers); single-question prompts (examples include 'What is the answer of 2 plus 3?')",
            "difficulty_level": "ASDIV (elementary grade 1-6); GSM8K (grade-school); MATH500 (competition problems level 1-5); AIME (high difficulty)",
            "prompting_method": "Long chain-of-thought (o1-style visible CoT). Generation used sampling (10 samples per instance, temperature=1.0) and greedy decoding as baselines; analyses used solution-splitting via Llama-3.3-70B and clustering via GPT-4o. Post-training / mitigation used SFT, DPO, RPO, SimPO and response simplification heuristics (FCS, FCS+Reflection, GDS).",
            "performance_metric": "Accuracy (exact-answer), # solutions per response, # tokens generated per response, outcome efficiency (ξ_O), process efficiency (ξ_P)",
            "performance_value": "ASDIV: 96.9% accuracy; avg #solutions 3.5; avg tokens 741.8; outcome efficiency 41.9%; process efficiency 66.5%. GSM8K: 94.8% accuracy; #solutions 3.1; tokens 772.8; outcome 50.7%; process 67.6%. MATH500: 93.0% accuracy; #solutions 3.2; tokens 2407.9; outcome 52.3%; process 71.2%. GPQA: 59.6% acc; #solutions 2.2; tokens 3228.4; outcome 51.4%; process 84.3%. AIME24: 46.7% acc; #solutions 2.6; tokens 9480.9; outcome 38.4%; process 84.4%.",
            "internal_analysis": "No low-level mechanistic probing (no attention/activation/logit-lens analysis); empirical analyses include (1) splitting generated text into discrete 'solutions' using Llama-3.3-70B, (2) clustering solutions into strategy groups with GPT-4o, (3) measuring the 'first-correct' distribution (finding &gt;92% of correct answers appear in the first solution round), and (4) computing distinctness ratios per solution index showing decreasing novelty in later solutions. The paper therefore analyzes process- and outcome-level behavior rather than neuron-level mechanisms.",
            "failure_modes": "Primary failure mode is 'overthinking': producing many redundant solution rounds and long token outputs that rarely improve accuracy (later solutions usually duplicate earlier reasoning), low diversity in later solutions, disproportionate compute on trivially simple problems (e.g., 13 solutions for '2+3'), and inefficient token usage. Overthinking is more frequent on easier problems and wastes inference compute; simplification can hurt performance on hard problems if over-applied.",
            "scaling_trend": "Characterized behaviorally rather than by parameter scaling: o1-like behavior (long CoT) yields more solution rounds and tokens compared to conventional models; more solution rounds are produced for easier problems (counterintuitively), while token count increases with problem difficulty. Post-training mitigation (SimPO and simplification heuristics) reduces token usage (SimPO reduced tokens by 22.3% on MATH500) while largely preserving accuracy; pure shortest-response SFT and extreme FCS can substantially reduce tokens but sometimes reduce accuracy on hard benchmarks.",
            "uuid": "e6671.0",
            "source_info": {
                "paper_title": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "DeepSeek-R1",
            "name_full": "DeepSeek-R1 (o1-like)",
            "brief_description": "An o1-like LLM (DeepSeek family) that exhibits visible multi-round CoT when solving math problems; evaluated alongside QwQ-32B-Preview to study overthinking patterns.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1",
            "model_family": "transformer (decoder-only LLM)",
            "model_size": null,
            "training_data_description": "Not specified in this paper; described as an o1-like model with explicit multi-round CoT behavior.",
            "benchmark_name": "ASDIV, GSM8K, MATH500",
            "task_type": "Math word problems and arithmetic (multi-step reasoning)",
            "problem_format": "Natural-language math word problems with free-response answers",
            "difficulty_level": "ASDIV (easy), GSM8K (grade-school), MATH500 (competition levels)",
            "prompting_method": "Long chain-of-thought (o1-style) visible reasoning; sampled/generative decoding producing multiple solutions per prompt.",
            "performance_metric": "Accuracy, # solutions per response, # tokens per response, ξ_O (outcome efficiency), ξ_P (process efficiency)",
            "performance_value": "ASDIV: 97.1% accuracy; avg #solutions 4.5; tokens 845.0; outcome 45.9%; process 64.3%. GSM8K: 96.4% accuracy; #solutions 4.3; tokens 1056.3; outcome 48.9%; process 62.0%. MATH500: 96.4% accuracy; #solutions 4.3; tokens 2704.3; outcome 51.0%; process 66.2%.",
            "internal_analysis": "Same empirical analyses as for QwQ: solution-count distributions, first-correct distribution (initial rounds often correct), and distinctness ratio showing reduced novelty in later rounds. No neuron- or attention-level mechanistic analysis presented.",
            "failure_modes": "Overthinking (excessive redundant rounds and tokens), later solutions contribute minimally to correctness, repetition of reasoning strategies across rounds, greater inefficiency compared to conventional single-solution models.",
            "scaling_trend": "Behavioral: DeepSeek-R1 tends to generate even more solution rounds than QwQ (avg more rounds across datasets) and correspondingly longer outputs, leading to similar inefficiency patterns; no parameter-size scaling analysis provided.",
            "uuid": "e6671.1",
            "source_info": {
                "paper_title": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Llama-3.3-70B-Instruct",
            "name_full": "Llama-3.3-70B-Instruct",
            "brief_description": "A conventional instruction-tuned LLM that typically returns a single solution (no long visible CoT) and serves as a baseline for efficiency comparisons in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.3-70B-Instruct",
            "model_family": "transformer (decoder-only LLM)",
            "model_size": "70B",
            "training_data_description": "Not detailed in this paper (standard instruction-tuned Llama family model); used as a conventional single-solution baseline.",
            "benchmark_name": "ASDIV, GSM8K, MATH500",
            "task_type": "Math word problems / arithmetic (multi-step and single-step)",
            "problem_format": "Natural-language math word problems (one-shot prompts/instruction-style)",
            "difficulty_level": "ASDIV (easy) to MATH500 (hard)",
            "prompting_method": "Instruction-following single-answer generation (no multi-round visible CoT in evaluated outputs)",
            "performance_metric": "Accuracy, # solutions, # tokens, ξ_O, ξ_P (process efficiency for single-solution models equals accuracy and 1.0 respectively)",
            "performance_value": "ASDIV: 95.6% accuracy; #solutions 1.0; avg tokens 166.4; ξ_O 95.6%; ξ_P 100.0%. GSM8K: 92.6% acc; #sol 1.0; tokens 220.3; ξ_O 92.6%; ξ_P 100.0%. MATH500: 75.4% acc; #sol 1.0; tokens 553.4; ξ_O 75.4%; ξ_P 100.0%.",
            "internal_analysis": "Used as processing/splitting tool for other analyses (paper used Llama-3.3-70B to separate solutions in o1 outputs); no internal mechanistic analysis of arithmetic processing presented.",
            "failure_modes": "Standard baseline failure modes for harder competition math (drop in accuracy at MATH500), but not prone to the 'overthinking' redundancy issues observed in o1-like models.",
            "scaling_trend": "Serves as a conventional baseline — single-solution behavior yields high process efficiency (ξ_P=1.0) even when accuracy is lower on hardest benchmarks; no additional size-scaling analysis presented in this paper.",
            "uuid": "e6671.2",
            "source_info": {
                "paper_title": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Qwen2.5-Math-72B-Instruct",
            "name_full": "Qwen 2.5 Math 72B Instruct",
            "brief_description": "A Math-specialized instruction-tuned large LLM used as a conventional single-solution baseline in the study; produces a single solution per prompt and attains high accuracy on easier benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-Math-72B-Instruct",
            "model_family": "transformer (decoder-only LLM)",
            "model_size": "72B",
            "training_data_description": "Not specified here; model name implies math-specialized instruction tuning but this paper does not detail the pretraining or fine-tuning corpora.",
            "benchmark_name": "ASDIV, GSM8K, MATH500, GPQA, AIME24",
            "task_type": "Math word problems and arithmetic (single-step and multi-step)",
            "problem_format": "Natural-language math word problems (instructed prompts)",
            "difficulty_level": "ASDIV (easy) to AIME (hard)",
            "prompting_method": "Instruction-following single-solution generation (no multi-round visible CoT in outputs used here)",
            "performance_metric": "Accuracy, #solutions, #tokens, ξ_O, ξ_P",
            "performance_value": "ASDIV: 96.3% accuracy; #sol 1.0; tokens 213.0; ξ_O 96.3%; ξ_P 100.0%. GSM8K: 95.8% acc; tokens 317.4. MATH500: 86.8% acc; tokens 593.1. GPQA: 46.5% acc; tokens 811.7. AIME24: 23.3% acc; tokens 1204.5.",
            "internal_analysis": "Used as a conventional baseline; no internal mechanistic probing reported in this paper.",
            "failure_modes": "Lower accuracy on very hard competition benchmarks (AIME), but does not exhibit multi-round overthinking behaviors.",
            "scaling_trend": "As a single-solution model, process efficiency remains 1.0; comparative trends in the paper focus on relative inefficiency of o1-like models rather than parameter scaling of Qwen2.5.",
            "uuid": "e6671.3",
            "source_info": {
                "paper_title": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "SimPO + FCS+Reflection (mitigation)",
            "name_full": "SimPO post-training with First-Correct-Solution + Reflection simplification",
            "brief_description": "A mitigation approach combining Simple Preference Optimization (SimPO) fine-tuning and a response-simplification strategy (keep first-correct solution plus a reflection round) to reduce overthinking while preserving accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "QwQ-32B-Preview + SimPO (FCS+Reflection)",
            "model_family": "preference optimization / fine-tuning applied to transformer LLM",
            "model_size": "32B (applied to QwQ-32B-Preview)",
            "training_data_description": "Self-generated training data from PRM12K (10 samples per instance), shortest-sample selection and simplified positive examples (FCS, FCS+Reflection, GDS) used for preference optimization",
            "benchmark_name": "ASDIV, GSM8K, MATH500, GPQA, AIME24",
            "task_type": "Math word problems (same tasks as base models)",
            "problem_format": "Natural-language prompts; model trained to prefer shorter / non-redundant multi-solution outputs",
            "difficulty_level": "All levels tested (elementary to competition); simplification tuned to preserve performance on hard tasks",
            "prompting_method": "Post-training via SimPO preference optimization on self-generated examples; positive examples constructed by FCS+Reflection (keep earliest correct solution and one reflective solution); generation still uses CoT but with learned length/diversity preferences",
            "performance_metric": "Accuracy, #solutions, #tokens, ξ_O, ξ_P",
            "performance_value": "ASDIV: accuracy 96.8%; #solutions 2.0; tokens 381.6; ξ_O 77.6%; ξ_P 86.0%. GSM8K: 96.0% acc; #solutions 2.0; tokens 416.6; ξ_O 80.2%; ξ_P 87.2%. MATH500: 92.8% acc; #solutions 1.9; tokens 1330.7; ξ_O 80.0%; ξ_P 89.5%. GPQA: 59.1% acc; #solutions 1.7; tokens 2085.7; ξ_O 55.7%; ξ_P 90.4%. AIME24: 43.3% acc; #solutions 1.7; tokens 5154.5; ξ_O 39.8%; ξ_P 92.0%. (Compared to vanilla QwQ-32B-Preview, SimPO + FCS+Reflection substantially reduces tokens and increases both outcome and process efficiencies while largely preserving accuracy.)",
            "internal_analysis": "Empirical evaluation shows SimPO (with FCS+Reflection) reduces average tokens and number of solution rounds; paper reports comparative tables showing the effect of SFT, DPO, RPO, SimPO and different positive-example strategies (Shortest, FCS, FCS+Reflection, GDS). No mechanistic probing into why preference optimization changes internal arithmetic representations is provided.",
            "failure_modes": "Simplification strategies can trade off computation for accuracy on very hard problems: pure FCS (first-correct-only) produced the best efficiency but decreased accuracy on MATH500; FCS+Reflection is chosen as a compromise. Some complex problem types still require deeper multi-round inference and may be harmed by too aggressive simplification.",
            "scaling_trend": "SimPO reduced tokens by 22.3% on MATH500 when applied to QwQ-32B-Preview in this study; SFT and other preference methods provided smaller improvements. The paper frames these as post-training behavioral changes rather than traditional parameter/data scaling trends.",
            "uuid": "e6671.4",
            "source_info": {
                "paper_title": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to reason with llms",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Let's verify step by step",
            "rating": 2
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2
        },
        {
            "paper_title": "Qwq: Reflect deeply on the boundaries of the unknown",
            "rating": 1
        },
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.018019999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs</h1>
<p>Xingyu Chen ${ }^{\star, 1,2}$, Jiahao Xu ${ }^{\star, 1}$, Tian Liang ${ }^{\star, 1}$, Zhiwei He ${ }^{\star, 1,2}$, Jianhui Pang ${ }^{1}$, Dian Yu ${ }^{1}$, Linfeng Song ${ }^{1}$, Qiuzhi Liu ${ }^{1}$, Mengfei Zhou ${ }^{2}$, Zhuosheng Zhang ${ }^{2}$, Rui Wang ${ }^{\dagger 2}$, Zhaopeng $\mathrm{Tu}^{\dagger 1}$, Haitao $\mathrm{Mi}^{1}$, and Dong $\mathrm{Yu}^{1}$<br>${ }^{1}$ Tencent AI Lab<br>${ }^{2}$ Shanghai Jiao Tong University</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of overthinking issue in Figure (a): o1-like models (right panel) spend much more tokens than conventional LLMs (left and middle panels). Our method reduces the overthinking issue when applied to QwQ-32B-Preview (Figure (b)).</p>
<h4>Abstract</h4>
<p>The remarkable performance of models like the OpenAI o1 can be attributed to their ability to emulate human-like long-time thinking during inference. These models employ extended chain-of-thought (CoT) processes, exploring multiple strategies to enhance problem-solving capabilities. However, a critical question remains: How to intelligently and efficiently scale computational resources during testing. This paper presents the first comprehensive study on the prevalent issue of overthinking in these models, where excessive computational resources are allocated for simple problems with minimal benefit. We introduce novel efficiency metrics from both outcome and process perspectives to evaluate the rational use of computational resources by o1-like models. Using a self-training paradigm, we propose strategies to mitigate overthinking, streamlining reasoning processes without compromising accuracy. Experimental results show that our approach successfully reduces computational overhead while preserving model performance across a range of testsets with varying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>The OpenAI o1 model (OpenAI, 2024) and its replicas (Qwen, 2024; Guo et al., 2025; Kimi et al., 2025) exemplify the state-of-the-art in AI reasoning. Their success is largely attributed to mimicking human-like long-time thinking before responding to a question. Specifically, o1-like models cultivate a long chain-of-thoughts (CoT), explore multiple strategies, break down complex steps, and perform double-checking, which ultimately enhance their ability to tackle intricate reasoning tasks. This approach, known as "scaling test-time compute", involves allocating more computational resources during the model's inference phase to generally yield more accurate responses.
While effective, a critical yet underexplored question remains: Are we scaling test-time compute efficiently and intelligently? This study provides an initial exploration of this problem. We first observe that o1-like models exhibit significant overthinking issues. Specifically, they tend to expend excessive compute (in terms of tokens or thinking rounds) on questions that are exceptionally simple or for which the answer is already evident. For example, Figure 1(a) compares the token usage of o1-like models with conventional models when answering the question, "what is the answer of 2 plus 3?" On average, o1-like models consumed $\mathbf{1 , 9 5 3 \%}$ more tokens than conventional models to reach the same answer. Figure 2 illustrates a concrete example where o1-style thinking results in generating 13 solutions for this trivially simple question. Across extensive analyses of mathematical benchmarks, we found these overthinking patterns: (1) contribute minimally to improving accuracy, (2) lack diversity in reasoning strategies, and (3) occur more frequently with simple problems.</p>
<p>The overthinking observed in o1-like models reveals inefficiency in inference and highlights fundamental limitations in their reasoning and decision-making processes. We assert that reasoning involves not only accuracy but also the application of the appropriate level of complexity based on the problem's requirements. This insight motivates our exploration of studying and mitigating overthinking. To address this, we propose two metrics from both outcome and process perspectives to evaluate o1-like models' efficiency. These metrics help provide a comprehensive assessment of the efficiency of o1-like models, augmenting the commonly-used effectiveness metrics.
To mitigate overthinking without introducing external information, we adopt a self-training paradigm. With our proposed efficiency metrics, we streamline the generated responses by removing redundant solutions while maintaining basic reflexivity. Experimental results across testsets of varying difficulty levels (e.g., GSM8K, MATH500, GPQA, and AIME) demonstrate our approach's effectiveness and robustness in mitigating overthinking issues. For instance, as shown in Figure 1(b), our approach can reduce token output by $48.6 \%$ while maintaining accuracy on the widely-used MATH500 testset as applied to QwQ-32B-Preview.
In summary, our contributions are three-fold:</p>
<ol>
<li>We present the first study offering both a definitive explanation and comprehensive analysis of the overthinking issue, showing that o1-like LLMs often expend unnecessary computational resources on redundant solutions that contribute minimally to final outcomes.</li>
<li>We introduce metrics considering both outcome and process aspects to assess the efficiency of o1-like models.</li>
<li>We explore several strategies to mitigate overthinking, significantly reducing token generation while maintaining model performance across testsets of varying difficulty.</li>
</ol>
<h2>2 Observing Overthinking Issues</h2>
<p>In this section, we present a comprehensive analysis of outputs generated by o1-like models. First, we provide a basic illustration of the solution distribution in responses from these models (§ 2.1). We then identify two inefficiencies in long CoT responses: their limited contribution to accuracy (§ 2.2) and diversity (§ 2.3). To evaluate these inefficiencies empirically, we propose two efficiency metrics based on our observations. Finally, we present empirical results in $\S 2.4$ and conclude that o1-like models often overthink, particularly with easier math problems.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An example of overthinking issue for QwQ-32B-Preview model's output response that consists of 13 solutions. We also list the outputs of other conventional LLMs for reference.</p>
<h1>2.1 Solution Distribution of o1-Like Models</h1>
<p>Experimental Setup We conduct experiments on three testsets:</p>
<ul>
<li>ASDIV (Miao et al., 2020): an English math word problem corpus with 2,305 instances, each annotated with its problem type and grade level ( 1 to 6 , indicating difficulty). The test set covers three main problem types (i.e., basic arithmetic operations, aggregative operations, and additional domain knowledge required), typically found in elementary schools.</li>
<li>GSM8K (Cobbe et al., 2021): a dataset of high-quality, linguistically diverse grade school math word problems created by human problem writers. The test set includes 1,319 problems, with solutions often involving a sequence of elementary calculations using basic arithmetic. A middle school student should be able to solve every problem.</li>
<li>MATH500 (Hendrycks et al., 2021): a challenging dataset consisting of problems from high school math competitions across seven subjects (e.g., Prealgebra, Algebra, Number Theory) and difficulty levels based on AoPS (ranging from 1 to 5). Problems in these competitions range from level 1, the easiest, often found in AMC 8 exams, to level 5, like those in AIME.</li>
</ul>
<p>The overall difficulty levels of the test sets are $\mathrm{ASDIV}&lt;\mathrm{GSM} 8 \mathrm{~K}&lt;\mathrm{MATH} 500$.
We mainly investigate two widely recognized o1-like models featuring a visible thinking process: Qwen-QwQ-32B-Preview (Qwen, 2024) and DeepSeek-R1 (DeepSeek, 2025).</p>
<p>Solution Distribution In this paper, we define solution as part of the full model generation that contains an answer explicitly. For example, in Figure 2, each solution in the QwQ generation contains the answer 5. We use the Llama-3.3-70B model to separate solutions from generated responses. Figure 3 shows the distribution of solutions in generated responses. Generally, o1-like models produce 2 to 4 solution rounds for most instances, covering $76 \%$ to $80 \%$ of cases for QwQ-32BPreview across the test sets and $59 \%$ to $63 \%$ for DeepSeek-R1. Regarding different test sets, o1-like models tend to generate more solutions for easier test sets. For instance, the average number of</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Distribution of solution counts in generated responses for different test sets and models (QwQ-32B-Preview ("QwQ") and DeepSeek-R1 ("R1")).
solutions of QwQ-32B-Preview on the easiest ASDIV test set is 3.5, whereas on the most difficult MATH500 test set, it is 3.2. The numbers for DeepSeek-R1 are respectively 4.5 and 4.3.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Average rounds of solutions ("Solutions") and number of tokens ("Tokens") in generated responses across different difficulty levels of the MATH500 test set.</p>
<p>To empirically validate this finding, we conducted an analysis across various difficulty levels in the MATH500 test set, as illustrated in Figure 4. Both QwQ-32B-Preview and DeepSeek-R1 generate more solution rounds for problems at easier levels 1-2 (e.g., averaging 3.7 rounds and 4.6 rounds, respectively) compared to levels 4-5 (e.g., averaging 3.0 rounds and 3.9 rounds, respectively), despite the number of tokens consistently increasing with the difficulty level. These results support our claim that o1-like models tend to generate more solution rounds for easier math problems.</p>
<h1>2.2 Efficiency on Accuracy Improvements</h1>
<p>Intuition In the example in Figure 2, we observe that the initial round of solutions already yields the correct answer. Subsequent solutions, which account for the majority of generated tokens, do not enhance accuracy. Based on this observation, we empirically investigate whether later solutions contribute to accuracy improvements. Specifically, for all cases where o1-like models produce the correct answer in the response, we calculate the distribution of occurrences for the first correct answer, termed the "first correctness distribution". If more correct answers appear in earlier</p>
<p>solutions, then the subsequent solutions contribute minimally to accuracy improvement, indicating reduced efficiency.</p>
<p>Observation Figure 5 illustrates the first correctness distribution across the test sets and models. In more than $92 \%$ of cases, the initial round of solutions produces the correct answer. Notably, the first round generally comprises less than $60 \%$ of the total tokens generated, suggesting that the extended CoT might not significantly enhance accuracy. For instance, the average length of the first round of solutions for QwQ-32B-Preview on the ASDIV test set is 287 tokens, constituting only $38.7 \%$ of the entire response. These results suggest that later solutions marginally contribute to improvements in accuracy.</p>
<p>Outcome Efficiency Metric Based on the above observation, we propose an outcome efficiency metric to empirically evaluate how effectively later solutions contribute to accuracy improvements. The outcome efficiency metric, denoted $\xi_{O}$, is defined by the following formula:</p>
<p>$$
\xi_{O}=\frac{1}{N} \sum_{i=1}^{N} \sigma_{i} \frac{\hat{T}<em i="i">{i}}{T</em>
$$}</p>
<p>where $N$ is the number of instances in a given test set, $T_{i}$ is the total number of tokens produced for the $i$-th instance, and $\hat{T}_{i}$ denotes the efficient tokens that contribute to reaching the correct answer:</p>
<p>$$
\hat{T}<em i="i">{i}= \begin{cases}# \text { tokens to first arrive at correct answer, } &amp; \sigma</em>
$$}=1 \ T_{i}, &amp; \sigma_{i}=0\end{cases</p>
<p>$\sigma_{i}$ denotes whether the evaluated model can produce a correct answer in the response:</p>
<p>$$
\sigma_{i}= \begin{cases}1, &amp; \text { if at least one solution in response is correct } \ 0, &amp; \text { otherwise }\end{cases}
$$</p>
<p>Intuitively, if a model correctly answers at an early stage, the tokens generated thereafter do not contribute to improving accuracy and are considered inefficient. Consider Figure 2 as an example: The first solution correctly addresses the problem with $\hat{T}=39$. Consequently, $\xi_{O}=\frac{39}{901}=4.3 \%$, which can be considered extremely inefficient.</p>
<h1>2.3 Efficiency on Diverse Thinking</h1>
<p>Intuition Some researchers might argue that while solving an easy math problem may appear straightforward, approaching it from different perspectives can deepen understanding and build flexibility in mathematical thinking, which is also valuable. Consider the example output of QwQ-32B-Preview in Figure 2: Solution 1 states the basic fact that 2 plus 3 equals 5; Solution 2 breaks the addition into smaller steps; Solution 3 uses a counting objects analogy. These three solutions provide different reasoning strategies. However, Solution 4 repeats Solution 3, and Solution 5 repeats Solution 2 using similar reasoning strategies. In this section, we empirically examine the diversity among solutions within a response.</p>
<p>Observation To empirically evaluate whether later solutions provide new reasoning strategies, we introduce the "distinctness ratio" as the measure for the ratio of distinct solutions for each data index. Consider $R_{i}=\left{s_{i}^{1}, \ldots, s_{i}^{m}, \ldots, s_{i}^{M_{i}}\right}$ as the set of $M_{i}$ solutions in the $i$-th instance response.</p>
<p>Let $S^{m}=\left{s_{1}^{m}, \ldots, s_{k}^{m}, \ldots, s_{K}^{m}\right}$ be the set of $m$-th solutions in the responses of all instances in the test subset. ${ }^{1}$ The distinctness ratio is defined as:</p>
<p>$$
\operatorname{Dis}^{m}=\frac{\sum_{k=1}^{K} \tau_{k}^{m}}{K}
$$</p>
<p>where</p>
<p>$$
\tau_{k}^{m}= \begin{cases}1, &amp; \text { if } \Phi\left(s_{k}^{m}\right) \not \mathcal{C}\left{\Phi\left(s_{k}^{1}\right), \ldots, \Phi\left(s_{k}^{m-1}\right)\right} \ 0, &amp; \text { otherwise }\end{cases}
$$</p>
<p>In this context, $\Phi\left(s_{k}^{m}\right)$ is the reasoning strategy of $s_{k}^{m}$. We use GPT-4o to cluster the solutions for each instance into groups via a prompt like (Ye et al., 2024). ${ }^{2}$ The clustering results for the QwQ-32B-Preview response in Figure 2 are:
cluster1 [Solution 1, Solution 6, Solution 11] stating or affirming the basic arithmetic fact that 2 plus 3 equals 5 .
cluster2 [Solution 2, Solution5] breaking down the addition into smaller, simpler steps to reach the result.
cluster3 [Solution 3, Solution 4] using a practical analogy of counting objects to explain the addition.
cluster4 [Solution 7] using subtraction as a reverse check to verify the addition result.
cluster5 [Solution 8] using algebraic manipulation and solving simple equations to confirm the result.
cluster6 [Solution 9, Solution 10] converting numbers into different systems (binary and Roman numerals) to verify the result.
cluster7 [Solution 12, Solution 13] considering specific contexts or frameworks like modular arithmetic or programming which could change traditional addition results.</p>
<p>Figure 6 displays the distinctness ratio for each solution index. Intuitively, the ratio for Solution#1 is always $100 \%$, as it has no preceding solutions, thus $\tau \equiv 1$ for all instances. Generally, the ratio decreases with higher indices, indicating that later solutions often repeat earlier ones. For example, the average distinctness ratio for Solution# $\geq 4$ across test sets decreases by $11.5 \%$ compared to Solution#3. The ratio of Solution#2 significantly decreases, underperforming Solution#3. By reviewing outputs, we find that Solution#2 often doublechecks answers from Solution#1 using the same reasoning strategy. Subsequently, Solution#3 attempts to solve the problem using a new reasoning strategy.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Ratio of whether a solution provides a new reasoning strategy for each index.</p>
<p>Process Efficiency Metric Based on the above observation, we propose a process efficiency metric to empirically evaluate the contribution of later solutions to solution diversity. The process efficiency metric, denoted $\xi_{P}$, is calculated using the formula:</p>
<p>$$
\xi_{P}=\frac{1}{N} \sum_{i=1}^{N} \frac{D_{i}}{T_{i}}
$$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Model efficiency results of strong LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Response</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Efficiency</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">#Solution</td>
<td style="text-align: center;">#Token</td>
<td style="text-align: center;">Outcome</td>
<td style="text-align: center;">Process</td>
</tr>
<tr>
<td style="text-align: center;">ASDIV</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.3-70B-Instruct</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">166.4</td>
<td style="text-align: center;">$95.6 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-72B-Instruct</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">213.0</td>
<td style="text-align: center;">$96.3 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">QwQ-32B-Preview</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">741.8</td>
<td style="text-align: center;">$41.9 \%$</td>
<td style="text-align: center;">$66.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">DeepSeek-R1</td>
<td style="text-align: center;">97.1</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">845.0</td>
<td style="text-align: center;">$45.9 \%$</td>
<td style="text-align: center;">$64.3 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.3-70B-Instruct</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">220.3</td>
<td style="text-align: center;">$92.6 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-72B-Instruct</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">317.4</td>
<td style="text-align: center;">$95.8 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">QwQ-32B-Preview</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">772.8</td>
<td style="text-align: center;">$50.7 \%$</td>
<td style="text-align: center;">$67.6 \%$</td>
</tr>
<tr>
<td style="text-align: center;">DeepSeek-R1</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">1056.3</td>
<td style="text-align: center;">$48.9 \%$</td>
<td style="text-align: center;">$62.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">MATH500</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Llama-3.3-70B-Instruct</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">553.4</td>
<td style="text-align: center;">$75.4 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-72B-Instruct</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">593.1</td>
<td style="text-align: center;">$86.8 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">QwQ-32B-Preview</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">2407.9</td>
<td style="text-align: center;">$52.3 \%$</td>
<td style="text-align: center;">$71.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">DeepSeek-R1</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">2704.3</td>
<td style="text-align: center;">$51.0 \%$</td>
<td style="text-align: center;">$66.2 \%$</td>
</tr>
</tbody>
</table>
<p>where $D_{i}$ represents the number of efficient tokens that contribute to the solutions' diversity. Here, we intentionally exclude the factor $\sigma_{i}$ to concentrate on diversity, independent of correctness. Let $T_{i}^{m}$ denote the number of tokens in solution $s_{i}^{m}$. We define:</p>
<p>$$
D_{i}=\sum_{m=1}^{M} \tau_{i}^{m} T_{i}^{m}
$$</p>
<p>Intuitively, the tokens in a distinct solution are regarded as process efficient tokens. In the example shown in Figure 2, the 13 solutions are categorized into 7 distinct reasoning strategies. Consequently, tokens in Solutions $1,2,3,7,8,9$, and 12 are efficient, resulting in $\xi_{P}=\frac{(39+109+39+29+29+19+59)}{901}=$ $35.8 \%$.</p>
<h1>2.4 Empirical Efficiency Results</h1>
<p>Table 1 presents the results on model efficiency. For comparison, we include two representative conventional LLMs: Llama-3.3-70B-Instruct and Qwen2.5-Math-72B-Instruct. These conventional LLMs produce only a single solution, meaning that $\frac{D_{i}}{T_{i}}=\frac{\hat{T}<em i="i">{i}}{T</em>=1.0$. In comparison, o1-like models generate significantly longer responses, which are less efficient in improving accuracy and solution diversity. We refer to the inefficient use of generated tokens as the "overthinking issue". The experimental results demonstrate that while o1-like models have the capacity to generate multiple solutions, their efficiency is hindered by the overthinking issue.
Figure 7 presents the detailed efficiency results across various difficulty levels of the MATH500 test set. Notably, both models perform poorly on the simplest Level 1 problems, achieving less than $50 \%$ outcome efficiency, a pattern that corresponds with results observed on the easy ASDIV test set. These findings underscore that the overthinking issues faced by o1-like models are particularly pronounced with simpler math problems.}}=1$. Therefore, in these cases, the outcome efficiency metric $\xi_{O}=\frac{1}{N} \sum_{i=1}^{N} \sigma_{i}$ equals accuracy, and the process efficiency metric $\xi_{P</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Efficiency results of (a) QwQ-32B-Preview and (b) DeepSeek-R1 across different difficulty levels of the MATH500 testset.</p>
<p>Table 2: Statistics on different types of generated responses based on the training data. "Greedy" denotes responses generated via greedy search; "Shortest" and "Longest" indicate the shortest and longest responses among 10 samples, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Response</th>
<th style="text-align: center;">#Solution</th>
<th style="text-align: center;">#Token</th>
<th style="text-align: center;">Efficiency</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Outcome</td>
<td style="text-align: center;">Process</td>
</tr>
<tr>
<td style="text-align: center;">Greedy</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">1434.8</td>
<td style="text-align: center;">$55.6 \%$</td>
<td style="text-align: center;">72.6\%</td>
</tr>
<tr>
<td style="text-align: center;">Shortest</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">1051.3</td>
<td style="text-align: center;">69.8\%</td>
<td style="text-align: center;">80.3\%</td>
</tr>
<tr>
<td style="text-align: center;">Longest</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">2258.7</td>
<td style="text-align: center;">46.0\%</td>
<td style="text-align: center;">66.4\%</td>
</tr>
</tbody>
</table>
<h1>3 Mitigating Overthinking Issues</h1>
<p>In this section, we explore several strategies aimed at enhancing the efficiency of o1-like models. We adopt the settings for LLM reasoning tasks and primarily utilize the self-training strategy (Zelikman et al., 2022; Ho et al., 2023), where the model itself generates the training data. Consistent with previous studies, we employ the PRM12K dataset (Lightman et al., 2024) as our training dataset to generate self-training data. The QwQ-32B-Preview model serves as our testing platform because it is available for post-training.</p>
<h3>3.1 Length Preference Optimization</h3>
<p>We began by assessing whether the model could produce more efficient responses. We generated 10 samples for each instance in the training dataset with a temperature of 1.0. We discard samples that failed to generate a correct answer. Table 2 presents the statistics of different types of generated responses. Our analysis of these sampled responses reveals that the shortest response performs better in terms of both outcome and process efficiency, using fewer rounds and tokens. These findings support our initiative to enhance model efficiency through self-improvement.
We explore several effective methods for self-improvement:</p>
<ul>
<li>Supervised Fine-Tuning (SFT; Wei et al. 2022a): This method involves fine-tuning a model using positive synthetic data. The model learns to map inputs to preferred outputs by minimizing the cross-entropy loss between predicted and actual outputs. SFT enables the model to mimic the behavior demonstrated in training examples.</li>
</ul>
<p>Table 3: Statistics on different types of positive examples. "#S" denotes the number of solutions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Positive Example</th>
<th style="text-align: center;">#S</th>
<th style="text-align: center;">#Token</th>
<th style="text-align: center;">Efficiency</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Outcome</td>
<td style="text-align: center;">Process</td>
</tr>
<tr>
<td style="text-align: center;">Shortest Response</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">1051.3</td>
<td style="text-align: center;">$69.8 \%$</td>
<td style="text-align: center;">$80.3 \%$</td>
</tr>
<tr>
<td style="text-align: center;">FCS</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">681.0</td>
<td style="text-align: center;">$99.5 \%$</td>
<td style="text-align: center;">$99.1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">FCS + Ref.</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">878.7</td>
<td style="text-align: center;">$78.4 \%$</td>
<td style="text-align: center;">$82.4 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GDS</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">856.8</td>
<td style="text-align: center;">$86.8 \%$</td>
<td style="text-align: center;">$94.2 \%$</td>
</tr>
</tbody>
</table>
<ul>
<li>Direct Preference Optimization (DPO; Rafailov et al. 2024): This method trains a model directly on human-preferred responses to increase the likelihood of preferred responses over unpreferred ones.</li>
<li>Reasoning Preference Optimization (RPO; Pang et al. 2024; Liu et al. 2024): This approach modifies the DPO loss by adding a negative log-likelihood term on the preferred response. RPO enhances DPO training stability by preventing a decreased probability of selected responses.</li>
<li>Simple Preference Optimization (SimPO; Meng et al. 2024): This method addresses the discrepancy between the reward function and the generation metric during inference found in other preference optimization methods. SimPO incorporates techniques like adaptive margin and length regularization into DPO training.</li>
</ul>
<p>Apart from the SFT method, which uses only the shortest sampled response as training data, the other three preference optimization methods require contrastive instance pairs (positive, negative). It is straightforward to use the response generated by greedy search as the negative example, aligning with the real-time inference scenario. However, in our preliminary experiments, we found it less effective than using the longest sampled response as the negative example. One possible reason is that the longest sampled response provides a clearer contrastive signal.</p>
<h1>3.2 Streamlining Responses to Enhance Efficiency</h1>
<p>Although shorter sampled responses improve the efficiency of o1-like models, they still suffer from overthinking issues. Based on the observations in Section 2, where earlier solutions in the response are more efficient, we further streamline the responses to enhance efficiency. We propose three types of simplification strategies that differ in how they streamline the responses from the beginning:</p>
<ul>
<li>First-Correct Solutions (FCS): This strategy retains the earliest solutions that first arrive at the correct answer.</li>
<li>FCS+Reflection: Since the majority of responses achieve the correct answer on the first solution (see Figure 5), maintaining only the First-Correct Solutions might cause o1-like models to revert to conventional LLM behavior. To counter this, we extend the approach to include the second solution that reaches the correct answer in positive examples, recalling the model's long-reflective capability while maintaining efficiency.</li>
<li>Greedily Diverse Solutions (GDS): Figure 6 demonstrates that the distinctiveness of Solution#2 significantly decreases because the second solution often double-checks answers from the first using the same reasoning strategy. Consequently, FCS+Reflection may reduce efficiency. To address this issue, we propose a simple heuristic that greedily expands solutions providing new reasoning strategies. Additionally, this strategy includes more solutions when the second solution does not repeat the first, thereby increasing diversity.</li>
</ul>
<p>For each instance, we select the shortest result of each type from 10 samples. Consequently, the three types of simplified responses may originate from different original responses. Table 3 presents the statistics for these simplified responses. Notably, all simplified responses enhance efficiency compared to the shortest response. "FCS" is the most efficient, both in terms of outcome and process,</p>
<p>Table 4: Experimental results of the proposed efficiency enhancing methods.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Response</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Efficiency</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">#Solution</td>
<td style="text-align: center;">#Token</td>
<td style="text-align: center;">Outcome</td>
<td style="text-align: center;">Process</td>
</tr>
<tr>
<td style="text-align: center;">ASDIV</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">QwQ-32B-Preview</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">741.8</td>
<td style="text-align: center;">$41.9 \%$</td>
<td style="text-align: center;">$66.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$+\operatorname{SimPO}_{\text {FCS+Reflection }}$</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">381.6</td>
<td style="text-align: center;">$77.6 \%$</td>
<td style="text-align: center;">$86.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">QwQ-32B-Preview</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">772.8</td>
<td style="text-align: center;">$50.7 \%$</td>
<td style="text-align: center;">$67.6 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$+\operatorname{SimPO}_{\text {FCS+Reflection }}$</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">416.6</td>
<td style="text-align: center;">$80.2 \%$</td>
<td style="text-align: center;">$87.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">MATH500</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">QwQ-32B-Preview</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">2407.9</td>
<td style="text-align: center;">$52.3 \%$</td>
<td style="text-align: center;">$71.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$+\mathrm{SFT}_{\text {Shortest Response }}$</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">2359.5</td>
<td style="text-align: center;">$60.4 \%$</td>
<td style="text-align: center;">$75.6 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$+\mathrm{DPO}_{\text {Shortest Response }}$</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">1929.5</td>
<td style="text-align: center;">$65.8 \%$</td>
<td style="text-align: center;">$79.1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$+\mathrm{RPO}_{\text {Shortest Response }}$</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">2015.7</td>
<td style="text-align: center;">$64.8 \%$</td>
<td style="text-align: center;">$79.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$+\operatorname{SimPO}_{\text {Shortest Response }}$</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">1871.8</td>
<td style="text-align: center;">$67.6 \%$</td>
<td style="text-align: center;">$80.9 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$+\operatorname{SimPO}_{\text {First-Correct Solution }}$</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">1016.0</td>
<td style="text-align: center;">$88.7 \%$</td>
<td style="text-align: center;">$98.1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$+\operatorname{SimPO}_{\text {FCS+Reflection (Ours) }}$</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">1330.7</td>
<td style="text-align: center;">$80.0 \%$</td>
<td style="text-align: center;">$89.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$+\operatorname{SimPO}_{\text {Greedily Diverse Solutions }}$</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">1286.1</td>
<td style="text-align: center;">$84.3 \%$</td>
<td style="text-align: center;">$93.6 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GPQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-72B-Instruct</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">811.7</td>
<td style="text-align: center;">$46.5 \%$</td>
<td style="text-align: center;">$100 \%$</td>
</tr>
<tr>
<td style="text-align: center;">QwQ-32B-Preview</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">3228.4</td>
<td style="text-align: center;">$51.4 \%$</td>
<td style="text-align: center;">$84.3 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$+\operatorname{SimPO}_{\text {FCS+Reflection }}$</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">2085.7</td>
<td style="text-align: center;">$55.7 \%$</td>
<td style="text-align: center;">$90.4 \%$</td>
</tr>
<tr>
<td style="text-align: center;">AIME24</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-Math-72B-Instruct</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1204.5</td>
<td style="text-align: center;">$23.3 \%$</td>
<td style="text-align: center;">$100.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">QwQ-32B-Preview</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">9480.9</td>
<td style="text-align: center;">$38.4 \%$</td>
<td style="text-align: center;">$84.4 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$+\operatorname{SimPO}_{\text {FCS+Reflection }}$</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">5154.5</td>
<td style="text-align: center;">$39.8 \%$</td>
<td style="text-align: center;">$92.0 \%$</td>
</tr>
</tbody>
</table>
<p>using the fewest number of solution rounds and tokens. "FCS+Reflection" incorporates reflection, requiring approximately one additional solution round, which reduces both outcome and process efficiencies. "Greedily Diverse Solutions" serves as a compromise, balancing the number of solutions and tokens, and achieving moderate to high efficiency.</p>
<h1>3.3 Experimental Results</h1>
<p>Table 4 presents the results of the proposed methods. We perform a detailed comparison on MATH500 and validate the most effective approach using the other test sets.</p>
<p>Performance of Length Preference Optimization Methods SFT only slightly reduces the number of solution rounds and tokens compared to the vanilla QwQ-32B-Preview model, underperforming the preference optimization methods. Among these methods, SimPO achieves the best results, reducing the number of generated tokens by $22.3 \%$ on MATH500. Consequently, SimPO is used as the default post-training method in the subsequent experiments.</p>
<p>Performance of Response Simplification Methods As anticipated, the First-Correction Solutions strategy achieves the greatest reduction in length. However, this method decreases performance on the difficult MATH500 test set, which may require more rounds of reflection. The "FCS+Reflection" approach alleviates this issue and surpasses the FCS method by $1.4 \%$ with an additional round of reflection. The "Greedily Diverse Solutions" strategy balances performance with the number of generated tokens. However, it significantly underperforms compared to "FCS+Reflection",</p>
<p>reinforcing our claim that the difficult MATH500 test set requires the deep inference provided by o1-like models. Hence, we adopt "FCS+Reflection" as the default response simplification method.</p>
<p>Results on Challenging Test Sets Our approach enhances performance on easier testsets such as ASDIV and GSM8K with fewer tokens, demonstrating the effectiveness and versatility of our method in addressing overthinking issues. To address the concerns of some researchers that our approach might weaken the ability of o1-like models to tackle complex problems requiring long-term reasoning, we validate our method using more challenging GPQA and AIME test sets. As seen, our approach maintains model performance while using fewer tokens, demonstrating the robustness and generalization capability of our approach.</p>
<h1>4 Related Work</h1>
<h3>4.1 Scaling Test-Time Compute</h3>
<p>Enhancing model performance on complex tasks can be achieved by scaling test-time compute, which involves:</p>
<p>Expanding Search Space LLMs have strong reasoning abilities, but their auto-regressive decoding often misses optimal solutions. Self-consistency generates multiple responses and use majority voting to select the best answer (Wang et al., 2023b). Other approaches include best-of-n decoding, minimum Bayes risk decoding (Lightman et al., 2024; Li et al., 2023; Khanov et al., 2024; Heineman et al., 2024; Wu et al., 2024), and structured search methods such as Tree-of-Thought, Graph-ofThought, and Monte Carlo Tree Search (Yao et al., 2024; Besta et al., 2024; Luo et al., 2024; Tian et al., 2024; Wan et al., 2024).</p>
<p>Human-Like Thinking Patterns LLMs often use natural language reasoning. Techniques like chain-of-thought encourage step-by-step reasoning instead of direct answers (Wei et al., 2022b; Kojima et al., 2022). This has been expanded with methods like debating, self-correction, selfcritique, and plan-and-solve (Liang et al., 2024; Du et al., 2024; Xiong et al., 2023; Kumar et al., 2024; Kamoi et al., 2024; Ke et al., 2023; Lin et al., 2024; Yu et al., 2024; Wang et al., 2023a). Recent studies also explore latent space reasoning to mimic human cognition (Hao et al., 2024; Goyal et al., 2024). Advanced models combine these patterns into extensive chains-of-thought, improving accuracy with more reasoning time (OpenAI, 2024).</p>
<h3>4.2 Efficient Thinking</h3>
<p>Scaling the search space and scaling human-like thinking involves two distinct aspects of efficiency: efficient search and efficient thinking. However, few works specifically focus on efficient thinking in LLMs. Kimi et al. (2025) leveraged the long to short strategy to compress generation context. Zhao et al. (2024) encourages the model to terminate reasoning by saying "I don't know" when the problem is hard to solve. Han et al. (2024) introduces token-budget-aware reasoning, where the model is prompted with a specified token budget to guide its reasoning process. There are also several contributions made to predict the distribution of the computation budget and allocate the computation power based on the prompt's difficulty (Damani et al., 2024; Wang et al., 2024; Xu et al., 2024). Another line of work emphasizes the early stopping strategy to save computation budget while reasoning (Manvi et al., 2024; Li et al., 2024). Moreover, multi-agent framework utilizes large LLMs for difficult tasks while small LLMs for simple tasks (Kirchner et al., 2024; Damani et al., 2024)</p>
<p>In summary, all the aforementioned works consider conventional models rather than o1-like models with longer chains-of-thought. In contrast, our work first identifies the overthinking problem in o1-like model. Additionally, instead of limiting the reasoning space or leaving the token budget to be specified by the user, we aim to train the model to learn how to think efficiently.</p>
<h1>5 Conclusion</h1>
<p>This study identifies a key challenge in o1-like LLMs - efficient and intelligent scaling of test-time computational resources. We have presented a comprehensive analysis of the overthinking issue in o1-like LLMs. By highlighting the overthinking phenomenon and proposing efficiency metrics, we enhance our understanding of resource utilization in o1-like models. Our self-training based approach effectively mitigates overthinking, reducing unnecessary computation while maintaining performance across reasoning benchmarks of varying difficulty levels.
This work not only improves model efficiency but also sets the groundwork for future research on optimizing computational resource allocation in AI reasoning tasks. Future directions include exploring adaptive compute strategies that dynamically adjust to problem complexity and refining efficiency metrics for broader model generalization.</p>
<h2>References</h2>
<p>Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 17682-17690, 2024.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv:2110.14168, 2021.</p>
<p>Mehul Damani, Idan Shenfeld, Andi Peng, Andreea Bobu, and Jacob Andreas. Learning how hard to think: Input-adaptive allocation of lm computation, 2024. URL https://arxiv.org/abs/2410. 04707 .</p>
<p>DeepSeek. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025. URL https://api.semanticscholar.org/CorpusID:275789950.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning, 2024.</p>
<p>Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=ph04CRkPdC.</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.</p>
<p>Tingxu Han, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen, and Zhenting Wang. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547, 2024.</p>
<p>Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in a continuous latent space, 2024. URL https: //arxiv.org/abs/2412.06769.</p>
<p>David Heineman, Yao Dou, and Wei Xu. Improving minimum bayes risk decoding with multiprompt. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 22525-22545, 2024.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In NeurIPS, 2021.</p>
<p>Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14852-14882, 2023.</p>
<p>Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. When can llms actually correct their own mistakes? a critical survey of self-correction of llms. Transactions of the Association for Computational Linguistics, 12:1417-1440, 2024.</p>
<p>Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, et al. Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation. corr, abs/2311.18702. detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 9004-9017, 2023.</p>
<p>Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. Args: Alignment as reward-guided search. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Team Kimi, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025.</p>
<p>Jan Hendrik Kirchner, Yining Chen, Harri Edwards, Jan Leike, Nat McAleese, and Yuri Burda. Proververifier games improve legibility of llm outputs, 2024. URL https://arxiv.org/abs/2407.13692.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 22199-22213, 2022.</p>
<p>Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315-5333, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.291. URL https://aclanthology. org/2023.acl-long. 291.</p>
<p>Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, and Kan Li. Escape sky-high cost: Early-stopping self-consistency for multi-step reasoning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=ndR8Ytrzhh.</p>
<p>Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multiagent debate. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 17889-17904, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.992. URL https://aclanthology.org/2024.emnlp-main.992.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=v8L0pN6EDi.</p>
<p>Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. CriticBench: Benchmarking LLMs for critique-correct reasoning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 1552-1587,</p>
<p>Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.findings-acl.91. URL https://aclanthology.org/2024.findings-acl.91.</p>
<p>Zhihan Liu, Miao Lu, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet, and Zhaoran Wang. Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer. arXiv preprint arXiv:2405.16436, 2024.</p>
<p>Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024.</p>
<p>Rohin Manvi, Anikait Singh, and Stefano Ermon. Adaptive inference-time compute: Llms can predict if they can do better, even mid-generation, 2024. URL https://arxiv.org/abs/2410.02725.</p>
<p>Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a referencefree reward. In Advances in Neural Information Processing Systems (NeurIPS), 2024.</p>
<p>Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.</p>
<p>OpenAI. Learning to reason with llms. https://openai.com/index/ learning-to-reason-with-llms, 2024.</p>
<p>Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, and Jason E Weston. Iterative reasoning preference optimization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=4XIKfvNYvx.</p>
<p>Qwen. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL https: //qwenlm.github.io/blog/qwq-32b-preview/.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. Toward selfimprovement of llms via imagination, searching, and criticizing. arXiv preprint arXiv:2404.12253, 2024.</p>
<p>Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. In Forty-first International Conference on Machine Learning, 2024.</p>
<p>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2609-2634, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/ v1/2023.acl-long.147. URL https://aclanthology.org/2023.acl-long.147.</p>
<p>Xinglin Wang, Shaoxiong Feng, Yiwei Li, Peiwen Yuan, Yueqi Zhang, Boyuan Pan, Heda Wang, Yao Hu, and Kan Li. Make every penny count: Difficulty-adaptive self-consistency for cost-efficient reasoning, 2024. URL https://arxiv.org/abs/2408.13457.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023b. URL https: //openreview.net/forum?id=1PL1NIMdrw.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022a.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022b.</p>
<p>Ian Wu, Patrick Fernandes, Amanda Bertsch, Seungone Kim, Sina Pakazad, and Graham Neubig. Better instruction-following through minimum bayes risk. arXiv preprint arXiv:2410.02902, 2024.</p>
<p>Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. Examining inter-consistency of large language models collaboration: An in-depth analysis via debate. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 7572-7590, 2023.</p>
<p>Mayi Xu, Yongqi Li, Ke Sun, and Tieyun Qian. Adaption-of-thought: Learning question difficulty improves large language models for reasoning. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 5468-5495, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.313. URL https://aclanthology.org/2024.emnlp-main. 313/.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.</p>
<p>Junyi Ye, Jingyi Gu, Xinyun Zhao, Wenpeng Yin, and Guiling Wang. Assessing the creativity of llms in proposing novel solutions to mathematical problems. arXiv preprint arXiv:2410.18336, 2024.</p>
<p>Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, et al. Self-generated critiques boost reward modeling for language models. arXiv preprint arXiv:2411.16646, 2024.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488, 2022.</p>
<p>Zirui Zhao, Hanze Dong, Amrita Saha, Caiming Xiong, and Doyen Sahoo. Automatic curriculum expert iteration for reliable llm reasoning, 2024. URL https://arxiv.org/abs/2410.07627.</p>
<h1>A Appendix</h1>
<h2>A. 1 Case Overview for Deepseek-R1-Preview</h2>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Deepseek-R1-Preview (936 tokens)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Solution1 158 tokens</td>
<td style="text-align: center;">Alright, so I've got this problem here: "What is the answer of 2 plus 3?" At first glance, it seems pretty straightforward, but I want to make sure I understand it fully. Okay, so addition is one of the basic operations in arithmetic, right? It's combining two quantities to find their total. So, I need to add 2 and 3. Let me think about what 2 and 3 represent. If I have 2 apples and someone gives me 3 more apples, how many apples do I have in total? Let me count: one, two (that's the original 2), and then three, four, five (that's the additional 3). So, altogether, I have five apples. That seems correct.</td>
</tr>
<tr>
<td style="text-align: center;">Solution2 64 tokens</td>
<td style="text-align: center;">Wait, maybe I should use my fingers to double-check. I'll hold up two fingers on one hand and three on the other. Now, if I count them all together: one, two, three, four, five. Yep, that's five fingers. So, 2 plus 3 equals 5 .</td>
</tr>
<tr>
<td style="text-align: center;">Solution3 67 tokens</td>
<td style="text-align: center;">Is there another way to look at this? Maybe using a number line. If I start at 2 and move 3 units to the right, where do I end up? Starting at 2 , one step to 3 , another to 4 , and another to 5 . So again, I'm at 5 .</td>
</tr>
<tr>
<td style="text-align: center;">Solution4 154 tokens</td>
<td style="text-align: center;">Hmm, I'm consistently getting 5 as the answer. I should consider if there's any reason to think otherwise. Could there be a different interpretation of "plus" or the numbers 2 and 3? In standard arithmetic, plus means addition, and 2 and 3 are positive integers. I don't think there's any trick here. <br> Let me try adding them in a different order, just to see if it changes anything. So, 3 plus 2. Using the same methods: 3 apples plus 2 apples is 5 apples, or 3 fingers plus 2 fingers is 5 fingers, or starting at 3 on the number line and moving 2 units to the right lands me at 5 . Still 5 .</td>
</tr>
<tr>
<td style="text-align: center;">Solution5 46 tokens</td>
<td style="text-align: center;">Okay, what if I use objects of different types? Like 2 oranges plus 3 bananas. Do I get 5 fruits? Yes, I do. So, even with different objects, the total is still 5 .</td>
</tr>
</tbody>
</table>
<p>Figure 8: Deepseek-R1-Preview response for the query "What is the answer of 2 plus 3?"</p>
<h2>A. 2 Prompts for Clustering Solutions</h2>
<p>Inspired by (Ye et al., 2024), we leverage GPT-4o to cluster the solutions for each instance into groups with the following prompt:</p>
<p>Criteria for clustering the mathematical solutions:</p>
<ol>
<li>If the solutions used to arrive at the solutions are fundamentally different from each other, such as algebraic manipulation versus geometric reasoning, they can be considered novel;</li>
<li>Even if the results are the same, if the intermediate steps or processes involved in reaching those solutions vary significantly, the solutions can be considered different;</li>
<li>If the solutions relies on different assumptions or conditions, they should be considered different from each other;</li>
<li>A solution might generalize to a broader class of problems, while another solution might be specific to certain conditions. In such cases, they are considered distinct;</li>
<li>If one solution is significantly simpler or more complex than the others, it can be regarded as essentially novel, even if they lead to the same result.</li>
</ol>
<p>Given the following mathematical problem:
<strong><em>problem</em></strong>
Solutions:
Solution 1: ...
Solution 2: ...
Please output the clusters strictly following the following format, each row containing a cluster, names, and reasons. Do not include any additional text or explanations outside of this format: cluster1 [solution names] reason for cluster cluster2 [solution names] reason for cluster cluster3 [solution names] reason for cluster</p>
<p>For example:
cluster1 [Solution 1, Solution 3, Solution 5] similar algebraic approach using the volume formula and canceling terms or directly solving for the height.
cluster2 [Solution 2, Solution 4] verifying the correctness and consistency of the formula and solution and considering unit checks or logical reasoning on how volume relates to base area and height.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ If a response does not contain the $m$-th solution (i.e. $M_{i}&lt;m$ ), that response is excluded from the set, hence $K$ does not necessarily equal the number of test set instances $N$.
${ }^{2}$ Refer to Appendix A. 2 for clustering prompt details.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2016 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2016</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2016</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-48.html">extraction-schema-48</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <p><strong>Paper ID:</strong> paper-278769588</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.13089v1.pdf" target="_blank">Systematic Generalization in Language Models Scales with Information Entropy</a></p>
                <p><strong>Paper Abstract:</strong> Systematic generalization remains challenging for current language models, which are known to be both sensitive to semantically similar permutations of the input and to struggle with known concepts presented in novel contexts. Although benchmarks exist for assessing compositional behavior, it is unclear how to measure the difficulty of a systematic generalization problem. In this work, we show how one aspect of systematic generalization can be described by the entropy of the distribution of component parts in the training data. We formalize a framework for measuring entropy in a sequence-to-sequence task and find that the performance of popular model architectures scales with the entropy. Our work connects systematic generalization to information efficiency, and our results indicate that success at high entropy can be achieved even without built-in priors, and that success at low entropy can serve as a target for assessing progress towards robust systematic generalization.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2016.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2016.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer encoder-decoder (Vaswani et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard encoder-decoder Transformer used as a seq2seq baseline; evaluated with absolute, RoPE and disentangled positional encodings and GLU activations; shows the strongest information-efficient systematic generalization under high-entropy training distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (encoder-decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard encoder-decoder Transformer trained from scratch on the synthetic modified-SCAN seq2seq task; uses attention, absolute positional encodings by default, Gated Linear Units activation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1,796,894 params (experiment main); 4,763,166 params (positional encoding experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>self-attention (encoder-decoder), absolute/relative/rotary positional encodings, GLU activation</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic (sequence-to-sequence mapping to action sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>modified SCAN (entropy-conditioned splits)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A synthetic seq2seq mapping where inputs are two embedded sentences conjoined (e1, conjunction, e2), verbs V and adverbial phrases A combine to form instructions; the model must learn deterministic mapping φ that expands verbs according to adverbials; compositionality is tested by withholding a particular verb v1 from one embedded position in training and requiring the model to handle v1 in that position at test (an out-of-distribution split controlled by the entropy H of the verb distribution).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel uses of primitives (verbs) in unseen syntactic positions / novel action sequences composed from seen primitives and adverbials</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>OOD inverted-support split: verb v1 never appears in e1 during training but always appears in e1 at test; training e2 distributions vary in entropy H (vertical mixing or incremental support)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>standard supervised training from scratch on synthetic datasets (varying entropy levels), same number of optimization steps across entropy levels</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Performance scales positively with training entropy H; in Experiment 1 (vertical mixing) Transformer reaches near-ceiling accuracy (close to 100%) by H = 2; in Experiment 2 (incremental support) at the same H = 2 it attains substantially lower accuracy (~80% reported), and performance degrades at low H (e.g., H = 1 is 'subpar'). Results averaged over 5 seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to RNN, CNN, and a permutation-equivariant model: Transformer outperforms RNN and CNN across all H in Experiment 1; in Experiment 2 architectures are statistically similar (within one SD) with CNN best on average; permutation-equivariant model solves all H levels.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Transformer (attention-based encoder-decoder) vs RNN (seq2seq with attention, bidirectional encoder) vs CNN (convolutional seq2seq with attention) vs permutation-equivariant (Gordon et al. cyclic-group prior). Transformer is more information-efficient under full support (higher performance at same H) but sensitive to reduced support regime (Experiment 2).</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Authors report that performance did not scale with model size in their hyperparameter selection (increasing model size did not improve low-H performance for same architecture).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Transformer generalization accuracy increases monotonically with the entropy H of the training verb distribution; reaches near-ceiling at high H (e.g., H≥2 in vertical mixing) but falls substantially at low H; more information-efficient than RNN/CNN under full support conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Fails primarily on low-entropy training distributions (H=0, H=1); for the same H, performance depends on how entropy is increased (vertical mixing vs incremental support) indicating sensitivity to support vs mixture structure.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Succeeds (near-ceiling) when the training distribution over component verbs has high entropy (diverse/near-uniform distribution) and when positional information is encoded effectively (absolute positional encoding performed best).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2016.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2016.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bidirectional RNN encoder-decoder with attention (seq2seq RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard seq2seq recurrent neural network with attention (Elman/LSTM/GRU variants explored); displays lower information-efficiency on entropy-conditioned systematic generalization than the Transformer but benefits differently from increases in support.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RNN seq2seq (bidirectional encoder-decoder with attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Seq2seq RNN implemented with bidirectional encoder, attention between encoder and decoder; grid over cell types (Elman/LSTM/GRU), trained from scratch on synthetic modified-SCAN datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>483,438 params (reported configuration for main experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>recurrent layers (RNN/LSTM/GRU), attention, bidirectional encoder</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic (sequence-to-sequence mapping to action sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>modified SCAN (entropy-conditioned splits)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same modified-SCAN seq2seq mapping as above: two embedded sentences conjoined, mapping φ repeats verbs as directed by adverbial phrases; compositional generalization tested by withholding a verb in one embedded position during training and exposing it in that position at test; training distributions manipulated to vary entropy H.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel action sequences / novel syntactic position for a known verb</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>OOD inverted-support split (verb withheld from e1 in training, present in e1 at test); training e2 distributions varied (vertical mixing or incremental support)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>standard supervised training from scratch; trained on 6,000 unique samples in main vertical-scaling experiments; grid-searched hyperparameters</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Accuracy increases with training entropy H but consistently below Transformer in Experiment 1; in Experiment 2 RNN/CNN sometimes show better gains from increases in support (horizontal scaling) than the Transformer. Exact numeric accuracies not enumerated in text beyond qualitative comparisons; averages reported over 5 seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared to Transformer, CNN, and permutation-equivariant model; RNN underperforms Transformer in vertical scaling, shows relatively better improvement when support increases (horizontal scaling).</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>RNN (recurrent + attention) performs worse than Transformer under full-support/high-entropy conditions but responds differently to support increases; CNN/RNN show opposite sensitivity to type of entropy increase compared to Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Authors report model size scaling did not yield improved low-H performance for the same architecture during hyperparameter selection.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RNN performance also scales with entropy H, but relative gains differ by how entropy is increased (vertical vs horizontal); RNNs are less information-efficient than Transformer under full support but gain more from increases in support in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Poor performance at low-entropy conditions (H=0, H=1); fails to generalize when training distribution lacks entropy in component parts.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Performs better when training verb distribution has higher entropy or when support size increases (horizontal scaling); still inferior to permutation-equivariant prior at low H.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2016.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2016.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convolutional seq2seq (Gehring et al. implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convolutional encoder-decoder seq2seq with attention; competitive with RNNs and Transformers depending on entropy-increase method and achieved best average in horizontal/incremental support experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CNN seq2seq (Gehring et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Convolutional encoder-decoder with attention between encoder and decoder, implemented based on Gehring et al. (2017) and FAIRSEQ reference; trained from scratch on the synthetic modified-SCAN datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>298,078 params (reported configuration)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>convolutional encoder-decoder, attention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic (sequence-to-sequence mapping to action sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>modified SCAN (entropy-conditioned splits)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same modified-SCAN seq2seq mapping: two embedded sentences conjoined; mapping expands verbs per adverbials; compositional generalization tested by withholding verb v1 from e1 in training and exposing in e1 at test; training distributions manipulated to vary entropy H (vertical mixing or incremental/support growth).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel action sequences / novel syntactic position for a known verb</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>OOD inverted-support split; entropy H of e2 varied by mixing or support expansion</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>standard supervised training from scratch; trained on datasets constructed to be exhaustive under given supports for horizontal scaling; hyperparameter grid searched</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Performance improves with higher H. In Experiment 1 (vertical mixing) CNN underperforms Transformer; in Experiment 2 (horizontal/incremental support) CNN performs best on average among vanilla architectures (but differences within one standard deviation). Exact numeric accuracies not enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against Transformer, RNN, and permutation-equivariant model. Outperformed Transformer/RNN in some horizontal-scaling settings on average; still far from permutation-equivariant model for low entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>CNN shows complementary sensitivity to entropy-increase method vs Transformer: benefits more from increases in support (horizontal scaling) than mixture increases (vertical scaling).</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>No reported benefit from simple model-size scaling for improving low-H generalization for the same architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CNNs generalize better as training entropy H increases; in incremental-support (horizontal scaling) experiments CNN achieved the best average accuracy among vanilla architectures, indicating architecture-specific sensitivities to how entropy is introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Like other vanilla architectures, fails at low entropy (H small); cannot match permutation-equivariant model without prior.</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Succeeds when training verb distribution has higher entropy and/or when the support of verbs in e2 increases (horizontal scaling).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2016.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2016.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Permutation-equivariant model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Permutation-equivariant seq2seq model (Gordon et al. cyclic-group prior)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder architecture that enforces verb equivariance by pooling verb representations via a cyclic group prior, enabling near-perfect compositional generalization even at low entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Permutation equivariant models for compositional generalization in language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Permutation-equivariant seq2seq (Gordon et al. 2019 implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Seq2seq encoder-decoder that enforces verb equivariance using a cyclic group applied to verb encoding to pool representations across verbs; implemented with an RNN backbone (GRU) and trained from scratch on the same synthetic datasets as baselines. Dataset-dependent prior requiring manual group definition.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈130,000 params (reported configuration)</td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>explicit structural prior: cyclic-group enforced verb equivariance (permutation equivariance), pooling across verb representations</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic (sequence-to-sequence mapping to action sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>modified SCAN (entropy-conditioned splits)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same modified-SCAN seq2seq mapping with OOD inverted-support splits; model uses built-in equivariance prior to generalize verbs across positions irrespective of their training-position frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel uses of primitives (verbs) in unseen syntactic positions</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>OOD inverted-support split (verb withheld from e1 during training, present in e1 at test); entropy H varied</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>standard supervised training from scratch; uses explicit architectural equivariance prior rather than data manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Solves both vertical and horizontal entropy experiments at all entropy levels (including H=0), achieving near-perfect accuracy across splits reported as 'solves both tasks at all entropy levels'.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Used as a strong structural-prior baseline against vanilla Transformer/RNN/CNN; it succeeds at low entropy where vanilla architectures fail.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>Unlike vanilla architectures, the permutation-equivariant model encodes verb equivariance and therefore generalizes to withheld-position verbs even when training entropy is zero; demonstrates that inductive biases can compensate for low training entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A strong compositional inductive bias (verb equivariance) allows perfect or near-perfect compositional generalization even when the training distribution has zero entropy over the relevant component (H=0), highlighting that data entropy and architectural priors are alternative paths to systematic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>Requires a dataset-dependent explicit prior (cyclic group over verbs) defined manually; succeeds even at H=0 where vanilla models fail.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2016.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2016.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of compositional generalization, systematic generalization, or out-of-distribution generalization experiments, including model architectures, task characteristics, performance metrics, and comparisons across different conditions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>modified-SCAN entropy experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Systematic generalization experiments using a modified SCAN grammar parameterized by component-entropy H</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Controlled seq2seq compositional generalization benchmark derived from SCAN where the training distribution over verbs is manipulated (degenerate, mixtures, or incremental support) to produce different entropy levels H; used to measure how model systematic generalization scales with information entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>n/a (task/dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Synthetic dataset based on a modified SCAN CFG with |V|=8 verbs, two conjunctive phrases, and adverbial modifiers; each input x = (e1, c, e2) maps deterministically to output y via φ; two experimental manipulations: vertical scaling (mixing degenerate and uniform distributions over e2 parametrized by λ) and horizontal scaling (increasing uniform support size S_i over e2).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_pretrained</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>linguistic/semantic (synthetic seq2seq)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>modified SCAN (entropy-conditioned splits)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Task: learn mapping φ from input instructions (two conjoined embedded sentences built from verbs V and adverbials A) to output action sequences. Compositional generalization is tested by creating an OOD split where a particular verb v1 is withheld from one embedded position in training (e1) but appears there in test (e1). The difficulty is quantified by the entropy H of the verb distribution in e2 during training, varied by (1) distribution mixing (vertical scaling) and (2) incremental support (horizontal scaling).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_type</strong></td>
                            <td>novel combinations and novel syntactic positions for known primitives (verbs + adverbials)</td>
                        </tr>
                        <tr>
                            <td><strong>split_type</strong></td>
                            <td>out-of-distribution inverted-support split (verb-position swap) varying by entropy H; test distribution is degenerate in e1 (v1 always) and training distribution excludes v1 from e1 but includes v1 in e2 with distribution controlled by H</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Exhaustive supervised training over all grammatical combinations under the given support sizes for horizontal scaling; fixed number of optimization steps across H levels; experiments run over 5 seeds; controlled sample-size experiments (3k/4k/6k unique samples) to test sample-size independence.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inoculation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>iid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compositional_performance</strong></td>
                            <td>Across architectures, accuracy increases with H; vanilla models (Transformer/RNN/CNN) require high H to achieve near-ceiling performance, while permutation-equivariant model succeeds at all H. Specific reported behaviors: Transformer ~100% at H=2 in vertical scaling; Transformer ~80% at H=2 in horizontal scaling; all models poor at H=1.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_depth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_composition_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_baseline_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Datasets include controlled comparisons across entropy-increase methods (vertical vs horizontal), different sample sizes (3k/4k/6k), and positional encoding schemes; compared models are Transformer, RNN, CNN, and permutation-equivariant.</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_comparison</strong></td>
                            <td>The dataset reveals architecture-specific sensitivities: Transformer is more information-efficient under full support (vertical scaling) but more harmed by reduced support (horizontal scaling) than RNN/CNN; positional encoding choice (absolute vs RoPE vs disentangled) affects performance, with absolute encoding best.</td>
                        </tr>
                        <tr>
                            <td><strong>scale_effects</strong></td>
                            <td>Performance scaling with model size and with the number of unique samples was tested: performance did not improve with larger models (within explored sizes) nor did it depend on number of unique samples (3k-6k) beyond the entropy effect.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Systematic generalization (OOD compositional generalization) of common seq2seq models scales monotonically with the information entropy H of the training distribution over component verbs; high entropy can compensate for lack of explicit architectural compositional priors, enabling near-perfect generalization for vanilla models, whereas low entropy prevents generalization unless an explicit equivariance prior is present.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_analysis</strong></td>
                            <td>Worst performance at low entropy (H=0 degenerate and H=1 low-entropy settings); when |C|=1 and e2 degenerate (H=0) composition is impossible without priors; positional encoding matters (absolute encoding best).</td>
                        </tr>
                        <tr>
                            <td><strong>success_conditions</strong></td>
                            <td>High entropy in the training distribution for the primitives that the composition operates on (diverse/uniform verb distribution) and explicit positional information (absolute positional encodings) enable vanilla architectures to generalize compositionally; explicit architectural priors (permutation equivariance) enable success even at H=0.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks <em>(Rating: 2)</em></li>
                <li>Permutation equivariant models for compositional generalization in language <em>(Rating: 2)</em></li>
                <li>Compositional generalization in seq2seq convolutional networks <em>(Rating: 2)</em></li>
                <li>Measuring compositional generalization: A comprehensive method on realistic data <em>(Rating: 2)</em></li>
                <li>Data factors for better compositional generalization <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2016",
    "paper_id": "paper-278769588",
    "extraction_schema_id": "extraction-schema-48",
    "extracted_data": [
        {
            "name_short": "Transformer",
            "name_full": "Transformer encoder-decoder (Vaswani et al. style)",
            "brief_description": "Standard encoder-decoder Transformer used as a seq2seq baseline; evaluated with absolute, RoPE and disentangled positional encodings and GLU activations; shows the strongest information-efficient systematic generalization under high-entropy training distributions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Transformer (encoder-decoder)",
            "model_description": "Standard encoder-decoder Transformer trained from scratch on the synthetic modified-SCAN seq2seq task; uses attention, absolute positional encodings by default, Gated Linear Units activation.",
            "model_size": "1,796,894 params (experiment main); 4,763,166 params (positional encoding experiment)",
            "is_pretrained": false,
            "architectural_features": "self-attention (encoder-decoder), absolute/relative/rotary positional encodings, GLU activation",
            "task_domain": "linguistic/semantic (sequence-to-sequence mapping to action sequences)",
            "task_name": "modified SCAN (entropy-conditioned splits)",
            "task_description": "A synthetic seq2seq mapping where inputs are two embedded sentences conjoined (e1, conjunction, e2), verbs V and adverbial phrases A combine to form instructions; the model must learn deterministic mapping φ that expands verbs according to adverbials; compositionality is tested by withholding a particular verb v1 from one embedded position in training and requiring the model to handle v1 in that position at test (an out-of-distribution split controlled by the entropy H of the verb distribution).",
            "compositional_depth": null,
            "composition_type": "novel uses of primitives (verbs) in unseen syntactic positions / novel action sequences composed from seen primitives and adverbials",
            "split_type": "OOD inverted-support split: verb v1 never appears in e1 during training but always appears in e1 at test; training e2 distributions vary in entropy H (vertical mixing or incremental support)",
            "training_strategy": "standard supervised training from scratch on synthetic datasets (varying entropy levels), same number of optimization steps across entropy levels",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": "Performance scales positively with training entropy H; in Experiment 1 (vertical mixing) Transformer reaches near-ceiling accuracy (close to 100%) by H = 2; in Experiment 2 (incremental support) at the same H = 2 it attains substantially lower accuracy (~80% reported), and performance degrades at low H (e.g., H = 1 is 'subpar'). Results averaged over 5 seeds.",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared to RNN, CNN, and a permutation-equivariant model: Transformer outperforms RNN and CNN across all H in Experiment 1; in Experiment 2 architectures are statistically similar (within one SD) with CNN best on average; permutation-equivariant model solves all H levels.",
            "architectural_comparison": "Transformer (attention-based encoder-decoder) vs RNN (seq2seq with attention, bidirectional encoder) vs CNN (convolutional seq2seq with attention) vs permutation-equivariant (Gordon et al. cyclic-group prior). Transformer is more information-efficient under full support (higher performance at same H) but sensitive to reduced support regime (Experiment 2).",
            "scale_effects": "Authors report that performance did not scale with model size in their hyperparameter selection (increasing model size did not improve low-H performance for same architecture).",
            "transfer_results": null,
            "key_findings": "Transformer generalization accuracy increases monotonically with the entropy H of the training verb distribution; reaches near-ceiling at high H (e.g., H≥2 in vertical mixing) but falls substantially at low H; more information-efficient than RNN/CNN under full support conditions.",
            "failure_analysis": "Fails primarily on low-entropy training distributions (H=0, H=1); for the same H, performance depends on how entropy is increased (vertical mixing vs incremental support) indicating sensitivity to support vs mixture structure.",
            "success_conditions": "Succeeds (near-ceiling) when the training distribution over component verbs has high entropy (diverse/near-uniform distribution) and when positional information is encoded effectively (absolute positional encoding performed best).",
            "uuid": "e2016.0"
        },
        {
            "name_short": "RNN",
            "name_full": "Bidirectional RNN encoder-decoder with attention (seq2seq RNN)",
            "brief_description": "Standard seq2seq recurrent neural network with attention (Elman/LSTM/GRU variants explored); displays lower information-efficiency on entropy-conditioned systematic generalization than the Transformer but benefits differently from increases in support.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RNN seq2seq (bidirectional encoder-decoder with attention)",
            "model_description": "Seq2seq RNN implemented with bidirectional encoder, attention between encoder and decoder; grid over cell types (Elman/LSTM/GRU), trained from scratch on synthetic modified-SCAN datasets.",
            "model_size": "483,438 params (reported configuration for main experiment)",
            "is_pretrained": false,
            "architectural_features": "recurrent layers (RNN/LSTM/GRU), attention, bidirectional encoder",
            "task_domain": "linguistic/semantic (sequence-to-sequence mapping to action sequences)",
            "task_name": "modified SCAN (entropy-conditioned splits)",
            "task_description": "Same modified-SCAN seq2seq mapping as above: two embedded sentences conjoined, mapping φ repeats verbs as directed by adverbial phrases; compositional generalization tested by withholding a verb in one embedded position during training and exposing it in that position at test; training distributions manipulated to vary entropy H.",
            "compositional_depth": null,
            "composition_type": "novel action sequences / novel syntactic position for a known verb",
            "split_type": "OOD inverted-support split (verb withheld from e1 in training, present in e1 at test); training e2 distributions varied (vertical mixing or incremental support)",
            "training_strategy": "standard supervised training from scratch; trained on 6,000 unique samples in main vertical-scaling experiments; grid-searched hyperparameters",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": "Accuracy increases with training entropy H but consistently below Transformer in Experiment 1; in Experiment 2 RNN/CNN sometimes show better gains from increases in support (horizontal scaling) than the Transformer. Exact numeric accuracies not enumerated in text beyond qualitative comparisons; averages reported over 5 seeds.",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared to Transformer, CNN, and permutation-equivariant model; RNN underperforms Transformer in vertical scaling, shows relatively better improvement when support increases (horizontal scaling).",
            "architectural_comparison": "RNN (recurrent + attention) performs worse than Transformer under full-support/high-entropy conditions but responds differently to support increases; CNN/RNN show opposite sensitivity to type of entropy increase compared to Transformer.",
            "scale_effects": "Authors report model size scaling did not yield improved low-H performance for the same architecture during hyperparameter selection.",
            "transfer_results": null,
            "key_findings": "RNN performance also scales with entropy H, but relative gains differ by how entropy is increased (vertical vs horizontal); RNNs are less information-efficient than Transformer under full support but gain more from increases in support in some settings.",
            "failure_analysis": "Poor performance at low-entropy conditions (H=0, H=1); fails to generalize when training distribution lacks entropy in component parts.",
            "success_conditions": "Performs better when training verb distribution has higher entropy or when support size increases (horizontal scaling); still inferior to permutation-equivariant prior at low H.",
            "uuid": "e2016.1"
        },
        {
            "name_short": "CNN",
            "name_full": "Convolutional seq2seq (Gehring et al. implementation)",
            "brief_description": "Convolutional encoder-decoder seq2seq with attention; competitive with RNNs and Transformers depending on entropy-increase method and achieved best average in horizontal/incremental support experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CNN seq2seq (Gehring et al. style)",
            "model_description": "Convolutional encoder-decoder with attention between encoder and decoder, implemented based on Gehring et al. (2017) and FAIRSEQ reference; trained from scratch on the synthetic modified-SCAN datasets.",
            "model_size": "298,078 params (reported configuration)",
            "is_pretrained": false,
            "architectural_features": "convolutional encoder-decoder, attention",
            "task_domain": "linguistic/semantic (sequence-to-sequence mapping to action sequences)",
            "task_name": "modified SCAN (entropy-conditioned splits)",
            "task_description": "Same modified-SCAN seq2seq mapping: two embedded sentences conjoined; mapping expands verbs per adverbials; compositional generalization tested by withholding verb v1 from e1 in training and exposing in e1 at test; training distributions manipulated to vary entropy H (vertical mixing or incremental/support growth).",
            "compositional_depth": null,
            "composition_type": "novel action sequences / novel syntactic position for a known verb",
            "split_type": "OOD inverted-support split; entropy H of e2 varied by mixing or support expansion",
            "training_strategy": "standard supervised training from scratch; trained on datasets constructed to be exhaustive under given supports for horizontal scaling; hyperparameter grid searched",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": "Performance improves with higher H. In Experiment 1 (vertical mixing) CNN underperforms Transformer; in Experiment 2 (horizontal/incremental support) CNN performs best on average among vanilla architectures (but differences within one standard deviation). Exact numeric accuracies not enumerated in text.",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Compared against Transformer, RNN, and permutation-equivariant model. Outperformed Transformer/RNN in some horizontal-scaling settings on average; still far from permutation-equivariant model for low entropy.",
            "architectural_comparison": "CNN shows complementary sensitivity to entropy-increase method vs Transformer: benefits more from increases in support (horizontal scaling) than mixture increases (vertical scaling).",
            "scale_effects": "No reported benefit from simple model-size scaling for improving low-H generalization for the same architecture.",
            "transfer_results": null,
            "key_findings": "CNNs generalize better as training entropy H increases; in incremental-support (horizontal scaling) experiments CNN achieved the best average accuracy among vanilla architectures, indicating architecture-specific sensitivities to how entropy is introduced.",
            "failure_analysis": "Like other vanilla architectures, fails at low entropy (H small); cannot match permutation-equivariant model without prior.",
            "success_conditions": "Succeeds when training verb distribution has higher entropy and/or when the support of verbs in e2 increases (horizontal scaling).",
            "uuid": "e2016.2"
        },
        {
            "name_short": "Permutation-equivariant model",
            "name_full": "Permutation-equivariant seq2seq model (Gordon et al. cyclic-group prior)",
            "brief_description": "An encoder-decoder architecture that enforces verb equivariance by pooling verb representations via a cyclic group prior, enabling near-perfect compositional generalization even at low entropy.",
            "citation_title": "Permutation equivariant models for compositional generalization in language",
            "mention_or_use": "use",
            "model_name": "Permutation-equivariant seq2seq (Gordon et al. 2019 implementation)",
            "model_description": "Seq2seq encoder-decoder that enforces verb equivariance using a cyclic group applied to verb encoding to pool representations across verbs; implemented with an RNN backbone (GRU) and trained from scratch on the same synthetic datasets as baselines. Dataset-dependent prior requiring manual group definition.",
            "model_size": "≈130,000 params (reported configuration)",
            "is_pretrained": false,
            "architectural_features": "explicit structural prior: cyclic-group enforced verb equivariance (permutation equivariance), pooling across verb representations",
            "task_domain": "linguistic/semantic (sequence-to-sequence mapping to action sequences)",
            "task_name": "modified SCAN (entropy-conditioned splits)",
            "task_description": "Same modified-SCAN seq2seq mapping with OOD inverted-support splits; model uses built-in equivariance prior to generalize verbs across positions irrespective of their training-position frequency.",
            "compositional_depth": null,
            "composition_type": "novel uses of primitives (verbs) in unseen syntactic positions",
            "split_type": "OOD inverted-support split (verb withheld from e1 during training, present in e1 at test); entropy H varied",
            "training_strategy": "standard supervised training from scratch; uses explicit architectural equivariance prior rather than data manipulation",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": "Solves both vertical and horizontal entropy experiments at all entropy levels (including H=0), achieving near-perfect accuracy across splits reported as 'solves both tasks at all entropy levels'.",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Used as a strong structural-prior baseline against vanilla Transformer/RNN/CNN; it succeeds at low entropy where vanilla architectures fail.",
            "architectural_comparison": "Unlike vanilla architectures, the permutation-equivariant model encodes verb equivariance and therefore generalizes to withheld-position verbs even when training entropy is zero; demonstrates that inductive biases can compensate for low training entropy.",
            "scale_effects": null,
            "transfer_results": null,
            "key_findings": "A strong compositional inductive bias (verb equivariance) allows perfect or near-perfect compositional generalization even when the training distribution has zero entropy over the relevant component (H=0), highlighting that data entropy and architectural priors are alternative paths to systematic generalization.",
            "failure_analysis": null,
            "success_conditions": "Requires a dataset-dependent explicit prior (cyclic group over verbs) defined manually; succeeds even at H=0 where vanilla models fail.",
            "uuid": "e2016.3"
        },
        {
            "name_short": "modified-SCAN entropy experiments",
            "name_full": "Systematic generalization experiments using a modified SCAN grammar parameterized by component-entropy H",
            "brief_description": "Controlled seq2seq compositional generalization benchmark derived from SCAN where the training distribution over verbs is manipulated (degenerate, mixtures, or incremental support) to produce different entropy levels H; used to measure how model systematic generalization scales with information entropy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "n/a (task/dataset)",
            "model_description": "Synthetic dataset based on a modified SCAN CFG with |V|=8 verbs, two conjunctive phrases, and adverbial modifiers; each input x = (e1, c, e2) maps deterministically to output y via φ; two experimental manipulations: vertical scaling (mixing degenerate and uniform distributions over e2 parametrized by λ) and horizontal scaling (increasing uniform support size S_i over e2).",
            "model_size": null,
            "is_pretrained": null,
            "architectural_features": null,
            "task_domain": "linguistic/semantic (synthetic seq2seq)",
            "task_name": "modified SCAN (entropy-conditioned splits)",
            "task_description": "Task: learn mapping φ from input instructions (two conjoined embedded sentences built from verbs V and adverbials A) to output action sequences. Compositional generalization is tested by creating an OOD split where a particular verb v1 is withheld from one embedded position in training (e1) but appears there in test (e1). The difficulty is quantified by the entropy H of the verb distribution in e2 during training, varied by (1) distribution mixing (vertical scaling) and (2) incremental support (horizontal scaling).",
            "compositional_depth": null,
            "composition_type": "novel combinations and novel syntactic positions for known primitives (verbs + adverbials)",
            "split_type": "out-of-distribution inverted-support split (verb-position swap) varying by entropy H; test distribution is degenerate in e1 (v1 always) and training distribution excludes v1 from e1 but includes v1 in e2 with distribution controlled by H",
            "training_strategy": "Exhaustive supervised training over all grammatical combinations under the given support sizes for horizontal scaling; fixed number of optimization steps across H levels; experiments run over 5 seeds; controlled sample-size experiments (3k/4k/6k unique samples) to test sample-size independence.",
            "curriculum_details": null,
            "inoculation_details": null,
            "iid_performance": null,
            "compositional_performance": "Across architectures, accuracy increases with H; vanilla models (Transformer/RNN/CNN) require high H to achieve near-ceiling performance, while permutation-equivariant model succeeds at all H. Specific reported behaviors: Transformer ~100% at H=2 in vertical scaling; Transformer ~80% at H=2 in horizontal scaling; all models poor at H=1.",
            "generalization_gap": null,
            "performance_by_depth": null,
            "performance_by_composition_type": null,
            "has_baseline_comparison": true,
            "baseline_comparisons": "Datasets include controlled comparisons across entropy-increase methods (vertical vs horizontal), different sample sizes (3k/4k/6k), and positional encoding schemes; compared models are Transformer, RNN, CNN, and permutation-equivariant.",
            "architectural_comparison": "The dataset reveals architecture-specific sensitivities: Transformer is more information-efficient under full support (vertical scaling) but more harmed by reduced support (horizontal scaling) than RNN/CNN; positional encoding choice (absolute vs RoPE vs disentangled) affects performance, with absolute encoding best.",
            "scale_effects": "Performance scaling with model size and with the number of unique samples was tested: performance did not improve with larger models (within explored sizes) nor did it depend on number of unique samples (3k-6k) beyond the entropy effect.",
            "transfer_results": null,
            "key_findings": "Systematic generalization (OOD compositional generalization) of common seq2seq models scales monotonically with the information entropy H of the training distribution over component verbs; high entropy can compensate for lack of explicit architectural compositional priors, enabling near-perfect generalization for vanilla models, whereas low entropy prevents generalization unless an explicit equivariance prior is present.",
            "failure_analysis": "Worst performance at low entropy (H=0 degenerate and H=1 low-entropy settings); when |C|=1 and e2 degenerate (H=0) composition is impossible without priors; positional encoding matters (absolute encoding best).",
            "success_conditions": "High entropy in the training distribution for the primitives that the composition operates on (diverse/uniform verb distribution) and explicit positional information (absolute positional encodings) enable vanilla architectures to generalize compositionally; explicit architectural priors (permutation equivariance) enable success even at H=0.",
            "uuid": "e2016.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
            "rating": 2
        },
        {
            "paper_title": "Permutation equivariant models for compositional generalization in language",
            "rating": 2
        },
        {
            "paper_title": "Compositional generalization in seq2seq convolutional networks",
            "rating": 2
        },
        {
            "paper_title": "Measuring compositional generalization: A comprehensive method on realistic data",
            "rating": 2
        },
        {
            "paper_title": "Data factors for better compositional generalization",
            "rating": 2
        }
    ],
    "cost": 0.0154305,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Systematic Generalization in Language Models Scales with Information Entropy
27 May 2025</p>
<p>Sondre Wold 
Language Technology Group
University of Oslo</p>
<p>Lucas Georges 
Language Technology Group
University of Oslo</p>
<p>Gabriel Charpentier 
Language Technology Group
University of Oslo</p>
<p>Étienne Simon 
Language Technology Group
University of Oslo</p>
<p>Systematic Generalization in Language Models Scales with Information Entropy
27 May 2025E7C101A1C1C6D3399B5DA8046EC83C5AarXiv:2505.13089v2[cs.CL]
Systematic generalization remains challenging for current language models, which are known to be both sensitive to semantically similar permutations of the input and to struggle with known concepts presented in novel contexts.Although benchmarks exist for assessing compositional behavior, it is unclear how to measure the difficulty of a systematic generalization problem.In this work, we show how one aspect of systematic generalization can be described by the entropy of the distribution of component parts in the training data.We formalize a framework for measuring entropy in a sequence-to-sequence task and find that the performance of popular model architectures scales with the entropy.Our work connects systematic generalization to information efficiency, and our results indicate that success at high entropy can be achieved even without built-in priors, and that success at low entropy can serve as a target for assessing progress towards robust systematic generalization.</p>
<p>Introduction</p>
<p>Human language is characterized by its combinatorial properties in syntax and semantics (Hadley, 1994) (Fodor and Pylyshyn, 1988).This type of bidirectional systematic generalization has long been considered a key feature of human language understanding (Chomsky, 1957), and is closely related to the compositionality of natural language semantics, a contentious but widely studied topic in both formal semantics (van Bethem et al., 1991) and NLP (McCurdy et al., 2024).</p>
<p>Early critics of connectionism, such as Fodor and Pylyshyn (1988), argued that systematic generalization was incompatible with continuous represen- Figure 1: Systematic generalization increases with the entropy of the training data for popular sequence-tosequence models, even without built-in priors.</p>
<p>tations.More than thirty years since this critique was initially presented, researchers still agree that achieving robust compositional behavior in neural networks remains an open problem, as shown by a recent survey by McCurdy et al. (2024).Research continues to demonstrate that current models struggle with systematic generalization (Lake and Baroni, 2018;Hupkes et al., 2020;Dziri et al., 2024;Wold et al., 2024;Mirzadeh et al., 2024).Some of the limitations of current models can be alleviated by implementing compositional priors, either in the architecture (Gordon et al., 2019) or through the training procedure (Lake and Baroni, 2023), but it remains unclear whether architectural constraints cause the limitations or whether they arise due to distributional properties of the data used for training.</p>
<p>In this paper, we ask the following question: Can neural networks display systematic out-ofdistribution generalization without any built-in priors?To answer this, we take a data-centric approach.Specifically, we show how the degree of systematic generalization of common architectures relates to the information entropy of the distribution of component parts found in the training data.</p>
<p>Contrary to previous work, we find that popu-
H = 0 H = 1 H = 1.58 H = 2</p>
<p>Systematic generalization</p>
<p>Figure 2: A schematic overview of the two approaches to increasing the entropy level.Top: Vertical scaling by distribution mixing.Bottom: Horizontal scaling by incrementing the support of the distribution.</p>
<p>lar sequence-to-sequence models used in NLP are capable of systematic generalization, even without built-in architectural priors.However, this is only the case when the training dataset contains high entropy.On a series of sequence-to-sequence datasets based on a modified version of the SCAN (Lake and Baroni, 2018) grammar, we show that model performance scales with the entropy of the training dataset, as illustrated in Figure 1, thereby creating a link between systematic generalization and information efficiency.In summary, our contributions are: (i) We develop and formalize a framework for studying systematic generalization based on information entropy; (ii) We experiment with two different approaches to increasing entropy, and we compare the performance of four different model architectures for these two settings; and (iii) we show that neural networks achieve systematic out-of-distribution generalization when the entropy of the training distribution is high, independently of the number of unique samples seen during training.</p>
<p>Background</p>
<p>Systematic generalization is typically described as a robust mode of composition, where a system is able to combine parts of its training data in a way that enables generalization to novel combinations.As such, systematic generalization relates to com-positionality, a topic which has both a long tradition and multiple formal definitions, most prominently in formal semantics (van Bethem et al., 1991;Pagin and Westerståhl, 2010).These definitions, however, often prove too restrictive when working with natural languages.As a result, researchers frequently introduce problem-specific relaxations, leading to ambiguity over time regarding the exact meaning of terms such as systematic generalization and compositionality.</p>
<p>Despite there being some ambiguity regarding the formal definitions, recent work shows that there is high agreement among researchers on the following, more informal definition of what constitutes compositional behavior: Definition 1 (CB) "When a model receives an input I that humans conceive as composed of component parts, if the model produces correct outputs for those parts (in isolation or in other combinations), then it will also produce a correct output for I (McCurdy et al., 2024, p. 9324)."</p>
<p>This definition, however, does not consider the conditions from which a system learns to produce these outputs.Consequently, compositional behavior differs from compositional generalization.While the former can be assessed through normal benchmarks, the latter requires information about the distribution the model generalizes from.</p>
<p>Although generalization is a loosely defined term in machine learning research, it generally refers to how well a model performs on a held-out dataset after being trained on data that has some distributional relationship with the held-out data (Hupkes et al., 2023).When training and test distributions are similar, we refer to this as in-distribution, and when they differ, we typically refer to this as outof-distribution.Naturally, the classification of a generalization problem as in-distribution or out-ofdistribution depends on which aspects of the distribution are considered.In this work, we focus on systematic generalization as an out-of-distribution problem, where the differences in the distribution of component parts and their combinations between the training and test data are the relevant aspects.</p>
<p>Systematic Generalization</p>
<p>Following earlier works, we will refer to the systematic generalization abilities of a model as the systematicity of that model.Our view of systematicity is based on Hadley (1994), who in contrast to earlier work by Fodor and Pylyshyn (1988) distinguished between degrees of systematicity.In the following paragraphs, we present and define these degrees under the assumption that we are considering a system tasked to generalize from a training corpus to a held-out test corpus:1</p>
<p>Definition 2 (Weak systematicity) A system displays weak systematicity if the system requires that all component parts that occur in the training data also occur at every permissible syntactic position.Even if the model succeeds at a test set that contains novel samples, none of the components in these samples occur at a novel syntactic position, essentially making the problem in-distribution with respect to the systematic properties of the data.Systematic generalization in embedded sentences.In this work, we focus on a relaxed version of strong systematicity as defined in Definition 3. We relax requirement (ii) from the original definition so that we only work with samples that have embedded sentences.As we show in the following section, this makes it easier to quantify the level of systematicity required for a given generalization problem, which is the main focus of this work.Furthermore, we also relax (ii) so that it allows component parts of the test set to occur at the same syntactic position in some embedded sentence in the train set, but not in an embedded sentence that occurs at the same position in the overall sequence.</p>
<p>In order to compare models with and without compositional priors from the literature, we base our experiments on the SCAN grammar (Lake and Baroni, 2018).This grammar has few permissible syntactic positions for each part of speech, making the original requirement from Hadley (1994) too restrictive with respect to the overall sample size needed by neural networks.In the following section, we formalize our framework and elaborate on these choices.</p>
<p>Measuring Systematicity</p>
<p>In this work, we demonstrate that sequence-tosequence models can generalize to embedded sentences where component parts occur at novel syntactic positions in the overall sequence.For our experiments, we construct datasets where each dataset sample contains an input sequence x ∈ X and an output sequence y ∈ Y , based on a modified version of the SCAN grammar.We discuss the specifics of this dataset generation procedure, including details about the context-free grammar that generates x, in Section 3.5.In this section, we define a framework for measuring the degree of systematicity displayed by a systematic generalization problem.</p>
<p>Task Description</p>
<p>Throughout this paper, x comprises two sentences conjoined by a conjunctive phrase.2Consequently, x can be modeled as a triple, with a conjunction c ∈ C, and the two embedded sentences e 1 , e 2 ∈ Σ * as arguments, x = (e 1 , c, e 2 ).The component parts of both embedded sentences are drawn from the same vocabulary Σ = V ∪ A, where V is a set of verbs and A is a set of adverbial phrases, e.g.: JUMP, RUN ∈ V , TWICE ∈ A, and AND ∈ C.There exists a deterministic procedure φ for transforming x into y: φ(RUN TWICE AND JUMP) = RUN RUN JUMP.</p>
<p>The goal of the described task is to learn a function f that approximates φ, which requires modeling the conditional distribution of output sequences given input sequences in the training data, i.e. p train (y | x).See Table 1 for more examples generated by our modified SCAN grammar.</p>
<p>Component Distributions</p>
<p>Since we are interested in systematic generalization, we also need to define the distribution of component parts in e 1 and e 2 , and we are specifically interested in the case where the distribution of e 1 and e 2 is different in the training and test data, but they share a similar distribution when marginalized over position, which in this case refers to which side of the conjunctive phrase they appear in.</p>
<p>In SCAN (Appendix A), e 1 and e 2 always contain one verb in V and zero or more adverbial phrases from A that control how many times that verb should be repeated, and in what direction it is executed.To create a systematic generalization problem that adheres to the definition given in Section 2.1, it is necessary to set the support of e 1 and e 2 to be different in the training and test distributions.For simplicity, we limit our study to focus on the setting where the support of the verbs V is different between the two distributions.This means that we can create a generalization problem by restricting a verb v 1 to never occur in e 1 but to occur in e 2 for p train , as illustrated by Figure 3.</p>
<p>For p test we invert the distribution of e 1 , making it a degenerate distribution with single-point support for the verb that is not in e 1 for p train , while e 2 has the same distribution as in p train .When generalizing from p train to p test in this scenario, a model will be exposed to all verbs in all permissible syntactic positions in some embedded sentence, but v 1 will never be seen in e 1 during training, while e 1 in p test always contains v 1 .</p>
<p>Systematicity as Entropy</p>
<p>The main contribution of this work is to demonstrate that the difficulty of generalizing from p train to p test in the scenario described above depends on the entropy H of the distribution of verbs in e 2 for p train :
H train e 2 (V) = − v∈V p train e 2 (v) log 2 p train e 2 (v),
where V is the verb random variable and V is the set of all verbs.For brevity, we will refer to H train e 2 (V) as H.When the verbs in e 2 have a degenerate distribution, as shown in Figure 4, the entropy is zero, and when e 2 is distributed uniformly, as shown in Figure 3, the entropy is at its maximum.Decreasing H within this interval increases the difficulty of the systematic generalization problem.Note that even at maximum entropy, the generalization problem is still out-of-distribution, as the restricted verb is never seen in e 1 .In Section 4, we show empirically that the performance of neural network architectures scales with H on this type of generalization problem.</p>
<p>The degenerate case.We note that H = 0 constitutes a special case in the described framework.If e 2 is described by a degenerate distribution, then instances of e 2 will always contain the same v 1 ∈ V .Now, if |C| = 1, there is no way of decoupling the semantics of the conjunctive phrase and v 1 , as these will always appear together in X.In this case, we argue, no system will be able to generalize from p train to p test .Consequently, it is necessary to set |C| &gt; 1 for evaluating performance at H = 0.</p>
<p>Increasing Entropy</p>
<p>In this work, we experiment with two different approaches to increasing H, as illustrated in Figure 2. In the following sections, we formalize these approaches, and in Section 4 we demonstrate how the performance of neural architectures scales for each type of increment.</p>
<p>Distribution Mixing</p>
<p>Let U be a uniform distribution with support over V \ {v 1 }, and let D be a degenerate distribution over V with single-point support for v 1 .Our first approach consists in defining a family of distributions parametrized by λ mixing U and D:
p train e 2 (V) = λU + (1 − λ)D,
Then, the entropy H is at the minimum when λ = 0 and at the maximum when λ = 1 − 1/|V |, which is equal to a uniform distribution over V .Consequently, increasing H can be achieved by tending λ towards 1 − 1/|V |.This approach to increasing H is also illustrated in the top part of Figure 2.</p>
<p>Incremental Support</p>
<p>In our second approach, we define another family of distributions, all uniforms over increasingly larger supports S i ⊆ V :
{v 1 } = S 1 ⊂ S 2 ⊂ • • • ⊂ S |V | = V
The uniform distribution over the smallest support S 1 is equivalent to the degenerate distribution D.</p>
<p>As i increases, H increases logarithmically until reaching maximum entropy at i = |V | where we have the uniform distribution over V .This approach to increasing H is also illustrated in the bottom part of Figure 2.</p>
<p>Data Generation</p>
<p>As posited in the previous sections, the quantification of systematic generalization requires information about the distribution of parts and combinations observed during training.This level of granularity is generally intractable when dealing with real-world corpora.Without this information, however, it is difficult to control for statistical patterns that permit a non-compositional solution.Studies on systematicity typically overcome this problem by studying synthetic languages where the distribution can be known a priori.This is also the approach taken in this work.</p>
<p>To generate datasets samples (x, y) ∈ X × Y we define a context-free grammar (CFG) based on the SCAN grammar (Lake and Baroni, 2018).An overview of the complete vocabulary can be found in Appendix A, while Table 1 shows examples from our generated data.Importantly, we set |V | to 8, as opposed to the original four verbs, thereby increasing the number of possible sequences generated by the grammar.We also remove the turn operators, as they are more syntactically constrained than other verbs, as was already done in Gordon et al. (2019).As previously stated, all instances of x contain embedded sentences e 1 and e 2 .</p>
<p>In our experiments, we use the standard conjunctive phrases in SCAN |C| = 2. Furthermore, we let the two conjunctive phrases take the embedded sentences in the opposite order: if v 1 always occurs in e 2 for c 1 , then v 1 will always occur in e 1 for c 2 .</p>
<p>Experiments</p>
<p>We validate our framework by evaluating the three most common sequence-to-sequence architectures: the Transformer, the RNN, and the CNN.We also run a model with built-in compositional priors to showcase how a model that encodes verb equivariance can solve the task even when H is low.We have two main experimental settings, corresponding to the two methods for increasing H described in Section 3.4.1 and Section 3.4.2.We also conduct a supporting experiment to control for a potential confounding effect resulting from an increased sample size, and an experiment that estimates the influence of different position encoding types.For all experimental settings, H = 0 constitutes the most difficult generalization problem, and H = log 2 |V | = 3 constitutes the easiest problem.We average and report results from five differently seeded runs.The following sections provide details  (Lake and Baroni, 2018;Gordon et al., 2019): a bidirectional encoderdecoder with attention.Details on the implementation and choice of hyperparameters can be found in Appendix B.1.</p>
<p>CNN.</p>
<p>For our CNN we use the encoder-decoder architecture from Gehring et al. (2017), which has been proven effective on SCAN in previous work (Dessì and Baroni, 2019).As in the RNN, this CNN uses an attention mechanism between the encoder and decoder.Hyperparameters can be found in Appendix B.2.</p>
<p>Transformer.We use the original encoderdecoder Transformer from Vaswani (2017).We use this for two reasons.Firstly, this makes comparison with previous work on systematicity easier, as this has been used in previous work such as Hupkes et al. (2020) and Lake and Baroni (2023).Secondly, recent work on machine translation has shown that encoder-decoders outperform the more widely used decoder-only formulation of the Transformer on similar parameter sizes (Pitorro et al., 2024).As SCAN is framed and modeled as a sequence-to-sequence task, the encoder-decoder formulation is the most reasonable choice.We use absolute position encodings for the two main experiments, as well as Gated Linear Units as the activation function (Shazeer, 2020).Details on the implementation and choice of hyperparameters can be found in Appendix B.3.</p>
<p>Permutation-equivariant model.Lastly, we experiment with a model that has built-in architectural priors.We use the encoder-decoder from Gordon et al. (2019), which enforces the verb equivariance found in SCAN using a cyclic group.This group is used in the encoding of the input sequences in a way that pools the representation of any verb with the representation of the other verbs, even when they are not present in the input.This is a strong structural prior that enables the model to solve the original SCAN splits efficiently.However, this approach is dataset-dependent, as the elements of the cyclic group must be defined manually.We use this model primarily as a sanity check for our experiments, but also as a comparison for the other architectures, comparing their efficiency to an idealized case.</p>
<p>Experiment 1: Vertical scaling</p>
<p>For Experiment 1, p test has a degenerate distribution for e 1 , where v 1 ∈ V always occur, while e 2 has a uniform distribution over V .For p train , e 1 has a uniform distribution over V \ {v 1 }, and e 2 is a mixture of a degenerate distribution at v 1 and a uniform distribution over V \ {v 1 }, where we increase the total samples drawn from the uniform while decreasing the number drawn from the degenerate distribution.By doing this, we increase H as described in section Section 3.4.1.We refer to this increase in H as vertical scaling, as the distribution over the verbs increases towards the uniform.The models are trained on 6 000 unique samples and evaluated on 7 056 samples for all values of H.</p>
<p>Experiment 2: Horizontal Scaling</p>
<p>For Experiment 2, p test is the same as for Experiment 1.For p train , e 1 is also the same as for Experiment 1, but e 2 is a uniform distribution over an increasing number of verbs, starting with {v 1 } and ending at V , as described in Section 3.4.2.We refer to this increase in H as horizontal scaling, as the increase of H is achieved by adding another verb to the support of e 2 .The models are trained on all grammatical combinations in Σ * permitted by the support at each level of H.When H = 2, for example, the support of the distribution of e 2 contains four verbs, and the training set contains all possible pairs (x, y) ∈ X × Y under these constraints.</p>
<p>To ensure a fair comparison across entropy levels, we train all models for the same number of optimization steps, so for H = 1, which has fewer training samples than H = 3, we do more iterations over the full data.</p>
<p>Effect of Number of Unique Samples</p>
<p>As H describes the relative distribution of verbs in e 2 , it is possible to construct datasets of varying sizes that have the same H.This can be achieved by increasing the total volume of unique samples while maintaining the same relative distribution.To verify that performance scales with H independently of the number of unique samples seen during training, we create datasets that have the same entropy as in the previous experiments, but where we vary the number of total unique samples.We experiment with three different sample sizes: 3 000, 4 000, and 6 000, using the vertical scaling setting.All models run for the same number of optimization steps for all sample sizes.</p>
<p>Effect of Positional Embedding Type</p>
<p>In our experiments, there is an intuitive connection between the generalization from p train to p test and the ability to encode verb equivariance.This is exemplified by Gordon et al. (2019), which achieves high performance on the original SCAN tasks precisely by enforcing verb equivariance in the model architecture explicitly.</p>
<p>As the verbs always occur on the first token position of e 1 and e 2 , positional information is central to our experiments.To estimate the effect of positional information, we show how the choice of positional encoding scheme affects performance in the vertical scaling setting.In addition to the original absolute position embeddings from Vaswani (2017), we experiment with embeddings that encode relative positional information through disentangled attention (He et al., 2021) and rotary position embeddings (RoPE; Su et al., 2024).In this experiment, we focus on the Transformer.Details on hyperparameter selection can be found in Appendix B.4</p>
<p>Results and Discussion</p>
<p>The results of Experiment 1 and Experiment 2 can be found in Figure 5.We observe a clear positive relationship between performance and H for both experiments, demonstrating that systematic generalization scales with entropy.Unsurprisingly, the permutation-equivariant model, which has a strict compositional prior, solves both tasks at all entropy levels.This shows that the task is solvable even at H = 0 when enforcing such a prior, but that vanilla architectures generally require much higher entropies for this type of generalization.</p>
<p>In Experiment 1, the Transformer outperforms both the RNN and CNN across all levels of H.In Experiment 2, the performances of all models are statistically similar, falling within one standard deviation of each other, but with the CNN performing the best on average in the reported runs.</p>
<p>We also observe that for the Transformer, a reduction in the support of the distribution is more detrimental to the performance than a step towards the degenerate distribution: At H = 2, the Transformer has close to 100% accuracy for Experiment 1, but for the same level of H in Experiment 2, it achieves around 80% accuracy.This is not the case for the RNN and CNN, where we observe the opposite: The increase in support that increases H from 1 to 2 in Experiment 2 gives a higher performance increase than what the same increase in H does for Experiment 1.We argue that these results indicate that the Transformer is more information-efficient than the RNN and CNN under full support, but that the models are similar under lacking support.In practice, this means that the Transformer is quicker to generalize to novel uses of component parts, as long as the overall syntactic pattern-constituted by the conjunctive phrase in our experiments-is frequent in the training data.This interpretation aligns with the findings of Loula et al. (2018), showcasing how our framework can provide a theoretical explanation for previous results.</p>
<p>We note that even though all models can generalize under high entropy for both experiments, their performance is subpar for low entropy, e.g.H = 1.This indicates a rather high lower bound on the entropy required for systematic generalization, both for vertical and horizontal scaling, and it shows that all models are unable to meet requirement iii from Definition 3. Furthermore, our results show that systematic generalization remains an open problem when there is no prior, and it remains unclear whether or not it is possible to build a model that performs well on low entropy without architectural priors as in Gordon et al. (2019) or priors in the training procedure as in Lake and Baroni (2023).</p>
<p>Supplemental results</p>
<p>In Figure 6a, we show the results of varying the number of unique samples at each level of H.We observe that the positive relationship between performance and H is independent of the number of unique samples.The Transformer performs similarly when trained on 3 000 to 6 000 unique samples.During hyperparameter selection (Appendix B), we also observe that performance does not scale with model size.This indicates that achieving higher accuracy on lower levels of H is not achievable by scaling the number of unique data samples or model size for the same architecture.</p>
<p>In Figure 6b, we report the performance of different positional encoding types.Although all encoding types achieve near-ceiling performance at high entropy, absolute encoding performs better for all levels of H. From our task formulation in Section 3, it is clear that positional information is crucial for solving the tasks.We attribute the efficiency of absolute encodings to the fact that the positional information of the verbs is independent of the remainder of the sequence.This invariance has to be learned by the other embedding types, making the absolute variant more information-efficient.</p>
<p>Previous Work</p>
<p>In this work, we base our experiments on a version of the SCAN grammar from Lake and Baroni (2018).The original grammar has been studied in other works, such as Loula et al. (2018); Dessì and Baroni (2019); Gordon et al. (2019), and there has also been work on the related PCFG SET dataset from Hupkes et al. (2020).</p>
<p>The work most similar to ours is Zhou et al. (2023), who also studies how data-centric aspects affect compositional generalization.In our work, we see compositionality as characterized by a set of primitives composed by a composition function, and we focus on how models capture this composition function depending on properties of the primitives.Their work, however, does not make this distinction, as their complexification process modify both the composition function and the distribution of primitives.The main distinction is hence that our work relates compositionality to dataset-level regularities, while they relate it to sample-level features.</p>
<p>Our work is also similar to Keysers et al. (2020), who quantifies the level of systematic generalization using distributional information.Specifically, they quantify the compositionality of an experiment as the divergence between the distribution of component parts and combinations in the training and test data.Their work differs from ours since they focus on the case where the distribution of component parts is as similar as possible between the training and test data, while our work focuses on the case where the distribution of component parts is inverted from the training to the test set for some parts of the input.</p>
<p>In another domain, Wiedemer et al. (2023) approaches systematicity in an image generation problem by factorizing the latent variables of the input image.The authors show that the support of these variables must be the same in the training and test distributions for any model to generalize systematically.We take a more fine-grained approach and our results demonstrate that support alone is insufficient as a predictor of systematic generalization; the performance also depends on the entropy, and higher levels of entropy in the training distribution can compensate for lower support.</p>
<p>There is also recent work on formalizing compositionality in terms of data complexity (Elmoznino et al., 2024), kernel-theory (Lippl and Stachenfeld, 2025), using graph formalisms (Ram et al., 2024), and decomposition strategies (Jarvis et al., 2023).Our work is similar to these in the sense that we try to formalize some properties of systematic generalization, which is closely tied to compositionality.</p>
<p>Conclusion</p>
<p>In this work, we show that systematic generalization of sequence-to-sequence models scales with information entropy.Through supporting experiments, we also show that this relationship is independent of the number of unique samples seen during training.Contrary to previous work, our findings demonstrate that these models are capable of systematic generalization even without any built-in architectural priors that incentivize compositional solutions.However, this requires that the training data have high entropy w.r.t. the parts that the target composition function operates on.This poses a new question: Is it possible to achieve systematicity also at low entropy without compositional priors?Through our formalizations, we hope to facilitate future work that attempts to answer this question, providing a clear method for assessing progress in systematic generalization research.</p>
<p>Limitations</p>
<p>Scope of our findings.In this work, we have considered an aspect of systematicity that is concerned with embedded sentences as defined by Hadley (1994).There is no guarantee that our findings generalize to scenarios that are different from ours, but that still falls in under some definition of systematic generalization.</p>
<p>Use of synthetic data.As is commonplace in compositionality research, our work relies on synthetic data that is generated by a CFG.This is necessary to quantify the entropy and to get a controlled experimental setup where the distribution of component parts in the embedded sentences can be measured precisely.This is generally not tractable for real-world corpora.Consequently, our method is constrained to cases where fine-grained distributional information is obtainable.outlined in Section 4.3 it has 1 layer and 64 hidden units (112 238 parameters).Teacher-forcing did not result in increased performance for either experiment.</p>
<p>B.2 CNN</p>
<p>We based our implementation of the CNN from Gehring et al. (2017) on the one found in FAIRSEQ (Ott, 2019).We conduct a grid search over the following learning rates: {1 × 10 −2 , 1 × 10 −3 , 1 × 10 −4 , 3 × 10 −4 }; kernel size: {3, 5}; number of hidden units: {64, 128, 256}; and number of layers: {1, 2, 3}; We train with a batch size of 32 and a cosine decay on the learning rate.</p>
<p>The CNN used for the experiment outlined in Section 4.2 has a learning rate of 3 × 10 −4 , a kernel size of 5, a hidden size of 64 and 3 layers (298 078 parameters), and the same configuration for Section 4.3.</p>
<p>B.3 Transformer</p>
<p>We implement the Transformer following the original from Vaswani (2017).We conduct a grid search over the following learning rates: {1 × 10 −3 , 1 × 10 −4 , 3×10 −4 , 1×10 −5 }; number of hidden units: {64, 128, 256}; and number of layers: {1, 2, 3}; As for the CNN, we train with a batch size of 32 and a cosine decay on the learning rate.</p>
<p>The Transformer used for the experiment outlined in Section 4.2 has a learning rate of 3 × 10 −4 , a hidden size of 128 and 3 layers (1 796 894 parameters), while for the experiment outlined in Section 4.3 it has 2 layers and 256 hidden units (4 763 166 parameters).</p>
<p>B.4 Positional Embedding</p>
<p>In addition to the absolute encodings used in Vaswani (2017), we implement two other variations of positional embeddings: RoPE Su et al. (2024) and Disentangled He et al. (2021).For all variations, we use the same hyperparameters found in the Transformer search (Appendix B.3).Additionally, we conduct a hyperparameter search for the encoding-specific parameters.For RoPE we conduct a search for the base value of the θ.We tried powers of 10, from one to one million.For Disentangled Attention we sweep over the input length for the relative attention.We have a separate length for the input and output encodings.We sweep over the following values: {4, 8, 16, 32, 64}.As for the Transformer, we train with a batch size of 32 and a cosine decay on the learning rate.</p>
<p>For the models used for the experiment outlined in Section 4.5, we use a RoPE θ value of 1 000, and for the disentangled encodings we use a relative input length of 64 and a relative output length of 4.</p>
<p>B.5 Permutation equivariant sequence model</p>
<p>For the permutation equivariant model, we use the original implementation from Gordon et al. (2019).We extend the cyclic group from four to eight verbs.For the hyperparameter selction, we conducted a grid search over the following learning rates: {1 × 10 −3 , 1 × 10 −4 , 3 × 10 −4 , 1 × 10 −5 }; the number of hidden units: {64, 128}; and RNN cell type: Elman RNN, GRU.All settings ran with 1 layer.</p>
<p>The model used for the experiment outlined in Section 4.2 has a learning rate of 1 × 10 −4 , a hidden size of 64 and the GRU cell types (≈ 130 000 parameters), and the same configuration for Section 4.3.</p>
<p>We note that the model presented in Lake and Baroni (2023) also encodes verb equivariance and achieves high performance on SCAN.However, this approach relies on automatically augmenting the training data by creating examples of the input that have different semantics than the original grammar, making our quantification of H impractical.Consequently, we only evaluate the permutationequivariant model from Gordon et al. (2019).</p>
<p>C Computational budget</p>
<p>All hyperparameter sweeps and experiments ran on AMD EPYC 7763 CPUs:</p>
<p>• RNN sweep: 2880 hours.</p>
<p>D Licenses and use</p>
<p>The present work uses the following scientific artifacts:</p>
<ol>
<li>We base our synthetic data on the SCAN grammar (Lake and Baroni, 2018).SCAN is released under a BSD License.We do not redistribute or use any of the source code, nor do we redistribute or use any of the original data files.We implement the grammar based</li>
</ol>
<p>Figure 3 :
3
Figure 3: Example of distributions of verbs for e 1 (left) and e 2 (right).</p>
<p>Figure 4 :
4
Figure 4: Probabilities in the degenerate case for e 2</p>
<p>Figure 5 :
5
Figure 5: Results from the Experiments 1 &amp; 2. The accuracy and standard deviations are from five seeds.</p>
<p>Figure 6 :
6
Figure 6: The result of the supplemental experiments.The accuracy and standard deviations are from five seeds.</p>
<p>. For example, when someone knows what it means for Person A to see Person B, they can readily generalize to what it means for Person B to see Person A, without having been exposed to examples where Person B is used in the subject position</p>
<p>Table 1 :
1
Output sequence y squat opposite right and squat RTURN RTURN SQUAT SQUAT squat twice and crawl opposite left SQUAT SQUAT LTURN LTURN CRAWL sprint right twice after sprint left LTURN SPRINT RTURN SPRINT RTURN SPRINT lunge opposite left and look thrice LTURN LTURN LUNGE LOOK LOOK LOOK Examples from our generated datasets, with green indicating e 1 and orange indicating e 2 .
Input sequence xabout our experimental setup. 34.1 ModelsRNN. For our RNN we use a similar configura-tion as previous work on SCAN
Hadley (1994) also discusses an intermediate degree between weak and strong systematicity, so-called quasisystematicity, but we leave this out for brevity, as it does not relate directly to our use case.
In contrast to the original SCAN, we do not have sentences without a conjunctive phrase.
Data generation scripts and code to reproduce our experiments can be found at https://github.com/ltgoslo/ systematicity-entropy.
AcknowledgementsWe acknowledge Sigma2-the National Infrastructure for High-Performance Computing and Data Storage in Norway-for providing access to the LUMI super-computer, part of the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium.We also want to thank Lilja Øvrelid, Erik Velldal, and David Samuel at the Language Technology Group for discussions and feedback on the manuscript.A Modified SCAN grammarOur modified SCAN grammar consists of the same parts of speech as the original, but we extend the set of verbs V with four additional items.We keep the original eight adjective phrases.The verbs and adjective phrases can be seen in Table2. Every verb can be combined with every adjective phrase in each embedded sequence (under no distributional constraints), combined with one of the two conjunctive phrases fromSCAN (and, after).B Model detailsAll models were implemented in Pytorch(Paszke et al., 2019).We conducted hyperparameter searches for each model per experiment.Below is a list of the search space used for each.All configurations used a weight decay of 1 × 10 −1 and a 10% dropout rate.B.1 RNNWe use the basic sequence-to-sequence implementation fromGordon et al. (2019), which is based on the Pytorch MT tutorial.We conduct a grid search over the following learning rates: {1 × 10 −3 , 1 × 10 −4 , 3 × 10 −4 }; cell type: Elman RNN (Elman, 1991), LSTM(Hochreiter and Schmidhuber, 1997), GRU(Chung et al., 2014); number of hidden units: {64, 128, 256}; number of layers: {1, 2, 3}; and teacher-forcing rate: {0.0, 0.5}.All models ran with an attention mechanism between the encoder and decoder, bidirectional layers, and with no learning rate decay.Following previous work, we used a batch size of 1.The RNN used for the experiment outlined in Section 4.2 has a learning rate of 1 × 10 −4 , normal RNN cell types, a hidden size of 128 and 2 layers (483 438 parameters), while for the experiment on the description provided in the paper.Our code implementation of the grammar includes the original license statement from SCAN.Our use of the permutation equivariant modelfromGordon et al. (2019)uses the original implementation.The code is released under an MIT license.All source code files redistributed with our work contain the original license statement, as per the license requirement.3. The Transformer(Vaswani, 2017)and the CNN(Gehring et al., 2017)is implemented by the authors in PyTorch(Paszke et al., 2019)and do not use existing code implementations.However, the CNN was largely based on the implementation found in Fairseq(Ott, 2019), which is released under an MIT license.We do not redistribute any of the original code files.E The use of AI assistantsThe authors used AI assistants for spellchecking and sentence-level editing in the writing of the current manuscript.
Noam Chomsky, Syntactic Structures. Mouton. 1957</p>
<p>Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, arXiv:1412.3555Empirical evaluation of gated recurrent neural networks on sequence modeling. 2014arXiv preprint</p>
<p>CNNs found to jump around more skillfully than RNNs: Compositional generalization in seq2seq convolutional networks. Roberto Dessì, Marco Baroni, 10.18653/v1/P19-1381Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Ronan Le Bras, et al. 2024. Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Jiang, Sean Yuchen Lin, Peter Welleck, Chandra West, Bhagavatula, Advances in Neural Information Processing Systems. 36</p>
<p>Distributed representations, simple recurrent networks, and grammatical structure. Jeffrey L Elman, Machine learning. 71991</p>
<p>Eric Elmoznino, Thomas Jiralerspong, Yoshua Bengio, Guillaume Lajoie, arXiv:2410.14817A complexitybased theory of compositionality. 2024arXiv preprint</p>
<p>Connectionism and cognitive architecture: A critical analysis. Jerry A Fodor, Zenon W Pylyshyn, Cognition. 281-21988</p>
<p>Convolutional sequence to sequence learning. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N Dauphin, International conference on machine learning. PMLR2017</p>
<p>Permutation equivariant models for compositional generalization in language. Jonathan Gordon, David Lopez-Paz, Marco Baroni, Diane Bouchacourt, International Conference on Learning Representations. 2019</p>
<p>. Hadley Robert, Systematicity in connectionist language learning. Mind &amp; Language. 931994</p>
<p>DeBERTa: Decoding-enhanced BERT with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, International Conference on Learning Representations. 2021</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Computation. 981997</p>
<p>Compositionality decomposed: How do neural networks generalise. Dieuwke Hupkes, Verna Dankers, Mathijs Mul, Elia Bruni, Journal of Artificial Intelligence Research. 672020</p>
<p>Ryan Cotterell, and Zhijing Jin. 2023. A taxonomy and review of generalization research in NLP. Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Arabella Sinclair, Dennis Ulmer, Florian Schottmann, Khuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske, 10.1038/s42256-023-00729-yNature Machine Intelligence. 510</p>
<p>On the specialization of neural modules. Devon Jarvis, Richard Klein, Benjamin Rosman, Andrew M Saxe, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Measuring compositional generalization: A comprehensive method on realistic data. Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc Van Zee, Olivier Bousquet, International Conference on Learning Representations. 2020</p>
<p>Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. Brenden Lake, Marco Baroni, International conference on machine learning. PMLR2018</p>
<p>Human-like systematic generalization through a meta-learning neural network. M Brenden, Marco Lake, Baroni, Nature. 62379852023</p>
<p>When does compositional structure yield compositional generalization? a kernel theory. Samuel Lippl, Kim Stachenfeld, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Rearranging the familiar: Testing compositional generalization in recurrent networks. João Loula, Marco Baroni, Brenden Lake, 10.18653/v1/W18-5413Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Toward compositional behavior in neural models: A survey of current views. Kate Mccurdy, Paul Soulos, Paul Smolensky, Roland Fernandez, Jianfeng Gao, 10.18653/v1/2024.emnlp-main.524Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar, arXiv:2410.052292024Preprint</p>
<p>fairseq: A fast, extensible toolkit for sequence modeling. Ott, arXiv:1904.010382019arXiv preprint</p>
<p>Compositionality i: Definitions and variants. Peter Pagin, Dag Westerståhl, Philosophy Compass. 532010</p>
<p>Pytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. 201932</p>
<p>Learnability and cognition: The acquisition of argument structure. Steven Pinker, 1989MIT Press</p>
<p>How effective are state space models for machine translation?. Hugo Pitorro, Pavlo Vasylenko, Marcos Treviso, André Martins, 10.18653/v1/2024.wmt-1.111Proceedings of the Ninth Conference on Machine Translation. the Ninth Conference on Machine TranslationMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>What makes models compositional? a theoretical view. Parikshit Ram, Tim Klinger, Alexander G Gray, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. the Thirty-Third International Joint Conference on Artificial Intelligence2024</p>
<p>Noam Shazeer, arXiv:2002.05202Glu variants improve transformer. 2020arXiv preprint</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, Yunfeng Liu, Neurocomputing. 5681270632024</p>
<p>Logic, Language, and Meaning. Jfak Van Bethem, Groenendijk, Dhj De Jong, Stockhof, Verkuyl, 1991Intensional Logic and Intensional Grammar. University of Chicago Press2Chicago, IL</p>
<p>Attention is all you need. Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Compositional generalization from first principles. Thaddäus Wiedemer, Prasanna Mayilvahanan, Matthias Bethge, Wieland Brendel, Advances in Neural Information Processing Systems. Curran Associates, Inc202336</p>
<p>Compositional generalization with grounded language models. Sondre Wold, Étienne Simon, Lucas Charpentier, Egor Kostylev, Erik Velldal, Lilja Øvrelid, 10.18653/v1/2024.findings-acl.205Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Data factors for better compositional generalization. Xiang Zhou, Yichen Jiang, Mohit Bansal, 10.18653/v1/2023.emnlp-main.898Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>            </div>
        </div>

    </div>
</body>
</html>
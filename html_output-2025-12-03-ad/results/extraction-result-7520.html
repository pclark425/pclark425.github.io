<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7520 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7520</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7520</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-df59d0098c1b2c1ee8995da802dd6b12d158c2b8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/df59d0098c1b2c1ee8995da802dd6b12d158c2b8" target="_blank">Large-scale chemical language representations capture molecular structure and properties</a></p>
                <p><strong>Paper Venue:</strong> Nature Machine Intelligence</p>
                <p><strong>Paper TL;DR:</strong> Encouraging evidence is provided that large-scale molecular language models can capture sufficient chemical and structural information to predict various distinct molecular properties, including quantum-chemical properties.</p>
                <p><strong>Paper Abstract:</strong> Models based on machine learning can enable accurate and fast molecular property predictions, which is of interest in drug discovery and material design. Various supervised machine learning models have demonstrated promising performance, but the vast chemical space and the limited availability of property labels make supervised learning challenging. Recently, unsupervised transformer-based language models pretrained on a large unlabelled corpus have produced state-of-the-art results in many downstream natural language processing tasks. Inspired by this development, we present molecular embeddings obtained by training an efficient transformer encoder model, M o LF ormer , which uses rotary positional embeddings. This model employs a linear attention mechanism, coupled with highly distributed training, on SMILES sequences of 1.1 billion unlabelled molecules from the PubChem and ZINC datasets. We show that the learned molecular representation outperforms existing baselines, including supervised and self-supervised graph neural networks and language models, on several downstream tasks from ten benchmark datasets. They perform competitively on two others. Further analyses, specifically through the lens of attention, demonstrate that M o LF ormer trained on chemical SMILES indeed learns the spatial relationships between atoms within a molecule. These results provide encouraging evidence that large-scale molecular language models can capture sufficient chemical and structural information to predict various distinct molecular properties, including quantum-chemical properties. Large language models have recently emerged with extraordinary capabilities, and these methods can be applied to model other kinds of sequence, such as string representations of molecules. Ross and colleagues have created a transformer-based model, trained on a large dataset of molecules, which provides good results on property prediction tasks.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7520.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7520.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolFormer-XL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MoLFormer-XL (Molecular Language transFormer - XL variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer encoder language model trained on SMILES (1.1B molecules) using linear attention and rotary positional embeddings, used as a text-based simulator to predict diverse molecular properties (classification and regression) from SMILES alone via fine-tuning or frozen embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoLFormer-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>12-layer encoder transformer, hidden size 768, 12 attention heads (parameter count not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>pre-trained masked language model (encoder) trained on domain corpus; used in downstream tasks both via fine-tuning and as frozen embedding (task-specific MLPs)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (small-molecule property prediction; quantum chemistry, physical, biophysical, physiological properties)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation / prediction of molecular properties from SMILES strings, including binary classification tasks (BBBP, Tox21, ClinTox, HIV, BACE, SIDER) and regression tasks including quantum-chemical properties (QM9, QM8) and physical properties (ESOL, FreeSolv, Lipophilicity).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Not a natural-language prompting setup; model trained with masked language modeling (MLM) pretraining (15% token masking) and then applied to downstream tasks by either (a) freezing encoder and training a task MLP (frozen strategy) or (b) fine-tuning encoder jointly with a 2-layer fully connected head (fine-tuned strategy). No few-shot / in-context prompting reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Classification: AUC-ROC; Regression: average MAE (QM9, QM8) or RMSE (ESOL, FreeSolv, Lipophilicity); correlation metrics for structural similarity (cosine, Euclidean). Attention vs distance similarity: cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Classification AUC-ROC: BBBP 93.7, Tox21 84.7, ClinTox 94.8, HIV 82.2, BACE 88.21, SIDER 69.0. Regression: QM9 avg MAE = 1.5894 (units consistent with dataset, eV for some QM9 measures as noted), QM8 avg MAE = 0.0102, ESOL RMSE = 0.2787, FreeSolv RMSE = 0.2308, Lipophilicity RMSE = 0.5289. Attention vs distance cosine similarities: short-range ~0.596-0.615, medium-range ~0.724-0.729, long-range ~0.210-0.211 (table values averaged across layers/heads as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Classification baselines (examples): N-Gram: BBBP 91.2; SVM: Tox21 81.8; MolCLR: ClinTox 93.2; Graph-based supervised baselines and GNNs varied per task. Regression baselines (examples from Table 2): GC QM9 avg MAE = 4.3536, A-FP QM9 = 2.6355, MPNN QM9 = 3.1898. Specialized 3D GNNs (SchNet, DimeNet) outperform MolFormer on QM9 but by factors ~8-10 (paper notes).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Pre-training dataset size and composition (PubChem + ZINC combined ~1.1B molecules vs subsets such as 100% ZINC or mixed smaller subsets) affects downstream accuracy', 'Model depth/size (MolFormer-XL vs MolFormer-Base) — deeper model improved performance', 'Fine-tuning vs frozen embeddings (fine-tuning substantially better than frozen)', 'Positional embedding choice (rotary/relative vs absolute) — rotary performed better at large scale; absolute outperformed rotary on smaller pretraining datasets but rotary won when scale >1B', 'Attention mechanism (linear attention vs full attention) — linear attention enabled scalability and in some analyses captured medium/long-range relations better', 'Availability of 3D geometric (privileged) information: absence of 3D coordinates limits quantum-chemical energy prediction accuracy relative to geometry-aware 3D GNNs', 'Pretraining vocabulary and molecule length distribution (e.g., ZINC-only had smaller vocabulary and shorter molecules leading to worse performance)', 'Training/stability factors: bucketing by sequence length, optimizer choice (Fused Lamb), batch size and distributed training configuration']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Pretraining: masked language modeling (15% tokens selected; 80% masked, 10% random token, 10% unchanged), trained 4 epochs on PubChem+ZINC (~1.1B molecules), fixed LR 1.6e-4, batch size 1600 molecules/GPU on 16 V100 GPUs, linear attention (generalized feature map size 32), rotary positional embeddings (modified linear RoFormer formula), adaptive bucketing by sequence length. Fine-tuning: batch sizes 64 or 128, LR ~3e-5 (QM9 fine-tuning), optimizer Fused Lamb for pretraining; downstream discriminator/head: 2-layer MLP hidden dim 768, dropout 0.1, GELU.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Underperforms specialized 3D geometry-aware models (SchNet, DimeNet) on QM9 quantum-chemical energy tasks (MolFormer-XL MAE higher by ~8-10x), frozen encoder strategy yields much worse performance than fine-tuning, models pre-trained only on ZINC performed worse (likely due to limited vocabulary/length diversity), attention correlates weakly with long-range (>4Å) interatomic distances (~0.2 cosine similarity), SMILES-only input lacks explicit 3D topology so tasks heavily dependent on 3D structure remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large-scale chemical language representations capture molecular structure and properties', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7520.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7520.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemBERTa (large-scale self-supervised pretraining for molecular property prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously published pre-trained molecular language model (SMILES-based) used in this paper as a baseline for comparison on molecular property prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemberta: large-scale self-supervised pretraining for molecular property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper (described as pre-trained transformer on a smaller chemical dataset than MolFormer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>pre-trained masked language model (baseline); pre-trained on a smaller chemical corpus and fine-tuned for downstream tasks</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (molecular property prediction from SMILES)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Baseline text-based prediction of molecular properties from SMILES; used as comparator on classification/regression tasks from MoleculeNet.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Pretraining via masked language modeling (as per original ChemBERTa); used in fine-tuned evaluation for downstream tasks (no prompting/in-context examples reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Same metrics used for comparison: AUC-ROC for classification, MAE/RMSE for regression depending on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Reported in paper Table 1 for available tasks: BBBP 64.3 AUC-ROC; ClinTox 90.6 AUC-ROC; HIV 62.2 AUC-ROC (note: several entries not reported '-'). (ChemBERTa results adopted from reference [25] as baseline values.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Serves as baseline itself compared to MolFormer-XL and graph-based baselines; MolFormer-XL substantially outperforms ChemBERTa on many benchmarks reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Pre-training corpus size (ChemBERTa trained on a smaller chemical dataset than MolFormer) which likely affects downstream performance', 'Model architecture and pretraining regimen differences (implied) compared to MolFormer']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>ChemBERTa values are taken from prior work and used as baselines under MoleculeNet evaluation splits; this paper does not re-train ChemBERTa but uses published metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>As a baseline reported values are incomplete for some tasks in Table 1; paper notes that smaller pretraining corpora can reduce downstream performance relative to very large pretraining (MolFormer-XL).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large-scale chemical language representations capture molecular structure and properties', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chemberta: large-scale self-supervised pretraining for molecular property prediction <em>(Rating: 2)</em></li>
                <li>Smiles-bert: large scale unsupervised pre-training for molecular property prediction <em>(Rating: 2)</em></li>
                <li>X-mol: large-scale pre-training for molecular understanding and diverse molecular analysis <em>(Rating: 2)</em></li>
                <li>Chemformer: a pre-trained transformer for computational chemistry <em>(Rating: 2)</em></li>
                <li>Pre-training molecular graph representation with 3d geometry <em>(Rating: 1)</em></li>
                <li>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7520",
    "paper_id": "paper-df59d0098c1b2c1ee8995da802dd6b12d158c2b8",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "MolFormer-XL",
            "name_full": "MoLFormer-XL (Molecular Language transFormer - XL variant)",
            "brief_description": "A transformer encoder language model trained on SMILES (1.1B molecules) using linear attention and rotary positional embeddings, used as a text-based simulator to predict diverse molecular properties (classification and regression) from SMILES alone via fine-tuning or frozen embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MoLFormer-XL",
            "model_size": "12-layer encoder transformer, hidden size 768, 12 attention heads (parameter count not specified)",
            "model_type": "pre-trained masked language model (encoder) trained on domain corpus; used in downstream tasks both via fine-tuning and as frozen embedding (task-specific MLPs)",
            "scientific_domain": "Chemistry (small-molecule property prediction; quantum chemistry, physical, biophysical, physiological properties)",
            "simulation_task_description": "Text-based simulation / prediction of molecular properties from SMILES strings, including binary classification tasks (BBBP, Tox21, ClinTox, HIV, BACE, SIDER) and regression tasks including quantum-chemical properties (QM9, QM8) and physical properties (ESOL, FreeSolv, Lipophilicity).",
            "prompting_strategy": "Not a natural-language prompting setup; model trained with masked language modeling (MLM) pretraining (15% token masking) and then applied to downstream tasks by either (a) freezing encoder and training a task MLP (frozen strategy) or (b) fine-tuning encoder jointly with a 2-layer fully connected head (fine-tuned strategy). No few-shot / in-context prompting reported.",
            "evaluation_metric": "Classification: AUC-ROC; Regression: average MAE (QM9, QM8) or RMSE (ESOL, FreeSolv, Lipophilicity); correlation metrics for structural similarity (cosine, Euclidean). Attention vs distance similarity: cosine similarity.",
            "reported_accuracy": "Classification AUC-ROC: BBBP 93.7, Tox21 84.7, ClinTox 94.8, HIV 82.2, BACE 88.21, SIDER 69.0. Regression: QM9 avg MAE = 1.5894 (units consistent with dataset, eV for some QM9 measures as noted), QM8 avg MAE = 0.0102, ESOL RMSE = 0.2787, FreeSolv RMSE = 0.2308, Lipophilicity RMSE = 0.5289. Attention vs distance cosine similarities: short-range ~0.596-0.615, medium-range ~0.724-0.729, long-range ~0.210-0.211 (table values averaged across layers/heads as reported).",
            "baseline_accuracy": "Classification baselines (examples): N-Gram: BBBP 91.2; SVM: Tox21 81.8; MolCLR: ClinTox 93.2; Graph-based supervised baselines and GNNs varied per task. Regression baselines (examples from Table 2): GC QM9 avg MAE = 4.3536, A-FP QM9 = 2.6355, MPNN QM9 = 3.1898. Specialized 3D GNNs (SchNet, DimeNet) outperform MolFormer on QM9 but by factors ~8-10 (paper notes).",
            "factors_reported": [
                "Pre-training dataset size and composition (PubChem + ZINC combined ~1.1B molecules vs subsets such as 100% ZINC or mixed smaller subsets) affects downstream accuracy",
                "Model depth/size (MolFormer-XL vs MolFormer-Base) — deeper model improved performance",
                "Fine-tuning vs frozen embeddings (fine-tuning substantially better than frozen)",
                "Positional embedding choice (rotary/relative vs absolute) — rotary performed better at large scale; absolute outperformed rotary on smaller pretraining datasets but rotary won when scale &gt;1B",
                "Attention mechanism (linear attention vs full attention) — linear attention enabled scalability and in some analyses captured medium/long-range relations better",
                "Availability of 3D geometric (privileged) information: absence of 3D coordinates limits quantum-chemical energy prediction accuracy relative to geometry-aware 3D GNNs",
                "Pretraining vocabulary and molecule length distribution (e.g., ZINC-only had smaller vocabulary and shorter molecules leading to worse performance)",
                "Training/stability factors: bucketing by sequence length, optimizer choice (Fused Lamb), batch size and distributed training configuration"
            ],
            "experimental_conditions": "Pretraining: masked language modeling (15% tokens selected; 80% masked, 10% random token, 10% unchanged), trained 4 epochs on PubChem+ZINC (~1.1B molecules), fixed LR 1.6e-4, batch size 1600 molecules/GPU on 16 V100 GPUs, linear attention (generalized feature map size 32), rotary positional embeddings (modified linear RoFormer formula), adaptive bucketing by sequence length. Fine-tuning: batch sizes 64 or 128, LR ~3e-5 (QM9 fine-tuning), optimizer Fused Lamb for pretraining; downstream discriminator/head: 2-layer MLP hidden dim 768, dropout 0.1, GELU.",
            "limitations_or_failure_modes": "Underperforms specialized 3D geometry-aware models (SchNet, DimeNet) on QM9 quantum-chemical energy tasks (MolFormer-XL MAE higher by ~8-10x), frozen encoder strategy yields much worse performance than fine-tuning, models pre-trained only on ZINC performed worse (likely due to limited vocabulary/length diversity), attention correlates weakly with long-range (&gt;4Å) interatomic distances (~0.2 cosine similarity), SMILES-only input lacks explicit 3D topology so tasks heavily dependent on 3D structure remain challenging.",
            "uuid": "e7520.0",
            "source_info": {
                "paper_title": "Large-scale chemical language representations capture molecular structure and properties",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "ChemBERTa",
            "name_full": "ChemBERTa (large-scale self-supervised pretraining for molecular property prediction)",
            "brief_description": "A previously published pre-trained molecular language model (SMILES-based) used in this paper as a baseline for comparison on molecular property prediction tasks.",
            "citation_title": "Chemberta: large-scale self-supervised pretraining for molecular property prediction",
            "mention_or_use": "mention",
            "model_name": "ChemBERTa",
            "model_size": "Not specified in this paper (described as pre-trained transformer on a smaller chemical dataset than MolFormer)",
            "model_type": "pre-trained masked language model (baseline); pre-trained on a smaller chemical corpus and fine-tuned for downstream tasks",
            "scientific_domain": "Chemistry (molecular property prediction from SMILES)",
            "simulation_task_description": "Baseline text-based prediction of molecular properties from SMILES; used as comparator on classification/regression tasks from MoleculeNet.",
            "prompting_strategy": "Pretraining via masked language modeling (as per original ChemBERTa); used in fine-tuned evaluation for downstream tasks (no prompting/in-context examples reported here).",
            "evaluation_metric": "Same metrics used for comparison: AUC-ROC for classification, MAE/RMSE for regression depending on dataset.",
            "reported_accuracy": "Reported in paper Table 1 for available tasks: BBBP 64.3 AUC-ROC; ClinTox 90.6 AUC-ROC; HIV 62.2 AUC-ROC (note: several entries not reported '-'). (ChemBERTa results adopted from reference [25] as baseline values.)",
            "baseline_accuracy": "Serves as baseline itself compared to MolFormer-XL and graph-based baselines; MolFormer-XL substantially outperforms ChemBERTa on many benchmarks reported in this paper.",
            "factors_reported": [
                "Pre-training corpus size (ChemBERTa trained on a smaller chemical dataset than MolFormer) which likely affects downstream performance",
                "Model architecture and pretraining regimen differences (implied) compared to MolFormer"
            ],
            "experimental_conditions": "ChemBERTa values are taken from prior work and used as baselines under MoleculeNet evaluation splits; this paper does not re-train ChemBERTa but uses published metrics.",
            "limitations_or_failure_modes": "As a baseline reported values are incomplete for some tasks in Table 1; paper notes that smaller pretraining corpora can reduce downstream performance relative to very large pretraining (MolFormer-XL).",
            "uuid": "e7520.1",
            "source_info": {
                "paper_title": "Large-scale chemical language representations capture molecular structure and properties",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chemberta: large-scale self-supervised pretraining for molecular property prediction",
            "rating": 2
        },
        {
            "paper_title": "Smiles-bert: large scale unsupervised pre-training for molecular property prediction",
            "rating": 2
        },
        {
            "paper_title": "X-mol: large-scale pre-training for molecular understanding and diverse molecular analysis",
            "rating": 2
        },
        {
            "paper_title": "Chemformer: a pre-trained transformer for computational chemistry",
            "rating": 2
        },
        {
            "paper_title": "Pre-training molecular graph representation with 3d geometry",
            "rating": 1
        },
        {
            "paper_title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
            "rating": 1
        }
    ],
    "cost": 0.012438499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large-Scale Chemical Language Representations Capture Molecular Structure and Properties</h1>
<p>Jerret Ross ${ }^{1, <em>}$, Brian Belgodere ${ }^{1}$, Vijil Chenthamarakshan ${ }^{1}$, Inkit Padhi ${ }^{1}$, Youssef Mroueh ${ }^{1}$, and Payel Das ${ }^{1, </em>}$<br>${ }^{1}$ IBM Research, Yorktown Heights, NY 10598, USA<br>*rossja@us.ibm.com and daspa@us.ibm.com</p>
<h4>Abstract</h4>
<p>Predicting the properties of a chemical molecule is of great importance in many applications, including drug discovery and material design. Machine learning-based models promise to enable more accurate and faster molecular property predictions than the current state-of-the-art techniques, such as Density Functional Theory calculations or wet-lab experiments. Various supervised machine learning models, including graph neural nets, have demonstrated promising performance in molecular property prediction tasks. However, the vast chemical space and the limited availability of property labels make supervised learning challenging, calling for learning a general-purpose molecular representation. Recently, unsupervised transformerbased language models pre-trained on large unlabeled corpus have produced state-of-the-art results in many downstream natural language processing tasks. Inspired by this development, we present molecular embeddings obtained by training an efficient transformer encoder model, MoLFormer, which uses rotary positional embeddings. This model employs a linear attention mechanism, coupled with highly distributed training, on SMILES sequences of 1.1 billion unlabeled molecules from the PubChem and ZINC datasets. Experiments show that utilizing the learned molecular representation outperforms existing baselines on downstream tasks, including supervised and self-supervised graph neural net baselines and language models, on several classification and regression tasks from ten benchmark datasets while performing competitively on two others. Further analyses, specifically through the lens of attention, demonstrate that MoLFormer trained on chemical SMILES indeed learns the spatial relationships between atoms within a molecule. These results provide encouraging evidence that the large-scale molecular language models can capture sufficient chemical and structural information to predict various distinct molecular properties, including quantum-chemical properties.</p>
<h2>Main</h2>
<p>Machine Learning (ML) has emerged as an appealing, computationally efficient approach for predicting molecular properties, with implications in drug discovery and material engineering. ML models for molecules can be trained directly on pre-defined chemical descriptors, such as unsupervised molecular fingerprints ${ }^{1}$, or hand-derived derivatives of geometric features such as a Coulomb Matrix (CM) ${ }^{2}$. However, more recent ML models have focused on automatically learning the features either from the natural graphs that encode the connectivity information or from the line annotations of molecular structures, such as the popular SMILES ${ }^{3}$ (Simplified Molecular-Input Line Entry System) representation. SMILES defines a character string representation of a molecule by performing a depth-first pre-order spanning tree traversal of the molecular graph, generating symbols for each atom, bond, tree-traversal decision, and broken cycles. Therefore, the resulting character string corresponds to a flattening of a spanning tree of the molecular graph. Learning on SMILES has been widely adopted for molecular property prediction ${ }^{4-7}$ as SMILES is generally more compact than other methods of representing structure, including graphs. Additionally, meaningful substructures such as branches, cyclic structures, and chirality information are explicitly represented in SMILES strings, which is not the case for the graph representation.</p>
<p>However, the SMILES grammar is complex and restrictive; most sequences over the appropriate character set do not belong to well-defined molecules. Alternative string-based representations exist, such as SMARTS ${ }^{8}$ and SELFIES ${ }^{9}$. Comparing benefits of these alternative representations with respect to SMILES is an active area of research. For example ${ }^{10}$, focusing on molecular optimization tasks on the learned representation space, suggested no obvious shortcoming of SMILES with respect to SELFIES in terms of optimization ability and sample efficiency, particularly when the language model is more advanced. Nevertheless, string-based representations are thought to not be topologically-aware, while graphs are. Due to these limitations, deep chemical language models may focus on learning the grammar of molecular strings and not the implicit topological structure of the molecular graphs. Accordingly, while string-based deep neural nets have been employed in predicting molecular properties ${ }^{5-7,11}$, they are typically outperformed by graph neural networks (GNNs) ${ }^{12}$ and their variants ${ }^{13-21}$. GNN frameworks can be generally viewed as "message passing", which includes local neighborhood information aggregation and information</p>
<p>updates across different levels of granularity, e.g., nodes, edges, or the full graph, according to the graph's connectivity structure.
One challenge with supervised training of GNNs and language models for molecular property prediction is the scarcity of labeled data. Label annotation of molecules is typically expensive and this problem is compounded by the fact that the size of the space consisting of plausible chemicals in need of annotation is astronomically large $\left(10^{60}\right.$ to $\left.10^{100}\right)^{22}$. Such a scenario creates the need for molecular representation learning which can be generalizable to various property prediction tasks in an un-/self-supervised setting. The recent success of large transformer-based ${ }^{23}$ foundation models ${ }^{24}$, using the paradigm of learning a task-agnostic language representation, obtained by pre-training on large unlabeled corpora and subsequently using it for fine-tuning on downstream tasks of interest, has been extended to other domains.</p>
<p>Pre-trained Language Models (LMs) ${ }^{25}$ and GNNs ${ }^{26}$ have only recently started to emerge for predicting molecular properties. However, to what extent pre-trained LMs, trained on a large corpus of billions of molecules, are able to capture the moleculeproperty relationships across various downstream tasks remains unexplored.</p>
<p>Towards this direction, here we present molecular SMILES transformer models referred to as MolFormer (Molecular Language transFormer). We name our best performing MolFormer variant MolFormer-XL. MolFormer-XL was obtained using an efficient linear attention mechanism trained on a large corpus of 1.1 billion molecules (see Figure 1). Results show, for the first time, that pre-trained transformer encoders of molecular SMILES perform competitively with existing supervised or unsupervised LM and GNN baselines on predicting a wide variety of molecular properties, including quantum-mechanical properties.</p>
<p>Our main contributions are:</p>
<ul>
<li>We train a large-scale and efficient Molecular Language model transFormer (MolFormer) on over a billion molecules, with relatively limited hardware resources (up to 16 V 100 GPUs). We owe our scalability and speedups to efficient linear time attention, adaptive bucketing of batches, and open-source parallelization provided in PyTorch Lightning and NCCL. With the combination of bucketing and linear attention we are able to achieve a batch size of 1600 molecules per GPU. Using 16 GPUs we need 208 hours to complete 4 epochs of pre-training for MolFormer-XL. To complete training in the same amount of time without bucketing and linear attention we would be limited to less than 50 molecules per GPU and require over 1000 GPUs for the task.</li>
<li>We explore the difference between absolute and relative position embeddings in representing molecular SMILES. We also provide a new, efficient, and accurate linear attention approximation of the recently proposed relative position RoFormer ${ }^{27}$.</li>
<li>We perform extensive experimentation and ablation studies on several classification and regression tasks from 10 benchmark datasets, covering quantum mechanical, physical, biophysical, and physiological property prediction of small molecule chemicals from MoleculeNet ${ }^{28}$.</li>
<li>Our results provide encouraging evidence that MolFormer representations can accurately capture sufficient chemical and structural information to predict a diverse range of chemical properties. Furthermore, the performance of MolFORMER is either better or on par with state-of-the-art GNNs that learn from precise graph topology information and beyond (e.g., bond distances).</li>
<li>We provide further analyses to demonstrate that MolFormer can capture substructures, as well as spatial interatomic distances within a molecule from SMILES annotations only.</li>
</ul>
<p>To our knowledge, the present study is the first one that explores the representational power of pre-trained chemical language models on predicting a broad range of downstream molecular properties from quantum chemical to physiological. In particular, predicting quantum-chemical properties from SMILES strings alone is non-trivial, as those properties are largely dependent on the accurate 3D molecular geometric information, which is considered privileged information and not available in general.</p>
<h1>Results and Discussion</h1>
<h2>MoLFormer Framework</h2>
<p>The goal of MolFormer is to learn a universal molecular representation from large scale chemical SMILES data and then evaluate the representation on various downstream molecular property prediction tasks, as shown in Figure 1. To do so, MolFormer model is developed using the masked language model framework ${ }^{29,30}$, which randomly masks a certain percentage of tokens within a SMILES sequence during training and then predicts those tokens. The masked language modeling thus exploits self-supervision and enables contextual learning. To allow better contextual learning and faster training, rotary positional embedding ${ }^{27}$ was used instead absolute positional embedding, along with linear attention ${ }^{31}$ (See Methods and Supplementary Information for further details on model architecture and training). We saw increased stability and faster convergence in training loss behavior when pre-training using rotary embeddings in contrast to absolute embeddings as observed</p>
<p>in Figure 2. To demonstrate the effectiveness of the pre-trained MolFormer as a universal and task-agnostic molecular representation, we benchmarked its adaptation performance on numerous challenging classification and regression tasks from MoleculeNet ${ }^{28}$. Details of the benchmark datasets can be found in SI Section C.</p>
<h1>Derivation of MolFormer Embeddings</h1>
<p>We encode a chemical SMILES by extracting the mean of all embeddings of the last hidden state from the encoder model. The resulting embedding is used for all downstream tasks. The downstream tasks themselves can be divided into two categories, The first category being called Frozen and the second being called Fine-tuned. The Frozen setting is defined by training a fully connected model for each task, while keeping the encoder embeddings fixed. The second setting, Fine-tuned, involves fine-tuning the weights of the encoder model jointly with the fully connected model for each downstream task. The ideal configuration and hyperparameters for the frozen strategy are discovered through a grid search as described in SI Table 1. For the fine-tuned strategy, we use a 2-layer fully connected network with a hidden dimension of 768 (matching the encoder embedding) with Dropout (set to 0.1 ) and GELU layers in-between, on top of a final single output dimension for regression tasks.</p>
<h2>Performance of MolFormer Embeddings on Downstream Tasks</h2>
<p>We evaluate the performance of MolFormer embeddings and compare them with existing baselines on six classification and five regression tasks from the MoleculeNet benchmark ${ }^{28}$, as discussed below. We refer to MolFormer which has been pre-trained on the entire training set comprised of $\approx 1.1 \mathrm{~B}$ molecules (all molecules from both PubChem and Zinc) as MolFormer-XL. Unless stated otherwise, the MolFormer-XL is trained with linear attention using rotary positional embeddings and the performance reported is of the model fine-tuned on the downstream task (see Methods for details). To predict various properties on the downstream tasks we fined-tuned the model as described in the previous section. We use the training, validation and testing data split as defined by the MoleculeNet benchmark for all tasks (see SI C).</p>
<p>Classification Tasks We choose six classification tasks from the MoleculeNet benchmark with nine total baselines, four supervised and five self-supervised, for comparison against MolFormer-XL. The supervised baselines consist of shallow machine learning models trained on molecular fingerprints (RF and SVM in Table 1) and graph neural nets. Among the pre-trained/self-supervised baselines, Hu, et al. ${ }^{32}$ pre-trains a Graph Isomorphism Network (GIN, a GNN that uses an MLP and weighted sum of node features in the aggregation) on molecular graphs that includes edge features involved in aggregation. N -gram graph ${ }^{33}$ uses a simple unsupervised representation for molecules by first embedding the nodes in a graph and then constructing a compact representation of the graph by assembling the vertex embeddings in short walks in the graph. MolCLR ${ }^{26}$ is a self-supervised learning framework based on GIN, which uses contrastive loss ${ }^{34,35}$. GraphMVP-C is the Graph MultiView Pre-training (GraphMVP) framework proposed by reference ${ }^{36}$, where self-supervised learning (SSL) is performed by leveraging the correspondence and consistency between 2D topological structures and 3D geometric views. We have considered three other geometry-aware GNN baselines, one supervised (DimeNet ${ }^{37}$ ), and two self-supervised (GeomGCL ${ }^{36}$ and $\mathrm{GEM}^{38}$ ). ChemBERTa ${ }^{25}$ is a pre-trained molecular language model trained on a smaller chemical dataset. Table 1 documents the performance comparison of MolFormer with these baselines on six classification benchmarks using the MoleculeNet scaffold data splits. MolFormer-XL outperforms all baselines in three (BBBP, ClinTox, and SIDER) out of six benchmarks and comes a close second in the other three (Tox21, HIV, and BACE).</p>
<p>Regression Tasks Next, we evaluate MolFormer-XL on more challenging regression tasks from MoleculeNet. We report our performance on five regression benchmarks, namely QM9, QM8, ESOL, FreeSolv, and Lipophilicity, in Table 2. In particular, QM9 and QM8 involve predicting several quantum chemical measures, which is considered challenging without having access to privileged 3D geometric information. Again we use the train, validation and test split as suggested in ${ }^{28}$ for these tasks. The baselines considered are a molecular graph convolutional network (GC, a GNN that utilizes a mean-pooling over the node and its neighbors before the linear transformation) ${ }^{39}$, the attentive-FP (A-FP) model ${ }^{40}$, and an MPNN variant ${ }^{18}$ that learns edge features such as pairwise interatomic distances. Results show that MolFormer-XL upon task-specific fine-tuning outperforms the existing supervised GNN baselines, specifically GC, A-FP, and MPNN (augmented with bond distances for QM8 and QM9), on all five tasks. Table 7 further shows MolFormer outperforming geometry-aware GNNs (DimeNet, GeomGCL, and GEM) on three physical property regression benchmarks. These results, combined with MolFormer-XL performance on the classification benchmarks confirm its generalizability.</p>
<p>A Closer Look at QM9 Table 9 further compares MolFormer-XL performance on the QM9 atomization energies and enthalpy (internal energy/enthalpy corrected for reference atomic energy, in eV ) prediction tasks with two exemplary supervised 3D GNNs, such as SchNet ${ }^{41}$ and Dimenet ${ }^{37}$. MolFormer-XL trained on SMILES alone is outperformed by both those models in all of the four tasks. However, SchNet, and DimeNet, which directly encode 3D information with specialized architecture for modeling quantum interactions, beats MolFormer-XL only by roughly a factor of 8 and by roughly a factor of 10 ,</p>
<p>respectively. This result, along with Tables 1 and 2, reinstates the power of learning an universal molecular representation from readily available information, such as SMILES, at a broader scale, while confirming the crucial role of privileged geometric information for quantum-chemical energy prediction. Further, results from this comparison opens up the door for future investigations, with the goal of estimating emergence of geometric awareness in MoLFormer (see later sections) or how the expressiveness of SMILES-only MoLFormer can be further enhanced by adding partial or complete 3D geometric information.</p>
<h1>Ablation Studies</h1>
<p>In this section we discuss several different ablations of MoLFORMER-XL in an attempt to provide insights into its impressive performance. The ablations we performed can be broadly divided in the following three categories (1) the effect of size and the nature of the pre-training data and model depth, and (2) the results with (frozen) and with (fine-tuned) model fine-tuning on the downstream data, (3) the effect of absolute and rotary positional embeddings.</p>
<p>Data/Model Size First we investigate how pre-training dataset size affects the performance of MoLFORMER-XL on several downstream tasks from the MoleculeNet benchmark. To accomplish this we chose 3 different weighted combinations of the PubChem and Zinc datasets, specifically a set consisting of $10 \%$ of Zinc and $10 \%$ PubChem, another with $100 \%$ of PubChem mixed with $10 \%$ of Zinc, and then one with $100 \%$ Zinc molecules and $0 \%$ PubChem. We also investigate the influence of model depth by pre-training a 6 layer model, named MoLFORMER-Base, on the complete Zinc and Pubchem dataset.All models are pre-trained with rotary embeddings and linear attention and then compared to MoLFORMER-XL. Identical learning rates, data splits, optimization, etc. are used for pre-training and fine-tuning. Tables 1 and 2 summarize these results. While MoLFORMER-XL performs better on average, we report two interesting observations. The first is that the model that is pre-trained on the second biggest data set, $100 \%$ Zinc, consistently performs worse than all other pre-trained models. A possible explanation for the poor performance of the model trained on only Zinc is due to the Zinc dataset consisting of a much smaller vocabulary than all other dataset combinations as well as much shorter molecules with little variance with respect to molecule length. The other point of interest is that when MoLFORMER-XL falls behind, it is only by a very small margin (See performance on ESOL, QM8, FreeSolv benchmarks in Table 2). Tables 1 and 2 further show that MoLFORMER-Base has a weaker performance than MoLFORMER-XL in majority of tasks, implying a deeper model helps in learning.</p>
<p>Fine-tuned versus Frozen Table 3 further summarizes the two remaining ablation experiments using the QM9 benchmark. For simplicity we observe that the fine-tuned ablation experiments achieves such a convincing win over the frozen experiments on all pre-training dataset sizes that we opted to only investigate fine-tuning for all other benchmarks. These results provide empirical insights onto the neural and data scaling behavior of MoLFORMER.</p>
<p>Position embeddings The positional embeddings ablation results are collected in Table 3, which show that MoLFORMER with Rotary embeddings and fine-tuning is behind the Absolute positional embedding model for the smaller datasets, but then wins as the dataset size passes 1 Billion molecules.</p>
<h2>Insights into MoLFORMER</h2>
<h2>Molecular Similarity Recovery</h2>
<p>Next, we analysed the correlation between pairwise similarities estimated using the Tanimoto distance, a popular measure of pairwise distance between chemicals, on the molecular fingerprints and those estimated using the Euclidean distance on the MoLFORMER-XL embeddings. We further looked into the correlation between the number of atoms in the maximum common subgraph of a pair of molecules with their corresponding euclidean distance in the embedding space for a set of random molecules picked from PubChem. The results are summarized in Table 4 and show that MoLFORMER-XL embeddings are better correlated with known molecule similarity measures when compared to ChemBERTa. These results are suggestive of MOLFORMER embeddings being informative of chemical structure similarity.</p>
<h2>Attention Analyses</h2>
<p>Finally, we inspect the average pooled attention matrices of MoLFORMER-XL to explore the chemical information embedded in them. For this purpose, we utilize the cosine similarities between attention values and the spatial distances between atoms within a molecule from the QM9 test set. Spatial distances are obtained from the corresponding energy-minimized geometries provided within QM9 benchmark ${ }^{28}$. MoLFORMER-XL is compared with a MoLFORMER variant trained with full attention and rotary embeddings on the entire PubChem+Zinc dataset. Note that the MoLFORMER models here are not fine-tuned for the QM9 dataset. The frozen MoLFORMER with full attention shows a much higher average MAE ( $\geq 12$ ) on QM9 downstream tasks, performance is particularly worse on internal energies ( U and $\mathrm{U}_{0}$ ), enthalpy (H), and free energy (G). We present attention results separately for three different categories of interatomic spatial distances: short ( $\leq 2 \AA$; that are mostly reflective of typical covalent bonds in the molecule, C-C single bond distance being $1.5 \AA$ ), medium ( $2-4 \AA$ ) and long ( $\geq 4 \AA$ ), and summarize</p>
<p>them in Table 3. Interestingly, attentions in MoLFORMER with linear or full attention (and rotary positional embeddings) show strong similarity with interatomic distances in both the short and medium categories, while revealing a weak (around 0.2 ) similarity with longer interatomic distances. This is an interesting observation, indicating that MoLFORMER is able to capture spatial relations between atomic tokens that are not necessarily neighbors in the SMILES sequence. The observed attentions in MoLFORMER-XL are slightly more in line with medium and long range distances, when compared to MoLFORMER with full attention. This observation suggests MoLFORMER-XL, with linear-attention, does in fact capture spatial relations between atoms more effectively.</p>
<p>Figure 3 further elaborates this point showing the average learned attention coefficients in an intermediate attention layer of MoLFORMER-XL with rotary positional embeddings. Attentions between different pairs of atomic tokens are compared to the corresponding covalent bond connectivity and 3D distances between atom pairs (complete attention matrices for the same molecules across all layers are shown in Figures 5 and 6 in SI). We chose two molecules from the QM9 test set whose attention values show a high cosine similarity with the medium range spatial distances for this visualization. Visual inspection indicates that an aggregation of heads on the intermediate rotary attention layer corresponds well to the covalent bonding pattern, while also capturing the signature of the spatial relations between non-bonded atoms within a molecule. These attention analysis results suggest that MoLFORMER-XL is able to recover molecular structural information from corresponding SMILES sequence to a significant extent. This capability likely stems from pre-training on a large corpus of chemical SMILES which also allows MoLFORMER-XL to learn fundamental properties of chemicals, including structural information and various downstream properties, ranging from quantum chemical to physiological. A similar observation has been reported in recent work on protein sequence modeling ${ }^{42,43}$. To our knowledge, this is the first confirmation that structural and diverse property information emerges in the representation learned by a chemical language model pre-trained on large-scale data.</p>
<h1>Conclusion</h1>
<p>In this work, we have explored the power of unsupervised large-scale pre-trained molecular language models at various molecular property prediction tasks. Unlike graphs, molecular languages such as SMILES do not explicitly encode molecular topology. However, with well-designed self-supervised training on a large-scale corpus and with an expressive architecture, such as a contextualized transformer-based language model with a linear attention mechanism, and a parallelized training protocol, our MoLFORMER can efficiently learn implicit rich structure-property relationship information.</p>
<p>Specifically, MoLFORMER outperforms existing graph-based baselines on a wide variety of molecular regression and classification benchmarks. To our knowledge, this is the first work that validates the power of large-scale self-supervised pre-trained molecular language models on predicting molecular properties across the entire range from quantum chemical to physiological. Further, by analysing the learned attentions, we show that MoLFORMER trained on SMILES sequences indeed is aware of interatomic relations within a molecule, even beyond the 2D topology. Finally, on the large-scale learning end, we showcased with MoLFORMER an efficient and environment-friendly use of computational resources, reducing the number of GPUs needed to perform the training by a factor of 60 (1000 vs. 16).</p>
<p>MolFORMER has immediate potential for faster in silico screening of molecules across diverse targets, which is important for material design and drug discovery applications with positive societal impact. However, it should be noted that misuse of such technology without a proper experimental and scientific validation in a wet lab can have harmful implications. Further, it has been shown that accurate property prediction models (for example., for predicting toxicity) along with generative models can be exploited for designing highly toxic molecules ${ }^{44}$. This highlights the need for a responsible framework around the use of these emerging powerful technologies. In addition, the present work calls for further exploration of the representational power of MoLFORMER in the context of its ability to learn structural molecular information directly from chemical language and can be extended beyond the small organic molecules studied in this work. Future work will also aim to improve MoLFORMER by employing larger models and larger training data, using improved and/or domain-specific self-supervised tasks, and using other string-based representations like SELFIES ${ }^{9}$.</p>
<h2>Methods</h2>
<h2>Model Details</h2>
<p>As we aim to train a large scale masked language model of chemical SMILES efficiently and effectively, while utilizing relatively limited hardware resources, we leveraged transformer-based neural nets ${ }^{23}$. Transformers process inputs through a series of blocks alternating between self-attention and feed-forward connections. Transformers encode the position in the sequence via a positional embedding, termed the absolute positional embedding. The input feature at a position $m$ is therefore concatenated with its corresponding absolute position embedding. Self-attention enables the network to construct complex representations that incorporate context from across the sequence. Attention mechanisms transform the features in the sequence into queries $(q)$, keys $(k)$, and value $(v)$ representations. These representations produce the output of the attention at position $m$</p>
<p>as follows:</p>
<p>$$
\text { Attention }<em n="1">{m}(Q, K, V)=\frac{\sum</em>
$$}^{N} \exp \left(\left\langle q_{m}, k_{n}\right\rangle\right) v_{n}}{\sum_{n=1}^{N} \exp \left(\left\langle q_{m}, k_{n}\right\rangle\right)</p>
<p>A well known computational bottlenecks of the vanilla transformer ${ }^{23}$ architecture is that the attention mechanism suffers from a quadratic computational cost with respect to the sequence length. Linear complexity attention models ${ }^{31,49}$ have tackled this issue utilizing kernel approximations and random feature approximations variants. This led us to design MoLFORMER that utilizes an encoder based on a transformer with linear attention ${ }^{31}$. MoLFORMER with linear attention consists of 12 layers, 12 attention heads per layer, and has a hidden state size of 768 . A Generalized Feature map ${ }^{31}$ for the linear attention was chosen (see SI Section A.1.1 for details).</p>
<p>As mentioned above, in a transformer architecture the dependency between tokens at different position of a (chemical) sequence is modeled under the supervision of position encoding. The seminal work of ${ }^{23}$ investigated absolute position embeddings to encode the position of a token in the sequence. More recent work ${ }^{46-48}$ showed that use of relative position embeddings between tokens results in improved performance. Rotary position embeddings were introduced in RoFormer ${ }^{27}$ as a means to enhance the relative encoding via position dependent rotations $R_{m}$ of the query and the keys at a position $m$. These rotations can be efficiently implemented as pointwise multiplications and do not result in a dramatic computational increase.</p>
<p>In order to leverage Rotary embeddings with linear transformers, the use of the following approximation was proposed in ${ }^{27}$ :</p>
<p>$$
\text { Attention }<em n="1">{m}(Q, K, V)=\frac{\sum</em>
$$}^{N}\left\langle R_{m} \phi\left(q_{m}\right), R_{n} \phi\left(k_{n}\right)\right\rangle v_{n}}{\sum_{n=1}^{N}\left\langle\phi\left(q_{m}\right), \phi\left(k_{n}\right)\right\rangle</p>
<p>where $Q, K, V$ are the query, key, and value respectively, and $\phi$ a random feature map.
After preliminary experimentation with this linear Roformer, we found it performed worse than its absolute position counterpart. We propose the following modification to Roformer that we found to train more gracefully (the training loss falls faster and lower) than the original Roformer, as well as observing better performance than the model using absolute embeddings:</p>
<p>$$
\text { Attention }<em n="1">{m}(Q, K, V)=\frac{\sum</em>
$$}^{N}\left\langle\phi\left(R_{m} q_{m}\right), \phi\left(R_{n} k_{n}\right)\right\rangle v_{n}}{\sum_{n=1}^{N}\left\langle\phi\left(R_{m} q_{m}\right), \phi\left(R_{n} k_{n}\right)\right\rangle</p>
<p>When compared with ${ }^{27}$ we rotate the original keys and queries instead of the transformed ones with the feature map $\phi$.</p>
<h1>Datasets and Tokenization</h1>
<p>We constructed several datasets for pre-training by combining the PubChem ${ }^{49}$ and ZINC $^{50}$ datasets with varying proportion from each. The PubChem dataset consists of 111 million molecules, while the much larger ZINC dataset contains over 1 billion molecules. To construct a vocabulary, we utilize the tokenizer from ${ }^{51}$. All molecules from both PubChem and ZINC are converted to a canonical format utilizing RDKit ${ }^{52}$ then tokenized. All unique tokens extracted from the resulting output gives us a vocabulary of 2357 tokens plus 5 special tokens, resulting in a total of 2362 vocabulary tokens which are used for all pre-trained models considered in this paper, irrespective of pre-training dataset size. In other words, all models have the same embedding capacity with a fixed vocabulary size. However, the total unique tokens that they are pre-trained on might only contain a subset of the model vocabulary capacity. The post tokenization sequence length of the molecules range from 1 to just over 2000 tokens. We decide to restrict the sequence length range from 1 token to 202 tokens, special tokens inclusive, to reduce computation time. Since over 99.4 percent of all molecules from our dataset contain less than 202 tokens we hypothesize that the removal of molecules with more than 202 tokens would be of minimal negative impact on pre-training.</p>
<h2>Large Scale Training and Parallelization</h2>
<p>For pre-training we use the masked language model method defined in ${ }^{30}$. Initially $15 \%$ of the tokens are selected for possible denoising. From that selection, $80 \%$ of the tokens will be randomly selected and replaced with the [MASK] token, $10 \%$ of the tokens will be randomly selected to be replaced with a random token, while the remaining $10 \%$ of the tokens will be unchanged. Training was performed for 4 epochs through the entire PubChem+ZINC dataset with a fixed learning rate of $1.6 \mathrm{e}^{-4}$ and a batch size of 1600 molecules per GPU on a total of 16 GPUs over 2 servers connected via Infiniband fabric. It should be noted that as the number of GPUs utilized increased we found an increase in learning rate was necessary up to a factor of 8 .</p>
<p>In order to scale our training to large datasets ( 1 Billion+ data points), we relied on adaptive bucketing of mini-batches by sequence length, as well as parallelization via distributed training (see Supplementary Information (SI) A for details). Using Linear attention and bucketing allowed us to reduce the number of GPUs needed from roughly 1000 for quadratic attention with no bucketing to 16 .</p>
<h1>Data availability</h1>
<p>Datasets used for model pre-training and finetuning on benchmark tasks are available at https://github.com/IBM/ molformer.</p>
<h2>Code availability</h2>
<p>Python codes for MoLFormer training and fine-tuning, and python notebooks for MoLFormer attention visualization, as well as instances of pre-trained models are available at https://github.com/IBM/molformer. For other enquiries contact the corresponding authors.</p>
<h1>Figures and Tables</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">BBBP</th>
<th style="text-align: left;">Tox21</th>
<th style="text-align: left;">ClinTox</th>
<th style="text-align: left;">HIV</th>
<th style="text-align: left;">BACE</th>
<th style="text-align: left;">SIDER</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Tasks</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">12</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">27</td>
</tr>
<tr>
<td style="text-align: left;">RF</td>
<td style="text-align: left;">71.4</td>
<td style="text-align: left;">76.9</td>
<td style="text-align: left;">71.3</td>
<td style="text-align: left;">78.1</td>
<td style="text-align: left;">$\mathbf{8 6 . 7}$</td>
<td style="text-align: left;">$\mathbf{6 8 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">SVM</td>
<td style="text-align: left;">72.9</td>
<td style="text-align: left;">$\mathbf{8 1 . 8}$</td>
<td style="text-align: left;">66.9</td>
<td style="text-align: left;">$\mathbf{7 9 . 2}$</td>
<td style="text-align: left;">86.2</td>
<td style="text-align: left;">68.2</td>
</tr>
<tr>
<td style="text-align: left;">MGCN $^{53}$</td>
<td style="text-align: left;">$\mathbf{8 5 . 0}$</td>
<td style="text-align: left;">70.7</td>
<td style="text-align: left;">63.4</td>
<td style="text-align: left;">73.8</td>
<td style="text-align: left;">73.4</td>
<td style="text-align: left;">55.2</td>
</tr>
<tr>
<td style="text-align: left;">D-MPNN $^{54}$</td>
<td style="text-align: left;">71.2</td>
<td style="text-align: left;">68.9</td>
<td style="text-align: left;">$\mathbf{9 0 . 5}$</td>
<td style="text-align: left;">75.0</td>
<td style="text-align: left;">85.3</td>
<td style="text-align: left;">63.2</td>
</tr>
<tr>
<td style="text-align: left;">DimeNet $^{37}$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">78.0</td>
<td style="text-align: left;">76.0</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">61.5</td>
</tr>
<tr>
<td style="text-align: left;">Hu, et al. ${ }^{32}$</td>
<td style="text-align: left;">70.8</td>
<td style="text-align: left;">78.7</td>
<td style="text-align: left;">78.9</td>
<td style="text-align: left;">80.2</td>
<td style="text-align: left;">85.9</td>
<td style="text-align: left;">65.2</td>
</tr>
<tr>
<td style="text-align: left;">N-Gram $^{33}$</td>
<td style="text-align: left;">91.2</td>
<td style="text-align: left;">76.9</td>
<td style="text-align: left;">85.5</td>
<td style="text-align: left;">$\mathbf{8 3 . 0}$</td>
<td style="text-align: left;">87.6</td>
<td style="text-align: left;">63.2</td>
</tr>
<tr>
<td style="text-align: left;">MolCLR $^{26}$</td>
<td style="text-align: left;">73.6</td>
<td style="text-align: left;">79.8</td>
<td style="text-align: left;">93.2</td>
<td style="text-align: left;">80.6</td>
<td style="text-align: left;">$\mathbf{8 9 . 0}$</td>
<td style="text-align: left;">68.0</td>
</tr>
<tr>
<td style="text-align: left;">GraphMVP-C $^{36}$</td>
<td style="text-align: left;">72.4</td>
<td style="text-align: left;">74.4</td>
<td style="text-align: left;">77.5</td>
<td style="text-align: left;">77.0</td>
<td style="text-align: left;">81.2</td>
<td style="text-align: left;">63.9</td>
</tr>
<tr>
<td style="text-align: left;">GeomGCL $^{36}$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$\mathbf{8 5 . 0}$</td>
<td style="text-align: left;">91.9</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">64.8</td>
</tr>
<tr>
<td style="text-align: left;">GEM $^{38}$</td>
<td style="text-align: left;">72.4</td>
<td style="text-align: left;">78.1</td>
<td style="text-align: left;">90.1</td>
<td style="text-align: left;">80.6</td>
<td style="text-align: left;">85.6</td>
<td style="text-align: left;">67.2</td>
</tr>
<tr>
<td style="text-align: left;">ChemBerta ${ }^{25}$</td>
<td style="text-align: left;">64.3</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">90.6</td>
<td style="text-align: left;">62.2</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">MOLFORMER-XL</td>
<td style="text-align: left;">$\mathbf{9 3 . 7}$</td>
<td style="text-align: left;">84.7</td>
<td style="text-align: left;">$\mathbf{9 4 . 8}$</td>
<td style="text-align: left;">82.2</td>
<td style="text-align: left;">88.21</td>
<td style="text-align: left;">$\mathbf{6 9 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 1</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">QM9</th>
<th style="text-align: center;">QM8</th>
<th style="text-align: center;">ESOL</th>
<th style="text-align: center;">FreeSolv</th>
<th style="text-align: center;">Lipophilicity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GC</td>
<td style="text-align: center;">4.3536</td>
<td style="text-align: center;">0.0148</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">1.40</td>
<td style="text-align: center;">0.655</td>
</tr>
<tr>
<td style="text-align: left;">A-FP</td>
<td style="text-align: center;">2.6355</td>
<td style="text-align: center;">0.0282</td>
<td style="text-align: center;">0.5030</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.578</td>
</tr>
<tr>
<td style="text-align: left;">MPNN</td>
<td style="text-align: center;">3.1898</td>
<td style="text-align: center;">0.0143</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">1.150</td>
<td style="text-align: center;">0.7190</td>
</tr>
<tr>
<td style="text-align: left;">MOLFORMER-XL</td>
<td style="text-align: center;">$\mathbf{1 . 5 8 9 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 1 0 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 7 8 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 3 0 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 2 8 9}$</td>
</tr>
</tbody>
</table>
<p>Table 2</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Distance-Category</th>
<th style="text-align: center;">Attention</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">7</th>
<th style="text-align: center;">9</th>
<th style="text-align: center;">11</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Short</td>
<td style="text-align: center;">Full ( $\checkmark$ Rotary)</td>
<td style="text-align: center;">$\mathbf{0 . 6 1 5}$</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;">$\mathbf{0 . 6 1 5}$</td>
<td style="text-align: center;">0.601</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 8}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Linear ( $\checkmark$ Rotary)</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.594</td>
</tr>
<tr>
<td style="text-align: left;">Medium</td>
<td style="text-align: center;">Full ( $\checkmark$ Rotary)</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">0.724</td>
<td style="text-align: center;">0.724</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">$\mathbf{0 . 7 2 7}$</td>
<td style="text-align: center;">0.727</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Linear ( $\checkmark$ Rotary)</td>
<td style="text-align: center;">$\mathbf{0 . 7 2 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 2 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 2 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 2 7}$</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Long</td>
<td style="text-align: center;">Full ( $\checkmark$ Rotary)</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.205</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.210</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Linear ( $\checkmark$ Rotary)</td>
<td style="text-align: center;">$\mathbf{0 . 2 1 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 1 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 1 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 1 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 0 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 1 0}$</td>
</tr>
</tbody>
</table>
<p>Table 3</p>
<p>Figure 1. Overview of MoLFormer pipeline. The transformer neural network based model is trained on the SMILES sequences corresponding to a large collection of chemical molecules from PubChem and Zinc, two public chemical databases in a self-supervised fashion. MOLFORMER was designed with an efficient linear attention mechanism and relative positional embeddings, with the goal of learning a meaningful and compressed representation of chemical molecules. This foundation model was then adopted to different downstream molecular property prediction tasks via fine-tuning on task-specific data. The representative power was further tested by recovering molecular similarity using the MoLFORMER encodings, as well as by analyzing the correspondence between the interatomic spatial distance and attention value for a given molecule.
Figure 2. (a) Training and (b) validation losses of our Linear attention MoLFORMER with rotary (relative) and absolute position embeddings on PubChem. We see that both rotary and absolute MoLFORMER have graceful training curves. Our Rotary Linear attention MoLFORMER leads to lower training and validation losses than MoLFORMER with absolute position embeddings.
Figure 3. Visualization of the learned attention map (using either full or linear attention) under rotary embedding and corresponding molecular structure (bond connectivity and 3D distance in Angstroms) for two random molecules: 'CC1 (C) C (C) (O) C1 (C) O' (a) and 'CC (C) C (C) (C) O' (b). The attention map (only tokens that map to constituent atoms are shown for clarity), comprised of the average-pooled heads of an intermediate attention layer, exhibits awareness of both covalent bond connectivity and interatomic long-range spatial relationship. The linear attention variant captures (encircled in green) the medium 3D range distance better in comparison to its counterpart.
Table 1. Comparison of fine-tuned MoLFORMER with existing supervised and pre-trained/self-supervised baselines on multiple classification benchmarks. Bold indicates the top-performing model. All models were evaluated by AUC-ROC on scaffold splits. Baseline performances are adopted from references ${ }^{25,26,36}$, '-' signifies that the values were not reported for the corresponding task. .
Table 2. Performance of fine-tuned MoLFORMER and other supervised GNN baselines on QM9, QM8, ESOL, FreeSolv, and Lipophilicity regression benchmarks. For QM9 and QM8, we report average MAE, while RSME is reported for the remaining tasks. Baseline performances are taken from references ${ }^{28,40}$. Bold indicates the top-performing model..
Table 3. Comparison of MoLFORMER models with respect to cosine similarity between the interatomic spatial distance map and the attention map, across three different distance categories for 7806 molecules from QM9 test set. Short, Medium, and Long distance categories are defined with interatomic distances in the range of $\leq 2,2-4$, and $4-10 \AA$, respectively. Bold indicates the top-performing model.</p>
<h1>Acknowledgements</h1>
<p>We thank IBM Research for supporting this work.</p>
<h2>Author information</h2>
<p>Contributions
All authors conceived the project, developed the MoLFormer framework, and designed experiments. J.R., B.B., V.C., and I.P. performed model training, fine-tuning, and inference experiments. I.P. and P.D. performed attention map analyses. All authors analysed the results and wrote the paper.</p>
<h2>Ethics declarations</h2>
<p>Competing interests
The authors declare no competing interests.</p>
<h1>References</h1>
<ol>
<li>Rogers, D. \&amp; Hahn, M. Extended-connectivity fingerprints. J. chemical information modeling 50, 742-754 (2010).</li>
<li>Rupp, M., Tkatchenko, A., Müller, K.-R. \&amp; Von Lilienfeld, O. A. Fast and accurate modeling of molecular atomization energies with machine learning. Phys. review letters 108, 058301 (2012).</li>
<li>Weininger, D. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. J. chemical information computer sciences 28, 31-36 (1988).</li>
<li>Goh, G. B., Hodas, N. O., Siegel, C. \&amp; Vishnu, A. Smiles2vec: An interpretable general-purpose deep neural network for predicting chemical properties. arXiv preprint arXiv:1712.02034 (2017).</li>
<li>Öztürk, H., Özgür, A. \&amp; Ozkirimli, E. Deepdta: deep drug-target binding affinity prediction. Bioinformatics 34, i821-i829 (2018).</li>
<li>Paul, A. et al. Chemixnet: Mixed dnn architectures for predicting chemical properties using multiple molecular representations. arXiv preprint arXiv:1811.08283 (2018).</li>
<li>Shin, B., Park, S., Kang, K. \&amp; Ho, J. C. Self-attention based molecule representation for predicting drug-target interaction. In Machine Learning for Healthcare Conference, 230-248 (PMLR, 2019).</li>
<li>Daylight Chemical Information Systems, I. Smarts ${ }^{\text {TM}}$ —a language for describing molecular patterns (2007).</li>
<li>Krenn, M., Häse, F., Nigam, A., Friederich, P. \&amp; Aspuru-Guzik, A. Self-referencing embedded strings (SELFIES): A 100\% robust molecular string representation. Mach. Learn. Sci. Technol. 1, 045024, DOI: 10.1088/2632-2153/aba947 (2020).</li>
<li>Gao, W., Fu, T., Sun, J. \&amp; Coley, C. W. Sample efficiency matters: A benchmark for practical molecular optimization. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (2022).</li>
<li>Jo, J., Kwak, B., Choi, H.-S. \&amp; Yoon, S. The message passing neural networks for chemical property prediction on smiles. Methods 179, 65-72 (2020). Interpretable machine learning in bioinformatics.</li>
<li>Duvenaud, D. et al. Convolutional networks on graphs for learning molecular fingerprints. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2, NIPS'15 (2015).</li>
<li>Defferrard, M., Bresson, X. \&amp; Vandergheynst, P. Convolutional neural networks on graphs with fast localized spectral filtering. Adv. neural information processing systems 29 (2016).</li>
<li>Kipf, T. N. \&amp; Welling, M. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (2017).</li>
<li>Li, Y., Tarlow, D., Brockschmidt, M. \&amp; Zemel, R. S. Gated graph sequence neural networks. In Bengio, Y. \&amp; LeCun, Y. (eds.) 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings (2016).</li>
<li>Veličković, P. et al. Graph attention networks. In International Conference on Learning Representations (2018).</li>
<li>Hamilton, W., Ying, Z. \&amp; Leskovec, J. Inductive representation learning on large graphs. Adv. neural information processing systems 30 (2017).</li>
<li>Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O. \&amp; Dahl, G. E. Neural message passing for quantum chemistry. In International conference on machine learning, 1263-1272 (PMLR, 2017).</li>
<li>Schlichtkrull, M. et al. Modeling relational data with graph convolutional networks. In European semantic web conference, 593-607 (Springer, 2018).</li>
<li>Liao, R., Zhao, Z., Urtasun, R. \&amp; Zemel, R. S. Lanczosnet: Multi-scale deep graph convolutional networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 (OpenReview.net, 2019).</li>
<li>Chen, P., Liu, W., Hsieh, C.-Y., Chen, G. \&amp; Zhang, S. Utilizing edge features in graph neural networks via variational information maximization. arXiv preprint arXiv:1906.05488 (2019).</li>
<li>Kirkpatrick, P. \&amp; Ellis, C. Chemical space. Nature 432, 823-824 (2004).</li>
<li>Vaswani, A. et al. Attention is all you need. Adv. neural information processing systems 30 (2017).</li>
<li>Bommasani, R. et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 DOI: 10.48550/ARXIV. 2108.07258 (2021).</li>
<li>
<p>Chithrananda, S., Grand, G. \&amp; Ramsundar, B. Chemberta: large-scale self-supervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885 (2020).</p>
</li>
<li>
<p>Wang, Y., Wang, J., Cao, Z. \&amp; Barati Farimani, A. Molecular contrastive learning of representations via graph neural networks. Nat. Mach. Intell. 4, 279-287 (2022).</p>
</li>
<li>Su, J., Lu, Y., Pan, S., Wen, B. \&amp; Liu, Y. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864 (2021).</li>
<li>Wu, Z. et al. Moleculenet: a benchmark for molecular machine learning. Chem. science 9, 513-530 (2018).</li>
<li>Liu, Y. et al. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).</li>
<li>Devlin, J., Chang, M.-W., Lee, K. \&amp; Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the NAACL: HLT, Vol 1 (2019).</li>
<li>Katharopoulos, A., Vyas, A., Pappas, N. \&amp; Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, 5156-5165 (PMLR, 2020).</li>
<li>$\mathrm{Hu}^{*}$, W. et al. Strategies for pre-training graph neural networks. In International Conference on Learning Representations (2020).</li>
<li>Liu, S., Demirel, M. F. \&amp; Liang, Y. N-gram graph: Simple unsupervised representation for graphs, with applications to molecules. In NeurIPS, 8464-8476 (2019).</li>
<li>Chen, T., Kornblith, S., Norouzi, M. \&amp; Hinton, G. A simple framework for contrastive learning of visual representations. In III, H. D. \&amp; Singh, A. (eds.) Proceedings of the 37th International Conference on Machine Learning, vol. 119 of Proceedings of Machine Learning Research, 1597-1607 (PMLR, 2020).</li>
<li>Oord, A. v. d., Li, Y. \&amp; Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).</li>
<li>Liu, S. et al. Pre-training molecular graph representation with 3d geometry. In International Conference on Learning Representations (2022).</li>
<li>Gasteiger, J., Groß, J. \&amp; Günnemann, S. Directional message passing for molecular graphs. In International Conference on Learning Representations (2020).</li>
<li>Fang, X. et al. Geometry-enhanced molecular representation learning for property prediction. Nat. Mach. Intell. 4, 127-134 (2022).</li>
<li>Altae-Tran, H., Ramsundar, B., Pappu, A. S. \&amp; Pande, V. Low data drug discovery with one-shot learning. ACS central science 3, 283-293 (2017).</li>
<li>Xiong, Z. et al. Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism. J. medicinal chemistry 63, 8749-8760 (2019).</li>
<li>Schütt, K. et al. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. In Guyon, I. et al. (eds.) Advances in Neural Information Processing Systems, vol. 30 (Curran Associates, Inc., 2017).</li>
<li>Rives, A. et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proc. Natl. Acad. Sci. USA 118, e2016239118, DOI: 10.1073/pnas. 2016239118 (2021).</li>
<li>Vig, J. et al. Bertology meets biology: Interpreting attention in protein language models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 (OpenReview.net, 2021).</li>
<li>Urbina, F., Lentzos, F., Invernizzi, C. \&amp; Ekins, S. Dual use of artificial-intelligence-powered drug discovery. Nat. Mach. Intell. 4, 189-191, DOI: 10.1038/s42256-022-00465-9 (2022).</li>
<li>Choromanski, K. M. et al. Rethinking attention with performers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 (OpenReview.net, 2021).</li>
<li>Shaw, P., Uszkoreit, J. \&amp; Vaswani, A. Self-attention with relative position representations. In NAACL-HLT, 464-468 (Association for Computational Linguistics, New Orleans, Louisiana, 2018).</li>
<li>Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR 21, 1-67 (2020).</li>
<li>Ke, G., He, D. \&amp; Liu, T.-Y. Rethinking positional encoding in language pre-training. In ICLR (2021).</li>
<li>Kim, S. et al. PubChem 2019 update: improved access to chemical data. Nucleic Acids Res. (2018).</li>
<li>Irwin, J. J. \&amp; Shoichet, B. K. ZINC-a free database of commercially available compounds for virtual screening. J. Chem. Inf. Model. 45, 177-182 (2005).</li>
<li>
<p>Schwaller, P. et al. Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction. ACS Cent. Sci. 5, 1572-1583, DOI: 10.1021/acscentsci.9b00576 (2019).</p>
</li>
<li>
<p>RDKit: Open-source cheminformatics. http://www.rdkit.org (2021). [Online; accessed 28-May-2021].</p>
</li>
<li>Lu, C. et al. Molecular property prediction: A multilevel quantum interactions modeling perspective. In AAAI, 1052-1060, DOI: 10.1609/aaai.v33i01.33011052 (AAAI Press, 2019).</li>
<li>Yang, K. et al. Analyzing learned molecular representations for property prediction. J. Chem. Inf. Model. 59, 3370-3388 (2019).</li>
<li>Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR (2020).</li>
<li>Beltagy, I., Peters, M. E. \&amp; Cohan, A. Longformer: The long-document transformer. arXiv:2004.05150 (2020).</li>
<li>Kitaev, N., Kaiser, L. \&amp; Levskaya, A. Reformer: The efficient transformer. In ICLR (2020).</li>
<li>Wang, S., Li, B. Z., Khabsa, M., Fang, H. \&amp; Ma, H. Linformer: Self-attention with linear complexity (2020). 2006.04768.</li>
<li>Bjerrum, E. J. Smiles enumeration as data augmentation for neural network modeling of molecules (2017). 1703.07076.</li>
<li>Coley, C. W., Barzilay, R., Green, W. H., Jaakkola, T. S. \&amp; Jensen, K. F. Convolutional embedding of attributed molecular graphs for physical property prediction. J. chemical information modeling 57, 1757-1772 (2017).</li>
<li>Morris, C. et al. Weisfeiler and leman go neural: Higher-order graph neural networks. In AAAI, vol. 33, 4602-4609 (2019).</li>
<li>Maron, H., Ben-Hamu, H., Serviansky, H. \&amp; Lipman, Y. Provably powerful graph networks. Adv. neural information processing systems 32 (2019).</li>
<li>Elnaggar, A. et al. Prottrans: towards cracking the language of lifes code through self-supervised deep learning and high performance computing. IEEE transactions on pattern analysis machine intelligence (2021).</li>
<li>Xue, D. et al. X-mol: large-scale pre-training for molecular understanding and diverse molecular analysis. bioRxiv DOI: 10.1101/2020.12.23.424259 (2020).</li>
<li>Wang, S., Guo, Y., Wang, Y., Sun, H. \&amp; Huang, J. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics, 429-436 (2019).</li>
<li>Kim, H., Lee, J., Ahn, S. \&amp; Lee, J. R. A merged molecular representation learning for molecular properties prediction with a web-based service. Sci. Reports 11, 1-9 (2021).</li>
<li>Irwin, R., Dimitriadis, S., He, J. \&amp; Bjerrum, E. J. Chemformer: a pre-trained transformer for computational chemistry. Mach. Learn. Sci. Technol. 3, 015022 (2022).</li>
<li>Rong, Y. et al. Self-supervised graph transformer on large-scale molecular data. Adv. Neural Inf. Process. Syst. 33, $12559-12571$ (2020).</li>
<li>Zhang, Z., Liu, Q., Wang, H., Lu, C. \&amp; Lee, C.-K. Motif-based graph self-supervised learning for molecular property prediction. Adv. Neural Inf. Process. Syst. 34, 15870-15882 (2021).</li>
<li>Wang, Y., Magar, R., Liang, C. \&amp; Barati Farimani, A. Improving molecular contrastive learning via faulty negative mitigation and decomposed fragment contrast. J. Chem. Inf. Model. (2022).</li>
<li>Zhu, J. et al. Dual-view molecule pre-training. arXiv preprint arXiv:2106.10234 (2021).</li>
<li>Stärk, H. et al. 3d infomax improves gnns for molecular property prediction. In International Conference on Machine Learning, 20479-20502 (PMLR, 2022).</li>
<li>You, Y. et al. Large batch optimization for deep learning: Training bert in 76 minutes. In International Conference on Learning Representations (2020).</li>
<li>Falcon, e. a., WA. Pytorch lightning. GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning 3 (2019).</li>
<li>Rupp, M., Tkatchenko, A., Müller, K.-R. \&amp; von Lilienfeld, O. A. Fast and accurate modeling of molecular atomization energies with machine learning. Phys. Rev. Lett. 108, 058301, DOI: 10.1103/PhysRevLett.108.058301 (2012).</li>
<li>Schütt, K. T., Arbabzadah, F., Chmiela, S., Müller, K. R. \&amp; Tkatchenko, A. Quantum-chemical insights from deep tensor neural networks. Nat. communications 8, 1-8 (2017).</li>
<li>Vignac, C., Loukas, A. \&amp; Frossard, P. Building powerful and equivariant graph neural networks with structural messagepassing. Adv. Neural Inf. Process. Syst. 33, 14143-14155 (2020).</li>
<li>van der Maaten, L. \&amp; Hinton, G. Visualizing data using t-sne. J. Mach. Learn. Res. 9, 2579-2605 (2008).</li>
</ol>
<h1>Extended data</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">BBBP</th>
<th style="text-align: center;">HIV</th>
<th style="text-align: center;">BACE</th>
<th style="text-align: center;">SIDER</th>
<th style="text-align: center;">Clintox</th>
<th style="text-align: center;">Tox21</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$10 \%$ ZINC $+10 \%$ PubChem</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">84.5</td>
</tr>
<tr>
<td style="text-align: left;">$10 \%$ ZINC $+100 \%$ PubChem</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">84.5</td>
</tr>
<tr>
<td style="text-align: left;">$100 \%$ ZINC</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">83.2</td>
</tr>
<tr>
<td style="text-align: left;">MOLFORMER-Base</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">43.2</td>
</tr>
<tr>
<td style="text-align: left;">MOLFORMER-XL</td>
<td style="text-align: center;">$\mathbf{9 3 . 7}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 2}$</td>
<td style="text-align: center;">$\mathbf{8 8 . 2}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 0}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 8}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 7}$</td>
</tr>
</tbody>
</table>
<p>Extended data Table 1. Comparison of MOLFORMER-XL with fine-tuned MOLFORMER models that are either of smaller size or pre-trained on smaller datasets on BBBP, HIV, Sider, Clintox, Tox21 and BACE classification benchmarks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">QM9</th>
<th style="text-align: center;">QM8</th>
<th style="text-align: center;">ESOL</th>
<th style="text-align: center;">FreeSolv</th>
<th style="text-align: center;">Lipophilicity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$10 \%$ Zinc $+10 \%$ Pub</td>
<td style="text-align: center;">1.7754</td>
<td style="text-align: center;">0.0108</td>
<td style="text-align: center;">0.3295</td>
<td style="text-align: center;">0.2221</td>
<td style="text-align: center;">0.5472</td>
</tr>
<tr>
<td style="text-align: left;">$10 \%$ Zinc $+100 \%$ Pub</td>
<td style="text-align: center;">1.9093</td>
<td style="text-align: center;">$\mathbf{0 . 0 1 0 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 7 7 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 0 5 0}$</td>
<td style="text-align: center;">0.5331</td>
</tr>
<tr>
<td style="text-align: left;">$100 \%$ Zinc</td>
<td style="text-align: center;">1.9403</td>
<td style="text-align: center;">0.0124</td>
<td style="text-align: center;">0.3023</td>
<td style="text-align: center;">0.2981</td>
<td style="text-align: center;">0.5440</td>
</tr>
<tr>
<td style="text-align: left;">MOLFORMER-Base</td>
<td style="text-align: center;">2.2500</td>
<td style="text-align: center;">0.0111</td>
<td style="text-align: center;">0.2798</td>
<td style="text-align: center;">0.2596</td>
<td style="text-align: center;">0.6492</td>
</tr>
<tr>
<td style="text-align: left;">MOLFORMER-XL</td>
<td style="text-align: center;">$\mathbf{1 . 5 9 8 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 1 0 2}$</td>
<td style="text-align: center;">0.2787</td>
<td style="text-align: center;">0.2308</td>
<td style="text-align: center;">$\mathbf{0 . 5 2 9 8}$</td>
</tr>
</tbody>
</table>
<p>Extended data Table 2. Performance comparison of fine-tuned MOLFORMER-XL with fine-tuned MOLFORMER models are either of smaller size or pre-trained on smaller datasets on QM9 (avg MAE), QM8 (avg MAE), ESOL (RMSE), FreeSolv (RMSE), and Lipophilicity (RMSE) regression benchmarks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Pre-training Data $\rightarrow$ <br> Dataset Size $\rightarrow$</th>
<th style="text-align: center;">QM9 Only</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PubChem Only</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PubChem+ZINC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Frozen</td>
<td style="text-align: center;">$111 \times 10^{3}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Frozen</td>
<td style="text-align: center;">Fine-tuned</td>
<td style="text-align: center;">Fine-tuned</td>
<td style="text-align: center;">Frozen</td>
<td style="text-align: center;">Fine-tuned</td>
<td style="text-align: center;">Fine-tuned</td>
</tr>
<tr>
<td style="text-align: center;">Measure $\downarrow$</td>
<td style="text-align: center;">$\times$ Rotary</td>
<td style="text-align: center;">$\times$ Rotary</td>
<td style="text-align: center;">$\checkmark$ Rotary</td>
<td style="text-align: center;">$\times$ Rotary</td>
<td style="text-align: center;">$\times$ Rotary</td>
<td style="text-align: center;">$\checkmark$ Rotary</td>
<td style="text-align: center;">$\times$ Rotary</td>
<td style="text-align: center;">$\times$ Rotary</td>
<td style="text-align: center;">$\checkmark$ Rotary</td>
</tr>
<tr>
<td style="text-align: center;">Avg MAE</td>
<td style="text-align: center;">8.3808</td>
<td style="text-align: center;">2.4621</td>
<td style="text-align: center;">2.6604</td>
<td style="text-align: center;">8.2600</td>
<td style="text-align: center;">2.9680</td>
<td style="text-align: center;">3.3990</td>
<td style="text-align: center;">2.5497</td>
<td style="text-align: center;">1.8620</td>
<td style="text-align: center;">1.5894</td>
</tr>
<tr>
<td style="text-align: center;">Avg std MAE</td>
<td style="text-align: center;">0.2390</td>
<td style="text-align: center;">0.0843</td>
<td style="text-align: center;">0.0937</td>
<td style="text-align: center;">0.2447</td>
<td style="text-align: center;">0.0801</td>
<td style="text-align: center;">0.1355</td>
<td style="text-align: center;">0.0978</td>
<td style="text-align: center;">0.0611</td>
<td style="text-align: center;">0.0567</td>
</tr>
</tbody>
</table>
<p>Extended data Table 3. Comparison of different MOLFORMER variants on QM9 test set, in terms of average MAE and average standard MAE. Variants considered are MOLFORMER pre-trained using QM9 only, PubChem only, and PubChem+ZINC dataset. The variants with and without fine-tuning on downstream task are compared, as well as models with, $(\checkmark)$ Rotary, and without ,( $\times$ )Rotary, rotary embeddings. Our best candidate variant (for Table 8) is chosen based on the average MAE (Mean Absolute Error) score, lower is better.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Correlation</th>
<th style="text-align: center;">ChemBERTa</th>
<th style="text-align: center;">MOLFORMER-XL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Fingerprint</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">$\mathbf{0 . 6 4}$</td>
</tr>
<tr>
<td style="text-align: left;">MCS</td>
<td style="text-align: center;">-0.44</td>
<td style="text-align: center;">$\mathbf{- 0 . 6 0}$</td>
</tr>
</tbody>
</table>
<p>Extended data Table 4. Correlation with structural similarity metrics on 10000 randomly selected pairs of molecules from the PubChem dataset. Reported correlations are between (1) the pairwise similarities estimated using molecular Fingerprints and those using MOLFORMER-XL (or ChemBERTa) embeddings and (2) the number of atoms in the maximum common subgraph (MCS) of two molecules and their corresponding Euclidean distance in the embedding space.</p>
<h1>Supplementary Information</h1>
<p>Supplementary Figs. 1-6, Discussion, and Tables 1-11.</p>
<h1>Supplementary Information</h1>
<h2>Related Work</h2>
<p>Large Scale Training of Language Model The recent advancement of transformer-based masked language models (MLMs) ${ }^{29,30}$ and prefix language models (PLMs) ${ }^{55}$ have shown remarkable performance on various natural language understanding tasks. Self-supervised pre-trained representation learning of sequences through MLMs randomly masks input tokens during training and predicts these masked tokens, whereas PLMs require adding task-specific text tags to the input sequences. These language models show substantial performance improvements on downstream tasks via increasing transformer models size and pretraining using large-scale data corpora. Recent efforts have addressed the resulting cost and memory challenges encountered due to scaling up models and data. One such effort is the linear-time attention transformers introduced in ${ }^{45,56-58}$ which address the quadratic memory challenges within the attention mechanism and allowing for more efficiency in training MLMs.</p>
<p>Molecular Representation Learning To represent molecules in vector space, traditional chemical fingerprints such as ECFP ${ }^{1}$, have been used. Deep neural nets were further trained on chemical fingerprints for supervised learning. Recurrent Neural Network (RNN) based models have been used for molecular representation learning using SMILES and other linear molecular annotations as inputs ${ }^{59}$. At the same time, graph convolutional networks have been used to learn the neural fingerprints of molecules ${ }^{12,60}$. Previous work ${ }^{18}$ implemented a single common framework to learn from graphs, referred to as a message passing framework, which computes node embeddings by aggregating neighborhood information during the message passing phase and computes a feature vector of the graph during the readout phase. Many attempts to extend GNNs have been made, which include variations of the original message passing concept to learn non-local effects; for instance, in ${ }^{40}$ an attention mechanism was introduced. One challenge faced by GNNs is achieving higher expressivity that can distinguish between two given graphs to that of the hierarchy of the Weisfeiler-Lehman (WL) graph isomorphism, while maintaining scalability. It has been shown that typical message passing models have limited expressiveness and are not better than the first WL test (1-WL) ${ }^{61}$. Powerful deep models that represent higher order interactions between graph nodes have been suggested ${ }^{61,62}$, but with a large increase in computational cost.</p>
<p>Molecular graphs can be further augmented with the 3D coordinates of atoms. Such augmentation is considered as privileged information due to the cost associated with deriving the 3D molecular geometry. To better model the spatial interactions among atoms the message passing framework was extended in ${ }^{18}$ to include pairwise interatomic distances as edge features when geometric information was available. More recently, variations of the message passing networks (MPNN) were proposed to better model the spatial interactions within molecules and increase the models expressive power, e.g., by using continuous filter convolutional layers (SchNet) ${ }^{41}$ or by using directional message passing (DimeNet) ${ }^{37}$ but at the cost of increased computational complexity. However, those models are not generalizable to settings where 3D structural information is not readily available and/or is expensive to compute (e.g. for larger molecules). Since the goal of this work is to learn a generalizable molecular representation from a large amount of unlabeled data without relying on expensive 3D information, we mainly focus on comparing the proposed MOLFORMER with existing supervised and un/self-supervised baselines that utilize different input representation (SMILES, graphs, fingerprints) and can be generalizable to a wide variety of tasks, from quantum mechanical to physiological.</p>
<p>Pre-trained Molecular Language and Graph Models The recent success of language representation models in downstream NLP tasks has inspired extending this paradigm to other domains. By combining the power of pre-training on large unlabeled corpus and contextual language models (LMs) using advanced neural nets, such as transformers, a domain-specific "language" embedding is obtained as the exclusive input for several downstream tasks.</p>
<p>Examples include understanding the language of life through advanced LMs trained on protein sequences. Here features extracted by LMs directly from single protein sequences reach state-of-the-art performance in downstream prediction tasks, even when those were used without evolutionary information ${ }^{42,43,63}$. Similar large-scale unsupervised pre-training on SMILES sequences have been explored for molecular property prediction ${ }^{25,64-67}$; however, those models did not attempt to predict a diverse range of molecular properties while exploiting the available chemical sequences at scale. Unsupervised/semi-supervised representation learning has been tested on molecular graphs as well ${ }^{26,33,68}$. A more recent line of work has leveraged the power of contrastive self-supervised pre-training using 2D graph topology and 3D conformal geometry ${ }^{36}$ (referred as GeomGCL), which showed performance improvement on molecular regression and classification tasks compared to prior pre-training baselines.</p>
<p>Previous work ${ }^{69}$ has further considered use of motif prediction during self-supervised pretraining on molecular graphs. Another study ${ }^{70}$ has also investigated the effect of including substructure information, as well as cheminformatics measures, in a molecular contrastive learning framework. References ${ }^{71,72}$ have shown the benefit of pre-training by maximizing information between different molecular views, e.g. 1D SMILES and 2D graph, or 2D graph and 3D geometry. Fang, et al. ${ }^{38}$ has also</p>
<p>explored the advantages of geometry-based pre-training as well as including higher-order information in learning through modeling the atom-bond-angle relations in the graph (referred as GEM).</p>
<h1>A Model and Methods</h1>
<h2>A. 1 MoLFORMER Model and Pre-Training Details</h2>
<p>In this section we include additional details and insights of MoLFORMER pre-training.</p>
<h2>A.1.1 Optimizer</h2>
<p>For optimization we used the Fused Lamb optimizer from ${ }^{73}$ as implemented via APEX due to Lamb's superior performance in several aspects of training. For example, learning rate warm ups were found to be unnecessary and training was found to be robust when large batch sizes were used. All other optimizers were unable to maintain their stability without large amounts of modification any time a training configuration needed to be changed.</p>
<h2>A.1.2 Linear Attention</h2>
<p>Preliminary experiments showed an acceptable balance between computation speed and minimal performance deficit when compared to the FAVOR ${ }^{45}$ feature map. Generalized Features are a simplification of the feature map in FAVOR ${ }^{45}$. The feature map size we settled on is 32 .</p>
<h2>A.1.3 Rotary versus Absolute position embeddings</h2>
<p>We show in Figure 1 that MoLFORMER with linear attention and rotary embeddings has a better validation loss than its absolute position counterpart. This observation lead us to adopt MoLFORMER with linear attention and rotary embedding throughout the paper.</p>
<h2>A. 2 Parallelization and Computing Environment</h2>
<p>All experiments were performed on a GPU cluster where each node contains either 8 NVIDIA Tesla V100 (32GB) or 8 Ampere A100 (40GB) GPUs connected via NVLink. The V100 nodes are equipped with dual 28-core (Intel Xeon Gold 6258R) CPUs, the A100 nodes are equipped with dual 64-core (AMD EPYC 7742) CPUs, and all nodes are connected by 2 non-blocking EDR InfiniBand (100Gbps) network adapters as well as 2 100Gbps Ethernet adapters. All nodes are installed with RHEL 8.3, CUDA 10.2, and cuDNN 7.5 .</p>
<p>Due to the size of the datasets utilized in pre-training, our training environment relies on the Distributed Data Parallel functions provided by Pytorch and Pytorch Lightning utilizing the NCCL backend. By utilizing RDMA to enable GPU-direct technology we were able to efficiently scale to multi-node multi-GPU training. Additionally, we utilized HuggingFace Datasets to localize the data onto the machines where pre-training took place to improve performance during pre-training. Our pretraining task consists of training on the full dataset to 4 epochs. Training a single epoch of just PubChem on a single NVIDIA V100 GPU would take approximately 60 hours. Utilizing Distributed Data Parallel, pre-training on the full PubChem dataset alone took approx. 22 hours on 16 NVIDIA V100 GPUs this averages to about 5.5 hours per epoch. The speed up achieved by parallelizing training to 16 GPUs gave us a factor of 10.9. Pre-training for 4 epochs on the combined PubChem+ZINC datasets took approx 208 hours on a 16 NVIDIA V100 GPUs which averages to about 52 hours of compute for a single epoch. All fine-tunning tasks were able to be performed on single GPUs (either V100 or A100) and completed in approx. 12 hours.</p>
<h2>A. 3 Memory Efficient Training with Adaptive Bucketing By Sequence Length</h2>
<p>We observed that the distribution of molecule lengths in our dataset centered around molecules that were less than 45 tokens long after tokenization. This fact coupled with large batch sizes increased the likelihood that padding tokens would overwhelm each batch and result in large amounts of computational waste. To address this problem we decided to break each minibatch into multiple buckets. This process is done on a batch by batch basis, i.e. on the fly, which means full dataset preprocessing does not take place. It should be noted that gathering of statistics for the full dataset did take place before training and buckets were defined by sequence interval length gathered from that process. The first bucket would contain SMILES strings of length 1 to 42 , the next bucket would be of size 43 to 66 , the third bucket would be of size 67 to 122 and finally the last bucket would be of size 123 to 202. Due to the length distribution of our dataset buckets 1 and 2 would always be present in all training steps while bucket 3 would be present for the majority of minibatches. Molecules that fell into bucket 4 appeared in most minibatches but would usually only represent a very small percentage of molecules found within the minibatch. With this information we decided to not utilize bucket 4 in the training procedure until it reached a threshold of 50 molecules thus preventing us from training on a bucket that consistently contained a very small amount of molecules which we believe aided training. Bucketing combined with gradient accumulation across buckets gave us stable training, maintained training randomization and reduced computational time needed compared to the traditional method of keeping GPU memory full at all times to maximize computation without consideration of the wasted computation that arises because of the large amount of padding tokens. To be</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 1. Training and validation losses of our Linear attention MOLFORMER with rotary (relative) and absolute position embeddings on PubChem and PubChem+ZINC ( $&gt;1$ billion data points) datasets. We see that both rotary and absolute MOLFORMER have graceful training curves. Our Rotary Linear attention MOLFORMER leads to lower training and validation losses than MoLFORMER with absolute position embeddings. This observation lead us to focus on MOLFORMER with rotary position embeddings.
more concrete, without bucketing on 1 V100 GPU and on PubChem only a single epoch would take approx. 1200 hours while with bucketing the same epoch only took around 60 hours giving adaptive bucketing a speedup of $20 \times$. A similar concept, namely micro-batching ${ }^{74}$ exists but we became aware of it after implementing our adaptive bucketing technique. We have not yet baselined the differences between our domain specific bucketing implementation towards molecular data against the generic micro-batching ${ }^{74}$. Using Linear attention and bucketing allowed us to reduce the number of GPUs needed for quadratic attention and no bucketing from roughly 1000 to 16 .</p>
<h1>A. 4 Pre-training Scaleout</h1>
<p>We give in Figure 2 the estimated training times of MoLFORMER as a function of the used GPUs in the parallelization.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 2. Estimated training times for our Linear attention MoLFORMER with rotary embeddings on a) PubChem and b) PubChem+ZINC datasets taken after 250 iterations. We see that training time decreases slightly sub-linearly as GPUs are added. Training time also scales approximately linearly as more data is added.</p>
<h1>B Fine-tuning MoLFORMER for Property Prediction</h1>
<p>During the fine-tuning process, where the MoLFORMER weights are not frozen, we experimented with different hyperparameters. In our experiments, we found that batch sizes of both 64 and 128 work best for the downstreams tasks. Also, among various learning rates, $3 \mathrm{e}-5$ seems to be the best fit for all the measures on QM9 dataset. We found that the beta values used by the optimzier were import and were set to beta1 equaling .9 and beta2 equaling .99 . We found the model to be very sensitive to the betas2 value during fine-tuning. The discriminator used was of a fixed size of 2 layers with a hidden size of 768 for all fine-tuning experiments.</p>
<p>For the frozen strategy where the embeddings from the MoLFORMER are fixed, we use a fully connected model to predict the properties. A hyperparameter sweep was performed for the frozen strategy using grid search and we randomly picked 25 different variations for each task. The best model with the lowest validation loss was picked for further analysis. The different values of the frozen strategy hyperparameters are summarized in the table below.</p>
<p>Table 1. Different Values of the hyperparameters for the Frozen Models</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: left;">Values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: left;">$0.001,0.0001,0.0005,0.00005$</td>
</tr>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: left;">$64,128,256$</td>
</tr>
<tr>
<td style="text-align: left;">Hidden Dimension</td>
<td style="text-align: left;">$64,128,256,512,1024$</td>
</tr>
<tr>
<td style="text-align: left;">Number of Layers</td>
<td style="text-align: left;">$2,3,4$</td>
</tr>
</tbody>
</table>
<h2>C Dataset, Vocabulary, and Property Units</h2>
<p>All our downstream evaluations are performed on tasks from the MoleculeNet dataset ${ }^{28}$. All the tasks mentioned in Table 2 use random splits as suggested in ${ }^{28}$, while those in Table 1 use scaffold splits as suggested in ${ }^{26}$. A brief description of the downstream datasets are in Tables 2 and 3. We refer the reader to ${ }^{28}$ for more details on the specific tasks.</p>
<p>We have observed that various related work in the past have used different units for quantitative analysis on QM9 dataset without explicitly stating the units, making it difficult to compare the relative performance of different methods. We have listed the units of the measures used in this paper in Table 6. We also give in Tables 4 and 5 the statistics of sequence length and vocabulary for the datasets considered in this work. While the vocabulary of each dataset varies the architecture of our models is identical for all experiments contained in this paper. The vocabulary for our models is defined by the union of the vocabularies of both PubChem and Zinc dataset.</p>            </div>
        </div>

    </div>
</body>
</html>
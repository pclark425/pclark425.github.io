<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9725 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9725</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9725</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-276885361</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.05142v1.pdf" target="_blank">RocketEval: Efficient Automated LLM Evaluation via Grading Checklist</a></p>
                <p><strong>Paper Abstract:</strong> Evaluating large language models (LLMs) in diverse and challenging scenarios is essential to align them with human preferences. To mitigate the prohibitive costs associated with human evaluations, utilizing a powerful LLM as a judge has emerged as a favored approach. Nevertheless, this methodology encounters several challenges, including substantial expenses, concerns regarding privacy and security, and reproducibility. In this paper, we propose a straightforward, replicable, and accurate automated evaluation method by leveraging a lightweight LLM as the judge, named RocketEval. Initially, we identify that the performance disparity between lightweight and powerful LLMs in evaluation tasks primarily stems from their ability to conduct comprehensive analyses, which is not easily enhanced through techniques such as chain-of-thought reasoning. By reframing the evaluation task as a multi-faceted Q&A using an instance-specific checklist, we demonstrate that the limited judgment accuracy of lightweight LLMs is largely attributes to high uncertainty and positional bias. To address these challenges, we introduce an automated evaluation process grounded in checklist grading, which is designed to accommodate a variety of scenarios and questions. This process encompasses the creation of checklists, the grading of these checklists by lightweight LLMs, and the reweighting of checklist items to align with the supervised annotations. Our experiments carried out on the automated evaluation benchmarks, MT-Bench and WildBench datasets, reveal that RocketEval, when using Gemma-2-2B as the judge, achieves a high correlation (0.965) with human preferences, which is comparable to GPT-4o. Moreover, RocketEval provides a cost reduction exceeding 50-fold for large-scale evaluation and comparison scenarios. Our code is available at https://github.com/Joinn99/RocketEval-ICLR .</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9725.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9725.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Capability gap (powerful vs lightweight judges)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance disparity between powerful and lightweight LLMs as evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that more capable (powerful) LLMs produce evaluation outputs that better align with human preferences, while lightweight LLMs tend to have lower agreement with human judgments when used as judges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General LLM evaluation (benchmarks: MT-BENCH, WILDBENCH)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Various (GPT-4o, Claude-3.5-Sonnet, Llama-3 series, Qwen series, Gemma-2-2B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Point-wise scoring: judges prompted to generate analysis (CoT) then output a 1–10 score; comparisons made across judges on the same benchmark data.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotations from MT-BENCH HUMAN JUDGMENTS and CHATBOT ARENA ELO RATINGS used as gold standard for agreement and ranking comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Percent agreement with human judgments and ranking correlations (Spearman/Kendall). Reported examples: human-human agreement ~64.7%–64.8; agreement between judges ranges from near human-to-human (~64.8%) down to lower than random (33.3% reported as lower bound).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using weaker/lightweight LLMs as judges reduces alignment with human preferences: they fail to reliably reproduce human judgments, yielding lower instance-level agreement and poorer list-level rankings; overall discriminatory power is degraded.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Figures/Tables show lightweight models (e.g., small Qwen/Llama variants) often have substantially lower agreement with MT-BENCH human annotations and produce skewed score distributions that cannot distinguish response quality.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Powerful proprietary judges (GPT-4o, Claude-3.5-Sonnet) show much higher alignment with human judgments; RocketEval with checklist allows some lightweight models (e.g., Gemma-2-2B) to achieve very high Spearman correlation (0.965) comparable to GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Introduction; 'HOW LIGHTWEIGHT LLMS PERFORM AS A JUDGE?' section; Figure 1; Table 2; Table 3</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RocketEval: Efficient Automated LLM Evaluation via Grading Checklist', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9725.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9725.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought ineffectiveness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limited impact of Chain-of-Thought (CoT) prompting on lightweight LLM judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper finds that prompting lightweight judges to generate internal CoT analyses does not significantly improve their agreement with humans, and in some cases the CoT process amplifies their weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General LLM evaluation (point-wise scoring with CoT analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Lightweight LLMs (various Qwen, Llama, Gemma sized models) and comparisons with powerful models (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Three scoring settings were compared: CoT (generate analysis then score), Direct (score without analysis), and CoT GPT-4o (provide GPT-4o's analysis to the lightweight judge and ask it to score).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Benchmark human annotations and CHATBOT ARENA ELO ratings for ranking ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Instance-level agreement and list-level Spearman correlation with GPT-4o/CHATBOT ARENA; results show negligible gains from a judge's own CoT compared to direct scoring, but large gains when lightweight judges are given GPT-4o analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>The self-generated CoT by lightweight judges does not compensate for weak comprehension and may produce unstable/less reliable judgments; CoT does not remedy inability to perform comprehensive analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Paper reports that lightweight models' own analyses failed to significantly improve ranking correlations, whereas replacing their analysis with GPT-4o's analysis significantly improved instance-level agreement and list-level correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Providing high-quality analyses from powerful LLMs (CoT GPT-4o) to lightweight judges substantially improves their evaluation performance — showing the analysis content, not mere CoT form, is the limiting factor.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>'WHERE DO LIGHTWEIGHT LLMS UNDERPERFORM AS A JUDGE?' section; Figure 3; related experiment descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RocketEval: Efficient Automated LLM Evaluation via Grading Checklist', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9725.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9725.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>High uncertainty (sampling disagreement)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>High decision uncertainty of lightweight LLM judges revealed by sampling disagreement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When checklist items are treated as independent binary questions and judges are sampled repeatedly, lightweight LLMs show high disagreement rates across samples, indicating unstable judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Checklist-style item judgments derived from WILDBENCH</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Qwen-2-1.5B, Qwen2.5 variants, Llama-3 series, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Repeated sampling on binary checklist item judgments ('Yes'/'No') to compute ratio of disagreement across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Checklist items correspond to evaluation criteria used in WILDBENCH and human annotations; disagreement is a model-internal stability measure compared to human consistency expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Ratio of disagreement across multiple sampling runs; numeric example: Qwen-2-1.5B exhibits disagreement ratio >50% across 3 sampling results (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>High uncertainty reduces reliability of final scores produced by lightweight judges: unstable decisions on checklist items translate to noisy analysis and degraded final judgments versus humans.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Qwen-2-1.5B having >50% disagreement across samples on checklist questions; skewed and indistinct score distributions from CoT scoring by lightweight models (Figure 10).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Larger/lightweight models in the Llama-3 series show comparatively better consistency. RocketEval's normalized scoring and independent checklist judgments reduce the impact of this uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>'WHY DO LIGHTWEIGHT LLMS NOT GOOD AT JUDGING?' section; Figure 4; Figure 10; related text</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RocketEval: Efficient Automated LLM Evaluation via Grading Checklist', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9725.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9725.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Positional bias in sequential analyses</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Positional (order) bias amplified in sequential chain-of-thought judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When checklist questions are posed in sequence (multi-turn dialogue), earlier judgments influence later ones, and lightweight models increasingly produce inconsistent answers as more previous items are present.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Checklist-item multi-turn dialog evaluation (WILDBENCH checklist items)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Various (Qwen series, Llama-3 series)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Transform item-level questions into multi-turn dialogue with the original checklist order; compare current-item judgments under different settings of previous answers.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human checklist-based evaluations implicitly assumed independent item judgments; paper compares model behavior to this ideal.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Ratio/proportion of inconsistencies as the number of previous questions increases (Figure 5). Observed pattern: inconsistency grows with number of prior items, smaller models show higher inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Sequential (CoT-style) analyses introduce position bias in lightweight judges, which alters judgments depending on history and reduces independence and reproducibility compared to human evaluators who are expected to judge items more stably.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Figure 5 shows that as previous questions accumulate, all models show increasing inconsistency; smaller models (e.g., Qwen-2-1.5B) show stronger amplification of uncertainty through sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Llama-3 series shows overall higher consistency than the Qwen-2 series; making item judgments independent (as RocketEval does) mitigates positional bias.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>'WHY DO LIGHTWEIGHT LLMS NOT GOOD AT JUDGING?' section; Figure 5</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RocketEval: Efficient Automated LLM Evaluation via Grading Checklist', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9725.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9725.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned judge limitations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Degradation in capabilities from fine-tuning lightweight judges for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuned judge models trained on evaluation data can better match human preferences but may lose other base-model capabilities, causing failures to understand complex instructions and harming evaluation robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Fine-tuned judge models for automated evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Prometheus-7B-v2.0 (example fine-tuned judge) and discussion of general fine-tuned judges</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Fine-tuned on high-quality evaluation data with rubrics; used to directly output judgments/scores.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human-annotated labels used as training signals; gold-standard annotations used for alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Reported instance-level and list-level agreement/correlations; Prometheus-7B-v2.0 example has 55.7% (Table 2 baseline), lower than best methods in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Fine-tuning can degrade base-model abilities (e.g., understanding complex instructions), leading to worse comprehension of queries and therefore poorer evaluation when confronted with hard or diverse inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Paper references prior work and reports that fine-tuned judge models may fail to correctly understand complex instructions and deteriorate subsequent evaluation performance (citing Huang et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Fine-tuned judges can be competitive and cost-effective compared to proprietary LLMs, and with careful tuning may perform well; RocketEval’s supervised weighting also outperforms some fine-tuned judges in ranking correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Introduction; 'Fine-Tuned Judge Models' paragraph; Table 2 (Prometheus-7B-v2.0 baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RocketEval: Efficient Automated LLM Evaluation via Grading Checklist', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9725.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9725.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RocketEval mitigation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Checklist-driven evaluation (RocketEval) that mitigates losses of LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RocketEval reframes evaluation as instance-specific checklist Q&A, uses independent item judgments with normalized scores, and (optionally) supervised reweighting to bring lightweight judges' outputs into alignment with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Automated LLM evaluation across benchmarks (MT-BENCH, WILDBENCH, ARENA-HARD, ALPACAEVAL)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Lightweight LLMs in experiments (e.g., Gemma-2-2B, Llama-3-8B, Mistral-Nemo) used as judges under RocketEval</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Checklist created by powerful LLM (GPT-4o) once per instance; lightweight judges evaluate each checklist item independently; conditional normalized scores computed from Yes/No probability logits; final aggregate via arithmetic mean or supervised predictor (Extremely Randomized Trees) with reweighting.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotations from MT-BENCH, CHATBOT ARENA ELO ratings, and WILDBENCH used to fit supervised predictors or to compute agreement/correlation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Improved instance-level agreement and list-level correlation. Reported example: Gemma-2-2B with RocketEval achieves Spearman correlation 0.965 with human preferences on WILDBENCH; many lightweight judges surpass their CoT baseline agreement (Table 2, Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>RocketEval identifies that what was lost in naive LLM-as-judge is the checklist-style comprehensive analysis, independence of item judgments, and stable certainty; RocketEval addresses those losses but at the cost of additional upfront checklist generation.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Before RocketEval, small models had agreement close to random; RocketEval improved small models to over 60% agreement and achieved comparable rankings to powerful judges (e.g., Mistral-Nemo Spearman 0.986 in some setups).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>RocketEval requires creation of instance-specific checklists (one-time cost using a powerful LLM) and extra per-instance item evaluations (but prefix caching reduces cost); supervised predictors can overfit when annotations are non-informative and require weight-factor adjustment.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Methodology section; 'CHECKLIST CREATION', 'CHECKLIST GRADING', 'SCORE PREDICTION'; Experiments: 'HUMAN AGREEMENT ON THE EVALUATION'; Table 2; Table 3</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RocketEval: Efficient Automated LLM Evaluation via Grading Checklist', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MT-BENCH <em>(Rating: 2)</em></li>
                <li>WildBench: Benchmarking LLMs with challenging tasks from real users in the wild <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>On the limitations of fine-tuned judge models for llm evaluation <em>(Rating: 2)</em></li>
                <li>Prometheus 2: An open source language model specialized in evaluating other language models <em>(Rating: 1)</em></li>
                <li>G-eval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9725",
    "paper_id": "paper-276885361",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "Capability gap (powerful vs lightweight judges)",
            "name_full": "Performance disparity between powerful and lightweight LLMs as evaluators",
            "brief_description": "The paper documents that more capable (powerful) LLMs produce evaluation outputs that better align with human preferences, while lightweight LLMs tend to have lower agreement with human judgments when used as judges.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "General LLM evaluation (benchmarks: MT-BENCH, WILDBENCH)",
            "llm_judge_model": "Various (GPT-4o, Claude-3.5-Sonnet, Llama-3 series, Qwen series, Gemma-2-2B, etc.)",
            "llm_judge_setup": "Point-wise scoring: judges prompted to generate analysis (CoT) then output a 1–10 score; comparisons made across judges on the same benchmark data.",
            "human_evaluation_setup": "Human annotations from MT-BENCH HUMAN JUDGMENTS and CHATBOT ARENA ELO RATINGS used as gold standard for agreement and ranking comparisons.",
            "agreement_metric": "Percent agreement with human judgments and ranking correlations (Spearman/Kendall). Reported examples: human-human agreement ~64.7%–64.8; agreement between judges ranges from near human-to-human (~64.8%) down to lower than random (33.3% reported as lower bound).",
            "losses_identified": "Using weaker/lightweight LLMs as judges reduces alignment with human preferences: they fail to reliably reproduce human judgments, yielding lower instance-level agreement and poorer list-level rankings; overall discriminatory power is degraded.",
            "examples_of_loss": "Figures/Tables show lightweight models (e.g., small Qwen/Llama variants) often have substantially lower agreement with MT-BENCH human annotations and produce skewed score distributions that cannot distinguish response quality.",
            "counterexamples_or_caveats": "Powerful proprietary judges (GPT-4o, Claude-3.5-Sonnet) show much higher alignment with human judgments; RocketEval with checklist allows some lightweight models (e.g., Gemma-2-2B) to achieve very high Spearman correlation (0.965) comparable to GPT-4o.",
            "paper_reference": "Introduction; 'HOW LIGHTWEIGHT LLMS PERFORM AS A JUDGE?' section; Figure 1; Table 2; Table 3",
            "uuid": "e9725.0",
            "source_info": {
                "paper_title": "RocketEval: Efficient Automated LLM Evaluation via Grading Checklist",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Chain-of-Thought ineffectiveness",
            "name_full": "Limited impact of Chain-of-Thought (CoT) prompting on lightweight LLM judges",
            "brief_description": "The paper finds that prompting lightweight judges to generate internal CoT analyses does not significantly improve their agreement with humans, and in some cases the CoT process amplifies their weaknesses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "General LLM evaluation (point-wise scoring with CoT analysis)",
            "llm_judge_model": "Lightweight LLMs (various Qwen, Llama, Gemma sized models) and comparisons with powerful models (GPT-4o)",
            "llm_judge_setup": "Three scoring settings were compared: CoT (generate analysis then score), Direct (score without analysis), and CoT GPT-4o (provide GPT-4o's analysis to the lightweight judge and ask it to score).",
            "human_evaluation_setup": "Benchmark human annotations and CHATBOT ARENA ELO ratings for ranking ground truth.",
            "agreement_metric": "Instance-level agreement and list-level Spearman correlation with GPT-4o/CHATBOT ARENA; results show negligible gains from a judge's own CoT compared to direct scoring, but large gains when lightweight judges are given GPT-4o analyses.",
            "losses_identified": "The self-generated CoT by lightweight judges does not compensate for weak comprehension and may produce unstable/less reliable judgments; CoT does not remedy inability to perform comprehensive analyses.",
            "examples_of_loss": "Paper reports that lightweight models' own analyses failed to significantly improve ranking correlations, whereas replacing their analysis with GPT-4o's analysis significantly improved instance-level agreement and list-level correlations.",
            "counterexamples_or_caveats": "Providing high-quality analyses from powerful LLMs (CoT GPT-4o) to lightweight judges substantially improves their evaluation performance — showing the analysis content, not mere CoT form, is the limiting factor.",
            "paper_reference": "'WHERE DO LIGHTWEIGHT LLMS UNDERPERFORM AS A JUDGE?' section; Figure 3; related experiment descriptions",
            "uuid": "e9725.1",
            "source_info": {
                "paper_title": "RocketEval: Efficient Automated LLM Evaluation via Grading Checklist",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "High uncertainty (sampling disagreement)",
            "name_full": "High decision uncertainty of lightweight LLM judges revealed by sampling disagreement",
            "brief_description": "When checklist items are treated as independent binary questions and judges are sampled repeatedly, lightweight LLMs show high disagreement rates across samples, indicating unstable judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Checklist-style item judgments derived from WILDBENCH",
            "llm_judge_model": "Qwen-2-1.5B, Qwen2.5 variants, Llama-3 series, etc.",
            "llm_judge_setup": "Repeated sampling on binary checklist item judgments ('Yes'/'No') to compute ratio of disagreement across samples.",
            "human_evaluation_setup": "Checklist items correspond to evaluation criteria used in WILDBENCH and human annotations; disagreement is a model-internal stability measure compared to human consistency expectations.",
            "agreement_metric": "Ratio of disagreement across multiple sampling runs; numeric example: Qwen-2-1.5B exhibits disagreement ratio &gt;50% across 3 sampling results (Figure 4).",
            "losses_identified": "High uncertainty reduces reliability of final scores produced by lightweight judges: unstable decisions on checklist items translate to noisy analysis and degraded final judgments versus humans.",
            "examples_of_loss": "Qwen-2-1.5B having &gt;50% disagreement across samples on checklist questions; skewed and indistinct score distributions from CoT scoring by lightweight models (Figure 10).",
            "counterexamples_or_caveats": "Larger/lightweight models in the Llama-3 series show comparatively better consistency. RocketEval's normalized scoring and independent checklist judgments reduce the impact of this uncertainty.",
            "paper_reference": "'WHY DO LIGHTWEIGHT LLMS NOT GOOD AT JUDGING?' section; Figure 4; Figure 10; related text",
            "uuid": "e9725.2",
            "source_info": {
                "paper_title": "RocketEval: Efficient Automated LLM Evaluation via Grading Checklist",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Positional bias in sequential analyses",
            "name_full": "Positional (order) bias amplified in sequential chain-of-thought judgments",
            "brief_description": "When checklist questions are posed in sequence (multi-turn dialogue), earlier judgments influence later ones, and lightweight models increasingly produce inconsistent answers as more previous items are present.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Checklist-item multi-turn dialog evaluation (WILDBENCH checklist items)",
            "llm_judge_model": "Various (Qwen series, Llama-3 series)",
            "llm_judge_setup": "Transform item-level questions into multi-turn dialogue with the original checklist order; compare current-item judgments under different settings of previous answers.",
            "human_evaluation_setup": "Human checklist-based evaluations implicitly assumed independent item judgments; paper compares model behavior to this ideal.",
            "agreement_metric": "Ratio/proportion of inconsistencies as the number of previous questions increases (Figure 5). Observed pattern: inconsistency grows with number of prior items, smaller models show higher inconsistency.",
            "losses_identified": "Sequential (CoT-style) analyses introduce position bias in lightweight judges, which alters judgments depending on history and reduces independence and reproducibility compared to human evaluators who are expected to judge items more stably.",
            "examples_of_loss": "Figure 5 shows that as previous questions accumulate, all models show increasing inconsistency; smaller models (e.g., Qwen-2-1.5B) show stronger amplification of uncertainty through sequence.",
            "counterexamples_or_caveats": "Llama-3 series shows overall higher consistency than the Qwen-2 series; making item judgments independent (as RocketEval does) mitigates positional bias.",
            "paper_reference": "'WHY DO LIGHTWEIGHT LLMS NOT GOOD AT JUDGING?' section; Figure 5",
            "uuid": "e9725.3",
            "source_info": {
                "paper_title": "RocketEval: Efficient Automated LLM Evaluation via Grading Checklist",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Fine-tuned judge limitations",
            "name_full": "Degradation in capabilities from fine-tuning lightweight judges for evaluation",
            "brief_description": "Fine-tuned judge models trained on evaluation data can better match human preferences but may lose other base-model capabilities, causing failures to understand complex instructions and harming evaluation robustness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Fine-tuned judge models for automated evaluation",
            "llm_judge_model": "Prometheus-7B-v2.0 (example fine-tuned judge) and discussion of general fine-tuned judges",
            "llm_judge_setup": "Fine-tuned on high-quality evaluation data with rubrics; used to directly output judgments/scores.",
            "human_evaluation_setup": "Human-annotated labels used as training signals; gold-standard annotations used for alignment.",
            "agreement_metric": "Reported instance-level and list-level agreement/correlations; Prometheus-7B-v2.0 example has 55.7% (Table 2 baseline), lower than best methods in some settings.",
            "losses_identified": "Fine-tuning can degrade base-model abilities (e.g., understanding complex instructions), leading to worse comprehension of queries and therefore poorer evaluation when confronted with hard or diverse inputs.",
            "examples_of_loss": "Paper references prior work and reports that fine-tuned judge models may fail to correctly understand complex instructions and deteriorate subsequent evaluation performance (citing Huang et al., 2024).",
            "counterexamples_or_caveats": "Fine-tuned judges can be competitive and cost-effective compared to proprietary LLMs, and with careful tuning may perform well; RocketEval’s supervised weighting also outperforms some fine-tuned judges in ranking correlations.",
            "paper_reference": "Introduction; 'Fine-Tuned Judge Models' paragraph; Table 2 (Prometheus-7B-v2.0 baseline)",
            "uuid": "e9725.4",
            "source_info": {
                "paper_title": "RocketEval: Efficient Automated LLM Evaluation via Grading Checklist",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RocketEval mitigation",
            "name_full": "Checklist-driven evaluation (RocketEval) that mitigates losses of LLM-as-a-judge",
            "brief_description": "RocketEval reframes evaluation as instance-specific checklist Q&A, uses independent item judgments with normalized scores, and (optionally) supervised reweighting to bring lightweight judges' outputs into alignment with human judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Automated LLM evaluation across benchmarks (MT-BENCH, WILDBENCH, ARENA-HARD, ALPACAEVAL)",
            "llm_judge_model": "Lightweight LLMs in experiments (e.g., Gemma-2-2B, Llama-3-8B, Mistral-Nemo) used as judges under RocketEval",
            "llm_judge_setup": "Checklist created by powerful LLM (GPT-4o) once per instance; lightweight judges evaluate each checklist item independently; conditional normalized scores computed from Yes/No probability logits; final aggregate via arithmetic mean or supervised predictor (Extremely Randomized Trees) with reweighting.",
            "human_evaluation_setup": "Human annotations from MT-BENCH, CHATBOT ARENA ELO ratings, and WILDBENCH used to fit supervised predictors or to compute agreement/correlation metrics.",
            "agreement_metric": "Improved instance-level agreement and list-level correlation. Reported example: Gemma-2-2B with RocketEval achieves Spearman correlation 0.965 with human preferences on WILDBENCH; many lightweight judges surpass their CoT baseline agreement (Table 2, Table 3).",
            "losses_identified": "RocketEval identifies that what was lost in naive LLM-as-judge is the checklist-style comprehensive analysis, independence of item judgments, and stable certainty; RocketEval addresses those losses but at the cost of additional upfront checklist generation.",
            "examples_of_loss": "Before RocketEval, small models had agreement close to random; RocketEval improved small models to over 60% agreement and achieved comparable rankings to powerful judges (e.g., Mistral-Nemo Spearman 0.986 in some setups).",
            "counterexamples_or_caveats": "RocketEval requires creation of instance-specific checklists (one-time cost using a powerful LLM) and extra per-instance item evaluations (but prefix caching reduces cost); supervised predictors can overfit when annotations are non-informative and require weight-factor adjustment.",
            "paper_reference": "Methodology section; 'CHECKLIST CREATION', 'CHECKLIST GRADING', 'SCORE PREDICTION'; Experiments: 'HUMAN AGREEMENT ON THE EVALUATION'; Table 2; Table 3",
            "uuid": "e9725.5",
            "source_info": {
                "paper_title": "RocketEval: Efficient Automated LLM Evaluation via Grading Checklist",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MT-BENCH",
            "rating": 2
        },
        {
            "paper_title": "WildBench: Benchmarking LLMs with challenging tasks from real users in the wild",
            "rating": 2,
            "sanitized_title": "wildbench_benchmarking_llms_with_challenging_tasks_from_real_users_in_the_wild"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "On the limitations of fine-tuned judge models for llm evaluation",
            "rating": 2,
            "sanitized_title": "on_the_limitations_of_finetuned_judge_models_for_llm_evaluation"
        },
        {
            "paper_title": "Prometheus 2: An open source language model specialized in evaluating other language models",
            "rating": 1,
            "sanitized_title": "prometheus_2_an_open_source_language_model_specialized_in_evaluating_other_language_models"
        },
        {
            "paper_title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 1,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        }
    ],
    "cost": 0.01416175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST</p>
<p>Tianjun Wei 
Wei Wen 
Tencent Youtu Lab</p>
<p>Ruizhi Qiao ruizhiqiao@tencent.com 
Tencent Youtu Lab</p>
<p>Xing Sun 
Tencent Youtu Lab</p>
<p>Jianghong Ma majianghong@hit.edu.cn 
Harbin Institute of Technology Shenzhen</p>
<p>City University of Hong Kong</p>
<p>ROCKETEVAL: EFFICIENT AUTOMATED LLM EVALUATION VIA GRADING CHECKLIST
F82CDA5E90BDA8BA2AACA2D34B291C9C
Evaluating large language models (LLMs) in diverse and challenging scenarios is essential to align them with human preferences.To mitigate the prohibitive costs associated with human evaluations, utilizing a powerful LLM as a judge has emerged as a favored approach.Nevertheless, this methodology encounters several challenges, including substantial expenses, concerns regarding privacy and security, and reproducibility.In this paper, we propose a straightforward, replicable, and accurate automated evaluation method by leveraging a lightweight LLM as the judge, named RocketEval.Initially, we identify that the performance disparity between lightweight and powerful LLMs in evaluation tasks primarily stems from their ability to conduct comprehensive analyses, which is not easily enhanced through techniques such as chain-of-thought reasoning.By reframing the evaluation task as a multi-faceted Q&amp;A using an instance-specific checklist, we demonstrate that the limited judgment accuracy of lightweight LLMs is largely attributes to high uncertainty and positional bias.To address these challenges, we introduce an automated evaluation process grounded in checklist grading, which is designed to accommodate a variety of scenarios and questions.This process encompasses the creation of checklists, the grading of these checklists by lightweight LLMs, and the reweighting of checklist items to align with the supervised annotations.Our experiments carried out on the automated evaluation benchmarks, MT-BENCH and WILDBENCH datasets, reveal that RocketEval, when using Gemma-2-2B as the judge, achieves a high correlation (0.965) with human preferences, which is comparable to GPT-4o.Moreover, RocketEval provides a cost reduction exceeding 50-fold for large-scale evaluation and comparison scenarios.Our code is available at https://github.com/Joinn99/RocketEval-ICLR.</p>
<p>INTRODUCTION</p>
<p>Why is automated LLM evaluation necessary?In recent years, the progress in large language models (LLMs) has been remarkable (Jiang et al., 2024a;Team et al., 2024;Yang et al., 2024), driven by continuous technological advancements.The rapid emergence of new models and techniques has broadened their applications, encompassing both general-purpose textual and visual LLMs, as well as those fine-tuned for specific tasks in various domains.These LLMs exhibit a range of capabilities and performances across different application scenarios.Therefore, evaluating their capabilities effectively has become crucial for guiding their development.Since most tasks performed by LLMs involve human interaction, human preferences are often considered the gold standard for LLM evaluation (Zheng et al., 2023).Currently, crowd-sourcing platforms like CHATBOT ARENA (Chiang et al., 2024) collect a significant number of human votes to evaluate LLMs.However, this approach relies on extensive and long-term human annotation, which is costly and challenging to reproduce and interpret (Ni et al., 2024).Considering the typical applications of LLM evaluation today, which indicates the judgments derived using the original Chain-of-thought (CoT) prompting, and "Ours" indicates the judgments derived using our proposed RocketEval framework.</p>
<p>include validating the effectiveness of LLM development and assisting users in selecting the bestperforming models for specific tasks, there is a growing demand for more efficient, reproducible, and interpretable evaluations of LLMs.This demand has led to the proposal of various methods (Wang et al., 2024b;Kim et al., 2024a) and benchmarks (Lin et al., 2025;Zheng et al., 2023) aimed at achieving reliable and efficient automated LLM evaluation.</p>
<p>Pros and cons of the existing automated LLM evaluation paths.Automated LLM evaluation methods can generally be categorized into three primary types:</p>
<p>Multiple-Choice Questions (MCQ) and Keyword Matching Benchmarks.These methods evaluate the accuracy of a model's responses by designing a series of closed-ended questions and comparing the model's answers to a predefined standard ground truth.Benchmarks constructed using this approach have proven effective for quickly assessing a model's capabilities across several tasks, such as reasoning (Zellers et al., 2019;Qiu et al., 2020), comprehension (Mihaylov et al., 2018;Liu et al., 2024), and knowledge retention (Hendrycks et al., 2021;Liang et al., 2023).However, the requirement for specific response formats limits the comprehensiveness of this evaluation method.</p>
<p>In practice, most LLM applications involve response styles that differ significantly from simple choices and keywords.</p>
<p>Notably, many open-ended tasks cannot be judged by a fixed ground truth, which strictly limits the applicability of this approach.</p>
<p>LLM-as-a-Judge.Early studies have attempted to adopt language models in automated evaluation (Zhang* et al., 2020;Yuan et al., 2021).Given the robust generalization capabilities of LLMs, employing a powerful LLM as an evaluator has emerged as a viable solution.This method typically involves prompting an LLM to serve as a judge, evaluating the responses from different LLMs to a set of well-designed queries.A crucial prerequisite for LLM-as-a-Judge is that the LLM must possess sufficient capability to fully comprehend the queries and discern the quality of different responses.Consequently, many existing benchmarks (Zheng et al., 2023;Dubois et al., 2023;Li et al., 2024a;Liu et al., 2023) tend to employ the most powerful proprietary LLMs, such as GPT-4o, as the judge.However, the use of these models for evaluation not only incurs high costs but also raises other issues such as reproducibility and data privacy.</p>
<p>Fine-Tuned Judge Models.These models are designed to address the limitations associated with powerful LLM-as-a-Judge approaches.Fine-tuned judge models, derived from lightweight base models, are trained with high-quality evaluation data to more closely align with human preferences (Jiang et al., 2024b;Zhu et al., 2025;Wang et al., 2024b;Kim et al., 2024a).Compared to proprietary LLMs, such fine-tuned judge models exhibit competitive evaluation capabilities in a more transparent and cost-effective manner.However, simply fine-tuning the model on evaluation task data may degrade other capabilities of the lightweight model, which are already weaker compared to those of powerful proprietary LLMs.This degradation can result in the model failing to correctly understand complex instructions in the queries, thereby deteriorating subsequent evaluation performance (Huang et al., 2024).Additionally, as the capabilities of the base models rapidly evolve and A {"Thought" "Answer": "Jan 9 2025"} B Thought: The 15th is 2025-1-9.</p>
<p>C {"Thought" "Answer": "2024-12-30"} Does the response provide detailed analysis?</p>
<p>Is the response correctly formatted as JSON?Does the response include answer "Jan 9 2025"? X  X X (Response) new data continuously emerges, these judge models may need iterative updates, leading to significant cost escalations and reproducibility issues.</p>
<p>RocketEval: Towards efficient automated LLM evaluation.Building on the aforementioned concepts, we find a pathway that synergizes the language modeling and human preference alignment capabilities of the most powerful LLMs with the evaluation efficiency of lightweight models.Specifically, we conduct a thorough analysis of the evaluation capabilities of various large models and observe the following:</p>
<p>1.The agreement between LLM judges and humans is significantly influenced by the inherent capabilities of the LLMs.More powerful LLMs tend to generate evaluation results that more accurately reflect human preferences.</p>
<ol>
<li>Prompt engineering techniques, such as Chain-of-Thought (CoT), exert minimal impact on the evaluation capabilities of the model, particularly when a lightweight LLM is employed as the judge.High uncertainty and positional bias during the decoding process are potential contributing factors to this phenomenon.</li>
</ol>
<p>Inspired by these observations, we introduce a novel evaluation framework named RocketEval, designed to meet the demands of evaluation scenarios that require high efficiency, low cost, alignment with human preferences, reproducibility, and interpretability.As illustrated in Figure 2, RocketEval operates through a three-stage framework to generate evaluations.Initially, an instance-level checklist is created, providing essential knowledge and critical focus areas to overcome the limitations of lightweight LLMs in constructing analyses.Subsequently, lightweight LLMs assess the quality of responses for each checklist item independently, resulting in a multifaceted and unbiased judgment.The normalized score of each judgment is then collected and aggregated to predict the final score, aiming to mitigate the uncertainty associated with lightweight LLMs.Considering the widespread availability of human annotations across various evaluation scenarios, we further introduce a supervised prediction process to align the scores from lightweight LLMs with these annotations.Experimental results demonstrate that RocketEval significantly enhances agreement with human judgments, achieving a remarkable Spearman correlation of 0.965 when utilizing the Gemma-2-2B model as the judge to rank test models.This offers a comparable solution to GPT-4o at only 2% of the evaluation cost in large-scale evaluation scenarios, rendering it suitable for performing efficient, reliable, and reproducible LLM evaluations.</p>
<p>HOW LIGHTWEIGHT LLMS PERFORM AS A JUDGE?</p>
<p>In this section, we verify the capability of lightweight LLMs in automated evaluation by conducting a series of experiments employing various LLM judges.</p>
<p>SETUP</p>
<p>We selected two benchmark datasets for our experiments:</p>
<p>• MT-BENCH (Zheng et al., 2023) is a classic benchmark that includes 80 multi-round queries from multiple domains and uses GPT-4 as the judge for multi-round evaluations.</p>
<p>• WILDBENCH (Lin et al., 2025) is a newly released benchmark containing 1,024 real-world user queries.WildBench first introduces a manually revised checklist as contextual information to guide the evaluation and uses GPT-4o as the judge.</p>
<p>Both benchmarks include pairwise comparison and point-wise scoring evaluation methods.Since the results of pairwise comparisons can be derived from comparing the scores derived from pointwise scoring, we focus on the point-wise scoring method.In point-wise setting, both benchmarks prompt the judge to first generate an analysis of the response, followed by a score in 1-10 as the final judgment.This can be seen as a chain-of-thought (CoT) (Wei et al., 2022) process, aimed at enhancing the ability of the LLM on the evaluation task that involves a reasoning process.</p>
<p>HOW DO LIGHTWEIGHT LLM JUDGES PERFORM?</p>
<p>First, we aim to understand the capabilities of different models when they serve as judges.A key metric for evaluating LLMs' evaluation capability in aligning user preferences is their agreement with humans.Here, we measure this agreement using MT-BENCH HUMAN JUDGMENTS (Zheng et al., 2023), which provides human-annotated results for MT-BENCH across six test models.Each sample includes a target query, responses from two LLMs, and human-annotated match results (including ties).We structure the scores obtained in a point-wise manner into the same format by comparing the scores of each pair of responses.Unlike the previous study (Kim et al., 2024a) that uses sampling decoding to derive results without ties, we follow the setting of Lin et al. ( 2025) and retain cases with scores difference smaller than 0.1 as ties.We then compared the agreement of different LLM judges with human annotations.The human-human agreement reported by Zheng et al. (2023) and the agreement between GPT-4o and human are listed as the baseline.As illustrated in Figure 1, agreement between LLMs and humans typically ranges from human-to-human agreement (64.8%) to lower than random outcomes (33.3%).This suggests that the existing evaluation method using CoT scoring imposes significant demands on the judges' abilities.The ability of LLMs, reflected by OPENLLM 2 (Fourrier et al., 2024) scores, significantly impacts its alignment with human preferences during evaluation, thereby complicating the process of performing efficient and reliable evaluations with lightweight LLMs.</p>
<p>WHERE DO LIGHTWEIGHT LLMS UNDERPERFORM AS A JUDGE?</p>
<p>In this part, our objective is to delve deeper and identify the key components that affect the judgment of lightweight LLMs when they serve as judges.We start by conducting an analysis of the two processes involved in evaluation: analysis generation and scoring.To determine whether a comprehensive analysis can boost or degrade the performance of judgment, we instruct the LLM judges to score the responses under three different settings:</p>
<p>• CoT: The original Chain-of-Thought style, generating the analysis and score step-by-step.</p>
<p>• Direct: The judge is prompted to skip the analysis step and score the response directly.</p>
<p>• CoT GPT-4o : We extract the analysis generated by GPT-4o to replace the part in judge model's output, then prompt the judge model to score the response accordingly.</p>
<p>Then, the scores of all responses are averaged to derive the score of the tested model for ranking.</p>
<p>For baselines, we derive the score predicted by GPT-4o and Claude-3.5-Sonnetas judges equipped with CoT.We use the CHATBOT ARENA ELO RATINGS (Hard-prompt English) (Chiang et al., 2024) as the ground-truth rankings of human preferences in these models.Figure 3 shows the scores of 12 test models and the Spearman correlation coefficient of ranking lists with GPT-4o when different LLMs served as judge.Also, we convert the scores into pairwise comparison results and compare the agreement of different LLM judges with strong baselines.The result is shown in Table 1.Compared to direct scoring, the process of lightweight models conducting their own analysis did not yield significant gains.However, utilizing analysis from powerful LLM as the context significantly improves the evaluation performance in terms of instance-level agreement and list-level correlations.This indicates that lightweight judges are capable of calibrating their judgments with</p>
<p>GPT-4o Analysis + Score</p>
<p>Qwen2-1.5B Qwen2-7B Qwen2-72B</p>
<p>Llama-3-8B Llama-3-70B GPT-4o Claude-3.5-Sonnetpowerful LLM when high-quality and comprehensive analysis is provided.In other words, the ability of such lightweight LLM judges is limited mostly due to the poor comprehension and analysis process when they dealing with hard queries and complicated responses.</p>
<p>WHY DO LIGHTWEIGHT LLMS NOT GOOD AT JUDGING?</p>
<p>In the evaluation scenario, conducting analysis can be viewed as a series of judgments on the target response.In order to explore the reasons why the lightweight model are struggling in the conducting analysis, we further conduct a fine-grained level experiments.Specifically, we transform the process of analyzing into a process of judging on a series of checklist questions.We treat each item in the checklist provided in WILDBENCH as a independent question, and prompt the LLM judge to make decisions.Since these questions can be viewed as binary choice questions (for example, "Does the response correctly identify..."), the judges are asked to simply output "Yes" or "No" for each question based on the content of the query and response.Then, we obtain judgments of different LLM judges on the checklist questions through repeated sampling, and calculate the ratio of disagreement in different sampling results among all checklist questions. Figure 4 shows that the ratio of disagreement varies significantly across different model sizes.Lightweight LLM such as Qwen-2-1.5B,shows an disagreement ratio exceeding 50% across 3 sampling results.This indicates a high uncertainty in making decisions on these checklist questions.When lightweight LLM judges conduct the CoT style analysis, this high uncertainty may lead to greater deviations in the final scoring results and degrade the performance.Although previous studies (Li et al., 2024b;Wang et al., 2024a) has demonstrated bias of response order in pairwise comparisons, bias in the analytical process remains unexplored.Therefore, we want to understand whether this form can affect the models' judgment results.We transform the item-level questions from the previous step into a multi-turn dialogue format, with the sequence consistent with the original checklist order.To show the impact of different judgment results, we set all previous judgments to "Yes" or "No" and then compare the disagreements in the current item's judgment results under the two settings.As shown in Figure 5, with an increase in the number of previous questions, the proportion of inconsistencies in all models shows a growing trend.Interestingly, the Llama-3 series show an overall higher consistency compared to the Qwen-2 series.Additionally, we notice a correlation between model size and consistency, with smaller models tending to produce higher inconsistency.This suggests that the uncertainty of lightweight LLMs is more likely to be amplified in the process of setting up sequential analyses with CoT, thus reducing confidence in automated evaluation.</p>
<p>Ratio of disagreement
Llama-3-70B Llama-3-8B Qwen2-72B Qwen2-7B Qwen2-1.5B</p>
<p>Ratio of disagreement
Llama3-70B Llama-3-8B Qwen2-72B Qwen2-7B Qwen2-1.5B
From the above analysis, we demonstrate that lightweight LLM judges exhibit high uncertainty and position bias, which can lead to difficulties in making reliable judgments.A feasible and efficient evaluation method should avoid the above defects, which inspired us to propose the RocketEval.</p>
<p>METHODOLOGY</p>
<p>In this section, based on the analysis on existing evaluation benchmarks, we introduce a new automated LLM evaluation framework named RocketEval.As shown in Figure 2, the entire RocketEval framework can be divided into three stages.First, we employ a powerful LLM such as GPT-4o to conduct meta-analysis and create a checklist for assessing user query.Subsequently, lightweight LLMs are employed to evaluate the checklist items for responses from each test model.Finally, the evaluations for each item are collected to derive the final score, which can be predicted using either an unsupervised arithmetic mean or a supervised predictor learned from annotations.</p>
<p>CHECKLIST CREATION</p>
<p>Evaluating LLM responses is challenging for both human evaluators and LLM judges.Human evaluations can be subjective, while LLM judges may struggle with query understanding, detailed analysis, and interpretation, especially when the lightweight LLMs are employed as the judge.Existing methods improve the accuracy of LLM judgments by providing additional information to the LLM Judge's analytical process, such as reference answers (Zheng et al., 2023;Dubois et al., 2023) and hand-crafted rubrics (Kim et al., 2024a).However, reference answers are of limited use in openended queries, while hand-crafted rubrics face a trade-off between labor cost, generalizability, and accuracy.Therefore, here we follow WILDBENCH (Lin et al., 2025) to create a instance-specific checklist, guiding judge LLMs in evaluation.The checklist items are expected to have the following characteristics: 1) Relevance to the topic of the query.2) Capability to effectively distinguish between different responses.3) Independence from each other, as independent questions can ensure a complete evaluation of the response's quality.Essentially, checklist questions can be considered as a distillation of knowledge from powerful LLMs to prompt the lightweight judges.A question like "Does the response include the correct reasoning steps/final answer as X?" can be helpful when lightweight LLMs are struggling to identify all key factors or solve the problem by itself.We employ GPT-4o as the checklist creator, with 5-10 questions created for each instance.This process only needs to be executed once, while the checklist can be leveraged by lightweight LLM judge to evaluate any number of responses.The prompts used are shown in Appendix A.1.</p>
<p>CHECKLIST GRADING</p>
<p>In Section 2.3, we examine the limitations of employing lightweight LLMs as judges, particularly the issues of high uncertainty and positional bias.To address these challenges, we propose the following two evaluation procedures when using lightweight LLMs as the judge.</p>
<p>Independent Checklist Item Judgment.To avoid interference from the judgments on other checklist questions, we prompt the LLM judge to evaluate each question in the checklist independently.While this method requires multiple queries per instance, the computational cost can be significantly reduced by leveraging prefix caching (Zheng et al., 2024) since they share the same prefix.</p>
<p>Normalized Score.Given the high uncertainty associated with lightweight LLMs, relying solely on binary outcomes such as "Yes" or "No" can result in significant errors in the final judgment.To mitigate this error, we introduce the conditional normalized score as the basis for judgment, which accounts for the certainty of the result.Assuming the probability of output token t from judge LLM parameterized by θ with context x is p θ (t|x), with the target query with context x and the corresponding response y.Then the conditional normalized score on checklist item c is defined as:
p(x, y, c) = p θ (Yes|x, y, c) p θ (Yes|x, y, c) + p θ (No|x, y, c) .(1)
In this manner, the judgments with low certainty have less significant impact on the final judgment.</p>
<p>SCORE PREDICTION</p>
<p>After obtaining all judgments for a single query-response instance, we can predict the final score by the normalized scores of all checklist items.Given the checklist c = [c 1 , c 2 , ..., c N ] ∈ C and the normalized scores p = [p(x, y, c 1 ), p(x, y, c 2 ), ..., p(x, y, c N )] ∈ P. The score predictor f : S → R can predict the score s by s = f (p).This methodology ensures a more reliable and accurate evaluation by addressing the inherent uncertainties and biases in lightweight LLM judgments.The score predictor can be any statistical method, hand-crafted rules, or machine learning models.For simplicity, here we use the arithmetic mean of all normalized scores as the score s unsup :
s unsup = N i=1 p(x, y, c i ).(2)
This method does not require additional data or effort to obtain the predictor.However, checklist items may have varying impacts on the final results.In many LLM evaluation scenarios, such as LLM development, the benchmark data often comes with annotations from humans and powerful LLMs that serve as the gold standard for evaluation.In this context, we can utilize these annotations to align our checklist judgment results with supervised learning.Specifically, we consider the judgments on the checklist items as features and the annotations as labels.Given a judgment set with N samples, with features P ∈ P N and labels r ∈ R N , a predictor f sup = min θ L(θ; P, r) can be derived by minimizing any loss function L. The predictor can be any classification or regression model, depending on the type of annotations.In this case, we select the Extremely Randomized Tree (Geurts et al., 2006) as estimator to learn a robust ensemble predictor with a limited number of annotations and unknown distributions of the judgment results.</p>
<p>Meanwhile, we have observed that queries may not always yield ideal separable annotations for predictor learning.Some queries may be too easy, too hard, or have vague descriptions, resulting in similar good or bad performance across all test models.This can cause significant performance degradation when learning a supervised predictor.Therefore, we propose a strategy to adjust the impact of the scores predicted by the supervised predictor based on the distribution of the annotations.Specifically, given the annotations r ∈ R |P| , we define the weight factor α r as
α r = ϵ − KL(P r ∥P ideal ) ϵ , ϵ = max X∼R N KL(X∥P ideal ),(3)
where ϵ is the maximum Kullback-Leibler (KL) divergence of any distribution X from the ideal distribution P ideal .In existing work (Kim et al., 2024a;Murugadoss et al., 2024), rubrics have been used as a reference in evaluation, including examples or standards for different rating levels.Therefore, we expect the annotated scores to be varied at different levels, providing the rubrics for the predictor.Hence, we use the uniform distribution across the score range (for example, 1-10) as
P ideal ∼ U N .
After deriving the weight factor α r and a fitted predictor f sup for each query, the final score assigned to the corresponding response is
s sup = (1 − α r )s unsup + α r f sup (p).(4)
This methodology ensures a more reliable and accurate evaluation by addressing the inherent uncertainties and biases in the supervised learning process.</p>
<p>EXPERIMENTS</p>
<p>In this section, we evaluate the effectiveness of the RocketEval framework for automated evaluation of LLMs.Initially, we analyze how well RocketEval aligns with human preferences at both the instance and the list levels.Next, we investigate the expenses associated with the evaluation procedure.Lastly, we perform an analysis of the checklist's content and its impact on the evaluation.</p>
<p>HUMAN AGREEMENT ON THE EVALUATION</p>
<p>The initial step involves comparing the proposed RocketEval framework with the traditional evaluation method, both with and without the inclusion of Chain of Thought (CoT).The idea of adapting checklist or aspect to enhance the robustness in LLM evaluation has been widely adopted in existing works (Lee et al., 2024;Zhou et al., 2024;Fu et al., 2024), where most of them come with a fixed human-curated lists to evaluate responses.To further validate the impact of the instance-level checklist, we introduce a baseline employing six fixed questions as the checklist.These questions, derived by analyzing the MT-BENCH (Zheng et al., 2023) evaluation prompt, encompass dimensions such as helpfulness, relevance, accuracy, depth, creativity, and detail.This baseline is henceforth referred to as the "Fixed".Details of the experimental setup are provided in Appendix A.3.</p>
<p>Instance-level Agreement.To measure the agreement ratio of different automated evaluation methods with MT-BENCH HUMAN JUDGMENTS, we adhere to the settings outlined in Section 2.2.As illustrated in Table 2, the proposed method consistently enhances agreement with human judgment across various LLMs acting as evaluators.Notably, the latest 7-12B parameter lightweight LLMs, such as Llama-3-8B and Mistral-Nemo, achieve over 64% agreement with human judgments and attaining a similar level of agreement as 70B open-source LLMs and human-to-human agreement.For smaller-sized LLMs, which initially exhibit agreement accuracy close to random choice, RocketEval significantly improves performance to over 60%, outperforming GPT-4.Conversely, the fixed checklist shows no notable performance enhancement and may even degrade performance.This suggests that checklist questions tailored to the specific query topic can better provide a knowledge context, thereby enhancing the performance of lightweight LLMs as judges.List-level Correlation.A critical objective of LLM evaluation is to compare the performance of LLM in specific or general scenarios, which makes the final ranking of LLM essential.Therefore, we conduct the experiment to compare the LLM rankings derived from different approaches.The detailed setup and results are elaborated in Appendix A.3.Table 3 presents the correlation coefficients of the score rankings on WILDBENCH with the CHATBOT ARENA ELO RATING (Hard Prompt -English).The table clearly indicates that RocketEval significantly improves the quality of by achieving a higher correlation with human annotations.Specifically, when Mistral-Nemo is used as a judge, the Spearman correlation reaches 0.986, surpassing GPT-4o.The smaller Gemma-2-2B also achieves a correlation of 0.965, surpassing Qwen2-72B and the fine-tuned judge model Prometheus-7B-v2.0 (Kim et al., 2024b).Furthermore, the supervised version of RocketEval achieves higher correlations compared to the unsupervised version.Compared to instance-level agreement, the supervised score data provides more significant improvements in list-level correlation.In addition to the agreement with humans, we compare the correlations between different lightweight LLM judges.As shown in Figure 6, RocketEval brings significant improvements in cross-judge correlation, especially on the lightweight LLMs.It suggests RocketEval exhibits high consistency and reliability in evaluation.</p>
<p>EVALUATION COST ESTIMATION</p>
<p>In this section, we analyze the costs of various evaluation methods when different LLMs serve as judges.For the purpose of this analysis, we assume that all responses required for evaluation are pre-generated, thereby excluding the inference costs for generating these responses.</p>
<p>In RocketEval, the evaluation process is comprised of two primary components: checklist generation and checklist grading.The supervised evaluation method incorporates an additional fitting and prediction process, whose costs are minimal since no LLM inference is involved.For proprietary LLMs serving as judges, we calculate the number of input and output tokens required for a single evaluation and derive the cost based on the official pricing2 .For open-source LLMs, we deploy them on NVIDIA RTX A5000 GPUs using vLLM (Kwon et al., 2023), and the cost is calculated based on the average execution time and the rental price of the GPU.We reference the pricing on RunPod3 at $0.36 per hour.To maximize efficiency, experiments are conducted in batch mode.</p>
<p>In practical evaluation scenarios, various LLMs, each with distinct tuning options, decoding settings, and prompting techniques, can yield numerous versions of responses.Consequently, evaluations on the same benchmark can be performed hundreds or even thousands of times across different models and their respective versions.We therefore compare the cost of LLM evaluation methods with different number of tests N .As shown in Table 4, the cost incurred for generating a checklist for each question is equivalent to the expense of running a single test using GPT-4o.Since the checklist generation is a one-time process, the cost of the checklist grading process becomes increasingly significant as the number of tests escalates.For instance, conducting 1000 tests on the WILDBENCH would incur a cost of $3400 when using GPT-4o as the evaluator, whereas employing Llama-3-8B would only require $71, with achieving a higher correlation with human preferences.</p>
<p>QUALITATIVE ANALYSIS</p>
<p>Checklist Statistical Analysis.To discern the distribution of checklist items, we extracted the relationships within the knowledge graphs associated with each checklist item, including the subject, predicate, and object, in a format consistent with the methodology introduced by Sun et al. (2024).</p>
<p>We conducted a statistical analysis of all predicates in the checklist items generated from WILD-BENCH (all predicates have been lemmatized).Further statistics are provided in Appendix A.5.As shown in Figure 7, the predicate distribution within checklist items reflects the intrinsic demands of various original questions.Predominant predicates such as "Include" and "Provide" underscore the necessity for comprehensive and supportive responses, ensuring that all pertinent information is considered.This is crucial for addressing the complex and multifaceted nature of questions across various general tasks.Meanwhile, predicates such as "Calculate" and "Specify" highlight the pre- cision required in quantitative and advisory responses.This distribution pattern not only guides the assessment of answer quality but also ensures that responses meet the specific criteria of each question type, thereby enhancing the overall reliability and applicability of the information conveyed.</p>
<p>Checklist Item Reweighting.Adjusting the weights of checklist items is crucial for accurately assessing the effectiveness of responses to the original question, especially when a gold standard is available.Initially, each checklist item was given equal weight, assuming an equal impact on the overall assessment.However, this does not accurately reflect the true importance of each item in validating the response's accuracy and completeness.As shown in Figure 8, Assigning the value of radial stress in item 1 (0.296) is critical, being a core part of the answer, significantly impacts subsequent calculations and the overall analysis.In contrast, item 3 (0.069), which calculates the inner radius, is fundamental but simple, less prone to error, and less critical than stress calculations, thus bearing a lower weight.By reweighting, we emphasize the steps that are the most crucial to the accurate responses, ensuring that the evaluation process is both rigorous and reflective of the actual importance of each component, which leads to more reliable and credible results, ultimately enhancing the overall validity and consistency with the gold standard of the assessment.</p>
<p>CONCLUSION</p>
<p>In this paper, we introduce RocketEval, an innovative evaluation framework that uses lightweight LLMs to achieve high efficiency, low cost, interpretability, and alignment with human preferences.By reframing the evaluation task as a multi-faceted Q&amp;A using instance-specific checklists, we addressed the challenges of high uncertainty and positional bias inherent in lightweight LLMs.Our method demonstrated a high correlation with human preferences, achieving a Spearman correlation of 0.965 with the Gemma-2-2B model, comparable to GPT-4o, but at a fraction of the cost.This significant cost reduction makes RocketEval a feasible solution for large-scale evaluation and comparison scenarios.</p>
<p>A APPENDIX A.1 PROMPTS USED IN ROCKETEVAL</p>
<p>Prompts for Checklist Creation.The following prompt is designed to generate an instance-level checklist.</p>
<p>Prompt for Checklist Creation</p>
<h1>Instruction</h1>
<p>You are an helpful assistant who identifies and summarizes key factors in large → language models (LLMs) evaluation to help humans evaluate LLMs → efficiently.</p>
<p>Feed any query into different LLMs, I will get various responses.I need to → know in quick whether these responses follows the instructions and → answers the question in the user query better.</p>
<p>I'll provide you with a user query.Your task is to identify those key factors → that will affect my judgment and summarize them into a list to improve → the efficiency of my evaluation.Prompts for Response Judgment.To keep the consistency with the previous work, we adopt the prompt in Lin et al. ( 2025) and make several modifications to adapt to different settings.</p>
<h1>Conversation between</h1>
<p>• For MT-BENCH, to keep consistent with the original paper which utilizes the reference answer to guide the judgment, we replace the part of the checklist with the reference answer and modify the prompt correspondingly.</p>
<p>• For other benchmarks that utilize pairwise comparison, we use the same prompt as WILD-BENCH, and create the checklist using GPT-4o for each instance in those benchmarks.</p>
<p>• When prompting LLM to output the score directly without conducting analysis, we modify the output format part in the prompt and change the score range from 1-10 to 0-9.This aims to avoid the ambiguity of scoring 1 or 10 when only the first token is captured.</p>
<p>The prompts with different versions of the modification are listed as follows.</p>
<p>Prompts for Response Judgment</p>
<h1>Instruction</h1>
<p>You are an expert evaluator.Your task is to evaluate the quality of the → responses generated by AI models.</p>
<p>We will provide you with the user query and an AI-generated responses.You should first read the user query and the conversation history carefully for → analyzing the task, and then evaluate the quality of the responses → based on and rules provided below.</p>
<h1>Conversation between User and AI ## History</h1>
<p>&lt;|begin _ of _ history|&gt; {history} &lt;|end _ of _ history|&gt; ## Current User Query &lt;|begin _ of _ query|&gt; {user _ query} &lt;|end _ of _ query|&gt; Checklist disabled ## Reference Response &lt;|begin _ of _ reference _ response|&gt; {ref _ answer} &lt;|end _ of _ reference _ response|&gt; ## AI Response &lt;|begin _ of _ response|&gt; {model _ output} &lt;|end _ of _ response|&gt; # Evaluation</p>
<p>Checklist disabled</p>
<h2>Rules You should first compare the AI response and reference response based on → your analysis of the user queries and the conversation history, → and then provide your assessment by scoring the AI response.</h2>
<p>Checklist enabled</p>
<h2>Checklist &lt;|begin _ of _ checklist|&gt; {checklist} &lt;|end _ of _ checklist|&gt;</h2>
<p>Please use this checklist to guide your evaluation, but do not limit your → assessment to the checklist.</p>
<h2>Rules</h2>
<p>You should compare the above response based on your analysis of the user → queries and the conversation history.</p>
<p>You should first write down your analysis and the checklist that you used → for the evaluation, and then provide your assessment according to → the checklist.</p>
<p>CoT enabled</p>
<p>The scores are in the range of 1~10, where 1 means the response is very → poor and 10 means the response is perfect.</p>
<p>Here are more detailed criteria for the scores:</p>
<p>-Score 1~2: The response is very poor and does not make sense at all.</p>
<p>-Score 3~4: The response is poor and does help user solve the problem in → a meaningful way.</p>
<p>-Score 5~6: The response is fair but has some issues (e.g., factual</p>
<p>→ errors, hallucinations, missing key information).</p>
<p>-Score 7~8: The response is good enough but could be improved in some → ways.</p>
<p>-Score 9~10: The response is perfect and provides helpful information → that can help user solve the problem.</p>
<h2>Output Format First, please output your analysis for the model response, and then → summarize your assessment to two aspects: "strengths" and " → weaknesses"; Finally, please write down your rating for the → assessment.</h2>
<p>Please provide your evaluation results in the following json format by → filling in the placeholders in []:</p>
<p>''' { "strengths": "[analysis for the strengths of the response]", "weaknesses": "[analysis for the weaknesses of the response]", "score": "[1~10]" } '''</p>
<p>CoT disabled</p>
<p>The scores are in the range of 0~9, where 0 means the response is very → poor and 9 means the response is perfect.</p>
<p>Here are more detailed criteria for the scores:</p>
<p>-Score 0~1: The response is very poor and does not make sense at all.-Score 2~3: The response is poor and does help user solve the problem in → a meaningful way.</p>
<p>-Score 4~5: The response is fair but has some issues (e.g., factual → errors, hallucinations, missing key information).</p>
<p>-Score 6~7: The response is good enough but could be improved in some → ways.</p>
<p>-Score 8~9: The response is perfect and provides helpful information → that can help user solve the problem.</p>
<h2>Output Format</h2>
<p>Please output the score directly as a digit from 0-9.Do not output other → text.</p>
<p>Your score:</p>
<p>Prompts for Checklist Grading.The prompt for grading checklist items is shown below.</p>
<p>Prompts for Response Judgment</p>
<p>A.2 DATASETS AND BASELINES</p>
<p>In addition to the results reported by Zheng et al. (2023) on instance-level agreement and Lin et al. ( 2025) on list-level correlation, we add more baselines with the following setup:</p>
<p>• GPT-4o: We replace GPT-4 with GPT-4o as judge and rerun the experiments using the publicly available code provided by Zheng et al. (2023).For WILDBENCH, we use the judgment of GPT-4o provided by Lin et al. (2025) as the baseline results.All baseline results are produced following the template of scoring from WILDBENCH to keep the consistency.</p>
<p>• Prometheus-2: We add the state-of-the-art fine-tuned judge model Prometheus-7B-v2.0 (Kim et al., 2024b) as the baseline.This model introduces custom scoring criteria and rubrics to conduct the evaluation.For simplicity, we use the rubrics in WILDBENCH (Lin et al., 2025), and set the criteria as "The response is in high quality and provides correct, relevant, and helpful information that focuses on the user query.".The complete prompt is shown below.</p>
<p>Prompts for Prometheus-2 judge</p>
<h3>Task Description: An instruction (might include an Input inside it), a response to evaluate, a → reference answer that gets a score of 5, and a score rubric representing → a evaluation criteria are given.</h3>
<ol>
<li>
<p>Write a detailed feedback that assess the quality of the response strictly → based on the given score rubric, not evaluating in general.</p>
</li>
<li>
<p>After writing a feedback, write a score that is an integer between 1 and 5.</p>
</li>
</ol>
<p>→ You should refer to the score rubric.</p>
<ol>
<li>The output format should look as follows: "Feedback: (write a feedback for → criteria) [RESULT] (an integer number between 1 and 5)" → hallucinations, missing key information).</li>
</ol>
<p>Score 4: The response is good enough but could be improved in some ways.Score 5: The response is perfect and provides helpful information that can help → user solve the problem.</p>
<h3>Feedback:</h3>
<p>A.3 DETAILS OF EXPERIMENTS ON LIST-LEVEL CORRELATION</p>
<p>To comprehensively assess the performance of RocketEval, we conduct the experiments by adding two additional benchmark datasets, ALPACAEVAL (Dubois et al., 2023) and ARENA-HARD (Li et al., 2024a).The statistics of all benchmarks are listed in Table 5.</p>
<p>A.3.1 EXPERIMENTAL SETUP</p>
<p>We draw inspiration from the study by Lin et al. (2025), where 12 models4 were selected for a correlation analysis between their rankings in the CHATBOT ARENA ELO RATING (Hard prompts-English) and WILDBENCH.However, upon closer inspection, we notice that some of the chosen models have overlapping elo rating confidence intervals, which may compromise the reliability of the correlation results.To address this issue, we revise the model selection to ensure that there is no overlap in the 95% CI.Similarly, when selecting test models for other benchmarks, we take into account the availability of model responses in the official released data and the elo ratings of the test models.The elo ratings of the selected models are presented in Figure 9.</p>
<p>A.3.2 RESULTS</p>
<p>Score Distribution.We visualize the distribution of the scores from all instances in a single benchmark.Figure 10 shows when lightweight LLMs are employed as the judge and using CoT to grade the responses, the distribution of scores are highly skewed, which indicates the poor ability of such models in distinguish the responses with different qualities.Meanwhile, the scores graded by the same judge under RocketEval are highly distinguishable and close to the distribution of GPT-4o.Also, the supervised scorer shows the strong ability to reduce the score deviation, making the score distribution closer to the ideal distribution like GPT-4o.This suggests the proposed method can provide valuable context information via checklist and guide the LLM to give differential judgment.</p>
<p>Elo Ratings.Given that the scores assigned by the same LLM judge can be utilized to establish pairwise comparisons between different test model responses, we follow the approach of (Chiang et al., 2024) by introducing a match simulator and importing all pairwise comparison results from an LLM judge.Specifically, we convert the scores of two responses to determine the winner, with considering differences in scores smaller than 0.1 as ties.For each pair of test models, we derive the fields.Conversely, common predicates like "explain" and "describe" are universally applicable, serving general verification objectives across all question types.</p>
<p>These predicate keywords augment the verification process by imparting explicit and goal-oriented directives.Specifically, exclusive keywords concentrate on criteria that are specific to each task type, ensuring a detailed and contextually relevant assessment.Meanwhile, common keywords ensure uniformity and exhaustiveness in the evaluation of responses.This dual strategy ensures that each answer is evaluated comprehensively and from multiple perspectives, thereby significantly enhancing the efficacy of the verification process.</p>
<p>Checklist Item Reweighting Analysis.Here we present more reweighted case examples under a variety of tasks.As shown in Figure 14, the checklist generated for Coding &amp; Debugging tasks ensures the complete generation of the code for the target problem.After reweighting, there is a greater emphasis on critical steps within the code, such as the structures of the forward, backward, and loss function in a neural network.For Planning &amp; Reasoning tasks, reweighting makes key reasoning items more prominent.For example, As shown in Figure 17, based on the symptoms described in the problem, the response needs to deduce diabetic ketoacidosis (DKA) to provide a correct and reasonable treatment in subsequent answers.For other Creative Tasks, see Figure 15, and for Information/Advice seeking tasks, see Figure 16.By reweighting checklist items, we can reasonably focus on critical steps or results across different types of tasks, ensuring effectiveness when using lightweight LLM as judges.</p>
<p>Figure 1 :
1
Figure 1: Agreements with MT-BENCH HUMAN JUDGMENTS with different LLM 1 judges."CoT" indicates the judgments derived using the original Chain-of-thought (CoT) prompting, and "Ours" indicates the judgments derived using our proposed RocketEval framework.</p>
<p>Figure 3 :
3
Figure 3: WILDBENCH scores predicted by different LLM judges and the ranking correlation with GPT-4o.</p>
<p>Figure 4 :
4
Figure 4: Ratio of disagreement on WILDBENCH with different number of sampling times.</p>
<p>Figure 5 :
5
Figure 5: Ratio of disagreement on WILDBENCH with checklist items in different positions.</p>
<p>Figure 6 :
6
Figure 6: Spearman correlation (in percentage) of test model rankings on WILDBENCH across different LLM judges.</p>
<p>Figure 8 :
8
Figure 8: Visualization of checklist item reweighting.(Default weight is the reciprocal of the number of checklists)</p>
<p>expert evaluator.Your task is to evaluate the quality of the → responses generated by AI models.We will provide you with the user query and an AI-generated responses.You should first read the user query and the conversation history carefully for → analyzing the task, and then evaluate the quality of the responses by → answer the question provided below.given question based on the conversation history and the AI → response.You can only answer 'Yes' or 'No'.Your answer (Yes/No):</p>
<p>Figure 12 :Figure 13 :
1213
Figure 12: Distribution of Subjects in checklist items which generated from WILDBENCH.</p>
<p>Figure 14 :
14
Figure 14: Visualization of checklist item reweighting in Coding &amp; Debugging.</p>
<p>Illustration of RocketEval framework for automated LLM evaluation.The framework consists of three components: Checklist Creation, Checklist Grading and Score Prediction.
Checklist GenerationJudgmentQuery with HistoryChecklist 🤖📊74👤 👤 🤖🤖🤖 🤖75 3 62 99 80 729.7 6.1🤖 🤖Responses👤9/10 Annotations 📝0 82 55 0 82 55 0 82 55 0 82 55 5 97 302.3 Score AlignmentFigure 2:</p>
<p>Table 1 :
1
Agreements of different judges on WILDBENCH.
Agreement with GPT-4oDirectCoTCoTGPT-4oClaude-3.5-Sonnet60.7%Qwen2-1.5B Qwen2-7B Qwen2-72B Llama-3-8B Llama-3-70B40.1% 43.1% 61.0% 46.7% 57.2%36.4% 45.0% 61.4% 48.4% 59.3%70.3% 76.6% 74.0% 74.0% 74.9%Agreement with Claude-3.5-SonnetGPT-4o60.7%Qwen2-1.5B Qwen2-7B Qwen2-72B Llama-3-8B Llama-3-70B40.6% 42.8% 61.5% 48.2% 58.3%36.4% 45.3% 62.3% 49.2% 62.0%58.2% 60.3% 62.9% 61.0% 63.4%</p>
<p>Table 2 :
2
Agreement ratios of different LLM judges with MT-BENCH HUMAN JUDGMENTS.
MethodCoT Direct Fixed Ours (Unsup.) Ours (Sup.)BaselineGPT-4 (Pairwise): 65.8% Human-to-human: 64.7% Prometheus-7B-v2.0: 55.7%GPT-4 (Single): 59.6% GPT-4o: 66.6%Llama-3-70B Qwen2-72B60.4% 64.8% 63.6% 64.0%------Mistral-Nemo Llama-3-8B Qwen2-7B Mistral-7B-v0.357.8% 53.0% 53.1% 55.6% 51.4% 46.9% 54.7% 47.6% 42.1% 55.3% 54.4% 42.8%63.2% 63.8% 58.6% 58.8%64.2% 62.9% 59.8% 58.3%Phi-3-mini-4k Qwen2.5-3B Llama-3.2-3B Gemma-2-2B InternLM2.5-1.8B 29.2% 22.3% 38.4% 52.8% 33.8% 42.1% 49.8% 52.2% 35.6% 53.2% 26.6% 54.1% 37.9% 39.2% 43.9%61.2% 57.4% 58.6% 57.9% 51.7%60.9% 58.7% 58.8% 57.3% 48.4%Qwen2.5-1.5B Qwen2-1.5B Llama-3.2-1B Qwen2.5-0.5B31.1% 25.1% 46.5% 30.4% 21.7% 41.0% 32.6% 22.5% 36.5% 24.9% 25.1% 40.8%60.7% 56.2% 33.6% 54.3%60.2% 55.6% 41.9% 50.9%</p>
<p>Table 3 :
3
Correlation of ranking with CHATBOT ARENA ELO RATING (Hard prompts-English) on WILDBENCH dataset."Kend."and "Spea."are denoted as Kendall's Tau and Spearman correlation coefficient respectively.
MethodCoTDirectFixedOurs (Unsup.)Ours (Sup.)CoefficientKend.Spea.Kend.Spea.Kend.Spea.Kend.Spea.Kend.Spea.GPT-4o Prometheus-7B-v2.00.909 0.8480.979 0.949----------------Llama-3-70B Qwen2-72B0.909 0.8480.979 0.9580.787 0.8480.923 0.958------------Mistral-Nemo Llama-3-8B Qwen2-7B Mistral-7B-v0.30.870 0.818 0.788 0.7270.956 0.923 0.909 0.8950.788 0.848 0.545 0.5450.909 0.944 0.727 0.6990.879 0.879 0.636 0.5150.958 0.972 0.804 0.6780.939 0.909 0.758 0.7580.986 0.979 0.895 0.8740.939 0.909 0.818 0.8180.986 0.979 0.930 0.930Phi-3-mini-4k Qwen2.5-3B Llama-3.2-3B Gemma-2-2B InternLM2.5-1.8B0.424 0.697 0.606 0.636 0.1210.587 0.839 0.797 0.818 0.1750.576 0.848 0.848 0.758 0.2730.762 0.951 0.951 0.888 0.3570.182 0.727 0.818 0.727 0.2730.273 0.895 0.937 0.867 0.4270.788 0.848 0.848 0.879 0.5760.916 0.944 0.944 0.965 0.7480.848 0.848 0.848 0.879 0.6060.958 0.944 0.944 0.965 0.769Qwen2.5-1.5B Qwen2-1.5B Llama-3.2-1B Qwen2.5-0.5B0.394 -0.061 0.091 -0.2120.517 0.035 0.133 -0.3150.273 0.303 0.152 0.4240.364 0.420 0.182 0.5030.606 -0.061 -0.273 0.3940.727 -0.014 -0.357 0.5870.818 0.455 -0.273 0.6670.923 0.622 -0.357 0.8110.848 0.667 0.697 0.7580.944 0.825 0.846 0.895</p>
<p>Table 4 :
4
Evaluation cost on WILDBENCH with different LLM judges.
MethodLLM JudgeDeploying EnvironmentPriceUsage for Single TestExtra CostTotal Cost for N Tests N=10 N=100 N=1000CoTGPT-4o (20240806) GPT-4o-mini (20240718)proprietaryI/O: $1.25 / $5.00 / 1M Tokens I/O: $0.075 / $0.30 / 1M TokensI/O: 1.84M/220k tokensN/A$34.0 $2.00$340 $20.0$3400 $200Llama-3-70BAWQ4 x A5000$1.44 / hours3760s$15.1$125$1224RocketEvalLlama-3-8B Gemma-2-2B1 x A5000 1 x A5000$0.36 / hours $0.36 / hours685s 248s$2.87<em>$3.55 $3.12$9.72 $5.35$71.4 $27.7Qwen2.5-1.5B1 x A5000$0.36 / hours165s$3.04$4.52$19.4</em>The checklist generation process on WILDBENCH consumes 1.38M input tokens and 228k output tokens on GPT-4o.</p>
<p>Your question should be concise and include any necessary key content and → information (such as keywords, formats, correct counts and values) in → the user query or expected to be shown in responses.Your questions → should not only consider evaluating the reference response, but all → possible responses.Avoid creating duplicate, cumbersome or vague → questions.For example, you should ask "Is this response contain the → correct answer ..." instead of "Is this response's answer correct?".Ask → fewer questions by aggregating questions with repeated contexts into → one question.
1. {{question1}}2. {{question2}}...'''User and AI&lt;|begin _ of _ history|&gt;{history}&lt;|end _ of _ history|&gt;## Current User Query&lt;|begin _ of _ query|&gt;{user _ query}&lt;|end _ of _ query|&gt;## Reference Response&lt;|begin _ of _ reference _ response|&gt;{reference _ response}&lt;|end _ of _ reference _ response|&gt;# TaskGiven the above information, I need you to create a binary question list, so→ that I can perform an efficient and accurate evaluation through→ answering several questions.## Output FormatPlease provide your outputs in the following markdown format by filling in the→ placeholders in {{}}:'''</p>
<p>Score 2: The response is poor and does help user solve the problem in a → meaningful way.Score 3: The response is fair but has some issues (e.g., factual errors,
4. Please do not generate any other opening, closing, and explanations.###The instruction to evaluate:{user _ query}###Response to evaluate:{model _ output}###Reference Answer (Score 5):{ref _ answer}###Score Rubrics:
[The response is in high quality and provides correct, relevant, and helpful → information that focuses on the user query.]Score 1: The response is very poor and does not make sense at all.</p>
<p>Table 5 :
5
Statistics of benchmark datasets.
Dataset#Instances Turns QueryLen PromptLenMT-BENCH (Zheng et al., 2023)160<em>1-2202.21123.4WILDBENCH (Lin et al., 2025)10241-5978.53402.1ALPACAEVAL (Dubois et al., 2023)8051164.9164.9ARENA-HARD (Li et al., 2024a)5001406.4406.4</em>Here we treat each 2-turn dialogues as 2 instances.</p>
<p>Visualization of checklist item reweighting in Information/Advice seeking.
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amp;O+&amp;2DQGSURYLGHWKHYDOXH" 'RHVWKHUHVSRQVHRXWOLQHDSSURSULDWHOLQHVRIWUHDWPHQW IRU'.$FRYHULQJIOXLGUHVXVFLWDWLRQLQVXOLQWKHUDS\ HOHFWURO\WHPDQDJHPHQWDQGDFLGRVLVFRUUHFWLRQ" 'RHVWKHUHVSRQVHDGGUHVVWKHSDWLHQWVLQIHFWHGIRRWXOFHU DVDSRVVLEOHVHFRQGDU\LQIHFWLRQDQGVXJJHVWDSSURSULDWH LQWHUYHQWLRQVOLNHDQWLELRWLFV" 'RHVWKHUHVSRQVHPHQWLRQDGGLWLRQDOSDWLHQW HGXFDWLRQRQGLDEHWHVPDQDJHPHQWDQGIRRWFDUHDVSDUWRI WKHWUHDWPHQWSODQRUSUHYHQWLRQRIIXWXUHHSLVRGHV" ,VWKHUHVSRQVHFOHDUFRQFLVHDQGDSSURSULDWHO\GHWDLOHG LQDGGUHVVLQJHDFKSDUWRIWKHRULJLQDOTXHU\" Figure 16: 'RHVWKHUHVSRQVHFRUUHFWO\LGHQWLI\WKHPRVWOLNHO\:ULWHDQLQWULFDWHP\VWHU\VWRU\ LQFOXGLQJGLDORJXHDERXWWKHPXUGHURI /XWKHU:DWVRQ$OOILYHVXVSHFWV &amp;KULVWLDQ9DXJKQ6HOLQD0RUDQR7HG 0RVOH\%OLL3D[URWWDQG6DPQWKD :LOOLDPVDUHJDWKHUHGDW/XWKHUVROG UHVLGHQFH2QHRIWKHPLVWKHPXUGHUHU DQGWKH\KDYHWRILQGRXWZKRLWLV7KH RQO\WKLQJWKH\NQRZLVWKDW/XWKHUZDV VXIIRFDWHG(DFKQLJKWRQHRIWKH VXVSHFWVJHWVNLOOHGE\WKHPXUGHUHUDQG WKHSUHVVXUHWRVROYHWKHFDVH LQFUHDVHV $\HDUROGPDQZLWKW\SHGLDEHWHV PHOOLWXVRQLQVXOLQSUHVHQWVWR('ZLWK IHYHUFRXJKYRPLWLQJDQGDEGRPLQDO SDLQ([DPLQDWLRQUHYHDOVWDFK\SQLDGU\ PXFRVDGHFUHDVHGVNLQWXUJRUD WHPSHUDWXUHRI&amp;DQGLQIHFWHG XOFHULQWKHOHIWIRRW,QYHVWLJDWLRQV VKRZ53*RIPJGO1DPPROO .PPROO%LFDUERQDWH PPRO/&amp;KORULGHPPRO/XUHD PJGO $:KDWLVWKHPRVWOLNHO\GLDJQRVLV" %0HQWLRQRQHWHVWWRSURYHWKH GLDJQRVLV &amp;:KDWLVWKHYDOXHRIDQLRQJDSLQ WKLVSDWLHQW" ':KDWDUHWKHOLQHVRIWUHDWPHQW"Figure 15: Visualization of checklist item reweighting in Creative Tasks.
Figure 17: Visualization of checklist item reweighting in Planning &amp; Reasoning.</p>
<p>https://openai.com/api/pricing/
https://www.runpod.io/pricing
The selected models are gpt-4-turbo-2024-04-09, claude-3-opus-20240229, Meta-Llama-3-70B-Instruct, Qwen1.5-72B-Chat, claude-3-sonnet-20240229, mistral-large-2402, dbrx-instruct@together, Mixtral-8x7B-Instruct-v0.1, Meta-Llama-3-8B-Instruct, tulu-2-dpo-70b, Llama-2-70b-chat-hf, Llama-2-7b-chat-hf, gemma-7b-it and gemma-2b-it.
ACKNOWLEDGMENTSThis work was partially supported by the National Natural Science Foundation of China (Project No. 62202122 and No. 62073272), the Shenzhen Science and Technology Program under Grant No. GXWD20231130110308001, and the Guangdong Basic and Applied Basic Research Foundation under Grant No. 2024A1515011949.GPT-4oMistral-NemoQwen2-1.5B  (MLE), as used by(Chiang et al., 2024), to fit the Elo rating and use bootstrap to estimate confidence intervals.Figure11illustrates the elo rating from CHATBOT ARENA, ARENA-HARD(Li et al., 2024a)and the results of judges Gemma-2-2B, Llama-3.2-3B,Qwen2.5-3B, and the ensemble result on the ARENA-HARD benchmark dataset.We utilize the average score and the elo ratings derived by combining all matches from the three judges.We conduct an experiment on all available responses from Li et al.(2024a) and exclude 10 models used to fit the predictor in RocketEval and the GPT-4o-2024-05-13, which is used as the judge to produce labels, resulting in 50 test models.It is evident that the scores and elo ratings produced by the LLM judge follow similar trends and are more closely aligned with the results derived from human preferences.Meanwhile, we notice that the test models that exhibit a large deviation from human judgments belong to the same Llama series, indicating the potential bias on different patterns of responses.Published as a conference paper at ICLR 2025A.4 ABLATION STUDYTo validate the effectiveness of strategies adopted in RocketEval, we conduct ablation study by testing the performance on the following variants:• w/o Norm Score: It removes the conditional normalized score and simply use the decoding result as the judgment.• w/o Indep.Judgment: It inputs the checklist into the LLM judge in a multi-turn format, so that the LLM can see its previous judgment result when judging on the current checklist item.• w/o Weight Factor: It sets the weight factor α r to the constant 1.The results, presented in Tables7 and 8, demonstrate that incorporating conditional normalized score consistently enhances the performance of LLM judges, particularly for smaller-sized LLMs.This observation confirms the high uncertainty associated with lightweight LLMs and supports the inference that introducing conditional normalized score can increase their reliability when serving as judges.Simultaneously, setting the weight factor α r to 1 causes the final score to be entirely determined by the supervised predictor.Predictors trained on a limited number of annotations may struggle to provide accurate scoring results but exhibit superior performance in aligning with human preferences at the list level.In such scenarios, the weight factor α r proves to be effective in mitigating the negative influences of biased annotations, thereby achieving strong performance in both instance-level agreement and list-level ranking correlation.Meanwhile, although there is a performance drop in the variant without independent checklist item judgment, the drop is not significant.This may be due to the fact that the position bias exists in all tests and is further alleviated in subsequent score prediction stage.Although the position bias has limited impact on the final prediction results, the form of multi-round dialogue prevents batch processing during LLM inference, thereby reducing efficiency.In conclusion, we believe that independent checklist judgment in RocketEval remains an optimal choice.A.5 CHECKLIST ANALYSISAs mentioned in Section 4.3 , we undertake a more detailed examination of the checklists generated by RocketEval on the WILDBENCH.This benchmark can be categorized into five major task types:Math &amp; Data Analysis, Coding &amp; Debugging, Creative Tasks, Information/Advice Seeking, and Planning &amp; Reasoning.In this section, we focus on extracting knowledge graph relationships from all the checklists and conducting a comprehensive analysis of these relationships.Furthermore, we investigate instances of checklist item reweighting across a broader spectrum of tasks to provide a more extensive understanding of the underlying dynamics.Subject Distribution.As shown in Figure12, the distribution of subject keywords in validating original question responses ensures a universal, compatible, and effective checklist for various tasks.High-frequency keywords like "explanation", and "code" are crucial for Coding &amp; Debugging, aiding in verifying code functionality and clarity.Keywords like "essay", "example", and "story" are vital for Creative Tasks and Information/Advice seeking, ensuring creativity, relevance, and clarity.For Planning &amp; Reasoning, keywords such as "strategy", "method" and "process" ensure comprehensive and practical solutions.In Math &amp; Data Analysis, keywords like "calculation", "algorithm", and "solution" validate mathematical logic and data analysis.Universally applicable keywords like "response", "explanation", and "example" consistently evaluate clarity, relevance, and accuracy across all tasks.This multifaceted approach ensures a robust, flexible, and thorough evaluation process, enhancing the overall effectiveness and reliability of the responses.Task-Predicate Relationship.We categorized the checklist items according to the types of tasks and conducted a statistical analysis of the corresponding predicates.As shown in Figure13
Chatbot arena: an open platform for evaluating llms by human preference. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I Jordan, Joseph E Gonzalez, Ion Stoica, 10.5555/3692070.3692401Proceedings of the 41st International Conference on Machine Learning, ICML'24. JMLR.org. the 41st International Conference on Machine Learning, ICML'24. JMLR.org2024</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, Tatsunori B Hashimoto, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Open llm leaderboard v2. Clémentine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, Thomas Wolf, 2024</p>
<p>GPTScore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, 10.18653/v1/2024.naacl-long.365Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Kevin Duh, Helena Gomez, Steven Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 20241</p>
<p>Extremely randomized trees. Pierre Geurts, Damien Ernst, Louis Wehenkel, 10.1007/s10994-006-6226-1Machine Learning. Apr 200663</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>On the limitations of fine-tuned judge models for llm evaluation. Hui Huang, Yingqi Qu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, Tiejun Zhao, 2024</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Yang, Mixtral of experts. Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed, 2024a</p>
<p>TIGER-Score: Towards building explainable metric for all text generation tasks. Transactions on Machine Learning Research. Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, Wenhu Chen, 2024b</p>
<p>Prometheus: Inducing finegrained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo, The Twelfth International Conference on Learning Representations. 2024a</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, 10.18653/v1/2024.emnlp-main.248Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember 2024b</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, 10.1145/3600006.3613165Proceedings of the 29th Symposium on Operating Systems Principles, SOSP '23. the 29th Symposium on Operating Systems Principles, SOSP '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Checkeval: Robust evaluation framework using large language model via checklist. Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, Pilsung Kang, 2024</p>
<p>Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, E Joseph, 2024a</p>
<p>Split and merge: Aligning position biases in LLM-based evaluators. Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember 2024b</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Transactions on Machine Learning Research. 2023</p>
<p>Wildbench: Benchmarking LLMs with challenging tasks from real users in the wild. Yuntian Bill Yuchen Lin, Khyathi Deng, Abhilasha Chandu, Valentina Ravichander, Nouha Pyatkin, Ronan Dziri, Yejin Le Bras, Choi, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeDecember 2023Association for Computational Linguistics</p>
<p>ChatQA: Surpassing GPT-4 on conversational QA and RAG. Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Can a suit of armor conduct electricity? a new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, 10.18653/v1/D18-1260Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Ellen Riloff, David Chiang, Julia Hockenmaier, Jun'ichi Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguisticsoct -nov 2018</p>
<p>Evaluating the evaluator: Measuring llms' adherence to task evaluation instructions. Bhuvanashree Murugadoss, Christian Poelitz, Ian Drosos, Nick Vu Le, Carina Suzana Mckenna, Chris Negreanu, Advait Parnin, Sarkar, 2024</p>
<p>Mixeval: Deriving wisdom of the crowd from LLM benchmark mixtures. Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, Yang You, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Automatic distractor generation for multiple choice questions in standard tests. Zhaopeng Qiu, Xian Wu, Wei Fan, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational Linguistics2020</p>
<p>Head-to-tail: How knowledgeable are large language models (llms)? aka will llms replace knowledge graphs?. Kai Sun, Yifan Xu, Hanwen Zha, Yue Liu, Xin Luna, Dong , Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20241</p>
<p>Oscar Wahltinez. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, A Christopher, Danila Choquette-Choo, David Sinopalnikov, Dimple Weinberger, Dominika Vijaykumar, Dustin Rogozińska, Elisa Herbison, Emma Bandy, Eric Wang, Erica Noland, Evan Moreira, Evgenii Senter, Francesco Eltyshev, Visin, Gary Gabriel Rasskin, Glenn Wei, Gus Cameron, Hadi Martins, Hanna Hashemi, Harleen Klimczak-Plucińska, Harsh Batra, Ivan Dhand, Jacinda Nardini, Jack Mein, James Zhou, Jeff Svensson, Jetha Stanway, Jin Peng Chan, Joana Zhou, Joana Carrasqueira, Jocelyn Iljazi, Joe Becker, Joost Fernandez, Josh Van Amersfoort, Josh Gordon, Josh Lipschultz, Newlan, Kareem Ju Yeong Ji, Kartikeya Mohamed, Kat Badola, Katie Black, Keelin Millican, Kelvin Mcdonell, Kiranbir Nguyen, Kish Sodhia, Lars Lowe Greene, Lauren Sjoesund, Laurent Usui, Lena Sifre, Leticia Heuermann, Lilly Lago, Mcnealus, Baldini Livio, Logan Soares, Lucas Kilpatrick, Luciano Dixon, Machel Martins, Manvinder Reid, Mark Singh, Martin Iverson, Mat Görner, Mateo Velloso, Matt Wirth, Matt Davidow, Matthew Miller, Matthew Rahtz, Meg Watson, Mehran Risdal, Michael Kazemi, Ming Moynihan, Minsuk Zhang, Minwoo Kahng, Mofi Park, Mohit Rahman, Natalie Khatwani, Nenshad Dao, Nesh Bardoliwalla, Neta Devanathan, Nilay Dumai, Timothy Chauhan ; Susan Chan, Ting Jordan, Tom Yu, Tom Eccles, Tomas Hennigan, Tulsee Kocisky, Vihan Doshi, Vikas Jain, Vilobh Yadav, Vishal Meshram, Warren Dharmadhikari, Wei Barkley, Wenming Wei, Woohyun Ye, Woosuk Han, Xiang Kwon, Zhe Xu, Zhitao Shen, Zichuan Gong, Victor Wei, Phoebe Cotruta, Anand Kirk, Minh Rao, Ludovic Giang, Tris Peran, Warkentin, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin. Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Perrin, M R Sébastien, Sebastian Arnold, Shengyang Krause, Shruti Dai, Shruti Garg, Sue Sheth, Ronstrom, Eli Collins; Slav Petrov, Oriol Vinyals, Jeff DeanRaia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan2024Petko Georgiev. Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving open language models at a practical size</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, Zhifang Sui, 10.18653/v1/2024.acl-long.511Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 2024a1</p>
<p>PandaLM: An automatic evaluation benchmark for LLM instruction tuning optimization. Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang, The Twelfth International Conference on Learning Representations. 2024b</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. 2024</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems. M Ranzato, A Beygelzimer, Y Dauphin, P S Liang, J Wortman Vaughan, Curran Associates, Inc202134</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>SGLang: Efficient execution of structured language model programs. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, Clark Barrett, Ying Sheng, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Is your model really a good math reasoner? evaluating mathematical reasoning with checklist. Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek F Wong, Xiaowei Huang, Qiufeng Wang, Kaizhu Huang, 2024</p>
<p>JudgeLM: Fine-tuned large language models are scalable judges. Lianghui Zhu, Xinggang Wang, Xinlong Wang, VWKHUHD*58EDVHG'HFRGHUFODVVZLWKCBBLQLWBBCCIRUZDUGCDQGDQRXWSXWOD\HU XVLQJC/RJ6RIWPD[CRUHTXLYDOHQW" 'RHVWKHUHVSRQVHVHWXSDFRPSUHKHQVLYHFKDUDFWHUWRLQGH[PDSSLQJDQGZRUGSDLU KDQGOLQJVSHFLILFDOO\WKURXJKDFXVWRPGDWDVHWFODVV. HURLQJJUDGLHQWVDQGH2025RHVWKHUHVSRQVHFRUUHFWO\LPSRUWQHFHVVDU\3\7RUFKFRPSRQHQWVVXFKDVCWRUFKC CQQCCRSWLPCC'DWD/RDGHUCDQGRWKHUUHOHYDQWOLEUDULHV. SODQDWLRQRULPSOHPHQWDWLRQVKRZLQJKRZSDGGLQJDQGEDWFKLQJDUH PDQDJHGLQPLQLEDWFKWUDLQLQJ" 'RHVWKHUHVSRQVHVKRZDFRPSOHWHWUDLQLQJIXQFWLRQWKDWLQFOXGHVLQLWLDOL]LQJKLGGHQ VWDWHV</p>
<p>$ Uhvwduwdqghqgrivhtxhqfhwrnhqlqglfhvfruuhfwo\lpsohphqwhgdqgxvhgzlwklq, Lqsxwkdqgolqjlqwkhwudlqlqjixqfwlrq" 'rhvwkhuhvsrqvhfruuhfwo\vhwxswkhwudlqlqjlqiudvwuxfwxuhlqfoxglqjwkhgdwdvhw 'dwd/Rdghuprgholqvwdqwldwlrqdqgwkhrswlpl]hu" $uhwkhuhfohdughwdlovridwudlqlqjorrszlwkhsrfklwhudwlrqvedwfksurfhvvlqjiurp D'dwd/ Rdghudqglqwhuphgldwhorvvrxwsxwv, QWURS\RU 1///RVVFOHDUO\H[SODLQHGRUGHPRQVWUDWHGLQWKHWUDLQLQJORRS" XVLQJS\WRUFKLPSOHPHQWDFKDUDFWHU OHYHOVHTXHQFHWRVHTXHQFHHQFRGHU GHFRGHUPRGHOZLWK*587UDLQHGWKH PRGHORQDGDWDVHWRIZRUGSDLUVZLWK PLQLEDWFKWUDLQLQJVWKHORVVIXQFWLRQIRUVHTXHQFHWRVHTXHQFHOHDUQLQJLQ1/3OLNH&amp;URVV. </p>            </div>
        </div>

    </div>
</body>
</html>
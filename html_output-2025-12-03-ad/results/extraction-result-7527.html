<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7527 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7527</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7527</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-267770682</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.13610v1.pdf" target="_blank">Data-driven Discovery with Large Generative Models</a></p>
                <p><strong>Paper Abstract:</strong> With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially. This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-to-end data-driven discovery -- a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments. We first outline several desiderata for an ideal data-driven discovery system. Then, through DATAVOYAGER, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata -- a feat previously unattainable -- while also highlighting important limitations in the current system that open up opportunities for novel ML research. We contend that achieving accurate, reliable, and robust end-to-end discovery systems solely through the current capabilities of LGMs is challenging. We instead advocate for fail-proof tool integration, along with active user moderation through feedback mechanisms, to foster data-driven scientific discoveries with efficiency and reproducibility.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7527.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7527.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DATAVOYAGER (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DATAVOYAGER — a GPT-4 powered data-driven discovery system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proof-of-concept multi-agent system (Planner, Programmer, Data Expert, Critic, User Proxy) built on top of GPT-4 using the AutoGen framework to perform text-based simulation of the scientific discovery pipeline: dataset understanding, hypothesis generation, multi-step planning, code/function generation, execution of statistical tests, and interpretation over real-world tabular datasets (e.g., National Longitudinal Survey).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Instruction‑tuned large generative model (LGM) used with function-calling, code-generation, and multi-agent role prompts via AutoGen; not reported as further fine-tuned in this work</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social sciences / Health economics / Public health (analysis of National Longitudinal Survey data); also applied to agent-behavior experiments (knowledge frontiers)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of end-to-end data-driven discovery: semantic dataset understanding, automated hypothesis generation/prioritization, multi-step research planning, generation of analysis code or function calls, execution of statistical tests (e.g., OLS, GLM, logistic regression, t-tests, correlations, Gini coefficient calculation, clustering), and interpretation of outputs to accept/reject hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Role-based multi-agent prompting (Planner, Programmer, Data Expert, Critic, User Proxy) using the AutoGen framework; uses function-calling to invoke pre-defined, structured analysis functions and code-generation (Python snippets) for unconstrained analyses; operates in two modes: fully-autonomous (dataset+metadata only) or user-guided (natural-language query + dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Tool integration / use of fail-proof structured functions vs unconstrained code generation', 'Hallucination tendency of LGMs (affects hypothesis generation, planning, and interpretation)', 'Limited System-2 / long-horizon reasoning in LGMs (planning and multi-step correctness)', 'Difficulty in debugging generated code (affects verification reliability)', 'Data heterogeneity and variable semantics across datasets', 'Scale and long-context attention limits (affecting long-horizon plans and large datasets)', 'Need for human feedback/moderation and continual learning', 'Domain specificity / long-tail domain performance (subpar in rare cases)', "Reliance on pre-defined statistical tools for 'fail-proof' verification vs. free code"]</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Implemented with AutoGen multi-agent architecture; access to a code execution environment and a set of pre-defined statistical functions (function-calling). Operates in two modes: fully-autonomous (only dataset+metadata) and user-guided (with natural-language objectives). No temperature, decoding settings, or exact API parameters reported; no model parameter count reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Reports hallucinations (inventing unsupported claims), limited long-horizon/complex planning ability, poor long-tail performance, difficulties debugging generated code, inability of LGMs alone to guarantee robust verification (necessity of fail-proof external tools), scalability and long-context limitations, potential bias propagation from data and LGMs, and risk of p-hacking when exploring very large hypothesis spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Discovery with Large Generative Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7527.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7527.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoScientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoScientist (Boiko et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed system that uses large generative models to automate parts of scientific workflows (notably chemistry), but in practice still requires substantial human intervention for hypothesis verification (e.g., wet-lab experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous chemical research with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>large generative models (LGMs) (unspecified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LGM-based system (details not provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Autonomous chemical research (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Automating parts of experimental planning and analysis in chemistry (citation indicates autonomous chemical research workflows assisted by LGMs).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Requires substantial human intervention for verification (wet-lab experiments) — limitation to being fully autonomous', 'Domain‑specific verification demands (wet lab) limit full automation']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Despite using LGMs to automate components, it still needs human/wet-lab verification and therefore does not qualify as fully autonomous end-to-end discovery in the authors' view.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Discovery with Large Generative Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7527.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7527.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DataLume</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DataLume (Gu et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system referenced as fully automating code generation for data transformation and hypothesis verification, but lacking modules for hypothesis search and complex scientific workflow orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General data-analysis / automated data transformation and verification (mentioned as related work)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Automated code generation for data transformation and hypothesis verification (per the citation in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Lacks hypothesis search and orchestration for complex scientific workflows (limitation noted by authors)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Does not provide hypothesis search or planning/orchestration capabilities required for end-to-end scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Discovery with Large Generative Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7527.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7527.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning (Shinn et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A repository/bench of language-agent experiments used to generate experimental data (agent behaviors) which the authors fed into DATAVOYAGER to simulate knowledge-frontier analyses and generate further hypotheses/insights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Language-agent framework / repository (composed of LLM-driven agents; exact LLMs not enumerated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Agent behavior / AI research agents (used as experimental data source to probe knowledge-frontier capabilities)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generation of agent-experiment data (language-agent behaviors) that DATAVOYAGER analyzes to produce novel analyses, cluster analyses, and suggested experiments for agent literature.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Tasks that are conceptual (genetics, life stages) are easier for agents to learn/improve; practical/hands-on tasks (chemistry mixing) are harder (observation from DATAVOYAGER analysis of Reflexion outputs)', 'Limited context on data affects the quality of analyses generated from agent logs']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>The authors modified Reflexion experimental design following Majumder et al. (2023) and fed resulting data to DATAVOYAGER for analysis; used as a way to simulate knowledge-frontier tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>When fed limited context, analyses can be shallow or suggest inappropriate methods; practical/hands-on task domains remain challenging for purely language-agent simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Data-driven Discovery with Large Generative Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Benchmarking large language models as ai research agents <em>(Rating: 2)</em></li>
                <li>Agentbench: Evaluating llms as agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7527",
    "paper_id": "paper-267770682",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "DATAVOYAGER (GPT-4)",
            "name_full": "DATAVOYAGER — a GPT-4 powered data-driven discovery system",
            "brief_description": "A proof-of-concept multi-agent system (Planner, Programmer, Data Expert, Critic, User Proxy) built on top of GPT-4 using the AutoGen framework to perform text-based simulation of the scientific discovery pipeline: dataset understanding, hypothesis generation, multi-step planning, code/function generation, execution of statistical tests, and interpretation over real-world tabular datasets (e.g., National Longitudinal Survey).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_type": "Instruction‑tuned large generative model (LGM) used with function-calling, code-generation, and multi-agent role prompts via AutoGen; not reported as further fine-tuned in this work",
            "scientific_domain": "Social sciences / Health economics / Public health (analysis of National Longitudinal Survey data); also applied to agent-behavior experiments (knowledge frontiers)",
            "simulation_task_description": "Text-based simulation of end-to-end data-driven discovery: semantic dataset understanding, automated hypothesis generation/prioritization, multi-step research planning, generation of analysis code or function calls, execution of statistical tests (e.g., OLS, GLM, logistic regression, t-tests, correlations, Gini coefficient calculation, clustering), and interpretation of outputs to accept/reject hypotheses.",
            "prompting_strategy": "Role-based multi-agent prompting (Planner, Programmer, Data Expert, Critic, User Proxy) using the AutoGen framework; uses function-calling to invoke pre-defined, structured analysis functions and code-generation (Python snippets) for unconstrained analyses; operates in two modes: fully-autonomous (dataset+metadata only) or user-guided (natural-language query + dataset).",
            "evaluation_metric": null,
            "reported_accuracy": null,
            "baseline_accuracy": null,
            "factors_reported": [
                "Tool integration / use of fail-proof structured functions vs unconstrained code generation",
                "Hallucination tendency of LGMs (affects hypothesis generation, planning, and interpretation)",
                "Limited System-2 / long-horizon reasoning in LGMs (planning and multi-step correctness)",
                "Difficulty in debugging generated code (affects verification reliability)",
                "Data heterogeneity and variable semantics across datasets",
                "Scale and long-context attention limits (affecting long-horizon plans and large datasets)",
                "Need for human feedback/moderation and continual learning",
                "Domain specificity / long-tail domain performance (subpar in rare cases)",
                "Reliance on pre-defined statistical tools for 'fail-proof' verification vs. free code"
            ],
            "experimental_conditions": "Implemented with AutoGen multi-agent architecture; access to a code execution environment and a set of pre-defined statistical functions (function-calling). Operates in two modes: fully-autonomous (only dataset+metadata) and user-guided (with natural-language objectives). No temperature, decoding settings, or exact API parameters reported; no model parameter count reported.",
            "limitations_or_failure_modes": "Reports hallucinations (inventing unsupported claims), limited long-horizon/complex planning ability, poor long-tail performance, difficulties debugging generated code, inability of LGMs alone to guarantee robust verification (necessity of fail-proof external tools), scalability and long-context limitations, potential bias propagation from data and LGMs, and risk of p-hacking when exploring very large hypothesis spaces.",
            "uuid": "e7527.0",
            "source_info": {
                "paper_title": "Data-driven Discovery with Large Generative Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "CoScientist",
            "name_full": "CoScientist (Boiko et al., 2023)",
            "brief_description": "A recently proposed system that uses large generative models to automate parts of scientific workflows (notably chemistry), but in practice still requires substantial human intervention for hypothesis verification (e.g., wet-lab experiments).",
            "citation_title": "Autonomous chemical research with large language models",
            "mention_or_use": "mention",
            "model_name": "large generative models (LGMs) (unspecified in this paper)",
            "model_size": null,
            "model_type": "LGM-based system (details not provided in this paper)",
            "scientific_domain": "Chemistry / Autonomous chemical research (as cited)",
            "simulation_task_description": "Automating parts of experimental planning and analysis in chemistry (citation indicates autonomous chemical research workflows assisted by LGMs).",
            "prompting_strategy": null,
            "evaluation_metric": null,
            "reported_accuracy": null,
            "baseline_accuracy": null,
            "factors_reported": [
                "Requires substantial human intervention for verification (wet-lab experiments) — limitation to being fully autonomous",
                "Domain‑specific verification demands (wet lab) limit full automation"
            ],
            "experimental_conditions": null,
            "limitations_or_failure_modes": "Despite using LGMs to automate components, it still needs human/wet-lab verification and therefore does not qualify as fully autonomous end-to-end discovery in the authors' view.",
            "uuid": "e7527.1",
            "source_info": {
                "paper_title": "Data-driven Discovery with Large Generative Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "DataLume",
            "name_full": "DataLume (Gu et al., 2023)",
            "brief_description": "A system referenced as fully automating code generation for data transformation and hypothesis verification, but lacking modules for hypothesis search and complex scientific workflow orchestration.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "model_type": null,
            "scientific_domain": "General data-analysis / automated data transformation and verification (mentioned as related work)",
            "simulation_task_description": "Automated code generation for data transformation and hypothesis verification (per the citation in the paper).",
            "prompting_strategy": null,
            "evaluation_metric": null,
            "reported_accuracy": null,
            "baseline_accuracy": null,
            "factors_reported": [
                "Lacks hypothesis search and orchestration for complex scientific workflows (limitation noted by authors)"
            ],
            "experimental_conditions": null,
            "limitations_or_failure_modes": "Does not provide hypothesis search or planning/orchestration capabilities required for end-to-end scientific discovery.",
            "uuid": "e7527.2",
            "source_info": {
                "paper_title": "Data-driven Discovery with Large Generative Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning (Shinn et al., 2023)",
            "brief_description": "A repository/bench of language-agent experiments used to generate experimental data (agent behaviors) which the authors fed into DATAVOYAGER to simulate knowledge-frontier analyses and generate further hypotheses/insights.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "model_type": "Language-agent framework / repository (composed of LLM-driven agents; exact LLMs not enumerated in this paper)",
            "scientific_domain": "Agent behavior / AI research agents (used as experimental data source to probe knowledge-frontier capabilities)",
            "simulation_task_description": "Generation of agent-experiment data (language-agent behaviors) that DATAVOYAGER analyzes to produce novel analyses, cluster analyses, and suggested experiments for agent literature.",
            "prompting_strategy": null,
            "evaluation_metric": null,
            "reported_accuracy": null,
            "baseline_accuracy": null,
            "factors_reported": [
                "Tasks that are conceptual (genetics, life stages) are easier for agents to learn/improve; practical/hands-on tasks (chemistry mixing) are harder (observation from DATAVOYAGER analysis of Reflexion outputs)",
                "Limited context on data affects the quality of analyses generated from agent logs"
            ],
            "experimental_conditions": "The authors modified Reflexion experimental design following Majumder et al. (2023) and fed resulting data to DATAVOYAGER for analysis; used as a way to simulate knowledge-frontier tasks.",
            "limitations_or_failure_modes": "When fed limited context, analyses can be shallow or suggest inappropriate methods; practical/hands-on task domains remain challenging for purely language-agent simulations.",
            "uuid": "e7527.3",
            "source_info": {
                "paper_title": "Data-driven Discovery with Large Generative Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Benchmarking large language models as ai research agents",
            "rating": 2
        },
        {
            "paper_title": "Agentbench: Evaluating llms as agents",
            "rating": 1
        }
    ],
    "cost": 0.0145045,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Data-driven Discovery with Large Generative Models
21 Feb 2024</p>
<p>Bodhisattwa Prasad Majumder 
Equal contribution</p>
<p>Allen Institute for AI</p>
<p>Harshit Surana <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#104;&#97;&#114;&#115;&#104;&#105;&#116;&#64;&#111;&#112;&#101;&#110;&#108;&#111;&#99;&#117;&#115;&#46;&#97;&#105;">&#104;&#97;&#114;&#115;&#104;&#105;&#116;&#64;&#111;&#112;&#101;&#110;&#108;&#111;&#99;&#117;&#115;&#46;&#97;&#105;</a> 
Equal contribution</p>
<p>OpenLocus</p>
<p>Dhruv Agarwal 
University of Massachusetts Amherst</p>
<p>Sanchaita Hazra 
University of Utah</p>
<p>Ashish Sabharwal 
Allen Institute for AI</p>
<p>Peter Clark 
Allen Institute for AI</p>
<p>Data-driven Discovery with Large Generative Models
21 Feb 202415D87CE7EB5188BE4801F0977BDA9F55arXiv:2402.13610v1[cs.CL]National Longitudinal Surveys Query: Study the relation between BMI and Time Preference
With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially.This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-toend data-driven discovery-a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments.We first outline several desiderata for an ideal data-driven discovery system.Then, through DATAVOYAGER, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata-a feat previously unattainable-while also highlighting important limitations in the current system that open up opportunities for novel ML research.We contend that achieving accurate, reliable, and robust endto-end discovery systems solely through the current capabilities of LGMs is challenging.We instead advocate for fail-proof tool integration, along with active user moderation through feedback mechanisms, to foster data-driven scientific discoveries with efficiency and reproducibility.</p>
<p>Introduction</p>
<p>The deluge of data collected in the digital age by advanced scientific instruments, sensors, and computational techniques has marked a transformative change in the process and pace of scientific discovery (Anderson, 2008;Ramakrishnan &amp; Grama, 1999;Jumper et al., 2021).This acceleration, however, paints a paradoxical scenario-while rapid development indicates the advancement of knowledge, it simultaneously poses significant challenges for scientists to absorb new findings, navigate interconnections, formulate novel hypotheses, and arrive at meaningful conclu-Developing an end-to-end discovery system is challenging.Previous works have either severely lacked the requisite computational power (Langley, 1981;Langley et al., 1984;1983), developed domain-specific bespoke methodologies (e.g., AlphaFold; Jumper et al. (2021)), or involved substantial human intervention (e.g., wet lab experiments) thus not qualifying as autonomous end-to-end (CoScientist; Boiko et al. (2023)).In this position paper, we argue that a focus on data-driven discovery using large generative models (LGMs) addresses each of these prior shortcomings and presents a practical first step towards the goal of an end-toend system for automating the scientific process.Following Newell &amp; Simon (1976), we define this paradigm as a heuristic search framework that aims to describe a given set of observations by uncovering the laws that govern its data-generating process.</p>
<p>For example, consider the flow described in Figure 1.Given a dataset of socio-economic variables collected from a set of respondents, a user might formulate a hypothesis about the relationship between the BMI of a subset of the respondents and their financial behavior (variables present in the dataset; top-left).A data-driven discovery system should be able to automatically generate a verification plan and execute multiple steps of statistical tests (e.g., OLS, GLM) over the provided data to confirm or reject the hypothesis (bottomleft).Alternatively, a user might only provide a high-level research question, such as specifying the domains of interest (i.e., finance and health; top-middle).In this scenario, a discovery system must first identify the relevant variables and then search the space of plausible hypotheses to generate and verify interesting questions conditioned on the provided data and existing world knowledge (bottom-middle).Finally, users may have diverse information-seeking needs necessitating the ability to provide feedback to the system, such as in using a particular statistical methodology for certain types of data during automatic verification (top-right).An automated discovery system must accommodate and persist such feedback in order to recover from mistakes and accurately handle future queries (bottom-right).</p>
<p>Wealt h disparit ies among racial minorities (0.91) are more prominent compared to others (0.67).This is excarbated among minority men who have been incarcerated earlier.</p>
<p>I learnt to always use Gini analysis for wealth disparties; I am checking t he code int o our t oolbench.</p>
<p>Feedback</p>
<p>Figure 1.A blueprint flow demonstrating ideal workflows for data-driven discovery.Left: User asks an explicit question around a particular line of inquiry or hypothesis.Middle: The user can also ask a broad and partially-defined high-level question, where the system must figure out the appropriate datasets, data transformations, variables, a list of possible hypotheses, and their verification.In this example, the system maps time preference and health outcomes to exact variables, runs the analysis across appropriate demographic cuts, and then shares the significant findings for further exploration and verification.Right: The user can provide follow-up feedback at any time and the continual learner will learn from it while providing updated experiments and results.</p>
<p>While our ultimate goal encompasses the full spectrum of scientific inquiry, we focus first on end-to-end discovery from observational or experimental data for two reasons:</p>
<p>(1) an abundance of large-scale datasets that would benefit highly from automated discovery; and (2) the practicality of automated verification enabled by data without the need for additional data collection 1 .</p>
<p>We identify two main challenges to automating data-driven discovery-(1) hypothesis search: the effective consumption of provided data and existing knowledge to devise novel hypotheses, and (2) hypothesis verification: the evaluation of the generated hypotheses for rapid iteration and continual discovery.A successful solution must further be able to generate and follow complex plans, execute diverse analytical tests, and parse through the abundant heterogeneity in real-world data.With the unprecedented success of LGMs operating on multiple modalities such as language (Achiam et al., 2023;Touvron et al., 2023), code (Liu et al., 2023b;Li et al., 2022), and images (Achiam et al., 2023;Liu et al., 2023a), we argue that it is now practical to build such a solution that can effectively tackle both challenges.</p>
<p>Hypothesis Search.The scientific process typically begins with the construction of a proposed hypothesis based on prior knowledge and exploratory observations regarding some phenomenon of interest.For example, discovering new insights from publicly available National Longitudinal 1 In contrast to hypothesis verification in the physical sciences, which often require wet lab experiments and where erroneous automation may lead to false discoveries (Leeman et al., 2024).Surveys2 will require prioritizing unexplored hypotheses over already verified results.However, it is non-trivial what an optimal method for such a prioritized search might be.</p>
<p>Foremost, we may ask whether the search should be driven by an extrinsic goal-a user-defined objective, a high-level research question, or a set of variables of interest.This setting might involve using algorithms that guide the search process using objective-gradients (Weitzman, 1978) that identify variables and models that directly, or greedily, optimize the extrinsic goal.We argue that LGMs, with their massive, web-scale pre-training, possess both the necessary priors and the ability to handle heterogeneity, to help guide such a goal-driven search for relevant hypotheses.</p>
<p>It can also be argued that goal-driven approaches may not yield desired outcomes, particularly when dealing with openended questions, where the search is often susceptible to capture by local optima (Whitley, 1991;Bengio et al., 2009).Drivers for search might then be intrinsic metrics (Oudeyer et al., 2007), such as diversity (Eysenbach et al., 2019;Agarwal et al., 2023;Trinh et al., 2024), interestingness (or curiosity) (Pathak et al., 2017;Zhang et al., 2023), or information gain (Hennig &amp; Schuler, 2012;Houthooft et al., 2016), that do not optimize for a user-defined extrinsic goal but instead encourage open-ended creativity and, eventually, serendipitous discovery (Foster &amp; Ford, 2003;Taleb, 2007;Stanley et al., 2017).Here, too, LGMs present a solution, for instance, in estimating the novelty or likelihood of hypotheses in the search space.</p>
<p>More int erdiscplinary insights based on the results</p>
<p>The correlat ion coefficient: -0.031, very weak negative linear relationship between dissaving and BMI.</p>
<p>The int eract ion term coefficient: 0.5259 st at ist ically significant (p &lt; 0.0000) ... "GENDER_MALE" has a significant positive association with BMI, indicating that males have a higher BMI t han females.</p>
<p>Economics and</p>
<p>The GLM confirms t he findings from t he OLS model regarding the interactions between time preference and demographic factors.</p>
<p>How to mitigate the effect of testing multiple hypotheses?</p>
<p>... Hypothesis Verification.</p>
<p>With a set of plausible hypotheses identified, it is next required to subject each claim through detailed inspection, often via a series of empirical evaluations and statistical tests, to determine veracity, which is highly tractable and could be fail-proof in data-driven discovery.This might involve selecting which analyses or statistical tests to run, transforming raw data into a format admissible for each test, handling missing or erroneous data, generating code to execute the tests, and finally analyzing the test results.Given the surge of recent advancements in language modeling capabilities, including instructionfollowing (Wei et al., 2022), tool use (Schick et al., 2023), program synthesis (Wang et al., 2023a;Agarwal et al., 2023), planning (Majumder et al., 2023), and orchestration (Hou et al., 2023), we argue that LGM agents present a promising solution for automating hypothesis evaluation.</p>
<p>The availability of these capabilities, however, must not be seen as a panacea.(1) LGMs often hallucinate, leading to incorrect insights that may not be grounded in the data.</p>
<p>(2) LGMs have limited or no "System-2" reasoning (Kahneman, 2011;LeCun, 2022;Kambhampati et al., 2024), thus necessitating additional scaffolding in order to utilize them for long-horizon tasks.(3) LGMs demonstrate subpar performance in the long tail, thus making their successful application in interfacing with external and domain-specific tools a major challenge to overcome.(4) Finally, LGMs are notoriously challenging to align and steer based on hu-man feedback (Wolf et al., 2023), a crucial component for reliable and useful scientific discovery.</p>
<p>We envision a blueprint of a data-driven discovery system in Figure 1 that allows researchers to ingest datasets, search and verify hypotheses using fail-proof tools, and consult literature to surface novel insights.Our survey in Figure 3 indicates the lack of systems capable of automated and robust data-driven discovery, with existing systems partially covering desired functionalities.To tackle this, we argue:</p>
<ol>
<li>Automated data-driven discovery warrants research attention owing to the abundance of (public or private) data and its tractable challenges (hypothesis search and verification) as opposed to discoveries requiring laborious data collection or physical experiments.</li>
</ol>
<p>2.</p>
<p>LGMs present an incredible potential to realize several properties of an ideal data-driven discovery system, such as knowledge-driven hypothesis search or tool usage to verify hypotheses-creating new avenues for ongoing efforts in the ML community on code generation, planning, and program synthesis.</p>
<p>3.</p>
<p>LGMs are not all we need.Interfacing with fail-proof tools and inference-time functions, catering to domains and long tail with user moderation, is required to have an accurate, reliable, and robust data-driven discovery system capable of advancing scientific progress with speed and reproducibility.</p>
<p>DATAVOYAGER: A Proof of Concept</p>
<p>As a proof of concept, we borrow a well-studied role-based multi-agent architecture (Liu et al., 2023c;Zhou et al., 2023) powered by GPT-4 (Achiam et al., 2023), a state-of-the-art language model, to build DATAVOYAGER-a system that can semantically understand a dataset, programmatically explore verifiable hypotheses using the available data, run basic statistical tests (e.g., correlation and regression analyses) by invoking pre-defined functions or generating code snippets, and finally analyze the output with detailed analyses.DATAVOYAGERis meant to represent a baseline system that utilizes existing functionalities of GPT-4, such as function calling, code generation, and language generation.</p>
<p>We envision any data-driven discovery system to be capable of operating in either of the following two settings.(1) Fullyautonomous: using only the dataset and its metadata as the input.In this case, the system should consider the full hypothesis space for search and verification.</p>
<p>(2) User-guided: combining the dataset with a (natural language) query stating a high-level objective to narrow down the hypothesis search space, akin to goal-directed agents (Majumder et al., 2023).DATAVOYAGER can operate in both settings.</p>
<p>The core components of our system consist of specialized agents that are designed to manage different aspects of the data-driven discovery process as well as structured functions or programs that help analyze the data in specific ways via function calling.We employ the AutoGen framework3 that allows agents to communicate in arbitrary order dependent on the context.Following is a brief description of all agents used in DATAVOYAGER (more in Figure 4):</p>
<p>• Planner: Interprets the user query and generates a comprehensive, structured plan to achieve it or, in the autonomous setting, generates an additional dataset exploration plan.The plan is then decomposed into executable sub-tasks and delegated to the relevant agents.• Programmer: Performs data transformations, filtering, and specialized coding for domain-specific analyses according to the generated plan.It can also call structured, pre-defined functions with relevant arguments to make execution fail-proof.4 • Data Expert: Interprets the results generated by the programmer, extracting insights, connecting interdisciplinary knowledge, and formulating conclusions.• Critic: Evaluates the analyses and provides constructive feedback on analytical methods and execution.• User Proxy: Facilitates on-demand human feedback.</p>
<p>A user can steer the discovery process towards an objective, rectify errors, and prevent off-course explorations.</p>
<p>Towards Data-driven Discovery Systems</p>
<p>In this section, we first outline a set of desired functionalities for a data-driven discovery system.Using these functionalities, and armed with our baseline system DATAVOY-AGER along with evidence from the literature, we demonstrate extensive support towards our positions 2 and 3. Functionalities such as data understanding, hypothesis generation, multi-step planning, and interdisciplinary knowledge integration provide evidence that a system (DATAVOYAGER) powered by a state-of-the-art LGM shows promise for ideal data-driven discovery, an ability not previously achievable before the wide adoption of LGMs.On the other hand, functionalities such as data transformation, scale, hypothesis verification, accommodating human feedback, and p-hacking proof confirm that LGMs alone are insufficient.Integrating robust tools to execute at scale and user-centric interventions is crucial for a tractable data-driven discovery system.</p>
<p>Comprehensive Data Understanding</p>
<p>Data Understanding.Understanding data forms the core of data-driven discovery and involves processing variables semantically as well as programmatically (Ristoski &amp; Paulheim, 2016).The former involves understanding how the data was collected or acquired, grounded in the domain knowledge, as well as the semantic relationship between the variables present in the data.The latter involves understanding the datatype of each variable and the values they can take.Progress in database query generation (Sun et al., 2023), commonsense reasoning on symbolic spaces (Qiu et al., 2023), and unsupervised KGQA (Agarwal et al., 2023) indicate reason for optimism for the use of LGMs for data understanding.</p>
<p>For example, Smith et al. (2005) explored the link between time preference and BMI from the National Longitudinal Surveys using several variables indicating the saving behavior of the respondents.To replicate this from scratch, a discovery system must understand the difference between the variable measuring if respondents withdrew more money from savings than they put in (DISSAVED) and the variable indicating if they have no savings or unchanged savings from the previous year (SAMESAVE)5 .Here, DATAVOY-AGER's LGM-based planner correctly identifies variables related to time preference:</p>
<p>To examine the effects of time preference on individuals, we need to understand the variables in the dataset that relate to time preference.In the provided dataset, the variables DISSAVED and SAMESAVE seem to be related to time preference (...) Full example: Figure 6 While it works for this example, a comprehensive data understanding is still challenging due to the complexity of various datasets with numerous types and complex metadata.We, therefore, ask: can a system achieve a comprehensive understanding of domains and variations in diverse datasets in a domain-agnostic manner as compared to domain-specific systems, such as CoScientist (Boiko et al., 2023)?</p>
<p>Data Transformation.Different datasets have unique characteristics, requiring custom transformations and filtering operations (Kang et al., 2017).Moreover, even within the same dataset, different hypotheses may demand different transformations for accurate verification and testing.Without this capability, the potential to conduct a wide range of statistical tests for hypothesis verification would be compromised (Bailis et al., 2017).A simple example of data transformation would be the ability to convert a categorical variable into a one-hot encoding.Further, the following is an example showing DATAVOYAGER's LGM-based programmer performing data transformation in order to derive interaction terms between variables:</p>
<p>Let's start by adding interaction terms to examine the potential link between time preference and BMI across different demographic groups (. . . ) Full example: Figure 7 The challenge lies in accommodating the abundant diversity of hypotheses and datasets, each requiring highly customized transformations (Bowers &amp; Ludäscher, 2004).The ability of LGMs to generate code for such domain-specific data (Sharma et al., 2023) hints towards a generalized solution; however, the difficulty in debugging generated code (Vaithilingam et al., 2022) demands a call to action for building better code generation models.</p>
<p>Scale.Modern scientific exploration often involves large amounts of data, a complex analytics workflow, and a large hypothesis space (Elliott et al., 2016).It is important, thus, for a useful autonomous discovery system to be able to sift through such large datasets efficiently while maintaining the state of its several processes and tracking previously conducted analyses.Without this ability to scale and handle complex workflows, several hypotheses would remain unexplored, and valuable insights left undiscovered.</p>
<p>For longitudinal studies, where it is important to understand how variables evolve over time (Weiss &amp; Ware, 1996), scalability is particularly crucial in order to handle data over extended time periods.Furthermore, in very large-scale data scenarios, such as the Cancer Moonshot project6 and the Cancer Genomics Cloud (Lau et al., 2017), the discovery system must be able to analyze petabytes of data in complex workflows, all while maintaining a state of the possible hypotheses and variable combinations as well as the explorations conducted thus far.In such scenarios, LGMs must be able to support long-horizon planning and longcontext attention.However,</p>
<p>LGMs are yet to show significant progress on both counts (Valmeekam et al., 2022), a limitation of DATAVOYAGER as well, thus highlighting a need for focused research towards these goals.</p>
<p>Hypothesis Generation</p>
<p>Connecting Data and Scientific Literature.The ability to bridge the provided data and existing scientific literature is important in providing an understanding of the hypothesis space grounded by contextual domain knowledge.This ability to learn from known knowledge may further result in various inter-disciplinary perspectives and insights-a phenomenon often called Swanson Linking (Bekhuis, 2006).</p>
<p>For example, to derive novel insights between social background and college graduation (Alexander et al., 1982) from the National Longitudinal Surveys, it is imperative to understand previous research on National Longitudinal Surveys to avoid duplication and incorporate verified knowledge from the literature to improve initial hypotheses.</p>
<p>Linking generated hypotheses to existing knowledge requires accurate retrieval, information extraction, and multistep reasoning (Wang et al., 2023b).Further, combining multiple research articles connects back to the original Swanson Linking problem (Swanson, 1986).While LGMs have recently been shown to perform well in augmenting citations with relevant context based on a user's history (Chang et al., 2023), connecting datasets to scientific literature is an open research problem.By utilizing annotated papers for datasets (Palani et al., 2023), we ask: can a system learn to combine insights from existing literature and a provided dataset in order to discover novel research gaps?</p>
<p>Formulating initial hypotheses.Scientists prioritize experiments based on academic intuition, empirical evidence, and existing theories.In data-driven discovery, this approach is akin to selecting hypotheses from a vast combinatorial space of variable interactions, often extensive for exhaustive exploration (Agrawal et al., 2023), to identify dependent and independent variables.</p>
<p>For example, to understand the relationship between education outcome and socioeconomic status, the system should prioritize investigating how the "rate of completion of BA degree" is influenced by socioeconomic indicators, such as accumulated wealth and parents' education, as a plausible hypothesis (Alexander et al., 1982).This is non-trivial because it not only requires the system to have a semantic understanding of the variable space but also the ability to prioritize hypotheses based on marginal costs and their scientific importance (Agrawal et al., 2023).Here, DATAVOY-AGER performs reasonably well on hypothesis generation:</p>
<p>H1: Females are more likely to complete a BA degree compared to males.H2: Family size has an impact (...).H3: Higher ability scores on the ASVAB test are positively correlated (...) Full example: Figure 18 Hypothesis generation can be seen as inductive reasoning (Qiu et al., 2023) using known evidence by connecting them using entailment-like relations (Dalvi et al., 2021).While</p>
<p>LGMs show good performance on reasoning benchmarks (Hendrycks et al., 2020), data heterogeneity (e.g., variable names, statistical interactions) and semantics make the reasoning problem harder for LGMs (Lu et al., 2023)-thus, we call for research attention.</p>
<p>Planning and Orchestrating Research Pathways</p>
<p>Multi-step planning.Data-driven discovery with complex problems and datasets requires a structured approach of breaking down a high-level objective into manageable subtasks, enabling the systematic exploration of the data and hypothesis landscape.This can be considered equivalent to planning (LeCun, 2022).Prioritized hypothesis search with planning involves states -the intermediate correlations found from data (sub-hypotheses), and operators -the statistical tools and literature to combine verified states (here, sub-hypotheses).Multi-step, iterative planning, thus, comprehensively facilitates the search for scientific discoveries.</p>
<p>Research planning involves incorporating known or novel research pathways, such as the order of analyses or the methods used, and they vary depending on the research goal of the exploration.It can be challenging to choose between a standardized or pre-defined flow as compared to a dynamic plan depending on the realized intermediate states of the planning.Though LGMs as planners are often faulty (Valmeekam et al., 2022), planning within the data hypothesis space presents a fertile ground to systematically benchmark LGMs and improve their abilities.</p>
<p>For example, analyzing the relationship between college education and socio-economic status from National Longitudinal Surveys (Alexander et al., 1982), the system generates the following plan: While the ability to decompose abstract plans into executable sub-plans is heavily explored in coding and symbolic reasoning (Khot et al., 2022), DATAVOYAGER presents a strong base case to improve the efficacy of planning by incorporating dynamic strategies that account for search uncertainties.</p>
<p>Exploration vs. exploitation.The debate concerning whether exploration should be goal-oriented or randomized is crucial in making novel discoveries (Agarwal et al., 2023).This applies directly to data-driven discovery, where variable selection by the planner directly impacts what subset of the hypothesis space is considered for search.Thus, this exploration-exploitation trade-off is a key factor in shaping the makeup of the final outcome (Foster &amp; Ford, 2003).</p>
<p>LGM-based planners, including DATAVOYAGER, prefer direct, goal-oriented variables, e.g., preferring parents' wealth towards success in college education, while de-prioritizing more implicit variables related to urban planning (e.g., location of schools).However, while exploration with intrinsic motivators could lead to novel outcomes, it can also sometimes result in false positives (Oudeyer &amp; Kaplan, 2008).How contexts, domains, and the hypothesis space influence the tradeoff between exploration and exploitation remains an open question, which, we argue, is worth considerable research focus (Majumder et al., 2022;Burda et al., 2018).</p>
<p>Hypothesis Evaluation</p>
<p>Hypothesis Verification.The practical possibility of programmatically verifying a set of hypotheses is a unique feature in data-driven discovery.This encompasses both the proper execution of code as well as the capacity to utilize the appropriate statistical methods and techniques aligned with the high-level research objective (Cai et al., 2023).</p>
<p>The verification of hypotheses can involve (1) the use of tools and (2) code generation.Tools represent a pre-defined set of structured functions, which may be invoked via function-calling by LGMs along with relevant arguments (Pelrine et al., 2023).Code generation, on the other hand, is often unconstrained and can optionally be combined with external tests (Schäfer et al., 2023) and methods such as self-refine (Madaan et al., 2023) in order to minimize hallucination and execution failure.</p>
<p>For example, to verify the hypotheses proposed by the planner, we show DATAVOYAGER's use of independent t-tests to uncover the impact of wealth distributions in two groups on their incarceration probability (Zaw et al., 2016).The results of the independent t-tests for the wealth variables across the two groups (those with and without a criminal record) for the years 1985, 1990, and 1996: (...) T-statistic: 9.7794 (...) Full Example: Figure 17</p>
<p>An ideal system must conduct statistical tests (e.g., correlation, regression, multivariate analyses, t-tests or ANOVA for hypothesis testing, etc.), consume execution results, perform analysis to either conclude or re-plan (Prasad et al., 2023) and support usage of domain-specific evaluation toolkits, such as clinical trials (Rotolo et al., 2018) and climate change (Hoffmann et al., 2021).</p>
<p>The complexity of this task arises from the need to support a plethora of analysis tools (see Figure 5) on diverse datasets through unconstrained code generation.Robust verification, further, must be able to analyze execution output and recover from failed initial generation (Ellis et al., 2020).Verification of program output can be enhanced plots, sub-codes, and numerical analyses, yet despite success in math reasoning (Cobbe et al., 2021), LGMs lack multi-modal symbolic understanding (Lu et al., 2023), calling to action the need for improved data experts in systems like DATAVOYAGER.</p>
<p>Continual Learning.Data-driven discovery is an evolving process.With each stage, from hypothesis generation to evaluation, the system collects new insights and successful (or failed) research flows.The system, thus, requires an adaptive learning approach to integrate and understand the changing context and update its understanding of the dataset (Majumder et al., 2023;Shinn et al., 2023) over time.</p>
<p>For example, execution errors while running generated code or failed research pathways provide opportunities for selfrefinement and possibly integrating learning into the next instances for more fail-proof planning and execution.Continual learning for data-driven discovery opens up research questions regarding the process of online learning (Majumder et al., 2023;Wang et al., 2023a) involving LGMs and avenues to collect supervision signals for continual finetuning (Lin et al., 2022).We argue that how LGMs adapt to novel tools and code at inference time is still an open question and remains critical to data-driven discovery.Despite high degree of natural language fluency, LGMbased systems are often not very proactive.It is desirable for these systems to possess a mixed-initiative ability, thus, optimizing the frequency of asking for human feedback and input (Majumder et al., 2021).Exploring user involvement in the decision-making process raise two questions: (1) Can we achieve an ideal outcome by enabling users to provide input for tasks like setting low-level objectives or summarizing insights?(2) How can we implement effective user intervention during errors or loops to guide the exploration when the system deviates, as raised in (Lahiri et al., 2022)?</p>
<p>Measurement of</p>
<p>Knowledge Integration</p>
<p>Interdisciplinary Knowledge Integration.Integrating interdisciplinary knowledge in data-driven discovery enables the interconnection of diverse domains with the highlevel research objective, uncovering nuanced associations and insights often overlooked in a single-domain analysis.The challenge lies in internalizing the complexities of different disciplines and recognizing implicit connections, similar to link prediction (Trouillon et al., 2016).</p>
<p>For example, while exploring time preference on BMI (Smith et al., 2005), it could be insightful to assess the role of economic pressure on health outcomes, using cultural anthropology to gauge spending habits, considering psychological factors to understand spending patterns, and proposing strategies for public health intervention and effective urban planning-partially achieved by DATAVOYAGER.</p>
<p>Knowledge Frontiers Support.Knowledge frontiers represent cutting edge scientific exploration and drive groundbreaking discoveries in fields like Machine Learning, gene editing, robotics, and renewable energy (Hassabis, 2002).Enhancing data-driven discovery systems by extending exploration, integrating new methods, and collecting more data can facilitate the investigation of novel scientific domains.</p>
<p>To simulate a knowledge frontier, we accessed a popular language agent repository, Reflexion (Shinn et al., 2023), and modified the experiment design following Majumder et al. (2023).The new experimental data was fed to DATAVOY-AGER, which resulted in the following concrete analysis:</p>
<p>Tasks that are more conceptual or require an understanding of complex systems (e.g., genetics, life stages) seem to be areas where the agent can learn and improve.In contrast, tasks that may involve more practical or hands-on activities (e.g., chemistry mixing, freezing) appear to be more challenging for the agent.(...) Full example: Figure 13 We seek to obtain emergent behaviors from curiosity-driven exploration and back-linking to knowledge frontiers (Groth et al., 2021).</p>
<p>We raise an open question to automatically search or generate novel datasets (Brickley et al., 2019) and conduct novel exploration with user moderation, leading to data-driven scientific discovery.</p>
<p>Research Ethics and Fairness</p>
<p>Reproducible Results.Reproducibility stands as cornerstone of the scientific process (Cao et al., 2023).However, persistent challenges in achieving reproducibility across disciplines call for innovative solutions (Magnusson et al., 2023).Complexity arises from the variations in research environments, methodologies, and resource limitations.These factors impede validation and replication of research findings, a phenomenon often evident in fields such as social science, economics, psychology, and biomedicine (Camerer et al., 2018;Collaboration, 2015;Fanelli, 2018).</p>
<p>For example, The Reproducibility Project: Psychology replicated 100 psychology studies and found only 36% of replications to yield significant results, prompting increased awareness and initiatives to enhance reproducibility across scientific disciplines (Collaboration, 2015).The ideal discovery system should ensure that the undertaken research pathways are reproducible.DATAVOYAGER shows a proof-of-concept for automated, reproducible experiments.However, it can be extended towards automatic documentation and code release, thus further improving transparency.</p>
<p>p-hacking Proof.Manipulating data or analyses to find false significance undermines the scientific process, leading to unreliable findings and subsequent slowdown of progress.</p>
<p>For an automated discovery system, this presents a particularly challenging concern and one that can affect it's trustworthiness (Wasserstein &amp; Lazar, 2016).For example, consider a scenario where an automated discovery system explores a large dataset to find potential relationships.phacking might involve tweaking variables or testing multiple hypotheses until a significant result is found (Dunn, 1961).The data-driven discovery opens up the unique case of evaluating a significant number of hypotheses at the same time, presenting opportunities for unintentional p-hacking.With a large hypotheses space, there is more chance for accidental findings.An ideal data-driven discovery system must perform tests to counter false discoveries (Korthauer et al., 2018) to keep the false discovery rate as low as possible.</p>
<p>Limitations of Data-driven Discovery</p>
<p>Hallucinations.</p>
<p>LGM-powered data-discovery struggles with output hallucinations, exacerbated by memorization and superposition issues (Elhage et al., 2022) -most susceptible being hypothesis generation, planning, and output comprehension.This undermines the benefits of automation, necessitating external verification and user moderation.</p>
<p>Cost at scale.In high-throughput fields (e.g., computational biology), it is common to test millions of hypotheses (Korthauer et al., 2019).Extensive reliance on these systems for orchestrating experiments can then incur significant computational costs-highlighting the need for integrated cost-benefit analyses into the discovery systems (Agrawal et al., 2023) using, for instance, predictive hazard functions.</p>
<p>Policy misuse.The autonomous discovery system is always at risk of misuse by bad actors to produce a substantial volume of dubious research to fit a particular agenda (Heaven, 2022).For certain disciplines like social science and economics, this could potentially impact policy-making institutions and result in sub-optimal policies and decisionmaking (Groh et al., 2022).</p>
<p>Legal Implications.Autonomous hypothesis generation and verification, supported by datasets, raise legal challenges around intellectual property rights and authorship (Callison-Burch, 2023) and liability in decision-making processes involving these systems (Farhadi et al., 2023).Defining responsibilities and establishing institutional, legal frameworks to navigate potential suboptimal policies are essential aspects of addressing this challenge.</p>
<p>Underlying Bias.An inherent challenge with the datadriven discovery system involves the potential percolation of bias originating from dual sources-the underlying dataset (Caliskan et al., 2016) and the LGMs (Feng et al., 2023).This introduces the risk of generating hypotheses that reflect and perpetuate existing biases present in the data source being utilized, potentially leading to skewed or unfair insights.</p>
<p>Survey on Related Systems</p>
<p>End-to-end Data-driven Discovery Most previous autonomous data-driven discovery systems, such as Bacon (Langley, 1981;Langley et al., 1984;1983) severely lacked the requisite computational power, restricting their scope with limited discovery of data-driven knowledge.A recent system, CoScientist Boiko et al. (2023), uses LGMs to au- Figure 3. Survey across several dimensions of a proposed data discovery system for several existing automated and semi-automated data analysis and discovery systems such as: MLAgentBench (Huang et al., 2023), CoScientist (Boiko et al., 2023), Bacon (Langley, 1977), DataLume (Gu et al., 2023) tomate some parts of the workflow, however, still requires substantial human intervention (e.g., wet lab experiments) for hypothesis verification, thus not qualifying as a fully autonomous discovery system.DataLume (Gu et al., 2023) fully automates the code generation for data transformation and hypothesis verification; however, do not have modules to support hypothesis search and orchestrating complex science workflows.Gil et al. (2022;2017;2013) as well as Automatic Analysis in WolframAlpha7 prototyped various workflows for conducting science in data-driven ways, however, such prototypes never explored the power of LGMs and are only exhibit limited generalizability to datasets and scientific methods.</p>
<p>AutoML AutoML is a workflow of automatically building optimal machine learning and predictive models.AutoML tools exist in scientific packages like Scikit (Feurer et al., 2015) and also in cloud platforms such as Google Cloud Platform8 , Microsoft Azure9 , and Amazon Web Services10 .Existing AutoML Cloud platform systems mainly focus on black box models, ensuring the models can be served at scale.Despite performing searches over hyperparameter space for optimal model development, these systems cannot comprehend the semantics of the data and hence cannot help with data-driven hypothesis generation, planning, orchestrating research pathways, and knowledge integration.</p>
<p>MLAgentBench (Huang et al., 2023) can be considered as an evolution of AutoML that performs end-to-end machine learning to benchmark AI research agents.MLAgentBench can plan, evaluate hypotheses, and measure progress, but with a focus on optimizing machine learning models, not on discovering new and novel scientific knowledge.</p>
<p>Automated Data Analysis Automated Data Analysis tools are primarily focused on exploring data under a userprovided hypothesis or query (e.g., "plot sales trends for last 12 months", etc.) and often do not have the capability of searching through the hypotheses space as defined by the data.Tools such as PowerBI11 , Tableau12 , and Thoughtspot13 can, however, perform multi-step hypothesis verification using inbuilt data transformation and statistical tools, though the interpretation and consumption of such analysis are left to the user.Spreadsheet tools such as Microsoft Excel and Google Sheets are often part of the scientific workflow as the data analysis and show limited ability for an autonomous data analysis framework even after having Python integration or coding support (Monroy, 2023;Google, 2023).Focus on integrating LGMs into data analysis with known workflows (Perlitz et al., 2022;Chakraborty et al., 2024) and code-first data analysis (Santos et al., 2023) increased recently-however, these are limited to small-scale tables and lack abilities such as the ability to orchestrate research plans, interpret results, measurement of progress, and knowledge integration.</p>
<p>Conclusion</p>
<p>We argue that ongoing ML research on reasoning, planning, code generation, and tool utilization with LGMs can have a significant influence on advancing and accelerating data-driven discovery.Such systems can transform domains overwhelmed with vast amounts of data, including but not limited to observational social sciences, medicine, astronomy, biology, climate science, computational science, consumer science, and social media analytics.</p>
<p>We posit that the time is ripe for advancing data-driven discovery, and that integrating LGMs with tools and user feedback can catalyze notable progress in scientific inquiry.We hope our timely position can increase interest and efforts in developing, debating, and enhancing the vision for an accurate, reliable, and robust system for data-driven discovery.It can help initiate a Cambrian explosion of discovery while promoting speed, reproducibility, and collaboration in scientific research.</p>
<p>Impact Statement</p>
<p>This position paper presents arguments for a goal to advance the field of science by building end-to-end data-driven discovery systems using ML.There are many potential societal consequences of our proposed direction since it involves using large generative models, some of which we cover in our Limitations section, including policy misuse, legal ramifications, and false discovery.On the positive side, our proposed system can advance the rate of discovery, leading to an improved standard of living and social well-being.</p>
<p>Group Agent Chat</p>
<p>Prompt : You interpret scientific queries, devise hypotheses, segment tasks into sequential subtasks with a focus on statistical methods, assign roles to team members, and ensure coordinated progress.(Smith et al., 2005).This figure: Data Understanding -In response to a high-level objective, the system demonstrates the need of understanding the variables before initiating statistical analysis.Moreover, it selects relevant Time Preference variables from the data and infers them (highlighted in green).(Smith et al., 2005).This figure: Data Transformation -The system generated insights from the results of Logistic Regression with L1 regularization.As a response to user input, the system showcases data transformation ability by creating new interaction terms in logistic regression models (as demonstrated in the code snippet), exploring the link between time preference and BMI across diverse demographic groups.(Smith et al., 2005).This figure: Hypothesis Verification -When the user prompted to perform sophisticated analysis to uncover new insights, the system generates new insights utilizing the Generalized Linear Model (highlighted in blue) that confirms the results from the previous OLS analysis (highlighted in green).Figure 14.Background: National Longitudinal Survey of Youth data with a question on how incarceration and race affected wealth was fed to DATAVOYAGER; it is a question studied in (Zaw et al., 2016).This figure: Knowledge Frontiers Support -Despite the original paper talking about wealth analysis post-incarceration and only doing basic statistical analysis over the data, the system was able to suggest new techniques like the application of Gini coefficients -a popular measure used in understanding wealth disparities (highlighted in green).</p>
<p>Figure 15.Background: National Longitudinal Survey of Youth data with a question on how incarceration and race affected wealth was fed to DATAVOYAGER; it is a question studied in (Zaw et al., 2016).This figure: Hypothesis Verification -After calculating Wealth Inequality across demographic groups using gini coefficients, the system interpreted the results (highlighted in green) and generated interesting insights (highlighted in blue).</p>
<p>Figure 16.Background: National Longitudinal Survey of Youth data with a question on how incarceration and race affected wealth was fed to DATAVOYAGER; it is a question studied in (Zaw et al., 2016).This figure: Human Feedback Accommodation -The system performed OLS Regression, suggested the presence of multicollinearity, and removed it using Variation Inflation Factor.Then, the system was set to address multicollinearity again, but user intervention prevented redundancy and redirected it to the objective (highlighted in green).(Alexander et al., 1982) This figure: Multi-step planning for hypothesis generation -following an initial analysis of data statistics and considering the specified goal, the system formulates a detailed plan to guide the hypothesis generation process.It generates a possible list of hypotheses (highlighted in green) and the core experimental loop (highlighted in blue.)</p>
<p>Figure 19.Background: Data from 1979 follow-up wave of the National Longitudinal Survey along with the question on how social background affects degree completion was fed to DATAVOYAGER; it is a research covered in (Alexander et al., 1982) This figure: Hypothesis Generation -The system conducts new experiments to comprehend SES data and generates hypotheses.Building upon initial statistical tests, the model delves deeply into proposing and conducting more sophisticated experiments (highlighted in green), subsequently formulating several hypotheses for further analysis (highlighted in blue).</p>
<p>Healt h Economics: Job status and income levels can affect health ... Psychology and Behavioral Economics: Stress, self-control influence saving habits and BMI ... Sociology and Cult ural St udies: Cultural norms and societal expectations can affect BMI ... Please connect BMI with graduation, family &amp; demographic data, run more sophisticated model.1. SES: Compare association between subject variables based on SES 2trasnform the data by adding int eract ion variables Measure effects using Generalized Linear Model on 'SES', 'SAMPLE_SEX', 'SAMPLE_RACE', 'AVSAB Scores' and 'Class Percentile'</p>
<p>Figure 2 .
2
Figure 2.An example workflow of DATAVOYAGER.Starting from a user-provided dataset and a high-level query, it navigates through cycles of hypothesis generation, validation, and analysis to uncover complex insights.See all examples in Appendix for full understanding.</p>
<p>I.</p>
<p>Understand the data (...) II.Generate initial hypotheses (...) III.Explore combinations of dependent variables (...) IV.Call the "run_logistic_regression" function (...) V. Repeat step IV for other combinations of dependent variables (...) VI.Document the findings (...) VII.Seek clarity where required (...).Full example: Figure18</p>
<p>Figure 6 .
6
Figure 6.Background: Data from National Longitudinal Survey of Youth along with question on relation between time preference &amp; BMI was fed into DATAVOYAGER; it is a question studied in(Smith et al., 2005).This figure: Data Understanding -In response to a high-level objective, the system demonstrates the need of understanding the variables before initiating statistical analysis.Moreover, it selects relevant Time Preference variables from the data and infers them (highlighted in green).</p>
<p>Figure 7 .
7
Figure 7. Background: Data from National Longitudinal Survey of Youth along with question on relation between time preference &amp; BMI was fed into DATAVOYAGER; it is a question studied in(Smith et al., 2005).This figure: Data Transformation -The system generated insights from the results of Logistic Regression with L1 regularization.As a response to user input, the system showcases data transformation ability by creating new interaction terms in logistic regression models (as demonstrated in the code snippet), exploring the link between time preference and BMI across diverse demographic groups.</p>
<p>Figure 8 .
8
Figure 8. Background: Data from National Longitudinal Survey of Youth along with question on relation between time preference &amp; BMI was fed into DATAVOYAGER; it is a question studied in(Smith et al., 2005).This figure: Interdisciplinary Knowledge Integration -The system extracts insights from BMI data, generates insights (highlighted in yellow) from the lens of different disciplines and integrates them into different interdisciplinary hypotheses for further exploration (highlighted in green).</p>
<p>Figure 9 .
9
Figure 9. Background: Data from National Longitudinal Survey of Youth along with question on relation between time preference &amp; BMI was fed into DATAVOYAGER; it is a question studied in(Smith et al., 2005).This figure: Hypothesis Verification -When the user prompted to perform sophisticated analysis to uncover new insights, the system generates new insights utilizing the Generalized Linear Model (highlighted in blue) that confirms the results from the previous OLS analysis (highlighted in green).</p>
<p>Figure 10 .
10
Figure10.Background: Experimental data of running methods using a popular agent-based repo Reflexion (https://github.com/noahshinn/reflexion) and(Majumder et al., 2023) was fed to DATAVOYAGER.This figure: Knowledge Frontiers Support -Data Expert suggested interesting list of analyses to find new insights.New analyses (highlighted in green) were created with limited context on the data just based on variable description.Cluster analysis (highlighted in blue) leads to novel insights in the agent literature.</p>
<p>Figure 11 .
11
Figure11.Background: Experimental data of running methods using a popular agent-based repo Reflexion (https://github.com/noahshinn/reflexion) and(Majumder et al., 2023) was fed to DATAVOYAGER.This figure: Multi-Step Planning -The system understood the variables and carved out the steps that need to be performed to draw interesting insights (highlighted in blue).The Planner created an excellent plan by breaking the objective into subtasks to carry-out learning progression analysis.It then assigned the subtask to a team member (highlighted in yellow).</p>
<p>Figure 12 .
12
Figure12.Background: Experimental data of running methods using a popular agent-based repo Reflexion (https://github.com/noahshinn/reflexion) and(Majumder et al., 2023) was fed to DATAVOYAGER.This figure: Hypothesis Generation -When asked to generate hypotheses and perform sophisticated analysis based on insights (highlighted in green), the system generates testable hypotheses, formulating clear null (H0) and alternative (H1) hypotheses with corresponding statistical tests (highlighted in yellow) for uncovering complex underlying patterns in the data.</p>
<p>Figure 13 .
13
Figure 13.Background: Experimental data of running methods using a popular agent-based repo Reflexion (https://github.com/noahshinn/reflexion) and (Majumder et al., 2023) was fed to DATAVOYAGER.This figure: Knowledge Frontiers Support -Ability to support and generate new insights on frontiers of knowledge, where novel insights on agents' behavior were generated.The new insights are highlighted in blue.</p>
<p>Figure 17 .
17
Figure17.Background: National Longitudinal Survey of Youth data with a question on how incarceration and race affected wealth was fed to DATAVOYAGER; it is a question studied in(Zaw et al., 2016).This figure: Hypothesis Verification -Following the results of Descriptive Statistics, the Data Expert proposed two hypotheses.When the user prompted the system to perform Hypothesis Testing it verified them by performing T-tests, interpreted them (highlighted in green) and shared the conclusions.</p>
<p>Figure 18 .
18
Figure18.Background: Data from 1979 follow-up wave of the National Longitudinal Survey along with the question on how social background affects degree completion was fed to DATAVOYAGER; it is a research covered in(Alexander et al., 1982) This figure: Multi-step planning for hypothesis generation -following an initial analysis of data statistics and considering the specified goal, the system formulates a detailed plan to guide the hypothesis generation process.It generates a possible list of hypotheses (highlighted in green) and the core experimental loop (highlighted in blue.)</p>
<p>Time preference could be 'DISSAVED' and 'SAMESAVE' variables.1.Initial Hypotheses: a. Hypot hesis 1: DISSAVED and BMI are related... 2. Perform OLS &amp; Correlation analysis ....</p>
<p>, ThoughtSpot (thoughtspot.com),Google AutoML (cloud.google.com/automl),and Automatic Analysis* from WolframAlpha (wolframalpha.com/examples/pro-features/data-input).</p>
<p>Agent Structure for DATAVOYAGER.Group Agent Chat has AutoGen agents that communicate with each other.The User Proxy links the user with the agents to share data, feedback, and goals.Code Execution Environment has access to structured functions and code generation methods that can be called depending on the context.
Aut oGen User ProxyCode Execut ion Envsyst em_message=" An admi nt hat t akes i nput f r om t het i meoutuser . "cache_seedconf i g_l i stt er mi nat i on_cr i t er i acode_execut i on_conf i gf unct i ons_f or _pyt hon_cel l ( )max_consecut i ve_aut o_r epl yl l m_conf i gf unct i ons_f or _shel l ( )human_i nput _modest at s_f unct i ons( )i ni t i at e_chat =" St ar t i ngmessage f or t hatgoal _pr edi ct or ( )exper i ment "PlannerDat a ExpertProgrammerCrit ic*Prompt : You specialize in analyzing statistical data and user queries, offering detailed inferences, formulating and testing hypotheses, and collaborating with programmers for sophisticated data modeling and inferential insights.Prompt : You develop and code tasks assigned by the Planner, utilizing specific function calls and adhering to guidelines to produce outputs in JSON format, with a focus on robust coding and comprehensive logging. t ool s code genPrompt : As the Critic, you evaluate and assure the quality of research processes and outcomes, scrutinize research methodologies, assess data quality, and provide constructive feedback on analytical methods and findings.Figure 4.
https://www.bls.gov/nls/
https://microsoft.github.io/autogen/
We develop several functions (e.g., statistical analysis tools based on datatype, python shell execution tools) for robustness.
Time preference reflects how individuals value present over future benefits. A lower time preference can lead to higher savings, better food consumption, and thus a healthier BMI in the future.
www.whitehouse.gov/cancermoonshot/
https://www.wolframalpha.com/examples/ pro-features/data-input
 cloud.google.com/automl <br />
azure.microsoft.com/en-us/products/ machine-learning/automatedml/
aws.amazon.com/machine-learning/automl/
https://www.microsoft.com/en-us/powerplatform/products/power-bi
12 https://www.tableau.
com/ 13 https://www.thoughtspot.com/
AcknowledgmentsWe sincerely thank Abhijeetsingh Meena, Aryan Prakhar, and Tirth Vora for their engineering and exploration efforts in making DATAVOYAGER.We also thank Peter Jansen, David Wadden, Yoav Golberg, and Daniel Weld for their useful comments.We thank Siddharth Sharma and Siddharth Narayanan for their help with proofreading.
. O J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, R Avila, I Babuschkin, S Balaji, V Balcom, P Baltescu, H Bao, M Bavarian, J Belgum, I Bello, J Berdine, G Bernadett-Shapiro, C Berner, L Bogdonoff, O Boiko, M Boyd, A.-L Brakman, G Brockman, T Brooks, M Brundage, K Button, T Cai, R Campbell, A Cann, B Carey, C Carlson, R Carmichael, B Chan, C Chang, F Chantzis, D Chen, S Chen, R Chen, J Chen, M Chen, B Chess, C Cho, C Chu, H W Chung, D Cummings, J Currier, Y Dai, C Decareaux, T Degry, N Deutsch, D Deville, A Dhar, D Dohan, S Dowling, S Dunning, A Ecoffet, A Eleti, T Eloundou, D Farhi, L Fedus, N Felix, S P Fishman, J Forte, I Fulford, L Gao, E Georges, C Gibson, V Goel, T Gogineni, G Goh, R Gontijo-Lopes, J Gordon, M Grafstein, S Gray, R Greene, J Gross, S S Gu, Y Guo, C Hallacy, J Han, J Harris, Y He, M Heaton, J Heidecke, C Hesse, A Hickey, W Hickey, P Hoeschele, B Houghton, K Hsu, S Hu, X Hu, J Huizinga, S Jain, S Jain, J Jang, A Jiang, R Jiang, H Jin, D Jin, S Jomoto, B Jonn, H Jun, T Kaftan, L Kaiser, A Kamali, I Kanitscheider, N S Keskar, T Khan, L Kilpatrick, J W Kim, C Kim, Y Kim, H Kirchner, J R Kiros, M Knight, D Kokotajlo, L Kondraciuk, A Kondrich, A Konstantinidis, K Kosic, G Krueger, V Kuo, M Lampe, I Lan, T Lee, J Leike, J Leung, D Levy, C M Li, R Lim, M Lin, S Lin, M Litwin, T Lopez, R Lowe, P Lue, A A Makanju, K Malfacini, S Manning, T Markov, Y Markovski, B Martin, K Mayer, A Mayne, B Mc-Grew, S M Mckinney, C Mcleavey, P Mcmillan, J Mcneil, D Medina, A Mehta, J Menick, L Metz, A Mishchenko, P Mishkin, V Monaco, E Morikawa, D P Mossing, T Mu, M Murati, O Murk, D M'ely, A Nair, R Nakano, R Nayak, A Neelakantan, R Ngo, H Noh, O Long, C O'keefe, J W Pachocki, A Paino, J Palermo, A Pantuliano, G Parascandolo, J Parish, E Parparita, A Passos, M Pavlov, A Peng, A Perelman, F De Avila Belbute Peres, M Petrov, H P De Oliveira Pinto, M Pokorny, M Pokrass, V H Pong, T Powell, A Power, B Power, E Proehl, R Puri, A Radford, J Rae, A Ramesh, C Raymond, F Real, K Rimbach, C Ross, B Rotsted, H Roussez, N Ryder, M D Saltarelli, T Sanders, S Santurkar, G Sastry, H Schmidt, D Schnurr, J Schulman, D Selsam, K Sheppard, T Sherbakov, J Shieh, S Shoker, P Shyam, S Sidor, E Sigler, M Simens, J Sitkin, K Slama, I Sohl, B D Sokolowsky, Y Song, N Staudacher, F P Such, N Summers, I Sutskever, J Tang, N A Tezak, M Thompson, P Tillet, A Tootoonchian, E Tseng, P Tuggle, N Turley, J Tworek, J F C Uribe, A Vallone, A Vijayvergiya, C Voss, C Wainwright, J J Wang, A Wang, B Wang, J Ward, J Wei, C Weinmann, A Welihinda, P Welinder, J Weng, L Weng, M Wiethoff, D Willner, C Winter, S Wolrich, H Wong, L Workman, S Wu, J Wu, M Wu, K Xiao, T Xu, S Yoo, K Yu, Q Yuan, W Zaremba, R Zellers, C Zhang, M Zhang, S Zhao, T Zheng, J Zhuang, W Zhuk, B Zoph, 2023257532815Gpt-4 technical report</p>
<p>Bring your own kg: Self-supervised program synthesis for zero-shot kgqa. D Agarwal, R Das, S Khosla, R Gangadharaiah, ArXiv, abs/2311.078502023</p>
<p>Artificial intelligence and scientific discovery: A model of prioritized search. A Agrawal, J Mchale, A Oettl, SSRN Electronic Journal. 2023</p>
<p>Social background, academic resources, and college graduation: Recent evidence from the national longitudinal survey. K L Alexander, C Riordan, J Fennessey, A M Pallas, American Journal of Education. 9041982</p>
<p>The end of theory: The data deluge makes the scientific method obsolete. Wired magazine. C Anderson, 200816</p>
<p>Prioritizing attention in fast data. P Bailis, E Gan, S Madden, D Narayanan, K Rong, S Suri, Macrobase, Proceedings of the 2017 ACM International Conference on Management of Data. the 2017 ACM International Conference on Management of Data2017</p>
<p>Conceptual biology, hypothesis discovery, and text mining: Swanson's legacy. T Bekhuis, Biomedical digital libraries. 32006</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Artificial intelligence in science: An emerging general method of invention. S Bianchini, M Müller, P Pelletier, Research Policy. 51101046042022</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 6242664320592023</p>
<p>An ontology-driven framework for data transformation in scientific workflows. S Bowers, B Ludäscher, International Workshop on Data Integration in the Life Sciences. Springer2004</p>
<p>Google dataset search: Building a search engine for datasets in an open web ecosystem. D Brickley, M Burgess, N Noy, The World Wide Web Conference. 201986688027</p>
<p>Y Burda, H Edwards, A J Storkey, O Klimov, Exploration by random network distillation. </p>
<p>Large language models as tool makers. T Cai, X Wang, T Ma, X Chen, D Zhou, ArXiv, abs/2305.171262018. 2023</p>
<p>Semantics derived automatically from language corpora contain human-like biases. A Caliskan, J J Bryson, A Narayanan, Science. 3562016</p>
<p>Understanding generative artificial intelligence and its relationship to copyright. Testimony before The U.S. House of Representatives Judiciary Committee. C Callison-Burch, Hearing on Artificial Intelligence and Intellectual Property: Part I -Interoperability of AI and Copyright Law. May 2023Subcommittee on Courts, Intellectual Property, and the Internet</p>
<p>Evaluating the replicability of social science experiments in nature and science between. C Camerer, A Dreber, F Holzmeister, T.-H Ho, J Huber, M Johannesson, M Kirchler, G Nave, B A Nosek, T Pfeiffer, A Altmejd, N Buttrick, T Chan, Y Chen, E Forsell, A Gampa, E Heikensten, L Hummer, T Imai, S Isaksson, D Manfredi, J Rose, E Wagenmakers, H Wu, Nature Human Behaviour. 2010. 2015. 20182</p>
<p>The rise of open science: Tracking the evolution and perceived value of data and methods link-sharing practices. H Cao, J Dodge, K Lo, D A Mcfarland, L L Wang, ArXiv, abs/2310.031932023</p>
<p>Navigator: A gen-ai system for discovery of factual and predictive insights on domain-specific tabular datasets. A Chakraborty, A Banerjee, S Dasgupta, V Raturi, A Soni, A Gupta, S Harsola, V T Subrahmaniam, Proceedings of the 7th Joint International Conference on Data Science &amp; Management of Data (11th ACM IKDD CODS and 29th COMAD). the 7th Joint International Conference on Data Science &amp; Management of Data (11th ACM IKDD CODS and 29th COMAD)2024</p>
<p>Augmenting citations in scientific papers with persistent and personalized historical context. J C Chang, A X Zhang, J Bragg, A Head, K Lo, D Downey, D S Weld, Citesee, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023256868353</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, ArXiv, abs/2110.141682021239998651</p>
<p>Reproducibility project: Psychology. O S Collaboration, 10.17605/OSF.IO/EZCUJ2015</p>
<p>Explaining answers with entailment trees. B Dalvi, P A Jansen, O Tafjord, Z Xie, H Smith, L Pipatanangkura, P Clark, Conference on Empirical Methods in Natural Language Processing. 2021233297051</p>
<p>Multiple comparisons among means. O J Dunn, Journal of the American Statistical Association. 196156</p>
<p>Toy models of superposition. N Elhage, T Hume, C Olsson, N Schiefer, T Henighan, S Kravec, Z Hatfield-Dodds, R Lasenby, D Drain, C Chen, R Grosse, S Mccandlish, J Kaplan, D Amodei, M Wattenberg, C Olah, Transformer Circuits Thread. 2022</p>
<p>Conceptions of good science in our datarich world. K C Elliott, K S Cheruvelil, G M Montgomery, P A Soranno, BioScience. 66102016</p>
<p>Dreamcoder: growing generalizable, interpretable knowledge with wake-sleep bayesian program learning. K Ellis, C Wong, M Nye, M Sablé-Meyer, L Cary, L Morales, L Hewitt, A Solar-Lezama, J B Tenenbaum, Philosophical Transactions of the Royal Society A. 3812020</p>
<p>Diversity is all you need: Learning skills without a reward function. B Eysenbach, A Gupta, J Ibarz, S Levine, International Conference on Learning Representations. 2019</p>
<p>Opinion: Is science really facing a reproducibility crisis, and do we need it to?. D Fanelli, Proceedings of the National Academy of Sciences. 11546398562018</p>
<p>AI2's Response to the US Copyright Requence for Comments on Artificial Intelligence and Copyright. A Farhadi, D Atkinson, C Callison-Burch, N Decario, J Dumas, K Lo, L Soldiani, 2023-6, 2023US Copyright Office Docket NoComment</p>
<p>From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair nlp models. S Feng, C Y Park, Y Liu, Y Tsvetkov, ArXiv, abs/2305.082832023258686693</p>
<p>Efficient and robust automated machine learning. Advances in neural information processing systems. M Feurer, A Klein, K Eggensperger, J Springenberg, M Blum, F Hutter, 201528</p>
<p>Serendipity and information seeking: an empirical study. A Foster, N Ford, Journal of documentation. 5932003</p>
<p>Using semantic workflows to disseminate best practices and accelerate discoveries in multi-omic data analysis. Y Gil, S K Mcweeney, C E Mason, AAAI Conference on Artificial Intelligence. 2013</p>
<p>Towards continuous scientific data analysis and hypothesis evolution. Y Gil, D Garijo, V Ratnakar, R Mayani, R Adusumilli, H Boyce, A Srivastava, P Mallick, AAAI Conference on Artificial Intelligence. 2017</p>
<p>Towards capturing scientific reasoning to automate data analysis. Y Gil, D Khider, M Osorio, V Ratnakar, H Vargas, D Garijo, CorpusID:248914202. Google. Introducing duet ai for google workspace. 2022. 2023</p>
<p>Human detection of political speech deepfakes across transcripts, audio, and video. M Groh, A Sankaranarayanan, N Singh, D Y Kim, A Lippman, R W Picard, 2022259342907</p>
<p>Is curiosity all you need? on the utility of emergent behaviours from curious exploration. O Groth, M Wulfmeier, G Vezzani, V Dasagi, T Hertweck, R Hafner, N M O Heess, M A Riedmiller, ArXiv, abs/2109.086032021</p>
<p>How do data analysts respond to ai assistance? a wizard-of-oz study. K Gu, M Grunde-Mclaughlin, A M Mcnutt, J Heer, T Althoff, ArXiv, abs/2309.101082023</p>
<p>Using ai to accelerate scientific discovery. D Hassabis, 2002</p>
<p>Why meta's latest large language model survived only three days online. W D Heaven, MIT Technology Review. Last. accessed December, 15:2022, 2022</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D X Song, J Steinhardt, ArXiv, abs/2009.033002020221516475</p>
<p>Entropy search for informationefficient global optimization. P Hennig, C J Schuler, Journal of Machine Learning Research. 1362012</p>
<p>A python package to calculate the olr-based index of the madden-julianoscillation (omi) in climate science and weather forecasting. C G Hoffmann, G N Kiladis, M Gehne, C Savigny, Journal of Open Research Software. 2365866552021</p>
<p>Large language models for software engineering: A systematic literature review. X Hou, Y Zhao, Y Liu, Z Yang, K Wang, L Li, X Luo, D Lo, J C Grundy, H Wang, ArXiv, abs/2308.106202023</p>
<p>Vime: Variational information maximizing exploration. R Houthooft, X Chen, Y Duan, J Schulman, F De Turck, P Abbeel, Advances in neural information processing systems. 292016</p>
<p>Benchmarking large language models as ai research agents. Q Huang, J Vora, P Liang, J Leskovec, ArXiv, abs/2310.033022023</p>
<p>Highly accurate protein structure prediction with alphafold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, Nature. 59678732021</p>
<p>Llms can't plan, but can help planning in llm-modulo frameworks. D Kahneman, Thinking, S Kambhampati, K Valmeekam, L Guan, K Stechly, M Verma, S Bhambri, L Saldyt, A Murthy, arXiv:2402.018172024arXiv preprint</p>
<p>Optimizing deep cnn-based queries over video streams at scale. D Kang, J Emmons, F Abuzaid, P D Bailis, M A Zaharia, Noscope, Proc. VLDB Endow. VLDB Endow201710</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. T Khot, H Trivedi, M Finlayson, Y Fu, K Richardson, P Clark, A Sabharwal, ArXiv, abs/2210.024062022252715485</p>
<p>A practical guide to methods controlling false discoveries in computational biology. K Korthauer, P K Kimes, C Duvallet, A Reyes, A Subramanian, M Teng, C Shukla, E J Alm, S C Hicks, 10.1186/s13059-019-1716-1Genome Biology. 202019</p>
<p>A practical guide to methods controlling false discoveries in computational biology. K D Korthauer, P K Kimes, C Duvallet, A Reyes, A Subramanian, M Teng, C J Shukla, E J Alm, S C Hicks, Genome Biology. 202018</p>
<p>Interactive code generation via testdriven user-intent formalization. S K Lahiri, A Naik, G Sakkas, P Choudhury, C Veh, M Musuvathi, J P Inala, C Wang, J Gao, ArXiv, abs/2208.059502022251492970</p>
<p>A production system that discovers empirical laws. P Langley, Bacon, International Joint Conference on Artificial Intelligence. 19772320342</p>
<p>Data-driven discovery of physical laws. P Langley, Cogn. Sci. 5396942511981</p>
<p>Rediscovering chemistry with the bacon system. P Langley, G L Bradshaw, H A Simon, 1983</p>
<p>The search for regularity: Four aspects of scientific discovery. P Langley, J M Zytkow, H A Simon, G L Bradshaw, 19843155192</p>
<p>The cancer genomics cloud: collaborative, reproducible, and democratized-a new paradigm in large-scale computational research. J W Lau, E Lehnert, A Sethi, R Malhotra, G Kaushik, Z Onder, N Groves-Kirkby, A Mihajlovic, J Digiovanna, M Srdic, Cancer research. 77212017</p>
<p>A path towards autonomous machine intelligence version 0. Y Lecun, Open Review. 912022</p>
<p>Challenges in high-throughput inorganic material prediction and autonomous synthesis. J Leeman, Y Liu, J Stiles, S Lee, P Bhatt, L Schoop, R Palgrave, 2024</p>
<p>Competition-level code generation with alphacode. Y Li, D H Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, Tom, Eccles, J Keeling, F Gimeno, A D Lago, T Hubert, P Choy, C De, M D'autume, I Babuschkin, X Chen, P.-S Huang, J Welbl, S Gowal, Alexey, Cherepanov, J Molloy, D J Mankowitz, E S Robson, P Kohli, N De, Freitas, K Kavukcuoglu, O Vinyals, Science. 3782022</p>
<p>On continual model refinement in out-of-distribution data streams. B Y Lin, S I Wang, X V Lin, R Jia, L Xiao, X Ren, W Yih, Annual Meeting of the Association for Computational Linguistics. 2022</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, ArXiv, abs/2304.084852023a</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. J Liu, C Xia, Y Wang, L Zhang, ArXiv, abs/2305.012102023b258437095</p>
<p>X Liu, H Yu, H Zhang, Y Xu, X Lei, H Lai, Y Gu, Y Gu, H Ding, K Men, K Yang, S Zhang, X Deng, A Zeng, Z Du, C Zhang, S Shen, T Zhang, Y Su, H Sun, M Huang, Y Dong, J Tang, ArXiv, abs/2308.03688Agentbench: Evaluating llms as agents. 2023c260682249</p>
<p>Evaluating mathematical reasoning of foundation models in visual contexts. P Lu, H Bansal, T Xia, J Liu, C Yue Li, H Hajishirzi, H Cheng, K.-W Chang, M Galley, J Gao, Mathvista, 2023</p>
<p>Pynguin: Automated unit test generation for python. S Lukasczyk, G Fraser, IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion). 2022. 2022246706202</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, S Welleck, B P Majumder, S Gupta, A Yazdanbakhsh, P Clark, ArXiv, abs/2303.176512023257900871</p>
<p>Reproducibility in nlp: What have we learned from the checklist?. I H Magnusson, N A Smith, J Dodge, Annual Meeting of the Association for Computational Linguistics. 2023</p>
<p>Ask what's missing and what's useful: Improving clarification question generation using global knowledge. B P Majumder, S Rao, M Galley, J Mcauley, North American Chapter. the Association for Computational Linguistics2021</p>
<p>Achieving conversational goals with unsupervised post-hoc knowledge injection. B P Majumder, H Jhamtani, T Berg-Kirkpatrick, J Mcauley, ArXiv, abs/2203.113992022</p>
<p>Clin: A continually learning language agent for rapid task adaptation and generalization. B P Majumder, B Dalvi, P Jansen, O Tafjord, N Tandon, L Zhang, C Callison-Burch, P Clark, ArXiv, abs/2310.101342023</p>
<p>Introducing copilot support for python in excel: Advanced data analysis using natural language. D Monroy, 2023</p>
<p>Computer science as empirical inquiry: symbols and search. A Newell, H A Simon, 10.1145/360018.360022Commun. ACM. 0001-0782193mar 1976</p>
<p>How can we define intrinsic motivation?. P.-Y Oudeyer, F Kaplan, 2008</p>
<p>Intrinsic motivation systems for autonomous mental development. P.-Y Oudeyer, F Kaplan, V V Hafner, IEEE transactions on evolutionary computation. 1122007</p>
<p>Relatedly: Scaffolding literature reviews with existing related work sections. S Palani, A Naik, D Downey, A X Zhang, J Bragg, J C Chang, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023256846632</p>
<p>Curiosity-driven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, International conference on machine learning. PMLR2017</p>
<p>K Pelrine, M Taufeeque, M Zajkac, E Mclean, A Gleave, Exploiting novel gpt-4 apis. </p>
<p>nbiig: A neural bi insights generation system for table reporting. Y Perlitz, D Sheinwald, N Slonim, M Shmueli-Scheuer, AAAI Conference on Artificial Intelligence. 2022</p>
<p>Adapt: As-needed decomposition and planning with language models. A Prasad, A Koller, M Hartmann, P Clark, A Sabharwal, M Bansal, T Khot, ArXiv, abs/2311.057722023</p>
<p>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. L Qiu, L Jiang, X Lu, M Sclar, V Pyatkin, C Bhagavatula, B Wang, Y Kim, Y Choi, N Dziri, X Ren, ArXiv, abs/2310.085592023</p>
<p>Data mining: From serendipity to science. N Ramakrishnan, A Y Grama, Computer. 3281999</p>
<p>Semantic web in data mining and knowledge discovery: A comprehensive survey. P Ristoski, H Paulheim, J. Web Semant. 36428461212016</p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, M Barekatain, A Novikov, M Balog, M P Kumar, E Dupont, F J R Ruiz, J S Ellenberg, P Wang, O Fawzi, P Kohli, A Fawzi, J Grochow, A Lodi, J.-B Mouret, T Ringer, T Yu, Nature. 6252023</p>
<p>An r package for the evaluation of failure time surrogate endpoints in individual patient data meta-analyses of randomized clinical trials. F Rotolo, X Paoletti, S Michiels, Surrosurv, Computer methods and programs in biomedicine. 15534804782018</p>
<p>Pandasprofiling now supports apache spark. M Santos, F Clemente, C Abshire, 2023</p>
<p>An empirical evaluation of using large language models for automated unit test generation. M Schäfer, S Nadi, A Eghbali, F Tip, IEEE Transactions on Software Engineering. 502023</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, J Dwivedi-Yu, R Dessi, R Raileanu, M Lomeli, E Hambro, L Zettlemoyer, N Cancedda, T Scialom, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Automatic data transformation using large language model -an experimental study on building energy data. A Sharma, X Li, H Guan, G Sun, L Zhang, L Wang, K Wu, L Cao, E Zhu, A Sim, T Wu, J Zou, IEEE International Conference on Big Data (BigData). 2023. 2023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, B Labash, A Gopinath, K Narasimhan, S Yao, 2023258833055</p>
<p>Are time preference and body mass index associated?: Evidence from the national longitudinal survey of youth. P K Smith, B Bogin, D Bishai, Economics &amp; Human Biology. 322005</p>
<p>Open-endedness: The last grand challenge you've never heard of. While open-endedness could be a force for discovering intelligence. K O Stanley, J Lehman, L Soros, 2017it could also be a component of AI itself</p>
<p>Sql-palm: Improved large language model adaptation for text-to-sql. R Sun, S Ö Arik, H Nakhost, H Dai, R Sinha, P Yin, T Pfister, ArXiv, abs/2306.007392023258999853</p>
<p>Undiscovered public knowledge. The Library Quarterly. D R Swanson, 198656144270735</p>
<p>The Black Swan: The Impact of the Highly Improbable. Random House Group. N N Taleb, 2007ISBN 1400063515</p>
<p>. H Touvron, L Martin, K R Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D M Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, B Fuller, C Gao, V Goswami, N Goyal, A S Hartshorn, S Hosseini, R Hou, H Inan, M Kardas, V Kerkez, M Khabsa, I M Kloumann, A V Korenev, P S Koura, M.-A Lachaux, T Lavril, J Lee, D Liskovich, Y Lu, Y Mao, X Martinet, T Mihaylov, P Mishra, I Molybog, Y Nie, A Poulton, J Reizenstein, R Rungta, K Saladi, A Schelten, R Silva, E M Smith, R Subramanian, X Tan, B Tang, R Taylor, A Williams, J X Kuan, P Xu, Z Yan, I Zarov, Y Zhang, A Fan, M Kambadur, S Narang, A Rodriguez, R Stojnic, S Edunov, Scialom, ArXiv, abs/2307.0928820232Open foundation and fine-tuned chat models</p>
<p>Solving olympiad geometry without human demonstrations. T H Trinh, Y Wu, Q V Le, H He, T Luong, Nature. 62579952024</p>
<p>Complex embeddings for simple link prediction. T Trouillon, J Welbl, S Riedel, É Gaussier, G Bouchard, ArXiv, abs/1606.063572016</p>
<p>Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. P Vaithilingam, T Zhang, E L Glassman, CHI Conference on Human Factors in Computing Systems Extended Abstracts. 2022247255943</p>
<p>An extensible benchmark for evaluating large language models on planning and reasoning about change. K Valmeekam, A Olmo, S Sreedharan, S Kambhampati, Planbench, 2022</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L J Fan, A Anandkumar, ArXiv, abs/2305.162912023a258887849</p>
<p>Learning to generate novel scientific directions with contextualized literature-based discovery. Q Wang, D Downey, H Ji, T Hope, ArXiv, abs/2305.142592023b258841365</p>
<p>The asa statement on p-values: Context, process, and purpose. R Wasserstein, N A Lazar, The American Statistician. 702016</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, International Conference on Learning Representations. 2022</p>
<p>Overview of issues in the longitudinal analysis of respiratory data. S T Weiss, J H Ware, 199615445049299American journal of respiratory and critical care medicine</p>
<p>Optimal search for the best alternative. M Weitzman, Econometrica. 47325308811978</p>
<p>Fundamental principles of deception in genetic search. L D Whitley, Foundations of genetic algorithms. Elsevier19911</p>
<p>Fundamental limitations of alignment in large language models. Y Wolf, N Wies, Y Levine, A Shashua, ArXiv, abs/2304.110822023</p>
<p>wealth and incarceration: Results from the national longitudinal survey of youth. K Zaw, D Hamilton, W A J Darity, Race, Race and Social Problems. 8137097792016</p>
<p>Open-endedness via models of human notions of interestingness. J Zhang, J Lehman, K Stanley, J Clune, Omni, arXiv:2306.017112023arXiv preprint</p>
<p>Interactive evaluation for social intelligence in language agents. X Zhou, H Zhu, L Mathur, R Zhang, H Yu, Z Qi, L.-P Morency, Y Bisk, D Fried, G Neubig, M Sap, Sotopia, ArXiv, abs/2310.116672023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8642 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8642</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8642</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-273098753</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.02198v1.pdf" target="_blank">G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> We introduce G2T-LLM, a novel approach for molecule generation that uses graph-to-tree text encoding to transform graph-based molecular structures into a hierarchical text format optimized for large language models (LLMs). This encoding converts complex molecular graphs into tree-structured formats, such as JSON and XML, which LLMs are particularly adept at processing due to their extensive pre-training on these types of data. By leveraging the flexibility of LLMs, our approach allows for intuitive interaction using natural language prompts, providing a more accessible interface for molecular design. Through supervised fine-tuning, G2T-LLM generates valid and coherent chemical structures, addressing common challenges like invalid outputs seen in traditional graph-based methods. While LLMs are computationally intensive, they offer superior generalization and adaptability, enabling the generation of diverse molecular structures with minimal task-specific customization. The proposed approach achieved comparable performances with state-of-the-art methods on various benchmark molecular generation datasets, demonstrating its potential as a flexible and innovative tool for AI-driven molecular design.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8642.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8642.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G2T-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that encodes molecular graphs as hierarchical tree-structured text (JSON/XML inspired by SMILES), fine-tunes an LLM to perform molecular completion, and uses token-level constraints at inference to generate chemically valid molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>G2T-LLM (fine-tuned LLaMA3.1-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based Large Language Model (LLM) fine-tuned for sequence generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B (LLaMA3.1-8B base model)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Supervised fine-tuning on 5,000 molecules (randomly selected from dataset training splits; experiments on QM9 and ZINC250k reported); base LLaMA pretraining corpus not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General molecular generation (drug discovery, materials chemistry) — benchmarked on QM9 and ZINC250k datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised fine-tuning cast as a molecular completion task using graph-to-tree (JSON) encoding; inference uses token constraining to enforce chemical/schema-valid tokens while iteratively expanding the tree representation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>High novelty reported: 100% novelty on ZINC250k and 88% on QM9 (as reported in paper tables for generated sets).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>No explicit application-specific optimization (e.g., binding affinity) presented; generation evaluated as general molecule synthesis and distributional similarity to dataset benchmarks (validity/novelty/diversity/FCD/Scaf metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity, Novelty, Fréchet ChemNet Distance (FCD), Scaffold (Scaf) score, NSPDK (MMD), Uniqueness, Tanimoto similarity on Morgan fingerprints (visualization), and dataset-specific metrics (QM9/ZINC250k).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>After fine-tuning, G2T-LLM attains very high validity (~98.6–99.6% depending on dataset/run), high uniqueness, and high novelty (100% on ZINC250k/88% on QM9). Token constraining during inference raised validity from ~41.6% (without constraining) to 98.6% (with constraining). Fine-tuning raised validity from 70.8% (pre-finetune) to 98.6% (post-finetune) in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against graph-based and diffusion methods (MoFlow, GraphAF, GraphDF, EDP-GNN, GDSS, DiGress, GruM) and previous graph-to-text encodings (Talk Like a Graph). G2T-LLM achieved top-two validity across datasets, superior novelty vs many baselines, and competitive or better FCD and Scaf scores; substantially outperformed Talk Like a Graph on ZINC250k across metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>LLMs are computationally intensive; mismatch between graph structure and sequence models requires careful encoding; token constraining is necessary to achieve high validity at inference; fine-tuning data size influences novelty (more fine-tuning data reduced novelty on small/simple QM9); comparisons with some prior LLM-based baselines are complicated by model-size differences (e.g., GPT-4 baselines). The method does not present application-specific objective optimization (e.g., target binding) and relies on supervised samples for learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8642.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8642.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The base large language model used for G2T-LLM fine-tuning; a transformer LLM family model used as the backbone for supervised molecular completion with graph-to-tree encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The llama 3 herd of models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based autoregressive LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Base pretraining corpus not detailed in this paper; fine-tuning performed on 5,000 molecule examples drawn from QM9/ZINC250k training splits for the molecular completion task.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular generation / computational chemistry benchmarks (QM9, ZINC250k)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised fine-tuning with graph-to-tree (JSON) molecular encodings; inference with token constraining to ensure valid tree-structured outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>When fine-tuned within G2T-LLM, generated molecules showed high novelty (100% on ZINC250k, 88% on QM9 reported for the method).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Used as a general generator of molecular graphs; no explicit property-conditioned generation (e.g., target-directed) reported in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity, Novelty, FCD, Scaf, NSPDK, Uniqueness (as reported for the fine-tuned system)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>As the backbone model in G2T-LLM, LLaMA3.1-8B fine-tuned with graph-to-tree encodings produced high-validity and high-novelty molecules; fine-tuning and token constraining were key to achieving high-quality outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Authors note that some prior LLM-based baselines used larger architectures (e.g., GPT-4), making direct comparisons imperfect; within their experiments LLaMA3.1-8B (fine-tuned) achieved competitive results versus state-of-the-art graph/diffusion models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Pretrained LLMs expect sequential/tree-structured inputs and so require careful encoding for graphs; model compute and memory requirements noted (training used A100 80GB; inference on 3090/4090). Fine-tuning set size (5k) may limit generalization in some dataset regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8642.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8642.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TalkLikeAGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Talk Like a Graph: Encoding graphs for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior graph-to-text encoding approach that represents graph structure as natural-language sentences describing node connections and attributes, intended for use with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Talk like a graph: Encoding graphs for large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Talk Like a Graph (graph-to-natural-language encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Natural-language graph encoding (used with LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper; original method encodes graphs into sentences describing nodes and edges.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General graph generation tasks, including molecular graphs when combined with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Encodes graph structure as descriptive natural-language sentences and relies on LLMs to parse/generate those sentences into graph structures.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>In the paper's ZINC250k comparison, Talk Like a Graph produced 100% novelty but low validity (59.20%), indicating many outputs were syntactically novel but chemically invalid.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Not designed to enforce chemical/schema constraints natively; relies on LLMs to learn mapping from natural-language descriptions to graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity, FCD, Scaf, Novelty (used in Table 2 comparison on ZINC250k).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>When compared on ZINC250k, Talk Like a Graph had much lower validity (~59.2%) and worse FCD (19.81) than G2T-LLM (validity ~98.6%, FCD 5.69); authors conclude natural-language encodings are less compatible with LLM pretraining distributions than structured JSON encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Directly compared to G2T-LLM in ablation (Table 2); G2T-LLM outperforms it across key generation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Authors note that natural-language graph encodings are unlikely to match distributions seen in LLM pretraining data, so LLMs struggle to process them effectively; poorer tokenization of molecular specifics and weaker signal for structure compared to tree-structured JSON encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8642.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8642.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMLF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generating novel leads for drug discovery using LLMs with logical feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior LLM-based approach for generating drug leads using logical feedback; discussed as a related work and contrasted with the supervised fine-tuning approach in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating novel leads for drug discovery using llms with logical feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LMLF (LLM pipeline employing logical feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Prompt-engineering / rule-based LLM approach (likely transformer-based LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / lead generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Rule-based prompt engineering with logical feedback guiding molecule proposals (as described in cited work); not used directly in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported in this paper (cited as related work); authors state such rule-based approaches differ fundamentally from their supervised fine-tuning methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designed to guide LLMs toward drug-lead generation using domain rules/feedback in prior work (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed in this paper; mentioned as a contrast to the supervised fine-tuning approach.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as existing LLM application in molecular design; authors note LMLF uses rule-based prompt engineering distinct from G2T-LLM's data-driven SFT approach.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Cited limitations (per authors) of rule-based prompt engineering include restricted ability to learn underlying data distributions and reliance on handcrafted heuristics; also baseline models may be much larger (e.g., GPT-4) complicating comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8642.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8642.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GrammarPrompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grammar Prompting for domain-specific language generation with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rule-based prompting technique that constrains LLM outputs to follow grammar/schema for domain-specific generation; mentioned as a contrasting baseline for molecular generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Grammar prompting for domain-specific language generation with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Grammar Prompting (prompt-engineering technique applied to LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Prompt-guided LLM generation (grammar/schema constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Domain-specific language generation; applied in other works to structured outputs including chemical representations</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Predefined grammar/prompt constraints to guide LLM output (rule-based prompting rather than supervised fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported in this paper; cited as a methodology that differs from G2T-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Aims to enforce schema-consistent output via grammar; can be applied to chemical generation but is distinct from the data-driven fine-tuning used here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned to highlight alternative LLM-based approaches that rely on rules/handcrafted prompting rather than supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Paper notes that such rule-based prompt-engineering approaches restrict ability to learn from data distributions and may be less flexible than SFT-based methods; also baseline model sizes vary which complicates direct comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8642.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8642.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4GraphGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exploring the potential of large language models in graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently-cited study exploring LLM capabilities for graph generation, mentioned as related work and contrasted with the supervised fine-tuning and encoding-based approach of G2T-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the potential of large language models in graph generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM4GraphGen (LLM-based graph generation exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM-based graph generation approaches (transformer LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Graph generation broadly, with implications for molecular graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Investigates LLMs for graph generation, often with rule-based prompt engineering or adaptations; used as comparative literature.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported in this paper; cited as related literature exploring LLMs for graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>General graph generation; the paper positions G2T-LLM as a more structured encoding+SFT alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed here; discussed as part of related work landscape.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited to situate the contribution of G2T-LLM versus other LLM-to-graph strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Cited literature often uses different methodologies (rule-based prompts, larger LLMs), complicating apples-to-apples performance comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8642.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8642.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molx: Enhancing large language models for molecular learning with a multi-modal extension</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-modal extension to LLMs for molecular learning cited by the authors as related recent work exploring LLMs in molecular contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molx: Enhancing large language models for molecular learning with a multi-modal extension.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolX (multi-modal LLM extension for molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Multimodal LLM / molecular foundation model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular learning and generation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Multimodal training/extension of LLMs for molecular tasks (cited as relevant work but not used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designed to improve LLM capability on molecule-specific tasks via multimodal inputs; cited for context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Referenced as an example of recent efforts to adapt LLMs to molecular domains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Not discussed in detail in this paper; listed as related recent work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Talk like a graph: Encoding graphs for large language models. <em>(Rating: 2)</em></li>
                <li>Generating novel leads for drug discovery using llms with logical feedback. <em>(Rating: 2)</em></li>
                <li>Grammar prompting for domain-specific language generation with large language models. <em>(Rating: 2)</em></li>
                <li>Exploring the potential of large language models in graph generation. <em>(Rating: 2)</em></li>
                <li>The llama 3 herd of models. <em>(Rating: 2)</em></li>
                <li>Molx: Enhancing large language models for molecular learning with a multi-modal extension. <em>(Rating: 1)</em></li>
                <li>Moleculargpt: Open large language model (llm) for few-shot molecular property prediction. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8642",
    "paper_id": "paper-273098753",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "G2T-LLM",
            "name_full": "Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models",
            "brief_description": "A pipeline that encodes molecular graphs as hierarchical tree-structured text (JSON/XML inspired by SMILES), fine-tunes an LLM to perform molecular completion, and uses token-level constraints at inference to generate chemically valid molecules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "G2T-LLM (fine-tuned LLaMA3.1-8B)",
            "model_type": "Transformer-based Large Language Model (LLM) fine-tuned for sequence generation",
            "model_size": "8B (LLaMA3.1-8B base model)",
            "training_data": "Supervised fine-tuning on 5,000 molecules (randomly selected from dataset training splits; experiments on QM9 and ZINC250k reported); base LLaMA pretraining corpus not specified in paper.",
            "application_domain": "General molecular generation (drug discovery, materials chemistry) — benchmarked on QM9 and ZINC250k datasets.",
            "generation_method": "Supervised fine-tuning cast as a molecular completion task using graph-to-tree (JSON) encoding; inference uses token constraining to enforce chemical/schema-valid tokens while iteratively expanding the tree representation.",
            "novelty_of_chemicals": "High novelty reported: 100% novelty on ZINC250k and 88% on QM9 (as reported in paper tables for generated sets).",
            "application_specificity": "No explicit application-specific optimization (e.g., binding affinity) presented; generation evaluated as general molecule synthesis and distributional similarity to dataset benchmarks (validity/novelty/diversity/FCD/Scaf metrics).",
            "evaluation_metrics": "Validity, Novelty, Fréchet ChemNet Distance (FCD), Scaffold (Scaf) score, NSPDK (MMD), Uniqueness, Tanimoto similarity on Morgan fingerprints (visualization), and dataset-specific metrics (QM9/ZINC250k).",
            "results_summary": "After fine-tuning, G2T-LLM attains very high validity (~98.6–99.6% depending on dataset/run), high uniqueness, and high novelty (100% on ZINC250k/88% on QM9). Token constraining during inference raised validity from ~41.6% (without constraining) to 98.6% (with constraining). Fine-tuning raised validity from 70.8% (pre-finetune) to 98.6% (post-finetune) in reported experiments.",
            "comparison_to_other_methods": "Compared against graph-based and diffusion methods (MoFlow, GraphAF, GraphDF, EDP-GNN, GDSS, DiGress, GruM) and previous graph-to-text encodings (Talk Like a Graph). G2T-LLM achieved top-two validity across datasets, superior novelty vs many baselines, and competitive or better FCD and Scaf scores; substantially outperformed Talk Like a Graph on ZINC250k across metrics.",
            "limitations_and_challenges": "LLMs are computationally intensive; mismatch between graph structure and sequence models requires careful encoding; token constraining is necessary to achieve high validity at inference; fine-tuning data size influences novelty (more fine-tuning data reduced novelty on small/simple QM9); comparisons with some prior LLM-based baselines are complicated by model-size differences (e.g., GPT-4 baselines). The method does not present application-specific objective optimization (e.g., target binding) and relies on supervised samples for learning.",
            "uuid": "e8642.0",
            "source_info": {
                "paper_title": "G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLaMA3.1-8B",
            "name_full": "LLaMA3.1-8B",
            "brief_description": "The base large language model used for G2T-LLM fine-tuning; a transformer LLM family model used as the backbone for supervised molecular completion with graph-to-tree encodings.",
            "citation_title": "The llama 3 herd of models.",
            "mention_or_use": "use",
            "model_name": "LLaMA3.1-8B",
            "model_type": "Transformer-based autoregressive LLM",
            "model_size": "8B parameters",
            "training_data": "Base pretraining corpus not detailed in this paper; fine-tuning performed on 5,000 molecule examples drawn from QM9/ZINC250k training splits for the molecular completion task.",
            "application_domain": "Molecular generation / computational chemistry benchmarks (QM9, ZINC250k)",
            "generation_method": "Supervised fine-tuning with graph-to-tree (JSON) molecular encodings; inference with token constraining to ensure valid tree-structured outputs.",
            "novelty_of_chemicals": "When fine-tuned within G2T-LLM, generated molecules showed high novelty (100% on ZINC250k, 88% on QM9 reported for the method).",
            "application_specificity": "Used as a general generator of molecular graphs; no explicit property-conditioned generation (e.g., target-directed) reported in experiments.",
            "evaluation_metrics": "Validity, Novelty, FCD, Scaf, NSPDK, Uniqueness (as reported for the fine-tuned system)",
            "results_summary": "As the backbone model in G2T-LLM, LLaMA3.1-8B fine-tuned with graph-to-tree encodings produced high-validity and high-novelty molecules; fine-tuning and token constraining were key to achieving high-quality outputs.",
            "comparison_to_other_methods": "Authors note that some prior LLM-based baselines used larger architectures (e.g., GPT-4), making direct comparisons imperfect; within their experiments LLaMA3.1-8B (fine-tuned) achieved competitive results versus state-of-the-art graph/diffusion models.",
            "limitations_and_challenges": "Pretrained LLMs expect sequential/tree-structured inputs and so require careful encoding for graphs; model compute and memory requirements noted (training used A100 80GB; inference on 3090/4090). Fine-tuning set size (5k) may limit generalization in some dataset regimes.",
            "uuid": "e8642.1",
            "source_info": {
                "paper_title": "G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "TalkLikeAGraph",
            "name_full": "Talk Like a Graph: Encoding graphs for large language models",
            "brief_description": "A prior graph-to-text encoding approach that represents graph structure as natural-language sentences describing node connections and attributes, intended for use with LLMs.",
            "citation_title": "Talk like a graph: Encoding graphs for large language models.",
            "mention_or_use": "use",
            "model_name": "Talk Like a Graph (graph-to-natural-language encoding)",
            "model_type": "Natural-language graph encoding (used with LLMs)",
            "model_size": null,
            "training_data": "Not specified in this paper; original method encodes graphs into sentences describing nodes and edges.",
            "application_domain": "General graph generation tasks, including molecular graphs when combined with LLMs",
            "generation_method": "Encodes graph structure as descriptive natural-language sentences and relies on LLMs to parse/generate those sentences into graph structures.",
            "novelty_of_chemicals": "In the paper's ZINC250k comparison, Talk Like a Graph produced 100% novelty but low validity (59.20%), indicating many outputs were syntactically novel but chemically invalid.",
            "application_specificity": "Not designed to enforce chemical/schema constraints natively; relies on LLMs to learn mapping from natural-language descriptions to graphs.",
            "evaluation_metrics": "Validity, FCD, Scaf, Novelty (used in Table 2 comparison on ZINC250k).",
            "results_summary": "When compared on ZINC250k, Talk Like a Graph had much lower validity (~59.2%) and worse FCD (19.81) than G2T-LLM (validity ~98.6%, FCD 5.69); authors conclude natural-language encodings are less compatible with LLM pretraining distributions than structured JSON encodings.",
            "comparison_to_other_methods": "Directly compared to G2T-LLM in ablation (Table 2); G2T-LLM outperforms it across key generation metrics.",
            "limitations_and_challenges": "Authors note that natural-language graph encodings are unlikely to match distributions seen in LLM pretraining data, so LLMs struggle to process them effectively; poorer tokenization of molecular specifics and weaker signal for structure compared to tree-structured JSON encoding.",
            "uuid": "e8642.2",
            "source_info": {
                "paper_title": "G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LMLF",
            "name_full": "Generating novel leads for drug discovery using LLMs with logical feedback",
            "brief_description": "A prior LLM-based approach for generating drug leads using logical feedback; discussed as a related work and contrasted with the supervised fine-tuning approach in this paper.",
            "citation_title": "Generating novel leads for drug discovery using llms with logical feedback.",
            "mention_or_use": "mention",
            "model_name": "LMLF (LLM pipeline employing logical feedback)",
            "model_type": "Prompt-engineering / rule-based LLM approach (likely transformer-based LLMs)",
            "model_size": null,
            "training_data": null,
            "application_domain": "Drug discovery / lead generation",
            "generation_method": "Rule-based prompt engineering with logical feedback guiding molecule proposals (as described in cited work); not used directly in experiments in this paper.",
            "novelty_of_chemicals": "Not reported in this paper (cited as related work); authors state such rule-based approaches differ fundamentally from their supervised fine-tuning methodology.",
            "application_specificity": "Designed to guide LLMs toward drug-lead generation using domain rules/feedback in prior work (cited).",
            "evaluation_metrics": "Not detailed in this paper; mentioned as a contrast to the supervised fine-tuning approach.",
            "results_summary": "Cited as existing LLM application in molecular design; authors note LMLF uses rule-based prompt engineering distinct from G2T-LLM's data-driven SFT approach.",
            "limitations_and_challenges": "Cited limitations (per authors) of rule-based prompt engineering include restricted ability to learn underlying data distributions and reliance on handcrafted heuristics; also baseline models may be much larger (e.g., GPT-4) complicating comparisons.",
            "uuid": "e8642.3",
            "source_info": {
                "paper_title": "G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GrammarPrompting",
            "name_full": "Grammar Prompting for domain-specific language generation with large language models",
            "brief_description": "A rule-based prompting technique that constrains LLM outputs to follow grammar/schema for domain-specific generation; mentioned as a contrasting baseline for molecular generation.",
            "citation_title": "Grammar prompting for domain-specific language generation with large language models.",
            "mention_or_use": "mention",
            "model_name": "Grammar Prompting (prompt-engineering technique applied to LLMs)",
            "model_type": "Prompt-guided LLM generation (grammar/schema constraints)",
            "model_size": null,
            "training_data": null,
            "application_domain": "Domain-specific language generation; applied in other works to structured outputs including chemical representations",
            "generation_method": "Predefined grammar/prompt constraints to guide LLM output (rule-based prompting rather than supervised fine-tuning).",
            "novelty_of_chemicals": "Not reported in this paper; cited as a methodology that differs from G2T-LLM.",
            "application_specificity": "Aims to enforce schema-consistent output via grammar; can be applied to chemical generation but is distinct from the data-driven fine-tuning used here.",
            "evaluation_metrics": "Not specified in this paper (cited as related work).",
            "results_summary": "Mentioned to highlight alternative LLM-based approaches that rely on rules/handcrafted prompting rather than supervised fine-tuning.",
            "limitations_and_challenges": "Paper notes that such rule-based prompt-engineering approaches restrict ability to learn from data distributions and may be less flexible than SFT-based methods; also baseline model sizes vary which complicates direct comparisons.",
            "uuid": "e8642.4",
            "source_info": {
                "paper_title": "G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLM4GraphGen",
            "name_full": "Exploring the potential of large language models in graph generation",
            "brief_description": "A recently-cited study exploring LLM capabilities for graph generation, mentioned as related work and contrasted with the supervised fine-tuning and encoding-based approach of G2T-LLM.",
            "citation_title": "Exploring the potential of large language models in graph generation.",
            "mention_or_use": "mention",
            "model_name": "LLM4GraphGen (LLM-based graph generation exploration)",
            "model_type": "LLM-based graph generation approaches (transformer LLMs)",
            "model_size": null,
            "training_data": null,
            "application_domain": "Graph generation broadly, with implications for molecular graph generation",
            "generation_method": "Investigates LLMs for graph generation, often with rule-based prompt engineering or adaptations; used as comparative literature.",
            "novelty_of_chemicals": "Not reported in this paper; cited as related literature exploring LLMs for graphs.",
            "application_specificity": "General graph generation; the paper positions G2T-LLM as a more structured encoding+SFT alternative.",
            "evaluation_metrics": "Not detailed here; discussed as part of related work landscape.",
            "results_summary": "Cited to situate the contribution of G2T-LLM versus other LLM-to-graph strategies.",
            "limitations_and_challenges": "Cited literature often uses different methodologies (rule-based prompts, larger LLMs), complicating apples-to-apples performance comparisons.",
            "uuid": "e8642.5",
            "source_info": {
                "paper_title": "G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MolX",
            "name_full": "Molx: Enhancing large language models for molecular learning with a multi-modal extension",
            "brief_description": "A multi-modal extension to LLMs for molecular learning cited by the authors as related recent work exploring LLMs in molecular contexts.",
            "citation_title": "Molx: Enhancing large language models for molecular learning with a multi-modal extension.",
            "mention_or_use": "mention",
            "model_name": "MolX (multi-modal LLM extension for molecules)",
            "model_type": "Multimodal LLM / molecular foundation model",
            "model_size": null,
            "training_data": null,
            "application_domain": "Molecular learning and generation",
            "generation_method": "Multimodal training/extension of LLMs for molecular tasks (cited as relevant work but not used in experiments).",
            "novelty_of_chemicals": "Not reported in this paper.",
            "application_specificity": "Designed to improve LLM capability on molecule-specific tasks via multimodal inputs; cited for context.",
            "evaluation_metrics": "Not specified in this paper.",
            "results_summary": "Referenced as an example of recent efforts to adapt LLMs to molecular domains.",
            "limitations_and_challenges": "Not discussed in detail in this paper; listed as related recent work.",
            "uuid": "e8642.6",
            "source_info": {
                "paper_title": "G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Talk like a graph: Encoding graphs for large language models.",
            "rating": 2,
            "sanitized_title": "talk_like_a_graph_encoding_graphs_for_large_language_models"
        },
        {
            "paper_title": "Generating novel leads for drug discovery using llms with logical feedback.",
            "rating": 2,
            "sanitized_title": "generating_novel_leads_for_drug_discovery_using_llms_with_logical_feedback"
        },
        {
            "paper_title": "Grammar prompting for domain-specific language generation with large language models.",
            "rating": 2,
            "sanitized_title": "grammar_prompting_for_domainspecific_language_generation_with_large_language_models"
        },
        {
            "paper_title": "Exploring the potential of large language models in graph generation.",
            "rating": 2,
            "sanitized_title": "exploring_the_potential_of_large_language_models_in_graph_generation"
        },
        {
            "paper_title": "The llama 3 herd of models.",
            "rating": 2,
            "sanitized_title": "the_llama_3_herd_of_models"
        },
        {
            "paper_title": "Molx: Enhancing large language models for molecular learning with a multi-modal extension.",
            "rating": 1,
            "sanitized_title": "molx_enhancing_large_language_models_for_molecular_learning_with_a_multimodal_extension"
        },
        {
            "paper_title": "Moleculargpt: Open large language model (llm) for few-shot molecular property prediction.",
            "rating": 1,
            "sanitized_title": "moleculargpt_open_large_language_model_llm_for_fewshot_molecular_property_prediction"
        }
    ],
    "cost": 0.01380475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>G2T-LLM: GRAPH-TO-TREE TEXT ENCODING FOR MOLECULE GENERATION WITH FINE-TUNED LARGE LANGUAGE MODELS
3 Oct 2024</p>
<p>Zhaoning Yu znyu@iastate.edu 
Iowa State University Ames
50010IA</p>
<p>Xiangyang Xu xyxu@iastate.edu 
Iowa State University Ames
50010IA</p>
<p>Hongyang Gao hygao@iastate.edu 
Iowa State University Ames
50010IA</p>
<p>G2T-LLM: GRAPH-TO-TREE TEXT ENCODING FOR MOLECULE GENERATION WITH FINE-TUNED LARGE LANGUAGE MODELS
3 Oct 20248CF3DBE6AFF3318F3EB5333BC6BEB188arXiv:2410.02198v1[cs.LG]
We introduce G2T-LLM, a novel approach for molecule generation that uses graph-to-tree text encoding to transform graph-based molecular structures into a hierarchical text format optimized for large language models (LLMs).This encoding converts complex molecular graphs into tree-structured formats, such as JSON and XML, which LLMs are particularly adept at processing due to their extensive pre-training on these types of data.By leveraging the flexibility of LLMs, our approach allows for intuitive interaction using natural language prompts, providing a more accessible interface for molecular design.Through supervised finetuning, G2T-LLM generates valid and coherent chemical structures, addressing common challenges like invalid outputs seen in traditional graph-based methods.While LLMs are computationally intensive, they offer superior generalization and adaptability, enabling the generation of diverse molecular structures with minimal task-specific customization.The proposed approach achieved comparable performances with state-of-the-art methods on various benchmark molecular generation datasets, demonstrating its potential as a flexible and innovative tool for AI-driven molecular design.</p>
<p>INTRODUCTION</p>
<p>Molecular generation is a critical task in fields such as drug discovery, material science, and chemistry (Schneider &amp; Fechner, 2005;Simonovsky &amp; Komodakis, 2018;Elton et al., 2019).The ability to design and create novel molecules with specific properties can accelerate the development of new therapies, advanced materials, and innovative chemicals.Traditional approaches to molecular generation, such as rule-based systems (Schneider &amp; Fechner, 2005;Sastry et al., 2011) and graphbased (You et al., 2018;Madhawa et al., 2019;Shi et al., 2020) models, have provided foundational tools.However, these methods often face limitations in generating diverse, valid, and chemically coherent molecular structures, restricting their ability to explore the vast chemical space effectively (Vignac et al., 2022;Jo et al., 2022).Recent advancements in deep learning, especially the rise of large language models (LLMs), offer new opportunities for molecular generation (Brahmavar et al., 2024;Wang et al., 2024;Yao et al., 2024).Unlike traditional methods, LLMs are not constrained by domain-specific rules and can generalize from vast amounts of data.This flexibility allows them to generate creative and diverse content, potentially uncovering novel chemical compounds.Prior non-LLM approaches, such as graph-based generative models (You et al., 2018;Madhawa et al., 2019;Shi et al., 2020;Luo et al., 2021;Vignac et al., 2022;Jo et al., 2022), often struggle with limited generalization, rule-based rigidity, or difficulty scaling to more complex chemical structures.In contrast, LLMs can adapt to a wide range of prompts and provide greater flexibility, making them an attractive choice for AI-driven molecular generation.</p>
<p>Despite the promise of LLMs, applying them to molecular generation presents a unique challenge.Molecular structures are typically represented as graphs, with atoms as nodes and bonds as edges.LLMs, however, are trained to understand sequences of tokens (Vaswani, 2017), particularly in structured text formats such as XML and JSON (Brown, 2020), and are not inherently designed to process graph-based data.This mismatch creates a barrier when attempting to use LLMs for tasks that require understanding the relational and non-linear properties of molecular structures.LLMs • We achieve comparable performances with state-of-the-art models on benchmark molecular generation datasets, demonstrating the effectiveness and potential of our approach for AI-driven molecular design.</p>
<p>RELATED WORK</p>
<p>Graph Generation.The graph generation task aims to learn the distribution of graphs.The traditional approaches (Zang &amp; Wang, 2020;Shi et al., 2020;Luo et al., 2021;You et al., 2018;Madhawa et al., 2019;Dai et al., 2018) such as auto-regression, Generative Adversarial Network (GAN), and Variational Autoencoder (VAE) have been explored for this purpose.However, they have faced challenges in modeling the permutation-invariant nature of graph distribution and learning the relationship between edges and nodes, often due to limitations in their model capacity.Recent advancements in diffusion methods (Niu et al., 2020;Jo et al., 2022;Vignac et al., 2022;Jo et al., 2023) have significantly improved graph generation.GDSS (Jo et al., 2022) generates both node features and adjacency matrices simultaneously, resulting in better alignment with graph datasets.DiGress (Vignac et al., 2022) addresses the challenge of generating graphs with categorical node and edge attributes, which is a difficult task due to the unordered nature and sparsity of graphs.GruM (Jo et al., 2023) directly learns graph topology, improving connectivity and structure recovery.</p>
<p>Graph to Text for LLM.The emergence of large language models (LLMs) has driven significant advancements in the natural sciences (Taylor et al., 2022;Liu et al., 2024).These models are trained on vast amounts of text data, the most abundant type of data, contributing to their success across many tasks.Multi-modal methods (Luo et al., 2023;Le et al., 2024) have been proposed to incorporate both graph and text information.They typically rely on graph neural networks or transformers to encode graphs.However, these methods often use text, such as SMILES, to represent molecular features.SMILES may not tokenize the molecular structure effectively, limiting the ability to represent the molecule structure accurately.As a result, the graph embeddings may be too weak for intricate molecular structures, limiting performance in molecular generation tasks.Recently, there have been attempts (Fatemi et al., 2023) to represent graphs in natural language formats, encoding their structure using descriptive language.However, this naive approach introduces challenges, as such encodings are unlikely to appear in typical text, meaning that LLMs-trained predominantly on conventional text data-may struggle to process them effectively.Using an encoding that aligns with the LLMs' training data is essential.We propose leveraging tree-structured formats like JSON and XML to encode molecules to address this issue.The JSON format is a widely used and structured data representation commonly found in LLM training.This allows us to capture the complexity of molecular graphs while ensuring compatibility with LLMs.</p>
<p>G2T-LLM</p>
<p>This section introduces G2T-LLM: Graph-to-Tree Text Encoding for Molecule Generation with Fine-Tuned Large Language Models.</p>
<p>CHALLENGES AND MOTIVATIONS</p>
<p>Molecular graphs pose a challenge for LLMs due to their inherently complex, non-linear structures, where atoms (nodes) and bonds (edges) form intricate connectivity patterns, including rings, branches, and cycles.Traditional LLMs excel at processing sequential data, such as natural language, where information flows in a linear manner.However, molecular graphs do not naturally conform to this format, as their connections often lack a clear, ordered sequence.This mismatch complicates the application of LLMs to molecule-related tasks.</p>
<p>Despite these challenges, LLMs have shown a capacity to handle structured, hierarchical data formats, such as JSON and XML.These formats share some of the complexity of graphs but are still expressed as trees, with clear parent-child relationships between elements.LLMs trained on such data can handle hierarchical structures by processing them as sequences while maintaining the relationships and nested dependencies inherent to these structures.This training has made LLMs particularly adept at handling data that can be decomposed into nested layers, making them better suited for tree-like representations than arbitrary graphs.</p>
<p>To leverage this strength, we propose encoding molecular graphs into a tree structure.This approach is inspired by SMILEs, which are essentially tree representations of molecular graphs, proving that molecular graphs can be effectively serialized as trees while preserving their chemical properties.This encoding acts as a bridge between the graph-based molecular structures and the LLM's ability to process and generate hierarchical data.The LLM can be trained on these tree-encoded molecules, and it can also output molecules in the same structured format, facilitating the generation of coherent Input: graph (dictionary of atom identifiers to connected atom identifiers)</p>
<p>3:</p>
<p>Output: text representation (tree structure in text format) return {"atom name": atom.atomname, "atom id": atom id, "bonds": bonds} return text representation molecular representations.By aligning graph data with a format that LLMs are well-equipped to handle, this method holds the potential for improving the coherence and plausibility of generated molecular structures.</p>
<p>GRAPH-TO-TREE TEXT ENCODING</p>
<p>To make molecular graphs accessible to LLMs, we introduce a tree-based encoding inspired by the SMILES format.SMILES encodes molecules by performing a depth-first traversal over the molecular graph and representing it as a linear string.In our approach, we extend this traversal to build a hierarchical tree structure, where atoms are represented as nodes and their bonds as edges connecting them.The hierarchical nature of the tree is well-suited for the LLM's training with tree-like structures.</p>
<p>However, molecular graphs often contain rings and cycles-features that trees cannot naturally represent.To address this, we assign each atom in the molecule a unique identifier (ID).When the traversal encounters a ring closure or cycle, the tree refers back to the atom's unique ID rather than creating a new node, thereby preserving both the hierarchical structure and chemical validity.This encoding technique ensures that we accurately capture the full molecular graph in a way the LLM can process, while maintaining the integrity of complex molecular features such as rings and branches.Algorithm 1 and Algorithm 2 describe the processes for converting a molecular graph to a tree-structured text representation and for reconstructing the graph from this format, respectively.Figure 3 illustrates the graph-to-tree text encoding.</p>
<p>TOKEN CONSTRAINING FOR VALID TREE-STRUCTURE GENERATION</p>
<p>Despite the advancements in LLMs, there remains a significant challenge in ensuring that the outputs adhere to valid tree-structured formats.LLMs, while capable of generating coherent text, may produce sequences that do not respect the hierarchical relationships required for molecular repre- Input: tree json (tree structure in JSON format)</p>
<p>3:</p>
<p>Output: graph (dictionary representing the molecular graph) return graph sentation.This can lead to outputs that are structurally invalid, failing to accurately represent the complex relationships inherent in molecular graphs.</p>
<p>To mitigate this issue, we implement a set of constraints that guide the token generation process of the LLM.These constraints filter the tokens allowed at each step, ensuring that generated outputs remain within the bounds of valid tree structures.Specifically, we impose rules that dictate acceptable parent-child relationships, enforce valid connections between atoms, and restrict the formation of non-hierarchical sequences.Additionally, we constrain the types of atoms and bonds that can be generated, ensuring that only valid atom types (e.g., carbon, oxygen) and bond types (e.g., single, double) are used in the output.This approach leverages domain knowledge of molecular structures to create a robust framework for guiding the LLM's outputs.</p>
<p>The application of token constraining significantly enhances the reliability of the generated treestructured outputs.By enforcing these constraints, we improve the chances that the LLM produces valid representations of molecular structures that can be effectively used in further analyses or applications.This technique not only aids in ensuring the accuracy of the generated data but also reinforces the overall effectiveness of our graph-to-tree text encoding approach, making it a vital component in achieving coherent and chemically valid molecular generation.</p>
<p>SUPERVISED FINE-TUNING LLMS FOR MOLECULAR GENERATION</p>
<p>A key challenge in leveraging large language models for molecular generation is that, without specialized training, they may struggle to produce valid molecular structures, particularly when dealing with complex features such as rings, cycles, and the inherent chemical constraints that govern molecular formation.Supervised fine-tuning addresses this issue by teaching the LLM domain-specific rules and patterns, enabling it to generate valid molecular structures that adhere to chemical principles.</p>
<p>We structure the fine-tuning process as a molecular completion task.The LLM is trained by prompting it with a partial molecular structure, encoded using the graph-to-tree text encoding and tasking it with predicting the remaining atoms and bonds necessary to complete the molecule.For each training example, we provide the LLM with an incomplete molecular graph, and the model is then expected to generate the missing parts based on the information provided.The model's output is evaluated against the full molecular structure's text encoding, and the loss is computed based on the accuracy of its predictions.By iterating through this process, the LLM learns to predict the completion of molecular graphs in a way that respects chemical validity, helping the model better handle challenging structural features.Note that token constraining is deliberately omitted during finetuning, allowing the LLM to explore and learn more freely before constraints are imposed during inference.Figure 3.4 illustrates the supervised fine-tuning process of G2T-LLM.</p>
<p>The fine-tuning process is integral to the success of our approach.By casting molecular generation as a completion task and using the proposed graph-to-tree encoding as a bridge between molecular structures and the LLM's capabilities, we enhance the model's ability to generate coherent and chemically valid outputs.This fine-tuning approach refines the LLM's understanding of molecular patterns and constraints, enabling it to produce outputs that are more reliable and scientifically grounded within the realm of molecular design.</p>
<p>INFERENCE PROCESS OF G2T-LLM</p>
<p>The molecular generation process begins with selecting a random molecular component, which could be an atom, a bond, or even a larger motif.This component serves as the initial prompt for the fine-tuned LLM.The component is encoded into the graph-to-tree text format, creating a tree-structured representation that the LLM can process.</p>
<p>Once the LLM receives this initial prompt, it is tasked with generating the subsequent components of the molecular structure.At each step, the LLM's output is constrained by the Token Constraining mechanism, ensuring that only chemical and schema-valid tokens-such as specific atom types and bond types-are generated.These constraints help guide the LLM in maintaining the coherence of the structure, preventing invalid or nonsensical outputs, and ensuring that the generated molecule adheres to the expected chemical rules.As the LLM iteratively predicts new components, these outputs are progressively combined into an expanding tree-structured text.This generated text represents the molecular graph, with nodes corresponding to atoms and edges corresponding to bonds.</p>
<p>Once the generation process is complete, the final tree-structured text is decoded back into a full molecular graph.This graph is then translated into a standard molecular format, fully reconstructing the molecule from the text generated by the LLM. Figure 3.4 illustrates the inference process of G2T-LLM.</p>
<p>EXPERIMENTS</p>
<p>In this section, we conduct comprehensive experiments on two real-world datasets to evaluate the effectiveness of our proposed methods.</p>
<p>EXPERIMENTAL SETUP</p>
<p>Datasets and Metrics.We evaluate the quality of molecule generation using two real-world datasets: QM9 (Ramakrishnan et al., 2014)  Although several studies have explored using LLMs for molecular generation, direct comparisons with our approach are not feasible.For instance, LMLF (Brahmavar et al., 2024), Grammar Prompting (Wang et al., 2024), and LLM4GraphGen (Yao et al., 2024) all employ rule-based promptengineering techniques that fundamentally differ from our SFT LLM approach.These models rely on predefined rules and heuristics to guide the generation process, which restricts their ability to learn from the underlying data distributions.In contrast, our method leverages a more flexible and adaptive encoding, allowing the LLM to capture the complexities of molecular structures more effectively.</p>
<p>Moreover, the baseline models utilize significantly larger architectures, such as GPT-4, whereas our experiments are conducted with LLaMA3.1-8B.This disparity in model size and complexity further complicates direct comparisons, as the performance capabilities and learned representations of these models can vary widely.Therefore, assessing our results against those achieved by larger, rule-based models may not provide a meaningful evaluation of performance, given the substantial differences in methodologies and model architectures.Implementation details.For our G2T-LLM, we conduct experiments using the LLaMA3.1-8Bmodel (Dubey et al., 2024) as our base LLM, selected for its strong performance in text generation tasks.The model parameters are fine-tuned with torchtune (Ansel et al., 2024), and we leverage QLoRA (Dettmers et al., 2024) to accelerate training while reducing memory consumption.The fine-tuning dataset consists of 5,000 molecules, and the model is trained with a batch size of 8, using the AdamW optimizer (Loshchilov, 2017) with a weight decay of 0.01 and a learning rate of 3e-4.The learning rate is adjusted by a cosine schedule with 100 warmup steps, and cross-entropy loss is employed for the loss computation.All model computations are performed with the bf16 data type.Fine-tuning is carried out on an NVIDIA A100 SXM4 80GB, and inference is done on NVIDIA GeForce RTX 3090 and 4090 GPUs.The implementation is done in PyTorch (Paszke et al., 2019).</p>
<p>EXPERIMENTAL RESULTS</p>
<p>Table 1 presents performance comparisons on both the QM9 and ZINC250k datasets against baseline models.Our approach consistently achieves top-two validity scores across both datasets, demonstrating its effectiveness in enabling the LLM to capture the underlying chemical rules essential for accurate molecule generation.For novelty, our method attains a perfect score of 100% on the ZINC250k dataset and 88% on QM9, highlighting its ability to consistently generate novel molecular structures.In terms of FCD and Scaf metrics-critical indicators of a model's ability to explore and replicate chemical space-our method delivers competitive performance compared to other baselines.While DiGress and Grum show strong FCD and Scaf scores on the QM9 dataset, their novelty scores fall significantly short (below 40%), suggesting potential overfitting to the training data rather than true generalization of molecular distributions.In contrast, our method not only maintains high novelty rates but also achieves strong performance on FCD and Scaf metrics.On the ZINC250k dataset, our approach attains the highest Scaf score and the second-best FCD score, further demonstrating its superior ability to generalize and innovate within chemical spaces.This robust performance underscores our model's advanced understanding and application of molecular distributions, making it a powerful tool for innovative molecular design in computational chemistry.</p>
<p>VISUALIZATION RESULTS OF GENERATED MOLECULES</p>
<p>In Fig. 4, we follow the experimental setup outlined in (Jo et al., 2022), using Tanimoto similarity based on Morgan fingerprints to evaluate the generated molecular graphs.For consistency and comparability, we select the same molecules as (Jo et al., 2022).Additionally, we perform experiments on molecular graphs generated by Grum (Jo et al., 2023).Across most cases, our method demonstrates superior performance compared to previous state-of-the-art diffusion-based approaches, showcasing its effectiveness and robustness in molecular graph generation.To evaluate how our proposed graphto-tree text encoding improves the LLM's ability to learn graph structures compared to the previous graphto-text methods such as Talk Like a Graph (Fatemi et al., 2023), we conducted experiments on the challenging Zinc250K dataset (Irwin et al., 2012), which contains larger molecules.Talk Like a Graph encodes graph structures by converting them into natural language, where each node's connections and attributes are described in sentence form.For the fine-tuning process, we randomly selected 5,000 molecules from the training set and generated 1,000 molecules for performance comparison.</p>
<p>As shown in Table 2, our method significantly outperforms the previous approach across all metrics, demonstrating that encoding molecular structures in JSON format enables LLMs to more effectively learn and replicate complex molecular structures.In this study, we aim to evaluate the impact of supervised fine-tuning on LLM performance.Specifically, we generate 1,000 molecules using the same prompt to compare the performance of the LLM before and after fine-tuning.This direct comparison allows us to assess how fine-tuning enhances the model's ability to accurately generate molecular structures.We conduct this experiment using the ZINC250k dataset, and the results are presented in Table 3.The results reveal that without fine-tuning, the LLM produces molecules with only 70.8% validity and 61.12% uniqueness, indicating that the model, in its initial state, struggles to fully comprehend and accurately replicate the text representation of molecular structures.However, after fine-tuning, there is a significant improvement, with validity and uniqueness increasing to 99.6% and 99.79%, respectively.These results highlight the effectiveness of fine-tuning in substantially improving the model's performance, demonstrating its critical role in enabling the LLM to better understand and generate precise molecular structures.</p>
<p>ABLATION STUDY: IMPACT OF SUPERVISED FINE-TUNING LLM</p>
<p>4.6 ABLATION STUDY: IMPACT OF SIZE OF THE FINE-TUNING DATASET In this section, we investigate the impact of dataset size on the performance of a LLM during fine-tuning.Our experiments use the QM9 dataset with three distinct dataset sizes for fine-tuning: 1,000, 5,000, and 10,000 molecules.Each model is trained over 10 epochs.This setup enables a systematic evaluation of how variations in fine-tuning data size affect the model's learning efficacy and its ability to generalize.Table 4 presents the results of these experiments.The results indicate an improvement in the FCD and Scaf scores as the dataset size increases.This improvement likely stems from the LLM's exposure to a larger array of data points, which enhances its understanding of the chemical distribution within the dataset.Conversely, we observe a decrease in novelty scores with larger datasets.This reduction may be attributed to the relatively small and structurally simple nature of the QM9 dataset, which comprises only four types of atoms and molecules not exceeding nine atoms.As the model encounters more data, it increasingly reproduces similar outputs, reflecting the limited diversity in the dataset.In this section, we examine the impact of token constraining on molecular generation, as introduced in Section 3.3.Token constraining is implemented to guide the LLM toward generating valid molecular structures by restricting its output to adhere to chemical rules.To evaluate the effectiveness of this approach, we perform an experimental comparison using the ZINC250k dataset.Specifically, we generate 1,000 molecules to compare the validity of the output with and without token constraining.The results, presented in Table 5, clearly demonstrate the efficacy of token constraining in improving the validity of generated molecules.Without token constraining, the validity of the generated molecules is only 41.6%.However, when token constraining is applied, validity dramatically increases to 98.6%.This significant improvement underscores the critical role of token constraining in guiding the LLM to produce valid molecular structures, ensuring closer adherence to the fundamental rules of chemical structure and leading to a higher rate of valid outputs.</p>
<p>ABLATION STUDY: IMPACT OF TOKEN CONSTRAINING</p>
<p>CONCLUSION</p>
<p>In this work, we introduced G2T-LLM, a novel approach for molecular generation that leverages LLMs to generate valid molecular structures through a novel graph-to-tree text encoding.By converting molecular graphs into hierarchical representations inspired by SMILES but adapted for LLMs, we bridge the gap between non-linear molecular structures and sequential data processing.This encoding allows the LLM to understand the molecular structure better and produce coherent chemical outputs.Our method addresses the challenges of generating valid molecular structures by introducing token constraints during the generation process, ensuring that the outputs respect some chemical and structural rules.Through supervised fine-tuning, we further align the LLM with molecular generation tasks, improving its ability to produce chemically valid molecules based on the learned data patterns from benchmark datasets like Zinc250K and QM9.Our results demonstrate the effectiveness of G2T-LLM, achieving state-of-the-art performance on benchmark datasets.This work highlights the potential of utilizing LLMs in molecular design, opening up new avenues for AI-driven discoveries in chemistry.The combination of hierarchical encoding, token constraining, and fine-tuning proves to be a powerful strategy for tackling the complexities of molecular generation.Future work will focus on refining these techniques to enhance efficiency and explore further applications in drug discovery and material science.</p>
<p>A ADDITIONAL EXPERIMENTS RESULTS</p>
<p>Here are additional experiment results on QM9 and ZINC250k datasets.The Neighborhood Subgraph Pairwise Distance Kernel (NSPDK) Maximum Mean Discrepancy (MMD) (Costa &amp; Grave, 2010) evaluates the difference between generated and test molecules, accounting for both node and edge features.Uniqueness refers to the percentage of valid molecules that are distinct from each other.Validity, FCD, Novelty, and Scaf have been introduced before.</p>
<p>Figure 1 :
1
Figure1: Illustration of the Graph-to-Tree Text Encoding process described in Section 3.2 and Algorithm 1.This figure shows how the molecular structure of cyclopropene is transformed into a hierarchical tree representation.Each atom and bond is mapped to nodes and edges in the tree, with unique identifiers assigned.</p>
<p>Algorithm 2
2
Convert Tree-Structured Text to Molecular Graph 1: function TREE2GRAPH(tree json) 2:</p>
<p>Figure 2 :Figure 3 :
23
Figure2: An illustration of the supervised fine-tuning process of G2T-LLM.The process begins by randomly selecting a starting component, exemplified by cyclopropene, which is encoded into a partial tree structure and passed as a prompt to the LLM.The LLM generates the remaining molecular structure, which is compared against the ground truth.A loss is computed and is used to fine-tune the model, iteratively improving its performance in generating valid molecular graphs.</p>
<p>Figure 4 :
4
Figure 4: Visualization of the generated molecules with Tanimoto similarity scores based on Morgan fingerprints.The best results are highlighted in bold.</p>
<p>Table 1 :
1
Generation results on the QM9 and ZINC250k datasets.We report the mean of 3 different runs.The best results are highlighted in bold.The second-best results are highlighted in underline.We provide the results of uniqueness, and NSPDK in Appendix A.
DatasetsQM9ZINC250KMethodsValid↑Novelty↑ FCD↓Scaf↑Valid↑Novelty↑ FCD↓Scaf↑MoFlow91.3694.724.4670.144763.11100.0020.9310.0133GraphAF74.4386.595.6250.304668.4799.9916.0230.0672GraphDF93.8898.5410.9280.097890.61100.0033.5460.0000EDP-GNN47.5286.582.6800.327082.97100.0016.7370.0000GDSS95.7286.272.9000.698397.01100.0014.6560.0467DiGress98.1925.580.0950.935394.9999.993.4820.4163Grum99.6924.150.1080.944998.6599.982.2570.5299Ours99.4788.290.8150.911298.03100.002.4450.6062</p>
<p>Table 2 :
2
Study of the impact of tree-structured text encoding on the ZINC250K dataset.
MethodsValid↑FCD↓Scaf↑Novelty↑Talk like a graph59.2019.81140.1317100Ours98.605.69060.1522100</p>
<p>Table 3 :
3
Comparison of LLM performance with and without SFT on the ZINC250k dataset.Methods
Valid↑Unique↑Novelty↑w/o SFT70.8061.12100.00w/ SFT98.6098.98100.00</p>
<p>Table 4
4: Comparison of LLM performance with differ-ent size of fine-tuning datasetsMethodsValid↑ Novelty↑ FCD ↓ Scaf ↑1k (10 epoch)98.5090.381.2260.69335k (10 epoch)98.7086.531.2190.777910k (10 epoch) 98.5073.891.1460.7980</p>
<p>Table 5
5: Comparison results of using token con-straining (TC) on molecular generation on theZINC250k dataset.w/o TCw/ TCValidity (%)↑41.6098.60</p>
<p>Table 6 :
6
Generation results on the QM9 dataset.We report the mean of 3 different runs.The best results are highlighted in bold.The second-best results are highlighted in underline.Methods Valid (%)↑ FCD ↓ NSPDK ↓ Scaf ↑ Unique (%)↑ Novelty (%)↑
MoFlow91.364.4670.0170.144798.6594.72GraphAF74.435.6250.0210.304688.6486.59GraphDF93.8810.9280.0640.097898.5898.54EDP-GNN47.522.6800.0050.327099.2586.58GDSS95.722.9000.0030.698398.4686.27DiGress98.190.0950.00030.935396.6725.58Grum99.690.1080.00020.944996.9024.15Ours99.470.8150.0020.911289.5788.29</p>
<p>Table 7 :
7
Generation results on the ZINC250k dataset.We report the mean of 3 different runs.The best results are highlighted in bold.The second-best results are highlighted in underline.
MoFlow63.1120.9310.0460.013399.99100.00GraphAF68.4716.0230.0440.067298.6499.99GraphDF90.6133.5460.1770.000099.63100.00EDP-GNN82.9716.7370.0490.000099.79100.00GDSS97.0114.6560.0190.046799.64100.00DiGress94.993.4820.00210.416399.9799.99Grum98.652.2570.00150.529999.9799.98Ours98.032.4450.00490.606294.69100.00
Methods Valid (%)↑ FCD ↓ NSPDK ↓ Scaf ↑ Unique (%)↑ Novelty (%)↑</p>
<p>PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation. Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary Devito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, Bert Luk, Yunjie Maher, Christian Pan, Matthias Puhrsch, Mark Reso, Marcos Yukio Saroufim, Helen Siraichi, Michael Suk, Phil Suo, Eikan Tillet, Xiaodong Wang, William Wang, Shunting Wen, Xu Zhang, Keren Zhao, Richard Zhou, Ajit Zou, Gregory Mathews, Peng Chanan, Soumith Wu, Chintala, 10.1145/3620665.364036629th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. ACMApril 2024224</p>
<p>Generating novel leads for drug discovery using llms with logical feedback. Shreyas Bhat Brahmavar, Ashwin Srinivasan, Tirtharaj Dash, Sowmya Ramaswamy Krishnan, Lovekesh Vig, Arijit Roy, Raviprasad Aduri, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Language models are few-shot learners. Tom B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>Fast neighborhood subgraph pairwise distance kernel. Fabrizio Costa, Kurt De Grave, Proceedings of the 27th International Conference on International Conference on Machine Learning. the 27th International Conference on International Conference on Machine Learning2010</p>
<p>Syntax-directed variational autoencoder for structured data. Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, Le Song, arXiv:1802.087862018arXiv preprint</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Advances in Neural Information Processing Systems. 202436</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Deep learning for molecular design-a review of the state of the art. Zois Daniel C Elton, Boukouvalas, Peter W Mark D Fuge, Chung, Molecular Systems Design &amp; Engineering. 442019</p>
<p>Talk like a graph: Encoding graphs for large language models. Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi, arXiv:2310.045602023arXiv preprint</p>
<p>Zinc: a free tool to discover chemistry for biology. Teague John J Irwin, Sterling, Erin S Michael M Mysinger, Ryan G Bolstad, Coleman, Journal of chemical information and modeling. 5272012</p>
<p>Score-based generative modeling of graphs via the system of stochastic differential equations. Jaehyeong Jo, Seul Lee, Sung Ju Hwang, International conference on machine learning. PMLR2022</p>
<p>Graph generation with diffusion mixture. Jaehyeong Jo, Dongki Kim, Sung Ju Hwang, arXiv:2302.035962023arXiv preprint</p>
<p>Molx: Enhancing large language models for molecular learning with a multi-modal extension. Khiem Le, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, Nitesh V Chawla, arXiv:2406.067772024arXiv preprint</p>
<p>Moleculargpt: Open large language model (llm) for few-shot molecular property prediction. Yuyan Liu, Sirui Ding, Sheng Zhou, Wenqi Fan, Qiaoyu Tan, arXiv:2406.12950arXiv:1711.051012024. I Loshchilov. Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Yizhen Luo, Kai Yang, Massimo Hong, Xing Yi Liu, Zaiqing Nie, arXiv:2307.09484Molfm: A multimodal molecular foundation model. 2023arXiv preprint</p>
<p>Graphdf: A discrete flow model for molecular graph generation. Youzhi Luo, Keqiang Yan, Shuiwang Ji, International conference on machine learning. PMLR2021</p>
<p>Graphnvp: An invertible flow model for generating molecular graphs. Kaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago, Motoki Abe, arXiv:1905.116002019arXiv preprint</p>
<p>Permutation invariant graph generation via score-based generative modeling. Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, Stefano Ermon, International Conference on Artificial Intelligence and Statistics. PMLR2020</p>
<p>Pytorch: An imperative style, highperformance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. 201932</p>
<p>Fréchet chemnet distance: a metric for generative models for molecules in drug discovery. Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, Gunter Klambauer, Journal of chemical information and modeling. 5892018</p>
<p>Quantum chemistry structures and properties of 134 kilo molecules. Raghunathan Ramakrishnan, Matthias Pavlo O Dral, O Rupp, Von Anatole, Lilienfeld, Scientific data. 112014</p>
<p>Rapid shape-based ligand alignment and virtual screening method based on atom/feature-pair similarities and volume overlap scoring. G Madhavi Sastry, Steven L Dixon, Woody Sherman, Journal of chemical information and modeling. 51102011</p>
<p>Computer-based de novo design of drug-like molecules. Gisbert Schneider, Uli Fechner, Nature Reviews Drug Discovery. 482005</p>
<p>Graphaf: a flow-based autoregressive model for molecular graph generation. Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, Jian Tang, arXiv:2001.093822020arXiv preprint</p>
<p>Graphvae: Towards generation of small graphs using variational autoencoders. Martin Simonovsky, Nikos Komodakis, Artificial Neural Networks and Machine Learning-ICANN 2018: 27th International Conference on Artificial Neural Networks. Rhodes, GreeceSpringerOctober 4-7, 2018. 2018Proceedings, Part I 27</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Attention is all you need. Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Digress: Discrete denoising diffusion for graph generation. Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, Pascal Frossard, arXiv:2209.147342022arXiv preprint</p>
<p>Grammar prompting for domain-specific language generation with large language models. Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A Saurous, Yoon Kim, Advances in Neural Information Processing Systems. 202436</p>
<p>Exploring the potential of large language models in graph generation. Yang Yao, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Xu Chu, Yuekui Yang, Wenwu Zhu, Hong Mei, arXiv:2403.143582024arXiv preprint</p>
<p>Graphrnn: Generating realistic graphs with deep auto-regressive models. Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, Jure Leskovec, International conference on machine learning. PMLR2018</p>
<p>Moflow: an invertible flow model for generating molecular graphs. Chengxi Zang, Fei Wang, Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining2020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1730 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1730</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1730</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-270702560</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.16294v1.pdf" target="_blank">LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in Large Language Models (LLMs) have shown inspiring achievements in constructing autonomous agents that rely on language descriptions as inputs. However, it remains unclear how well LLMs can function as few-shot or zero-shot embodied agents in dynamic interactive environments. To address this gap, we introduce LangSuit · E, a versatile and simulation-free testbed featuring 6 representative embodied tasks in textual embodied worlds. Compared with previous LLM-based testbeds, LangSuit · E (i) offers adaptability to diverse environments without multiple simulation engines, (ii) evaluates agents’ capacity to develop “internalized world knowledge” with embodied observations, and (iii) allows easy customization of communication and action strategies. To address the embodiment challenge, we devise a novel chain-of-thought (CoT) schema, EmMem , which summarizes embodied states w.r.t. history information. Comprehensive benchmark results illustrate challenges and insights of embodied planning. LangSuit · E represents a significant step toward building embodied gener-alists in the context of language models.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1730.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1730.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-3.5-turbo (OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial chat-based large language model used in this paper as the central planner/controller via prompting (zero-/few-shot); evaluated on LangSuit•E with several prompting strategies (Act, ReAct, Reflexion) and the novel EmMem chain-of-thought schema.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Chat-oriented large language model used as a planner/controller; driven by textual prompts that request an explicit Thought (CoT) and an Act (textual action string), optionally augmented by EmMem (embodied memory summaries) or Reflexion multi-trial self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>General language pretraining (large-scale text); exact corpora not specified in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper treats gpt-3.5-turbo as a pretrained commercial LLM (no fine-tuning reported). The authors do not provide dataset names, sizes, or training details for the model (usage via OpenAI API).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>LangSuit•E suite (Instruction Grounding, Rearrangement, Interactive QA, Household, MA-Household, MA-WAH)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>A unified, simulation-free textual embodied testbed emulating partially-observable indoor 3D tasks by providing per-step textual embodied observations (objects in view, limited view distances) and executing discrete embodied actions; tasks include navigation + manipulation, rearrangement, question answering requiring interaction, household instruction following, and multi-agent cooperation/teaching.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>No explicit action space during LLM pretraining; pretraining is language modeling. At inference the LLM emits natural-language formatted action tokens (e.g. 'Act: move_ahead', 'Act: pick_up [obj]') akin to text-based game commands.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete, symbolic embodied action set: low-level actions (move_ahead by one step, turn_left/right 90°, pick_up[obj], put[obj, recep], open/close, toggle_on/off, slice, etc.) and optionally high-level actions (goto [loc], heat [obj, heater], chat [message], ask [message]).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Direct textual mapping: the LLM is prompted to output a single textual action line ('Act: <action>'); the environment parses that string and executes the corresponding symbolic action. EmMem/CoT prompts add an explicit predicted embodied-state summary before action selection; no learned mapping network is used.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Textual, symbolic observations only (no visual pixels): per-step natural-language descriptions of objects and relative positions within a limited view; partially observable.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>On LangSuit•E with gpt-3.5-turbo used as a zero-/few-shot agent: high-level action setting -- Instruction Grounding SRs in the range ≈77%–86% depending on prompt strategy (Act vs ReAct vs ReAct-EmMem vs Reflexion/Reflexion-EmMem); low-level action setting -- much lower SRs (roughly ≈11%–20% across strategies). EmMem and Reflexion often improved performance (examples: ReAct-EmMem and Reflexion-EmMem gave the best IG high-level SRs up to ≈85–86% in the paper's tables). Exact per-task numbers are reported in Tables 3 and 7 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not directly applicable: the experiments evaluate different prompt/CoT strategies rather than a non-pretrained baseline. The paper uses Act-only prompting as a baseline; e.g., in high-level IG Act-only SR ≈77% vs ReAct/Reflexion strategies up to ≈84–86% (improvements of several percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>No fine-tuning sample-complexity reported — the model is used in zero-/few-shot prompting. For evaluation the authors sampled 100 tasks for Instruction Grounding, Household, and Rearrangement, and 50 tasks for each IQA subtask and the multi-agent tasks (these are evaluation counts, not training samples).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>1) Modality match: LangSuit•E uses textual observations and textual actions, matching the LLM's language modality; 2) LLMs' stored commonsense and world knowledge that supports planning and object reasoning; 3) Explicit CoT and EmMem prompt schemas that surface internal state and history, improving planning in partially observable settings; 4) Shared discrete symbolic action vocabulary enabling direct text-to-action execution.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>1) Perception gap to real 3D pixel inputs (LangSuit•E is text-only; bridging to pixel-based 3D still requires VLMs); 2) Action-space mismatch for low-level control: LLMs struggle more in fine-grained low-level action settings than with high-level discrete actions; 3) Hallucination and reasoning errors from CoT (can degrade performance on some tasks); 4) No grounding to continuous motor commands.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained language models like gpt-3.5-turbo can be operated as embodied agents in textualized 3D-style tasks with substantial success when the LLM input/output remain in language (high-level actions); chain-of-thought styles (ReAct, Reflexion) and the proposed EmMem embodied-memory prompt improve performance, especially in low-level action settings, but absolute performance remains limited in fine-grained low-level control and there is a large gap compared to supervised-fine-tuned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1730.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1730.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A more powerful chat-based LLM mentioned by the authors as used in some experiments; reported to boost performance relative to smaller API models, though detailed per-task numbers are not presented.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Larger commercial chat LLM used via API prompting similarly to GPT-3.5, with the same Act/CoT/EmMem prompting pipelines; treated as a more capable planner/controller in qualitative remarks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>General language pretraining (large-scale text); exact corpora not specified in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper does not give dataset, size, or training specifics for GPT-4; it is referenced as a more powerful API model used for some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>LangSuit•E suite (same set of textual embodied tasks as evaluated for GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Textual, partially-observable indoor embodied tasks (Instruction Grounding, Rearrangement, IQA, Household, multi-agent tasks) under LangSuit•E.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>LLM pretraining is not action-conditioned; the model is prompted to emit textual actions during interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete symbolic low-level and high-level actions as defined by LangSuit•E (same set as for GPT-3.5).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Prompted textual action outputs parsed into environment action calls; same EmMem/CoT augmentation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Textual observations only (as provided by LangSuit•E).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>The paper notes qualitative performance boosts using a more powerful model (GPT-4) but does not provide detailed per-task quantitative numbers for GPT-4; reported experiments focus primarily on gpt-3.5-turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Zero-/few-shot prompting; no fine-tuning data counts reported for GPT-4 in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Increased model capacity and reasoning abilities likely improve planning and CoT effectiveness; same modality matching advantages as for smaller LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Paper notes uncertainty whether observations are model-dependent; lacks systematic GPT-4 breakdown so limitations remain unquantified.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Larger LLMs (GPT-4) can further improve performance in textual embodied tasks, but the paper provides only qualitative statements and not detailed quantitative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1730.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1730.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-7B (Open-source LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source LLM that the authors tried in preliminary experiments; reported to fail on the evaluated Instruction Grounding and IQA tasks in LangSuit•E without task-specific tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A smaller open-source transformer language model used in a preliminary attempt as an embodied agent via prompting; no fine-tuning reported in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>General language pretraining (open web text); exact details not provided in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper states the authors attempted to use LLaMA-7B for Instruction Grounding and IQA but it 'failed in all tasks' in preliminary trials; no further dataset or training detail provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>LangSuit•E (Instruction Grounding and Interactive QA attempted)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Same LangSuit•E textual embodied tasks; the model was evaluated as a zero-/few-shot prompted agent and did not produce appropriate embodied predictions in preliminary attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>LLM pretraining is language-modeling only; prompting-based action generation attempted.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>LangSuit•E discrete textual action tokens (same as others), but the model produced inadequate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Prompted textual actions parsed by environment; no successful mapping achieved in practice for this model in the authors' attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Textual observations from LangSuit•E (model received prompts describing observations).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported as failing in all (attempted) tasks; the paper does not provide numeric SRs but states the attempts 'failed in all tasks'.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>No fine-tuning; preliminary zero-/few-shot prompting only. No training sample counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>N/A for this preliminary negative result; paper suggests model capacity and training regimen matter and that small open-source LLMs performed poorly out-of-the-box.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Insufficient model scale/capacity and/or lack of instruction-tuning/coherent CoT behaviors for embodied prediction; authors note small models like LLaMA-7B were unable to make appropriate embodied predictions regardless of prompt strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Small open-source LLMs (LLaMA-7B, as tested here) failed to act successfully as zero-shot embodied agents in LangSuit•E without further fine-tuning or adaptation, highlighting a capacity gap relative to larger API models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1730.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1730.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALFWorld / TextWorld mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALFWorld (aligned ALFRED to TextWorld) and TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that aligns text-based game frameworks (TextWorld) with embodied benchmarks (ALFRED) so that agents trained in text environments can be evaluated on embodied planning; discussed in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>agents trained in TextWorld / ALFWorld pipelines (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Frameworks and agents from prior literature where text-based game environments (TextWorld) were used to create embodied-like tasks (ALFWorld) to bridge language-game pretraining and embodied instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Text-based games / interactive fiction (TextWorld)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper; ALFWorld is referenced as prior work that simplifies embodied challenges by providing full world observations and relying on simulators to emit textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>ALFWorld / ALFRED-aligned tasks (household instruction-following, pick-and-place, navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Benchmarks for instruction following and embodied manipulation originally in 3D simulators (ALFRED/AI2-THOR) that can be expressed as text games in ALFWorld; used to study transferring text-based planning to embodied action.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Text-game style high-level commands (e.g., 'go to the kitchen', 'pick up the cup'); typical TextWorld command set.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Symbolic simulator actions corresponding to navigation and manipulation primitives in 3D simulators (AI2-THOR / ALFRED).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Prior ALFWorld work provides a mapping between textual commands (from TextWorld) and simulator actions in ALFRED/AI2-THOR; this paper cites ALFWorld as background but does not experiment with it.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Prior work often used textual descriptions produced by simulator or full (global) observations simplified into text.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>ALFWorld demonstrates the utility of text-based interactive training data for embodied planning when the environment emits informative textual descriptions and action vocabularies align.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Prior criticism: ALFWorld and similar approaches often present full world observations (less realistic partial observability) and rely on simulators; thus transfer to realistic embodied perception/control is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text-based game pretraining has been leveraged to create embodied-style tasks (ALFWorld) but previous setups often over-simplify agent observability; LangSuit•E addresses this by providing partial observations and customizable action spaces to better probe transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1730.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1730.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wu et al. (GPT-3.5 annotation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wu et al. (2023b) — GPT-3.5 used to annotate action plans for AI2-THOR dataset (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior pipeline cited where GPT-3.5 was employed to automatically generate action-plan annotations for indoor scene tasks in AI2-THOR, effectively using a pretrained language model to produce structured embodied plans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-3.5 (used as annotator in cited prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Pretrained LLM used to auto-annotate action plan sequences for multimodal embodied datasets (triplets of scenes, instructions, action plans).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained language model (GPT-3.5) used to generate textual action plans; original pretraining corpora not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Paper cites Wu et al. who leveraged AI2-THOR to expand scenes and GPT-3.5 to annotate action plans; details about sizes and dataset composition are in the cited work, not in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>AI2-THOR based indoor planning/action annotation (used to train multimodal planners in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Generation of low-level or mid-level action sequences for simulated indoor manipulation/navigation tasks in AI2-THOR to create training data for embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>GPT-3.5 generated structured textual action sequences representing simulator actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Simulator symbolic actions in AI2-THOR (navigation and manipulation primitives).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>LLM was used to produce textual action plans; these plans were interpreted as simulator action sequences (annotation pipeline). The current paper cites this as related work; no experiments are run here.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Cited work operates on symbolic/scene descriptions from AI2-THOR; specifics are in the referenced paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Using LLMs to generate action plans can bootstrap datasets for training multimodal embodied agents, leveraging LLM commonsense for plan decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Annotation quality depends on LLM accuracy; any hallucinations or errors in generated plans propagate into downstream supervised training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper cites Wu et al. as an example where pretrained LLMs (GPT-3.5) were used to create action-plan annotations for embodied simulators, illustrating a route from language models to embodied data creation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1730.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1730.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Say-Can / Do-as-I-can</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Say-Can and related LLM-for-robotics mentions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior robotics works (e.g., Say-Can, LLM-Planner, Vemprala et al.) that apply pretrained language models for high-level task planning and grounding in robot control are cited as motivation; these works use LLMs pretrained on language to assist embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Say-Can / LLM-Planner / ChatGPT-based controllers (cited literature)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Systems that combine LLMs (pretrained on language) for high-level planning with grounding modules/affordance models trained on robot-specific data for low-level control.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Language model pretraining (general text) combined with task-specific robot affordance datasets (in the cited works); not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The present paper references Ahn et al. (Say-Can), Song et al. (LLM-Planner), and Vemprala et al. as prior works applying language models to robot or embodied control; details are in the cited papers.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Real-world robotics control / grounded navigation and manipulation (as in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Robotic tasks requiring mapping language instructions to robot actions/affordances; typically combine LLM planning with perception and low-level controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level natural language instructions and discrete textual planning outputs from LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Low-level robot controllers, motor primitives, or affordance-conditioned policies (continuous or discrete depending on the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Hybrid: LLM proposes high-level steps; an affordance model or low-level controller scores/executes grounded actions (cited works use learned affordance models or learned value functions). This paper cites these as related approaches but does not experiment with them.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB / depth / proprioception and affordance estimators in the cited robotics systems (not part of LangSuit•E).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Combining LLM commonsense with learned affordances/robot controllers enables practical robot instruction following in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Requires large amounts of robot-specific data and careful grounding; pure language pretraining alone is insufficient to control low-level motors without learned perception/affordance modules.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper positions LangSuit•E as complementary to these robotics efforts by isolating the language-planning component in a textual embodied world to study language-to-action transfer without visual perception noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ALFWorld <em>(Rating: 2)</em></li>
                <li>TextWorld <em>(Rating: 2)</em></li>
                <li>PaLM-E <em>(Rating: 2)</em></li>
                <li>Do as I can, not as I say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>Wu et al. (2023b) - GPT-3.5 used to annotate AI2-THOR action plans <em>(Rating: 2)</em></li>
                <li>Vemprala et al. (2023) - ChatGPT for zero-shot robotics controls <em>(Rating: 1)</em></li>
                <li>Reed et al. (2022) - LLMs in Minecraft / language-to-action work <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1730",
    "paper_id": "paper-270702560",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "gpt-3.5-turbo (OpenAI API)",
            "brief_description": "A commercial chat-based large language model used in this paper as the central planner/controller via prompting (zero-/few-shot); evaluated on LangSuit•E with several prompting strategies (Act, ReAct, Reflexion) and the novel EmMem chain-of-thought schema.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "gpt-3.5-turbo",
            "model_agent_description": "Chat-oriented large language model used as a planner/controller; driven by textual prompts that request an explicit Thought (CoT) and an Act (textual action string), optionally augmented by EmMem (embodied memory summaries) or Reflexion multi-trial self-reflection.",
            "pretraining_data_type": "General language pretraining (large-scale text); exact corpora not specified in the paper",
            "pretraining_data_details": "The paper treats gpt-3.5-turbo as a pretrained commercial LLM (no fine-tuning reported). The authors do not provide dataset names, sizes, or training details for the model (usage via OpenAI API).",
            "embodied_task_name": "LangSuit•E suite (Instruction Grounding, Rearrangement, Interactive QA, Household, MA-Household, MA-WAH)",
            "embodied_task_description": "A unified, simulation-free textual embodied testbed emulating partially-observable indoor 3D tasks by providing per-step textual embodied observations (objects in view, limited view distances) and executing discrete embodied actions; tasks include navigation + manipulation, rearrangement, question answering requiring interaction, household instruction following, and multi-agent cooperation/teaching.",
            "action_space_text": "No explicit action space during LLM pretraining; pretraining is language modeling. At inference the LLM emits natural-language formatted action tokens (e.g. 'Act: move_ahead', 'Act: pick_up [obj]') akin to text-based game commands.",
            "action_space_embodied": "Discrete, symbolic embodied action set: low-level actions (move_ahead by one step, turn_left/right 90°, pick_up[obj], put[obj, recep], open/close, toggle_on/off, slice, etc.) and optionally high-level actions (goto [loc], heat [obj, heater], chat [message], ask [message]).",
            "action_mapping_method": "Direct textual mapping: the LLM is prompted to output a single textual action line ('Act: &lt;action&gt;'); the environment parses that string and executes the corresponding symbolic action. EmMem/CoT prompts add an explicit predicted embodied-state summary before action selection; no learned mapping network is used.",
            "perception_requirements": "Textual, symbolic observations only (no visual pixels): per-step natural-language descriptions of objects and relative positions within a limited view; partially observable.",
            "transfer_successful": true,
            "performance_with_pretraining": "On LangSuit•E with gpt-3.5-turbo used as a zero-/few-shot agent: high-level action setting -- Instruction Grounding SRs in the range ≈77%–86% depending on prompt strategy (Act vs ReAct vs ReAct-EmMem vs Reflexion/Reflexion-EmMem); low-level action setting -- much lower SRs (roughly ≈11%–20% across strategies). EmMem and Reflexion often improved performance (examples: ReAct-EmMem and Reflexion-EmMem gave the best IG high-level SRs up to ≈85–86% in the paper's tables). Exact per-task numbers are reported in Tables 3 and 7 of the paper.",
            "performance_without_pretraining": "Not directly applicable: the experiments evaluate different prompt/CoT strategies rather than a non-pretrained baseline. The paper uses Act-only prompting as a baseline; e.g., in high-level IG Act-only SR ≈77% vs ReAct/Reflexion strategies up to ≈84–86% (improvements of several percentage points).",
            "sample_complexity_with_pretraining": "No fine-tuning sample-complexity reported — the model is used in zero-/few-shot prompting. For evaluation the authors sampled 100 tasks for Instruction Grounding, Household, and Rearrangement, and 50 tasks for each IQA subtask and the multi-agent tasks (these are evaluation counts, not training samples).",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "1) Modality match: LangSuit•E uses textual observations and textual actions, matching the LLM's language modality; 2) LLMs' stored commonsense and world knowledge that supports planning and object reasoning; 3) Explicit CoT and EmMem prompt schemas that surface internal state and history, improving planning in partially observable settings; 4) Shared discrete symbolic action vocabulary enabling direct text-to-action execution.",
            "transfer_failure_factors": "1) Perception gap to real 3D pixel inputs (LangSuit•E is text-only; bridging to pixel-based 3D still requires VLMs); 2) Action-space mismatch for low-level control: LLMs struggle more in fine-grained low-level action settings than with high-level discrete actions; 3) Hallucination and reasoning errors from CoT (can degrade performance on some tasks); 4) No grounding to continuous motor commands.",
            "key_findings": "Pretrained language models like gpt-3.5-turbo can be operated as embodied agents in textualized 3D-style tasks with substantial success when the LLM input/output remain in language (high-level actions); chain-of-thought styles (ReAct, Reflexion) and the proposed EmMem embodied-memory prompt improve performance, especially in low-level action settings, but absolute performance remains limited in fine-grained low-level control and there is a large gap compared to supervised-fine-tuned policies.",
            "uuid": "e1730.0",
            "source_info": {
                "paper_title": "LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A more powerful chat-based LLM mentioned by the authors as used in some experiments; reported to boost performance relative to smaller API models, though detailed per-task numbers are not presented.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "GPT-4",
            "model_agent_description": "Larger commercial chat LLM used via API prompting similarly to GPT-3.5, with the same Act/CoT/EmMem prompting pipelines; treated as a more capable planner/controller in qualitative remarks.",
            "pretraining_data_type": "General language pretraining (large-scale text); exact corpora not specified in the paper",
            "pretraining_data_details": "The paper does not give dataset, size, or training specifics for GPT-4; it is referenced as a more powerful API model used for some experiments.",
            "embodied_task_name": "LangSuit•E suite (same set of textual embodied tasks as evaluated for GPT-3.5)",
            "embodied_task_description": "Textual, partially-observable indoor embodied tasks (Instruction Grounding, Rearrangement, IQA, Household, multi-agent tasks) under LangSuit•E.",
            "action_space_text": "LLM pretraining is not action-conditioned; the model is prompted to emit textual actions during interaction.",
            "action_space_embodied": "Discrete symbolic low-level and high-level actions as defined by LangSuit•E (same set as for GPT-3.5).",
            "action_mapping_method": "Prompted textual action outputs parsed into environment action calls; same EmMem/CoT augmentation pipeline.",
            "perception_requirements": "Textual observations only (as provided by LangSuit•E).",
            "transfer_successful": null,
            "performance_with_pretraining": "The paper notes qualitative performance boosts using a more powerful model (GPT-4) but does not provide detailed per-task quantitative numbers for GPT-4; reported experiments focus primarily on gpt-3.5-turbo.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "Zero-/few-shot prompting; no fine-tuning data counts reported for GPT-4 in this paper.",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Increased model capacity and reasoning abilities likely improve planning and CoT effectiveness; same modality matching advantages as for smaller LLMs.",
            "transfer_failure_factors": "Paper notes uncertainty whether observations are model-dependent; lacks systematic GPT-4 breakdown so limitations remain unquantified.",
            "key_findings": "Larger LLMs (GPT-4) can further improve performance in textual embodied tasks, but the paper provides only qualitative statements and not detailed quantitative comparisons.",
            "uuid": "e1730.1",
            "source_info": {
                "paper_title": "LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLaMA-7B",
            "name_full": "LLaMA-7B (Open-source LLM)",
            "brief_description": "An open-source LLM that the authors tried in preliminary experiments; reported to fail on the evaluated Instruction Grounding and IQA tasks in LangSuit•E without task-specific tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "LLaMA-7B",
            "model_agent_description": "A smaller open-source transformer language model used in a preliminary attempt as an embodied agent via prompting; no fine-tuning reported in this work.",
            "pretraining_data_type": "General language pretraining (open web text); exact details not provided in the paper",
            "pretraining_data_details": "The paper states the authors attempted to use LLaMA-7B for Instruction Grounding and IQA but it 'failed in all tasks' in preliminary trials; no further dataset or training detail provided.",
            "embodied_task_name": "LangSuit•E (Instruction Grounding and Interactive QA attempted)",
            "embodied_task_description": "Same LangSuit•E textual embodied tasks; the model was evaluated as a zero-/few-shot prompted agent and did not produce appropriate embodied predictions in preliminary attempts.",
            "action_space_text": "LLM pretraining is language-modeling only; prompting-based action generation attempted.",
            "action_space_embodied": "LangSuit•E discrete textual action tokens (same as others), but the model produced inadequate outputs.",
            "action_mapping_method": "Prompted textual actions parsed by environment; no successful mapping achieved in practice for this model in the authors' attempts.",
            "perception_requirements": "Textual observations from LangSuit•E (model received prompts describing observations).",
            "transfer_successful": false,
            "performance_with_pretraining": "Reported as failing in all (attempted) tasks; the paper does not provide numeric SRs but states the attempts 'failed in all tasks'.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "No fine-tuning; preliminary zero-/few-shot prompting only. No training sample counts reported.",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "N/A for this preliminary negative result; paper suggests model capacity and training regimen matter and that small open-source LLMs performed poorly out-of-the-box.",
            "transfer_failure_factors": "Insufficient model scale/capacity and/or lack of instruction-tuning/coherent CoT behaviors for embodied prediction; authors note small models like LLaMA-7B were unable to make appropriate embodied predictions regardless of prompt strategy.",
            "key_findings": "Small open-source LLMs (LLaMA-7B, as tested here) failed to act successfully as zero-shot embodied agents in LangSuit•E without further fine-tuning or adaptation, highlighting a capacity gap relative to larger API models.",
            "uuid": "e1730.2",
            "source_info": {
                "paper_title": "LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ALFWorld / TextWorld mention",
            "name_full": "ALFWorld (aligned ALFRED to TextWorld) and TextWorld",
            "brief_description": "Prior work that aligns text-based game frameworks (TextWorld) with embodied benchmarks (ALFRED) so that agents trained in text environments can be evaluated on embodied planning; discussed in related work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "agents trained in TextWorld / ALFWorld pipelines (generic)",
            "model_agent_description": "Frameworks and agents from prior literature where text-based game environments (TextWorld) were used to create embodied-like tasks (ALFWorld) to bridge language-game pretraining and embodied instruction following.",
            "pretraining_data_type": "Text-based games / interactive fiction (TextWorld)",
            "pretraining_data_details": "Not specified in this paper; ALFWorld is referenced as prior work that simplifies embodied challenges by providing full world observations and relying on simulators to emit textual descriptions.",
            "embodied_task_name": "ALFWorld / ALFRED-aligned tasks (household instruction-following, pick-and-place, navigation)",
            "embodied_task_description": "Benchmarks for instruction following and embodied manipulation originally in 3D simulators (ALFRED/AI2-THOR) that can be expressed as text games in ALFWorld; used to study transferring text-based planning to embodied action.",
            "action_space_text": "Text-game style high-level commands (e.g., 'go to the kitchen', 'pick up the cup'); typical TextWorld command set.",
            "action_space_embodied": "Symbolic simulator actions corresponding to navigation and manipulation primitives in 3D simulators (AI2-THOR / ALFRED).",
            "action_mapping_method": "Prior ALFWorld work provides a mapping between textual commands (from TextWorld) and simulator actions in ALFRED/AI2-THOR; this paper cites ALFWorld as background but does not experiment with it.",
            "perception_requirements": "Prior work often used textual descriptions produced by simulator or full (global) observations simplified into text.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "ALFWorld demonstrates the utility of text-based interactive training data for embodied planning when the environment emits informative textual descriptions and action vocabularies align.",
            "transfer_failure_factors": "Prior criticism: ALFWorld and similar approaches often present full world observations (less realistic partial observability) and rely on simulators; thus transfer to realistic embodied perception/control is limited.",
            "key_findings": "Text-based game pretraining has been leveraged to create embodied-style tasks (ALFWorld) but previous setups often over-simplify agent observability; LangSuit•E addresses this by providing partial observations and customizable action spaces to better probe transfer.",
            "uuid": "e1730.3",
            "source_info": {
                "paper_title": "LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Wu et al. (GPT-3.5 annotation)",
            "name_full": "Wu et al. (2023b) — GPT-3.5 used to annotate action plans for AI2-THOR dataset (as cited)",
            "brief_description": "A prior pipeline cited where GPT-3.5 was employed to automatically generate action-plan annotations for indoor scene tasks in AI2-THOR, effectively using a pretrained language model to produce structured embodied plans.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "GPT-3.5 (used as annotator in cited prior work)",
            "model_agent_description": "Pretrained LLM used to auto-annotate action plan sequences for multimodal embodied datasets (triplets of scenes, instructions, action plans).",
            "pretraining_data_type": "Pretrained language model (GPT-3.5) used to generate textual action plans; original pretraining corpora not specified in this paper.",
            "pretraining_data_details": "Paper cites Wu et al. who leveraged AI2-THOR to expand scenes and GPT-3.5 to annotate action plans; details about sizes and dataset composition are in the cited work, not in this paper.",
            "embodied_task_name": "AI2-THOR based indoor planning/action annotation (used to train multimodal planners in related work)",
            "embodied_task_description": "Generation of low-level or mid-level action sequences for simulated indoor manipulation/navigation tasks in AI2-THOR to create training data for embodied agents.",
            "action_space_text": "GPT-3.5 generated structured textual action sequences representing simulator actions.",
            "action_space_embodied": "Simulator symbolic actions in AI2-THOR (navigation and manipulation primitives).",
            "action_mapping_method": "LLM was used to produce textual action plans; these plans were interpreted as simulator action sequences (annotation pipeline). The current paper cites this as related work; no experiments are run here.",
            "perception_requirements": "Cited work operates on symbolic/scene descriptions from AI2-THOR; specifics are in the referenced paper.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Using LLMs to generate action plans can bootstrap datasets for training multimodal embodied agents, leveraging LLM commonsense for plan decomposition.",
            "transfer_failure_factors": "Annotation quality depends on LLM accuracy; any hallucinations or errors in generated plans propagate into downstream supervised training.",
            "key_findings": "The paper cites Wu et al. as an example where pretrained LLMs (GPT-3.5) were used to create action-plan annotations for embodied simulators, illustrating a route from language models to embodied data creation.",
            "uuid": "e1730.4",
            "source_info": {
                "paper_title": "LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Say-Can / Do-as-I-can",
            "name_full": "Say-Can and related LLM-for-robotics mentions",
            "brief_description": "Prior robotics works (e.g., Say-Can, LLM-Planner, Vemprala et al.) that apply pretrained language models for high-level task planning and grounding in robot control are cited as motivation; these works use LLMs pretrained on language to assist embodied control.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_agent_name": "Say-Can / LLM-Planner / ChatGPT-based controllers (cited literature)",
            "model_agent_description": "Systems that combine LLMs (pretrained on language) for high-level planning with grounding modules/affordance models trained on robot-specific data for low-level control.",
            "pretraining_data_type": "Language model pretraining (general text) combined with task-specific robot affordance datasets (in the cited works); not specified here.",
            "pretraining_data_details": "The present paper references Ahn et al. (Say-Can), Song et al. (LLM-Planner), and Vemprala et al. as prior works applying language models to robot or embodied control; details are in the cited papers.",
            "embodied_task_name": "Real-world robotics control / grounded navigation and manipulation (as in cited works)",
            "embodied_task_description": "Robotic tasks requiring mapping language instructions to robot actions/affordances; typically combine LLM planning with perception and low-level controllers.",
            "action_space_text": "High-level natural language instructions and discrete textual planning outputs from LLMs.",
            "action_space_embodied": "Low-level robot controllers, motor primitives, or affordance-conditioned policies (continuous or discrete depending on the cited work).",
            "action_mapping_method": "Hybrid: LLM proposes high-level steps; an affordance model or low-level controller scores/executes grounded actions (cited works use learned affordance models or learned value functions). This paper cites these as related approaches but does not experiment with them.",
            "perception_requirements": "RGB / depth / proprioception and affordance estimators in the cited robotics systems (not part of LangSuit•E).",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Combining LLM commonsense with learned affordances/robot controllers enables practical robot instruction following in prior work.",
            "transfer_failure_factors": "Requires large amounts of robot-specific data and careful grounding; pure language pretraining alone is insufficient to control low-level motors without learned perception/affordance modules.",
            "key_findings": "Paper positions LangSuit•E as complementary to these robotics efforts by isolating the language-planning component in a textual embodied world to study language-to-action transfer without visual perception noise.",
            "uuid": "e1730.5",
            "source_info": {
                "paper_title": "LangSuit · E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ALFWorld",
            "rating": 2
        },
        {
            "paper_title": "TextWorld",
            "rating": 2
        },
        {
            "paper_title": "PaLM-E",
            "rating": 2
        },
        {
            "paper_title": "Do as I can, not as I say: Grounding language in robotic affordances",
            "rating": 2,
            "sanitized_title": "do_as_i_can_not_as_i_say_grounding_language_in_robotic_affordances"
        },
        {
            "paper_title": "Wu et al. (2023b) - GPT-3.5 used to annotate AI2-THOR action plans",
            "rating": 2,
            "sanitized_title": "wu_et_al_2023b_gpt35_used_to_annotate_ai2thor_action_plans"
        },
        {
            "paper_title": "Vemprala et al. (2023) - ChatGPT for zero-shot robotics controls",
            "rating": 1,
            "sanitized_title": "vemprala_et_al_2023_chatgpt_for_zeroshot_robotics_controls"
        },
        {
            "paper_title": "Reed et al. (2022) - LLMs in Minecraft / language-to-action work",
            "rating": 1,
            "sanitized_title": "reed_et_al_2022_llms_in_minecraft_languagetoaction_work"
        }
    ],
    "cost": 0.023989999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LangSuit•E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments
24 Jun 2024</p>
<p>Zixia Jia 
Mengmeng Wang 
National Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Baichen Tong 
National Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Song-Chun Zhu 
National Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Peking University</p>
<p>Tsinghua University</p>
<p>Zilong Zheng <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#122;&#108;&#122;&#104;&#101;&#110;&#103;&#64;&#98;&#105;&#103;&#97;&#105;&#46;&#97;&#105;">&#122;&#108;&#122;&#104;&#101;&#110;&#103;&#64;&#98;&#105;&#103;&#97;&#105;&#46;&#97;&#105;</a> 
National Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Weize Chen 
Yusheng Su 
Jingwei Zuo 
Cheng Yang 
Chenfei Yuan 
Chen Qian 
Chi-Min Chan 
Yujia Qin 
Yaxi Lu 
Ruobing Xie 
Maxime Chevalier-Boisvert 
Dzmitry Bahdanau 
Salem Lahlou 
Lucas Willems 
Chitwan Saharia 
Huu Thien 
Yoshua Nguyen 
2019 Bengio 
Babyai 
Marc-Alexandre Côté 
Ákos Kádár 
Xingdi Yuan 
Ben Kybartas 
Tavian Barnes 
Emery Fine 
RuoJames Moore 
Yu Tao 
Matthew 2021 Hausknecht 
Layla El Asri 
Mahmoud Adada 
Wendy Tay 
Ishita Dasgupta 
Christine Kaeser-Chen 
Kenneth Marino 
Xiang Deng 
Yu Gu 
Boyuan Zheng 
Shijie Chen 
Danny Driess 
Fei Xia 
S M Mehdi 
Corey Sajjadi 
Lynch 
Kiana Ehsani 
Winson Han 
Alvaro Herrasti 
Eli Van- Derbilt 
Luca Weihs 
Eric Kolve 
Aniruddha Kemb- Havi 
Roozbeh 2021 Mottaghi 
Manipulathor 
Linxi Fan 
Guanzhi Wang 
Yunfan Jiang 
Ajay Man- Dlekar 
National Key Laboratory of General Artificial Intelligence
BIGAI</p>
<p>Yuncong Yang 
Haoyi Zhu 
Andrew Tang 
De-An Huang 
Yuke Zhu 
Anima Anandkumar 
Minedojo 
Daniel Gordon 
Aniruddha Kembhavi 
Mohammad Rastegari 
Joseph Redmon 
Wenlong Huang 
Ted Xiao 
Harris Chan 
Jacky Liang 
Pete Florence 
Andy Zeng 
Jonathan Tomp- Son 
Igor Mordatch 
Yevgen Chebotar 
Abhinav Gupta 
Ali 2017b Farhadi 
Nelson F Liu 
Kevin Lin 
John Hewitt 
Ashwin Paran- Jape 
Michele Bevilacqua 
Fabio Petroni 
Percy 2023a Liang 
Lost 
Xiao Liu 
Hao Yu 
Hanchen Zhang 
Yifan Xu 
Xuanyu Lei 
Hanyu Lai 
Hangliang Ding 
Kaiwen Men 
Kejuan Yang 
Shudan Zhang 
Ao- Han Zeng 
Zhengxiao Du 
Chenhui Zhang 
Sheng Shen 
Tianjun Zhang 
Huan Sun 
Minlie Huang 
Yuxiao Dong 
Xavier Puig 
Kevin Ra 
Marko Boben 
Jiaman Li 
Scott Reed 
Konrad Zolna 
Emilio Parisotto 
Ser- Gio Gomez Colmenarejo 
Alexander Novikov 
Gabriel Barth-Maron 
Mai Gimenez 
Yury Sulsky 
JostJackie Kay 
Tobias Springenberg 
Noah Shinn 
Federico Cassano 
Ashwin Gopinath 
Karthik R Narasimhan 
Shunyu 2023 Yao 
Mohit Shridhar 
Jesse Thomason 
Yonatan Bisk 
Luke Zettlemoyer 
Dieter 2020a Fox 
Alfred 
Luke 
Adam Trischler 
Alfworld 
Chan Hee Song 
Jiaman Wu 
Clayton Washington 
Brian M Sadler 
Wei-Lun Chao 
Mohit IyyerTu Vu 
Xuezhi Wang 
Noah Constant 
Jerry Wei 
Jason Wei 
Chris Tar 
Yun-Hsuan Sung 
Denny 
Shenzhi Wang 
Chang Liu 
Siyuan Qi 
Shuo Chen 
Qisen Yang 
Andrew Zhao 
Jason Wei 
Dale Schuurmans 
Maarten Bosma 
Ed Chi 
V Quoc 
Denny Le 
Zhou 
Howard Chen 
John Yang 
Karthik Preprint Narasimhan 
Webshop 
Jeffrey Zhao 
Dian Yu 
Nan Du 
Izhak Shafran 
Karthik Narasimhan 
Yuan 2022 Cao 
Shuyan Zhou 
Frank F Xu 
Xuhui Zhou 
Robert Lo 
Abishek Sridhar 
Xianyi Cheng 
Daniel Fried 
Uri Alon 
Wangchunshu Zhou 
Yuchen Eleanor Jiang 
Long Li 
Jia- Long Wu 
Tiannan Wang 
Shi Qiu 
Jintian Zhang 
Jing Chen 
Ruipu Wu 
Shuai Wang 
Xizhou Zhu 
Yuntao Chen 
Hao Tian 
Chenxin Tao 
Wei- Jie Su 
Chenyu Yang 
Gao Huang 
Bin Li 
Lewei Lu 
Brianna Zitkovich 
Tianhe Yu 
Sichun Xu 
Peng Xu 
Jialin Wu 
Paul Wohlhart 
Stefan Welker 
Ayzaan Wahid </p>
<p>Arun Ahuja
Sheila Babayan, Felix Hill</p>
<p>Samuel Stevens
Boshi Wang, Huan Sun, and Yu Su</p>
<p>Aakanksha Chowdhery
Jonathan Tompson
Brian Ichter, Ayzaan Wahid, Tianhe YuQuan Vuong</p>
<p>Neural Information Processing Systems (NeurIPS)
35:, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei18343-18362, 2023Ran Gong, Zane Durante</p>
<p>Matt Deitke
Kiana Ehsani, Yuke Zhu, et al. 2017aDaniel Gordon</p>
<p>Tingwu Wang
Sanja Fidler</p>
<p>and Antonio Torralba</p>
<p>Shiji Song, and Gao Huang. 2023b
Chaofei Wang</p>
<p>LangSuit•E: Planning, Controlling and Interacting with Large Language Models in Embodied Text Environments
24 Jun 202432AD5BD8CB7A7B767ABEBD429DADDAEFarXiv:2406.16294v1[cs.CL]
Recent advances in Large Language Models (LLMs) have shown inspiring achievements in constructing autonomous agents that rely on language descriptions as inputs.However, it remains unclear how well LLMs can function as few-shot or zero-shot embodied agents in dynamic interactive environments.To address this gap, we introduce LangSuit•E, a versatile and simulation-free testbed featuring 6 representative embodied tasks in textual embodied worlds.Compared with previous LLM-based testbeds, LangSuit•E (i) offers adaptability to diverse environments without multiple simulation engines, (ii) evaluates agents' capacity to develop "internalized world knowledge" with embodied observations, and (iii) allows easy customization of communication and action strategies.To address the embodiment challenge, we devise a novel chain-of-thought (CoT) schema, EmMem, which summarizes embodied states w.r.t.history information.Comprehensive benchmark results illustrate challenges and insights of embodied planning.LangSuit•E represents a significant step toward building embodied generalists in the context of language models.</p>
<p>Introduction</p>
<p>Creating embodied agents that can perceive, think, and interact with simulated environments from the agents' own perspective is a long-standing yet imperative challenge within the realms of robots and artificial intelligence.Recent striking progress of Large Language Models (LLMs) has ushered promising avenues for constructing autonomous agents, wherein LLMs serve as their central planners or controllers, evidenced by several intriguing demos and benchmarks within simulated environments (Reed et al., 2022;Park et al., 2023;Chevalier-Boisvert et al., 2019) or even realistic physical worlds (Driess et al., 2023;Huang et al., 2022) across a diverse spectrum of domains, spanning from web browsing (Zhou et al., 2023a;Yao et al., preprint), language games (Park et al., 2023) and householding (Shridhar et al., 2021), etc.These language agents, reliant solely upon language descriptions as their inputs, can benefit from LLMs' exhibited commonsense knowledge, emergent semantic reasoning and problem-solving skills.Despite the promising advances of prior LLM-asagent tasks, it remains unclear that:</p>
<p>Without perception error caused by visual models, can LLMs behave as embodied agents to solve complicated tasks within dynamic environments?</p>
<p>Previous works mostly treat LLM agents as task planners, providing them with the global observations and all required information (e.g., entire history) in natural language to produce the next step, which contrasts the nature of chat-based language models that can potentially form their internal map and history across conversational turns.Moreover, the action spaces across tasks are specially designed and can only be evaluated within specific environments, which limits the potential capacity of evaluating the generalizability across tasks.In stark contrast, research on embodied agents and environments has gone through many successful attempts -numerous open-source embodied systems developed with realistic simulation engines, such as AI2Thor (Kolve et al., 2017b), VirtualHome (Puig et al., 2018), spurring the trend of learning embodied instruction-following challenges, such as navigation (Chattopadhyay et al., 2021), manipulations (Ehsani et al., 2021), question answering (Gordon et al., 2018),etc.Models learned within these environments, nevertheless, struggle with their dependency on simulation engines and are therefore limited to specific tasks.Other prominent research works such as PaLM-E (Driess et al., 2023) and RT (Zitkovich et al., 2023) sought to powerful vision-language models (VLMs) for embod-</p>
<p>[System]: You and your friend are two autonomus intelligent agents tasked with a shared household work.The task is to put a vase on the table.</p>
<p>[System][→Bot1]: You can see a table, two chairs in front.</p>
<p>[Bot1] Thought: To slove this task, we have to first find a vase.</p>
<p>[Bot1][→Bot2]: Search for a vase.[Bot2] Thought: Now I find two vases, I should know which one to pick up.</p>
<p>[System][→Bot2]: You can see a desk, a sofa in front.On the desk, there is a blue vase, a brown vase.</p>
<p>Interactive QA</p>
<p>How many vases are around here?I will go and explore the room...</p>
<p>Answer is [2]</p>
<p>Instruction Grounding</p>
<p>Go to the red box.</p>
<p>Sure.I can see a red box ahead.I will move ahead.</p>
<p>Multi-agent Teach</p>
<p>You are commander you have to complete a task with a follower: ...</p>
<p>Rearrangement</p>
<p>You have to recover the objects that are misplaced.The original states are ...</p>
<p>Household</p>
<p>Put a clean lettuce in dining table.</p>
<p>Sure.I will first find a lettuce...The cup_1 is on the sofa.It should be on the table.I will...</p>
<p>Multi-agent Coopeartion</p>
<p>You are hurry to finish a shared task with your friend together: ... ied scene perception and language models for lowlevel control.However, these works can only be applied to constrained environments and require large volumes of data for pre-training.
I
To fill the gap and step towards general embodied intelligence, we introduce LangSuit•E, a systematic and simulation-free testbed for evaluating LLMs' capabilities across different embodied tasks in textual embodied worlds.As listed in Tab. 1, LangSuit•E showcases 6 representative embodied tasks, with variants of the number of rooms, the number of agents, the action spaces of agents (whether they can communicate with each other or ask humans).Compared with prior LLMbased agent testbed, LangSuit•E presents superior benefits on building and benchmarking LLMs' embodied capabilities.First, due to the nature of textual worlds, LangSuit•E incorporates a general yet fully customizable indoor environment that can easily adapted to most simulated embodied environments, (e.g., AI2Thor (Kolve et al., 2017b), Vir-tualHome (Puig et al., 2018)), without resorting to variants of simulation engines and benchmark versions.Second, LangSuit•E only presents embodied observations to agents at each step.A typical example is shown in Figure 1, where only in-front observations within certain distances (unblocked by walls) are presented to agents at each step.Prior study (Huang et al., 2022) has shown that language models can potentially form "internalized world knowledge" through thinking in language.For in-stance, they shall potentially know that executing "turn_right by 90 degrees" four times shall remain in the same direction as the beginning.Such capacity could potentially enable LLM agents to reason over action histories and observations to plan for the next.Third, the agents in LangSuit•E are fully-customizable w.r.t.their action spaces and communicative capabilities, i.e., one can easily adapt the communication and acting strategy from one task to another.</p>
<p>To evaluate the current prevalent LLMs's performance on LangSuit•E, we derive a comprehensive benchmark across 6 different tasks.Of note, annotating or generating golden trajectories is commonly labor-intensive and time-consuming.In LangSuit•E, we can easily form a benchmark by leveraging and modifying existing annotations, a by-product benefit of pursuing such a general textual embodied world.An underlying hypothesized benefit is that knowledge learned from one task can also enhance the performance of the others.</p>
<p>Lastly, inspired by grid cells in neural science (Banino et al., 2018) and the "lost-in-middle" observation of long-context language models (Liu et al., 2023a), we propose EmMem, an Embodied planning strategy with an emergent embodied Memory.Specifically, the language model learns to summarize and predict their embodied states in an in-context manner given long-context trajectories and observations.Results on the Household task present the effectiveness of the EmMem strategy.In summary, this paper makes the following contributions: i) we deliver a systematic and useful testbed with 6 exemplar tasks for testing and training LLMs as embodied agents; ii) We introduce EmMem, a novel prompting strategy for embodied tasks; iii) We show substantive benchmark results and call for future exploration in LangSuit•E.</p>
<p>Related Work</p>
<p>Evaluating LLM as Autonomous Agents The concept of LLM-as-Agent has triggered the development of many systematic and interactive environments that allow for textual interactions and show well-aligned language groundings in visual or physical modalities.TextWorld (Côté et al., 2018) is one of the earliest testbeds that presented a text-based game sandbox for training and testing reinforcement learning (RL) agents.Based on this framework, ALFWorld (Shridhar et al., 2021) proposed an embodied challenge that aligns the ALFRED (Shridhar et al., 2020a) benchmark to TextWorld.However, this environment still requires a simulator to generate text descriptions and simplifies the embodied challenge by presenting full world observations to agents (Fig. 2).More recently, enhanced by harnessing the capacities of chat-based LLMs, a variety of language agents targeting a spectrum of different tasks have been demonstrated.For example, WebShop (Yao et al., preprint), Mind2Web (Deng et al., 2023) and WebArena (Zhou et al., 2023a) present environments on open-domain web shopping and browsing; Agents (Zhou et al., 2023b) design a library that supports long-short term memory; Agent-Bench (Liu et al., 2023b) presents a comprehensive evaluation of 25 different LLMs across 8 environments.In addition to single-agent settings, recent advances also show emergent behaviors in multiagent LLM systems (Gong et al., 2023;Park et al., 2023;Chen et al., 2023;Wang et al., 2023b).However, most of these tasks over-simplify the agent's observations and actions by presenting with full knowledge of the world's status.</p>
<p>LLMs for Embodied Tasks and Robotics The rise of LLMs has spurred many inspirations on conventional embodied and robotics systems.(Vemprala et al., 2023) presented an empirical success of using ChatGPT in zero-shot robotics controls.Say-Can (Ahn et al., 2022) and LLM-Planner (Song et al., 2022) employ visual navigation to collect information in the house for the challenging grounded plan generation.Another thread of works presents attempts on multimodal embodied environments by leveraging a visual-language pretraining model for world perception and LLMs for planning.For instance, many works leverage LLMs to enhance agent ability in the Minecraft environment (Wang et al., 2023a;Zhu et al., 2023;Reed et al., 2022;Baker et al., 2022;Fan et al., 2022).Wu et al. (2023b) construct a multimodal dataset containing triplets of indoor scenes, instructions, and action plans, where they employ the AI2-THOR simulator (Kolve et al., 2017a) to expand indoor scenes as environments and leverage GPT-3.5 to automatically annotating action plans.However, due to the imperfect multimodal understanding and dynamic scene environments, these tasks are limited to single-agent settings and simple manipulations and navigation in domain-specific environments.Differently, LangSuit•E presents a simulation-free and embodied framework that allows a variety of environments and agent settings.</p>
<p>Planning and Acting with LLMs Inspired by the success of chain-of-thought (CoT) on language reasoning tasks, many works propose to enhance the planning and acting capabilities of LLMs in a similar manner.ReAct (Yao et al., 2022) proposes to combine CoT reasoning and actions to enhance the efficiency of language models.Dasgupta et al. (2023)</p>
<p>High-level actions goto [loc]</p>
<p>Go to a location that is in current observation.... ...</p>
<p>Communicative actions chat [message]</p>
<p>Send messages to other agents.</p>
<p>ask [message]</p>
<p>Send queries to users.</p>
<p>Pan et al. ( 2023) transfers visual perception from the virtual environment to language descriptions, then prompts LLM to solve the specific navigation tasks only leveraging language descriptions.But different from them, we provide a unified embodied environment that is capable of real-time feedback and integrates various embodied tasks.</p>
<p>The LangSuit•E Testbed</p>
<p>The goal of LangSuit•E is to provide a unified testbed thereby evaluating the diversified ability of LLM-as-Agents in a dynamic embodied environment with textualized perceptual representation and realistic physical limitation.Moreover, by unifying various embodied tasks in one environment, agents could improve the ability of knowledge transfer and construct consistent world cognition from interactions with the environment to solve different kinds of tasks.</p>
<p>Generic Embodied Task Planning</p>
<p>LangSuit•E aims to construct a general textual embodied world by defining a lightweight and simulation-free environment system.The embodied task of LangSuit•E can be formulated as a partially observable Markov decision process (POMDP) (S, A, T , R, U, O), which encompasses embodied agent state space S, action space A, deterministic transition function T : S × A → S, reward function R, task instruction space U and embodied observation space O.</p>
<p>ALFWorld LangSuit•E</p>
<p>Global Description</p>
<p>Welcome !You are in the middle of the room.Looking around you, you see a diningtable, a stove, a microwave, and a cabinet.</p>
<p>Your task is to: Put a pan on the diningtable.</p>
<p>[1] goto the cabinet You arrive at the cabinet.</p>
<p>The cabinet is closed.</p>
<p>[2] open the cabinet</p>
<p>The cabinet is empty.……</p>
<p>Embodied Description</p>
<p>You  Embodied States The state refers to the representation of the environment that an agent is in at a given time.LangSuit•E encompasses diverse indoor scene layouts.The state of our embodied setting includes the indoor arrangement, object information, the history of the previous action decisions and executive outcomes, the overall target or programmatic sub-goals, the limitations that must be followed, and so on.</p>
<p>VS.</p>
<p>VS.</p>
<p>High-Level Actions Low-Level Actions</p>
<p>Embodied Observations To simulate real-world conditions, the environment is partially observable, which means that an agent is not capable of perceiving the entire world or environment states.Based on this, LangSuit•E provides the embodied observations of partial objects in the environment with full customization of each agent's view range.In particular, LangSuit•E includes a module dedicated to describing environmental objects that are perceptible within an agent's uniquely configured visual scope using natural language.For instance, the language expressions could be "You can see a red ball on your right" or "You can see nothing ahead".Embodied Actions LangSuit•E incorporates a wide spectrum of executable embodied actions, covering most actions typically encountered within various embodied environments.The agent decides to execute a specific action in each time step according to the embodied state and its internal world knowledge.Tab. 2 presents an enumeration of the action types that are currently accommodated in the LangSuit•E.Refer to Tab. 9 in Appendix for the complete action set.Furthermore, LangSuit•E exhibits substantial flexibility by allowing for the customized configuration of action scopes.For example, the movement distance associated with the move_ahead action can be readily modified by adjusting the relevant hyperparameters, thereby facilitating tailored adjustments to suit specific embodied task requirements.</p>
<p>Feedback LangSuit•E environment conveys feedback through the use of natural language descriptions.This feedback is automatically generated by our system by evaluating the feasibility of the action predicted by LLMs in the context of the current state.The evaluation takes into account physical constraints imposed by both the agent and the environment, such as the agent's grasping range or environmental obstacles, ensuring that the action aligns with the agent's physical capabilities and the environmental context; refer to Fig. 3 and Appendix for examples.</p>
<p>Communications in LangSuit•E</p>
<p>The platform additionally provides multi-agent and human-agent communication channels to enable multi-agent cooperation and human-in-the-loop tasks.The Communicative actions in Tab. 2 are additional actions to be appended for this capacity.</p>
<p>(Multi-)Agent Interactions Multi-agent cooperations are widely explored in many recent LLM-asagent tasks (Zhou et al., 2023b;Park et al., 2023).</p>
<p>In LangSuit•E, the agents are fully customizable regarding their action spaces and communicative capabilities, i.e., one can easily adapt the communication and acting strategy from one task to another.See Fig. 1 for a typical example.</p>
<p>Human Communication LangSuit•E supports human-agent interactions through two primary mechanisms: firstly, it empowers humans to control an agent or play the role of the agent to cooperate with other agents.Secondly, it affords agents the capability to seek clarification from humans in instances of environmental or instructional ambiguity, enabling humans to offer assistance by imparting pertinent information via a communication protocol, thus enhancing task completion.</p>
<p>Benchmark LangSuit•E Tasks</p>
<p>We integrate a diverse range of embodied tasks within the unified LangSuit•E testbed.Tab. 1 demonstrates the specific categories of tasks that have been accommodated within LangSuit•E.We carefully select the six tasks to illustrate how our testbed can be applied to single-agent embodied understanding and planning (Instruction Grounding, Interactive QA, Household and Rearrangement), multi-agent cooperation (MA-WAH), multi-agent teaching (MA-Household), each of which represents a significant challenge within the current embodied community.We re-implement all tasks in our embodied environments to reach the objective of a unified simulation-free testbed.Notably, LangSuit•E has an inherent capacity for facile expansion and adaptation to accommodate additional embodied tasks with minimal resource consumption, for the reason of the wealth meta-capabilities it possesses, encompassing easy customization of communication and action strategies and the ability to support interactions involving multiple agents, among others.</p>
<p>Task Instructions Task instruction provided to an agent focuses on the characteristics and constraints of tasks with a particular type, involving customized descriptions of the specific type, action spaces of tasks, and optional few-shot examples to show the diverse execution processes of some tasks under the same type.</p>
<p>LangSuit•E provides a template for the automated generation of diverse task instructions.By filling out customized task descriptions, definitions of action spaces pertinent to a particular task type, or the incorporation of few-shot illustrative examples, LangSuit•E is capable of generating distinct sets of instructions that serve as prompts for LLMs regarding different tasks.The tasks that the current version of LangSuit•E supports:</p>
<p>Task I. Instruction Grounding (IG) (Chevalier-Boisvert et al., 2019) provides synthetic naturallooking instructions (e.g., "put the red ball next to the box on your left"), requiring the agent to navigate the world (including unlocking doors) and move objects to specified locations.</p>
<p>Task II.Rearrangement (Weihs et al., 2021) involves moving and modifying (i.e., opening/closing) randomly placed objects within a room to obtain a goal configuration.</p>
<p>Task III.Interactive QA (IQA) (Gordon et al., 2018) is the task of answering questions that require the agent to interact with a dynamic environment, which contains three different question types: Existence questions ("Is there an apple in the kitchen?"),Counting questions ("How many forks are present in the scene?"), and Spatial Relationship, called Contains questions ("Is there lettuce in the fridge?").</p>
<p>Task IV.Household (Shridhar et al., 2020a) tasks contain both high-level goals like "Rinse off a mug and place it in the coffee maker."and low-level language instructions like "Walk to the coffee maker on the right."Task V. Multi-agent Teach (MA-Teach) (Padmakumar et al., 2022) tasks should be completed with the cooperation of two agents.A Commander with access to oracle information about a task communicates in natural language with a Follower.The Follower navigates through and interacts with the environment to complete tasks varying in complexity from "make coffee" to "prepare breakfast", asking questions, and getting additional information from the Commander.</p>
<p>Task VI.Multi-agent Coopeartion tasks focus on evaluating the cooperation and communication ability of multiple agents to accomplish shared goals.We follow C-WAH (Zhang et al., 2023) to use a watch-and-help (WAH) configuration in this task.Specifically, all agents shall negotiate with each other to break down the task based on their individual observations and capabilities.Different from C-WAH, all agents in this task can only perceive their own embodied observations.</p>
<p>Gold Trajectories Generation</p>
<p>We have implemented an extra expert module for task I-IV (the multi-agent settings require agent interactions), dedicated to generating gold trajectories and affording us the capability to construct training datasets that may be subsequently employed for the potential fine-tuning of LLMs.Given a task without gold trajectories, our expert module automatically produces a reasonable and executable trajectory, including every-step actions.Specifically, we first analyze a list of objects necessitating manipulation according to the answer or the final state of the task.Second, we construct a list of sub-tasks corresponding to each operation of an object and carefully devise the execution sequence of sub-tasks.Finally, we adopt the A-star algorithm to chart the shortest paths between the dynamic agent location (the agent's location after finishing the last sub-task) and the target object for each sub-task.As a consequence, our expert module outputs an exhaustive sequence of actions meticulously crafted to fulfill the task at hand.</p>
<p>Evaluation Metrics</p>
<p>We follow metric design schemas as in (Weihs et al., 2021;Shridhar et al., 2020a) to provide the following evaluation metrics:</p>
<p>• Success Rate (SR): the ratio of tasks in which an agent achieves the target goal or state.• Goal-Conditioned Success Rate (Goal-SR): the sub-goal completion ratio of a task, averaged across all the tasks of the same type.• Misplaced Rate (Misplaced): the misplaced objects in the Rearrangement task, which computes the ratio of misplaced objects at the end of the task, denoting better performance with a lower value.• Fixed Strict Rate (Fixed Strict): the correctly reset objects in the Rearrangement task, which computes the ratio of correctly reset objects at the end.• Accuracy (Acc): the proportion of correct answers compared to the total number of questions.</p>
<p>The metric solely evaluates on the IQA task.However, all these strategies failed to reveal satisfactory results in our embodied environments; refer to Fig. 3 for typical failures.We present EmMem, a novel embodied planning paradigm that reasons and summarizes the agent's current embodied memory in an in-context and</p>
<p>additional trail of verbal reflection). Because</p>
<p>Reflexion will try an additional trail if the task fails and the summarization of the whole last trail will be input to the new trail planning through the Task prompt, we do not show the case in this figure.More details and the prompt instructions for these strategies can be found in Appendix G. emergent manner.Specifically, the LLM agent has to explicitly predict and summarize their current embodied state before proceeding to the actual planning and action prediction step.We provide two underlying rationales for such prompts: First, as for mammals' embodied navigation, neurons like grid cells integrate path information and are thought to be critical in self-motion (Banino et al., 2018;Hafting et al., 2005).The EmMem, auto-regressively in principle, produces integrated embodied information as grid cells.Second, recent literature has observed the lost-in-middle performance for longcontext language models (Liu et al., 2023a).Our EmMem enhances the "middle" information in a residual-like approach (He et al., 2016).Fig. 3 illustrates a case study of EmMem and other strategies.</p>
<p>Experiments</p>
<p>Setup</p>
<p>We evaluate the ability of popular LLMs as agents to solve embodied tasks in our embodied textual environment.It is crucial to note that our testbed does not provide any global information, necessitating that agents mobilize more capabilities in the more realistic and complicated embodied environment to execute tasks, including comprehension of embodied description, retention of historical exploration within the environment, and selfawareness of its current state.We mainly use1 gpt-3.5-turbo(with temperature being 1) to evaluate on LangSuit•E.Notably, for API-cost economy, We evaluated 100 randomly sampled tasks for IG, Household, and Rearrangement, along with 50 randomly sampled tasks for each subtask of IQA (Exists, Contains, and Counts), MA-Household, and MA-WAH.We conduct experiments both in the low-level (fine-grained actions) and high-level action settings.The evaluation metrics adopted mainly are introduced in Section 3.4.</p>
<p>Main Results and Observations</p>
<p>In this section, we summarize the empirical results and focus on addressing the following questions: Q1: How do previous CoT strategies perform in LangSuit•E?Fig. 3 and Tab. 3 show the qualita-   et al., 2022).In LangSuit•E, we also incorporate the ask-for-help setting to obtain human preferences and selection, especially when it comes to ambiguous task instructions (a qualitative demonstration in Fig. 4).</p>
<p>Q4: Does the temperature highly influence the EmMem performance?We conducted experiments with different temperature settings for GPT-3.5 and reported the corresponding standard deviations in Tab. 5. We have observed consistent Task: Put the gold vase into the white safe.Obs: You see nothing.You can try to take action like ask, move_ahead, turn_left or turn_right …… [1] Thought: To complete the task, I need to find the gold vase and the white safe.Then I can put the gold vase into the white safe.I don't have any information about the location of the gold vase or the white safe.I should ask for help to find them.</p>
<p>[1] Act: ask [Where is the gold vase?] Feedback: Human response: On your left.</p>
<p>[2] Thought: Since the gold vase and the white safe are not in my current view, I need to explore the environment to find them.performance across various temperature settings.Our hypothesis is that the CoT strategy, coupled with exemplar cases, plays a crucial role in guiding LLMs to follow a consistent reasoning trajectory.This observation underscores the effectiveness of our approach in maintaining stability and reliability in the model's performance.Q5: How does the EmMem perform in the Multiagent settings?We only use high-level actions in multi-agent tasks to reduce task complexities.We observe that all API-based agents maintain the capacity of multi-agent cooperations with relatively high Goal-SRs.We did not report low-level results as there is barely a successful case.As demonstrated in Tab. 4, EmMem achieves consistent improvement, except for the Goal-SR metric in MA-Household with Reflexion.Note that GPT-3.5 achieves high performance in MA-WAH tasks in most of the prompt strategies, because the feedback of each action provided in MA-WAN contains the self-state of agents.It is consistent with our intuition for designing EmMem: explicitly predicting and summarizing agents' current embodied state before proceeding to the actual planning and action prediction step is beneficial.It is exciting that even with the self-state of agents the environment provided, EmMem inspiring the self-awareness of agents' current stat still further improves the performance.</p>
<p>Conclusion</p>
<p>We introduce LangSuit•E, a lightweight and simulation-free testbed in textual embodied worlds.Designed to be fully embodied, LangSuit•E aims to test LLM agent's embodied planning capacity without errors from visual perceptions.To further address the embodiment challenge, we devise a novel CoT schema, EmMem, which summarizes embodied states w.r.t.history information.Although benchmark results indicate the effectiveness of Em-Mem, there still remains a huge performance gap compared with SFT results on individual tasks.As such, we call for future exploration of embodied generalists on LangSuit•E.</p>
<p>Limitation</p>
<p>In this work, we endeavor to investigate how LLM performs and understands embodiment in a generic embodied textual setting.Nevertheless, our platform maintains the potential to be bridged with most simulators via additional visual language models (VLMs).Due to the page limitation and workload capacity, it remains unclear whether such linkage would be a successful attempt, even if we have obtained a powerful LLM embodied agent.We would like to leave it as a strong inclination for future research on generic embodied multimodal agents.</p>
<p>Moreover, the high demand of LLM API calls leads to a substantial querying price for experiments.Preliminary studies show that open-source language models, especially small models (e.g., LLAMA 2-7B), are unable to make appropriate embodied predictions in our environment, no matter what kind of prompting strategy is used.Nevertheless, it would be more than valuable to investigate how open-source language models can be trained or finetuned in the LangSuit•E environment.</p>
<p>Additionally, following previous works (Yao et al., 2022), we mainly use GPT-4 and gpt-3.5turbofor all experiments.Though we have observed performance boosts with a more powerful model, whether all observations and analyses are model-dependent remains unknown.Intuitively, our framework is model-agnostic and can be generalized to all embodied tasks.We are more than willing to experiment on more APIs once we have the access.</p>
<p>Lastly, we didn't go into too much on human communications and multi-agent tasks.Nevertheless, our platform maintains the capabilities to be generalized to multi-agent settings and human communicative settings (all you need is to update the configuration of agents).As such, learning and experimenting with multiple embodied agents in LangSuitE could also be a valuable future exploration trend</p>
<p>Ethics Statement</p>
<p>In this work, we aim to deliver a generic, lightweight and simluation-free testbed for general embodied planning.By leveraging large language models, the main purpose of this research is to benefit the embodied AI and language reasoning community.Moreover, we devise a novel reasoning paradigm, EmMem, which grants agents with emergent embodied memories.Going beyond embodied understanding, we believe the CoT paradigm is general and can be generalized to more reasoning and planning tasks.A The motivation of the design for LangSuit•E</p>
<p>First, we want to design our unified textual environment being general, embodied, interactive, and communicative, thereby containing most of the typical embodied task categories, such as navigation, rearrangement, human interaction, multi-agent cooperation, and so on.Therefore, we transfer typical embodied tasks, carefully selecting action sets that are both suitable for our environment and the original task.The differences between our benchmark and original embodied tasks are that our environment is fully embodied and the actions are lowlevel (fine-grained atom actions).Second, As the previous platform generally gives the global description of the environment, trained models in the simulated environment are difficult to apply to real environments.Our platform simulation more realistic situation that agent has a limited view scope and action scope.</p>
<p>Third, The simulation-free platform makes various embodied tasks conveniently and easily performed in a unified fashion.</p>
<p>B Data and Code Release</p>
<p>All code and data will be publicly available for use.The data and code are under MIT license.All resources can only be used for research purposes.More details of the construction process of LangSuit•E can be found in the codebase.</p>
<p>C Time complexity comparison</p>
<p>We compare the computational cost for one-step execution/rendering on simulated environments vs. LangSuit•E.The time interval (Time Int) counts from receiving one LLM inference results to the next query to LLM (thus the latency caused by APIcall is not counted).We run all experiments on an Intel Core i9 with 64GB RAM.Results are shown in Tab. 6.Our LangSuit•E demonstrates significant systemwide efficiency improvements.
p s = s × L * max(L * , L)(1)
where L is the number of actions the agent took in the episode, and L * is the number of actions in the expert demonstration.</p>
<p>As can be seen from the Tab. 7, after using the path weighted score, the performance on the Act strategy does not decline significantly.However, for other tasks, the agent is encouraged to think more, leading to an increase in the number of steps and consequently greater penalties, resulting in a more noticeable decline.</p>
<p>E Customized configuration of an agent.</p>
<p>LangSuit•E supports the customized configuration of agents.It allows the configuration of prompt strategies, the initialization position of agents, inventory capacity, focal length, manipulate distance and view distance.For more details, see Tab. 8.  Clean object with a basin, a sink or a bathtub.go_explore [room] Explore around the room.go_check [recep] Go to receptacle and check its inside.go_grab [obj] Go to object and pick it up.go_put [recep] Go to receptacle and put the object in hand into it.</p>
<p>Communicative actions chat [message]</p>
<p>Send messages to other agents.</p>
<p>ask [message]</p>
<p>Send queries to users.</p>
<p>F Pseudo-Code for Expert Trajectory Generation</p>
<p>Algorithm 1 shows the pseudo-code for generating expert trajectories as references.</p>
<p>G Examples of 5 prompt strategies for task Instruction Grounding</p>
<p>We show the detailed instruction for different prompt strategies: Act, ReAct, ReAct+EmMem, Reflexion, and Reflexion+EmMem.We highlight the pivotal instruction sentences.</p>
<p>G.4 Reflexion</p>
<p>Instruction:</p>
<p>You are an autonomous intelligent agent tasked with completing a minigrid game.These tasks will be accomplished through the use of specific actions you can issue.</p>
<p>Here's the information you'll have: Task: This is the task you are trying to accomplish.</p>
<p>[:</p>
<p>Bot2] Act: move_ahead [Bot2] Act: pick_up [blue vase] [Bot2][→User ]: I can see two vases in front, which one should I pick up?The blue one.</p>
<p>Figure 1 :
1
Figure 1: Task illustration of LangSuit•E.Top-Left: a typical example of a cooperative task between two agents.The agents are entirely blind (i.e., without visual perception) and can only obtain embodied information from the system.Around: Exemplar tasks supported by LangSuit•E.</p>
<p>Figure 2 :
2
Figure 2: Comparison of our Embodied Environment Description with ALFWorld (Shridhar et al., 2021).</p>
<p>Figure 3 :
3
Figure 3: A case study for four prompt strategies on Household.Act: The model predicts the next action only.Re-Act+EmMem: Our EmMem strategy with ReAct strategy (Reason+Act) (Yao et al., 2022).Reflexion+EmMem: Our EmMem strategy with Reflexion strategy (Shinn et al., 2023) (1 additional trail of verbal reflection).BecauseReflexion will try an additional trail if the task fails and the summarization of the whole last trail will be input to the new trail planning through the Task prompt, we do not show the case in this figure.More details and the prompt instructions for these strategies can be found in Appendix G.</p>
<p>[2] Act: turn_left Feedback: Action succeeded.Turn left by '90' degrees.Obs: On your left, you see a drawer_1 ……</p>
<p>Figure 4 :
4
Figure 4: Demonstration of human communications.</p>
<p>Table 1 :
1
Statistics of Tasks Integrated in LangSuit•E.We provide a unified and simulation-free environment that can easily adapt to most embodied tasks and benchmarks.
Task TypeSimulator/Env.# of Scenes # of Tasks # of Actions Multi-Room Multi-Agent CommunicativeInstruction Grounding (IG)Mini Grid/BabyAI (2019)1055006✓✗✗RearrangementAI2THOR/Rearrangement (2021)1205008✗✗✗Interactive QA (IQA)AI2THOR/IQA (2018)3030005✗✗✓HouseholdAI2THOR/ALFred (2020a)12050612✗✗✗Multi-agent Household (MA-Household) AI2THOR/TEACh (2022)12020013✗✓✓Multi-agent Watch-and-Help (MA-WAH) VirtualHome/Co-WaH (2023)2506✓✓✓</p>
<p>investigate a Planner-Actor-Reporter agent paradigm, where the Planner is equipped with a language model to perform reasoning.Wu et al. (2023a) propose a Plan, Eliminate and Track framework, aiming to mitigate constraints, such as finetuning inefficiency and bias from pre-training, of LLM with transformer architecture.Related to us,</p>
<p>Table 2 :
2
Examples of different action settings in LangSuit•E.Refer to Appendix for the complete set.
Action TypeDescriptionLow-level actionsmove_aheadMove ahead by 1 step.turn_leftTurn left by 90 degrees.turn_rightTurn right by 90 degrees.pick_up [obj]Pick up an object identified by [obj].put [obj, recep] Put the object [obj] into the receptacle[recep].open [obj]Open object [obj] if it is openable.close [obj]close object [obj] if it is closeable.stop [answer]Stop making actions and conclude thefinal answer in [answer].......</p>
<p>Table 3 :
3
Evaluation results on the test set of single-agent task types in LangSuit•E.The evaluation results on Household task in terms of Path Weighted Metrics are in Appendix D.
ModelSR.IG Steps.↓ AvgRearrangement SR. Misplaced.↓ Fixed Strict.Avg Steps.↓Exists Acc.IQA Contains Acc. Counts Acc.Avg Steps.↓Household SR. Goal-SR.Avg Steps.↓Low-Level Actions SettingAct13.028.28.0115.317.612.414.936.010.916.79.021.522.8ReAct12.025.35.0117.912.38.48.918.32.113.214.026.744.7ReAct-EmMem11.022.86.0108.313.79.548.058.06.014.810.026.328.5Reflexion20.029.98.099.314.313.762.072.038.023.719.034.628.6Reflexion-EmMem16.036.810.0100.415.712.560.086.072.020.714.032.035.1High-Level Actions SettingAct77.06.95.0112.317.27.024.076.030.010.323.037.816.2ReAct84.014.37.0111.215.26.944.088.018.015.122.042.021.4ReAct-EmMem85.016.96.0109.711.99.032.080.014.013.132.047.835.2Reflexion86.014.48.0102.717.56.544.090.022.09.933.053.027.7Reflexion-EmMem86.015.78.0100.2216.28.942.080.020.016.639.753.334.1</p>
<p>Table 4 :
4
Evaluation results on the test set of multi-agent task types in LangSuit•E.
ModelMA-Household SR. Goal-SR. SR. Goal-SR. MA-WAHHigh-Level Actions SettingAct18.0044.8036.0070.17ReAct18.0060.6338.0078.20ReAct-EmMem18.0064.7446.0076.13Reflexion18.0066.5856.0088.43Reflexion-EmMem20.0065.8164.0090.37ca-pability of considering the correct plans after theaction failed, but explores more steps, resultingin exceeding our step limit (in the low-level ac-tion setting on Household). Notably, ReAct dropsthe performance seriously in the IQA tasks in low-level action setting. A possible reason is that CoTcauses more severe hallucination problems, as illus-trated in Vu et al. (2023). The Reflexion, whichgoes through more trials and self-reflection pro-cess, showcases significantly better performanceon most task types. And equipped with EmMem,the performances are further improved in somecases. ➋ Low-level action vs. high-level actions:We compare the high-level action sets with low-
tive and numerical results of previous CoT methods on single-agent tasks.Our observations can be summarized as follows: ➊ Act-only vs. Re-Act vs. Reflexion: The Act can not strictly follow our desired format (it generates two actions in each step, where we only require the first one), and it predicts the stop action even though the task is not completed.Differently, ReAct has the level actions introduced in Tab. 2. In a high-level setting, an agent does not need to know how to perform low-level action sequences to achieve a goal place/object.Empirically and intuitively, we can observe that high-level actions make embodied tasks relatively easy to solve (fewer steps and higher SR.), which indicates the challenges of our fully embodied action setting.Although prompts with Thought (Reflexion, EmMem) increase the average execution steps of GPT-3.5, the performances of GPT-3.5 are improved in most cases.It is surprising that the Thought process drops the Fixed Strict scores in the low-level actions setting on Rearrangement.Q2: How does EmMem help in embodied planning tasks?As shown in Tab. 3 and case results Fig.3, with reasoning and summarizing the current embodied state in the ReAct+EmMem setting, an agent is more clear about the state of itself and the execution process of a task, hereby successfully carrying out the task; With both reflexion and memory (Reflexion+EmMem) strategies, an agent can recognize its mistakes quickly and reformulate following plans efficiently.We also observe that EmMem is more useful under the low-level action settings compared with high-level ones.Such observation is in line with the intuition that low-level settings can benefit more from embodied memory.Q3: How can human-in-the-loop be helpful?Previous works have shown that appropriate human-agent communication can substantially improve the efficiency in embodied grounding (Huang</p>
<p>Table 5 :
5
Evaluation results of varying temperatures (Temp) on Household using Reflexion-EmMem strategy in high-level actions setting.
Temp0.51.01.5SR.40.00±3.46 39.67±0.58 40.67±0.58Goal-SR. 52.47±3.71 53.30±2.43 52.86±1.13</p>
<p>Table 6 :
6
Time complexity comparison of simulated environments with LangSuit•E.Omni is the abbreviation of OmniGibson (OmniVerse) which is a physical-based simulator, and AI2Thor (Unity) takes only symbolic actions.
Env. (Engine)Omni AI2Thor LangSuit•ETime Int (ms) 67.907.811.89</p>
<p>Table 7 :
7
Evaluation results on Household task with Path Weighted Metrics.
ModelHousehold SR. Goal-SR.Low-Level Actions SettingAct6.6521.94ReAct9.5024.71ReAct-EmMem7.1921.13Reflexion14.8925.73Reflexion-EmMem11.1922.77High-Level Actions SettingAct22.3837.13ReAct18.6435.52ReAct-EmMem28.2743.02Reflexion28.2340.00Reflexion-EmMem34.4543.80D Path Weighted Metrics
We additionally utilized Path Weighted Metrics(Shridhar et al., 2020b)to calculate the path weighted score on Household task.This weighted score penalizes results that complete the task but take a longer number of steps.The introduction of Path Weighted Metrics allows the evaluation to focus not only on task completion but also on efficiency and path length, thereby providing a more comprehensive assessment of the model's performance.The path weighted score p s for metric s is given as:</p>
<p>Table 8 :
8
Customized configuration of an agent.
TypeDescriptionagent_typeWhether an agent has ReAct ability.agent_positionThe initial position of an agent.inventory_capacityThe number of objects an agent canhold.focal_lengthInversely proportional to viewing angle;shorter focal length corresponding tolarger view field.manipulate_distance The max distance that an agent's handcan reach.view_distanceThe max distance that an agent's viewcan reach.</p>
<p>Table 9 :
9
Action settings in LangSuit•E.
Action TypeDescriptionLow-level actionsmove_aheadMove ahead by 1 step.turn_leftTurn left by 90 degrees.turn_rightTurn right by 90 degrees.pan_leftMove left by 1 step.pan_rightMove right by 1 step.pick_up [obj]Pick up an object identified by [obj].drop [obj]Drop the object [obj] to the agent's cur-rent position.put [obj, recep]Put the object [obj] into the receptacle[recep].open [obj]Open object [obj] if it is openable.close [obj]close object [obj] if it is closeable.toggle_on [obj]Toggle on object [obj] if it is tog-gleable.toggle_off [obj]Toggle off object [obj] if it is tog-gleable.slice [obj]Slice object [obj] if it is sliceable.stop [answer]Stop making actions and conclude thefinal answer in [answer].High-level actionsgoto [loc]Go to a location that is in current obser-vation.heat [obj, heater] Heat object with a microwave or a stoveburner.cool [obj, cooler] Cool object with a fridge.clean [obj, sink]</p>
<p>This is the task you and your frend Alice are trying to accomplish.At each step, you can choose to think your plan or issue an action from one of the following: <code>chat [message].</code>:You can use this action to send a message to your friend Alice.<code>go_explore [room].</code>:Use this action to go to a room and explore around.<code>go_check [container].</code>:Use this action to go to a unchecked container and check its inside.You must have at least one free hands to check.<code>go_grab [object]</code>: Use this action to go to an object and grab it.<code>go_put [container].</code>:Use this action to go to the container and place the object in the container.You need to make sure you have the object(s) you need for the task.<code>stop [answer].</code>:Issue this action when you believe the task is complete.If you believe the task is impossible to complete, provide the answer as "N/A" in the bracket.To be successful, it is very important to follow the following rules: 1.You should only issue an action or your thought that is valid given the current observation 2.You should only issue one action at a time.3. Generate the action in the correct format starting with "Act: ".For example, "Act: go_grab [phone_0].".
Obs: These are the objects you can see .You can see at most { max_view_steps} step(s) in front of you, {side_steps} step(s) on your left, {side_steps} step(s) on turn_left, turn_right to appraoch it until you are facing it; 6. Issue stop action when you think you have achieved the objective. Don't generate anything after stop. Here are some examples: {example} Respond YES if you can play this game. Example: Task: go to the red box. Obs: You can see a blue key in front of you; You can see a red box on your right. Manipulable object: A blue key. &gt;Thought: To solve this task, I need to find a red box. There is a blue key just before me and blocked my way. The red box is on my right, so I should turn_right first to go to the red box. &gt;Act: turn_right. Task: Task: {task} Your memory from last trails is: { memory} Obs: {observation} What is your next step? Try to think before act. G.5 Reflexion+EmMem Instruction: You are an autonomous agent tasked with completing a minigrid game. These tasks will be accomplished through the use of specific actions you can issue. Here's the information you'll have: Task: This is the task you are trying to accomplish. Obs: These are the objects you can see 5. You can only operate the object in your sight, if the object is in front of you, on your left or right, take action like move_ahead, turn_left, turn_right to appraoch it until you it in your sight; 6. Issue stop action when you think you have achieved the objective.\ n7. Remember you are facing NORTH at the begining. Don't generate anything after stop. Here are some examples: {example} Respond YES if you can play this game. Example: Task: go to the red box. Obs: You can see nothing ahead. &gt;Thought: I see nothing, it could be I have reached the border or I am in a middle grid but there is nothing in front of me. I have taken 0 steps and I am facing NORTH now. I should remember what I saw, so that I can find them easily when necessary; I need to find a red box. &gt;OK. &gt;Act: move_ahead &gt;Feedback: Action failed. Can not move ahead, because there is an Obs: You see nothing ahead. &gt;Thought: I can't move ahead because of an obstacle but I saw nothing in front of me. so I have reached the border, I have taken 0 step and am facing NORTH now since last action failed. Next I will try to look around. &gt;OK. &gt;Act: turn_right &gt;Feedback: Action succeeded. Turned right by '90' degrees. Obs: You see a red box. &gt;Thought: I have reached the border left, 3 step(s) on your right. Manipulable object: The object that you are facing and you can take an operate action like pick_up [ object_name], toggle [object_name ] on it. Inventory: These are the objects you are holding, and you can take an action like drop on it. Feedback: Whether the action is succeed or not and why is it. At each step, you can choose to think your plan or execute an action from one of the following: <code>move_ahead</code>: it means you move ahead by 1 step. <code>turn_left</code>: it means you turn left 90 degrees. <code>turn_right</code>: it means you turn right 90 degrees. <code>pick_up [object_name]</code>: Use this action to pick up a pickupable and visible object directly in front of you without obstacle. <code>drop [object_name]</code>: it means you places the object in hand at the place just in front of you. <code>toggle [object_name]</code>: it means you toggle a toggleable and visible object in front of you without obstacle. <code>stop [answer]</code>: Issue this action when you believe the task is complete. If you believe the task is impossible to complete, provide the answer as "N/A" in the bracket. To be successful, it is very important to follow the following rules: 1. You should only issue a action or your thought that is valid given the current observation 2. You should only issue ONE action at a time, and ONLY action is enough. System: Feedback: Action failed. Failed to pick up red key. You can only pickup the object one step in front of you without obstacle. Obs: You can see a red key in front of you; You can see a green door,a blue key,a green box,a yellow key on your left; You can see a purple ball,a purple door on your right. Assistant: Thought: The red key is still in front of me, but I may need to move closer to pick it up. System: OK. Assistant: Act: move_ahead. System: Feedback: Action succeeded. Moved forward by 1 step. Obs: You can see a red key in front of you; You can see a green door,a blue key,a green box your left; You can see a purple ball on your right. Assistant: Thought: The red key is now directly in front of me. I can pick it up now. System: OK. Assistant: Act: pick_up [red key]. System: Feedback: Action failed. Failed to pick up red key. You can only pickup the object one step in front of you without obstacle. Obs: You can see a red key in front of you; You can see a green door,a blue key,a green box on your left; You can see a purple ball on your right. Assistant: Thought: I must have been too far to pick up the red key. I should move one step ahead to reach the key. System: OK. Assistant: Act: move_ahead. System: Feedback: Action succeeded. Moved forward by 1 step. Obs: You can see a red key in front of you; You can see a green door,a blue key on your left; Manipulable object: a red key. <code>drop [object_name]</code>: it means you place the object in hand at the place just in front of you. <code>open [object_name]</code>: it means you open an openable and visible object within 8.0 step(s). <code>close [object_name]</code>: it means you close a closeable and visible object. <code>stop [answer]</code>: Issue this action when you believe the task is complete. If you believe the task is impossible to complete, provide the answer as 'N/A' in the bracket. To be successful, it is very important to follow the following rules: 1. You should only issue ONE action or ONE thought that is valid given the current observation 2. When you issue a action, and ONLY ONE action is enough, and then wait for your feedback from system. 3. Generate the action in the correct format starting with "Act: " For example, "Act: pick_up [cup_0]" Do not add adjectives before object name. 4. State your thought or think process starting with "Thought: " 5. ONLY operate the object within your view distance. 6. If got action failed feedback, try to take other actions to proceed the task. 7. Issue stop action when you think you have achieved the objective. Don't generate anything after stop. Here are some examples: Original status: In the north of the room, there is a armchair_0; a pen_1; In the east of the room, cabinet_8; a cabinet_10; a cabinet_15; a cabinet_18; a countertop_2, in/on it you can see a bowl_0, a butterknife_0, a faucet_0, a faucet_1, a knife_0, a pan_0, a soapbottle_0, a toaster_0; a drawer_3. Obs: In front of you, You see a cabinet_0; a cabinet_4; a cabinet_9; a cabinet_11; an opened fridge_0, there is a egg_0 in/on it. On your left, you see a cabinet_2; a cabinet_10; a cabinet_13; a cabinet_18; a countertop_2; a bowl_0; a faucet_1 ; a knife_0; a pan_0; a toaster_0; a diningtable_0; a apple_0; a bread_0; a butterknife_0; a dishsponge_0; a lettuce_0; a potato_0; a saltshaker_0; a soapbottle_0; a spoon_0; a tomato_0; a drawer_3; a microwave_0; a sink_0; a sink_1; a stool_0; a window_0. On your right, you see a cabinet_3; a cabinet_22; a faucet_0. what is your next step? Assistant: Thought: As there are many objects in the room, I need to identify which objects have changed. Since I can see an egg in the fridge but the is open, I will close the fridge to reset it to its original state. Assistant: Act: close fridge_0. System: Feedback: Action succeeded. You closed fridge_0. Obs: In front of you, You see a cabinet_0; a cabinet_4; a cabinet_9; a cabinet_11; a fridge_0. On your left, you see a cabinet_2; a cabinet_10; a cabinet_13; a cabinet_18; a countertop_2; a bowl_0; a faucet_1 ; a knife_0; a pan_0; a toaster_0; a diningtable_0; a apple_0; a bread_0; a butterknife_0; a Assistant: Act: move_ahead. System: Feedback: Action succeeded. Moved forward by 1 step. Obs: In front of you, You see a diningtable_0; a window_0. On your left, you see a apple_0; a bread_0; a lettuce_0; a spoon_0; a stoveburner_0; a pot_0; a stoveburner_1; a stoveburner_2; a stoveknob_0. On your right, you see a cabinet_17; a countertop_1; a cup_0; a plate_0; a butterknife_0; a potato_0; a saltshaker_0; a soapbottle_0; a tomato_0; a drawer_2; a stool_0. Assistant: Thought: The sink_0 is not in my view. I need to turn to my right to find it. Assistant: Act: turn_right. System: Feedback: Action succeeded. Turned right by '90' degrees. Obs: In front of you, You see a cabinet_0; a cabinet_9; a cabinet_11; a fridge_0. On your left, you see a cabinet_2; a cabinet_4; a cabinet_10; a cabinet_13; a countertop_2; a bowl_0; a faucet_1; a knife_0; a pan_0; a toaster_0; a diningtable_0; a apple_0; a bread_0; a lettuce_0; a potato_0; a saltshaker_0; a soapbottle_0; a spoon_0; a tomato_0; a drawer_3; a microwave_0; a sink_0; a sink_1; a stool_0; a window_0. On your right, you see a cabinet_3; a cabinet_5; a cabinet_12; a cabinet_21; a cabinet_22; a faucet_0. Assistant: Thought: The sink_0 is in front of me. I need to move ahead and put the dishsponge back to the sink_0. Assistant: Act: move_ahead. System: Feedback: Action succeeded. take action like move_ahead, turn_left or turn_right to explore the room. Assistant: Act: move_ahead System: Feedback: Action succeeded. Moved forward by '0.25' meter(s). Obs: You see nothing. You can try to turn_left or turn_right to explore the room. Assistant: Act: turn_right System: Feedback: Action succeeded. Turn right by '90' degrees. Obs: On your left, you see a lightswitch_0; On your right, you see a bottle_1, a creditcard_1, a mug_1, a creditcard_1, a stool_1, Assistant: Act: move_ahead System: Feedback: Action succeeded. Moved forward by '0.25' meter(s). Obs: On your left, you see a lightswitch_0; On your right, you see a bottle_1, a creditcard_1, a stool_1, Assistant: Act: turn_right System: Feedback: Action succeeded. Turn right by '90' Obs: In front of you, You see a stoveknob_0; a stoveknob_1; a stoveknob_2; a cabinet_5; a stoveknob_3; a houseplant_1; a spoon_1; On your left, you see a bowl_5; a drawer_1; a drawer_2; a cabinet_0; a cabinet_1; a window_0 ; a sink_0; a drawer_6; a cabinet_6; a drawer_8; a toaster_1 ; a winebottle_1; a vase_3; a coffeemachine_1; a bowl_5; a lettuce_1; a stool_1; a faucet_0; On your right, you see a bowl_3, a bottle_1, a bottle_1, a knife_1, a soapbottle_2, a vase_1, a bowl_3, a creditcard_1, a pot_1, a papertowelroll_1, a fridge_0, a lightswitch_0, a mug_1, a creditcard_1, H.6 Multi-agent Cooperation System: You are an autonomous intelligent agent tasked with navigating a virtual home. Your name is Alice. You are hurry to finish a housework with your friend Bob together. There are kitchen, bedroom, bathroom. You will be given a household task. These tasks will be accomplished through the use of specific actions you and your friend can issue. You have two hands to pickup, hold, put and check objects. Here's the information you will have: Task: This is the task you and your frend Bob are trying to accomplish. At each step, you can choose to think your plan or issue an action from one of the following: <code>chat [message].</code>: You can use this action to send a message to your friend Bob. <code>go_explore [room].</code>: Use this action to go to a room and explore around. <code>go_check [container].</code>: Use this action to go to a unchecked container and check its inside. You must have at least one free hands to check. <code>go_grab [object]</code>: Use this action to go to an object and grab it. <code>go_put [container].</code>: Use this action to go to the container and place the object in the container. You need to make sure you have the object(s) you need for the task. <code>stop [answer].</code>: Issue this action when you believe the task is complete. If you believe the task is impossible to complete, provide Task: 4. State your thought or think process on it. Inventory: These are the objects you are holding, and you can take an action like drop on it. Feedback: Whether the action is succeed or not and why is it. At each step, you can choose to think your plan or execute an action from one of the following: <code>move_ahead</code>: it means you move ahead by 1 step. <code>turn_left</code>: it means you turn left 90 degrees. <code>turn_right</code>: it means you turn right 90 degrees. <code>pick_up [object_name]</code>: Use this action to pick up a pickupable and visible object directly in front of you without obstacle. <code>drop [object_name]</code>: it means you places the object in hand at the place just in front of you. <code>toggle [object_name]</code>: it means you toggle a toggleable and visible object directly in front of you without obstacle. <code>stop [answer]</code>: Issue this action when you believe the task is complete. If you believe the task is impossible to complete, provide the answer as \"N/A\" in the bracket. To be successful, it is very important to follow the following rules: 1. You should only issue a action or your thought that is valid given the current observation 2. You should only issue ONE action at a time, and ONLY action is enough. 3. Generate the action in the correct format starting with \"Act: \". For example, \"Act: pick_up [cup_0 ]\". 4. State your thought or think process Manipulable object: The object that you are facing and you can take an operate action like pick_up [ object_name], toggle [object_name] on it. Inventory: These are the objects you are holding, and you can take an action like drop on it. &gt;Feedback: Whether the action is succeed or not and why is it. At each step, you can choose to think your plan or execute an action from one of the following: <code>move_ahead</code>: it means you move ahead by 1 step. <code>turn_left</code>: it means you turn left 90 degrees. <code>turn_right</code>: it means you turn right 90 degrees. <code>pick_up [object_name]</code>: Use this action to pick up a pickupable and visible object directly in front of you without obstacle. <code>drop [object_name]</code>: it means you places the object in hand at the place just in front of you. <code>toggle [object_name]</code>: it means you toggle a toggleable and visible object directly in front of you without obstacle. <code>stop [answer]</code>: Issue this action when you believe the task is complete. If you believe the task is impossible to complete, provide the answer as \"N/A\" in the bracket. To be successful, it is very important to follow the following rules: 1. You should only issue a action or your thought that is valid given the current observation 2. You should only issue ONE action at a time, and ONLY action is enough. 3. Generate the action in the correct go to it. &gt;OK. &gt;Act: move_ahead &gt;Feedback: Action succeeded. Moved forward by 1 step. Obs: You can see a red box. &gt;Thought: I am moving along the border now, I have taken 3 steps and am still facing EAST after move ahead at last step. I need to go to the red box. &gt;OK. &gt;Act: move_ahead() &gt;Feedback: Action failed. Can not move ahead, because there is an obstacle ahead. Obs: You can see a red box. &gt;Thought: I see a red box and can not move ahead, I have taken 3 steps and am still facing EAST since last action failed. I have reached the red box. &gt;OK. &gt;Act: stop[]. Task: Task: {task} Your memory from last trails is: { memory} Obs: {observation} What is your next step? Try to summarize your status, recall what you have done and think before act. H Examples of different task prompts H.1 Instruction Grounding System: You are an autonomous intelligent agent tasked with completing a minigrid game. These tasks will be accomplished through the use of specific actions you can issue. Here's the information you'll have: 5. You can only operate the object you are facing, if the object is in front of you, on your left or right, take action like move_ahead, turn_left, turn_right to appraoch it until you are facing it; 6. Issue stop action when you think you have achieved the objective. Don't generate anything after stop. Here are some examples: Task: go to the red box. Obs: You can see a blue key in front of you; You can see a red box on your right. Manipulable object: A blue key. &gt;Thought: To solve this task, I need to find a red box. There is a blue key just before me and blocked my way. The red box is on my right, so I should turn_right first to go to the red box. &gt;Act: turn_right. Respond YES if you can play this game. Assistant: YES System: Task: pick up the red key Obs: You can see a yellow door, a yellow key in front of you; You can see a green door on your left; You can see a green box,a red key on your right. What is your next step? Assistant: Thought: To accomplish the task, I need to pick up the red key. The red key is on my right, so the first step is to turn right. System: OK. Assistant: Act: turn_right. System: Feedback: Action succeeded. Turned right by '90' degrees. Obs: You can see a red key in front of you; You can see a green door,a blue key,a green box,a yellow key on your left; You can see a purple ball,a purple door on your right. System: Feedback: Action succeeded. You picked a red key up. Inventory: You are now holding a red key. Obs: You can see a green door,a blue key on your left. System: [SUCCESS] You have completed the task. Congratulations! System: DONE! H.2 Rearrangement System: You are an autonomous intelligent agent tasked with rearranging a vitual home. System will randomly change between 1 to 5 objects in the room. Your goal is to identify which objects have changed and reset those objects to their original state. Here are the information you will have : Original status: The original room status that you should remember, pay your attention to the position and openness states of each of the objects. Then find out all the changed objects and reset them to their origianl states. Obs: These are the objects you can see . You can see at most 8.0 step(s) in front of you; 60 degrees on your left, 60 degrees on your right. Inventory: These are the objects you are holding. You can hold at most 1 objects. Feedback: Whether the action is succeed or not and why is it. At each step, you can choose to think your plan or execute an action from one of the following: <code>move_ahead</code>: it means you move ahead by 1 step. <code>turn_left</code>: it means you turn left 90 degrees. see a pen_1, because I am in the south of the room, so the pen_1 is in the south of the room, but I remember the pen_1 should be in the north of the room according to the origianl status. So I need to pick it up and put it back. OK. &gt; Act: move_ahead Respond YES if you can play this game. Rearrange Agent: YES System: Let's start. Original state: In the north of the room, there is a cabinet_2; a cabinet_3; a cabinet_13; a cabinet_16; a cabinet_19; a cabinet_22; a microwave_0; a sink_0; a sink_1, in/on it you can see a dishsponge_0. In the northeast of the room, there is a lightswitch_0. In the east of the room, there is a cabinet_7; a cabinet_20; a countertop_0, in/on it you can see a coffeemachine_0, a fork_0, a papertowelroll_0, a peppershaker_0, a saltshaker_0, a spatula_0, a spoon_0; a drawer_0; a drawer_1; a garbagecan_0; a stoveknob_2. In the southeast of the room, there is a cabinet_17; a countertop_1, in/on it you can see a cup_0, a plate_0; a drawer_2 ; a stoveburner_0; a stoveburner_1 ; a stoveburner_2, in/on it you can see a pot_0; a stoveburner_3; a stoveknob_0; a stoveknob_1; a stoveknob_3. In the south of the room, there is a diningtable_0, in /on it you can see a apple_0, a bread_0, a lettuce_0, a mug_0, a potato_0, a tomato_0; a stool_0; a window_0. In the southwest of the room, there is a cabinet_5; a cabinet_12; a cabinet_14; a cabinet_21. In the west of the room, there is a cabinet_0; a stool_0; a window_0. On your right, you see a cabinet_3; a cabinet_22; a faucet_0. Assistant: Thought: Now that the fridge is closed, I can see the original status of the room. I need to check if there are any changes in the room. Assistant: Act: turn_left. System: Feedback: Action succeeded. Turned left by '90' degrees. Obs: In front of you, You see a diningtable_0; a apple_0; a dishsponge_0; a spoon_0; a window_0. On your left, you see a bread_0; a butterknife_0; a lettuce_0; a potato_0; a saltshaker_0; a soapbottle_0; a tomato_0; a stool_0; a stoveburner_0; a pot_0; a stoveburner_3. On your right, you see a cabinet_17; a countertop_1; a cup_0; a plate_0; a drawer_2; a fridge_0; a stoveburner_1; a stoveburner_2; a stoveknob_0; a stoveknob_3. Assistant: Thought: I can see a dishsponge in the dining table which was not there before. So, I need to pick it up and put it back to its original position. Assistant: Act: pick_up dishsponge_0. System: Feedback: Action succeeded. You picked dishsponge_0 up. Inventory: dishsponge_0. Obs: In front of you, You see a diningtable_0; a apple_0; a spoon_0; a window_0. On your left, you see a bread_0; a butterknife_0; a lettuce_0; a potato_0; a saltshaker_0; a soapbottle_0; a tomato_0; a stool_0; a stoveburner_0; a pot_0; a stoveburner_3. On your right, you see a cabinet_17; a countertop_1; a cup_0; a plate_0; a drawer_2; a fridge_0; a H.4 Household System: You are an autonomous intelligent agent tasked with navigating a vitual home. You will be given a household task. These tasks will be accomplished through the use of specific actions you Here's the information you'll have: Task: This is the task you are trying to accomplish. Obs: These are the objects you can see . Inventory: These are the objects you are holding. Feedback: Whether the action is succeed or not and why is it. At each step, you can choose to think your plan or execute an action from one of the following: <code>move_ahead</code>: it means you move ahead by 1 step. <code>turn_left</code>: it means you turn left 90 degrees. <code>turn_right</code>: it means you turn right 90 degrees. <code>pick_up [object_name]</code>: Use this action to pick_up a pick_upable and visible object within your manipulation distance. <code>drop [object_name]</code>: it means you places the object in hand at the place just in front of it. <code>open [object_name]</code>: it means you open an openable and visible object within your manipulation distance. <code>close [object_name]</code>: it means you close a closeable and visible object. <code>toggle_on [object_name]</code>: it means you toggle a toggleable and visible object on. <code>toggle_off [object_name]</code>: it means 1. You should only issue an action or your thought that is valid given the current observation 2. You should only issue one action at a time. 3. Generate the action in the correct format starting with "Act: ". For example, "Act: go_grab [phone_0].". 4. State your thought or think process starting with "Thought: " 5. Issue stop action when you think you have achieved the objective. Don't generate anything after stop. 6. You should generate chats like this : chat [Hello, how can I help you ?], taking care to enclose them in square brackets. Respond YES. if you can play this game . System: You are an autonomous intelligent agent tasked with navigating a virtual home. Your name is Bob. You are hurry to finish a housework with your friend Alice together. There are four kinds of rooms, livingroom, kitchen, bedroom, bathroom. You will be given a household task. These tasks will be accomplished through the use of specific actions you and your friend can issue. You have two hands to pickup, hold, put and check objects. Here's the information you will have: starting with "Thought: " 5. Issue stop action when you think you have achieved the objective. Don't generate anything after stop. 6. You should generate chats like this : chat [Hello, how can I help you ?], taking care to enclose them in square brackets. Respond YES. if you can play this game . Alice: YES. System: Task: Find and put 1 wine onto the coffeetable_0. Obs: You have already found and put kitchencabinet_1, kitchencabinet_2, kitchencabinet_3, kitchencabinet_4, kitchencabinet_5, kitchencabinet_6, kitchencabinet_7, stove_0, dishwasher_0, fridge_0, fridge_1, microwave_0. You don't know where Bob is. The livingroom is unexplored. The bathroom is unexplored. What is your next step? Let's think and act. Bob: YES. System: Task: Find and put 1 wine onto the coffeetable_0. Obs: You have already found and put pudding_0, juice_0, juice_1 onto the coffeetable_0. You are holding nothing. You are in the bedroom, where you found an unchecked container cabinet_0. You don't know where Alice is. The livingroom is unexplored. The kitchen is unexplored. The bathroom is unexplored. What is your next step? Let's think and act. Alice: Thought: The task is to find and put 1 wine onto the coffeetable_0. I am currently in the kitchen with several unchecked containers. I will start by checking these containers as the wine could be in any of them. Alice: Act: go_check [kitchencabinet_0 ]. System: Feedback: Action succeeded. You opened kitchencabinet_0. Obs: In it you see nothing Bob: Thought: Since I need to find the wine and I am currently in an unexplored bedroom, I'll first check the unchecked container in the bedroom, the cabinet_0. If the wine is not there, I'll move to the next unexplored room. Alice: Thought: The first kitchen cabinet was empty. I'll check the next one. Alice: Act: go_check [kitchencabinet_1 ] System: Feedback: Action succeeded. You opened kitchencabinet_1. Obs: In it you see nothing cabinet_0 in the bedroom, so I need to explore other rooms. The kitchen is a good place to start as it is common to store wine there. Bob: Act: go_explore [kitchen] System: Feedback: Action succeeded. Go to kitchen_0. You have already found and put pudding_0, juice_0, juice_1 onto the coffeetable_0. You are holding nothing. You are in the kitchen, where you found unchecked containers kitchencabinet_2, kitchencabinet_3, kitchencabinet_4, kitchencabinet_5, kitchencabinet_6, kitchencabinet_7, stove_0, dishwasher_0, fridge_0, fridge_1, microwave_0. You also see Alice here in the kitchen, she is holding nothing. The livingroom unexplored. You found nothing in the bedroom. The bathroom is unexplored. Alice: Act: go_check [kitchencabinet_2 ] System: Feedback: Action succeeded. You opened kitchencabinet_2. Obs: In it you see book_1 Bob: Act: go_check [fridge_0] System: Feedback: Action succeeded. You opened fridge_0. Obs: In it you see wine_0 Alice: Act: go_check [kitchencabinet_3 ] System: Feedback: Action succeeded. You opened kitchencabinet_3. Obs: In it you see nothing the coffeetable_0. You are holding wine_0. You are in the kitchen, where you found unchecked containers kitchencabinet_4, kitchencabinet_5, kitchencabinet_6, kitchencabinet_7, stove_0, dishwasher_0, fridge_1, microwave_0. You also see Alice holding nothing. The livingroom is unexplored. You found nothing in the bedroom. The bathroom is unexplored. Alice: Act: go_check [kitchencabinet_4 ] System: Feedback: Action succeeded. You opened kitchencabinet_4. Obs: In it you see nothing Bob: Act: go_explore [livingroom] System: Feedback: Action succeeded. Go to livingroom_0. You have already found and put pudding_0, juice_0, juice_1 onto the coffeetable_0. You are holding wine_0. You are in the livingroom, where you found coffeetable_0. Last time you saw Alice was in the kitchen, she was holding nothing. You found unchecked containers kitchencabinet_5, kitchencabinet_6, kitchencabinet_7, stove_0, dishwasher_0, fridge_1, microwave_0 in the kitchen. You found nothing in the bedroom. The bathroom is unexplored. Alice: Act: go_check [kitchencabinet_5 ] System: Feedback: Action succeeded. You opened kitchencabinet_5. Obs: In it you see book_2 Bob: Act: go_put [coffeetable_0] System: Feedback: Action succeeded. You put wine_0 on coffeetable_0. Obs: You have already found and put pudding_0, juice_0, juice_1, wine_0 to the coffeetable_0. You are holding nothing. You are in kitchencabinet_5, kitchencabinet_6, kitchencabinet_7, stove_0, dishwasher_0, fridge_1, microwave_0 in the kitchen. You found nothing in the bedroom. The bathroom is unexplored. System: [SUCCESS] You have completed the task. Congratulations! Moved forward by 1 step. Obs: You see nothing. You can try to take action like move_ahead, can issue. four kinds of rooms, livingroom, unexplored. The bedroom is Bob: Thought: The wine is not in the here in the kitchen, she is System: DONE!your right. .You can see at most { when facing NORTH. I haven taken 2 3. Generate the action in the correct Assistant: Thought: I am now facing there is a chair_0;. dishsponge_0; a lettuce_0; a Assistant: Thought: Found it! the answer as "N/A" in the pudding_0, juice_0, juice_1 onto Bob: Act: go_grab [wine_0]starting with \"Thought: \" format starting with \"Act: \". Task: This is the task you are trying Assistant: Thought: Now that I'm <code>turn_right</code>: it means you turn right cabinet_4; a cabinet_9; a stoveburner_1; a stoveburner_2; a you toggle a toggleable and Bob: Act: go_check [cabinet_0]. the livingroom, where you foundManipulable object: The object that max_view_steps} step(s) in front steps and am facing EAST after format starting with "Act: ". For the red key and can pick it up. Obs: You can see nothing ahead. potato_0; a saltshaker_0; a Assistant: Act: answer [True] bracket. the coffeetable_0. You are holding System: Feedback: Action succeeded.5. You can only operate the object you For example, \"Act: pick_up [cup_0 to accomplish. facing the red key, I can pick it 90 degrees. cabinet_11; a fridge_0, in/on it stoveknob_0; a stoveknob_3. visible object off. System: Feedback: Action succeeded. coffeetable_0. Last time you sawyou are facing and you can take an of you, {side_steps} step(s) on turn right at last step. The example, "Act: pick_up [cup_0]". System: OK. &gt; Thought: To solve the task, I need soapbottle_0; a spoon_0; a System: Your are right! nothing. You are in the kitchen, You picked wine_0 up.are facing, if the object is in ]\". Obs: These are the objects you can see up to accomplish my task. <code>pick_up [object_name]</code>: Use this you can see a egg_0. In the Assistant: Thought: I need to put the <code>slice [object_name]</code>: it means you You opened cabinet_0. Alice was in the kitchen, she wasoperate action like pick_up [ your left, {side_steps} step(s) on border is on my leftside. Now I 4. State your thought or think process Assistant: Act: pick_up [red key]. to explore the room to find out tomato_0; a drawer_3; a System: DONE! To be successful, it is very important where you found unchecked Obs: You have already found and putfront of you, on your left or 4. State your thought or think process .You can see at most 7 step(s) in System: OK. action to pick up a pickupable and northwest of the room, there is a dishsponge back to the sink_0 slice a sliceable and visible Obs: In it you see book_3 holding nothing. You foundobject_name], toggle [object_name] your right. find the red box. Next I need to starting with "Thought: " what objects have been changed. I microwave_0; a sink_0; a sink_1; a to follow the following rules: containers kitchencabinet_0, pudding_0, juice_0, juice_1 ontoright, take action like move_ahead, starting with \"Thought: \" front of you, 3 step(s) on your Assistant: Act: pick_up [red key]. visible object within 8.0 step(s). cabinet_1; a cabinet_6; a where it was originally placed. object. unchecked containers
We did not fully test some open-source LLMs with small sizes, such as LLaMA, because we made a preliminary attempt to utilize LLaMA
-7B to solve Instruction Grounding and IQA tasks but found it failed in all tasks
Obs: In front of you, You see a cabinet_0; a cabinet_11; a fridge_0. On your left, you see a cabinet_1; a cabinet_2; a cabinet_4; a cabinet_6; a cabinet_9; a cabinet_10; a cabinet_18; a countertop_2; a bowl_0; a faucet_1; a knife_0; a pan_0; a toaster_0; a diningtable_0; a apple_0; a bread_0; a butterknife_0; a lettuce_0; a potato_0; a saltshaker_0; a soapbottle_0; a spoon_0; a tomato_0; a drawer_3; a stool_0; a window_0. On your right, you see a cabinet_3; a cabinet_5; a cabinet_12; a cabinet_13; a cabinet_14; a cabinet_21; a faucet_0; a microwave_0; a sink_0; a sink_1. Assistant: Thought: I am in front of the sink_0. I need to drop the dishsponge_0 to put it back to its original position. Assistant: Act: drop dishsponge_0 in sink_0. System: Feedback: Action succeeded. You dropped dishsponge_0. Inventory: Empty Obs: In front of you, You see a cabinet_0; a cabinet_11; a fridge_0; a dishsponge_0. On your left, you see a cabinet_1; a cabinet_2; a cabinet_4; a cabinet_6; a cabinet_9; a cabinet_10; a cabinet_18; a countertop_2; a bowl_0; a faucet_1 ; a knife_0; a pan_0; a toaster_0; a diningtable_0; a apple_0; a bread_0; a butterknife_0; a lettuce_0; a potato_0; a saltshaker_0; a soapbottle_0; a spoon_0; a tomato_0; a drawer_3; a stool_0; a window_0. On your right, you see a cabinet_3; a cabinet_5; a cabinet_12; a cabinet_13; a cabinet_14; a cabinet_21; a faucet_0; a microwave_0; a sink_0; a sink_1. Assistant: Thought: I have successfully reset the changed objects to their original states. Now, there is no change in the room. I can stop here. Assistant: Act: stop [] System: You stopped the game.
AcknowledgementsThe authors thank Dr. Wentao Ding for his great contributions in refining code repository and Mrs. Chen Zhen for her design of illustrative teasers.This work presented herein is supported by the National Natural Science Foundation of China (62376031).Zettlemoyer, and Dieter Fox. 2020b. Alfred: A benchmark for interpreting grounded instructions for everyday tasks.In Conference on Computer Vision and Pattern Recognition (CVPR), pages 10740-10749.System: {'misplaced': 0.0, 'fixed':1.0} System:[SUCCESS]
Task: Move the keys to the computer on the sofa. Obs: In front of you, You see a faucet_0; a sink_0; a sink_1. On your left, you see a cabinet_0</p>
<p>On your right, you see a countertop_0. a coffeemachine_0; a toaster_0; a cabinet_2; a fridge_0; a glassbottle_1. a butterknife_0; a bread_0; a knife_0; a apple_0; a saltshaker_0; a potato_0; a butterknife_1; a cabinet_4; a cabinet_6; a cabinet_7; a drawer_1; a cabinet_9; a spoon_1</p>
<p>Act: turn_right. Then Act: move_ahead. Feedback: Action succeeded. Obs: In front of you. You see ……</p>
<p>Feedback: Action succeeded. Obs: In front of you. Then Act: move_ahead. Then Act: put. Act: turn_left. You see a [11] Act: stop [keys are already put on the sofa</p>
<p>You stopped the game. failed to complete the task</p>
<p>Thought: I am facing north at the beginning. </p>
<p>Act: put [keychain_0, laptop_0] Feedback: Action succeeded. You put keychain_0 into/onto laptop_0. Obs: In front of you. </p>
<p>Thought: I have successfully placed the keys from the keychain_0 onto the laptop_0 on the sofa_0. The task is complete. </p>
<p>Act: put [keychain_0, computer_0] Feedback: Action failed. The object is not provided or not in sight. You can only manipulate the object in sight. </p>
<p>Thought: I need to be closer to the computer to put the keys on it. </p>
<p>Act: move_ahead Feedback: Action succeeded. Obs: You see nothing ……. </p>
<p>Act: turn_left. turn_left. Feedback: Action succeeded. Turn left by '90' degrees. </p>
<p>Thought: I have successfully moved forward and now I am next to the opened laptop. I can now put the keys on the laptop</p>
<p>Act: put [keychain_0, laptop_0] Feedback: Action failed. Object keychain_0 is not in your inventory. </p>
<p>Thought: Oops, I forgot to pick up the keys before trying to put them on the laptop. I need to pick up the keys first. </p>
<p>Act: pick_up [keychain_0] Feedback: Action succeeded. 0</p>
<p>Act: stop. The keys have been moved to the computer on the sofa.] You stopped the game [SUCCESS] You have completed the task</p>
<p>References Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Video pretraining (vpt): Learning to act by watching unlabeled online videos. Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune, Advances in Neural Information Processing Systems (NeurIPS). 202235</p>
<p>Vector-based navigation using grid-like representations in artificial agents. Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, Nature. 55777052018</p>
<p>Robustnav: Towards benchmarking robustness in embodied navigation. Prithvijit Chattopadhyay, Judy Hoffman, Proceedings of the IEEE/CVF Interna. the IEEE/CVF Interna2021Roozbeh Mottaghi, and Aniruddha Kembhavi</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8425 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8425</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8425</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-aae6991b9ebdc53523931a0115c8b51bd5e4cc94</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/aae6991b9ebdc53523931a0115c8b51bd5e4cc94" target="_blank">HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes to use a knowledge graph (KG) that is context-aware and is distilled to contain query-relevant information that results in the method utilizing up to $67\%$ fewer tokens to represent the query relevant information present in the supporting documents, compared to the state-of theart (SoTA) method.</p>
                <p><strong>Paper Abstract:</strong> Given unstructured text, Large Language Models (LLMs) are adept at answering simple (single-hop) questions. However, as the complexity of the questions increase, the performance of LLMs degrade. We believe this is due to the overhead associated with understanding the complex question followed by filtering and aggregating unstructured information in the raw text. Recent methods try to reduce this burden by integrating structured knowledge triples into the raw text, aiming to provide a structured overview that simplifies information processing. However, this simplistic approach is query-agnostic and the extracted facts are ambiguous as they lack context. To address these drawbacks and to enable LLMs to answer complex (multi-hop) questions with ease, we propose to use a knowledge graph (KG) that is context-aware and is distilled to contain query-relevant information. The use of our compressed distilled KG as input to the LLM results in our method utilizing up to $67\%$ fewer tokens to represent the query relevant information present in the supporting documents, compared to the state-of-the-art (SoTA) method. Our experiments show consistent improvements over the SoTA across several metrics (EM, F1, BERTScore, and Human Eval) on two popular benchmark datasets (HotpotQA and MuSiQue).</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8425.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8425.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HOLMES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that constructs a query-focused hyper-relational knowledge graph (KG) from supporting documents and uses the distilled KG (no raw text) as an external structured memory input to an LLM reader; pruning via a query-aligned schema and complementary fact retrieval reduce tokens and improve multi-hop QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HOLMES</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A training-free, retrieval-augmented LLM agent pipeline: (1) extract named entities and build an entity-document bipartite graph, (2) level-order (BFS) traversal starting from query entities and LLM-based triple extraction to create hyper-relational quadruples (subject, relation, object, document-title), (3) induce a query-aligned graph schema (from the query + an auxiliary schema) and prune hyper-triples by embedding-based cosine similarity, (4) verbalize pruned hyper-triples into ranked facts and feed to a reader LLM; includes a complementary fact retrieval verification step.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-3.5 / Gemini-Pro (reader); gpt-3.5-turbo-instruct or gpt-4-1106-preview (triple extractor)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art large language models used as reader LLMs (GPT-4, GPT-3.5, Gemini-Pro); triple extraction performed by smaller instruction/tuned LLMs (gpt-3.5-turbo-instruct or gpt-4-1106-preview) to produce KG triples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA; MuSiQue (multi-hop QA benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop question answering: given a multi-hop question and a set of supporting documents (distractor setting), retrieve and aggregate information across documents to produce the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-hop question answering / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external structured retrieval-augmented memory (query-focused hyper-relational knowledge graph + auxiliary graph schema + entity-document graph)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Constructed by BFS traversal of an entity-document graph with LLM-based triple extraction; each triple augmented with source document title to form hyper-triples; pruning uses embedding-based (text-embedding-ada-002) cosine similarity against a query-aligned schema; complementary retrieval fetches additional documents when reader LLM requests missing entity facts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Hyper-relational tuples (subject, relation, object, document-title) — verbalized into sentences — plus an auxiliary graph schema (relations and entity types) and the entity-document bipartite graph used for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Traversal-based (level-order / BFS) seeded by query entities to select candidate documents and entities; semantic pruning via cosine similarity between relation embeddings and schema relation embeddings; complementary retrieval via LLM-instructed listing of missing named entities and fetching their documents.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>HotpotQA (reader GPT-4): EM=0.66, F1=0.78; MuSiQue (reader GPT-4): EM=0.48, F1=0.58. Reader input token counts (average): HotpotQA 1230.90 tokens, MuSiQue 1398.15 tokens. Additional: With gpt-4-1106-preview triple extractor on a 100-sample HotpotQA eval: EM=0.68, F1=0.79 (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Base (reader GPT-4) with supporting documents (no distilled hyper-relational KG): HotpotQA EM=0.54, F1=0.74; MuSiQue EM=0.39, F1=0.55. Ablation in Table 5 (component removals): configuration with only Hyper-KG (no pruning/aux/schema/comp-retrieval) EM=0.56 (reader tokens 1867).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablations in Table 5: starting from only Hyper-relational KG (EM=0.56, tokens=1867), adding pruning reduces tokens (EM remains 0.56, tokens=1227), adding auxiliary schema increases EM to 0.58 (tokens ~1220), and adding complementary fact retrieval raises EM to 0.61 (tokens ~1230). Sensitivity studies: pruning down to p=50 hyper-triples suffices (performance plateaus after 50); traversal depth beyond dataset complexity (≥3 hops) yields no gain. Triple-extractor quality impacts results (Table 6): better extractor (gpt-4-1106-preview) improves EM vs gpt-3.5 extractor.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a query-focused hyper-relational KG as an external memory for LLM readers: (1) improves EM/F1 across HotpotQA and MuSiQue versus baselines (StructQA and raw-doc reader), (2) reduces reader input tokens substantially (up to ~67% reduction vs StructQA, and up to ~60% vs raw supporting docs), (3) pruning by a query-aligned schema and complementary fact retrieval materially improves accuracy and efficiency, and (4) allows the system to abstain (output None) in some cases improving self-aware EM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Constructed graphs can be incomplete because automated triple extraction may miss facts; auxiliary schema creation adds compute overhead (one-time) and relies on in-domain question sampling; method depends on the quality of LLM triple extraction and embedding similarity; complementary retrieval is single-step verification which may be insufficient for highly incomplete graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8425.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8425.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StructQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StructQA (Structuring extracted triples + CoT for multi-hop QA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior training-free approach that extracts KG triples from text and supplies both the extracted triples and raw text to an LLM (often with chain-of-thought prompting) to perform multi-hop QA; triples are query-agnostic and lack contextual qualifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging structured information for explainable multi-hop question answering and reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>StructQA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A prompting-based pipeline that extracts structured KG triples (subject, relation, object) from supporting documents (via LLMs), provides those triples along with raw passages to a reader LLM, and employs chain-of-thought prompting for reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-3.5 (reader in baseline experiments reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reader LLMs used as in this paper; StructQA's triple-extraction also uses LLM-based extractors (same extractor used for fair comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA; MuSiQue (multi-hop QA baselines in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-hop QA tasks: answer multi-hop questions using extracted triples plus raw text.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-hop question answering / retrieval-augmented reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external structured memory (KG triples) but query-agnostic and not hyper-relational (no contextual document qualifiers attached to triples)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>LLM-based triple extraction from documents to form a (flat) KG of triples; both triples and raw supporting passages are concatenated into the reader prompt (no explicit pruning by a query-aligned schema described).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Triples of the form (subject, relation, object) extracted from supporting documents; raw passages are also included as part of input (redundancy).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Triples are extracted from each document and provided together with documents to the reader; no explicit BFS traversal seed/pruning process like HOLMES (i.e., more query-agnostic extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in this paper (Table 1): HotpotQA (reader GPT-4) EM=0.55, F1=0.77; MuSiQue (reader GPT-4) EM=0.42, F1=0.56. Reader input token counts (Table 3): HotpotQA ~3078.85 tokens; MuSiQue ~5908.87 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Base (reader GPT-4) with supporting documents (no extra KG triples) EM=0.54, F1=0.74 on HotpotQA (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared directly against HOLMES (Tables 1,3,6): StructQA uses more input tokens and attains lower EM/F1 than HOLMES; the paper highlights that StructQA's triples lack context (no document-title qualifiers) and that duplicating triples + raw text increases prompt length and redundancy. Sensitivity to triple-extractor quality benefits both StructQA and HOLMES but HOLMES retains a larger margin (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Query-agnostic triple extraction (StructQA) provides some benefit over raw reader-only baselines but is outperformed by HOLMES, which adds context via hyper-relational triples and prunes to query-relevant facts; StructQA's inclusion of raw text plus triples increases prompt size and redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Triples are ambiguous without contextual qualifiers (e.g., entity sense or document context); including both triples and raw text increases input token cost; lacks query-aligned pruning, leading to less efficient prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8425.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8425.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Base (reader w/ supporting docs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reader LLM baseline using raw supporting documents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline where the reader LLM is given the multi-hop question plus all supporting documents (distractor setting), testing the model's multi-hop reasoning over unstructured text without intermediate structured memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reader LLM with supporting documents (Base)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simple baseline: provide the reader LLM (GPT-4 / GPT-3.5 / Gemini-Pro) with the question and the raw supporting documents (titles + passages) and ask for an answer; tests LLM's ability to directly retrieve and chain reasoning from unstructured context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-3.5 / Gemini-Pro (as reported in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large LMs used as readers; no additional structured external memory other than the raw documents passed in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA; MuSiQue (multi-hop QA baselines in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop QA using only the unstructured supporting documents provided in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-hop question answering / document comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external unstructured memory (supporting documents passed in prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>No explicit retrieval or pruning beyond providing all supporting documents in the prompt; relies on LLM internal attention over the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw document text (title + passage) concatenated into the reader prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt-attention / recency and ordering in the prompt; no explicit semantic retrieval beyond what the prompt provides.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>HotpotQA (reader GPT-4): EM=0.54, F1=0.74; MuSiQue (reader GPT-4): EM=0.39, F1=0.55 (Table 1). Reader input token count (Table 3) HotpotQA ~1333.81 tokens, MuSiQue ~2361.36 tokens (average).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Base (w/o supporting docs — testing parametric knowledge): HotpotQA (GPT-4) EM=0.26, F1=0.45; MuSiQue EM=0.09, F1=0.21 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to structured-memory methods: HOLMES outperforms Base with supporting docs by substantial EM gains (e.g., +0.12 EM on HotpotQA with GPT-4); StructQA gives marginal improvement over Base but at higher token cost. Demonstrates that naive inclusion of all supporting docs is less efficient than a distilled structured memory.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing raw supporting documents helps over no-document baselines but is less effective and less input-efficient than supplying a distilled, query-focused structured memory (HOLMES).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Passing all supporting documents increases prompt size and contains distractors; the LLM must filter and connect relevant facts itself, which leads to lower performance on multi-hop queries and higher token / computation costs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Leveraging structured information for explainable multi-hop question answering and reasoning <em>(Rating: 2)</em></li>
                <li>Leveraging passage retrieval with generative models for open domain question answering <em>(Rating: 2)</em></li>
                <li>PullNet: Open domain question answering with iterative retrieval on knowledge bases and text <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8425",
    "paper_id": "paper-aae6991b9ebdc53523931a0115c8b51bd5e4cc94",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "HOLMES",
            "name_full": "HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs",
            "brief_description": "A pipeline that constructs a query-focused hyper-relational knowledge graph (KG) from supporting documents and uses the distilled KG (no raw text) as an external structured memory input to an LLM reader; pruning via a query-aligned schema and complementary fact retrieval reduce tokens and improve multi-hop QA performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "HOLMES",
            "agent_description": "A training-free, retrieval-augmented LLM agent pipeline: (1) extract named entities and build an entity-document bipartite graph, (2) level-order (BFS) traversal starting from query entities and LLM-based triple extraction to create hyper-relational quadruples (subject, relation, object, document-title), (3) induce a query-aligned graph schema (from the query + an auxiliary schema) and prune hyper-triples by embedding-based cosine similarity, (4) verbalize pruned hyper-triples into ranked facts and feed to a reader LLM; includes a complementary fact retrieval verification step.",
            "model_name": "GPT-4 / GPT-3.5 / Gemini-Pro (reader); gpt-3.5-turbo-instruct or gpt-4-1106-preview (triple extractor)",
            "model_description": "State-of-the-art large language models used as reader LLMs (GPT-4, GPT-3.5, Gemini-Pro); triple extraction performed by smaller instruction/tuned LLMs (gpt-3.5-turbo-instruct or gpt-4-1106-preview) to produce KG triples.",
            "task_name": "HotpotQA; MuSiQue (multi-hop QA benchmarks)",
            "task_description": "Multi-hop question answering: given a multi-hop question and a set of supporting documents (distractor setting), retrieve and aggregate information across documents to produce the answer.",
            "task_type": "multi-hop question answering / multi-step reasoning",
            "memory_used": true,
            "memory_type": "external structured retrieval-augmented memory (query-focused hyper-relational knowledge graph + auxiliary graph schema + entity-document graph)",
            "memory_mechanism": "Constructed by BFS traversal of an entity-document graph with LLM-based triple extraction; each triple augmented with source document title to form hyper-triples; pruning uses embedding-based (text-embedding-ada-002) cosine similarity against a query-aligned schema; complementary retrieval fetches additional documents when reader LLM requests missing entity facts.",
            "memory_representation": "Hyper-relational tuples (subject, relation, object, document-title) — verbalized into sentences — plus an auxiliary graph schema (relations and entity types) and the entity-document bipartite graph used for retrieval.",
            "memory_retrieval_method": "Traversal-based (level-order / BFS) seeded by query entities to select candidate documents and entities; semantic pruning via cosine similarity between relation embeddings and schema relation embeddings; complementary retrieval via LLM-instructed listing of missing named entities and fetching their documents.",
            "performance_with_memory": "HotpotQA (reader GPT-4): EM=0.66, F1=0.78; MuSiQue (reader GPT-4): EM=0.48, F1=0.58. Reader input token counts (average): HotpotQA 1230.90 tokens, MuSiQue 1398.15 tokens. Additional: With gpt-4-1106-preview triple extractor on a 100-sample HotpotQA eval: EM=0.68, F1=0.79 (Table 6).",
            "performance_without_memory": "Base (reader GPT-4) with supporting documents (no distilled hyper-relational KG): HotpotQA EM=0.54, F1=0.74; MuSiQue EM=0.39, F1=0.55. Ablation in Table 5 (component removals): configuration with only Hyper-KG (no pruning/aux/schema/comp-retrieval) EM=0.56 (reader tokens 1867).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Ablations in Table 5: starting from only Hyper-relational KG (EM=0.56, tokens=1867), adding pruning reduces tokens (EM remains 0.56, tokens=1227), adding auxiliary schema increases EM to 0.58 (tokens ~1220), and adding complementary fact retrieval raises EM to 0.61 (tokens ~1230). Sensitivity studies: pruning down to p=50 hyper-triples suffices (performance plateaus after 50); traversal depth beyond dataset complexity (≥3 hops) yields no gain. Triple-extractor quality impacts results (Table 6): better extractor (gpt-4-1106-preview) improves EM vs gpt-3.5 extractor.",
            "key_findings": "Using a query-focused hyper-relational KG as an external memory for LLM readers: (1) improves EM/F1 across HotpotQA and MuSiQue versus baselines (StructQA and raw-doc reader), (2) reduces reader input tokens substantially (up to ~67% reduction vs StructQA, and up to ~60% vs raw supporting docs), (3) pruning by a query-aligned schema and complementary fact retrieval materially improves accuracy and efficiency, and (4) allows the system to abstain (output None) in some cases improving self-aware EM.",
            "limitations_or_challenges": "Constructed graphs can be incomplete because automated triple extraction may miss facts; auxiliary schema creation adds compute overhead (one-time) and relies on in-domain question sampling; method depends on the quality of LLM triple extraction and embedding similarity; complementary retrieval is single-step verification which may be insufficient for highly incomplete graphs.",
            "uuid": "e8425.0",
            "source_info": {
                "paper_title": "HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "StructQA",
            "name_full": "StructQA (Structuring extracted triples + CoT for multi-hop QA)",
            "brief_description": "A prior training-free approach that extracts KG triples from text and supplies both the extracted triples and raw text to an LLM (often with chain-of-thought prompting) to perform multi-hop QA; triples are query-agnostic and lack contextual qualifiers.",
            "citation_title": "Leveraging structured information for explainable multi-hop question answering and reasoning",
            "mention_or_use": "use",
            "agent_name": "StructQA",
            "agent_description": "A prompting-based pipeline that extracts structured KG triples (subject, relation, object) from supporting documents (via LLMs), provides those triples along with raw passages to a reader LLM, and employs chain-of-thought prompting for reasoning.",
            "model_name": "GPT-4 / GPT-3.5 (reader in baseline experiments reported in this paper)",
            "model_description": "Reader LLMs used as in this paper; StructQA's triple-extraction also uses LLM-based extractors (same extractor used for fair comparison).",
            "task_name": "HotpotQA; MuSiQue (multi-hop QA baselines in this paper)",
            "task_description": "Same multi-hop QA tasks: answer multi-hop questions using extracted triples plus raw text.",
            "task_type": "multi-hop question answering / retrieval-augmented reasoning",
            "memory_used": true,
            "memory_type": "external structured memory (KG triples) but query-agnostic and not hyper-relational (no contextual document qualifiers attached to triples)",
            "memory_mechanism": "LLM-based triple extraction from documents to form a (flat) KG of triples; both triples and raw supporting passages are concatenated into the reader prompt (no explicit pruning by a query-aligned schema described).",
            "memory_representation": "Triples of the form (subject, relation, object) extracted from supporting documents; raw passages are also included as part of input (redundancy).",
            "memory_retrieval_method": "Triples are extracted from each document and provided together with documents to the reader; no explicit BFS traversal seed/pruning process like HOLMES (i.e., more query-agnostic extraction).",
            "performance_with_memory": "Reported in this paper (Table 1): HotpotQA (reader GPT-4) EM=0.55, F1=0.77; MuSiQue (reader GPT-4) EM=0.42, F1=0.56. Reader input token counts (Table 3): HotpotQA ~3078.85 tokens; MuSiQue ~5908.87 tokens.",
            "performance_without_memory": "Base (reader GPT-4) with supporting documents (no extra KG triples) EM=0.54, F1=0.74 on HotpotQA (Table 1).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared directly against HOLMES (Tables 1,3,6): StructQA uses more input tokens and attains lower EM/F1 than HOLMES; the paper highlights that StructQA's triples lack context (no document-title qualifiers) and that duplicating triples + raw text increases prompt length and redundancy. Sensitivity to triple-extractor quality benefits both StructQA and HOLMES but HOLMES retains a larger margin (Table 6).",
            "key_findings": "Query-agnostic triple extraction (StructQA) provides some benefit over raw reader-only baselines but is outperformed by HOLMES, which adds context via hyper-relational triples and prunes to query-relevant facts; StructQA's inclusion of raw text plus triples increases prompt size and redundancy.",
            "limitations_or_challenges": "Triples are ambiguous without contextual qualifiers (e.g., entity sense or document context); including both triples and raw text increases input token cost; lacks query-aligned pruning, leading to less efficient prompts.",
            "uuid": "e8425.1",
            "source_info": {
                "paper_title": "HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Base (reader w/ supporting docs)",
            "name_full": "Reader LLM baseline using raw supporting documents",
            "brief_description": "A baseline where the reader LLM is given the multi-hop question plus all supporting documents (distractor setting), testing the model's multi-hop reasoning over unstructured text without intermediate structured memory.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Reader LLM with supporting documents (Base)",
            "agent_description": "Simple baseline: provide the reader LLM (GPT-4 / GPT-3.5 / Gemini-Pro) with the question and the raw supporting documents (titles + passages) and ask for an answer; tests LLM's ability to directly retrieve and chain reasoning from unstructured context.",
            "model_name": "GPT-4 / GPT-3.5 / Gemini-Pro (as reported in experiments)",
            "model_description": "Large LMs used as readers; no additional structured external memory other than the raw documents passed in the prompt.",
            "task_name": "HotpotQA; MuSiQue (multi-hop QA baselines in this paper)",
            "task_description": "Multi-hop QA using only the unstructured supporting documents provided in the prompt.",
            "task_type": "multi-hop question answering / document comprehension",
            "memory_used": true,
            "memory_type": "external unstructured memory (supporting documents passed in prompt)",
            "memory_mechanism": "No explicit retrieval or pruning beyond providing all supporting documents in the prompt; relies on LLM internal attention over the prompt.",
            "memory_representation": "Raw document text (title + passage) concatenated into the reader prompt.",
            "memory_retrieval_method": "Prompt-attention / recency and ordering in the prompt; no explicit semantic retrieval beyond what the prompt provides.",
            "performance_with_memory": "HotpotQA (reader GPT-4): EM=0.54, F1=0.74; MuSiQue (reader GPT-4): EM=0.39, F1=0.55 (Table 1). Reader input token count (Table 3) HotpotQA ~1333.81 tokens, MuSiQue ~2361.36 tokens (average).",
            "performance_without_memory": "Base (w/o supporting docs — testing parametric knowledge): HotpotQA (GPT-4) EM=0.26, F1=0.45; MuSiQue EM=0.09, F1=0.21 (Table 1).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared to structured-memory methods: HOLMES outperforms Base with supporting docs by substantial EM gains (e.g., +0.12 EM on HotpotQA with GPT-4); StructQA gives marginal improvement over Base but at higher token cost. Demonstrates that naive inclusion of all supporting docs is less efficient than a distilled structured memory.",
            "key_findings": "Providing raw supporting documents helps over no-document baselines but is less effective and less input-efficient than supplying a distilled, query-focused structured memory (HOLMES).",
            "limitations_or_challenges": "Passing all supporting documents increases prompt size and contains distractors; the LLM must filter and connect relevant facts itself, which leads to lower performance on multi-hop queries and higher token / computation costs.",
            "uuid": "e8425.2",
            "source_info": {
                "paper_title": "HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Leveraging structured information for explainable multi-hop question answering and reasoning",
            "rating": 2
        },
        {
            "paper_title": "Leveraging passage retrieval with generative models for open domain question answering",
            "rating": 2
        },
        {
            "paper_title": "PullNet: Open domain question answering with iterative retrieval on knowledge bases and text",
            "rating": 2
        }
    ],
    "cost": 0.01631025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs</h1>
<p>Pranoy Panda ${ }^{* 1}$ Ankush Agarwal ${ }^{1}$ Chaitanya Devaguptapu ${ }^{1}$ Manohar Kaul ${ }^{1}$ Prathosh A P ${ }^{1,2}$<br>${ }^{1}$ Fujitsu Research India<br>${ }^{2}$ Indian Institute of Science, Bengaluru<br>{pranoy.panda, ankush.agarwal, manohar.kaul}@fujitsu.com<br>email@chaitanya.one, prathoshap@gmail.com</p>
<h4>Abstract</h4>
<p>Given unstructured text, Large Language Models (LLMs) are adept at answering simple (single-hop) questions. However, as the complexity of the questions increase, the performance of LLMs degrade. We believe this is due to the overhead associated with understanding the complex question followed by filtering and aggregating unstructured information in the raw text. Recent methods try to reduce this burden by integrating structured knowledge triples into the raw text, aiming to provide a structured overview that simplifies information processing. However, this simplistic approach is query-agnostic and the extracted facts are ambiguous as they lack context. To address these drawbacks and to enable LLMs to answer complex (multi-hop) questions with ease, we propose to use a knowledge graph (KG) that is context-aware and is distilled to contain query-relevant information. The use of our compressed distilled KG as input to the LLM results in our method utilizing up to $67 \%$ fewer tokens to represent the query relevant information present in the supporting documents, compared to the state-of-the-art (SoTA) method. Our experiments show consistent improvements over the SoTA across several metrics (EM, F1, BERTScore, and Human Eval) on two popular benchmark datasets (HotpotQA and MuSiQue).</p>
<h2>1 Introduction</h2>
<p>Multi-Hop Question Answering (MHQA) is a field that presents unique challenges in the realm of natural language processing. To illustrate the challenges of MHQA, consider extracting information from data arising during board meetings. While current technologies (such as LLMs) are proficient at addressing simple (single-hop) questions, such as "How many board meetings were held in the last twelve months?", they falter when confronted with</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Multi-Dimensional Improvements: Our method (with GPT-4 as reader LLM) achieves SoTA results on several datasets and multiple Multi-hop QA metrics. EM: Exact-Match with the gold answer, SelfAware EM: Confidence-aware EM, BertScore (Zhang et al., 2019): Semantic similarity between predicted and gold answer; Query Info Efficiency: Efficiency of representing query-relevant information in the supporting documents - inversely proportional to the input token count for the reader LLM.
complex (multi-hop) questions. An example of a multi-hop question is "For the board meeting with the most divided votes in the last twelve months, what was the agenda, who voted against it, and by what margin did it pass or fail?". Answering this question requires a series of interconnected steps: first, identifying the meeting with the most divided votes; next, determining the main agenda of that meeting; then, listing the members who voted against it; and finally, calculating the margin by which it was approved or rejected. Each of these steps, or "hops", demands not only the retrieval of additional information but also a nuanced understanding of the context and the relationships between various entities. This complexity underscores the challenge of MHQA, where the goal is to navigate through layers of information to arrive at the answer. Our work is aimed at tackling these multifaceted questions.
LLMs have emerged as a powerful set of tools for various NLP tasks (Zhang et al., 2023; Wad-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Multi-Hop QA Case Study: This figure illustrates a bridge-type multi-hop question from HotpotQA dataset for comparing our method with the baselines. It highlights our method's accurate identification of <em>'Snow Patrol'</em> as the crucial bridge entity and subsequently finding the lead vocalist, a feat not achieved by baselines.</p>
<p>hwa et al., 2023; Robinson et al., 2022; Xu et al., 2023). However, despite their prowess, LLMs oftentimes are found lacking, when confronted with complex, multi-hop questions (see Figure 1 for an example). We hypothesize that the degradation in performance is due to the complexity involved in <em>filtering and contextual aggregation of information present in the unstructured text</em>.</p>
<p>To address this challenge with unstructured text, recent methods, such as StructQA (Li and Du, 2023), extract structured knowledge in the form of KG triples from raw text and couple it with Chain-of-Thought (CoT) reasoning (Wei et al., 2022). However, the extracted KG triples are not dependent on the query and lack the context under which these facts are valid. To understand the ambiguity due to the lack of context, consider this KG triple: {<em>subject: "Apple", relation: "prices rose", object: "10%"</em>}. Without additional context, it is difficult to determine whether the entity "<em>Apple</em>" refers to the fruit or the company. Moreover, StructQA provides both the extracted KG triples and the raw text as input to the LLM, leading to significantly longer prompts (see Table 3) and information redundancy. Our method, HOLMES<sup>1</sup>, addresses these challenges by creating a query-focused context-aware KG and using it as the sole input for the LLM, <em>i.e.</em>, without inputting the raw text. Specifically, we</p>
<p>(i) synthesize a hyper-relational KG from unstructured text that captures both facts, and the context under which these facts are valid, and (ii) prune the hyper-relational KG using a knowledge schema that encodes the type of information necessary to answer the query. These steps collectively furnish the LLM with a curated set of relevant facts. As an example, we provide a case study on the HotpotQA dataset in Figure 2.</p>
<p>To rigorously evaluate our approach, we use two challenging multi-hop QA datasets, HotpotQA (Yang et al., 2018) and MuSiQue (Trivedi et al., 2022b), and experiment with three SoTA reader LLMs for QA: GPT-3.5, GPT-4, and Gemini-Pro. We compare with the baselines across 6 metrics: EM, F1, Precision, Recall, Human-Eval, BERTScore, and achieve significant gains.</p>
<p>Our contributions are as follows:</p>
<ul>
<li>A new multi-hop QA approach that transforms unstructured text into a hyper-relational KG using a query-derived schema, serving as an input to the LLM.</li>
<li>A significant improvement over the SoTA multi-hop QA method (Li and Du, 2023): 18.7% and 20% in EM scores on the Hotpot dataset, and 26% and 14.3% on the MuSiQue dataset for GPT-3.5 and GPT-4, respectively (see Table 1).</li>
<li>Using our query-focused hyper-relational KG we use up to 67% fewer tokens to represent</li>
</ul>
<p><sup>1</sup>Named after Sherlock Holmes for its ability to deduce query relevant information from unstructured text</p>
<p>query relevant information than the current SoTA method, and up to $60 \%$ fewer tokens w.r.t. original supporting documents (see Table 3).</p>
<h2>2 Related Work</h2>
<p>Multi-hop Question Answering (MHQA) Recent achievements in addressing straightforward questions (Lan et al., 2019) coupled with availability of high-quality MHQA datasets (Yang et al., 2018; Trivedi et al., 2022b) have prompted a shift towards tackling multi-hop questions.
Existing works in MHQA have adopted various approaches, including: (i) Construction of dynamic graphs (Qiu et al., 2019), hierarchical (Fang et al., 2020) and Abstract meaning representation (AMR) based graphs (Deng et al., 2022) as well as leveraging KGs (Li and Du, 2023) (ii) Employing end-to-end differentiable learning methods, utilizing Multi-task Transformers for retrieval, reranking and predicting in an iterative manner (Qi et al., 2021) or using Recurrent Neural Networks (RNN) to sequentially retrieve documents from a graph of entities (Asai et al., 2020) and (iii) dynamically converting multi-hop questions into single-hop questions by generating subsequent questions based on the answers to previous ones (Perez et al., 2020), or by updating single-hop questions in the embedding space (Sun et al., 2021). In contrast to these methodologies, our work introduces a novel approach by constructing a hyper-relational KG from the documents, which is then utilized exclusively to answer questions.
Relational Graphs for MHQA While retrieving and relying on purely unstructured text is a go to approach for single-hop question answering (Watanabe et al., 2017). It may not be ideal for MHQA, as it is designed to make multi-step, comprehensive reasoning difficult. This issue is often addressed by constructing structured sources of information from raw text and in most of the cases using KGs (Li and Du, 2023). By using KGs, the relational information among question concepts and answers can be easily captured (Dong et al., 2023). However, one of the main drawback of KGs is the lack of context i.e., they focus only on triples, overlooking qualifiers important for inference. To avoid overlooking qualifiers, our method involves building hyper-relational KGs to serve as an input to LLMs for MHQA, as explained in detail in Sect 4.2.1. To the best of our knowledge, our work is the first to use hyper-relational KGs for MHQA.</p>
<p>Training Free MHQA Previously, KGs have been applied to MHQA in two ways: training-based (Sun et al., 2018, 2019; Yavuz et al., 2022; Ramesh et al., 2023) and training-free (Li and Du, 2023), also known as prompting. In these methods, KGs used were either human-curated (Speer et al., 2017; Bollacker et al., 2008) or training-based (Izacard and Grave, 2021; Bosselut et al., 2019). In some cases, LLMs were employed for triple extraction from documents to form graphs (Li and Du, 2023; Carta et al., 2023). However, no prior works automatically create a schema for the graph and use the graph for MHQA, a unique approach used by us.</p>
<h2>3 Preliminaries</h2>
<p>Hyper-Relational Knowledge Graph - A hyperrelational KG $\mathcal{H}$ is an enriched form of a traditional KG. It allows for the representation of multiple relationships between entities.
Let $\mathcal{A}$ denote a collection of attribute sets whose elements represent different types of attribute sets. For example, $A_{t s} \in \mathcal{A}$ is a set of timestamps, while $A_{d t} \in \mathcal{A}$ is a set of document titles, that serve as additional attributes. We then define our hyper relational KG $\mathcal{H}$ as: $\mathcal{H}=\left{\left(e_{s}, r, e_{o}, a\right) \mid e_{s}, e_{o} \in\right.$ $E, r \in R, a \in A_{d t}}$, where $E$ is the set of named entities, $R$ is the set of relations and $A_{d t}$ is the set of document titles that are additional attributes. This structure not only links entities $e_{s}$ and $e_{o}$ via relation $r$ but also integrates the corresponding document title attributes $A_{d t}$, offering a more nuanced description. We refer $\left(e_{s}, r, e_{o}, a\right)$ as a hyper triple. Graph Schema - A graph schema $\mathcal{G}<em S="S">{S}$, defined as $\mathcal{G}</em>}=\left{\left(\tilde{e<em o="o">{s}, r, \tilde{e}</em>}\right) \mid \tilde{e<em o="o">{s}, \tilde{e}</em>} \in E_{T}, r \in R\right}$, outlines the structure of a KG through entity types $E_{T}$ and relations $R$. Each triple in the schema specifies the permissible entity types $\tilde{e<em o="o">{s}$ (subject) and $\tilde{e}</em>$ (object) for each relation $r$, serving as a blueprint for organizing knowledge.
Named Entity and Relation Extraction - Named entity extraction identifies textual spans referring to specific entities, while relation extraction classifies semantic relationships between entities in text, enabling structured representation of unstructured data. In this work, we harness LLMs' capabilities for these tasks. Pre-trained on extensive text, these models accurately classify named entities and extract relations with minimal examples (Li and Du, 2023; Wang et al., 2023; Wadhwa et al., 2023), showcasing their proficiency in contextual understanding. See Appendices A.4.1, A.4.2 for detailed prompts and few-shot examples used in our method</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: <strong>Method Overview</strong>: Our method has three key steps - (i) Query-Dependent Structured Knowledge Discovery (Section 4.2.1), (ii) Knowledge Schema Construction for Information Refinement (Section 4.2.2), and (iii) Reader LLM Prompt Construction (Section 4.2.3). Step (i) involves creation of an entity document graph (1-9 in the Figure), and performing a level-order traversal on it to get a Hyper-relational KG (2 in the Figure). Next, in step (ii), we create a query-aligned knowledge schema from the question and an auxiliary graph schema (1-9 in the Figure), and use it to prune the Hyper-Relational KG (3 in the Figure) - which forms the input for the LLM.</p>
<p>for named entity and relation extraction.</p>
<h2>4 Methodology</h2>
<p>The key idea of our method is to identify the subset of documents that contain the answer to the multi-hop query, and subsequently extract context-aware structured information from them (where context comes from the documents). We further perform a refinement step to retain query-relevant information. Below, we begin with a brief description of the problem statement in Section 4.1, followed by an explanation of our method in Section 4.2. The overview of our approach can be seen in Figure 3.</p>
<h3>4.1 Problem Statement</h3>
<p>In this work, we address the challenge of MHQA in a zero-shot setting. This implies that we assume no prior domain-specific labeled data, rendering the problem both challenging and practically useful (Davison, 2020). Moreover, we operate in a training-free setting, leveraging the reasoning capabilities of LLMs for this task.</p>
<p>Formally, the task is to extract the answer (a_q) from a given natural language question (q) and a collection of supporting documents (\mathcal{S}<em q_1="q^1">q). The set of supporting documents is defined as (\mathcal{S}_q := { D</em>_q) (where (i \neq j) ).}, D_{q^2}, \ldots, D_{q^m} } ), where each document (D_{q^i} := ( t_{q^i}, p_{q^i} ) ) consists of a title (t_{q^i} ) and the associated passage (p_{q^i} ). Given that (q) is a multi-hop question, deducing its answer requires the aggregation of information from at least two documents (D_{q^i}, D_{q^j} \in \mathcal{S</p>
<h3>4.2 Proposed Method (HOLMES)</h3>
<p>We begin by traversing the supporting documents to identify the subset of documents relevant to the query, and extract structured, context-aware information (Section 4.2.1). This information is then refined using a query-based knowledge schema (Section 4.2.2). Next, we format the distilled graph for the reader LLM prompt, including a complementary fact retrieval step within the prompt to ensure higher coverage of query-relevant information. (Section 4.2.3).</p>
<h4>4.2.1 Query-Dependent Structured Knowledge Discovery</h4>
<p>Unstructured text, contains a complex web of facts and relationships forming a latent semantic graph. This graph represents interconnected factual information not explicitly structured but implied by relationships and named entities in the text. Our approach navigates this structure using the entity-document graph—a bipartite graph linking documents and named entities. It helps uncover parts of the latent graph relevant to the given query.</p>
<p><strong>Entity-Document Graph Construction</strong>
The entity-document graph has two node</p>
<p>types—documents and entities-with a single edge connecting them. We begin by extracting named entities from supporting documents. Then, we establish edges between document and entity nodes, forming a bipartite graph that captures the connections between entities and the documents they appear in (See Figure 3 - 4.4 for an example).
Level Order Traversal and Structured Information Extraction We begin by extracting named entities from the query and use them for a level-order/breadth-first traversal of the entity-document graph. Similar to Li and Du (2023); Wadhwa et al. (2023), we use LLMs to extract KG triples from document nodes, framing the task as a sequence-tosequence problem. By providing the LLM with a detailed prompt (a clear description of the task and output format) and few-shot examples, we guide the model to generate triples directly from raw text (see Appendix A.4.4 for prompts).
After extracting triples, we enhance them into hyper-relational KG quadruples (or hyper triples) by appending the title of the source document to each triple. This title serves as an additional attribute, offering the LLM context to understand when the triple is valid. During the traversal, we filter hyper triples with the current entity node as either the subject or object, adding the counterpart entity to the traversal queue. This iterative process continues for a predefined number of hops $k$, where the value of $k$ defines the query complexity, i.e., the number of facts required to answer the question. Example of a hyper-relational KG in Figure 3-3. When we perform the above specified $k$-hop traversal starting from the named entities in the query, we are effectively probing the latent semantic graph in the raw text by incrementally selecting subsets of documents related to the query.</p>
<h3>4.2.2 Knowledge Schema Construction for Information Refinement</h3>
<p>The hyper-relational graph construction is rooted in the inference query. However, it contains some hyper triples which although are related to the named entities in the query, capture relationships that are not useful for answering it. For instance, a query might focus on a person's occupation, but the graph also captures their hobbies. To eliminate such extraneous information, we construct a query-aligned knowledge schema (Figure 3 - 4.9 shows an example) and then carry out a refinement step (refined output example in Figure 3 - 3).
Query-Aligned Graph Schema Creation The
graph schema acts as a structural template, guiding the organization of information crucial for answering the given question. It is essential for capturing the types of entities and relations likely to form the backbone of the answer. For example, in response to the question "Who is the 2nd daughter of the 1st president of $X$ ?" the schema should capture relations such as ${$ <Person $>$, daughter of, <Person> $}$ and ${&lt;$ Person $&gt;$, president of, <Country> $}$, representing the direct information sought by question. Our graph schema is populated using two sources:
(i) We first derive schema elements from the inference query by identifying relations in it, and then using LLMs to estimate the subject and object entity types for each of these relations (prompt in Appendix A.4.3). This forms a schema with a direct alignment with the question's intent.
(ii) We enrich the schema with additional domainspecific relations to aid multi-hop reasoning, using an auxiliary graph schema derived from indomain questions (Appendix A.5). This includes a wider array of entity-relation pairs, such as ${&lt;$ Person $&gt;$, child of, <Person>} and ${&lt;$ Person $&gt;$, head of, <Country> $}$, to account for potential inferential steps when direct query matches are absent in supporting documents. We select schema triples with relations similar to the question's (based on cosine similarity) to retrieve a relevant subset. While constructing this auxiliary schema incurs a one-time compute cost, its amortized cost is minimal over multiple queries, especially given it helps to reduce the input token length for the reader LLM by upto $60 \%$ w.r.t. original source documents.
In cases where the inference query diverges from the auxiliary graph schema, we rely on the schema derived from the query. This ensures system flexibility, adapting to specific query requirements, while still benefiting from the broader knowledge in the auxiliary schema when applicable.</p>
<h2>Pruning Hyper-relational Knowledge Graph</h2>
<p>The final step in our methodology involves refining the constructed hyper-relational $\mathrm{KG}{ }^{\wedge} \mathcal{H}^{\prime}$ by pruning it using the query-aligned graph schema $\mathcal{G}<em r="r">{S}$. This pruning process is essential to distill the graph to its most relevant components, thereby enhancing the efficiency and effectiveness of the reader LLM in generating answers.
The pruning process begins by computing embeddings for the relations in both the hyper-relational KG and the graph schema. Let $\mathbf{v}</em>$ denote the embedding of a relation $r$ in the hyper-relational KG,</p>
<p>and let $\mathbf{u}_{r^{\prime}}$ denote the embedding of a relation $r^{\prime}$ in the graph schema. We compute cosine similarity between each pair of relation embeddings:</p>
<p>$$
\operatorname{sim}\left(\mathbf{v}<em r_prime="r^{\prime">{r}, \mathbf{u}</em>}}\right)=\frac{\mathbf{v<em r_prime="r^{\prime">{r} \cdot \mathbf{u}</em>}}}{\left|\mathbf{v<em r_prime="r^{\prime">{r}\right|\left|\mathbf{u}</em>
$$}}\right|</p>
<p>where $\cdot$ denotes the dot product and $|\cdot|$ denotes the Euclidean norm. For each relation $r_{h}$ in a hyper triple $\left(e_{s}, r_{h}, e_{o}, a\right) \in \mathcal{H}$, we compute its highest similarity score w.r.t. any relation $r_{s}$ in the schema:</p>
<p>$$
\operatorname{score}\left(r_{h}\right)=\max <em s="s">{r</em>} \in \mathcal{G<em r__h="r_{h">{S}} \operatorname{sim}\left(\mathbf{v}</em>}}, \mathbf{u<em s="s">{r</em>\right)
$$}</p>
<p>We compute the scores for all those relations whose entity types in the hyper triple match with that in the schema. We then select $p$ hyper triples with the highest scores. This gives us the pruned graph $\mathcal{H}^{\prime}=\operatorname{sort}<em _score="{score" _text="\text">{\text {score }}(\mathcal{H}, p)$, where $\operatorname{sort}</em>$ denotes the sorting operation based on the computed similarity scores, and $p$ - no. of hyper triples to retain.}</p>
<h3>4.2.3 Reader LLM Prompt Construction</h3>
<p>Next, we create the prompt for the reader LLM. Each hyper triple in the pruned hyper-relational $\mathrm{KG}, \mathcal{H}^{\prime}$, is verbalized into an English sentence as the LLMs are adept at understanding the same (Jiang et al., 2023). The verbalization process transforms the structured triple into a natural language text. To be specific, we concatenate each item in the hyper triple into a long sentence marked by specific separation and boundary symbols. The resulting sentences are then arranged in the descending order of their similarity scores w.r.t. the schema to form the input prompt. We arrange the facts based on the relevance to the query (measured via similarity scores) as the information retrieval by LLMs (w.r.t. their input prompt) is done best when the gold information is placed closest to query (Cuconasu et al., 2024).
As structured information extraction is an unsolved problem, some pertinent details may be missed in the input graph. To mitigate this, we include a verification step in the prompt, described below.
Complementary Fact Retrieval If the LLM identifies that facts about a particular set of named entities is missing from the input graph, then we instruct it to list those named entities. We then fetch the corresponding documents from the entitydocument graph and integrate them with the initial set of relevant facts. This process not only enriches the input for the LLM but also ensures that any missing query-relevant information is retrieved, enhancing the accuracy of the system's responses. We assume that a single-step verification is sufficient. Refer Appendix A.4.5 for the reader LLM prompt.</p>
<h2>5 Experimental Setup</h2>
<h3>5.1 Evaluation Details</h3>
<p>Datasets: We use two benchmark multi-hop question-answering datasets, namely HotpotQA (Yang et al., 2018) and MuSiQue (Trivedi et al., 2022b) for evaluating our method. Table 9 displays the total number of samples for both datasets in the training and development sets.
For our evaluation process, we utilize questions, context, and gold answers from the development sets of both HotpotQA and MuSiQue. To tune hyperparameters, we randomly select 50 questions from the development set and use them (following (Trivedi et al., 2022a; Li and Du, 2023)). We create our test set by randomly sampling an additional 1000 questions from the development set of HotpotQA and 1200 questions from development set of MuSiQue. Its worth noting that our test set size is twice as big as StructQA (Li and Du, 2023). In both HotpotQA and MuSiQue datasets, for each question there are ten and twenty supporting documents available, respectively. Each document in both datasets contains a title and a passage of text.
Baselines: We operate in a training-free setting, utilizing LLMs as the reader model (takes the question and corresponding supporting documents as input and gives an answer). We experiment with two popular LLMs - GPT-3.5, GPT-4 (Achiam et al., 2023) in the main paper, and report results with Gemini (Lee et al., 2023) in Appendix A.1. We compare our method against three baselines
(i) StructQA (Li and Du, 2023): CoT (Wei et al., 2022) based SoTA method for MHQA with LLMs. We use the same prompts and methodology as outlined in their study.
(ii) Base (with supp docs): To test the base multihop reasoning of the reader LLM, in this baseline, we feed the reader LLM with the question and supporting documents directly.
(iii) Base (w/o supp docs): To test the parametric knowledge of the reader LLM, in this baseline, we feed it with just the question and elicit a response. All prompts used in our experiments are documented in the Appendix A. 4 for reproducibility.
Evaluation Metrics: We use Exact-Match (EM), Precision, Recall, and F1-Score as automatic metrics (Li and Du, 2023; Mavi et al., 2022) to measure correctness of the predicted answers.</p>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>HotpotQA</th>
<th></th>
<th></th>
<th></th>
<th>MuSiQue</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Methods</td>
<td>EM (↑)</td>
<td>F1 (↑)</td>
<td>P (↑)</td>
<td>R (↑)</td>
<td>EM (↑)</td>
<td>F1 (↑)</td>
<td>P (↑)</td>
<td>R (↑)</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Reader: gpt-4-1106-preview</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Base (w/o supp docs)</td>
<td>0.26</td>
<td>0.45</td>
<td>0.45</td>
<td>0.50</td>
<td>0.09</td>
<td>0.21</td>
<td>0.22</td>
<td>0.21</td>
</tr>
<tr>
<td>Base (with supp docs)</td>
<td>0.54</td>
<td>0.74</td>
<td>0.75</td>
<td>0.77</td>
<td>0.39</td>
<td>0.55</td>
<td>0.55</td>
<td>0.56</td>
</tr>
<tr>
<td>StructQA <em>Li and Du (2023)</em></td>
<td>0.55</td>
<td>0.77</td>
<td>0.75</td>
<td>0.80</td>
<td>0.42</td>
<td>0.56</td>
<td>0.57</td>
<td>0.56</td>
</tr>
<tr>
<td>Our Method</td>
<td>0.66</td>
<td>0.78</td>
<td>0.80</td>
<td>0.79</td>
<td>0.48</td>
<td>0.58</td>
<td>0.59</td>
<td>0.59</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Reader: gpt-3.5-turbo-1106</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Base (w/o supp docs)</td>
<td>0.23</td>
<td>0.37</td>
<td>0.38</td>
<td>0.40</td>
<td>0.06</td>
<td>0.15</td>
<td>0.17</td>
<td>0.15</td>
</tr>
<tr>
<td>Base (with supp docs)</td>
<td>0.47</td>
<td>0.65</td>
<td>0.66</td>
<td>0.68</td>
<td>0.24</td>
<td>0.36</td>
<td>0.36</td>
<td>0.37</td>
</tr>
<tr>
<td>StructQA <em>Li and Du (2023)</em></td>
<td>0.48</td>
<td>0.64</td>
<td>0.62</td>
<td>0.67</td>
<td>0.23</td>
<td>0.37</td>
<td>0.37</td>
<td>0.37</td>
</tr>
<tr>
<td>Our Method</td>
<td>0.57</td>
<td>0.69</td>
<td>0.69</td>
<td>0.70</td>
<td>0.29</td>
<td>0.38</td>
<td>0.39</td>
<td>0.37</td>
</tr>
</tbody>
</table>
<p>Table 1: Multi-hop Reasoning Evaluation (Automatic Metrics): We report the Exact-Match (EM), F1, Precision (P) and Recall (R) scores of all methods in comparison on two MHQA datasets. We experiment with two SoTA reader LLMs for the QA task - GPT-4 and GPT-3.5. We report results on Gemini-pro in the Appendix A.1. The results indicate consistent and significant improvements across datasets, metrics and LLMs. Base: Only reader LLM; supp docs: supporting documents w.r.t. query</p>
<p>For semantic evaluation of the predicted answers, we also compute Human Evaluation Score (H-Eval) and BERTScore <em>Zhang et al. (2019)</em> on a random sample of 100 questions from the development set of the HotpotQA dataset. The Human Evaluation Scores are obtained by averaging scores from three annotators. We use a smaller subset of question for this analysis due to the resource-intensive nature of human evaluation.</p>
<h3>5.2 Implementation Details</h3>
<p>Problem Setting We focus on the distractor setting <em>Yang et al. (2018); Trivedi et al. (2022b)</em> for question answering. In this setting, the supporting documents for each question consist of a set of distractor documents, i.e., documents not useful for predicting the answer. This setting poses a challenge, demanding robustness to noise in input.</p>
<p>Knowledge Triple Extraction Both StructQA and our method use LLMs for knowledge triple extraction. Thus, for a fair comparison, we always use the same triple extractor LLM for both. Results in Table 1, use gpt-3.5-turbo-instruct for triple extraction. We further study the impact of different triple extractor LLM in Table 6.</p>
<p>HOLMES hyperparameters We use OpenAI embedding model (text-embedding-ada-002) to compute text embeddings. We set the value of $k$ (number of levels in the level order traversal) to 4 across both the datasets in Table 1,6. Beyond 4 levels of traversal, performance remains the same as the complexity of the dataset vary from 2-4 hop (See Table 10). Similarly, we set the value of $p$ (number of hyper triples to be retained after pruning) to 50 across all result tables (sensitivity analysis in the Appendix A.1). Both of these values were chosen by experimenting on 50 samples from the development sets of the respective datasets.</p>
<p>Auxiliary Graph Schema Creation We randomly sample 10,000 questions (without answers and supporting documents) from the training data of HotpotQA and MuSiQue, for creating the graph schema. We provide further details about the auxiliary schema creation in the Appendix A.5.</p>
<h2>6 Results and Analysis</h2>
<p>Here, we study the performance of our method by investigating several key dimensions of multi-hop question answering (especially in the era of LLMs):</p>
<ol>
<li>Multi-hop reasoning capability</li>
<li>w.r.t automatic metrics</li>
<li>w.r.t human &amp; semantic metrics</li>
<li>Performance w.r.t. different question types</li>
<li>Reasoning type-wise performance</li>
<li>Hop-wise performance</li>
<li>Query Information efficiency (input token count for reader LLM)</li>
<li>Measure of confident predictions</li>
</ol>
<p>We also study the impact of the LLM used for knowledge triple extraction, and conduct further studies in the App A.1 and a case study in App A.6.</p>
<p>Multi-hop Reasoning: (a) Evaluation using Automatic Metrics - In Table 1 we report the performance of our method w.r.t. Exact Match (EM) and F1 scores for both datasets. We find that, across reader LLMs, our method consistently outperforms all baseline methods. This underscores the importance of our data organization and pruning process, retaining only the relevant information in input prompts for the reader LLMs. Notably, our method even outperforms the SoTA StructQA, which employs the CoT mechanism <em>Wei et al. (2022)</em>.</p>
<p>| Dataset | HotpotQA | |
| Methods | H-Eval (↑) | B-Score (↑) |
| --- | --- | --- |
| Base | 86.5 | 87.00 |
| StructQA | 87.5 | 85.20 |
| Our Method | $\mathbf{8 9 . 0}$ | $\mathbf{8 9 . 0 5}$ |</p>
<p>Table 2: Multi-hop Reasoning Evaluation (Semantic Metrics): Human evaluation (H-Eval) on 100 samples \&amp; BERTScore (B-Score) for 1000 samples from HotpotQA.
(a) Human and Semantic Evaluation As generative models can generate long worded answers, its important to semantically evaluate the predicted answers. Thus, we use BertScore and human evaluators to judge the correctness of the predicted answers. We use GPT-4 reader based responses for this study and report the results in Table 2.</p>
<p>We observe that our method achieves a 1-2 point improvement on both the metrics w.r.t. the baselines, confirming the efficacy of our method.
Query Information Efficiency In multi-hop question answering, where reader LLMs like GPT-4 are employed, the presence of irrelevant information poses a significant challenge because (i) it increases the computational load (and thus the cost) (ii) and complicates the LLM's task of connecting disparate pieces of information across documents. Thus, efficiently managing input data by filtering out unnecessary content becomes crucial to both performance and cost-effectiveness. To quantify this factor, we measure the reader input token count and use an efficiency metric for input tokens. The query information effiency (or reader input token efficiency) metric is a normalized score between zero and one, computed through min-max normalization of input token count (min \&amp; max are based on input token counts of all methods in comparison). Our results, in Figure 1 and Table 3, show a significant reduction in average input token count across both datasets compared to baselines.</p>
<p>| Datasets | HotpotQA | MuSiQue |
| Methods | Token Count $(\downarrow)$ | Token Count $(\downarrow)$ |
| Supp-docs | 1333.81 | 2361.36 |
| StructQA | 3078.85 | 5908.87 |
| Our Method | $\mathbf{1 2 3 0 . 9 0}$ | $\mathbf{1 3 9 8 . 1 5}$ |</p>
<p>Table 3: Reader Input Token Count: Comparing the average input token size across datasets to measure the efficiency of representing query relevant information.</p>
<h2>Performance w.r.t. different question types</h2>
<p>To analyze HOLMES's performance across question types and complexities, we perform an analysis based on reasoning types (with HotpotQA dataset) and hops (with MuSiQue dataset). We choose re- spective datasets for each analysis based on the query type information available in the datasets.</p>
<h2>(a) Reasoning type-wise performance</h2>
<p>For any multi-hop question, the data generating process dictates the type of reasoning required to answer the question. HotpotQA dataset contains questions which fall in two categories w.r.t. reasoning - (i) bridge question (ii) comparison questions. We advise the reader to refer to the dataset paper (Yang et al., 2018) for more details. We report results across question categories in Table 4.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;"># Samples</th>
<th style="text-align: center;">Base</th>
<th style="text-align: center;">StructQA</th>
<th style="text-align: center;">HOLMES</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Bridge</td>
<td style="text-align: center;">787</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">$\mathbf{0 . 6 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Comparison</td>
<td style="text-align: center;">213</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">$\mathbf{0 . 7 1}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance Across Reasoning Types: Evaluation of different methods. Base - Reader LLM with supp docs.</p>
<h2>(b) Hop-wise Performance Comparison</h2>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Hop-wise Performance: Comparison on MuSiQue dataset with varying question complexity. Bars denote standard error $-\sqrt{\frac{c m{1-c m}}{m}}$.
We evaluated the hop-wise performance of our method on MuSiQue dataset (Figure 4). This evaluation aimed to determine if our performance improvements (in Table 1) were primarily attributed to the system's ability to answer 2-hop questions, which are less complex than 3 and 4-hop questions. We find that our method maintains its performance across an increasing number of hops and outperforms the baselines too.
Measure of Confident Predictions LLMs demonstrate reasoning capabilities but often struggle to recognize their limitations in understanding queries or retrieving correct answers. We found that structured inputs and task-dependent reasoning steps, including the option for models to indicate uncertainty, alleviates this issue. Inspired by this, we introduce the Self-Aware Exact Match (Self-</p>
<p>Aware EM) score to assess LLMs on complex QA tasks. This metric evaluates the accuracy and confidence of model responses, focusing on instances where the model provides answers with high confidence (or self-awareness). It aims to highlight the model's precision and reliability, offering a nuanced view of its performance. It is defined as: Self Aware EM $=\frac{\sum_{q \in Q_{A}} \mathrm{EM}(q)}{|Q_{A}|}$ where $Q_{A}$ is the set of questions the system answers. We report the Self-Aware EM scores in Figure 1.
In our evaluation using the HotpotQA and MuSiQue datasets, HOLMES opted for "None" or "No answer" in 3\% and 10\% of the samples, respectively (with GPT-4 as reader LLM). In contrast, StructQA always provided an answer, often inaccurately (see Table 1). Upon reviewing instances where HOLMES did not provide an answer, we found it was due to incomplete information in the input graph or a misunderstanding of the question. This self-awareness-recognizing when it cannot reliably answer-is a key feature of dependable systems. Our findings, detailed in Figure 1, show that HOLMES achieves significantly higher selfaware EM scores compared to other baselines.</p>
<h2>7 Sensitivity \&amp; Ablation Analysis</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Configuration</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Performance</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Hyper <br> KG</td>
<td style="text-align: center;">Prune</td>
<td style="text-align: center;">Aux. <br> Schema</td>
<td style="text-align: center;">Comp. Fact <br> Retrieval</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">1 Reader In <br> Tokens</td>
</tr>
<tr>
<td style="text-align: center;">X</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">1867</td>
</tr>
<tr>
<td style="text-align: center;">X</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">1227</td>
</tr>
<tr>
<td style="text-align: center;">X</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">1220</td>
</tr>
<tr>
<td style="text-align: center;">X</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">X</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">1230</td>
</tr>
</tbody>
</table>
<p>Table 5: Impact of different components: Performance metrics (EM, F1) and Reader Input Token Count for different configurations of HOLMES. X refers to "included", and refers to "excluded" in the table. For example, in the first row, only Hyper-Relational KG is included in the HOLMES algorithm, every other component is removed. These results are reported on a set of 100 samples from HotpotQA dev set.</p>
<p>Ablation Studies In Table 5 we share the results of our ablation study that systematically evaluates the contribution of each component within the HOLMES framework.</p>
<p>Sensitivity Analysis We evaluated the effect of using a different LLM (gpt-4-1106-preview) for triple extraction, comparing our performance with StructQA in Table 6 (reader: GPT-4). Our approach consistently outperforms StructQA, although improved triple extraction models benefit both.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">HotpotQA (100 samples)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Methods</td>
<td style="text-align: center;">EM ( $\uparrow$ )</td>
<td style="text-align: center;">F1 ( $\uparrow$ )</td>
<td style="text-align: center;">P ( $\uparrow$ )</td>
<td style="text-align: center;">R ( $\uparrow$ )</td>
</tr>
<tr>
<td style="text-align: center;">Triple Extractor: gpt-4-1106-preview</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">StructQA</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.77</td>
</tr>
<tr>
<td style="text-align: center;">Our Method</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;">Triple Extractor: gpt-3.5-turbo-1106</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">StructQA</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: center;">Our Method</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.78</td>
</tr>
</tbody>
</table>
<p>Table 6: Impact of Triple Extractor on MHQA performance</p>
<h2>8 Conclusion</h2>
<p>We introduce HOLMES, an approach leveraging a hyper-relational KG to enhance multi-hop question answering by minimizing noise and refining relevant facts. Constructing an entity-doc graph from the question's supporting documents, we employ a level-order traversal, prune with an auxiliary graph schema, and utilize distilled graph as input for LLM-based answering. Our method achieves SoTA performance with a 20\% improvement on HotpotQA and 26\% on MuSiQue dataset w.r.t EM while using upto 67\% fewer tokens to represent query relevant information w.r.t. SoTA methods.</p>
<h2>9 Acknowledgement</h2>
<p>We would like to thank Shivangana Rawat and other members of the AI lab at Fujitsu Research India for providing valuable feedback on the manuscript. We also thank the anonymous ARR reviewers, the meta-reviewer, and the ACL program chairs for their comments and feedback, which helped improve our draft.</p>
<h2>Limitations</h2>
<p>The following details outline areas for future improvements:</p>
<h2>Possible Incompleteness in Constructed Graphs</h2>
<p>Extracting entities and triples to construct graphs via LLMs demonstrates high accuracy, yet the task of capturing all relevant triples faces inherent challenges. Structured information extraction remains an unsolved problem across the field, leading occasionally to incomplete graphs in our method as well
Increased Computational Effort: Our method's use of LLMs to generate an auxiliary schema introduces additional computational steps. While this increases the computational effort compared to prior methods, it's offset by our efficiency w.r.t fewer input tokens during inference.</p>
<p>While acknowledging the limitations of our approach, we remain optimistic, recognizing our work as a step forward in enhancing the capabilities of LLMs for multi-hop question answering.</p>
<h2>Ethical Concerns</h2>
<p>There are no ethical concerns associated with this work.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph for question answering. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247-1250.</p>
<p>Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. Comet: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4762-4779.</p>
<p>Salvatore Carta, Alessandro Giuliani, Leonardo Piano, Alessandro Sebastian Podda, Livio Pompianu, and Sandro Gabriele Tiddia. 2023. Iterative zero-shot llm prompting for knowledge graph construction. arXiv preprint arXiv:2307.01128.</p>
<p>Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The power of noise: Redefining retrieval for rag systems. arXiv preprint arXiv:2401.14887.</p>
<p>Joe Davison. 2020. Zero-shot learning in modern nlp.
Zhenyun Deng, Yonghua Zhu, Yang Chen, Michael Witbrock, and Patricia Riddle. 2022. Interpretable amrbased question decomposition for multi-hop question answering. arXiv preprint arXiv: 2206.08486.</p>
<p>Junnan Dong, Qinggang Zhang, Xiao Huang, Keyu Duan, Qiaoyu Tan, and Zhimeng Jiang. 2023. Hierarchy-aware multi-hop question answering over knowledge graphs. In Proceedings of the ACM Web Conference 2023, WWW '23, page 2519-2527, New</p>
<p>York, NY, USA. Association for Computing Machinery.</p>
<p>Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, and Jingjing Liu. 2020. Hierarchical graph network for multi-hop question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8823-8838, Online. Association for Computational Linguistics.</p>
<p>Gautier Izacard and Édouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880.</p>
<p>Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Structgpt: A general framework for large language model to reason over structured data. arXiv preprint arXiv:2305.09645.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. International Conference on Learning Representations.</p>
<p>Gyeong-Geon Lee, Ehsan Latif, Lehong Shi, and Xiaoming Zhai. 2023. Gemini pro defeated by gpt-4v: Evidence from education. arXiv preprint arXiv:2401.08660.</p>
<p>Ruosen Li and Xinya Du. 2023. Leveraging structured information for explainable multi-hop question answering and reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6779-6789.</p>
<p>Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. 2022. A survey on multi-hop question answering and generation. arXiv preprint arXiv:2204.09140.</p>
<p>Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela. 2020. Unsupervised question decomposition for question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8864-8880, Online. Association for Computational Linguistics.</p>
<p>Peng Qi, Haejun Lee, Tg Sido, and Christopher Manning. 2021. Answering open-domain questions of varying reasoning steps from text. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3599-3614, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Lin Qiu, Yunxuan Xiao, Yanru Qu, Hao Zhou, Lei Li, Weinan Zhang, and Yong Yu. 2019. Dynamically fused graph network for multi-hop reasoning. In</p>
<p>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 61406150, Florence, Italy. Association for Computational Linguistics.</p>
<p>Gowtham Ramesh, Makesh Narsimhan Sreedhar, and Junjie Hu. 2023. Single sequence prediction over reasoning graphs for multi-hop qa. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11466-11481.</p>
<p>Joshua Robinson, Christopher Michael Rytting, and David Wingate. 2022. Leveraging large language models for multiple choice question answering. arXiv preprint arXiv:2210.12353.</p>
<p>Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: an open multilingual graph of general knowledge. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, pages $4444-4451$.</p>
<p>Haitian Sun, Tania Bedrax-Weiss, and William Cohen. 2019. Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2380-2390.</p>
<p>Haitian Sun, William W. Cohen, and Ruslan Salakhutdinov. 2021. Iterative hierarchical attention for answering complex questions over long documents. arXiv preprint arXiv: 2106.00200.</p>
<p>Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William Cohen. 2018. Open domain question answering using early fusion of knowledge bases and text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4231-4242.</p>
<p>Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022a. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509.</p>
<p>Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022b. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539-554.</p>
<p>Somin Wadhwa, Silvio Amir, and Byron C Wallace. 2023. Revisiting relation extraction in the era of large language models. arXiv preprint arXiv:2305.05003.</p>
<p>Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. 2023. Gpt-ner: Named entity recognition via large language models. arXiv preprint arXiv:2304.10428.</p>
<p>Yusuke Watanabe, Bhuwan Dhingra, and Ruslan Salakhutdinov. 2017. Question answering from unstructured text by retrieval and comprehension. arXiv preprint arXiv: 1703.08885.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, and Enhong Chen. 2023. Large language models for generative information extraction: A survey. arXiv preprint arXiv:2312.17617.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600.</p>
<p>Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, Nitish Shirish Keskar, and Caiming Xiong. 2022. Modeling multi-hop question answering as single sequence prediction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 974-990.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.</p>
<p>Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. 2023. Sentiment analysis in the era of large language models: A reality check. arXiv preprint arXiv:2305.15005.</p>
<h2>Appendix</h2>
<p>In this section, we provide additional results and details that we could not include in the main paper due to space constraints. In particular, this appendix contains the following:</p>
<ul>
<li>Additional Results and Analysis</li>
<li>Dataset Statistics</li>
<li>Guidelines for Human Annotators</li>
<li>Prompts used in HOLMES</li>
<li>Auxilliary schema construction</li>
<li>Case Study of HOLMES on HotpotQA dataset</li>
</ul>
<h2>A. 1 Additional Results and Analysis</h2>
<p>Extending our analysis in Section 6, here we report the following - (i) MHQA performance with Gemini-Pro reader LLM (ii) Sensitivity Analysis w.r.t. pruning process (iii) Sensitivity Analysis w.r.t. the number of depth of traversal while creating the Hyper-Relational KG.</p>
<h2>Gemini-Pro MHQA Results</h2>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>HotpotQA</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Methods</td>
<td>EM ( $\uparrow$ )</td>
<td>F1 ( $\uparrow$ )</td>
<td>SA-EM ( $\uparrow$ )</td>
</tr>
<tr>
<td>Reader: Gemini-Pro</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Base (w/ supp docs)</td>
<td>0.48</td>
<td>0.66</td>
<td>0.48</td>
</tr>
<tr>
<td>StructQA</td>
<td>0.49</td>
<td>0.66</td>
<td>0.52</td>
</tr>
<tr>
<td>Our Method</td>
<td>$\mathbf{0 . 5 8}$</td>
<td>$\mathbf{0 . 6 7}$</td>
<td>$\mathbf{0 . 6 6}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Multi-Hop QA performance SA-EM: SelfAware EM; Base: Only reader LLM; supp docs: supporting documents w.r.t. query</p>
<p>Similar to results with GPT-3.5 and GPT-4 reader LLMs (in Table 1), we observe consistent improvements across metrics. Thus demonstrating the efficacy of our method, across LLMs.</p>
<h2>Impact of Pruning</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Impact of pruning on MHQA performance in HotpotQA dataset</p>
<p>In Tables 1 and 6, we conducted our experiments by retaining 50 hyper triples after the pruning stage. Here we vary the number of hyper triples being retained after pruning and study the impact on the MHQA performance (EM and F1 scores). Figure 5 shows that after 50 triples, the MHQA performance does not change indicating that the first 50 triples captures all the query relevant information. This number is dependent on the dataset, so we suggest conducting experiments to deduce this threshold.</p>
<h2>Impact of Depth in the Level Order Traversal</h2>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Impact of Depth on MHQA performance in HotpotQA dataset</p>
<p>In the Tables 1 and 6, we conducted our experiments by traversing the entity document graph upto a depth of four levels. As stated in Section 4.2.1, the depth of traversal depends on the maximum complexity of query we expect to see during inference. To verify this, we perform a sensitivity analysis on this variable. We experiment on 100 randomly sampled datapoints from HotpotQA development set(question complexity varies between 2-3 hops). Figure 6 reflects that as the max query complexity of the dataset is three, beyond three levels of traversal the performance does not improve. Thus, verifying our claim. We choose a depth of four in the main paper as we wanted to cater to all the datasets. Total Input and Output Token Lengths \&amp; Costs</p>
<p>|  Method | Inp Token
Length ( $\downarrow$ ) | Out Token
Length ( $\downarrow$ ) | Total Cost
in \$ ( $\downarrow$ ) | EM
( $\uparrow$ )  |
| --- | --- | --- | --- | --- |
|  StructQA | $\mathbf{9 0 1 2}$ | 3590 | 0.99 | 0.48  |
|  HOLMES | 9388 | $\mathbf{2 5 2 4}$ | $\mathbf{0 . 8 5}$ | $\mathbf{0 . 5 7}$  |</p>
<p>Table 8: Performance and Cost Comparison: Comparison of total input and output token lengths (across all LLM calls), total cost, and EM scores for different methods.</p>
<p>In table 8 we report the average total input and</p>
<p>output token lengths on HotpotQA dataset (w.r.t. 100 random samples from dev set). We also share the cost estimate w.r.t. the latest GPT-3.5 LLM (both as reader and triple extractor model)
The average input and output token lengths when simply using the reader LLM are 1334 and 10, respectively. Its worth noting that for performance improvements beyond LLMs on complex NLP datasets such as HotpotQA and MuSiQue, structured information extraction is immensely useful, as can be seen from the improvements across metrics in Table 1 of the paper.</p>
<h3>A. 2 Dataset Statistics</h3>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Dev</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">HotpotQA</td>
<td style="text-align: center;">90,447</td>
<td style="text-align: center;">7,405</td>
</tr>
<tr>
<td style="text-align: center;">MuSiQue</td>
<td style="text-align: center;">19,938</td>
<td style="text-align: center;">2,417</td>
</tr>
</tbody>
</table>
<p>Table 9: Train and Dev Statistics - number of samples</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">number of instances</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MuSiQue 2-hop</td>
<td style="text-align: center;">667</td>
</tr>
<tr>
<td style="text-align: center;">MuSiQue 3-hop</td>
<td style="text-align: center;">366</td>
</tr>
<tr>
<td style="text-align: center;">MuSiQue 4-hop</td>
<td style="text-align: center;">200</td>
</tr>
</tbody>
</table>
<p>Table 10: Hop-wise distribution of questions used for evaluation in Section 6 for MuSiQue</p>
<p>Table 9 provides details on the total number of training and development set samples in each dataset. From these, we have randomly sampled 10, 000 questions (unlabelled or unannotated) from the training set to create our auxiliary schema. For each dataset, we randomly sampled 1000 data points from the development set for evaluation purposes (as illustrated in section 5.1). Table 10 details the hop-wise question statistics of the MuSiQue dataset (HotpotQA dataset (Yang et al., 2018) does not provide hop-wise distribution of their dataset).</p>
<h3>A. 3 Guidelines for Human Annotators</h3>
<p>In the evaluation phase of HOLMES (Table 2), human annotators played a crucial role in assessing the accuracy of the predicted answers. The process was structured as follows:</p>
<ul>
<li>
<p>Annotators were presented with a set of inputs for each question, which included:</p>
</li>
<li>
<p>The question</p>
</li>
<li>The gold answer</li>
<li>The predicted answer generated by one of the MHQA methods</li>
<li>The primary task for the annotators was to determine the correctness of the predicted answer in comparison to the gold answer. This evaluation was binary, with annotators assigning:</li>
<li>A score of $\mathbf{0}$ if the predicted answer was deemed incorrect</li>
<li>A score of $\mathbf{1}$ if it was considered correct</li>
<li>The evaluation process encompassed all methods: Base with supporting documents, StructQA, and HOLMES, each evaluated with a total of 100 questions. Therefore:</li>
<li>Each annotator was responsible for evaluating a comprehensive total of $\mathbf{3 0 0}$ questions, covering all possible combinations of questions and methods.</li>
</ul>
<p>The numbers in Table 2 represent the average scores from three human annotators.</p>
<h2>A. 4 Prompts used in HOLMES</h2>
<p>In this section we detail the prompts used for solving different subtasks using LLMs.</p>
<h2>A.4.1 Entity Extraction from supporting documents</h2>
<p>For extracting named entities from the supporting documents, we use the following prompt. Few-shot examples used to improve the performance of the LLM is also included in the prompt.</p>
<p>Task: Extract ALL the named entities from the given sentence (extract all time intervals, names, dates, organizations and locations).</p>
<p>Examples: Use the following examples to understand the task better. Sentence: William Rast is an American clothing line founded by Justin Timberlake and Trace Ayala. Entities: William Rast, American, Justin Timberlake, Trace Ayala</p>
<p>Sentence: The Glennwanis Hotel is a historic hotel in Glennville, Georgia, Tattnall County, Georgia, built on the site of the Hughes Hotel.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Entities: Glennwanis Hotel,</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Glennville, Georgia, Tattnall</td>
</tr>
<tr>
<td style="text-align: left;">County, Georgia, Hughes Hotel</td>
</tr>
</tbody>
</table>
<h2>A.4.2 Entity and Relation Extraction from the Question</h2>
<p>Instead of separately extracting the entity and the relations from the inference query, we use a single LLM call. Below we detail the prompt along with the few-shot examples used.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Task</span><span class="o">:</span><span class="w"> </span><span class="n">extract</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">named</span><span class="w"> </span><span class="n">entities</span>
<span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">above</span><span class="w"> </span><span class="n">question</span><span class="o">.</span><span class="w"> </span><span class="n">Then</span>
<span class="n">extract</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">important</span>
<span class="n">information</span><span class="w"> </span><span class="o">(</span><span class="k">each</span><span class="w"> </span><span class="n">information</span>
<span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="mi">2</span><span class="o">-</span><span class="mi">3</span><span class="w"> </span><span class="n">words</span><span class="o">)</span><span class="w"> </span><span class="n">needed</span><span class="w"> </span><span class="n">to</span>
<span class="n">answer</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">question</span><span class="o">.</span><span class="w"> </span><span class="n">Output</span><span class="w"> </span><span class="n">format</span>
<span class="o">-</span><span class="w"> </span><span class="s2">&quot;entities: [ent1, ent2, ...]</span>
<span class="s2">important relations: [info1, info2,</span>
<span class="s2">...]&quot;</span><span class="o">.</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="n">note</span><span class="o">,</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">give</span>
<span class="n">named</span><span class="w"> </span><span class="n">entities</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="s1">&#39;important</span>
<span class="s1">relations&#39;</span>
<span class="n">Use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">examples</span><span class="w"> </span><span class="n">to</span>
<span class="n">understand</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="o">:</span>
</code></pre></div>

<p>Question: Who is the author of the book that inspired the movie starring Tom Hanks as a symbologist? Entities: [Tom Hanks]
Important Relations: [author of, inspired the movie, stars,as a symbologist]</p>
<p>Question: Did the company that Elon Musk co-founded in 2002 eventually merge with a firm that had been contracted by NASA to resupply the International Space Station?
Entities: ['Elon Musk', '2002', 'NASA', 'International Space Station'] Important Relations: ['co-founded', 'merge with', 'contracted by', 'resupply']</p>
<h2>A.4.3 Subject and Object Entity type Estimation</h2>
<p>As discussed in Section 4.2.2 and shown in Figure 3, to create the knowledge schema from the query, we need to estimate the subject and object entity types corresponding to a given relation. We use an LLM for the same and report the prompt below.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Task</span><span class="o">:</span><span class="w"> </span><span class="n">Generate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">subject</span><span class="w"> </span><span class="n">and</span>
<span class="n">object</span><span class="w"> </span><span class="n">entity</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">following</span>
<span class="n">relation</span><span class="o">,</span><span class="w"> </span><span class="n">w</span><span class="o">.</span><span class="na">r</span><span class="o">.</span><span class="na">t</span><span class="o">.</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">provided</span>
<span class="n">context</span><span class="w"> </span><span class="n">information</span><span class="o">.</span>
<span class="n">relation</span><span class="o">:</span><span class="w"> </span><span class="n">input</span>
<span class="n">Context</span><span class="o">:</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="n">DO</span><span class="w"> </span><span class="n">NOT</span><span class="w"> </span><span class="n">extract</span>
<span class="n">partial</span><span class="w"> </span><span class="n">info</span><span class="o">.</span>
<span class="n">Output</span><span class="w"> </span><span class="n">format</span><span class="o">:</span><span class="w"> </span><span class="o">[&lt;</span><span class="n">subject</span><span class="w"> </span><span class="n">entity</span>
<span class="n">type</span><span class="o">&gt;,</span><span class="w"> </span><span class="o">&lt;</span><span class="n">object</span><span class="w"> </span><span class="n">entity</span><span class="w"> </span><span class="n">type</span><span class="o">&gt;]</span>
</code></pre></div>

<h2>A.4.4 Triples Extraction from supporting documents</h2>
<p>Here we report the prompt along with the few-shot examples we used for knowledge triple extraction from raw text.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Task</span><span class="o">:</span><span class="w"> </span><span class="n">Comprehensively</span><span class="w"> </span><span class="n">extract</span><span class="w"> </span><span class="n">ALL</span>
<span class="n">the</span><span class="w"> </span><span class="n">triples</span><span class="w"> </span><span class="o">(</span><span class="n">subject</span><span class="o">,</span><span class="w"> </span><span class="n">relation</span><span class="o">,</span>
<span class="n">object</span><span class="o">)</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">below</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">paragraph</span><span class="o">.</span>
<span class="n">Ensure</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">subject</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">objects</span>
<span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">triples</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">named</span><span class="w"> </span><span class="n">entities</span>
<span class="o">(</span><span class="n">name</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">person</span><span class="o">,</span><span class="w"> </span><span class="n">organization</span><span class="o">,</span>
<span class="n">dates</span><span class="w"> </span><span class="n">etc</span><span class="o">)</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="k">in</span>
<span class="n">number</span><span class="o">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">HEAVILY</span>
<span class="n">PENALIZED</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">violate</span><span class="w"> </span><span class="k">this</span>
<span class="n">constraint</span><span class="o">.</span>
</code></pre></div>

<p>Examples: Use the following examples to understand the task better. Paragraph: William Rast is an American clothing line founded by Justin Timberlake and Trace Ayala. It is most known for their premium jeans. On October 17, 2006, Justin Timberlake and Trace Ayala put on their first fashion show to launch their new William Rast clothing line. The label also produces other clothing items such as jackets and tops. The company started first as a denim line, later evolving into a men's and women's clothing line.
Triples:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">i. subject:</th>
<th style="text-align: left;">William</th>
<th style="text-align: left;">Rast,</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">relation:</td>
<td style="text-align: left;">clothing</td>
<td style="text-align: left;">line,</td>
</tr>
<tr>
<td style="text-align: left;">object: American</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">ii. subject:</td>
<td style="text-align: left;">William</td>
<td style="text-align: left;">Rast,</td>
</tr>
<tr>
<td style="text-align: left;">relation: founded by, object:</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Justin Timberlake</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">iii. subject:</td>
<td style="text-align: left;">William</td>
<td style="text-align: left;">Rast,</td>
</tr>
<tr>
<td style="text-align: left;">relation: founded by, object:</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Trace Ayala</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">iv. subject: William Rast, relation: known for, object: premium jeans</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">v. subject: William Rast, relation: launched on , object: October 17, 2006</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">vi. subject: Justin Timberlake, relation: first fashion show, object: October 17, 2006</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">vii. subject: Trace Ayala, relation: first fashion show, object: October 17, 2006</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">viii. subject: William Rast, relation: produces, object: jackets</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ix. subject: William Rast, relation: produces, object: tops</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">x. subject: William Rast, relation: started as, object: denim line</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">xi. subject: William Rast, relation: evolved into, object: men's and women's clothing line</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Paragraph: The Glennwanis Hotel is a historic hotel in Glennville, Georgia, Tattnall County, Georgia, built on the site of the Hughes Hotel. The hotel is located at 209-215 East Barnard Street. The old Hughes Hotel was built out of Georgia pine circa 1905 and burned in 1920. The Glennwanis was built in brick in 1926. The local Kiwanis club led the effort to get the replacement hotel built, and organized a Glennville Hotel Company with directors being local business leaders. The wife of a local doctor won a naming contest with the name "Glennwanis Hotel", a suggestion combining "Glennville" and "Kiwanis <br> Triples:</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">i. subject: Glennwanis Hotel, relation: is located in, object: 209-215 East Barnard Street, Glennville, Tattnall County, Georgia</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<div class="codehilite"><pre><span></span><code>ii. subject: Glennwanis Hotel,
        relation: was built on the site
        of, object: Hughes Hotel
iii. subject: Hughes Hotel,
        relation: was built out
        of, object: Georgia pine
iv. subject: Hughes Hotel,
        relation: was built circa,
        object: 1905
v. subject: Hughes Hotel,
        relation: burned in, object:
        1920
vi. subject: Glennwanis Hotel,
        relation: was re-built in,
        object: 1926
vii. subject: Glennwanis Hotel,
        relation: was re-built using,
        object: brick
viii. subject: Kiwanis club,
        relation: led the effort
        to re-build, object: Glennwanis
        Hotel
ix. subject: Kiwanis club,
        relation: organized, object:
        Glennville Hotel Company
x. subject: Glennville Hotel
        Company, relation: directors,
        object: local business leaders
xi. subject: Glennwanis Hotel,
        relation: combines, object:
        &quot;Glennville&quot; and &quot;Kiwanis&quot;
</code></pre></div>

<h2>A.4.5 LLM Reader</h2>
<p>Below we report the reader LLM prompt. Please note that we instruct the LLM to reason in three steps - (i) relevant fact extraction (ii) reasoning based on the relevant facts (iii) final response (with an option to request details about named entities)</p>
<h2>Question: {question}</h2>
<p>Read the above question carefully, understand the answer category. Now, given you understand the question, use the following facts to answer the question. Please note that the meaning of facts is HEAVILY dependent on the context or the document from which it was extracted. prunned_and_verbalized_hyperkg (Note: the above set of facts</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Auxilliary Graph Schema Construction Overview
can be noisy, so if ambiguous information is present then focus on the question and keywords in the question - relations_in_question) First, fetch the relevant set of facts from the above taking into consideration the context of the fact (DO NOT generate your facts), then by combining them answer the question - question.
Instruction: Give the answer in the following format -
Relevant facts: <facts (and context of those facts) relevant to the question>
Reasoning: <Here, give your thought process of how you use the named entities named_entities_in_question, relations - relations_in_question of the question and accordingly traverse the facts given above to arrive at your answer>
Final Response: <If all the necessary facts are available to answer the question, express it in 3-4 words - Else, say None>
Further query: <if 'Final Response' is None, then think step-by-step using the 'Reasoning' and other facts and state which named entity's direct information is missing></p>
<h2>A. 5 Auxilliary Schema Construction</h2>
<p>As stated in Section 4.2.2, we use a collection of indomain questions to construct a graph schema (See definition in Section 3), that captures the global level blueprint of information required to answer questions in the target domain. In this section, we detail the procedure used to create the same. We
employ a four-step process, starting from the unannotated in-domain multi-hop questions.</p>
<h2>A.5.1 Question Decomposition</h2>
<p>Given the complexity of multi-hop questions and the challenges in processing their embeddings, we first decompose these into simpler, single-hop questions. This decomposition is facilitated by the llama-13b LLM, which effectively breaks down complex questions into their constituent single-hop components. Decomposing 10,000 questions using SoTA LLMs is very expensive, thus we opted for a smaller, open-weight LLM. We use few-shot prompting to ensure the model understands the task well. We use the following prompt for the same:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Task</span><span class="o">:</span><span class="w"> </span><span class="n">Decompose</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">multi</span><span class="o">-</span><span class="n">hop</span>
</code></pre></div>

<p>question into a series of single-hop questions to assist in finding the answer in a step-by-step manner. Instruction:
i. Proceed with the decomposition according to the outlined chain of thought.
ii. Only return the sub-questions, nothing else.
iii. Focus on the entities in the question
iv. If decomposition is not clear, do not guess.</p>
<p>Question: Lily's Driftwood Bay premiered on what British television channel that is operated by a joint venture between Viacom International Media Networks Europe and Sky plc?
Chain of Thought Instructions:</p>
<ol>
<li>Identify the ultimate goal of the question: Find the</li>
</ol>
<p>television channel on which Lily's Driftwood Bay premiered.
2. Recognize that there are two pieces of information needed: a) Determine the British television channel operated by the specific joint venture. b) Find out if Lily's Driftwood Bay premiered on this channel.
3. Create single-hop sub-questions that will answer these pieces of information separately.</p>
<h2>Output:</h2>
<ol>
<li>Which British television channel is operated by a joint venture between Viacom International Media Networks Europe and Sky plc?</li>
<li>Did Lily's Driftwood Bay premiere on this specific British television channel?</li>
</ol>
<h2>A.5.2 Cluster Question Embeddings</h2>
<p>We then apply K-Means clustering to the embeddings of the decomposed single-hop questions to group similar questions. We use OpenAI embedding model (text-embedding-ada-002) to compute text embeddings. The optimal number of clusters $(k)$ is determined by evaluating the clustering quality through a combined clustering evaluation score (CCES) which is a weighted sum of three normalized scores (normalized to lie between 0 and 1): silhouette score ( $S_{\text {norm }}$ ), inverted DaviesBouldin score ( $D B_{\text {norm }}$ ), and Calinski-Harabasz score $\left(C H_{\text {norm }}\right)$, with equal weightage assigned to each. These scores collectively assess the cohesion, separation, compactness, distinctness, and dispersion of the clusters. We use the elbow method for determining the optimal number of clusters. We report the elbow plot below.
Below, we report a couple of cluster centroids with a few data points (questions) around the centroid.</p>
<h2>Cluster 1</h2>
<ul>
<li>Which football team did Kevin John Ufuoma Akpoguma play for?</li>
<li>Who is the former Houston Texans head coach?</li>
<li>Which NFL team currently employs him?</li>
</ul>
<p>Cluster 2
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Elbow Plot for Kmeans clustering on 10000 questions from HotpotQA dataset.</p>
<ul>
<li>What is the name of the actress?</li>
<li>Who are Rohan Bopanna and Cara Black?</li>
<li>What is the name of the protagonist in the show "Pretty Little Liars"?</li>
</ul>
<p>As can be seen from the above clusters, they capture latent topics in the dataset such as sports (cluster 1) and movies (cluster 2).</p>
<h2>A.5.3 Latent Question Category Modelling</h2>
<p>The next step involves latent topic modeling to identify question categories within the single-hop question clusters. By extracting the cluster centroid question and the five questions closest to each centroid, we prompt an LLM (gpt-4-1106-preview in this case) to discern latent topics, focusing on entity types and relations rather than specific entities. The prompt we use deducing the latent question category is as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Task</span><span class="o">:</span><span class="w"> </span><span class="n">Extract</span><span class="w"> </span><span class="n">latent</span><span class="w"> </span><span class="n">topics</span><span class="w"> </span><span class="k">with</span>
<span class="n">focus</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">entity</span><span class="w"> </span><span class="n">types</span><span class="w"> </span><span class="o">(</span><span class="n">not</span><span class="w"> </span><span class="n">the</span>
<span class="n">entities</span><span class="w"> </span><span class="n">themselves</span><span class="o">)</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">relations</span>
<span class="k">in</span><span class="w"> </span><span class="n">below</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">questions</span><span class="o">.</span><span class="w"> </span><span class="n">These</span>
<span class="n">topics</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">underlying</span>
<span class="n">question</span><span class="w"> </span><span class="n">categories</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">these</span>
<span class="n">questions</span><span class="o">.</span>
<span class="n">Instructions</span><span class="o">:</span>
</code></pre></div>

<p>i. Keep in mind that I will later use these latent topics to create a KG schema such that the KG can answer questions such as the ones listed below.
ii. Answer in 1 sentence using detailed and informative words only.</p>
<p>iii. Consider cross-question information too for determining the latent topics.
iv. Directly give the latent topics without preamble text like "the latent topic is ...", and use bullets to separate topics.</p>
<h3>1.5.4 Schema Induction</h3>
<p>Finally, we prompt the LLM to generate a graph schema based on the broad question categories identified in the previous step. This schema is explicitly designed as an information organization blueprint relevant to the identified question categories, thereby facilitating a structured approach to answering questions within the domain. We use the following prompt for the same -</p>
<div class="codehilite"><pre><span></span><code><span class="n">Task</span><span class="o">:</span><span class="w"> </span><span class="n">Create</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">schema</span><span class="w"> </span><span class="k">for</span>
<span class="n">my</span><span class="w"> </span><span class="n">KG</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">broad</span><span class="w"> </span><span class="n">question</span>
<span class="n">categories</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">want</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">be</span>
<span class="n">answered</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">KG</span><span class="o">.</span>
<span class="n">The</span><span class="w"> </span><span class="n">broad</span><span class="w"> </span><span class="n">question</span><span class="w"> </span><span class="n">categories</span><span class="w"> </span><span class="n">are</span><span class="o">:</span>
<span class="o">{</span><span class="n">question</span><span class="w"> </span><span class="n">categories</span><span class="o">}</span>
<span class="n">Instructions</span><span class="o">:</span>
</code></pre></div>

<p>i. Create a set of relations for a knowledge graph that clearly and unambiguously express the relationships between entities ensuring they are reusable, standardized, and semantically meaningful.
ii. Generate a list of distinct, relevant, and comprehensive entities for a knowledge graph about wikipedia text, ensuring they are specific, meaningful, and cover all aspects of the domain.
iii. After extracting the entities and relation types, for the graph schema of the knowledge graph return the all the triplets "entity type Relation Type - Entity Type" in the output as a list
iv. Important: ensure that the triplets generated forms a connected graph
v. Important: - nothing else other than the list of triplets should be returned. Format for the list: [("Entity type", "relation", "Entity type"), ("Entity type", "relation", "Entity type"),...]</p>
<h2>A. 6 Case Study of HOLMES on HotpotQA dataset</h2>
<p>In this section, we showcase our constructed hyperrelational KG extracted from the supporting documents. We illustrate the corresponding reasoning process, highlighting relevant facts, and ultimately derive the answer for the question.
Now, consider the following example question from the HotpotQA dataset.
Question: 'What major truck road is located in Backford Cross?'</p>
<p>Based on the question, our method discovers related supporting documents and then fetches structured information from them (Section 4.2.1). Below we list those discovered supporting documents.</p>
<h2>Supporting Document: 1</h2>
<p>Title: Backford Cross
Passage: Backford Cross is a village on the Wirral Peninsula, Cheshire, England. It is a suburb of the town of Ellesmere Port and part of Cheshire West and Chester. Backford Cross is located around the A41/A5117 junction, south of Great Sutton and about 1.5 mi north of the village of Backford, near Chester. Backford Cross is largely made up of residential homes built from 1990 onwards and serves as a commuter village to Ellesmere Port and Chester, although inhabitants show no allegiance to either locality. The area is split between postcode districts, with parts of the village in Great Sutton, Ellesmere Port CH66 and other areas in Backford, Chester CH1.</p>
<h2>Supporting Document: 2</h2>
<p>Title: A5117 road
Passage: The A5117 is a road in Cheshire, England. It runs between Shotwick ( ) and Helsby ( ) and connects the A550 at Woodbank to the M56. As such it forms a northerly bypass to Chester and</p>
<p>a shorter route between the North West and North Wales than the A55. The road is dualled west of the M56. There is roundabout with the A540 and at Dunkirk at the western terminus of the M56. East of the junction the road is single carriageway and crosses the A41 by way of a roundabout at Backford Cross. The A5117 intersects the M53 at Junction 10. This junction is just east of Cheshire Oaks Designer Outlet. The road then continues almost parallel to the M56, which it intersects at Junction 14, at which there is a Motorway service area. The road then continues south east to terminate where it joins the A56 at Helsby.</p>
<h2>Supporting Document: 3</h2>
<p>Title: Strawberry Park, Cheshire
Passage: Strawberry Park and Strawberry Fields are suburbs in the town of Ellesmere Port, Cheshire West and Chester. They are located to the south of Hope Farm and to the west of Backford Cross.</p>
<h2>Supporting Document: 4</h2>
<p>Title: A41 road
Passage: The A41 is a major trunk road in England that links London and Birkenhead, although it has now in parts been superseded by motorways. It passes through or near various towns and cities including Watford, Kings Langley, Hemel Hempstead, Aylesbury, Solihull, Birmingham, West Bromwich, Wolverhampton, Newport, Whitchurch, Chester and Ellesmere Port.</p>
<p>From these supporting documents, we create the Hyper-relational KG and further refine this graph to retain query relevant facts (Section 4.2.2). Below we report the same (blue colored text refers to the additional attributes of the hyper triple, red colored text refers to the subject entity, green colored text refers to the relation, and purple colored text refers to the object entity).</p>
<h2>Distilled Hyper-Relational KG:</h2>
<ul>
<li>context: 'Strawberry Park, Cheshire' subject: 'Strawberry Park' relation: 'is a suburb in' object: 'Ellesmere Port, Cheshire West'</li>
<li>context: 'Strawberry Park, Cheshire' subject: 'Strawberry Fields' relation: 'is a suburb in' object: 'Ellesmere Port, Cheshire West'</li>
<li>context: 'Strawberry Park, Cheshire' subject: 'Ellesmere Port'
relation: 'located in' object: 'Cheshire West and Chester'</li>
<li>context: 'Strawberry Park, Cheshire' subject: 'Strawberry Park' relation: 'located to the south of' object: 'Hope Farm'</li>
<li>context: 'Backford Cross' subject: 'Backford Cross' relation: 'is located in' object: 'Wirral Peninsula, Cheshire, England'</li>
<li>context: 'A5117 road' subject: 'A5117' relation: 'connects' object: 'A550 at Woodbank to M56'</li>
<li>context: 'A5117 road' subject: 'A5117' relation: 'forms a bypass to' object: 'Chester'</li>
<li>context: 'A5117 road' subject: 'A5117' relation: 'forms a shorter route between' object: 'North West and North Wales'</li>
<li>context: 'A5117 road' subject: 'A5117' relation: 'is dualled west of' object: 'M56'</li>
<li>context: 'A5117 road' subject: 'A5117' relation: 'has a roundabout with' object: 'A540'</li>
<li>context: 'A5117 road' subject: 'A5117' relation: 'has a roundabout at' object: 'Dunkirk'</li>
<li>context: 'A5117 road' subject: 'A5117' relation: 'intersects' object: 'M53 at Junction 10'</li>
<li>context: 'A5117 road' subject: 'M53' relation: 'is intersected by' object: 'A5117 at Junction 10'</li>
<li>
<p>context: 'A41 road' subject: 'A41' relation: 'is a major trunk road in' object: 'England'</p>
</li>
<li>
<p>context: 'A41 road'
subject: 'A41'
relation: 'links'
object: 'London and Birkenhead'</p>
</li>
<li>context: 'A41 road'
subject: 'A41'
relation: 'superseded by'
object: 'motorways'</li>
<li>context: 'A41 road'
subject: 'A41'
relation: 'passes through'
object: 'Watford'</li>
<li>context: 'A41 road'
subject: 'A41'
relation: 'passes near'
object: 'Kings Langley'</li>
<li>context: 'A41 road'
subject: 'A41'
relation: 'passes near'
object: 'Hemel Hempstead'</li>
<li>context: 'A41 road'
subject: 'A41'
relation: 'passes near'
object: 'Aylesbury'</li>
<li>context: 'A41 road'
subject: 'A41'
relation: 'passes near'
object: 'Solihull'</li>
<li>context: 'A41 road'
subject: 'A41'
relation: 'passes through'
object: 'Birmingham'</li>
<li>context: 'A41 road'
subject: 'A41'
relation: 'passes near'
object: 'West Bromwich'</li>
<li>context: 'A41 road'
subject: 'A41'
relation: 'passes near'
object: 'Wolverhampton'</li>
<li>context: 'A41 road'
subject: 'A41'
relation: 'passes through'
object: 'Newport'</li>
</ul>
<p>This distilled graph (after verbalization and proper formatting (Section 4.2.3)) is provided as input, along with the question, to the reader LLM. For this example, we use the SoTA, gpt-4-1106-preview as the reader LLM. Below, we describe the LLMs response which comprises of three things:</p>
<ul>
<li>Relevant facts set of hyper triples which the LLM thought were important for the answering the question. These are verbalized hyper triples as the input to the LLM was also verbalized hyper triples.</li>
<li>Reasoning: LLMs thought process of going from the distilled hyper-relational KG to the final answer</li>
<li>Final answer: answer predicted by the LLM.</li>
</ul>
<h2>Our Method's Response</h2>
<p>Relevant facts:</p>
<ul>
<li>Backford Cross is located around A41/A5117 junction (This fact was extracted from document named "Backford Cross ")</li>
<li>A41 is a major trunk road in England (This fact was extracted from document named "A41 road ")</li>
<li>A5117 is a road in Cheshire, England (This fact was extracted from document named "A5117 road ")</li>
</ul>
<h2>Reasoning:</h2>
<p>The question asks for a major truck road located in Backford Cross. The fact that Backford Cross is located around the A41/A5117 junction indicates that these are the major roads in that area. The A41 is specifically described as a major trunk road, which fits the description of a "major truck road." The A5117 is also mentioned as a road in Cheshire, but it is not specified as a major trunk road. Therefore, based on the information provided, the A41 is the major truck road located in Backford Cross.</p>
<p>Final Answer:
A41</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding author: pranoy.panda@fujitsu.com&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
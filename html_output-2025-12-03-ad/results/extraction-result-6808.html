<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6808 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6808</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6808</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-268536850</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.13312v1.pdf" target="_blank">LeanReasoner: Boosting Complex Logical Reasoning with Lean</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) often struggle with complex logical reasoning due to logical inconsistencies and the inherent difficulty ofsuch reasoning. We use Lean, a theorem proving framework, to address these challenges. By formalizing logical reasoning problems intotheorems within Lean, we can solve them by proving or disproving the corresponding theorems. This method reduces the risk of logical inconsistencies with the help of Lean’s symbolic solver. It also enhances our ability to treat complex reasoning tasks using Lean’s extensive library of theorem proofs. Our method achieves state-of-the-art performance on the FOLIO dataset and achieves performance near this level on ProofWriter. Notably, these results were accomplished by fine-tuning on fewer than 100 in-domain samples for each dataset</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6808.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6808.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LeanReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LeanReasoner: Boosting Complex Logical Reasoning with Lean</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses LLMs to formalize natural-language logical contexts into Lean theorems, generates tactics via a retrieval-augmented tactic generator, and performs proof search inside the Lean theorem prover to answer True/False/Unknown questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LeanReasoner (fine-tuned Byte-T5/Re-Prover style model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pipeline combining LLM-based formalization (GPT-4 or text-davinci-003), a retrieval-augmented tactic generator (Re-Prover style), and proof search interacting with the Lean theorem prover; the tactic-generator model is implemented as a seq2seq model (Byte-T5 family) with a DPR-based retriever for premise selection and pretrained on mathlib for some variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>retrieval-augmented seq2seq transformer for tactic generation + formalization LLM + Lean theorem prover integration</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretraining on mathlib theorem-proving data; fine-tuned on 100 ProofWriter annotated theorem proofs and 27 FOLIO annotated theorem proofs (two annotation styles: Intuitive and Concise).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Neuro-symbolic formal proof generation: (1) formalize NL to Lean axioms/theorems, (2) retrieve premises (DPR), (3) generate tactics (seq2seq LLM), (4) guided proof search in Lean, (5) interpret provability as True/False/Unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Lean theorem prover used as the strict symbolic checker and execution environment; proof search performed programmatically (via LeanDoJo) with leansmt used to compare Lean outcomes to Z3-style SAT/SMT results in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter (depth-5 OWA subset) and FOLIO (full test set)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>ProofWriter: deductive logical reasoning dataset (natural-language rules), Open-World Assumption, depth-5 subset used (600 examples for fair comparison). FOLIO: first-order logic reasoning with richer natural language and FOL constructs (204 examples in test set used).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-choice logical deduction mapped to theorem proving (first-order theorem proving / proof generation / entailment producing True/False/Unknown)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (final answer correctness, based on provability) and proof validation (valid Lean proofs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ProofWriter: 98.3% accuracy (fine-tuned on Intuitive or Concise, with pretraining on mathlib); 95.8% without pretraining. FOLIO: 82.6% accuracy (fine-tuned on Concise), 78.4% (Intuitive), 66.2% without pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>On ProofWriter, LeanReasoner (fine-tuned on 100 samples) achieves near-perfect accuracy (98.3%) using far less in-domain training data than most baselines; on FOLIO, LeanReasoner (Concise) outperforms Logic-LM (74.5%), GPT-4 CoT (70.6%), and Lean Z3 / SATLM (77.5%), achieving state-of-the-art (82.6%) with only 27 fine-tuning samples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining on mathematical theorem-proving data (mathlib) significantly improves tactic-generation and overall proof accuracy; small amounts of targeted fine-tuning (<< full dataset) yield near-SOTA on ProofWriter and SOTA-level results on FOLIO; concise (mathlib-style) annotations help premise selection and search efficiency on harder FOL problems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Depends critically on accurate formalization; formalization errors and proof-search timeouts/loops occur; struggles with commonsense/factual reasoning tasks (MMLU, SummEdits), numeric-answer math word problems, very complex formalizations (TheoremQA), and constraint-style problems where CSP solvers are more appropriate; search bottlenecks on longer FOLIO proofs and sensitivity to annotation style.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeanReasoner: Boosting Complex Logical Reasoning with Lean', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6808.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6808.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (formalizer / baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (used for formalization and prompting baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained generative transformer LLM used for formalizing natural language to Lean and as a prompting-based proof generator/baseline (including chain-of-thought prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based generative model from OpenAI, used here with tailored prompts (system prompts & examples) for formalization and as a step-by-step proof baseline (including CoT prompting variants).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer autoregressive LLM; chain-of-thought prompting used in some baselines</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting-based formalization and chain-of-thought style proof generation; separate or combined prompts for formalize+prove; used both with line-by-line comments and separated formalization/proof prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter and FOLIO (prompting-based baselines and formalization accuracy checks)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same datasets as LeanReasoner; GPT-4 evaluated for quality of formalization, proof generation success in Lean, and final answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formalization to Lean, proof generation (prompting), multi-choice logical deduction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>formalization accuracy, proof validation accuracy, final answer accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ProofWriter (100 samples manually checked): formalization 94% (Base), 99% (Base+Comments), proof success 15% (Base and Base+Comments), final answer accuracy 80%; FOLIO (40 samples): formalization 60% (Base), 75% (Base+Comments), proof success 10-15%, answer accuracy 35% across variants. Separate prompting lowered proof success further (e.g., 5% prove for Base Separate on ProofWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Despite low proof validation rates (15% or lower), GPT-4 sometimes achieves high final-answer accuracy on ProofWriter (80%), possibly due to memorization or dataset exposure; prompting-only approaches underperform the LeanReasoner pipeline in provable-proof accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs like GPT-4 can produce high-quality formalizations (especially with comments) but struggle to generate fully verifiable Lean proofs in a prompting-only setup; final answer correctness can be decoupled from provable proofs (high answer accuracy despite low proof validation).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Low proof-generation accuracy when required to produce valid Lean proofs; sensitive to prompt design; prone to hallucinated or syntactically incorrect formalizations for complex FOL contexts; performance on formalization and proof varies strongly with problem complexity and prompting style.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeanReasoner: Boosting Complex Logical Reasoning with Lean', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6808.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6808.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI text-davinci-003 (GPT-3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 (text-davinci-003) was used as an alternative formalizer with the same prompting approach as Logic-LM; it performs worse than GPT-4 especially on complex formalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An earlier-generation autoregressive transformer LLM from OpenAI used here for formalization prompts mirroring Logic-LM's setup.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer autoregressive LLM</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompt-based formalization to Lean (separate task specification and problem prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter and FOLIO (formalization evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same datasets; formalization quality manually inspected on sample subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formalization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>formalization accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ProofWriter formalization accuracy ~77% (on sampled validation), proof success ~12%, final answer ~63%; FOLIO formalization ~45%, proof success ~10%, final answer ~35% (from manual checks reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Performs substantially worse than GPT-4 on complex formalizations and downstream proof tasks; CodeLlama performed worse still (<30% on ProofWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-3 level models are less reliable for formalizing complex logical contexts into Lean; prompting improvements (comments/separate prompts) help but do not close the gap to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Struggles with intricate logic and complex problems; more prone to syntactic/semantic formalization errors than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeanReasoner: Boosting Complex Logical Reasoning with Lean', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6808.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6808.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Re-Prover</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Re-Prover (retriever + prover model for tactic generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented tactic generator that splits premise selection (DPR retriever) and tactic generation (seq2seq generator); used here as the tactic generator model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Re-Prover (retriever + seq2seq tactic generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage model: Dense Passage Retriever (DPR) for premise selection (embeddings + cosine retrieval) and a sequence-to-sequence generator (Byte-T5 family) that takes goal+retrieved premises to generate Lean tactics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>retrieval-augmented neural tactic generation (DPR + seq2seq transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretraining on theorem-proving corpora (mathlib) for some variants; fine-tuned on annotated Lean proofs drawn from ProofWriter and FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Retrieve relevant premises for the current goal, then autoregressively generate Lean tactics to manipulate goals in proof search.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Integrates with Lean execution environment to run produced tactics and observe resulting goals/states; retrieval uses DPR embeddings of candidate premises.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Evaluated on proof search tasks for ProofWriter and FOLIO (recall@k for premise selection and proof accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Retrieval accuracy measured with recall@1 and recall@4; proof accuracy measured by succeeding in Lean proof search and final answer correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>premise retrieval, tactic generation for formal proofs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>recall@1, recall@4 for premise selection; proof accuracy (successful valid Lean proofs / final answer accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Fine-tuning increased recall@1/4 and overall proof accuracy; exact numbers reported in paper Table 2 (e.g., improvements relative to non-pretrained variants); concise annotations improved premise selection. (Paper reports significant gains after pretraining on mathlib but does not give a single scalar here for every configuration.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Pretraining on math theorem proving data outperforms the same architecture without such pretraining; Re-Prover baseline (without pretraining) had near-negligible proof accuracy on the broader tactic generation space prior to fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Separating premise retrieval from tactic generation reduces complexity and improves troubleshooting; pretraining on theorem-proving corpora provides valuable reasoning 'nuggets' that transfer to natural-language logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Domain mismatch between mathlib and natural-language logical reasoning can cause incorrect tactic choices (math-specific tactics like ring/linarith used inappropriately); premise selection still harder on FOLIO due to richer FOL tactics required; proof-search search-space and loops remain issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeanReasoner: Boosting Complex Logical Reasoning with Lean', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6808.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6808.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Byte-T5 / Byt5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Byte-T5 (ByT5) model family</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The Byte-T5 (ByT5) transformer family was used as the seq2seq backbone for the tactic generator model, following the ReProver model structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Byte-T5 (ByT5) seq2seq transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Token-free byte-level encoder-decoder transformer used as the generator backbone for producing Lean tactics; adopted hyperparameters and structure from the ReProver paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>seq2seq transformer (byte-level input representation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on web/corpus data as ByT5; further pretrained/fine-tuned on theorem-proving data (mathlib) and small annotated Lean proof corpora from ProofWriter/FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Generative tactic synthesis conditioned on goals and retrieved premises.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Outputs Lean tactics which are executed in the external Lean prover during proof search.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used within LeanReasoner evaluations on ProofWriter and FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See LeanReasoner entry for dataset descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>tactic generation for formal theorem proving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>proof accuracy (valid Lean proofs) and premise selection recall (when combined with DPR retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>When pretrained on mathlib and fine-tuned on small in-domain proof data, ByT5-based tactic generator achieved the reported LeanReasoner accuracies: ProofWriter ~98.3% (fine-tuned) and FOLIO up to 82.6% (concise fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms same architecture without mathlib pretraining; Re-Prover baseline (unpretrained) had much lower proof accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Byte-level seq2seq models are effective for tactic generation when combined with retrieval and theorem-proving pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Large action space for tactics makes zero/low-shot proof generation difficult; pretraining domain mismatch issues can cause use of irrelevant math tactics on NL logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeanReasoner: Boosting Complex Logical Reasoning with Lean', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6808.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6808.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that integrates LLM formalization with symbolic solvers (Prolog/Pyke) using rule-based solver techniques (forward/backward chaining); used as a baseline comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Logic-LM (baseline system from Pan et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A hybrid approach where LLMs formalize problems into solver-compatible code (Prolog / FOL) and an off-the-shelf symbolic engine (Pyke / Prolog) finds solutions using classical techniques like forward/backward chaining and custom heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>LLM formalizer + symbolic solver (Pyke/Prolog) pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Neuro-symbolic: LLM for formalization + deterministic symbolic solver for inference (forward/backward chaining)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Uses symbolic solvers like Pyke or Prolog systems rather than a theorem prover; deterministic proof search and rule application.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter and FOLIO (reported baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same datasets; Logic-LM reported and reproduced baseline scores for both ProofWriter and FOLIO.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formalization + symbolic deduction (logical entailment / multi-choice reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (final answer accuracy), proof/formalization success rates</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported accuracies in paper: ProofWriter 79.3% (Logic-LM), FOLIO 74.5% (Logic-LM). Formalization accuracy ~98% (on ProofWriter samples) and higher proof success rates in some formalizations compared to prompting-only LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Logic-LM's static solver approach is outperformed by LeanReasoner when LeanReasoner is pretrained on mathlib and fine-tuned on small in-domain data (LeanReasoner 98.3% on ProofWriter, 82.6% on FOLIO), although Logic-LM shows stronger proof-generation rates than prompting-only GPT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Symbolic solvers improve faithfulness versus purely generative LLM outputs, but solver capability limits (e.g., Pyke struggling with multi-path search and loops) can reduce final answer quality; dynamic LLM-driven tactic generation in LeanReasoner offers more flexible proof strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on heuristics and pre-defined solver strategies (forward/backward chaining); less adaptive than LLM-driven tactic generation; solver-specific limitations (e.g., Pyke loops) reduce effectiveness on complex search spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeanReasoner: Boosting Complex Logical Reasoning with Lean', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6808.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6808.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lean Z3 / leansmt (SATLM comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lean Z3 (via leansmt) / SATLM style satisfiability-aided approaches</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SAT/SMT-based baseline approach used for comparison: leansmt runs SMT solving (Z3) on formalized Lean code producing sat/unsat outcomes mapped to provability; compared to LeanReasoner and SATLM results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Lean Z3 (leansmt) / SATLM (Z3-based approach)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SMT solving pipeline that operates on formalized logic; Z3 returns 'sat'/'unsat' and is interpreted as 'found a proof'/'didn't find a proof'. SATLM refers to systems that leverage Z3 to verify formalized queries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>SMT solver integration (Z3) with formalized input</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Declarative translation to SMT formulas and satisfiability checking (satisfiable/unsatisfiable) to determine entailment.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Z3 SMT solver used via the leansmt tool to evaluate formalized Lean code; results interpreted as proof existence.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FOLIO (comparison baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>FOLIO contains first-order logic problems where SMT solving is applicable; leansmt used to get sat/unsat results comparable to SATLM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>first-order satisfiability checking / entailment detection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (final answer correctness from sat/unsat interpretation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Lean Z3 (SATLM) reported 77.5% accuracy on FOLIO (as shown in paper Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>LeanReasoner (fine-tuned Concise) achieved 82.6% on FOLIO, outperforming Lean Z3 (77.5%) in the paper's comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SMT-based approaches provide a strong, deterministic baseline for FOL-style reasoning but can be outperformed by adaptive LLM-driven tactic search when good formalization and tactic-generation are available.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>SMT solvers operate on the translated formalism and depend on the correctness and suitability of the translation; lack the flexible heuristic search of LLM-guided tactic generation and may not exploit human-written lemma libraries; leansmt/Z3 comparisons can be sensitive to the translation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LeanReasoner: Boosting Complex Logical Reasoning with Lean', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. <em>(Rating: 2)</em></li>
                <li>Satisfiability-aided language models using declarative prompting. <em>(Rating: 2)</em></li>
                <li>LeanDojo: Theorem proving with retrieval-augmented language models. <em>(Rating: 2)</em></li>
                <li>Generative language modeling for automated theorem proving. <em>(Rating: 2)</em></li>
                <li>Proof artifact cotraining for theorem proving with language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6808",
    "paper_id": "paper-268536850",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "LeanReasoner",
            "name_full": "LeanReasoner: Boosting Complex Logical Reasoning with Lean",
            "brief_description": "A framework that uses LLMs to formalize natural-language logical contexts into Lean theorems, generates tactics via a retrieval-augmented tactic generator, and performs proof search inside the Lean theorem prover to answer True/False/Unknown questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LeanReasoner (fine-tuned Byte-T5/Re-Prover style model)",
            "model_description": "A pipeline combining LLM-based formalization (GPT-4 or text-davinci-003), a retrieval-augmented tactic generator (Re-Prover style), and proof search interacting with the Lean theorem prover; the tactic-generator model is implemented as a seq2seq model (Byte-T5 family) with a DPR-based retriever for premise selection and pretrained on mathlib for some variants.",
            "model_size": null,
            "architecture_type": "retrieval-augmented seq2seq transformer for tactic generation + formalization LLM + Lean theorem prover integration",
            "training_data": "Pretraining on mathlib theorem-proving data; fine-tuned on 100 ProofWriter annotated theorem proofs and 27 FOLIO annotated theorem proofs (two annotation styles: Intuitive and Concise).",
            "reasoning_method": "Neuro-symbolic formal proof generation: (1) formalize NL to Lean axioms/theorems, (2) retrieve premises (DPR), (3) generate tactics (seq2seq LLM), (4) guided proof search in Lean, (5) interpret provability as True/False/Unknown.",
            "external_tool_used": true,
            "external_tool_description": "Lean theorem prover used as the strict symbolic checker and execution environment; proof search performed programmatically (via LeanDoJo) with leansmt used to compare Lean outcomes to Z3-style SAT/SMT results in some experiments.",
            "benchmark_name": "ProofWriter (depth-5 OWA subset) and FOLIO (full test set)",
            "benchmark_description": "ProofWriter: deductive logical reasoning dataset (natural-language rules), Open-World Assumption, depth-5 subset used (600 examples for fair comparison). FOLIO: first-order logic reasoning with richer natural language and FOL constructs (204 examples in test set used).",
            "task_type": "multi-choice logical deduction mapped to theorem proving (first-order theorem proving / proof generation / entailment producing True/False/Unknown)",
            "performance_metric": "accuracy (final answer correctness, based on provability) and proof validation (valid Lean proofs)",
            "performance_value": "ProofWriter: 98.3% accuracy (fine-tuned on Intuitive or Concise, with pretraining on mathlib); 95.8% without pretraining. FOLIO: 82.6% accuracy (fine-tuned on Concise), 78.4% (Intuitive), 66.2% without pretraining.",
            "comparison_with_baseline": "On ProofWriter, LeanReasoner (fine-tuned on 100 samples) achieves near-perfect accuracy (98.3%) using far less in-domain training data than most baselines; on FOLIO, LeanReasoner (Concise) outperforms Logic-LM (74.5%), GPT-4 CoT (70.6%), and Lean Z3 / SATLM (77.5%), achieving state-of-the-art (82.6%) with only 27 fine-tuning samples.",
            "key_findings": "Pretraining on mathematical theorem-proving data (mathlib) significantly improves tactic-generation and overall proof accuracy; small amounts of targeted fine-tuning (&lt;&lt; full dataset) yield near-SOTA on ProofWriter and SOTA-level results on FOLIO; concise (mathlib-style) annotations help premise selection and search efficiency on harder FOL problems.",
            "limitations": "Depends critically on accurate formalization; formalization errors and proof-search timeouts/loops occur; struggles with commonsense/factual reasoning tasks (MMLU, SummEdits), numeric-answer math word problems, very complex formalizations (TheoremQA), and constraint-style problems where CSP solvers are more appropriate; search bottlenecks on longer FOLIO proofs and sensitivity to annotation style.",
            "uuid": "e6808.0",
            "source_info": {
                "paper_title": "LeanReasoner: Boosting Complex Logical Reasoning with Lean",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-4 (formalizer / baseline)",
            "name_full": "GPT-4 (used for formalization and prompting baselines)",
            "brief_description": "A large pre-trained generative transformer LLM used for formalizing natural language to Lean and as a prompting-based proof generator/baseline (including chain-of-thought prompts).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large transformer-based generative model from OpenAI, used here with tailored prompts (system prompts & examples) for formalization and as a step-by-step proof baseline (including CoT prompting variants).",
            "model_size": null,
            "architecture_type": "transformer autoregressive LLM; chain-of-thought prompting used in some baselines",
            "training_data": null,
            "reasoning_method": "Prompting-based formalization and chain-of-thought style proof generation; separate or combined prompts for formalize+prove; used both with line-by-line comments and separated formalization/proof prompts.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "ProofWriter and FOLIO (prompting-based baselines and formalization accuracy checks)",
            "benchmark_description": "Same datasets as LeanReasoner; GPT-4 evaluated for quality of formalization, proof generation success in Lean, and final answer accuracy.",
            "task_type": "formalization to Lean, proof generation (prompting), multi-choice logical deduction",
            "performance_metric": "formalization accuracy, proof validation accuracy, final answer accuracy (percentage)",
            "performance_value": "ProofWriter (100 samples manually checked): formalization 94% (Base), 99% (Base+Comments), proof success 15% (Base and Base+Comments), final answer accuracy 80%; FOLIO (40 samples): formalization 60% (Base), 75% (Base+Comments), proof success 10-15%, answer accuracy 35% across variants. Separate prompting lowered proof success further (e.g., 5% prove for Base Separate on ProofWriter).",
            "comparison_with_baseline": "Despite low proof validation rates (15% or lower), GPT-4 sometimes achieves high final-answer accuracy on ProofWriter (80%), possibly due to memorization or dataset exposure; prompting-only approaches underperform the LeanReasoner pipeline in provable-proof accuracy.",
            "key_findings": "LLMs like GPT-4 can produce high-quality formalizations (especially with comments) but struggle to generate fully verifiable Lean proofs in a prompting-only setup; final answer correctness can be decoupled from provable proofs (high answer accuracy despite low proof validation).",
            "limitations": "Low proof-generation accuracy when required to produce valid Lean proofs; sensitive to prompt design; prone to hallucinated or syntactically incorrect formalizations for complex FOL contexts; performance on formalization and proof varies strongly with problem complexity and prompting style.",
            "uuid": "e6808.1",
            "source_info": {
                "paper_title": "LeanReasoner: Boosting Complex Logical Reasoning with Lean",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "text-davinci-003",
            "name_full": "OpenAI text-davinci-003 (GPT-3 family)",
            "brief_description": "GPT-3 (text-davinci-003) was used as an alternative formalizer with the same prompting approach as Logic-LM; it performs worse than GPT-4 especially on complex formalizations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (GPT-3)",
            "model_description": "An earlier-generation autoregressive transformer LLM from OpenAI used here for formalization prompts mirroring Logic-LM's setup.",
            "model_size": null,
            "architecture_type": "transformer autoregressive LLM",
            "training_data": null,
            "reasoning_method": "Prompt-based formalization to Lean (separate task specification and problem prompts)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "ProofWriter and FOLIO (formalization evaluation)",
            "benchmark_description": "Same datasets; formalization quality manually inspected on sample subsets.",
            "task_type": "formalization",
            "performance_metric": "formalization accuracy",
            "performance_value": "ProofWriter formalization accuracy ~77% (on sampled validation), proof success ~12%, final answer ~63%; FOLIO formalization ~45%, proof success ~10%, final answer ~35% (from manual checks reported).",
            "comparison_with_baseline": "Performs substantially worse than GPT-4 on complex formalizations and downstream proof tasks; CodeLlama performed worse still (&lt;30% on ProofWriter).",
            "key_findings": "GPT-3 level models are less reliable for formalizing complex logical contexts into Lean; prompting improvements (comments/separate prompts) help but do not close the gap to GPT-4.",
            "limitations": "Struggles with intricate logic and complex problems; more prone to syntactic/semantic formalization errors than GPT-4.",
            "uuid": "e6808.2",
            "source_info": {
                "paper_title": "LeanReasoner: Boosting Complex Logical Reasoning with Lean",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Re-Prover",
            "name_full": "Re-Prover (retriever + prover model for tactic generation)",
            "brief_description": "A retrieval-augmented tactic generator that splits premise selection (DPR retriever) and tactic generation (seq2seq generator); used here as the tactic generator model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Re-Prover (retriever + seq2seq tactic generator)",
            "model_description": "Two-stage model: Dense Passage Retriever (DPR) for premise selection (embeddings + cosine retrieval) and a sequence-to-sequence generator (Byte-T5 family) that takes goal+retrieved premises to generate Lean tactics.",
            "model_size": null,
            "architecture_type": "retrieval-augmented neural tactic generation (DPR + seq2seq transformer)",
            "training_data": "Pretraining on theorem-proving corpora (mathlib) for some variants; fine-tuned on annotated Lean proofs drawn from ProofWriter and FOLIO.",
            "reasoning_method": "Retrieve relevant premises for the current goal, then autoregressively generate Lean tactics to manipulate goals in proof search.",
            "external_tool_used": true,
            "external_tool_description": "Integrates with Lean execution environment to run produced tactics and observe resulting goals/states; retrieval uses DPR embeddings of candidate premises.",
            "benchmark_name": "Evaluated on proof search tasks for ProofWriter and FOLIO (recall@k for premise selection and proof accuracy)",
            "benchmark_description": "Retrieval accuracy measured with recall@1 and recall@4; proof accuracy measured by succeeding in Lean proof search and final answer correctness.",
            "task_type": "premise retrieval, tactic generation for formal proofs",
            "performance_metric": "recall@1, recall@4 for premise selection; proof accuracy (successful valid Lean proofs / final answer accuracy)",
            "performance_value": "Fine-tuning increased recall@1/4 and overall proof accuracy; exact numbers reported in paper Table 2 (e.g., improvements relative to non-pretrained variants); concise annotations improved premise selection. (Paper reports significant gains after pretraining on mathlib but does not give a single scalar here for every configuration.)",
            "comparison_with_baseline": "Pretraining on math theorem proving data outperforms the same architecture without such pretraining; Re-Prover baseline (without pretraining) had near-negligible proof accuracy on the broader tactic generation space prior to fine-tuning.",
            "key_findings": "Separating premise retrieval from tactic generation reduces complexity and improves troubleshooting; pretraining on theorem-proving corpora provides valuable reasoning 'nuggets' that transfer to natural-language logical reasoning.",
            "limitations": "Domain mismatch between mathlib and natural-language logical reasoning can cause incorrect tactic choices (math-specific tactics like ring/linarith used inappropriately); premise selection still harder on FOLIO due to richer FOL tactics required; proof-search search-space and loops remain issues.",
            "uuid": "e6808.3",
            "source_info": {
                "paper_title": "LeanReasoner: Boosting Complex Logical Reasoning with Lean",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Byte-T5 / Byt5",
            "name_full": "Byte-T5 (ByT5) model family",
            "brief_description": "The Byte-T5 (ByT5) transformer family was used as the seq2seq backbone for the tactic generator model, following the ReProver model structure.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Byte-T5 (ByT5) seq2seq transformer",
            "model_description": "Token-free byte-level encoder-decoder transformer used as the generator backbone for producing Lean tactics; adopted hyperparameters and structure from the ReProver paper.",
            "model_size": null,
            "architecture_type": "seq2seq transformer (byte-level input representation)",
            "training_data": "Pretrained on web/corpus data as ByT5; further pretrained/fine-tuned on theorem-proving data (mathlib) and small annotated Lean proof corpora from ProofWriter/FOLIO.",
            "reasoning_method": "Generative tactic synthesis conditioned on goals and retrieved premises.",
            "external_tool_used": true,
            "external_tool_description": "Outputs Lean tactics which are executed in the external Lean prover during proof search.",
            "benchmark_name": "Used within LeanReasoner evaluations on ProofWriter and FOLIO",
            "benchmark_description": "See LeanReasoner entry for dataset descriptions.",
            "task_type": "tactic generation for formal theorem proving",
            "performance_metric": "proof accuracy (valid Lean proofs) and premise selection recall (when combined with DPR retriever)",
            "performance_value": "When pretrained on mathlib and fine-tuned on small in-domain proof data, ByT5-based tactic generator achieved the reported LeanReasoner accuracies: ProofWriter ~98.3% (fine-tuned) and FOLIO up to 82.6% (concise fine-tuning).",
            "comparison_with_baseline": "Outperforms same architecture without mathlib pretraining; Re-Prover baseline (unpretrained) had much lower proof accuracy.",
            "key_findings": "Byte-level seq2seq models are effective for tactic generation when combined with retrieval and theorem-proving pretraining.",
            "limitations": "Large action space for tactics makes zero/low-shot proof generation difficult; pretraining domain mismatch issues can cause use of irrelevant math tactics on NL logical tasks.",
            "uuid": "e6808.4",
            "source_info": {
                "paper_title": "LeanReasoner: Boosting Complex Logical Reasoning with Lean",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Logic-LM",
            "name_full": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "brief_description": "Prior work that integrates LLM formalization with symbolic solvers (Prolog/Pyke) using rule-based solver techniques (forward/backward chaining); used as a baseline comparison in this paper.",
            "citation_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning.",
            "mention_or_use": "mention",
            "model_name": "Logic-LM (baseline system from Pan et al., 2023)",
            "model_description": "A hybrid approach where LLMs formalize problems into solver-compatible code (Prolog / FOL) and an off-the-shelf symbolic engine (Pyke / Prolog) finds solutions using classical techniques like forward/backward chaining and custom heuristics.",
            "model_size": null,
            "architecture_type": "LLM formalizer + symbolic solver (Pyke/Prolog) pipeline",
            "training_data": null,
            "reasoning_method": "Neuro-symbolic: LLM for formalization + deterministic symbolic solver for inference (forward/backward chaining)",
            "external_tool_used": true,
            "external_tool_description": "Uses symbolic solvers like Pyke or Prolog systems rather than a theorem prover; deterministic proof search and rule application.",
            "benchmark_name": "ProofWriter and FOLIO (reported baselines)",
            "benchmark_description": "Same datasets; Logic-LM reported and reproduced baseline scores for both ProofWriter and FOLIO.",
            "task_type": "formalization + symbolic deduction (logical entailment / multi-choice reasoning)",
            "performance_metric": "accuracy (final answer accuracy), proof/formalization success rates",
            "performance_value": "Reported accuracies in paper: ProofWriter 79.3% (Logic-LM), FOLIO 74.5% (Logic-LM). Formalization accuracy ~98% (on ProofWriter samples) and higher proof success rates in some formalizations compared to prompting-only LLMs.",
            "comparison_with_baseline": "Logic-LM's static solver approach is outperformed by LeanReasoner when LeanReasoner is pretrained on mathlib and fine-tuned on small in-domain data (LeanReasoner 98.3% on ProofWriter, 82.6% on FOLIO), although Logic-LM shows stronger proof-generation rates than prompting-only GPT baselines.",
            "key_findings": "Symbolic solvers improve faithfulness versus purely generative LLM outputs, but solver capability limits (e.g., Pyke struggling with multi-path search and loops) can reduce final answer quality; dynamic LLM-driven tactic generation in LeanReasoner offers more flexible proof strategies.",
            "limitations": "Relies on heuristics and pre-defined solver strategies (forward/backward chaining); less adaptive than LLM-driven tactic generation; solver-specific limitations (e.g., Pyke loops) reduce effectiveness on complex search spaces.",
            "uuid": "e6808.5",
            "source_info": {
                "paper_title": "LeanReasoner: Boosting Complex Logical Reasoning with Lean",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Lean Z3 / leansmt (SATLM comparison)",
            "name_full": "Lean Z3 (via leansmt) / SATLM style satisfiability-aided approaches",
            "brief_description": "A SAT/SMT-based baseline approach used for comparison: leansmt runs SMT solving (Z3) on formalized Lean code producing sat/unsat outcomes mapped to provability; compared to LeanReasoner and SATLM results.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Lean Z3 (leansmt) / SATLM (Z3-based approach)",
            "model_description": "SMT solving pipeline that operates on formalized logic; Z3 returns 'sat'/'unsat' and is interpreted as 'found a proof'/'didn't find a proof'. SATLM refers to systems that leverage Z3 to verify formalized queries.",
            "model_size": null,
            "architecture_type": "SMT solver integration (Z3) with formalized input",
            "training_data": null,
            "reasoning_method": "Declarative translation to SMT formulas and satisfiability checking (satisfiable/unsatisfiable) to determine entailment.",
            "external_tool_used": true,
            "external_tool_description": "Z3 SMT solver used via the leansmt tool to evaluate formalized Lean code; results interpreted as proof existence.",
            "benchmark_name": "FOLIO (comparison baseline)",
            "benchmark_description": "FOLIO contains first-order logic problems where SMT solving is applicable; leansmt used to get sat/unsat results comparable to SATLM.",
            "task_type": "first-order satisfiability checking / entailment detection",
            "performance_metric": "accuracy (final answer correctness from sat/unsat interpretation)",
            "performance_value": "Lean Z3 (SATLM) reported 77.5% accuracy on FOLIO (as shown in paper Table 4).",
            "comparison_with_baseline": "LeanReasoner (fine-tuned Concise) achieved 82.6% on FOLIO, outperforming Lean Z3 (77.5%) in the paper's comparison.",
            "key_findings": "SMT-based approaches provide a strong, deterministic baseline for FOL-style reasoning but can be outperformed by adaptive LLM-driven tactic search when good formalization and tactic-generation are available.",
            "limitations": "SMT solvers operate on the translated formalism and depend on the correctness and suitability of the translation; lack the flexible heuristic search of LLM-guided tactic generation and may not exploit human-written lemma libraries; leansmt/Z3 comparisons can be sensitive to the translation quality.",
            "uuid": "e6808.6",
            "source_info": {
                "paper_title": "LeanReasoner: Boosting Complex Logical Reasoning with Lean",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning.",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "Satisfiability-aided language models using declarative prompting.",
            "rating": 2,
            "sanitized_title": "satisfiabilityaided_language_models_using_declarative_prompting"
        },
        {
            "paper_title": "LeanDojo: Theorem proving with retrieval-augmented language models.",
            "rating": 2,
            "sanitized_title": "leandojo_theorem_proving_with_retrievalaugmented_language_models"
        },
        {
            "paper_title": "Generative language modeling for automated theorem proving.",
            "rating": 2,
            "sanitized_title": "generative_language_modeling_for_automated_theorem_proving"
        },
        {
            "paper_title": "Proof artifact cotraining for theorem proving with language models.",
            "rating": 1,
            "sanitized_title": "proof_artifact_cotraining_for_theorem_proving_with_language_models"
        }
    ],
    "cost": 0.01765275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LeanReasoner: Boosting Complex Logical Reasoning with Lean</p>
<p>Dongwei Jiang djiang21@jhu.edu 
Johns Hopkins University ♢ University of Edinburgh</p>
<p>Marcio Fonseca m.fonseca@ed.ac.uk 
Johns Hopkins University ♢ University of Edinburgh</p>
<p>Shay B Cohen scohen@inf.ed.ac.uk 
Johns Hopkins University ♢ University of Edinburgh</p>
<p>LeanReasoner: Boosting Complex Logical Reasoning with Lean
492E2DBFDE1C899F1B63CB4B81C53728
Large language models (LLMs) often struggle with complex logical reasoning due to logical inconsistencies and the inherent difficulty of such reasoning.We use Lean, a theorem proving framework, to address these challenges.By formalizing logical reasoning problems into theorems within Lean, we can solve them by proving or disproving the corresponding theorems.This method reduces the risk of logical inconsistencies with the help of Lean's symbolic solver.It also enhances our ability to treat complex reasoning tasks by using Lean's extensive library of theorem proofs.Our method achieves state-of-the-art performance on the FOLIO dataset and achieves performance near this level on ProofWriter.Notably, these results were accomplished by fine-tuning on fewer than 100 in-domain samples for each dataset. 1</p>
<p>Introduction</p>
<p>Logical reasoning, a bedrock of intelligence and a core capability of humans, has been a challenging issue for machine learning systems for a long time.LLMs, despite their impressive abilities to understand and generate natural language, often fall short when dealing with complex logical reasoning tasks.They frequently suffer from logical inconsistencies, where the model hallucinates and makes statements not grounded in premises, leading to spurious results (Saparov and He, 2023;Dasgupta et al., 2022).</p>
<p>Recent advances in AI have adopted a structured approach to tackle these reasoning problems by splitting them into symbolic formalization and problem-solving (He-Yueya et al., 2023;Pan et al., 2023;Ye et al., 2023).Specifically, the formalization step is often handled by a large language model, while problem-solving is handled by an offthe-shelf symbolic solver.In this approach, sym-1 Our code and data is available at https://github.com/Some-random/theorem-proving-reasoning.</p>
<p>bolic solvers essentially act as a rigorous checkpoint, ensuring that the model outputs align with logical rules, thereby mitigating the issue of logic inconsistency.In these approaches, solvers may range from being completely deterministic, like SymPy (He-Yueya et al., 2023), or relying on a combination of heuristics and basic machine learning techniques, as is the case with Pyke (Pan et al., 2023) and Z3 (Ye et al., 2023;de Moura and Bjørner, 2008).While this approach successfully addresses hallucinations, it still struggles with more complex problems.</p>
<p>As a powerful theorem prover and a versatile programming language, Lean (de Moura et al., 2015) presents a compelling solution to connect symbolic solvers with linguistic resources.Much like symbolic solvers, Lean has a strict checking system that ensures each reasoning step is certified.What distinguishes it, however, is its additional functionality as a programming language developed specifically for theorem proving.Every day, a substantial amount of code is written in Lean, capturing reasoning "nuggets" with step-by-step rationals that are useful for training LLMs.A few recent studies have already tapped into Lean for mathematical theorem-proving tasks (Polu et al., 2023;Han et al., 2022a;Lample et al., 2022), showing its potential in tackling difficult reasoning challenges.</p>
<p>In this paper, we propose LeanReasoner, a Leanbased framework to tackle logical reasoning problems.We use LLMs to formalize natural language context into Lean and fine-tune a custom model on these problems using a modest amount of data annotated ourselves.As we use LLMs to dynamically generate solutions within the Lean environment, our approach stands in stark contrast to the static, pre-defined solution-finding methods of Log-icLM (Pan et al., 2023), which only rely on traditional techniques like forward and backward chaining, and SATLM (Ye et al., 2023), which operates within the Z3 environment using a suite of prede-arXiv:2403.13312v1[cs.CL] 20 Mar 2024 termined algorithms and heuristics.The adaptive nature of LLMs as a solution-finding tool allows our system to evolve continuously, harnessing a vast array of reasoning data and information.</p>
<p>Our contributions in this paper are three-fold.</p>
<p>• To our knowledge, this is the first attempt to use Lean, traditionally associated with mathematical theorem proving, for natural language logical reasoning.This effort highlights a possible intersection between mathematical theorem proving and logical reasoning.</p>
<p>Problem Definition and Notation</p>
<p>The task we aim to solve is logical reasoning, taking the form of multi-choice questions given a natural language context.The answer to the question can be logically deduced based on the context.The framework we use for solving the problem is Lean. 2ean is an open-source theorem-proving programming language with vibrant community support.Its current base includes over 100,000 theorems and 1,000,000 lines of code. 3 We use Lean as a generic theorem prover, outside of mathematics.</p>
<p>The task and our solution to it, consist of the following components:</p>
<p>• Context, which is composed of natural language utterances, composing a set of rules and facts.For example: Hudson is a cat, all cats are animals, and cats often meow.• Question, which denotes the posed question.For example, Does Hudson often meow?• Options is a set of available answers (discrete categories) from which an answer can be chosen.For example, True, False or Unknown.</p>
<p>• Formalized context refers to the representation of context in Lean.For example, the formalized context for our example would be: axiom A1 is_cat Hudson, axiom A2 ∀x, is_cat x → is_animal x and axiom A3 ∀x, is_cat x → of-ten_meow x.</p>
<p>• Formalized question: Given that Lean operates as a theorem prover, questions are transformed into dual theorems: one asserting the positive stance and the other negating it.For the given example, the formalized questions would be: Theorem hudson_often_meows: often_meow Hudson and Theorem not_hudson_often_meows: ¬ of-ten_meow Hudson.</p>
<p>• Goal: In the context of proving theorems with Lean, a "goal" is a logical statement that needs to be proven true, given a set of axioms and rules.</p>
<p>When we set out to answer a question using the Lean prover, this question becomes our root goal.</p>
<p>At that point, we can apply various instructions in Lean to simplify or break down this primary goal and generate intermediate goals.</p>
<p>For instance, using our earlier examples, if the root goal is proving Theorem hud-son_often_meows: often_meow Hudson, an intermediate goal might be proving that Hudson is a cat.We aim to resolve each intermediate goal using our provided context, gradually working our way towards proving the root goal.Once all intermediate goals are resolved, we have effectively proven our root goal, and the proof search concludes successfully.</p>
<p>• Tactics are the instructions in the Lean theorem proving language that are used to manipulate goals to obtain a proof for a given goal.For example, apply A3 Hudson is a tactic that uses modus ponens on the Goal often_meow Hudson and transforms it to a new Goal is_cat Hudson A diagram of these components and the relations between them is depicted in Figure 1.This procedure is framed within the language of the Lean theorem prover as a goal-satisfying process.</p>
<p>LeanReasoner</p>
<p>Our framework, LeanReasoner, is composed of four main components: a formalizer, a tactic generator, a proof search mechanism, and a result interpreter.The formalizer converts the context and question to formalized context and formalized</p>
<p>Context, Question and Options</p>
<p>Context: The cow is big.The cow likes the dog.The cow visits the dog.The dog needs the cow The cow needs the cow.If something visits the dog and the dog needs the cow then it needs the cow.If the dog visits the cow then the cow visits the dog.If something needs the cow and the cow likes the dog then it likes the cow. Figure 1: An overview of our approach.The natural language context is first processed by the "formalizer".It then advances to the proof search stage, where all the tactics (in red) generated by the "tactic generator" are used to manipulate goals.Finally, the outcome is interpreted by the "result interpreter".</p>
<p>Question</p>
<p>question.The tactic generator then generates tactics based on premises extracted from the formalized context.The proof search mechanism oversees tactic execution and goal expansion.The result interpreter analyses the output of the proof search and identifies the correct answer in the options.In this section, we detail each of those components.</p>
<p>Formalizer</p>
<p>As formalizers, we used OpenAI models textdavinci-003 (GPT-3) and GPT-4 (OpenAI, 2023).</p>
<p>For text-davinci-003, we followed the same prompting approach as Logic-LM (Pan et al., 2023) to separate task specifications and problems, thereby enabling the model to continue with the task of formalization through next-token-prediction.For GPT-4, we used similar prompts but included the task specification in the system prompt.</p>
<p>There is no automatic way to assert all the entities, relationships, and constraints of the context have been captured by the formalized result.However, the syntax of the formalized result can be checked by Lean.Because correct syntax is a prerequisite for downstream theorem proving, if an error is encountered during compilation, we provide the error message generated by Lean along with the faulty formalization and ask the formalizer to regenerate the result.We further manually inspect the formalizer in §5.We note that we take a strict approach, and if the formalizer fails more than once, then the problem is counted as not being correctly solved.</p>
<p>Tactic Generator</p>
<p>The model we used for tactic generation is Re-Prover (Yang et al., 2023).This model contains two parts: a retriever that employs retrieval mechanisms to explicitly select premises when provided with the current goal, and a generator that generates tactics using the goal and the retrieved premises.</p>
<p>The division of the problem-solving task into premise selection and tactic generation simplifies the process and facilitates easier troubleshooting.</p>
<p>It isolates the source of potential issues, be it in the premise selection or the tactic generation, thus reducing the complexity of the problem.Also, this division of responsibilities eases the burden on the tactic generator.Choosing the right premise with numerous distractions is challenging, especially in logical reasoning problems when several options might seem promising for the current step but will not ultimately lead to the desired goal.</p>
<p>The premise retrieval component of our process draws from the Dense Passage Retriever (DPR) (Karpukhin et al., 2020).Provided with a goal g as the query and a set of candidate premises P , it generates a ranked list of m premises from P .In DPR, both g and P are treated as raw texts that are embedded in a vector space.We then retrieve the top m premises that maximize the cosine similarity between the goal and the premise.For tactic generation, we use a standard sequence-to-sequence model.The goal and the premises are concatenated together as a string to generate new tactics.</p>
<p>As a baseline, we also prompt GPT-4 to generate proofs.For cases when the chosen theorem to prove aligns with the answer (say the chosen theorem is the positive stance of the question and the answer is YES), we present GPT-4 with the correct proof as part of the prompt.Conversely, if the answer does not align with the chosen theorem or the answer is UNKNOWN, the formalized theorem is unprovable.In those cases, we still encourage the model to engage in step-by-step reasoning, even though it will eventually hit a roadblock.An example of the prompt to GPT-4 can be found in Appendix A.1.</p>
<p>Proof Search</p>
<p>The proof search module controls the overall search process that selects tactics and maintains states during proof construction.Essentially, the goal of the search method is to build a proof tree that incrementally evolves the goal through tactic invocations.This approach was first introduced in GPT-F (Polu and Sutskever, 2020).LeanDoJo (Yang et al., 2023), a recently released framework that enables interaction with Lean programmatically, subsequently provided an implementation of this method, which we use for our study.</p>
<p>As a reference, the middle part of Figure 1 provides a practical illustration of this process.Starting from the root goal, for each given proof goal, we explore 64 possible tactics.All goals are maintained in a priority queue and expanded based on cumulative log probabilities of the goal.The cumulative log probability is defined as the summation of the log probabilities of the tactics that brought us to the goal from the root.This implies that we tend to expand those goals where our generative model has the highest global confidence.</p>
<p>To enhance search efficiency and circumvent potential loops, we have incorporated a mechanism that stops the expansion of a node N if we have already explored another node M with a state sequence that prefixes N .Essentially, if the current goal being explored contains all the elements of a previously explored goal, then it shouldn't be further expanded.This is based on the observation that if we have already assessed the potential paths and outcomes for a specific goal, then exploring a more generalized version of the same goal is redundant.Such a mechanism avoids unnecessary repetitions, which streamlines the search process and improves the overall efficiency.Moreover, we define a valid proof as one that is devoid of "cheating" tactics (such as sorry) that tell Lean to assume that the current goal is completed, even though it has not been proven.This means that every path containing "cheating" tactics is disregarded.</p>
<p>Errors in the search process typically manifest in two ways: a timeout or an exhaustion of nodes to search.We have allocated a three-minute window for each search, which is usually sufficient.We provided more analysis of the errors made by tactic generator in the experiment section.</p>
<p>Result Intepreter</p>
<p>If the correct answer is Unknown, we only regard the result as correct if neither True nor False can be proven.All datasets investigated in this study only contain questions with only one correct answer.Consequently, if the proof system verifies more than one option, the response is immediately marked as incorrect.</p>
<p>Experimental Setup</p>
<p>We now describe our experimental setup: the datasets we used for evaluation and model training and the details of model training.</p>
<p>Evaluation Data</p>
<p>In our evaluation, we use two common logical reasoning datasets as testbeds:</p>
<p>ProofWriter: This deductive logical reasoning dataset presents problems in an intuitive language form.We incorporated the Open-World Assumption (OWA) subset as per (Pan et al., 2023), where each instance is characterized by a {problem, goal} pairing.The label for each pair contains TRUE, FALSE, or UNKNOWN.It encompasses five segments based on the required reasoning depth.Our focus is the depth-5 subset, which is the most challenging one.To get a fair comparison against Logic-LM, we used the same 600 sample tests, ensuring an even label distribution.</p>
<p>FOLIO: Unlike ProofWriter, FOLIO is constructed using first-order logic.This increases the complexity of the proving part.The dataset also presents problems in a more natural wording, with relationships that are considerably more complex.Such a combination of advanced logic and rich linguistic structure makes the formalization task in FOLIO substantially tougher than in ProofWriter.For our analysis, we turned to the entire FOLIO test set, encompassing 204 examples.</p>
<p>Training Data for Domain Adaptation</p>
<p>Regarding the data for model training, we collected 100 theorem proofs for ProofWriter and 27 theorem proofs for FOLIO, where each problem's proof was either manually annotated or collected from successful proofs generated by GPT-4.The data collection took about eight days.</p>
<p>During data annotation, we adopted two divergent approaches for constructing proofs.One approach emulated a straightforward strategy, encompassing a detailed procedure with all of the intermediate steps and lemmas, similar to how we humans might derive proof when given theorem-proving tasks.Conversely, the second approach resembles the proof formats found in mathlib. 4We generate more succinct proofs of the same problem by reducing the number of intermediate lemmas and combining multiple tactics into a single compound tactic.The objective of having two annotations for the same problem was to examine the influence of annotation style on downstream logical reasoning.In the following experiments, we use Intuitive to refer to the first annotation style and Concise to denote the second annotation style.An illustrative example is available in Appendix C.</p>
<p>It is important to mention that despite the limited data collected, the reasoning patterns for logical reasoning likely mirror those found in mathematical reasoning, which were potentially learned during pretraining.The main purpose of this data collection is domain adaptation to transfer from math to natural language logical reasoning.</p>
<p>Model Training</p>
<p>We used the same model structure for pretraining as in the ReProver paper, namely, Google's Byte-T5 (Xue et al., 2022).We also experimented with the pretrained ReProver from LeanDoJo (Yang et al., 2023), which was pretrained on mathlib.The finetuning of our collected data took about six hours on one A100 40G.The hyperparameters are the same as in the original LeanDoJo paper.</p>
<p>Results</p>
<p>We present our experimental results, including an examination of prompting-based baseline, experimental results for LeanReasoner, and a comparison between our work and other baselines.</p>
<p>Prompting-Based Baselines</p>
<p>Since there is no automated method to verify the accuracy of formalization, we conducted manual examinations of the formalized results to determine whether errors occur during formalization or proof generation stages.In this examination, only formalizations that correctly capture every fact, axiom, and rule are counted as accurate.We manually examined 100 questions from ProofWriter's validation set and 40 questions from FOLIO's training set.The findings have been summarized in Table 1.</p>
<p>Comparison of formalization accuracy.The formalization accuracy of ProofWriter is much higher than FOLIO.This can be attributed to its simpler language structure.In the case of FOLIO, although using LLM for formalization helped in filtering out unnecessary details from the natural language context, there still exists some common error patterns.We have illustrated typical GPT-4 error patterns in Appendix B using a composite sample derived from various error instances.Interestingly, Lean's formalization accuracy is on par with both Prolog and FOL in Logic-LM.This consistency underscores Lean's versatility, allowing it to uniformly represent different problem types within a single framework.</p>
<p>Adding textual comments increases formalization accuracy.We observed improved results when formalized code was paired with descriptive textual comments (example in Appendix A.1) sourced from the context.This approach further splits the formalization task into two subtasks: 1) linking textual context with formalized code and 2) generating formalized code based on the prior textual context.These textual cues acted as a bridge between raw text and formalized code, enhancing the performance of formalization.</p>
<p>GPT-3's performance on formalization is worse than GPT-4 .The distinction in performance between GPT-3 and GPT-4 is evident.While the formalization for simpler problems is the same, GPT-3 struggles with intricate logic and complex problems.As such, we opted not to use GPT-3 in further tests.Additionally, we experimented with the CodeLLAMA (Baptiste Rozière and et.al, 2023) model family for similar tasks, but found that their accuracy in formalization was significantly lower than that of GPT-3, achieving less than 30% on ProofWriter.</p>
<p>The proof accuracy of prompting-based baseline is very low.The proof accuracy section of the table is determined by whether the generated proof can be validated successfully in Lean.If the formalization of the question as a theorem is correct and the proof can be validated without any error or warning, then we can treat the proof as valid.However, the accuracy of rendered proofs is very low.The issue could stem from assigning too many tasks to the large language model, making it challenging to address both within a single prompt.Despite our efforts to separate formalization and proof, the results were still disappointing, which highlights GPT-3 and GPT-4's struggle with generating correct Lean proof.Interestingly, the proof accuracy of Logic-LM wasn't as high as we expected.Upon replicating their code, we found their chosen solver Pyke to be suboptimal, struggling to identify an answer when multiple search paths are available and some could result in loops.</p>
<p>The answer accuracy of prompting-based baseline is surprisingly high.Despite the low accuracy in most of GPT-4's proofs, it achieved high accuracy for final choices on ProofWriter (as shown in column Answer).We believe this may be due to GPT-4's training exposure to the dataset, potentially leading to a degree of memorization.</p>
<p>LeanReasoner</p>
<p>In this section, we focus on training our own models to do tactic generation using our annotated training data.To isolate the impact of erroneous formalization, we only used the accurate formalizations from the previous subsection for testing.This gave us 99 test examples for ProofWriter and 28 for FOLIO.All findings are detailed in Table 2.</p>
<p>Fine-tuning on annotated data increases premise selection accuracy.We first compare the results on premise selection using the metrics recall@1 and recall@4.The recall@k metric is defined as follows:
recall@k = |GT_Prem ∩ Pred_Prem[0 : k]| |GT_Prem| ,
where GT_Prem means ground truth premises and Pred_Prem means top predicted premises.The suboptimal results of LeanReasoner pretrained solely with math data may be attributed to the domain mismatch between mathematical theorem proving and logical reasoning.The model frequently makes mistakes by attempting to use other, unrelated tactics that are useful in mathematical theorem proving (like ring, linarith) but not applicable in logical reasoning.Furthermore, the accuracy for FO-LIO was noticeably poorer than that of ProofWriter.This disparity is likely due to FOLIO's intricate logic and its need for a broader array of first-order logic tactics such as cases, have, and contradiction.In contrast, ProofWriter primarily employs tactics like apply, exact, and split.</p>
<p>Pretraining on theorem proving data increases overall accuracy.Regarding the overall proof results, LeanReasoner pretrained on math theorem proving data consistently outperformed other approaches for both ProofWriter and FOLIO datasets.This success indicates that our model effectively uses the logical "nuggets" found in mathematical theorem proofs.While the premise selector benefits from distinct cues and a limited range of choices, the realm of tactic generation is much broader.This vastness of options renders the Re-Prover baseline's proof accuracy nearly negligible.But other than that, there is a strong correlation between premise selection accuracy and overall proof accuracy.While the benefits of a pretrained LeanReasoner may not be as noticeable for simpler datasets like ProofWriter, its value becomes evident for more complex datasets, such as FOLIO.</p>
<p>Concise annotation gives better result on premise selection.Fine-tuning with different an-  notations has a slight effect on premise selection and tactic generation in this small test set.When fine-tuned with Concise annotations, LeanReasoner would also try to generate concise proofs, which usually use compound tactics that offer more information for premise selection.However, the final proof accuracy has not changed on this small test set.Figure 2 displays an example of proofs for the same question, produced by the three primary methods we compared.In the absence of pretraining, the model struggles to identify an appropriate approach for solving the problem.It merely attempts to apply the next applicable theorem, lacking a clear objective.While Intuitive data offers numerous lemmas that assist in the thought process during proof-writing, these excessive lemmas do not aid LLMs in generating tactics effectively.</p>
<p>Other Baselines</p>
<p>Having demonstrated that pretraining on theoremproving data yields superior performance, we proceed to benchmark our results against established baselines for both ProofWriter and FOLIO.The evaluation uses the same set of 600 problems from LogicLM and the entire FOLIO test set.</p>
<p>Our approach yields near-perfect accuracy on ProofWriter with significantly less data.As illustrated in Table 3, our approach yields nearperfect accuracy on the ProofWriter dataset.While other methods except Logic-LM and GPT-4 COT use the entire training set of ProofWriter, our approach relies on just 100 examples, underscoring the efficiency of our method.Fine-tuning on Concise annotation does not bring any advantage to the final performance on this dataset.</p>
<p>Method Acc Full training set method</p>
<p>Abs Biases (Gontier et al., 2022) 80.6% MetaInduce (Yang et al., 2022) 98.6% RECKONING (Chen et al., 2023b) 99.8% Zero-shot method GPT-4 CoT (Pan et al., 2023) 68.1% Logic-LM (Pan et al., 2023) 79.3%Our method (finetuned on 100 samples) LeanReasoner without Pretraining 95.8% LeanReasoner fine-tuned on Intuitive 98.3% LeanReasoner fine-tuned on Concise 98.3%</p>
<p>Method</p>
<p>Acc Full training set method Roberta (Han et al., 2022b) 62.1% FOLNet (Chen, 2023) 70.6% Zero-shot method GPT-4 CoT (Pan et al., 2023) 70.6% Logic-LM (Pan et al., 2023) 74.5% Lean Z3 (SATLM) 77.5% Our method (finetuned on 27 samples) LeanReasoner without Pretraining 66.2% LeanReasoner fine-tuned on Intuitive 78.4% LeanReasoner fine-tuned on Concise 82.6%Our approach achieves state-of-the-arts performance on FOLIO.Table 4 presents our performance on FOLIO.For a fair comparison with SATLM that uses the Z3 solver, we used the leansmt tool 5 on our formalized Lean code.This tool produces outcomes in the form of "sat/unsat".In Z3, "sat" stands for "satisfiable."When Z3 returns "sat" as the result, it means that there exists a set of variable values that makes the theorem true.On the other hand, "unsat" stands for "unsatisfiable".When Z3 returns "unsat", it means that the formula is inherently contradictory and cannot be satisfied under any circumstance.We interpret these results similarly to "found a proof/didn't find a proof" using our result interpreter.Due to the extensive length of proofs for FOLIO problems, we observed that when LeanReasoner is fine-tuned on the Intuitive dataset, it often allocates an excessive amount of time for exploration and occasionally enters loops.In contrast, generating shorter proofs tends to ease the discovery of the proof.While the tactics generated when fine-tuned on the Concise dataset are more challenging to produce, the bottleneck for LeanReasoner on FOLIO resides in the search process.</p>
<p>Challenges in Benchmarking.</p>
<p>It is important to acknowledge that there can be scenarios where errors in problem formalization or proof generation may occur, yet the final answer is still deemed 5 https://github.com/ufmg-smite/lean-smtcorrect.A case in point is when the answer to a problem is U nknown, and errors arise in these stages.In such instances, the model would struggle to prove either the positive or negative theorem.However, with our result interpreter, these instances would still be classified as correct despite the underlying issues in problem handling.</p>
<p>Related Work</p>
<p>Combining LLM with symbolic solver.Several past studies (Chen, 2023;Creswell and Shanahan, 2022;Chen et al., 2023b) used symbolic solvers to augment neural networks with logical reasoning.Many of these approaches have limitations, like the necessity for custom or specialized module designs that lack broad applicability.Recent work (Pan et al., 2023;Ye et al., 2023;Poesia et al., 2023;Olausson et al., 2023) presents a more general framework that combines contemporary LLMs with symbolic logic, bypassing the need to train or craft intricate modules tailored for specific problems.While our research aligns with these, we do not exclusively rely on off-the-shelf solvers.</p>
<p>Boosting the reasoning skill of LLM by training on reasoning data.A common way to boost the reasoning skills of LLMs is by training them on data that requires some form of reasoning.As noted by Lewkowycz et al. (2022), LLMs trained with science and math data do better on tasks that require reasoning, especially when using CoT prompting.Other results by Fu and Khot (2022) and Fu et al. (2023) suggest that powerful LLMs obtain advanced reasoning capabilities from being trained on code.This work is an extension of this idea to theorem proving.</p>
<p>Conclusion</p>
<p>We introduced LeanReasoner, a framework based on Lean that augments the logical reasoning abilities of LLMs.We follow an extensive examination of errors from the formalization and proof generation stages that are present in our framework.We also examine the performance enhancements from pretraining on theorem-proving data and annotation styles (concise v.s.intuitive).We offered a comprehensive comparison with other techniques that highlight the strengths of our model.Our results underscore the potential of integrating theoremproving frameworks with LLMs in advancing logical reasoning.</p>
<p>Limitations</p>
<p>Despite our promising results, our method encounters limitations when dealing with problems that involve commonsense and factual reasoning.In these cases, it is challenging to retrieve all the necessary information and accurately represent it in Lean.Consider MMLU (Hendrycks et al., 2020) and SummEdits (Laban et al., 2023): MMLU requires the model to possess extensive world knowledge, while SummEdits involves determining consistency in summaries of different edits.In both instances, the ability to represent the complexity and nuance of real-world knowledge in Lean is severely limited.</p>
<p>Further complications arise when dealing with math word problems (Cobbe et al., 2021) and similar tasks (Hendrycks et al., 2021), where the goal is to derive a numeric solution rather than a proof.The theorem-proving approach, while effective for certifying the validity of logical reasoning, does not directly yield a numerical answer.Lastly, our method grapples with problems found in more complicated reasoning datasets like TheoremQA (Chen et al., 2023a).These problems require an advanced understanding of complex concepts and the ability to formalize these concepts into Lean.Our current framework struggles with this level of complexity, underscoring the need for more sophisticated formalization techniques and a deeper integration between language understanding and theorem proving.</p>
<p>Even in the context of symbolic problems, there are challenges.For instance, consider the Logi-calDeduction task from the BigBench dataset (Srivastava et al., 2022).Although this problem appears straightforward, employing Lean to solve it is neither the most practical nor the most efficient approach.Lean, as a theorem prover, is excellent in abstract reasoning and proof construction, but when faced with tasks involving constraints and variable possibilities, it falls short.To solve the problems in LogicDeduction, using Lean would require us to formalize the concepts of ordering and relative positioning.Even after doing so, generating proof would necessitate significant labor and wouldn't necessarily yield a readily interpretable answer.In contrast, a Constraint Satisfaction Problem (CSP) solver can effectively manage constraints and generate potential solutions efficiently.</p>
<p>Ethical Considerations</p>
<p>Incorporating Lean's theorem-proving capabilities into LLMs offers a layer of mathematical rigor that improves the reliability of conclusions derived.However, LLMs are known to be susceptible to data biases, which may manifest in critical applications.This issue can inadvertently lead to skewed logic or unintended bias in sensitive domains such as medical diagnoses or legal interpretations.While our method's foundation in Lean's theorem proving data acts as a rigorous check, complete reliance on it is not foolproof.A proactive approach in reviewing both training data and model outcomes is essential to uphold unbiased reasoning.Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett.2023.Satisfiability-aided language models using declarative prompting.</p>
<p>A Prompts for Formalization</p>
<p>A.1 Prompts for ProofWriter</p>
<p>In subsection 5.1, we discussed various formalization approaches.In this section, we present the results using the GPT-4 Base Comments method on ProofWriter when the answer is False.As evident from the last line, the predicted outcome from GPT-4 can be derived easily.</p>
<p>System Message:</p>
<p>You are a logician with a background in mathematics that translates natural language reasoning text to Lean code so that these natural language reasoning problems can be solved.</p>
<p>A.2 Prompts Used for FOLIO</p>
<p>For FOLIO, the prompts differ slightly from those used for ProofWriter.Since FOLIO's textual context remains consistent across multiple questions, we concatenate questions sharing the same context and prompt the large language model in a single instance.An illustrative example is provided below.</p>
<p>System Message:</p>
<p>You are a logician with a background in mathematics that translates natural language reasoning text to Lean code so that these natural language reasoning problems can be solved.During the translation, please pay close attention to the predicates and entities.There is an additional requirement: I also want you to try to prove the theorem you translated to Lean.If you can prove the theorem, give me True at the end of the answer.If you can prove the negation of the theorem, write False at the end of the answer.If you can neither prove the original theorem nor the negation of the theorem, please give me Unknown at the end of the answer.</p>
<p>Input:</p>
<p>Textual context: There are six types of wild turkeys: Eastern wild turkey, Osceola wild turkey, Gould's wild turkey, Merriam's wild turkey, Rio Grande wild turkey, and Ocellated wild turkey.Tom is not an Eastern wild turkey.Tom is not an Osceola wild turkey.Tom is also not a Gould's wild turkey, or a Merriam's wild turkey, or a Rio Grande wild turkey.Tom is a wild turkey.</p>
<p>Question 1: Based on the above information, is the following statement true, false, or uncertain?Tom is an Ocellated wild turkey.Question 2: Based on the above information, is the following statement true, false, or uncertain?Tom is an Eastern wild turkey.Question 3: Based on the above information, is the following statement true, false, or uncertain?Joey is a wild turkey.</p>
<p>There are a few errors in the above code, including:</p>
<p>• There is a missing axiom that corresponds to "There are three types of wild turkeys":</p>
<p>Goulds Tom ∨ Eastern Tom ∨ Osceola Tom</p>
<p>• The formalization of numbers is incorrect, it should be: • The formalization of logic is incorrect, it should be:
¬ bite_causes_death t ∧ bite_causes_itching t) ∨ (bite_causes_death ∧ ¬ bite_causes_itching t
• There is an incorrect division of concepts that would make the proving impossible, the correct version should be:</p>
<p>∀ (t : Turkey), Eastern t → bite_causes_death t</p>
<p>C Example Proof Annotation with Different Annotation Styles</p>
<p>Here we're showing two example proofs created on the same problem with 'Intuitive' annotation style and 'Concise' annotation style.</p>
<p>Figure 2 :
2
Figure 2: Sample proofs created by LeanReasoner without pretraining (left), finetuned on Intuitive data (middle), and finetuned on Concise data (right).</p>
<p>-</p>
<dl>
<dt>-If someone likes the cat and they chase the cat then they are blue axiom R1 : ∀ x : obj, Likes x Cat ∧ Chases x Cat → Blue x --If someone likes the cow and they are red then the cow is round axiom R2 : ∀ x : obj, Likes x Cow ∧ Red x → Round Cow --If someone needs the tiger and they need the cat then they do not chase the cow axiom R3 : ∀ x : obj, Needs x Tiger ∧ Needs x Cat → ¬ Chases x Cow --If someone needs the cat and the cat is blue then the cat is red axiom R4 : ∀ x : obj, Needs x Cat ∧ Blue Cat → Red Cat --If someone is round then they need the cat axiom R5 : ∀ x : obj, Round x → Needs x Cat --If someone likes the cat and the cat needs the rabbit then the cat chases the tiger axiom R6 : ∀ x : obj, Likes x Cat ∧ Needs Cat Rabbit → Chases Cat Tiger --If the rabbit needs the tiger then the rabbit is not round axiom R7 : ∀ x : obj, Needs Rabbit Tiger → ¬ Round Rabbit theorem cow_chases_cow : Chases Cow Cow := begin have H1 : Needs Cow Tiger := T7, have H2 : Needs Cat Cow := T4, have H3 : ¬ Chases Cow Cow := R3 Cow (and.introH1 like the theorem cannot be proven in Lean.--So let's try to prove the negation of the theorem</dt>
<dd>
<p>Does the Cow like the Cow Options: True, False or Unknown Formalizer constant Cow: obj constant Dog: obj constant Cat: obj constant Big: obj
-&gt; Propconstant Likes: obj-&gt;obj-&gt;Propconstant Needs: obj-&gt;obj-&gt;Propaxiom R1: ∀ x : obj, Visits x Dog ∧ Needs Dog Cow → Needs x Cowaxiom R2: Visits Dog Cow → Visits Cow Dogaxiom R3: ∀ x : obj, Needs x Cow ∧ Likes Cow Dog → Likes x Cowaxiom R4: ∀ x : obj, Needs Cat Cow ∧ Likes Cow Cat → Likes Cow Cowtheorem does_cow_like_cow: Likes Cow Cow</p>
</dd>
</dl>
<p>Formalizer axiom T1: Big Cow axiom T2: Likes Cow Dog axiom T3: Visits Cow Dog axiom T4: Needs Dog Cow axiom T5: Needs Cow Cow Likes Cow Cow No Goals Needs Cow Cow ∧ Likes Cow Dog Needs Cat Cow ∧ Likes Cow Cat Needs Cow Cow, Likes Cow Dog Likes Cow Dog No Goals apply R3 Cow sorry apply R4 exact T2 exact T5 apply R1 Cow split … … Unknown True False Check every proof path Exists path that found a proof Theorem is the positive version Formalizer Tactic Generator + Proof Search Result Interpreter</p>
<p>Table 1 :
1
Formalization, Proof, and Answer choice accuracy of 100 ProofWriter samples and 40 FOLIO samples via OpenAI Language Model API, with manual annotation.'GPT-4 Base' serves as our baseline, where few-shot examples include both formalization and proof generation in a single prompt.In 'GPT-4 Base Comments', we augment these examples with line-by-line comments in Lean code.For 'GPT-4 Base Separate', we divide the task into two parts, using separate prompts for formalization and proof generation.For simplicity, we did not use the self-refinement technique when evaluating Logic-LM.
ModelProofWriter Formalize Prove Answer Formalize Prove Answer FOLIOGPT-4 Base94%15%80%60%10%35%GPT-4 Base Comments99%15%80%75%15%35%GPT-4 Base Separate95%5%75%60%10%40%GPT-3 Base Comments77%12%63%45%10%35%Logic-LM98%75.5%74%65%69.2%55%</p>
<p>Table 2 :
2
Comparative Analysis of Recall@k in premise selection and overall proof accuracy for 99 ProofWriter test samples and 28 FOLIO test samples.Note the proof accuracy here is different from Table1because it is directly linked to the final accuracy.The effects of pretraining and fine-tuning on LeanReasoner are evaluated using theorem-proving data and both Intuitive and Concise annotation sets, respectively.Premise Selection accuracy was not calculated for the GPT-4 baseline due to the complexities in prompting GPT-4 with Lean goals.</p>
<p>Table 3 :
3
The fine-tuned LeanReasoner has been pretrained on mathlib.Full training set method means the model has been trained on the full training set of ProofWriter.Fine-tuning on Concise achieves nearperfect accuracy with significantly less data.</p>
<p>Table 4 :
4
Result from "Lean Z3" is derived from leansmt applied to formalized Lean Code.The fine-tuned LeanReasoner has been pretrained on mathlib.Full training set method means the model has been trained on the full training set of FOLIO.Our approach achieves state-of-the-arts performance on FOLIO.All rabbits are cute.Some turtles exist.An animal is either a rabbit or a squirrel.If something is skittish, then it is not still.All squirrels are skittish.Rock is still.
Formalization: axiom A1 : ∀ (a : Animal), is_rabbit a → is_cute a axiom A2 : ∀ (a : Animal), is_rabbit a ∨ is_squirrel a axiom A3 : ∀ (a : Animal), is_skittish a → ¬ is_still a axiom A4 : ∀ (a : Animal), is_squirrel a → is_skittish a axiom A5 : is_still Rock theorem rock_condition : is_turtle Rock ∨ is_cute Rock:= Problem Statement: LeanReasoner Concise: LeanReasoner Intuitive: Question: Rock is a turtle or cute. LeanReasoner without Pretraining:right,have h1 : ¬ is_skittish Rock, {cases A2 Rock, {apply A1 Rock,intro h, have h2 : is_still Rock, from A5,right,cases A2 Rock, {have h3 : ¬ is_still Rock, from A3 Rock h,exact A3 Rock (A4 Rock h) A5,exact h,contradiction,}, {}, {}, cases A2 Rock, {exfalso,...have h2 : is_cute Rock, from A1 Rock h,exact A1 Rock h},right, exact h2,}✔}, {have h3 : ...,have h4 : ..., }</p>
<p>compresses multiple tactics into one reduces the workload of LLM ✘ model fails to find the correct solution ✘ model is confused by excessive lemmas</p>
<p>Stanislas Polu and Ilya Sutskever.2020.Generative language modeling for automated theorem proving.CoRR, abs/2009.03393.Abulhair Saparov and He He. 2023.Language models are greedy reasoners: A systematic formal analysis of chain-of-thought.In ICLR.OpenReview.net.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,and et al. 2022. Beyond the imitation game: Quanti-fying and extrapolating the capabilities of languagemodels.Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts,and Colin Raffel. 2022. Byt5: Towards a token-freefuture with pre-trained byte-to-byte models. Trans.Assoc. Comput. Linguistics, 10.Kaiyu Yang, Jia Deng, and Danqi Chen. 2022. Gen-erating natural language proofs with verifier-guidedsearch. In EMNLP.Kaiyu Yang, Aidan M. Swope, Alex Gu, Rahul Chala-mala, Peiyang Song, Shixing Yu, Saad Godil, RyanPrenger, and Anima Anandkumar. 2023. Leandojo:Theorem proving with retrieval-augmented languagemodels.</p>
<p>During the translation, please pay close attention to the predicates and entities.There is an additional requirement: I also want you to try to prove the theorem you translated to Lean.If you can prove the theorem, give me True at the end of the answer.If you can prove the negation of the theorem, write False at the end of the answer.If you can neither prove the original theorem nor the negation of the theorem, please give me Unknown at the end of the answer.Textual context: The cat is blue.The cat is nice.The cat likes the cow.The cat needs the cow.The cat needs the rabbit.The cow does not need the rabbit.The cow needs the tiger.The rabbit is round.The tiger is nice.The tiger does not like the cow.If someone likes the cat and they chase the cat then they are blue.If someone likes the cow and they are red then the cow is round.If someone needs the tiger and they need the cat then they do not chase the cow.If someone needs the cat and the cat is blue then the cat is red.If someone is round then they need the cat.If someone likes the cat and the cat needs the rabbit then the cat chases the tiger.If the rabbit needs the tiger then the rabbit is not round.
Output:universe uconstant obj : Type uconstant Cat : objconstant Cow : objconstant Rabbit : objconstant Tiger : objconstant Blue : obj → Propconstant Nice : obj → Propconstant Likes : obj → obj → Propconstant Needs : obj → obj → Propconstant Round : obj →constant Red : obj → Propconstant Chases : obj → obj → Propaxiom T1 : Blue Cataxiom T2 : Nice Cataxiom T3 : Likes Cat Cowaxiom T4 : Needs Cat Cowaxiom T5 : Needs Cat Rabbitaxiom T6 : ¬ Needs Cow Rabbitaxiom T7 : Needs Cow Tigeraxiom T8 : Round Rabbitaxiom T9 : Nice Tigeraxiom T10 : ¬ Likes Tiger CowInput:Question: Based on the above information, is thefollowing statement true, false, or unknown? Thecow chases the cow.</p>
<p>, which is Cow does not chase Cow The negation of the theorem can be proven in Lean.So --The answer is False
--theorem not_cow_chases_cow : ¬ Chases Cow Cow :=beginapply R3 Cow,split,exact T7,apply R5,apply R2 Cat,split,exact T3,apply R4 Rabbit,split,apply R5,exact T8,exact T1,end
https://leanprover.github.io/.
https://en.wikipedia.org/wiki/Lean_ (proof_assistant).
https://github.com/ leanprover-community/mathlib
AcknowledgementsWe thank the reviewers and members of the Cohort for the valuable feedback and comments on the paper.We appreciate the provision of computing resources through the Baskerville cluster at the University of Birmingham.Output:--Introducing basic types for entities in our universe.constant Turkey : Type constant Tom : Turkey constant Joey : Turkey --Defining properties for the turkeys in our universe.constant is_wild_turkey : Turkey → Prop constant is_eastern_wild_turkey : Turkey → Prop constant is_osceola_wild_turkey : Turkey → Prop constant is_goulds_wild_turkey : Turkey → Prop is_merriams_wild_turkey : Turkey → Prop constant is_rio_grande_wild_turkey : Turkey → Prop constant is_ocellated_wild_turkey : Turkey → Prop --Context from the information --Tom is not an Eastern wild turkey.axiom A1 : ¬ is_eastern_wild_turkey Tom --Tom is not an Osceola wild turkey.axiom A2 : ¬ is_osceola_wild_turkey Tom --Tom is also not a Gould's wild turkey.axiom A3 : ¬ is_goulds_wild_turkey Tom --Tom is not a Merriam's wild turkey.axiom A4 : ¬ is_merriams_wild_turkey Tom --Tom is not a Rio Grande wild turkey.axiom A5 : ¬ is_rio_grande_wild_turkey Tom --Tom is a wild turkey.
Code llama: Open foundation models for code. Fabian Gloeckle Baptiste Rozière, Jonas Gehring, Et , CoRR, abs/2308.129502023</p>
<p>Learning language representations with logical inductive bias. Jianshu Chen, ICLR. OpenReview.net. 2023</p>
<p>Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, Tony Xia, Theoremqa: A theorem-driven question answering dataset. 2023a</p>
<p>RECKON-ING: reasoning through dynamic knowledge encoding. Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyilmaz, Antoine Bosselut, 10.48550/arXiv.2305.06349CoRR, abs/2305.063492023b</p>
<p>. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021Training verifiers to solve math word problems</p>
<p>Faithful reasoning using large language models. Antonia Creswell, Murray Shanahan, CoRR, abs/2208.142712022</p>
<p>Language models show human-like content effects on reasoning. Ishita Dasgupta, Andrew K Lampinen, C Y Stephanie, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, 2022CoRR</p>
<p>Z3: an efficient SMT solver. Leonardo Mendonça, De Moura, Nikolaj S Bjørner, TACAS. 2008</p>
<p>The lean theorem prover (system description). Leonardo Mendonça De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, Jakob Von Raumer, 2015In CADE-2</p>
<p>How does gpt obtain its ability? tracing emergent abilities of language models to their sources. Hao Fu, ; Yao, Tushar Peng, Khot, 2022Yao Fu's Notion</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, ICLR. OpenReview.net2023</p>
<p>Does entity abstraction help generative transformers reason?. Nicolas Gontier, Siva Reddy, Christopher Pal, Trans. Mach. Learn. Res. 2022. 2022</p>
<p>Proof artifact cotraining for theorem proving with language models. Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W Ayers, Stanislas Polu, ICLR. 2022a</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R Joty, Alexander R Fabbri, Wojciech Kryscinski, Caiming Xiong, and Dragomir Radev. 2022b. FOLIO: natural language reasoning with first-order logic. Xi Victoria LinCoRR</p>
<p>Solving math word problems by combining language models with symbolic solvers. Joy He-Yueya, Gabriel Poesia, Rose E Wang, Noah D Goodman, 2023</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 2020</p>
<p>Measuring Mathematical Problem Solving With the MATH Dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, NeurIPS. Menlo Park, CalifAAAI Press2021</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, S H Patrick, Ledell Lewis, Sergey Wu, Danqi Edunov, Wen-Tau Chen, Yih, EMNLP. 2020</p>
<p>Philippe Laban, Wojciech Kryscinski, Divyansh Agarwal, Alexander R Fabbri, Caiming Xiong, Shafiq Joty, Chien-Sheng Wu, Llms as factual reasoners: Insights from existing benchmarks and beyond. 2023</p>
<p>Hypertree proof search for neural theorem proving. Guillaume Lample, Timothée Lacroix, Marie-Anne Lachaux, Aurélien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, Xavier Martinet, NeurIPS. 2022</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, V Vinay, Ambrose Ramasesh, Slone, Cem Anil, Imanol Schlag. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra2022In NeurIPS</p>
<p>LINC: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Theo Olausson, Alex Gu, Benjamin Lipkin, Cedegao E Zhang, Armando Solar-Lezama, Joshua B Tenenbaum, Roger Levy, EMNLP. 2023</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Xinyi Wang, GPT-4 technical report. 2023. 2023OpenAI ; CoRR. Liangming Pan, Alon Albalak,</p>
<p>Certified reasoning with language models. Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, Noah D Goodman, 2023CoRR</p>
<p>Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. 2023. Formal mathematics statement curriculum learning. Stanislas Polu, Jesse Michael Han, Kunhao Zheng, ICLR. </p>            </div>
        </div>

    </div>
</body>
</html>
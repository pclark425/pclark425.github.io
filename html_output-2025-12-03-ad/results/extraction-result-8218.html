<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8218 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8218</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8218</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-277993681</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.15965v2.pdf" target="_blank">From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Memory is the process of encoding, storing, and retrieving information, allowing humans to retain experiences, knowledge, skills, and facts over time, and serving as the foundation for growth and effective interaction with the world. It plays a crucial role in shaping our identity, making decisions, learning from past experiences, building relationships, and adapting to changes. In the era of large language models (LLMs), memory refers to the ability of an AI system to retain, recall, and use information from past interactions to improve future responses and interactions. Although previous research and reviews have provided detailed descriptions of memory mechanisms, there is still a lack of a systematic review that summarizes and analyzes the relationship between the memory of LLM-driven AI systems and human memory, as well as how we can be inspired by human memory to construct more powerful memory systems. To achieve this, in this paper, we propose a comprehensive survey on the memory of LLM-driven AI systems. In particular, we first conduct a detailed analysis of the categories of human memory and relate them to the memory of AI systems. Second, we systematically organize existing memory-related work and propose a categorization method based on three dimensions (object, form, and time) and eight quadrants. Finally, we illustrate some open problems regarding the memory of current AI systems and outline possible future directions for memory in the era of large language models.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8218.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8218.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A long-term non-parametric memory module that stores conversation histories and condensed summaries as vector embeddings, enabling retrieval-augmented personalized responses and profile construction for LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorybank: Enhancing large language models with long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoryBank (as used with LLM agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory module / system integrated with LLM agents to construct and retrieve long-term user profiles and conversation summaries to inform generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Personalized multi-session dialogue / memory-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Store and retrieve user-specific information across sessions to provide personalized responses and recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>personalized dialogue / retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>non-parametric long-term (vector store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Construct summaries and vector embeddings of conversations; index and retrieve with dense retrieval (dual-tower) and FAISS; use retrieved context in prompt for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Conversation summaries / key events encoded as vectors; optionally key-value entries.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Dense vector similarity search (dual-tower dense retrieval) indexed with FAISS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey reports MemoryBank uses vector retrieval and summaries to improve personalization; no quantitative ablations reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Non-parametric long-term vector memory (summaries + dense retrieval) enables personalization across sessions without loading full histories, improving contextual relevance for LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires good summarization/condensation and indexing; retrieval relevance and memory management (e.g., forgetting/prioritization) remain challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8218.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent paradigm that interleaves chain-of-thought style reasoning (thoughts) with actions (tool calls) to enable reasoning + acting in language model agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent architecture that produces explicit intermediate reasoning steps and corresponding external actions, enabling iterative tool use and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning + tool-using tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step problem solving requiring planning, retrieval, and tool invocation (e.g., question answering with web/tools).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / tool-using</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>system short-term (scratchpad / chain-of-thought buffer)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Intermediate reasoning traces (thoughts) are produced and used within the current episode as short-term memory; alternation between thoughts and actions forms ephemeral system memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Intermediate thoughts / actions / tool calls; reasoning trace in prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation of recent thought-action sequence (recency-based within context window).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey notes ReAct empirically improves planning and decision making relative to baselines that do not expose intermediate reasoning, but the survey does not provide numeric ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Exposing intermediate reasoning (short-term system memory) aids complex task solving by guiding actions and enabling corrective steps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on context window capacity; ephemeral traces are lost across sessions unless externally stored.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8218.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that gives agents dynamic memory and self-reflection capability to self-evaluate and iteratively refine behavior using textual reflection and memory updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM agent framework that records outcomes, generates reflective critiques, and updates future behavior via a memory of past reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Autonomous agent tasks requiring self-improvement</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-ended tasks where the agent can learn from failures by reflecting and updating strategies (e.g., sequential tasks, simulator-based tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>self-improving agent / lifelong learning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>system long-term non-parametric (reflective memory repository)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Store reflective logs and outcome summaries; retrieve past reflections to guide future planning and policy adjustments.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Reflections, critiques, outcome traces, and past strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieval of past reflections deemed relevant to current task (semantic/heuristic selection described in original work; survey mentions concept).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey highlights Reflexion as demonstrating benefits from self-reflection loops; no detailed numbers given in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reflection-based long-term system memory enables agents to learn from failures and refine behavior, improving performance on subsequent tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>How to scale and prioritize reflections; representation and retrieval quality impact effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8218.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Buffer-of-Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Buffer of Thoughts (BoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that refines chains-of-thought from historical tasks to form reusable templates (buffers) that guide future reasoning and decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Buffer of Thoughts: Thought-augmented reasoning with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Buffer of Thoughts (BoT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>System that stores refined chain-of-thought fragments (thought templates) in a memory repository for reuse by LLM agents on similar tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning tasks requiring reusable thought patterns</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning problems where prior solved thought templates can accelerate or improve solution generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / template reuse</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>system non-parametric long-term (thought template repository)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Aggregate and refine past chain-of-thought outputs into templates stored for retrieval and reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Refined thought templates / reasoning fragments.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Selection of relevant templates based on task similarity (semantic matching described in original work; survey mentions conceptually).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey reports BoT as an example that long-term thought memories can guide future reasoning; no numeric ablations in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Storing reusable reasoning fragments helps agents apply previously successful reasoning strategies to new but related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Defining templates, retrieval granularity, and preventing overfitting of templates to narrow contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8218.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HippoRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HippoRAG</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurobiologically inspired long-term memory architecture that combines phrase-based knowledge graphs with retrieved passages to improve memory recall in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>HippoRAG: Neurobiologically inspired long-term memory for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HippoRAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieval system building graph-like memory representations (entities/phrases) plus passage storage, enabling richer retrieval for long-term memory needs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-term memory retrieval for dialogue / knowledge-intensive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Recall of relevant personal or factual memories across sessions using structured phrase- and passage-level memory representations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented generation / long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>non-parametric long-term (graph + passages)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Construct phrase-based knowledge graphs and link to original passages; retrieve both conceptual graph nodes and contextual passages for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Knowledge graph nodes (phrases/entities) plus linked original text passages.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Graph-based selection combined with passage retrieval (semantic search + graph traversal).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey notes HippoRAG improves recall by combining conceptual and contextual signals; numerical comparisons are in the original paper (not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hybrid graph+passage memories can capture both conceptual relationships and context, improving retrieval relevance for LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Complexity of graph construction, update dynamics, and retrieval integration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8218.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RET-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RET-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A read-write memory design for LLMs that stores triplet-structured factual memories and supports fuzzy retrieval and updating.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ret-llm: Towards a general read-write memory for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RET-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory module that constructs structured triplet memories and provides read-write access for LLM agents to update and query factual state.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-term factual memory maintenance / environment tracking</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maintain and update facts about the environment or user over time and retrieve relevant facts for current reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>memory maintenance / retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>non-parametric long-term (structured triplets)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Store information as triplets; use fuzzy search to retrieve relevant triplets for prompt augmentation; support writes to update facts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Triplet-structured facts (subject, relation, object).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Fuzzy search over triplet store (semantic matching over structured entries).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey describes RET-LLM as enabling continual environmental monitoring; no quantitative ablations in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structured read-write memories allow ongoing updates and retrieval of environment/user facts, supporting long-term personalization and state tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Triplet extraction quality and scalability of fuzzy retrieval; conflict resolution and memory consolidation remain nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8218.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>vLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>vLLM (PagedAttention serving system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An efficient LLM serving system implementing a virtual-memory-inspired attention (PagedAttention) to minimize KV cache waste and enable flexible sharing across requests for high-throughput inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>vLLM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>High-efficiency serving stack for transformer LLMs using PagedAttention to manage KV caches across requests and improve batching and throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>High-throughput LLM serving / long-context inference</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Serve LLM inference efficiently when many concurrent requests and long contexts create KV cache memory pressure.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>system-level serving / inference optimization</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric short-term (KV cache management)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>PagedAttention virtual memory for KV caches enabling near-zero KV cache waste and flexible sharing across requests.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Attention keys and values (KV cache) stored and paged for reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>KV reuse via efficient page management and sharing policies (recency and batching informed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey states vLLM significantly improves batching efficiency and throughput over naive KV cache handling; no numeric details included here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Careful KV cache management at serving level is crucial for efficient long-context inference and high throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Designing eviction and sharing strategies that balance latency and memory footprint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8218.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KV Cache</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KV Cache (Key-Value Cache)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Short-term parametric memory storing attention keys and values produced during sequence generation, enabling reuse across subsequent decoding steps to accelerate inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KV Cache</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Inference-time cache of attention keys and values stored per token/layer to avoid recomputation during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-text generation / multi-turn dialogue inference</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Accelerate autoregressive generation by reusing cached attention states rather than recomputing from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>inference acceleration / working memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric short-term (working memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Store and reuse attention keys and values across decoding steps; techniques include compression, chunking, and eviction policies.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Layer-wise attention keys and values (numerical tensors).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Direct reuse by addressing cached KV entries for current decoding positions (recency / token-indexed access).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey references many KV cache compression and reuse methods (ChunkKV, Prompt Cache, RAGCache) that reduce memory and latency; specific quantitative ablations are in cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>KV cache reuse is an essential parametric short-term memory mechanism to reduce computation and latency in LLM serving, and compression/eviction strategies substantially affect efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Compression may hurt fidelity; eviction and sharing across requests complicate correctness and latency tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8218.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Cache</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Cache</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sentence-level caching technique that stores prior input prompts and corresponding outputs to avoid recomputation when similar prompts recur, improving latency and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompt cache: Modular attention reuse for low-latency inference.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Prompt Cache</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Caching layer that maps input prompts to outputs (or intermediate states) and returns cached responses for repeated or similar prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Dialogue / repeated-query inference</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reduce compute and latency for repeated or near-duplicate prompts by returning cached outputs or reusing cached context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>inference acceleration / caching</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric short-term (prompt-level cache)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Store full prompt -> output mappings (or prefix KV states) and reuse when similar prompts are seen.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Cached prompts and outputs or corresponding prefix KV cache entries.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Exact or approximate matching of current prompt to cached entries (recency and similarity-based).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey notes prompt caching reduces API call cost and latency; detailed measurements are in original engineering papers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompt-level caching is effective engineering leverage to speed up common repeated interactions and to reduce computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cache hit rate depends on prompt repetitiveness; correctness under near-matches requires careful handling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8218.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChunkKV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChunkKV</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A KV cache compression method that groups tokens into semantic chunks and enables layer-wise index reuse to compress attention caches for long-context inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChunkKV: Semantic-preserving KV cache compression for efficient long-context LLM inference.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChunkKV</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Compression strategy for KV caches that retains the most informative chunks to reduce memory and computation while preserving quality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context LLM inference / long-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reduce KV cache memory footprint and inference cost for long-context sequences while maintaining generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>inference optimization / cache compression</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric short-term (compressed KV cache)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Semantic chunking of tokens and layer-wise reuse of compressed indices to decrease KV cache size.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Compressed KV representations corresponding to semantic token chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Index-based retrieval of compressed chunks for attention computation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey reports ChunkKV reduces memory/computation and outperforms some prior compression baselines on benchmarks (no numeric details provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Semantic-aware KV compression can reduce resource usage in long-context inference while retaining performance better than nave compression.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Choosing chunk granularity and maintaining alignment with attention computations are nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8218.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoryLLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of approaches that treat an LLM itself as a parametric long-term memory, enabling self-updates and knowledge injections into model parameters for persistent knowledge retention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoryLLM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLMs augmented with mechanisms to inject, edit, and retain long-term knowledge in parameters (parameterized memory / lifelong editing frameworks).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Lifelong knowledge editing / continual adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Integrate new facts or user-specific information into model parameters for durable retention across sessions and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>parametric long-term memory / knowledge editing</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric long-term (model parameter edits)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Parameter-efficient fine-tuning, memory injection, or dual-memory designs (main + side memory) with routing to access edited knowledge at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Edited model weights (parameterized knowledge) or side-memory parameter blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Internalized via forward pass (no external retrieval) with routing mechanisms to consult edited memory partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey references MemoryLLM and WISE showing parametric edits can preserve and route new knowledge; details and ablations are in cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Parametric memory enables compressed, durable retention of knowledge but faces scalability and personalization cost challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fine-tuning per-user or frequent editing is computationally expensive and raises scalability, stability, and privacy issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8218.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WISE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WISE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lifelong editing framework for LLMs using dual-parametric memory (main + side memory) and routing/knowledge sharding to support reliable continual edits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>WISE: Rethinking the knowledge memory for lifelong model editing of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>WISE</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Framework that separates preserved pretrained knowledge (main memory) from edited knowledge (side memory), using routing to select appropriate memory during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Model editing / lifelong knowledge updates</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Inject and manage edits to LLM knowledge while preserving original capabilities and ensuring locality and generalization of edits.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>parametric long-term memory / model editing</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric long-term (dual-memory parameter blocks)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Maintain main pretrained parameters and separate edited side memory; use routing to access the correct memory at inference; knowledge sharding for edit distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Parameter partitions for main and edited knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Routing mechanism during inference to consult side memory when relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey highlights dual-memory approach as improving edit reliability and generalization; original paper contains detailed evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dual-memory designs help maintain pretrained knowledge while enabling reliable edits; routing and sharding important for scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Engineering complexity, routing accuracy, and bookkeeping for many edits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8218.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Echo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Echo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM augmented with temporal episodic memory to improve performance on multi-turn, complex memory-based dialogues by maintaining temporally structured episodic traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Echo: A large language model with temporal episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Echo</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model that integrates temporal episodic memory storage and retrieval mechanisms to better handle tasks requiring recall of temporally ordered events.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Temporal multi-turn dialogue / episodic memory tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Recall temporally structured user events or conversation history to sustain coherent long-term dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>personalized long-term dialogue / episodic recall</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric long-term / non-parametric episodic (depending on implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Temporal episodic traces stored and retrieved to inform generation; details described in original Echo work.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Temporally indexed episodes (summaries, events) and possibly parameterized memory augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Temporal-aware retrieval of episodes relevant to current query (semantic + temporal proximity).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey mentions Echo as example of temporal episodic memory improving dialogue tasks; numerical comparisons are in the source paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Temporal episodic memory structures help LLMs maintain coherence and personal details over long conversations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Designing temporal indexing and scalable retrieval; tradeoffs between fidelity and storage cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e8218.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-augmented LLM architecture designed to support multi-hop reasoning by storing and retrieving intermediate facts and reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memreasoner: A memory-augmented llm architecture for multi-hop reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architecture that augments an LLM with an explicit memory component to record and retrieve intermediate reasoning steps and facts for complex multi-hop problems.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-hop reasoning / multi-step QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve problems that require chaining multiple reasoning steps and retrieving intermediate facts to reach an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-hop reasoning / question answering</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>system non-parametric short/long-term (intermediate reasoning memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Store intermediate facts and reasoning chains; retrieve relevant stored reasoning fragments to support multi-hop inference.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Intermediate reasoning chains, extracted facts, and supporting evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic search / similarity retrieval over stored reasoning fragments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey lists MemReasoner as an architecture for improved multi-hop capability; detailed experimental ablations are in the original work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly recording intermediate reasoning can improve multi-hop reasoning and reduce error propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Memory growth, retrieval precision, and integration of retrieved fragments into coherent final outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e8218.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulated agents that use memory of observations, interactions, and reflections to produce human-like behavior over time in sandbox environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that maintain personal memories (observations, events, plans) and use retrieval and reflection to inform actions and interactions in simulated social environments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Social simulation / long-horizon interactive behavior</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Sustain believable, temporally coherent agent behavior in a simulated environment by leveraging stored episodic memories and reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>simulation / long-term behavior modeling</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>non-parametric long-term personal memory (episodic store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Record observations and interactions; index and retrieve salient memories, use reflection to summarize and prioritize memories for future behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Episodic event records, reflections, plans, and salient summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieval based on relevance to current context (semantic + salience heuristics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey cites Generative Agents as demonstrating that explicit episodic memories and reflection improve long-term believable behavior; no numeric ablation included here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structured personal episodic memories plus reflection mechanisms enable more coherent, personalized agent behavior over extended interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Scalability of storing many events, privacy of stored personal data, and retrieval accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e8218.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach framing LLMs as operating-system-like managers that maintain and use memory to support agentic behaviors (read-write memory interface).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memgpt: Towards llms as operating systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Conceptual/architectural approach that gives an LLM an explicit memory interface to read, write, and manage long-term memory for continuous agentic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Autonomous multi-step agent tasks / long-term personal assistants</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Coordinate complex tasks over time by persisting state and retrieving relevant memories.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>autonomous agent / long-term memory management</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>non-parametric long-term (read-write memory store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Expose read-write memory APIs for LLM to commit and query memory entries; use retrieved memory to inform subsequent planning and execution.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored events, facts, summaries accessible via read-write operations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic retrieval / read operations invoked by agent when relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey mentions MemGPT conceptually as enabling OS-like memory but does not report numerical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing LLMs with read-write memory interfaces facilitates persistent behavior and stateful agent operation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Designing safe, scalable read-write semantics and avoiding memory corruption or privacy leaks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8218.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e8218.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RETRO / RAGCache (caching in RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAGCache (multilevel dynamic caching for Retrieval-Augmented Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multilevel dynamic caching system tailored for retrieval-augmented generation that caches intermediate knowledge states and overlaps retrieval with inference to reduce latency and improve throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RAGCache: Efficient knowledge caching for retrieval-augmented generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAGCache</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Caching mechanism that stores retrieved documents / intermediate retrieval states for reuse across generation requests to reduce retrieval latency in RAG systems.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Retrieval-augmented generation / knowledge-intensive QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reduce latency and improve throughput by caching retrieved knowledge and reusing it during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>system non-parametric short-term (retrieval cache)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Multi-level caching of retrieved documents and intermediate knowledge states; special replacement policies informed by LLM retrieval/inference patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Cached retrieved passages and intermediate retrieval results.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Cache lookup by query similarity; overlap retrieval with inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey reports RAGCache reduces latency and improves throughput in RAG settings; numerical evaluations are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Intelligent caching in RAG pipelines yields large latency and throughput benefits compared to fresh retrieval each request.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cache staleness, replacement policy design, and consistency with underlying corpora updates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memorybank: Enhancing large language models with long-term memory. <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Buffer of Thoughts: Thought-augmented reasoning with large language models <em>(Rating: 2)</em></li>
                <li>HippoRAG: Neurobiologically inspired long-term memory for large language models <em>(Rating: 2)</em></li>
                <li>Ret-llm: Towards a general read-write memory for large language models <em>(Rating: 2)</em></li>
                <li>Memreasoner: A memory-augmented llm architecture for multi-hop reasoning <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8218",
    "paper_id": "paper-277993681",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "MemoryBank",
            "name_full": "MemoryBank",
            "brief_description": "A long-term non-parametric memory module that stores conversation histories and condensed summaries as vector embeddings, enabling retrieval-augmented personalized responses and profile construction for LLM agents.",
            "citation_title": "Memorybank: Enhancing large language models with long-term memory.",
            "mention_or_use": "mention",
            "agent_name": "MemoryBank (as used with LLM agents)",
            "agent_description": "Memory module / system integrated with LLM agents to construct and retrieve long-term user profiles and conversation summaries to inform generation.",
            "model_name": null,
            "model_description": null,
            "task_name": "Personalized multi-session dialogue / memory-augmented generation",
            "task_description": "Store and retrieve user-specific information across sessions to provide personalized responses and recommendations.",
            "task_type": "personalized dialogue / retrieval-augmented generation",
            "memory_used": true,
            "memory_type": "non-parametric long-term (vector store)",
            "memory_mechanism": "Construct summaries and vector embeddings of conversations; index and retrieve with dense retrieval (dual-tower) and FAISS; use retrieved context in prompt for generation.",
            "memory_representation": "Conversation summaries / key events encoded as vectors; optionally key-value entries.",
            "memory_retrieval_method": "Dense vector similarity search (dual-tower dense retrieval) indexed with FAISS.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey reports MemoryBank uses vector retrieval and summaries to improve personalization; no quantitative ablations reported in this survey.",
            "key_findings": "Non-parametric long-term vector memory (summaries + dense retrieval) enables personalization across sessions without loading full histories, improving contextual relevance for LLM agents.",
            "limitations_or_challenges": "Requires good summarization/condensation and indexing; retrieval relevance and memory management (e.g., forgetting/prioritization) remain challenges.",
            "uuid": "e8218.0",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct",
            "brief_description": "An agent paradigm that interleaves chain-of-thought style reasoning (thoughts) with actions (tool calls) to enable reasoning + acting in language model agents.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "mention",
            "agent_name": "ReAct",
            "agent_description": "Agent architecture that produces explicit intermediate reasoning steps and corresponding external actions, enabling iterative tool use and planning.",
            "model_name": null,
            "model_description": null,
            "task_name": "Reasoning + tool-using tasks",
            "task_description": "Multi-step problem solving requiring planning, retrieval, and tool invocation (e.g., question answering with web/tools).",
            "task_type": "multi-step reasoning / tool-using",
            "memory_used": true,
            "memory_type": "system short-term (scratchpad / chain-of-thought buffer)",
            "memory_mechanism": "Intermediate reasoning traces (thoughts) are produced and used within the current episode as short-term memory; alternation between thoughts and actions forms ephemeral system memory.",
            "memory_representation": "Intermediate thoughts / actions / tool calls; reasoning trace in prompt context.",
            "memory_retrieval_method": "Prompt concatenation of recent thought-action sequence (recency-based within context window).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey notes ReAct empirically improves planning and decision making relative to baselines that do not expose intermediate reasoning, but the survey does not provide numeric ablations.",
            "key_findings": "Exposing intermediate reasoning (short-term system memory) aids complex task solving by guiding actions and enabling corrective steps.",
            "limitations_or_challenges": "Relies on context window capacity; ephemeral traces are lost across sessions unless externally stored.",
            "uuid": "e8218.1",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion",
            "brief_description": "An approach that gives agents dynamic memory and self-reflection capability to self-evaluate and iteratively refine behavior using textual reflection and memory updates.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "LLM agent framework that records outcomes, generates reflective critiques, and updates future behavior via a memory of past reflections.",
            "model_name": null,
            "model_description": null,
            "task_name": "Autonomous agent tasks requiring self-improvement",
            "task_description": "Open-ended tasks where the agent can learn from failures by reflecting and updating strategies (e.g., sequential tasks, simulator-based tasks).",
            "task_type": "self-improving agent / lifelong learning",
            "memory_used": true,
            "memory_type": "system long-term non-parametric (reflective memory repository)",
            "memory_mechanism": "Store reflective logs and outcome summaries; retrieve past reflections to guide future planning and policy adjustments.",
            "memory_representation": "Reflections, critiques, outcome traces, and past strategies.",
            "memory_retrieval_method": "Retrieval of past reflections deemed relevant to current task (semantic/heuristic selection described in original work; survey mentions concept).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey highlights Reflexion as demonstrating benefits from self-reflection loops; no detailed numbers given in survey.",
            "key_findings": "Reflection-based long-term system memory enables agents to learn from failures and refine behavior, improving performance on subsequent tasks.",
            "limitations_or_challenges": "How to scale and prioritize reflections; representation and retrieval quality impact effectiveness.",
            "uuid": "e8218.2",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Buffer-of-Thoughts",
            "name_full": "Buffer of Thoughts (BoT)",
            "brief_description": "A method that refines chains-of-thought from historical tasks to form reusable templates (buffers) that guide future reasoning and decision-making.",
            "citation_title": "Buffer of Thoughts: Thought-augmented reasoning with large language models",
            "mention_or_use": "mention",
            "agent_name": "Buffer of Thoughts (BoT)",
            "agent_description": "System that stores refined chain-of-thought fragments (thought templates) in a memory repository for reuse by LLM agents on similar tasks.",
            "model_name": null,
            "model_description": null,
            "task_name": "Reasoning tasks requiring reusable thought patterns",
            "task_description": "Multi-step reasoning problems where prior solved thought templates can accelerate or improve solution generation.",
            "task_type": "multi-step reasoning / template reuse",
            "memory_used": true,
            "memory_type": "system non-parametric long-term (thought template repository)",
            "memory_mechanism": "Aggregate and refine past chain-of-thought outputs into templates stored for retrieval and reuse.",
            "memory_representation": "Refined thought templates / reasoning fragments.",
            "memory_retrieval_method": "Selection of relevant templates based on task similarity (semantic matching described in original work; survey mentions conceptually).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey reports BoT as an example that long-term thought memories can guide future reasoning; no numeric ablations in survey.",
            "key_findings": "Storing reusable reasoning fragments helps agents apply previously successful reasoning strategies to new but related tasks.",
            "limitations_or_challenges": "Defining templates, retrieval granularity, and preventing overfitting of templates to narrow contexts.",
            "uuid": "e8218.3",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "HippoRAG",
            "name_full": "HippoRAG",
            "brief_description": "A neurobiologically inspired long-term memory architecture that combines phrase-based knowledge graphs with retrieved passages to improve memory recall in LLMs.",
            "citation_title": "HippoRAG: Neurobiologically inspired long-term memory for large language models",
            "mention_or_use": "mention",
            "agent_name": "HippoRAG",
            "agent_description": "Retrieval system building graph-like memory representations (entities/phrases) plus passage storage, enabling richer retrieval for long-term memory needs.",
            "model_name": null,
            "model_description": null,
            "task_name": "Long-term memory retrieval for dialogue / knowledge-intensive tasks",
            "task_description": "Recall of relevant personal or factual memories across sessions using structured phrase- and passage-level memory representations.",
            "task_type": "retrieval-augmented generation / long-term memory",
            "memory_used": true,
            "memory_type": "non-parametric long-term (graph + passages)",
            "memory_mechanism": "Construct phrase-based knowledge graphs and link to original passages; retrieve both conceptual graph nodes and contextual passages for generation.",
            "memory_representation": "Knowledge graph nodes (phrases/entities) plus linked original text passages.",
            "memory_retrieval_method": "Graph-based selection combined with passage retrieval (semantic search + graph traversal).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey notes HippoRAG improves recall by combining conceptual and contextual signals; numerical comparisons are in the original paper (not reproduced here).",
            "key_findings": "Hybrid graph+passage memories can capture both conceptual relationships and context, improving retrieval relevance for LLM agents.",
            "limitations_or_challenges": "Complexity of graph construction, update dynamics, and retrieval integration.",
            "uuid": "e8218.4",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "RET-LLM",
            "name_full": "RET-LLM",
            "brief_description": "A read-write memory design for LLMs that stores triplet-structured factual memories and supports fuzzy retrieval and updating.",
            "citation_title": "Ret-llm: Towards a general read-write memory for large language models",
            "mention_or_use": "mention",
            "agent_name": "RET-LLM",
            "agent_description": "Memory module that constructs structured triplet memories and provides read-write access for LLM agents to update and query factual state.",
            "model_name": null,
            "model_description": null,
            "task_name": "Long-term factual memory maintenance / environment tracking",
            "task_description": "Maintain and update facts about the environment or user over time and retrieve relevant facts for current reasoning.",
            "task_type": "memory maintenance / retrieval-augmented generation",
            "memory_used": true,
            "memory_type": "non-parametric long-term (structured triplets)",
            "memory_mechanism": "Store information as triplets; use fuzzy search to retrieve relevant triplets for prompt augmentation; support writes to update facts.",
            "memory_representation": "Triplet-structured facts (subject, relation, object).",
            "memory_retrieval_method": "Fuzzy search over triplet store (semantic matching over structured entries).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey describes RET-LLM as enabling continual environmental monitoring; no quantitative ablations in the survey text.",
            "key_findings": "Structured read-write memories allow ongoing updates and retrieval of environment/user facts, supporting long-term personalization and state tracking.",
            "limitations_or_challenges": "Triplet extraction quality and scalability of fuzzy retrieval; conflict resolution and memory consolidation remain nontrivial.",
            "uuid": "e8218.5",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "vLLM",
            "name_full": "vLLM (PagedAttention serving system)",
            "brief_description": "An efficient LLM serving system implementing a virtual-memory-inspired attention (PagedAttention) to minimize KV cache waste and enable flexible sharing across requests for high-throughput inference.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "vLLM",
            "agent_description": "High-efficiency serving stack for transformer LLMs using PagedAttention to manage KV caches across requests and improve batching and throughput.",
            "model_name": null,
            "model_description": null,
            "task_name": "High-throughput LLM serving / long-context inference",
            "task_description": "Serve LLM inference efficiently when many concurrent requests and long contexts create KV cache memory pressure.",
            "task_type": "system-level serving / inference optimization",
            "memory_used": true,
            "memory_type": "parametric short-term (KV cache management)",
            "memory_mechanism": "PagedAttention virtual memory for KV caches enabling near-zero KV cache waste and flexible sharing across requests.",
            "memory_representation": "Attention keys and values (KV cache) stored and paged for reuse.",
            "memory_retrieval_method": "KV reuse via efficient page management and sharing policies (recency and batching informed).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey states vLLM significantly improves batching efficiency and throughput over naive KV cache handling; no numeric details included here.",
            "key_findings": "Careful KV cache management at serving level is crucial for efficient long-context inference and high throughput.",
            "limitations_or_challenges": "Designing eviction and sharing strategies that balance latency and memory footprint.",
            "uuid": "e8218.6",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "KV Cache",
            "name_full": "KV Cache (Key-Value Cache)",
            "brief_description": "Short-term parametric memory storing attention keys and values produced during sequence generation, enabling reuse across subsequent decoding steps to accelerate inference.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "KV Cache",
            "agent_description": "Inference-time cache of attention keys and values stored per token/layer to avoid recomputation during generation.",
            "model_name": null,
            "model_description": null,
            "task_name": "Long-text generation / multi-turn dialogue inference",
            "task_description": "Accelerate autoregressive generation by reusing cached attention states rather than recomputing from scratch.",
            "task_type": "inference acceleration / working memory",
            "memory_used": true,
            "memory_type": "parametric short-term (working memory)",
            "memory_mechanism": "Store and reuse attention keys and values across decoding steps; techniques include compression, chunking, and eviction policies.",
            "memory_representation": "Layer-wise attention keys and values (numerical tensors).",
            "memory_retrieval_method": "Direct reuse by addressing cached KV entries for current decoding positions (recency / token-indexed access).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey references many KV cache compression and reuse methods (ChunkKV, Prompt Cache, RAGCache) that reduce memory and latency; specific quantitative ablations are in cited works.",
            "key_findings": "KV cache reuse is an essential parametric short-term memory mechanism to reduce computation and latency in LLM serving, and compression/eviction strategies substantially affect efficiency.",
            "limitations_or_challenges": "Compression may hurt fidelity; eviction and sharing across requests complicate correctness and latency tradeoffs.",
            "uuid": "e8218.7",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Prompt Cache",
            "name_full": "Prompt Cache",
            "brief_description": "A sentence-level caching technique that stores prior input prompts and corresponding outputs to avoid recomputation when similar prompts recur, improving latency and cost.",
            "citation_title": "Prompt cache: Modular attention reuse for low-latency inference.",
            "mention_or_use": "mention",
            "agent_name": "Prompt Cache",
            "agent_description": "Caching layer that maps input prompts to outputs (or intermediate states) and returns cached responses for repeated or similar prompts.",
            "model_name": null,
            "model_description": null,
            "task_name": "Dialogue / repeated-query inference",
            "task_description": "Reduce compute and latency for repeated or near-duplicate prompts by returning cached outputs or reusing cached context.",
            "task_type": "inference acceleration / caching",
            "memory_used": true,
            "memory_type": "parametric short-term (prompt-level cache)",
            "memory_mechanism": "Store full prompt -&gt; output mappings (or prefix KV states) and reuse when similar prompts are seen.",
            "memory_representation": "Cached prompts and outputs or corresponding prefix KV cache entries.",
            "memory_retrieval_method": "Exact or approximate matching of current prompt to cached entries (recency and similarity-based).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey notes prompt caching reduces API call cost and latency; detailed measurements are in original engineering papers.",
            "key_findings": "Prompt-level caching is effective engineering leverage to speed up common repeated interactions and to reduce computational cost.",
            "limitations_or_challenges": "Cache hit rate depends on prompt repetitiveness; correctness under near-matches requires careful handling.",
            "uuid": "e8218.8",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ChunkKV",
            "name_full": "ChunkKV",
            "brief_description": "A KV cache compression method that groups tokens into semantic chunks and enables layer-wise index reuse to compress attention caches for long-context inference.",
            "citation_title": "ChunkKV: Semantic-preserving KV cache compression for efficient long-context LLM inference.",
            "mention_or_use": "mention",
            "agent_name": "ChunkKV",
            "agent_description": "Compression strategy for KV caches that retains the most informative chunks to reduce memory and computation while preserving quality.",
            "model_name": null,
            "model_description": null,
            "task_name": "Long-context LLM inference / long-text generation",
            "task_description": "Reduce KV cache memory footprint and inference cost for long-context sequences while maintaining generation quality.",
            "task_type": "inference optimization / cache compression",
            "memory_used": true,
            "memory_type": "parametric short-term (compressed KV cache)",
            "memory_mechanism": "Semantic chunking of tokens and layer-wise reuse of compressed indices to decrease KV cache size.",
            "memory_representation": "Compressed KV representations corresponding to semantic token chunks.",
            "memory_retrieval_method": "Index-based retrieval of compressed chunks for attention computation.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey reports ChunkKV reduces memory/computation and outperforms some prior compression baselines on benchmarks (no numeric details provided here).",
            "key_findings": "Semantic-aware KV compression can reduce resource usage in long-context inference while retaining performance better than nave compression.",
            "limitations_or_challenges": "Choosing chunk granularity and maintaining alignment with attention computations are nontrivial.",
            "uuid": "e8218.9",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "MemoryLLM",
            "name_full": "MemoryLLM",
            "brief_description": "A class of approaches that treat an LLM itself as a parametric long-term memory, enabling self-updates and knowledge injections into model parameters for persistent knowledge retention.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "MemoryLLM",
            "agent_description": "LLMs augmented with mechanisms to inject, edit, and retain long-term knowledge in parameters (parameterized memory / lifelong editing frameworks).",
            "model_name": null,
            "model_description": null,
            "task_name": "Lifelong knowledge editing / continual adaptation",
            "task_description": "Integrate new facts or user-specific information into model parameters for durable retention across sessions and tasks.",
            "task_type": "parametric long-term memory / knowledge editing",
            "memory_used": true,
            "memory_type": "parametric long-term (model parameter edits)",
            "memory_mechanism": "Parameter-efficient fine-tuning, memory injection, or dual-memory designs (main + side memory) with routing to access edited knowledge at inference.",
            "memory_representation": "Edited model weights (parameterized knowledge) or side-memory parameter blocks.",
            "memory_retrieval_method": "Internalized via forward pass (no external retrieval) with routing mechanisms to consult edited memory partitions.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey references MemoryLLM and WISE showing parametric edits can preserve and route new knowledge; details and ablations are in cited works.",
            "key_findings": "Parametric memory enables compressed, durable retention of knowledge but faces scalability and personalization cost challenges.",
            "limitations_or_challenges": "Fine-tuning per-user or frequent editing is computationally expensive and raises scalability, stability, and privacy issues.",
            "uuid": "e8218.10",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "WISE",
            "name_full": "WISE",
            "brief_description": "A lifelong editing framework for LLMs using dual-parametric memory (main + side memory) and routing/knowledge sharding to support reliable continual edits.",
            "citation_title": "WISE: Rethinking the knowledge memory for lifelong model editing of large language models",
            "mention_or_use": "mention",
            "agent_name": "WISE",
            "agent_description": "Framework that separates preserved pretrained knowledge (main memory) from edited knowledge (side memory), using routing to select appropriate memory during inference.",
            "model_name": null,
            "model_description": null,
            "task_name": "Model editing / lifelong knowledge updates",
            "task_description": "Inject and manage edits to LLM knowledge while preserving original capabilities and ensuring locality and generalization of edits.",
            "task_type": "parametric long-term memory / model editing",
            "memory_used": true,
            "memory_type": "parametric long-term (dual-memory parameter blocks)",
            "memory_mechanism": "Maintain main pretrained parameters and separate edited side memory; use routing to access the correct memory at inference; knowledge sharding for edit distribution.",
            "memory_representation": "Parameter partitions for main and edited knowledge.",
            "memory_retrieval_method": "Routing mechanism during inference to consult side memory when relevant.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey highlights dual-memory approach as improving edit reliability and generalization; original paper contains detailed evaluations.",
            "key_findings": "Dual-memory designs help maintain pretrained knowledge while enabling reliable edits; routing and sharding important for scalability.",
            "limitations_or_challenges": "Engineering complexity, routing accuracy, and bookkeeping for many edits.",
            "uuid": "e8218.11",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Echo",
            "name_full": "Echo",
            "brief_description": "An LLM augmented with temporal episodic memory to improve performance on multi-turn, complex memory-based dialogues by maintaining temporally structured episodic traces.",
            "citation_title": "Echo: A large language model with temporal episodic memory",
            "mention_or_use": "mention",
            "agent_name": "Echo",
            "agent_description": "Model that integrates temporal episodic memory storage and retrieval mechanisms to better handle tasks requiring recall of temporally ordered events.",
            "model_name": null,
            "model_description": null,
            "task_name": "Temporal multi-turn dialogue / episodic memory tasks",
            "task_description": "Recall temporally structured user events or conversation history to sustain coherent long-term dialogues.",
            "task_type": "personalized long-term dialogue / episodic recall",
            "memory_used": true,
            "memory_type": "parametric long-term / non-parametric episodic (depending on implementation)",
            "memory_mechanism": "Temporal episodic traces stored and retrieved to inform generation; details described in original Echo work.",
            "memory_representation": "Temporally indexed episodes (summaries, events) and possibly parameterized memory augmentation.",
            "memory_retrieval_method": "Temporal-aware retrieval of episodes relevant to current query (semantic + temporal proximity).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey mentions Echo as example of temporal episodic memory improving dialogue tasks; numerical comparisons are in the source paper.",
            "key_findings": "Temporal episodic memory structures help LLMs maintain coherence and personal details over long conversations.",
            "limitations_or_challenges": "Designing temporal indexing and scalable retrieval; tradeoffs between fidelity and storage cost.",
            "uuid": "e8218.12",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "MemReasoner",
            "name_full": "MemReasoner",
            "brief_description": "A memory-augmented LLM architecture designed to support multi-hop reasoning by storing and retrieving intermediate facts and reasoning chains.",
            "citation_title": "Memreasoner: A memory-augmented llm architecture for multi-hop reasoning",
            "mention_or_use": "mention",
            "agent_name": "MemReasoner",
            "agent_description": "Architecture that augments an LLM with an explicit memory component to record and retrieve intermediate reasoning steps and facts for complex multi-hop problems.",
            "model_name": null,
            "model_description": null,
            "task_name": "Multi-hop reasoning / multi-step QA",
            "task_description": "Solve problems that require chaining multiple reasoning steps and retrieving intermediate facts to reach an answer.",
            "task_type": "multi-hop reasoning / question answering",
            "memory_used": true,
            "memory_type": "system non-parametric short/long-term (intermediate reasoning memory)",
            "memory_mechanism": "Store intermediate facts and reasoning chains; retrieve relevant stored reasoning fragments to support multi-hop inference.",
            "memory_representation": "Intermediate reasoning chains, extracted facts, and supporting evidence.",
            "memory_retrieval_method": "Semantic search / similarity retrieval over stored reasoning fragments.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey lists MemReasoner as an architecture for improved multi-hop capability; detailed experimental ablations are in the original work.",
            "key_findings": "Explicitly recording intermediate reasoning can improve multi-hop reasoning and reduce error propagation.",
            "limitations_or_challenges": "Memory growth, retrieval precision, and integration of retrieved fragments into coherent final outputs.",
            "uuid": "e8218.13",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Generative Agents",
            "name_full": "Generative Agents",
            "brief_description": "Simulated agents that use memory of observations, interactions, and reflections to produce human-like behavior over time in sandbox environments.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents",
            "agent_description": "Agents that maintain personal memories (observations, events, plans) and use retrieval and reflection to inform actions and interactions in simulated social environments.",
            "model_name": null,
            "model_description": null,
            "task_name": "Social simulation / long-horizon interactive behavior",
            "task_description": "Sustain believable, temporally coherent agent behavior in a simulated environment by leveraging stored episodic memories and reflections.",
            "task_type": "simulation / long-term behavior modeling",
            "memory_used": true,
            "memory_type": "non-parametric long-term personal memory (episodic store)",
            "memory_mechanism": "Record observations and interactions; index and retrieve salient memories, use reflection to summarize and prioritize memories for future behavior.",
            "memory_representation": "Episodic event records, reflections, plans, and salient summaries.",
            "memory_retrieval_method": "Retrieval based on relevance to current context (semantic + salience heuristics).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey cites Generative Agents as demonstrating that explicit episodic memories and reflection improve long-term believable behavior; no numeric ablation included here.",
            "key_findings": "Structured personal episodic memories plus reflection mechanisms enable more coherent, personalized agent behavior over extended interactions.",
            "limitations_or_challenges": "Scalability of storing many events, privacy of stored personal data, and retrieval accuracy.",
            "uuid": "e8218.14",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "MemGPT",
            "name_full": "MemGPT",
            "brief_description": "An approach framing LLMs as operating-system-like managers that maintain and use memory to support agentic behaviors (read-write memory interface).",
            "citation_title": "Memgpt: Towards llms as operating systems",
            "mention_or_use": "mention",
            "agent_name": "MemGPT",
            "agent_description": "Conceptual/architectural approach that gives an LLM an explicit memory interface to read, write, and manage long-term memory for continuous agentic tasks.",
            "model_name": null,
            "model_description": null,
            "task_name": "Autonomous multi-step agent tasks / long-term personal assistants",
            "task_description": "Coordinate complex tasks over time by persisting state and retrieving relevant memories.",
            "task_type": "autonomous agent / long-term memory management",
            "memory_used": true,
            "memory_type": "non-parametric long-term (read-write memory store)",
            "memory_mechanism": "Expose read-write memory APIs for LLM to commit and query memory entries; use retrieved memory to inform subsequent planning and execution.",
            "memory_representation": "Stored events, facts, summaries accessible via read-write operations.",
            "memory_retrieval_method": "Semantic retrieval / read operations invoked by agent when relevant.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey mentions MemGPT conceptually as enabling OS-like memory but does not report numerical comparisons.",
            "key_findings": "Providing LLMs with read-write memory interfaces facilitates persistent behavior and stateful agent operation.",
            "limitations_or_challenges": "Designing safe, scalable read-write semantics and avoiding memory corruption or privacy leaks.",
            "uuid": "e8218.15",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "RETRO / RAGCache (caching in RAG)",
            "name_full": "RAGCache (multilevel dynamic caching for Retrieval-Augmented Generation)",
            "brief_description": "A multilevel dynamic caching system tailored for retrieval-augmented generation that caches intermediate knowledge states and overlaps retrieval with inference to reduce latency and improve throughput.",
            "citation_title": "RAGCache: Efficient knowledge caching for retrieval-augmented generation.",
            "mention_or_use": "mention",
            "agent_name": "RAGCache",
            "agent_description": "Caching mechanism that stores retrieved documents / intermediate retrieval states for reuse across generation requests to reduce retrieval latency in RAG systems.",
            "model_name": null,
            "model_description": null,
            "task_name": "Retrieval-augmented generation / knowledge-intensive QA",
            "task_description": "Reduce latency and improve throughput by caching retrieved knowledge and reusing it during inference.",
            "task_type": "retrieval-augmented generation",
            "memory_used": true,
            "memory_type": "system non-parametric short-term (retrieval cache)",
            "memory_mechanism": "Multi-level caching of retrieved documents and intermediate knowledge states; special replacement policies informed by LLM retrieval/inference patterns.",
            "memory_representation": "Cached retrieved passages and intermediate retrieval results.",
            "memory_retrieval_method": "Cache lookup by query similarity; overlap retrieval with inference.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey reports RAGCache reduces latency and improves throughput in RAG settings; numerical evaluations are in the cited work.",
            "key_findings": "Intelligent caching in RAG pipelines yields large latency and throughput benefits compared to fresh retrieval each request.",
            "limitations_or_challenges": "Cache staleness, replacement policy design, and consistency with underlying corpora updates.",
            "uuid": "e8218.16",
            "source_info": {
                "paper_title": "From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory.",
            "rating": 2,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Buffer of Thoughts: Thought-augmented reasoning with large language models",
            "rating": 2,
            "sanitized_title": "buffer_of_thoughts_thoughtaugmented_reasoning_with_large_language_models"
        },
        {
            "paper_title": "HippoRAG: Neurobiologically inspired long-term memory for large language models",
            "rating": 2,
            "sanitized_title": "hipporag_neurobiologically_inspired_longterm_memory_for_large_language_models"
        },
        {
            "paper_title": "Ret-llm: Towards a general read-write memory for large language models",
            "rating": 2,
            "sanitized_title": "retllm_towards_a_general_readwrite_memory_for_large_language_models"
        },
        {
            "paper_title": "Memreasoner: A memory-augmented llm architecture for multi-hop reasoning",
            "rating": 2,
            "sanitized_title": "memreasoner_a_memoryaugmented_llm_architecture_for_multihop_reasoning"
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 1,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        }
    ],
    "cost": 0.0233565,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs</p>
<p>Yaxiong Wu wu.yaxiong@huawei.com 
Huawei Noah's Ark Lab</p>
<p>Sheng Liang 
Huawei Noah's Ark Lab</p>
<p>Chen Zhang 
Huawei Noah's Ark Lab</p>
<p>Yichao Wang 
Huawei Noah's Ark Lab</p>
<p>Yongyue Zhang 
Huawei Noah's Ark Lab</p>
<p>Huifeng Guo 
Huawei Noah's Ark Lab</p>
<p>Ruiming Tang 
Huawei Noah's Ark Lab</p>
<p>Yong Liu 
Huawei Noah's Ark Lab</p>
<p>From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era of LLMs
E574540AB72DCCC2DB6E8D6B56A8C8EC
Memory is the process of encoding, storing, and retrieving information, allowing humans to retain experiences, knowledge, skills, and facts over time, and serving as the foundation for growth and effective interaction with the world.It plays a crucial role in shaping our identity, making decisions, learning from past experiences, building relationships, and adapting to changes.In the era of large language models (LLMs), memory refers to the ability of an AI system to retain, recall, and use information from past interactions to improve future responses and interactions.Although previous research and reviews have provided detailed descriptions of memory mechanisms, there is still a lack of a systematic review that summarizes and analyzes the relationship between the memory of LLM-driven AI systems and human memory, as well as how we can be inspired by human memory to construct more powerful memory systems.To achieve this, in this paper, we propose a comprehensive survey on the memory of LLM-driven AI systems.In particular, we first conduct a detailed analysis of the categories of human memory and relate them to the memory of AI systems.Second, we systematically organize existing memory-related work and propose a categorization method based on three dimensions (object, form, and time) and eight quadrants.Finally, we illustrate some open problems regarding the memory of current AI systems and outline possible future directions for memory in the era of large language models.</p>
<p>Introduction</p>
<p>Recently, large language models (LLMs) have become the core component of AI systems due to their powerful language understanding and generation capabilities, and are widely used in various applications such as intelligent customer service, automated writing, machine translation, information retrieval, and sentiment analysis [1][2][3][4].Unlike traditional AI systems, which rely on predefined rules and manually labeled features, LLM-driven AI systems offer greater flexibility, handling a diverse range of tasks with enhanced adaptability and contextual awareness.Moreover, the introduction of memory enables LLMs to retain historical interactions with users and store contextual information, thereby providing more personalized, continuous, and context-aware responses in future interactions [2,5,6].AI systems powered by LLMs with memory capabilities will not only elevate the user experience but also support more complex and dynamic use cases, steering AI technology toward greater intelligence and human-centric design [7,8].</p>
<p>In neuroscience, human memory refers to the brain's ability to store, retain, and recall information [9,10].Human memory serves as the foundation for understanding the world, learning new knowledge, adapting to the environment, and making decisions, allowing us to preserve past experiences, skills, and knowledge, and helping us form our personal identity and behavior patterns [11].Human memory can be broadly classified into short-term memory and long-term memory based on the duration of new memory formation [12].Short-term memory refers to the information we temporarily store and process, typically lasting from a few seconds to a few minutes, and includes sensory memory and working memory [11].Long-term memory refers to the information we can store for extended periods, ranging from minutes to years, and includes declarative explicit memory (such as episodic and semantic memory) and non-declarative implicit memory (such as conditioned reflexes and procedural memory) [11].Human memory is a complex and dynamic process that relies on different memory systems to process information for various purposes, influencing how we understand and respond to the world.The different types of human memory and their working mechanisms can greatly inspire us to develop more scientific and reasonable memory-enhanced AI systems [13][14][15][16].</p>
<p>In the era of large language models (LLMs), the most typical memory-enhanced AI system is the LLM-powered autonomous agent system [10].Large language model (LLM) powered agents are AI systems that can perform complex tasks using natural language, incorporating capabilities like planning, tool use, memory, and multi-step reasoning to enhance interactions and problemsolving [1,2,10].This memory-enhanced AI system is capable of autonomously decomposing complex tasks, remembering interaction history, and invoking and executing tools, thereby efficiently completing a series of intricate tasks.In particular, memory, as a key component of the LLM-powered agent, can be defined as the process of acquiring, storing, retaining, and subsequently retrieving information [10].It enables the large language model to overcome the limitation of LLM's context window, allowing the agent to recall interaction history and make more accurate and intelligent decisions.For instance, MemoryBank [17] proposed a long-term memory mechanism to allow LLMs for retrieving relevant memories, continuously evolving through continuous updates, and understanding and adapting to a user's personality by integrating information from previous interactions.In addition, many commercial and open-source AI systems have also integrated memory systems to enhance the personalization capabilities of the system, such as OpenAI ChatGPT Memory [18], Apple Personal Context [19], mem0 [20], MemoryScope [21], etc.</p>
<p>Although previous studies and reviews have provided detailed explanations of memory mechanisms, most of the existing work focuses on analyzing and explaining memory from the temporal (time) dimension, specifically in terms of short-term and long-term memory [8,7,17].We believe that categorizing memory solely based on the time dimension is insufficient, as there are many other aspects (such as object and form) to memory classification in AI systems.For example, from the object dimension, since AI systems often interact with humans, they need to perceive, store, recall, and use memories related to individual users, thus generating personal memories.Meanwhile, when AI systems perform complex tasks, they generate intermediate results (such as reasoning and planning processes, internet search results, etc.), which form system memory.In addition, from the form dimension, since AI systems are powered by large language models (LLMs), they can store memories through the parametric memory encoded within the model parameters, as well as through non-parametric memory in the form of external memory documents that are stored and managed outside the model.Therefore, insights that consider memory from the perspectives of object (personal and system), form (parametric and non-parametric), and time (short-term and long-term) are still lacking in the current era of large language models.There is still no comprehensive review that systematically analyzes the relationship between memory in LLM-driven AI systems and human memory, and how insights from human memory can be leveraged to build more efficient and powerful memory systems.</p>
<p>To fill this gap, this paper presents a comprehensive review of the memory mechanisms in LLMdriven AI systems.First, we provide a detailed analysis of the categories of human memory and relate them to the memory systems in AI.In particular, we explore how human memory types -short-term memory (including sensory memory and working memory) and long-term memory (including explicit memory and implicit memory) -correspond to personal and system memory, parametric and non-parametric memory, and short-term and long-term memory in LLM-driven AI systems.Next, we systematically organize the existing work related to memory and propose a classification method based on three dimensions (object, form, and time) with eight quadrants.In the object dimension, memory can be divided into personal memory and system memory; in the form dimension, it can be classified into parametric memory and non-parametric memory; in the time dimension, memory can be categorized into short-term memory and long-term memory.Finally, based on the classification results from the three dimensions and eight quadrants mentioned above, we an-alyze some open issues in the memory of current AI systems and outline potential future directions for memory development in the era of large language models.</p>
<p>The main contributions of this paper are summarized as follows: (1) We systematically and comprehensively define LLM-driven AI systems' memory and establish corresponding relationships with human memory.(2) We propose a classification method for memory based on three dimensions (object, form, and time) and eight quadrants, which facilitates a more systematic exploration of memory in the era of large language models.(3) From the perspective of enhancing personalized capabilities, we analyze and summarize research related to personal memory.(4) From the perspective of AI system's ability to perform complex tasks, we analyze and summarize research related to system memory.(5) We identify the existing issues and challenges in current memory research and point out potential future directions for development.</p>
<p>The remainder of the paper is organized as follows: In Section 2, we present a detailed description of human memory and AI systems' memory, comparing their differences and relationships, and introduce the classification method for memory based on three dimensions (object, form, and time) and eight quadrants.In Section 3, we summarize research related to personal memory, aimed at enhancing the personalized response capabilities of AI systems.In Section 4, we summarize research related to system memory, aimed at improving AI systems' ability to perform complex tasks.In Section 5, we analyze some open issues related to memory and point out potential future directions for development.Finally, in Section 6, we conclude the survey.</p>
<p>Overview</p>
<p>The human brain has evolved complex yet efficient memory mechanisms over a long period, enabling it to encode, store, and recall information effectively [9].Accordingly, in the development of AI systems, we can draw insights from human memory to design effective &amp; efficient memory mechanisms or systems.In this section, we will first describe in detail the complex memory mechanisms and related memory systems of the human brain from the perspective of memory neuroscience.Then, we will discuss the memory mechanisms and types specific to LLM-driven AI systems.Finally, based on the memory features of LLM-driven AI systems, we will systematically review and categorize existing work from different dimensions.</p>
<p>Human Memory</p>
<p>Human memory typically relies on different memory systems to process information for various purposes, such as working memory for temporarily storing and processing information to support ongoing cognitive activities, and episodic memory for recording personal experiences and events for a long time [11].</p>
<p>Short-Term and Long-Term Memory</p>
<p>Based on the time range, human memory can be roughly divided into short-term memory and long-term memory according to the well-known Multi-Store Model (or Atkinson-Shiffrin Memory Model) [22].</p>
<p>Short-Term Memory Short-term memory is a temporary storage system that holds small amounts of information for brief periods, typically ranging from seconds to minutes.It includes sensory memory, which briefly captures raw sensory information from the environment (like sights or sounds), and working memory, which actively processes and manipulates information to complete tasks such as problem-solving or learning.Together, these components allow humans to temporarily hold and work with information before either discarding it or transferring it to long-term memory.</p>
<p> Sensory memory: Sensory memory is the brief storage of sensory information we acquire from the external world, including iconic memory (visual), echoic memory (auditory), haptic memory (touch), and other sensory data.It typically lasts only a few milliseconds to a few seconds.Some sensory memories are transferred to working memory, while others are eventually stored in long-term memory (such as episodic memory). Working memory: Working memory is the system we use to temporarily store and process information.It not only helps us maintain current thoughts but also plays a role in decision-making and problem-solving.For example, when solving a math problem, it allows us to keep track of both the problem and the steps involved in finding the solution.</p>
<p>Long-Term Memory Long-term memory is a storage system that holds information for extended periods, ranging from minutes to a lifetime.It includes explicit memory, which involves conscious recall of facts and events, and implicit memory, which involves unconscious skills and habits, like riding a bike.These two types work together to help humans retain knowledge, experiences, and learned abilities over time.</p>
<p> Explicit memory: Explicit memory, also known as declarative memory, refers to memories that we can easily verbalize or declare.It can be further divided into episodic memory and semantic memory.Episodic memory refers to memories related to personal experiences and events, such as what you had for lunch.This type of memory is typically broken down into stages like encoding, storage, and retrieval.Semantic memory, on the other hand, refers to memories related to facts and knowledge, such as knowing that the Earth is round or that the Earth orbits the Sun. Implicit memory: Implicit memory, also known as non-declarative memory, refers to memories that are difficult to describe in words.It is associated with habits, skills, and procedures, and does not require conscious recall.Procedural memory (or "muscle memory") is a typical form of implicit memory.It refers to memories gained through actions, such as riding a bicycle or playing the piano.The planning and coordination of movements are key components of procedural memory.</p>
<p>Multiple memory systems typically operate simultaneously, storing information in various ways across different brain regions.These memory systems are not completely independent; they interact with each other and, in many cases, depend on one another.For example, when you hear a new song, the sensory memory in your ears and the brain regions responsible for processing sound will become active, storing the sound of the song for a few seconds.This sound is then transferred to your working memory system.As you use your working memory and consciously think about the song, your episodic memory will automatically activate, recalling where you heard the song and what you were doing at the time.As you hear the song in different places and at different times, a new semantic memory gradually forms, linking the melody of the song with its title.So, when you hear the song again, you'll remember the song's title, rather than a specific instance from your multiple listening experiences.When you practice playing the song on the guitar, your procedural memory will remember the finger movements involved in playing the song.</p>
<p>Memory Mechanisms</p>
<p>Memory is the ability to encode, store and recall information.The three main processes involved in human memory are therefore encoding (the process of acquiring and processing information into a form that can be stored), storage (the retention of encoded information over time in short-term or long-term memory), and retrieval (recall, the process of accessing and bringing stored information back into conscious awareness when needed).</p>
<p> Encoding Memory encoding is the process of changing sensory information into a form that our brain can cope with and store effectively.In particular, there are different types of encoding in terms of how information is processed, such as visual encoding, which involves processing information based on its visual features like color, shape, or texture; acoustic encoding, which focuses on the auditory characteristics of information, such as pitch, tone, or rhythm; and semantic encoding, which is based on the meaning of the information, making it easier to structure and remember.In addition, there are many approaches to make our brain better at encoding memory, such as mnemonics, which involve using acronyms or peg-word systems to aid recall, chunking, where information is broken down into smaller, meaningful units to enhance retention, imagination, which strengthens encoding by linking images to words, and association, where new information is connected to prior knowledge to improve understanding and long-term memory storage. Storage The storage of memory involves the coordinated activity of multiple brain regions, with key areas including: the prefrontal cortex, which is associated with working memory and decision-making, helping us maintain and process information in the short term; the hippocampus, which helps organize and consolidate information to form new explicit memories (such as episodic memory); the cerebral cortex, which is involved in the storage and retrieval of semantic memory, allowing us to retain facts, concepts, and general knowledge over time; and the cerebellum, which is primarily responsible for procedural memory formed through repetition.</p>
<p> Retrieval Memory retrieval is the ability to access information and get it out of the memory storage.When we recall something, the brain reactivates neural pathways (also called synapses) linked to that memory.The prefrontal cortex helps in bringing memories back to awareness.Similarly, there are different types of memory retrieval, including recognition, where we identify previously encountered information or stimuli, such as recognizing a familiar face or a fact we have learned before; recall, which is the ability to retrieve information from memory without external cues, like remembering a phone number or address from memory; and relearning, a process in which we reacquire previously learned but forgotten information, often at a faster pace than initial learning due to the residual memory traces that still exist.</p>
<p>In addition to the fundamental memory processing stages of encoding, storage, and retrieval, human memory also includes consolidation (the process of stabilizing and strengthening memories to facilitate long-term storage), reconsolidation (the modification or updating of previously stored memories when they are reactivated, allowing them to adapt to new information or contexts), reflection (the active review and evaluation of one's memories to enhance self-awareness, improve learning strategies, and optimize decision-making), and forgetting (the process by which information becomes inaccessible).</p>
<p> Consolidation Memory consolidation refers to the process of converting short-term memory into long-term memory, allowing information to be stably stored in the brain and reducing the likelihood of forgetting.It primarily involves the hippocampus and strengthens neural connections through synaptic plasticity (strengthening of connections between neurons) and systems consolidation (the gradual transfer and reorganization of memories from the hippocampus to the neocortex for long-term storage).</p>
<p> Reconsolidation Memory reconsolidation refers to the process in which a previously stored memory is reactivated, entering an unstable state and requiring reconsolidation to maintain its storage.This process allows for the modification or updating of existing memories to adapt to new information or contexts, potentially leading to memory enhancement, weakening, or distortion.Once a memory is reactivated, it involves the hippocampus and amygdala and may be influenced by emotions, cognitive biases, or new information, resulting in memory adjustment or reshaping.</p>
<p> Reflection Memory reflection refers to the process in which an individual actively reviews, evaluates, and examines their own memory content and processes to enhance self-awareness, adjust learning strategies, or optimize decision-making.It helps improve metacognitive ability, correct memory biases, facilitate deep learning, and regulate emotions.This process primarily relies on the brain's metacognitive ability (Metacognition) and involves the prefrontal cortex, which monitors and regulates memory functions.</p>
<p> Forgetting Forgetting is a natural process that occurs when the brain fails to retrieve or retain information, which can result from encoding failure (when information is not properly encoded due to lack of attention or meaningful connection), memory decay (when memories fade over time without reinforcement as neural connections weaken), interference (when similar or new memories compete with or overwrite existing ones), retrieval failure (when information is inaccessible due to missing contextual cues despite being stored), or motivated forgetting (when individuals consciously suppress or unconsciously repress traumatic or distressing memories).However, forgetting is a natural and necessary process that enables our brains to filter out irrelevant and outdated information, allowing us to prioritize what is most important for our current needs.</p>
<p>Memory of LLM-driven AI Systems</p>
<p>Similar to humans, LLM-driven AI systems also rely on memory systems to encode, store and recall information for future use.A typical example is the LLM-driven agent system, which lever-ages memory to enhance the agent system's abilities in reasoning, planning, personalization, and more [10].</p>
<p>Fundamental Dimensions of AI Memory</p>
<p>The memory of an LLM-driven AI system is closely related to the features of the LLM, that define how information is processed, stored, and retrieved based on its architecture and capabilities.We primarily categorize and organize memory based on three dimensions: object (personal and system memory), form (non-parametric and parametric memory), and time (short-term and long-term memory).These three dimensions comprehensively capture what type of information is retained (object), how information is stored (form), and how long it is preserved (time), aligning with both the functional structure of LLMs and practical requirements for efficient recall and adaptability.</p>
<p>Object Dimension</p>
<p>The object dimension is closely tied to the interaction between LLM-driven AI systems and humans, as it defines how information is categorized based on its source and purpose.On one hand, the system receives human input and feedback (i.e., personal memory); on the other hand, it generates a series of intermediate output results during task execution (i.e., system memory).</p>
<p>Personal memory helps the system improve its understanding of user behavior and enhances its personalization capabilities, while system memory can strengthen the system's reasoning ability, such as in approaches like CoT (Chain-of-Thought) [23] and ReAct [24].</p>
<p>Form Dimension</p>
<p>The form dimension focuses on how memory is represented and stored in LLMdriven AI systems, shaping how information is encoded and retrieved.Some memory is embedded within the model's parameters through training, forming parametric memory, while other memory exists externally in structured databases or retrieval mechanisms, constituting non-parametric memory.Non-parametric memory serves as a supplementary knowledge source that can be dynamically accessed by the large language model, enhancing its ability to retrieve relevant information in realtime, as seen in retrieval-augmented generation (RAG) [25].</p>
<p>Time Dimension</p>
<p>The time dimension defines how long memory is retained and how it influences the LLM's interactions over different timescales.Short-term memory refers to contextual information temporarily maintained within the current conversation, enabling coherence and continuity in multi-turn dialogues.In contrast, long-term memory consists of information from past interactions that is stored in an external database and retrieved when needed, allowing the model to retain userspecific knowledge and improve personalization over time.This distinction ensures that the system can balance real-time responsiveness with accumulated learning for enhanced adaptability.</p>
<p>In addition to the three primary dimensions discussed above, memory can also be classified based on other criteria, such as modality, which distinguishes between unimodal memory (single data type) and multimodal memory (integrating multiple data types, such as text, images, and audio), or dynamics, which differentiates between static memory (fixed and unchanging) and streaming memory (dynamically updated in real-time).However, these alternative classifications are not considered the primary criteria here, as our focus is on the core structural aspects that most directly influence memory organization and retrieval in LLM-driven AI systems.</p>
<p>Parallels Between Human and AI Memory</p>
<p>The memory of LLM-driven AI system exhibits similarities to human memory in terms of structure and function.Human memory is generally categorized into short-term memory and long-term memory, a distinction that also applies to AI memory systems.Below, we draw a direct comparison between these categories, mapping human cognitive memory processes to their counterparts in intelligent AI systems.Figure 1 illustrates the parallels between human and AI memory.</p>
<p> Sensory Memory: When an LLM-driven AI system perceives external information, it converts inputs such as text, images, speech, and video into machine-processable signals.</p>
<p>This initial stage of information processing is analogous to human sensory memory, where raw data is briefly held before further cognitive processing.If these signals undergo additional processing, they transition into working memory, facilitating reasoning and decisionmaking.However, if no further processing or storage occurs, the information is quickly discarded, mirroring the transient nature of human sensory memory.</p>
<p>Figure 1: Illustrating the parallels between human and AI memory.</p>
<p> Working Memory: The working memory of an AI system serves as a temporary storage and processing mechanism, enabling real-time reasoning and decision-making.It encompasses personal memory, such as contextual information retained during multi-turn dialogues, and system memory, including the chain of thoughts generated during task execution.As a form of short-term memory, working memory can undergo further processing and consolidation, eventually transitioning into long-term memory (e.g., episodic memory) that can be retrieved for future use.Additionally, during inference, large language models generate intermediate computational results, such as KV-Caches, which act as a form of parametric short-term memory that enhances efficiency by accelerating the inference process.</p>
<p> Explicit Memory: The explicit memory of an AI system can be categorized into two distinct components.The first is non-parametric long-term memory, which involves the storage and retrieval of user-specific information, allowing the system to retain and utilize personalized data-analogous to episodic memory in humans.The second is parametric long-term memory, where factual knowledge and learned information are embedded within the model's parameters, forming an internalized knowledge base-corresponding to semantic memory in human cognition.Together, these components enable the system to recall past interactions and apply acquired knowledge effectively.</p>
<p> Implicit Memory: The implicit memory of an AI system encompasses the learned processes and patterns involved in task execution, enabling the development of specialized skills for specific tasks-analogous to procedural memory in humans.This form of memory can parallel the human process of learning from both successes and failures in a non-parameterized manner, involving the reflection and refinement of accumulated traces, which allows the retention and replication of effective strategies from past experiences.</p>
<p>Additionally, it can be encoded within the model's parameters, enabling the system to internalize task-related knowledge and perform operations efficiently without the need for explicit recall.</p>
<p>Beyond these parallels, insights from human memory can further guide the design of more effective and efficient AI memory systems, enhancing their ability to process, store, and retrieve information in a more structured and adaptive manner.</p>
<p>3D-8Q Memory Taxonomy</p>
<p>Building upon the three fundamental memory dimensions-object (personal &amp; system), form (nonparametric &amp; parametric), and time (short-term &amp; long-term)-as well as the established parallels between human and AI memory, we propose a three-dimensional, eight-quadrant (3D-8Q) memory taxonomy for AI memory.This memory taxonomy systematically categorizes AI memory based on its function, storage mechanism, and retention duration, providing a structured approach to understanding and optimizing AI memory systems.Table 1 presents the eight quadrants and their respective roles and functions.Table 1: Three-dimensional, eight-quadrant (3D-8Q) memory taxonomy for LLM-driven AI systems.</p>
<p>Next, we will provide insights and descriptions of existing works from the perspectives of personal memory (in Section 3) and system memory (in Section 4).In particualr, personal memory focuses more on the individual data perceived and observed by the model from the environment, while system memory emphasizes the system's internal or endogenous memory, such as the intermediate memory generated during task execution.</p>
<p>Personal Memory</p>
<p>Personal memory refers to the process of storing and utilizing human input and response data during interactions with an LLM-driven AI system.The development and application of personal memory play a crucial role in enhancing AI systems' personalization capabilities and improving user experience.In this section, we explore the concept of personal memory and relevant research, examining both non-parametric and parametric approaches to its construction and implementation.Table 2 shows the categories, features, and related research work of personal memory.</p>
<p>Contextual Personal Memory</p>
<p>In personal memory, the non-parametric contextual memory that can be loaded is generally divided into two categories: the short-term memory of the current session's multi-turn dialogue and the long-term memory of historical dialogues across sessions.The former can effectively supplement contextual information, while the latter can effectively fill in missing information and overcome the limitations of context length.</p>
<p>Loading Multi-Turn Dialogue (Quadrant-I)</p>
<p>In multi-turn dialogue scenarios, the conversation history of the current session can significantly enhance the LLM-driven AI system's understanding of the user's real-time intent, leading to more relevant and contextually appropriate responses.Many modern dialogue systems are capable of</p>
<p>Multi-Turn Dialogue</p>
<p>ChatGPT [26], DeepSeek-Chat [27], Claude [28], QWEN-CHAT [29], Llama 2-Chat [30], Gemini [31], PANGU-BOT [32], ChatGLM [33], OpenAssistant [34] II Personal Non-Parametric Long-Term Personal Assistant</p>
<p>ChatGPT Memory [18], Apple Intelligence [19], Microsoft Recall [35], Me.bot [36] Open-Source Framework MemoryScope [21], mem0 [20], Memary [37], LangGraph Memory [38], Charlie Mnemonic [39], Memobase [40], Letta [41], Cognee [42]</p>
<p>Construction MPC [43], RET-LLM [44], MemoryBank [17],</p>
<p>MemGPT [45], KGT [46], Evolving Conditional Memory [47], SECOM [48], Memory 3 [49], MemInsight [50] Management MemoChat [51], MemoryBank [17], RMM [52], LD-Agent [53], A-MEM [54], Generative Agents [55],</p>
<p>EMG-RAG [56], KGT [46], LLM-Rsum [57], COMEDY [58] Retrieval RET-LLM [44], ChatDB [59], Human-like Memory [60], HippoRAG [13], HippoRAG 2 [61], EgoRAG [62], MemInsight [50] Usage MemoCRS [63], RecMind [64], RecAgent [65], InteRecAgent [66], SCM [67], ChatDev [68], MetaAgents [69], S 3 [70], TradingGPT [71], Memolet [72],</p>
<p>Synaptic Resonance [14], MemReasoner [73] Benchmark MADial-Bench [74], LOCOMO [75], MemDaily [76], ChMapData [77], MSC [78], MMRC [79], Ego4D [80], EgoLife [62], BABILong [81,82] III Personal Parametric Short-Term</p>
<p>Caching for Acceleration</p>
<p>Prompt Cache [83], Contextual Retrieval [84] IV Personal Parametric Long-Term</p>
<p>Knowledge Editing</p>
<p>Character-LLM [85], AI-Native Memory [36], MemoRAG [86], Echo [87] Table 2: Personal Memory handling multi-turn conversations and fully consider the current dialogue context in their responses.Notable examples include ChatGPT [26], DeepSeek-Chat [27], and Claude [28], which excel at maintaining coherence and relevance over extended interactions.</p>
<p>For instance, ChatGPT [26] is a prime example of a multi-turn dialogue system where the conversation history of the current session serves as short-term memory, helping to supplement the contextual information of the dialogue.In ChatGPT, the dialogue memory is encoded in a role-content format, with distinct roles such as "User" and "Assistant".This encoding allows the system to maintain clarity regarding the speaker and the flow of the conversation.</p>
<p>Through effective dialogue management at different levels, including "Assistant", "Threads", "Messages", and "Runs", the system can precisely track the state of each turn and each step of the conversation, ensuring continuity and consistency in interactions.Additionally, when the conversation length becomes too extensive, the dialogue system manages the conversation's input by truncating the number of turns, thereby preventing the input from exceeding the model's length limitations.This ensures that the system can continue processing the dialogue without losing track of essential context, maintaining the effectiveness of multi-turn interactions.</p>
<p>Memory Retrieval-Augmented Generation (Quadrant-II)</p>
<p>In cross-session dialogue scenarios, retrieving relevant user long-term memories from historical conversations can effectively supplement missing information in the current session, such as personal preferences and character relationships.The advantage of memory retrieval-augmented generation is that large language models (LLMs) do not need to load all multi-session conversations.Given the limited length of LLMs' context windows-even when extended to millions of tokens-retrieving relevant information from historical sessions is also more efficient and cost-effective in terms of computation.In addition to multi-session conversations, long-term personal memory also encom-passes users' behavioral history, preferences, and interaction records with AI agents over an extended period of time.</p>
<p>By leveraging retrieval-augmented generation from long-term memory, LLM-driven AI systems can better tailor their responses and behaviors, thereby improving user satisfaction and engagement.For instance, a personal assistant that remembers a user's preferred news sources can prioritize those outlets in daily briefings, while a recommendation system that understands past viewing habits can suggest content more aligned with the user's tastes.Currently, many commercial and open-source platforms are striving to construct and utilize long-term memory for personalized AI systems-for example, ChatGPT Memory [18] and Me.bot [36] for personal assistants, and MemoryScope [21] and mem0 [20] as open-source frameworks.Long-term personal memory typically follows four core processing stages: construction, management, retrieval, and usage.The second section of Table 2 (organized by rows) provides an overview of existing work on personal non-parametric long-term memory, classified based on their primary contributions.</p>
<p>Construction The construction of user memory requires extraction and refinement from raw memory data, such as multi-turn conversations.This process is analogous to human memory consolidation-the process of stabilizing and strengthening memories to facilitate their long-term storage.</p>
<p>Well-organized long-term memory enhances both the efficiency of storage and the effectiveness of retrieval in user memory.For example, MemoryBank [17] leverages a memory module to store conversation histories and summaries of key events, enabling the construction of a long-term user profile.Similarly, RET-LLM [44] uses its memory module to retain essential factual knowledge about the external world, allowing the agent to monitor and update real-time environmental context relevant to the user.In addition, to accommodate different types of memory, a variety of storage formats have been developed, including key-value, graph, and vector representations.Specifically, key-value formats [44,50,63] enable efficient access to structured information such as user facts and preferences.Graph-based formats [46,13,61,20] are designed to capture and represent relationships among entities, such as individuals and events.Meanwhile, vector formats [17,48,20], which are typically derived from textual, visual, or audio memory representations, are utilized to encode the semantic meaning and contextual information of conversations.</p>
<p>Management The management of user memory involves further processing and refinement of previously constructed memories, such as deduplication, merging, and conflict resolution.This process is analogous to human memory reconsolidation and reflection, where existing memories are reactivated, updated, and integrated to maintain coherence and relevance over time.For instance, Reflective Memory Management (RMM) [52] is a user long-term memory management framework that combines Prospective Reflection for dynamic summarization with Retrospective Reflection for retrieval optimization via reinforcement learning.This dual-process approach addresses limitations such as rigid memory granularity and fixed retrieval mechanisms, enhancing the accuracy and flexibility of long-term memory management.LD-Agent [53] enhances long-term dialogue personalization and consistency by constructing personalized persona information for both users and agents through a dynamic persona modeling module, while integrating retrieved memories to optimize response generation.A-MEM [54] introduces a self-organizing memory system inspired by the Zettelkasten method [88], which constructs interconnected knowledge networks through dynamic indexing, linking, and memory evolution, enabling LLM agents to more flexibly organize, update, and retrieve long-term memories, thereby enhancing task adaptability and contextual awareness.In addition, MemoryBank [17] incorporates a memory updating mechanism inspired by the Ebbinghaus Forgetting Curve [89], allowing the AI to forget or reinforce memories based on the time elapsed and their relative importance, thereby enabling a more human-like memory system and enhancing the user experience.</p>
<p>Retrieval Retrieving personal memory involves identifying memory entries relevant to the user's current request, and the retrieval method is closely tied to how the memory is stored.For keyvalue memory, ChatDB [59] performs retrieval using SQL queries over structured databases.RET-LLM [44], on the other hand, employs a fuzzy search to retrieve triplet-structured memories, where information is stored as relationships between two entities connected by a predefined relation.For graph-based memory, HippoRAG [13] constructs knowledge graphs over entities, phrases, and summarization to recall more relative and comprehensive memories, while HippoRAG 2 [61] further combines original passages with phrase-based knowledge graphs to incorporate both conceptual and contextual information.For vector memory, MemoryBank [17] adopts a dual-tower dense retrieval model, similar to Dense Passage Retrieval [90], to accurately identify relevant memories.</p>
<p>The resulting vector representations are then indexed using FAISS [91] for efficient similarity-based retrieval.</p>
<p>Usage The use of personal memory can effectively empower downstream applications with personalization, enhancing the user's individualized experience.For instance, the recalled relevant memory is used as contextual information to enhance the personalized recommendation and response capability of the conversational recommender agents [63][64][65][66], improving the personalized user experience.In addition to memory-augmented personalized dialogue and recommendation, personal memory can also be leveraged to enhance a wide range of applications, including software development [68], social-network simulation [69,70], and financial trading [71].</p>
<p>To facilitate in-depth research on personal memory, a variety of memory-related benchmarks have emerged in recent years, including long-term conversational memory (MADial-Bench [74], LOCOMO [75], MSC [78]), everyday life memory (MemDaily [76]), memory-aware proactive dialogue (ChMapData [77]), multimodal dialogue memory (MMRC [79]), egocentric video understanding (Ego4D [80], EgoLife [62]), and long-context reasoning-in-a-haystack (BABI-Long [81,82]).</p>
<p>Parametric Personal Memory</p>
<p>In addition to external non-parametric memory, a user's personal memory can also be stored parametrically.Specifically, personal data can be used to fine-tune an LLM, embedding the memory directly into its parameters (i.e., parametric long-term memory) to create a personalized LLM .Alternatively, historical dialogues can be cached as prompts during inference (i.e., parametric shortterm memory), enabling quick reuse in future interactions.</p>
<p>Memory Caching For Acceleration (Quadrant-III)</p>
<p>Personal parametric short-term memory typically refers to intermediate attention states produced by the LLM when processing personal data, which is usually utilized as memory caches to accelerate inference.Specifically, prompt caching [83] is usually used as an efficient data management technique that allows for the pre-storage of large amounts of personal data or information that may be frequently requested, such as a user's conversational history.For instance, during multi-turn dialogues, the dialogue system can quickly provide the personal context information directly from the parametric memory cache, avoiding the need to recalculate or retrieve it from the original data source, saving both time and resources.Major platforms such as DeepSeek, Anthropic, OpenAI, and Google employ prompt caching to reduce API call costs and improve response speed in dialogue scenarios.Moreover, personal parametric short-term memory can enhance the performance of retrieval-augmented generation (RAG) through Contextual Retrieval [84], where prompt caching helps reduce the overhead of generating contextualized chunks.At present, research specifically targeting caching techniques for personal memory data remains limited.Instead, most existing work considers caching as a fundamental capability of system memory, particularly in the context of keyvalue (KV) management and KV reuse.A more detailed discussion of these aspects is provided in Section 4.</p>
<p>Personalized Knowledge Editing (Quadrant-IV)</p>
<p>Personal parametric long-term memory utilizes personalized Knowledge Editing technology [92], such as Parameter-Efficient Fine-Tuning (PEFT) [93], to encode personal data into the LLM's parameters in a parametric manner, thereby facilitating the long-term, parameterized storage of memory.For instance, Character-LLM [85] enables the role-playing of specific characters, such as Beethoven, Queen Cleopatra, Julius Caesar, etc., by training large language models to remember the roles and experiences of these characters.AI-Native Memory [36] proposes using deep neural network models, specifically large language models (LLMs), as Lifelong Personal Models (LPMs) to parameterize, compress, and continuously evolve personal memory through user interactions, enabling a more comprehensive understanding of the user.MemoRAG [86] utilizes LLM parametric memory to store user conversation history and preferences, forming a personalized global memory that enhances personalization and enables tailored recommendations.Echo [87] is a large language model enhanced with temporal episodic memory, designed to improve performance in applications requiring multi-turn, complex memory-based dialogues.The parameterization of personal longterm memory presents several challenges, notably the need to fine-tune models on individual user data, which demands substantial computational resources.This requirement significantly hinders the scalability and practical deployment of parametric approaches to long-term personal memory.</p>
<p>Discussion</p>
<p>In this section, we describe personal memory and related work from the perspectives of nonparametric and parametric approaches.Specifically, personal non-parametric short-term memory necessitates efficient mechanisms for memory encoding and management.Existing literature predominantly emphasizes the design and implementation of systems that facilitate the construction, management, retrieval, and effective utilization of a user's personal non-parametric long-term memory.In contrast, personal parametric short-term memory can employ techniques such as prompt caching to reduce computational costs and enhance efficiency.Parametric long-term memory offers advantages in memory compression, thereby supporting a more comprehensive and global representation of the user's accumulated experiences.Recent trends in the field indicate a growing interest in integrating both short-term and long-term memory paradigms, wherein parametric and non-parametric memory components complement and reinforce one another.The subsequent section will present a detailed discussion of system memory and its associated research developments.</p>
<p>System Memory</p>
<p>System memory constitutes a critical component of LLM-driven AI systems.It encompasses a sequence of intermediate representations or results generated throughout the task execution process.By leveraging system memory, LLM-driven AI systems can enhance their capabilities in reasoning, planning, and other higher-order cognitive functions.Moreover, the effective use of system memory contributes to the system's capacity for self-evolution and continual improvement.In this section, we examine system memory and its associated research from both non-parametric and parametric perspectives.
Quadrant Dimension Feature Models V System Non-Parametric Short-Term
Reasoning &amp; Planning Enhancement ReAct [24], RAP [94], Reflexion [95],</p>
<p>Talker-Reasoner [96], TPTU [97] VI System Non-Parametric Long-Term</p>
<p>Reflection &amp; Refinement</p>
<p>Buffer of Thoughts [98], AWM [99], Think-in-Memory [100], GITM [101], Voyager [102],</p>
<p>Retroformer [103], Expel [104], Synapse [105], MetaGPT [106], Learned Memory Bank [107], M+ [108] VII System Parametric Short-Term</p>
<p>KV Management</p>
<p>LookupFFN [109], ChunkKV [110], vLLM [111], FastServe [112], StreamingLLM [113], Orca [114], DistServe [115], LLM.int8() [116], FastGen [117], Train Large, Then Compress [118], Scissorhands [119],
H 2 O [120]
, Mooncake [121], MemServe [122], SLM Serving [123], IMPRESS [124], AdaServe [125], MPIC [126], IntelLLM [127] KV Reuse KV Cache [128], Prompt Cache [83], Contextual Retrieval [84], CacheGen [129], ChunkAttention [130],</p>
<p>RAGCache [131], SGLang [132], Ada-KV [133], HCache [134], Cake [135], EPIC [136], RelayAttention [137], Marconi [138], IKS [139], FastCache [140], Cache-Craft [141], KVLink [142],</p>
<p>RAGServe [143], BumbleBee [144] VIII System Parametric Long-Term</p>
<p>Parametric Memory Structures</p>
<p>Memorizing Transformer [145], Focused Transformer [146], MAC [147], MemoryLLM [148], WISE [149], LongMem [150], LM2 [151], Titans [152] Table 3: System Memory</p>
<p>Contextual System Memory</p>
<p>From a temporal perspective, non-parametric short-term system memory refers to a series of reasoning and action results generated by large language models during task execution.This form of memory supports enhanced reasoning and planning within the context of the current task, thereby contributing to improved task accuracy, efficiency, and overall completion rates.In contrast, nonparametric long-term system memory represents a more abstracted and generalized form of shortterm memory.It encompasses the consolidation of prior successful experiences and mechanisms of self-reflection based on historical interactions, which collectively facilitate the continual evolution and adaptive enhancement of LLM-driven AI systems.</p>
<p>Reasoning &amp; Planning Enhancement (Quadrant-V)</p>
<p>Analogous to human cognition, the reasoning and planning processes of large language models (LLMs) give rise to a sequence of short-term intermediate outputs.These outputs may reflect taskrelated attempts, which can be either successful or erroneous.Regardless of their correctness, such intermediate results serve as informative and constructive references that can guide subsequent task execution.This form of system non-parametric short-term memory plays a pivotal role in LLMdriven AI systems.Empirical evidence demonstrates that leveraging this memory structure significantly enhances the reasoning and planning capabilities of LLMs.For instance, ReAct [24] integrates reasoning and action by generating intermediate reasoning steps alongside corresponding actions, enabling the model to alternate between thought and execution.This approach facilitates intelligent planning and adaptive decision-making in complex problem-solving scenarios.Similarly, Reflexion [95] introduces mechanisms for dynamic memory and self-reflection, allowing the LLM to self-evaluate and iteratively refine its behavior based on prior errors or limitations.This self-improvement loop promotes enhanced performance in future tasks, resembling a continuous learning and optimization process.</p>
<p>Reflection &amp; Refinement (Quadrant-VI)</p>
<p>The development of system non-parametric long-term memory parallels the human process of learning from both successes and failures.It involves the reflection upon and refinement of accumulated short-term memory traces.This memory mechanism enables the system not only to retain and replicate effective strategies from past experiences but also to extract valuable lessons from failures, thereby minimizing the likelihood of repeated errors.Through continuous updating and optimization, the system incrementally enhances its decision-making capabilities and improves its responsiveness to novel challenges.Moreover, the progressive accumulation of long-term memory empowers the system to address increasingly complex tasks with greater adaptability and resilience.For instance, Buffer of Thoughts (BoT) [98] refines the chain of thoughts from historical tasks to form thought templates, which are then stored in a memory repository, guiding future reasoning and decision-making processes.Agent Workflow Memory (AWM) [99] introduces reusable paths, called workflows, and guides subsequent task generation by selecting different workflows.Thinkin-Memory (TiM) [100] continuously generates new thoughts based on conversation history, which is more conducive to reasoning and computation compared to raw observational data.Ghost in the Minecraft (GITM) [101] uses reference plans recorded in memory, allowing the agent planner to more efficiently handle encountered tasks, thereby improving task execution success rates.Voyager [102] refines skills based on environmental feedback and stores acquired skills in memory, forming a skill library for future reuse in similar situations (e.g., fighting zombies vs. fighting spiders).Retroformer [103] leverages recent interaction trajectories as short-term memory and reflective feedback from past failures as long-term memory to guide decision-making and reasoning.ExpeL [104] enhances task resolution by drawing on contextualized successful examples and abstracting insights from both successes and failures through comparative and pattern-based analysis of past experiences.</p>
<p>Parametric System Memory</p>
<p>The parametric system memory refers to the temporary storage of knowledge information in parametric forms, such as KV Cache [128], during the inference process (short-term memory), or the long-term editing and storage of knowledge information in the model parameters (long-term memory).The former, parametric short-term system memory, corresponds to human working memory, enabling cost reduction and efficiency improvement in large language model inference.The latter, parametric long-term system memory, corresponds to human semantic memory, facilitating the efficient integration of new knowledge.</p>
<p>KV Management &amp; Reuse (Quadrant-VII)</p>
<p>Parametric short-term system memory primarily focuses on the management and reuse of attention keys (Key) and values (Value) in LLMs, aiming to address issues such as high inference costs and latency during the reasoning process.KV management optimizes memory efficiency and inference performance through techniques such as KV cache organization [111], compression [110], and quantization [116].In particular, vLLM [111] is a high-efficiency LLM serving system built on PagedAttention, a virtual memory-inspired attention mechanism that enables near-zero KV cache waste and flexible sharing across requests, substantially improving batching efficiency and inference throughput.ChunkKV [110] is a method for compressing the key-value cache in long-context inference with LLMs by grouping tokens into semantic chunks, retaining the most informative ones, and enabling layer-wise index reuse, thereby reducing memory and computational costs while outperforming existing approaches on several benchmarks.LLM.int8() [116] is a mixed-precision quantization method that combines vector-wise Int8 quantization with selective 16-bit handling of emergent outlier features, enabling memory-efficient inference of large language models (up to 175B parameters) without performance degradation.</p>
<p>Meanwhile, KV reuse focuses on reusing inference-related parameters through token-level KV Cache [128] and sentence-level Prompt Cache [83], which helps reduce computational costs and improve the efficiency of large language model (LLM) usage.Specifically, KV Cache [128] stores the attention keys (Key) and values (Value) generated by the neural network during sequence generation, allowing them to be reused in subsequent inference steps.This reuse accelerates attention computation in long-text generation and reduces redundant computation.In contrast, Prompt Cache [83] operates at the sentence level by caching previous input prompts along with their corresponding output results.When similar prompts are encountered, the LLM can retrieve and return cached responses directly, saving computation and accelerating response generation.By avoiding frequent recomputation of identical or similar contexts, KV reuse enables more efficient inference and significantly reduces computational overhead.Additionally, it enhances the flexibility and responsiveness of LLM-based systems in handling continuous or interactive tasks.Building on these ideas, RAGCache [131] introduces a multilevel dynamic caching system tailored for Retrieval-Augmented Generation (RAG), which caches intermediate knowledge states, optimizes memory replacement policies based on LLM inference and retrieval patterns, and overlaps retrieval with inference to significantly reduce latency and improve throughput.</p>
<p>Parametric short-term system memory overlaps somewhat with the previously mentioned parametric short-term personal memory in terms of technical approach.The difference lies in their focus: parametric short-term personal memory is more concerned with improving the processing of individual input data, while parametric short-term system memory focuses on optimizing the storage and reuse of system-level context during task execution.The former primarily addresses how to quickly process and adapt to an individual's input information, whereas the latter aims to reduce inference costs in multi-turn reasoning and enhance the consistency and efficiency of global tasks.</p>
<p>Parametric Memory Structures (Quadrant-VIII)</p>
<p>From the perspective of large language models (LLM) as long-term parametric memory, LLMs are not merely tools that provide immediate responses based on input and output; they can also store and integrate information over long time spans, forming an ever-evolving knowledge system.LLMs based on the Transformer [153] architecture are capable of memorizing knowledge information, primarily due to the self-attention mechanism in the Transformer-based model and the large-scale parameterized training approach.By training on vast corpora, LLMs learn extensive world knowledge, language patterns, and solutions to various tasks.Additionally, LLMs can modify, update, or refine the internal knowledge through parameterized knowledge editing, allowing for more precise task handling or responses that better align with user needs.MemoryLLM [148] has the ability to self-update and inject memory with new knowledge, effectively integrating new information and demonstrating excellent model editing performance and long-term information retention capabilities.WISE [149] is a lifelong editing framework for large language models that employs a dual-parametric memory design, with the main memory preserving pretrained knowledge and the side memory storing edited information.It leverages a routing mechanism to dynamically access the appropriate memory during inference and uses knowledge sharding to distribute and integrate edits efficiently, ensuring reliability, generalization, and locality throughout continual updates.The core function of parameterized knowledge editing [92] is to enable large language models (LLMs) with dynamic and flexible knowledge updating capabilities, allowing them to respond to constantly changing task requirements, domain knowledge, and new information from the real world.This allows LLMs to remain efficient and accurate across various application scenarios and be customized and optimized according to user or environmental needs.</p>
<p>Discussion</p>
<p>In this section, we describe system memory and related work from the perspectives of nonparametric and parametric approaches.Non-parametric short-term system memory can enhance the reasoning and planning abilities for current tasks, while non-parametric long-term system memory enables the reuse of successful experiences and the self-reflection based on historical experience, facilitating the evolution of LLM-driven AI system capabilities.On the other hand, parametric short-term system memory can reduce costs and improve efficiency in large language model inference, and long-term parametric system memory can store and integrate information over long time spans, forming a continuously evolving knowledge system.In the next section, we will summarize the issues and challenges in memory research in the era of large language models and point out potential future directions for development.</p>
<p>Open Problems and Future Directions</p>
<p>Although substantial progress has been made in current memory research across the three dimensions-object, form, and time-as well as within the eight corresponding quadrants, numerous open issues and challenges remain.Building upon recent advancements and recognizing existing limitations, we outline the following promising directions for future research:</p>
<p>From Unimodal Memory to Multimodal Memory In the era of large language models, LLMdriven AI systems are gradually expanding from being able to process only a single type of data (such as text) to handle multiple types of data simultaneously (such as text, images, audio, video, and even sensor data).This transition enhances perceptual capabilities and enables robust performance in complex real-world tasks.For example, in the medical field, by combining text (medical records), images (medical imaging), and speech (doctor-patient conversations), AI systems can more accurately understand and diagnose medical conditions.Multimodal memory systems can integrate information from different sensory channels into a unified understanding, thereby approaching human cognitive processes more closely.Moreover, the expansion of multimodal memory also opens up possibilities for more personalized and interactive AI applications [154].For instance, personal AI assistants can not only communicate with users through text but also interpret users' emotions by recognizing facial expressions, voice intonations, or body language, thus providing more personalized and empathetic responses.</p>
<p>From Static Memory to Stream Memory Static memory can be viewed as a batch-processing approach to memory storage.It accumulates information or experiences in discrete batches, typically processing, storing, and retrieving them at specific intervals or predetermined points in time.</p>
<p>As an offline memory model, static memory emphasizes the systematic organization and consolidation of large volumes of information, making it well-suited for long-term knowledge retention and structured learning.In contrast, stream memory operates in a continuous, real-time manner.Analogous to data stream processing, it handles information as it arrives, prioritizing immediacy and adaptability.As an online or real-time memory model, stream memory focuses on the dynamic updating of information and rapid responsiveness to evolving contexts.These two memory paradigms are not mutually exclusive and often function complementarily: while static memory supports the accumulation of stable, long-term knowledge, stream memory enables agile adaptation to ongoing tasks and real-time information demands.</p>
<p>From Specific Memory to Comprehensive Memory The human memory system comprises multiple interconnected subsystems-such as sensory memory, working memory, explicit memory, and implicit memory-each fulfilling distinct functions and contributing to the overall cognitive process.In the context of large language models (LLMs), current memory architectures often concentrate on narrow or task-specific components, such as short-term memory for immediate inference or domainspecific knowledge storage.While such targeted memory mechanisms can enhance performance in specific scenarios, their limited scope constrains the system's overall flexibility, generalization, and adaptability.Looking forward, the development of comprehensive and collaborative memory systems is essential.These systems should integrate diverse memory types and support efficient interaction, self-organization, and updating, enabling LLMs to manage increasingly complex and dynamic tasks.By more closely emulating the multi-layered, multi-dimensional, and adaptive characteristics of human memory, such architectures have the potential to significantly advance the general intelligence and autonomy of LLM-based AI systems.</p>
<p>From Exclusive Memory to Shared Memory At present, the memory of each LLM-driven AI system operates independently, typically confined to a specific domain and tailored to processing isolated tasks or environments.However, as AI technologies continue to evolve, memory systems are expected to become increasingly interconnected, transcending domain boundaries and enabling enhanced collaboration among models.For instance, a large language model specialized in the medical domain could share its memory or knowledge base with another model focused on finance, facilitating cross-domain knowledge transfer and cooperative task solving.Such a shared memory paradigm would not only improve the efficiency and adaptability of individual systems but also empower multiple LLMs to dynamically access and leverage one another's domain-specific expertise.This shift toward collaborative memory architectures could give rise to a more intelligent, resource-efficient network of AI systems capable of addressing complex, multi-domain challenges.Ultimately, shared memory is poised to broaden the scope of AI applications and accelerate its integration into increasingly diverse and demanding real-world scenarios.</p>
<p>From Individual Privacy to Collective Privacy With the increasing prevalence of data sharing in the AI era, the focus of privacy protection is gradually shifting from the traditional notion of individual privacy to the broader and emerging concept of collective privacy.Conventional privacy frameworks primarily aim to safeguard personal data, preventing unauthorized access, leakage, or misuse of individually identifiable information.However, in the context of large language models, individual data is often aggregated into group-level datasets for large-scale analysis and prediction.Collective privacy concerns the protection of the rights and interests of groups or communities whose data is used collectively, raising questions about how to prevent misuse, profiling, or excessive surveillance at the group level.As memory systems in AI become more advanced and interconnected, ensuring collective privacy will emerge as a critical challenge.Addressing this issue will require innovative techniques that can effectively balance the trade-off between data utility and privacy preservation [155].</p>
<p>From Rule-Based Evolution to Automated Evolution Traditional AI systems evolve by reflecting on past experiences-such as reusing successful strategies-based on accumulated knowledge and historical data.However, this evolutionary process often depends on manually crafted rules and heuristic adjustments to enable such self-reflection.While rule-based evolution can be effective, it inherently limits the system's flexibility, scalability, and efficiency, with the quality and generalizability of the rules directly constraining the system's adaptive capabilities.Looking ahead, AI systems are expected to achieve automated evolution, dynamically adjusting and optimizing themselves by leveraging both personal and system-level memories in response to changing data and environmental contexts.Such systems will be capable of autonomously identifying performance bottlenecks and initiating self-improvement without relying on explicit, human-defined rules.This transition toward self-directed adaptation will significantly enhance system responsiveness, reduce the need for human intervention, and enable a more intelligent, dynamic, and continuously selfevolving paradigm.</p>
<p>Conclusion</p>
<p>Memory plays a pivotal role in the advancement of AI systems in the era of large language models (LLMs).It not only shapes the degree of personalization in AI behavior but also influences key capabilities such as adaptability, reasoning, planning, and self-evolution.This article systematically examines the relationship between human memory and memory mechanisms in LLM-driven AI systems, exploring how principles of human cognition can inspire the design of more efficient and flexible memory architectures.We begin by analyzing various categories of human memory-including perceptual memory, working memory, and long-term memory-and compare them with existing memory models in AI.Building upon this, we propose an eight-quadrant classification framework grounded in three dimensions: object, form, and time, offering a theoretical foundation for the construction of multi-level and comprehensive memory systems.Furthermore, we review the current state of memory development in AI from both personal memory and system memory perspectives.Finally, we identify key open challenges in contemporary AI memory design and outline promising directions for future research in the LLM era.We believe that, with continued technological progress, AI systems will increasingly adopt more dynamic, adaptive, and intelligent memory architectures, thereby enabling more robust applications across complex, real-world tasks.</p>
<p>Preprint. Under review.</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 1861863452024</p>
<p>Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, arXiv:2401.05459Personal llm agents: Insights and survey about the capability, efficiency and security. 2024arXiv preprint</p>
<p>A survey of large language models. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.1822312023arXiv preprint</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM transactions on intelligent systems and technology. 1532024</p>
<p>All roads lead to rome: Unveiling the trajectory of recommender systems across the llm era. Bo Chen, Xinyi Dai, Huifeng Guo, Wei Guo, Weiwen Liu, Yong Liu, Jiarui Qin, Ruiming Tang, Yichao Wang, Chuhan Wu, arXiv:2407.100812024arXiv preprint</p>
<p>A survey on multi-turn interaction capabilities of large language models. Chen Zhang, Xinyi Dai, Yaxiong Wu, Qu Yang, Yasheng Wang, Ruiming Tang, Yong Liu, arXiv:2501.099592025arXiv preprint</p>
<p>Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen, arXiv:2404.13501A survey on the memory mechanism of large language model based agents. 2024arXiv preprint</p>
<p>Long term memory: The foundation of ai self. Xun Jiang, Feng Li, Han Zhao, Jiaying Wang, Jun Shao, Shihao Xu, Shu Zhang, Weiling Chen, Xavier Tang, Yize Chen, arXiv:2410.156652024arXiv preprint</p>
<p>Human physiology: from cells to systems. Lauralee Sherwood, Robert Thomas Kell, Christopher Ward, 2004Thomson/Brooks/Cole</p>
<p>Llm-powered autonomous agents. lilianweng.github.io. Lilian Weng, Jun 2023</p>
<p>E Andrew, Elizabeth A Budson, Kensinger, Why we forget and how to remember better: the science behind memory. Oxford University Press2023</p>
<p>Working memory, thought, and action. Alan Baddeley, OuP Oxford. 452007</p>
<p>Jimnez Bernal, Yiheng Gutirrez, Yu Shu, Michihiro Gu, Yu Yasunaga, Su, arXiv:2405.14831Hipporag: Neurobiologically inspired long-term memory for large language models. 2024arXiv preprint</p>
<p>Exploring synaptic resonance in large language models: A novel approach to contextual memory integration. George Applegarth, Christian Weatherstone, Maximilian Hollingsworth, Henry Middlebrook, Marcus Irvin, arXiv:2502.106992025arXiv preprint</p>
<p>Key-value memory in the brain. Ila Samuel J Gershman, Kazuki Fiete, Irie, arXiv:2501.029502025arXiv preprint</p>
<p>Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, Yuheng Cheng, Suyuchen Wang, Xiaoqiang Wang, Yuyu Luo, Haibo Jin, Peiyan Zhang, Ollie Liu, Jiaqi Chen, Huan Zhang, Zhaoyang Yu, Haochen Shi, Boyan Li, Dekun Wu, Fengwei Teng, Xiaojun Jia, Jiawei Xu, Jinyu Xiang, Yizhang Lin, Tianming Liu, Tongliang Liu, Yu Su, Huan Sun, Glen Berseth, Jianyun Nie, Ian Foster, Logan Ward, Qingyun Wu, Yu Gu, Mingchen Zhuge, Xiangru Tang, Haohan Wang, Jiaxuan You, Chi Wang, Jian Pei, Qiang Yang, Xiaoliang Qi, and Chenglin Wu. Advances and challenges in foundation agents: From. 2025inspired intelligence to evolutionary, collaborative, and safe systems</p>
<p>Memorybank: Enhancing large language models with long-term memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Memory and new controls for chatgpt. openai.com. Openai, February 2024</p>
<p>Introducing apple intelligence, the personal intelligence system that puts powerful generative models at the core of iphone, ipad, and mac. apple.com. Apple, June 2024</p>
<p>mem0: The memory layer for personalized ai. mem0.ai. July 20240</p>
<p>Modelscope, Memoryscope, Equip your llm chatbot with a powerful and flexible long term memory system. github.com. September 2024</p>
<p>Human memory: A proposed system and its control processes. C Richard, Richard M Atkinson, Shiffrin, Psychology of learning and motivation. Elsevier19682</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-Tau Yih, Tim Rocktschel, Advances in neural information processing systems. 202033</p>
<p>. OpenAI. Introducing chatgpt. openai.com. November 2022</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>Introducing claude. anthropic.com. Anthropic, March 2023</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Pangu-bot: Efficient generative dialogue pre-training from pre-trained language model. Fei Mi, Yitong Li, Yulong Zeng, Jingyan Zhou, Yasheng Wang, Chuanfei Xu, Lifeng Shang, Xin Jiang, Shiqi Zhao, Qun Liu, arXiv:2203.170902022arXiv preprint</p>
<p>Glm Team, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, arXiv:2406.12793A family of large language models from glm-130b to glm-4 all tools. 2024arXiv preprint</p>
<p>Openassistant conversations-democratizing large language model alignment. Andreas Kpf, Yannic Kilcher, Dimitri Von Rtte, Sotiris Anagnostidis, Rui Zhi, Keith Tam, Abdullah Stevens, Duc Barhoum, Oliver Nguyen, Richrd Stanley, Nagyfi, Advances in Neural Information Processing Systems. 202336</p>
<p>Jingbo Shang, Zai Zheng, Jiale Wei, Xiang Ying, Felix Tao, Mindverse Team, arXiv:2406.18312Ai-native memory: A pathway from llms towards agi. 2024arXiv preprint</p>
<p>Beyond short-term memory: How memary makes chatbots remember. github.com. Memary, April 2024</p>
<p>Langgraph memory service. github.com. Ai Langchain, October 2024</p>
<p>. GoodAI. Charlie mnemonic. github.com. March 2024</p>
<p>Memobase: User profile-based memory for genai apps. memobase.io. January 2025</p>
<p>. Letta-Ai , Letta. github.com. September 2024</p>
<p>Prompted llms as chatbot modules for long open-domain conversation. Gibbeum Lee, Jongho Volker Hartmann, Dimitris Park, Kangwook Papailiopoulos, Lee, arXiv:2305.045332023arXiv preprint</p>
<p>Ret-llm: Towards a general read-write memory for large language models. Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, Hinrich Schtze, arXiv:2305.143222023arXiv preprint</p>
<p>Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, G Shishir, Ion Patil, Joseph E Stoica, Gonzalez, arXiv:2310.08560Memgpt: Towards llms as operating systems. 2023arXiv preprint</p>
<p>Knowledge graph tuning: Real-time large language model personalization based on human feedback. Jingwei Sun, Zhixu Du, Yiran Chen, arXiv:2405.196862024arXiv preprint</p>
<p>Ruifeng Yuan, Shichao Sun, Yongqi Li, Zili Wang, Ziqiang Cao, Wenjie Li, arXiv:2312.17257Personalized large language model assistant with evolving conditional memory. 2023arXiv preprint</p>
<p>On memory construction and retrieval for personalized conversational agents. Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew Lin, Vicky Zhao, Lili Qiu, arXiv:2502.055892025arXiv preprint</p>
<p>Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, arXiv:2407.01178Language modeling with explicit memory. 2024arXiv preprint</p>
<p>Meminsight: Autonomous memory augmentation for llm agents. Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, Yassine Benajiba, arXiv:2503.217602025arXiv preprint</p>
<p>Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, Yunsheng Wu, arXiv:2308.08239Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. 2023arXiv preprint</p>
<p>Zhen Tan, Jun Yan, Rujun Hsu, Zifeng Han, Long T Wang, Yiwen Le, Yanfei Song, Hamid Chen, George Palangi, Lee, arXiv:2503.08026prospect and retrospect: Reflective memory management for long-term personalized dialogue agents. 2025arXiv preprint</p>
<p>Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, Tat-Seng Chua, arXiv:2406.05925Hello again! llm-powered personalized agent for long-term dialogue. 2024arXiv preprint</p>
<p>A-mem: Agentic memory for llm agents. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang, arXiv:2502.121102025arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023</p>
<p>Crafting personalized agents through retrieval-augmented generation on editable memory graphs. Zheng Wang, Zhongyang Li, Zeren Jiang, Dandan Tu, Wei Shi, arXiv:2409.194012024arXiv preprint</p>
<p>Recursively summarizing enables long-term dialogue memory in large language models. Qingyue Wang, Liang Ding, Yanan Cao, Zhiliang Tian, Shi Wang, Dacheng Tao, Li Guo, arXiv:2308.150222023arXiv preprint</p>
<p>Compress to impress: Unleashing the potential of compressive memory in real-world long-term conversations. Nuo Chen, Hongguang Li, Juhua Huang, Baoyuan Wang, Jia Li, arXiv:2402.119752024arXiv preprint</p>
<p>Chatdb: Augmenting llms with databases as their symbolic memory. Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao, arXiv:2306.039012023arXiv preprint</p>
<p>my agent understands me better": Integrating dynamic human-like memory recall and consolidation in llm-based agents. Yuki Hou, Haruki Tamoto, Homei Miyashita, Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 2024</p>
<p>From rag to memory: Non-parametric continual learning for large language models. Jimnez Bernal, Yiheng Gutirrez, Weijian Shu, Sizhe Qi, Yu Zhou, Su, arXiv:2502.148022025arXiv preprint</p>
<p>Jingkang Yang, Shuai Liu, Hongming Guo, Yuhao Dong, Xiamengwei Zhang, Sicheng Zhang, Pengyun Wang, Zitang Zhou, Binzhu Xie, Ziyue Wang, arXiv:2503.03803Towards egocentric life assistant. 2025arXiv preprint</p>
<p>Memocrs: Memory-enhanced sequential conversational recommender systems with large language models. Yunjia Xi, Weiwen Liu, Jianghao Lin, Bo Chen, Ruiming Tang, Weinan Zhang, Yong Yu, Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. the 33rd ACM International Conference on Information and Knowledge Management2024</p>
<p>Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Yingzhen Yang, Recmind, arXiv:2308.14296Large language model powered agent for recommendation. 2023arXiv preprint</p>
<p>Lei Wang, Jingsen Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2306.02552Recagent: A novel simulation paradigm for recommender systems. 2023arXiv preprint</p>
<p>Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, Xing Xie, arXiv:2308.16505Recommender ai agent: Integrating large language models for interactive recommendations. 2023arXiv preprint</p>
<p>Enhancing large language model with self-controlled memory framework. Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, Zhoujun Li, arXiv:2304.133432023arXiv preprint</p>
<p>Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Communicative agents for software development. 2024. 20247924</p>
<p>Yuan Li, Yixuan Zhang, Lichao Sun, Metaagents, arXiv:2310.06500Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. 2023arXiv preprint</p>
<p>Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, Yong Li, arXiv:2307.14984Social-network simulation system with large language modelempowered agents. 20233arXiv preprint</p>
<p>Tradinggpt: Multiagent system with layered memory and distinct characters for enhanced financial trading performance. Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, Khaldoun Khashanah, arXiv:2309.037362023arXiv preprint</p>
<p>Memolet: Reifying the reuse of user-ai conversational memories. Ryan Yen, Jian Zhao, Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology. the 37th Annual ACM Symposium on User Interface Software and Technology2024</p>
<p>Memreasoner: A memory-augmented llm architecture for multi-hop reasoning. Ching-Yun Ko, Sihui Dai, Payel Das, Georgios Kollias, Subhajit Chaudhury, Aurelie Lozano, The First Workshop on System-2 Reasoning at Scale, NeurIPS'24. 2024</p>
<p>Madialbench: Towards real-world evaluation of memory-augmented dialogue generation. Junqing He, Liang Zhu, Rui Wang, Xi Wang, Reza Haffari, Jiaxing Zhang, arXiv:2409.152402024arXiv preprint</p>
<p>Evaluating very long-term conversational memory of llm agents. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang, arXiv:2402.177532024arXiv preprint</p>
<p>Zeyu Zhang, Quanyu Dai, Luyu Chen, Zeren Jiang, Rui Li, Jieming Zhu, Xu Chen, Yi Xie, Zhenhua Dong, Ji-Rong Wen, arXiv:2409.20163Memsim: A bayesian simulator for evaluating memory of llm-based personal assistants. 2024arXiv preprint</p>
<p>Bowen Wu, Wenqing Wang, Haoran Li, Ying Li, Jingsong Yu, Baoxun Wang, arXiv:2503.05150Interpersonal memory matters: A new task for proactive dialogue utilizing conversational history. 2025arXiv preprint</p>
<p>Beyond goldfish memory: Long-term opendomain conversation. Jing Xu, Arthur Szlam, Jason Weston, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, Editors, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Haochen Xue, Feilong Tang, Ming Hu, Yexin Liu, Qidong Huang, Yulong Li, Chengzhi Liu, Zhongxing Xu, Chong Zhang, Chun-Mei Feng, arXiv:2502.11903A large-scale benchmark for understanding multimodal large language model in real-world conversation. 2025arXiv preprint</p>
<p>Ego4d: Around the world in 3,000 hours of egocentric video. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Babilong: Testing the limits of llms with long context reasoning-in-ahaystack. Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, Mikhail Burtsev, 2024</p>
<p>Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, Mikhail Burtsev, search of needles in a 10m haystack: Recurrent memory finds what llms miss. 2024</p>
<p>Prompt cache: Modular attention reuse for low-latency inference. In Gim, Guojun Chen, Seung-Seob Lee, Nikhil Sarda, Anurag Khandelwal, Lin Zhong, Proceedings of Machine Learning and Systems. Machine Learning and Systems20246</p>
<p>Introducing contextual retrieval. anthropic.com. Anthropic, September 2024</p>
<p>Character-llm: A trainable agent for role-playing. Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu, arXiv:2310.101582023arXiv preprint</p>
<p>Memorag: Moving towards next-gen rag via memory-inspired knowledge discovery. Hongjin Qian, Peitian Zhang, Zheng Liu, Kelong Mao, Zhicheng Dou, arXiv:2409.055912024arXiv preprint</p>
<p>Wentao Liu, Ruohua Zhang, Aimin Zhou, Feng Gao, Jiali Liu, arXiv:2502.16090Echo: A large language model with temporal episodic memory. 2025arXiv preprint</p>
<p>David Kadavy, Digital Zettelkasten: Principles, Methods, &amp; Examples. Kadavy, Inc2021</p>
<p>Replication and analysis of ebbinghaus' forgetting curve. M J Jaap, Joeri Murre, Dros, PloS one. 107e01206442015</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, S H Patrick, Ledell Lewis, Sergey Wu, Danqi Edunov, Wen-Tau Chen, Yih, EMNLP (1). 2020</p>
<p>Billion-scale similarity search with gpus. Jeff Johnson, Matthijs Douze, Herv Jgou, IEEE Transactions on Big Data. 732019</p>
<p>Knowledge editing for large language models: A survey. Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, Jundong Li, ACM Computing Surveys. 5732024</p>
<p>Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang, arXiv:2403.14608Parameter-efficient fine-tuning for large models: A comprehensive survey. 2024arXiv preprint</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Agents thinking fast and slow: A talker-reasoner architecture. Konstantina Christakopoulou, Shibl Mourad, Maja Matari, arXiv:2410.083282024arXiv preprint</p>
<p>Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Ziyue Li, Xingyu Zeng, arXiv:2308.03427Tptu: large language model-based ai agents for task planning and tool usage. 2023arXiv preprint</p>
<p>Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, Bin Cui, arXiv:2406.04271Buffer of thoughts: Thought-augmented reasoning with large language models. 2024arXiv preprint</p>
<p>. Zora Zhiruo, Wang , Jiayuan Mao, Daniel Fried, Graham Neubig, arXiv:2409.074292024Agent workflow memory. arXiv preprint</p>
<p>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang, arXiv:2311.08719Think-in-memory: Recalling and post-thinking enable llms with long-term memory. 2023arXiv preprint</p>
<p>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Wang, arXiv:2305.171442023arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, arXiv:2308.02151Retrospective large language agents with policy gradient optimization. 2023arXiv preprint</p>
<p>Expel: Llm agents are experiential learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Synapse: Trajectory-as-exemplar prompting with memory for computer control. Longtao Zheng, Rundong Wang, Xinrun Wang, Bo An, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352Meta programming for multi-agent collaborative framework. 2023arXiv preprint</p>
<p>Enhancing reasoning with collaboration and memory. Julie Michelman, Nasrin Baratalipour, Matthew Abueg, arXiv:2503.059442025arXiv preprint</p>
<p>Yu Wang, Dmitry Krotov, Yuanzhe Hu, Yifan Gao, Wangchunshu Zhou, Julian Mcauley, Dan Gutfreund, arXiv:2502.00592Rogerio Feris, and Zexue He. M+: Extending memoryllm with scalable long-term memory. 2025arXiv preprint</p>
<p>Lookupffn: making transformers compute-lite for cpu inference. Zhanpeng Zeng, Michael Davies, Pranav Pulijala, Karthikeyan Sankaralingam, Vikas Singh, International Conference on Machine Learning. PMLR2023</p>
<p>Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference. Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Bo Li, Xuming Hu, Xiaowen Chu, arXiv:2502.002992025arXiv preprint</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the 29th Symposium on Operating Systems Principles. the 29th Symposium on Operating Systems Principles2023</p>
<p>Fast distributed inference serving for large language models. Bingyang Wu, Yinmin Zhong, Zili Zhang, Shengyu Liu, Fangyue Liu, Yuanhang Sun, Gang Huang, Xuanzhe Liu, Xin Jin, arXiv:2305.059202023arXiv preprint</p>
<p>Efficient streaming language models with attention sinks. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis, arXiv:2309.174532023arXiv preprint</p>
<p>Orca: A distributed serving system for {Transformer-Based} generative models. Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, Byung-Gon Chun, 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 2022</p>
<p>{DistServe}: Disaggregating prefill and decoding for goodput-optimized large language model serving. Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, Hao Zhang, 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). 2024</p>
<p>8-bit matrix multiplication for transformers at scale. Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, Advances in neural information processing systems. 202235int8 (</p>
<p>Model tells you what to discard: Adaptive kv cache compression for llms. Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng Gao, arXiv:2310.018012023arXiv preprint</p>
<p>Train big, then compress: Rethinking model size for efficient training and inference of transformers. Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, Joey Gonzalez, International Conference on machine learning. PMLR2020</p>
<p>Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Advances in Neural Information Processing Systems. 202336Anastasios Kyrillidis, and Anshumali Shrivastava</p>
<p>H2o: Heavy-hitter oracle for efficient generative inference of large language models. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R, Clark Barrett, Advances in Neural Information Processing Systems. 202336</p>
<p>Mooncake: A kvcache-centric disaggregated architecture for llm serving. Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, Xinran Xu, arXiv:2407.000792024arXiv preprint</p>
<p>Memserve: Context caching for disaggregated llm serving with elastic memory pool. Cunchen Hu, Heyang Huang, Junhao Hu, Jiang Xu, Xusheng Chen, Tao Xie, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, arXiv:2406.175652024arXiv preprint</p>
<p>Towards pareto optimal throughput in small language model serving. Yue Pol G Recasens, Chen Zhu, Eun Wang, Kyung Lee, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll, Berral , Proceedings of the 4th Workshop on Machine Learning and Systems. the 4th Workshop on Machine Learning and Systems2024</p>
<p>{IMPRESS}: An {Importance-Informed}{Multi-Tier} prefix {KV} storage system for large language model inference. Weijian Chen, Shuibing He, Haoyang Qu, Ruidong Zhang, Siling Yang, Ping Chen, Yi Zheng, Baoxing Huai, Gang Chen, 23rd USENIX Conference on File and Storage Technologies (FAST 25). 2025</p>
<p>Adaserve: Slo-customized llm serving with fine-grained speculative decoding. Zikun Li, Zhuofu Chen, Remi Delacourt, Gabriele Oliaro, Zeyu Wang, Qinghan Chen, Shuhuai Lin, April Yang, Zhihao Zhang, Zhuoming Chen, arXiv:2501.121622025arXiv preprint</p>
<p>Mpic: Positionindependent multimodal context caching system for efficient mllm serving. Shiju Zhao, Junhao Hu, Rongxiao Huang, Jiaqi Zheng, Guihai Chen, arXiv:2502.019602025arXiv preprint</p>
<p>IntelLLM: Little hints make a big difference for LLM KV cache compression. Tinglong Li, Qiuyu Shao, 2024</p>
<p>Efficiently scaling transformer inference. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, Jeff Dean, Proceedings of Machine Learning and Systems. Machine Learning and Systems20235</p>
<p>. Yuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng, Yuyang Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, 2023Fast context loading for language model applications. CoRR</p>
<p>Chunkattention: Efficient self-attention with prefix-aware kv cache and two-phase partition. Lu Ye, Ze Tao, Yong Huang, Yang Li, arXiv:2402.152202024arXiv preprint</p>
<p>Ragcache: Efficient knowledge caching for retrieval-augmented generation. Zili Chao Jin, Xuanlin Zhang, Fangyue Jiang, Xin Liu, Xuanzhe Liu, Xin Liu, Jin, arXiv:2404.124572024arXiv preprint</p>
<p>Sglang: Efficient execution of structured language model programs. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, Clark Barrett, Ying Sheng, 2024</p>
<p>Ada-kv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference. Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, Kevin Zhou, arXiv:2407.115502024arXiv preprint</p>
<p>Fast state restoration in llm serving with hcache. Shiwei Gao, Youmin Chen, Jiwu Shu, arXiv:2410.050042024arXiv preprint</p>
<p>Shuowei Jin, Xueshen Liu, Qingzhao Zhang, Mao Morley, arXiv:2410.03065Compute or load kv cache? why not both?. 2024arXiv preprint</p>
<p>Epic: Efficient position-independent context caching for serving large language models. Junhao Hu, Wenrui Huang, Haoyi Wang, Weidong Wang, Tiancheng Hu, Qin Zhang, Hao Feng, Xusheng Chen, Yizhou Shan, Tao Xie, arXiv:2410.153322024arXiv preprint</p>
<p>Relayattention for efficient large language model serving with long system prompts. Lei Zhu, Xinjiang Wang, Wayne Zhang, Rynson Wh Lau, arXiv:2402.148082024arXiv preprint</p>
<p>Rui Pan, Zhuang Wang, Zhen Jia, Can Karakus, Luca Zancato, Tri Dao, Yida Wang, Ravi Netravali, Marconi, arXiv:2411.19379Prefix caching for the era of hybrid llms. 2024arXiv preprint</p>
<p>Accelerating retrieval-augmented generation. Derrick Quinn, Mohammad Nouri, Neel Patel, John Salihu, Alireza Salemi, Sukhan Lee, Hamed Zamani, Mohammad Alian, arXiv:2412.152462024arXiv preprint</p>
<p>Fastcache: Optimizing multimodal llm serving through lightweight kv-cache compression framework. Jianian Zhu, Hang Wu, Haojie Wang, Yinghui Li, Biao Hou, Ruixuan Li, Jidong Zhai, arXiv:2503.084612025arXiv preprint</p>
<p>Cache-craft: Managing chunk-caches for efficient retrieval-augmented generation. Shubham Agarwal, Sai Sundaresan, Subrata Mitra, Debabrata Mahapatra, Archit Gupta, Rounak Sharma, Joshua Nirmal, Tong Kapu, Shiv Yu, Saini, arXiv:2502.157342025arXiv preprint</p>
<p>Jingbo Yang, Bairu Hou, Wei Wei, Yujia Bao, Shiyu Chang, Kvlink, arXiv:2502.16002Accelerating large language models via efficient kv cache reuse. 2025arXiv preprint</p>
<p>Ragserve: Fast quality-aware rag systems with configuration adaptation. Siddhant Ray, Rui Pan, Zhuohan Gu, Kuntai Du, Ganesh Ananthanarayanan, Ravi Netravali, Junchen Jiang, arXiv:2412.105432024arXiv preprint</p>
<p>Bumblebee: Dynamic kv-cache streaming submodular summarization for infinite-context transformers. Lilly Kumari, Shengjie Wang, Tianyi Zhou, Nikhil Sarda, Anthony Rowe, Jeff Bilmes, First Conference on Language Modeling. 2024</p>
<p>. Yuhuai Wu, Markus N Rabe, Delesley Hutchins, Christian Szegedy, arXiv:2203.089132022Memorizing transformers. arXiv preprint</p>
<p>Focused transformer: Contrastive training for context scaling. Szymon Tworkowski, Konrad Staniszewski, Mikoaj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr Mio, Advances in Neural Information Processing Systems. 202436</p>
<p>Online adaptation of language models with a memory of amortized contexts. Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, Jonathan Richard Schwarz, arXiv:2403.043172024arXiv preprint</p>
<p>Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, arXiv:2402.04624Towards self-updatable large language models. 2024arXiv preprint</p>
<p>Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, arXiv:2405.14768Wise: Rethinking the knowledge memory for lifelong model editing of large language models. 2024arXiv preprint</p>
<p>Augmenting language models with long-term memory. Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei, Advances in Neural Information Processing Systems. 202336</p>
<p>Lm2: Large memory models for long context reasoning. Jikun Kang, Wenqi Wu, Filippos Christianos, Alex James Chan, Fraser David Greenlee, George Thomas, Marvin Purtorab, Andrew Toulis, Workshop on Reasoning and Planning for Large Language Models. 2025</p>
<p>Ali Behrouz, Peilin Zhong, Vahab Mirrokni, Titans, arXiv:2501.00663Learning to memorize at test time. 2024arXiv preprint</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Memoro: Using large language models to realize a concise interface for real-time memory augmentation. Samantha Wazeer Deen Zulfikar, Pattie Chan, Maes, Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. the 2024 CHI Conference on Human Factors in Computing Systems2024</p>
<p>Bo Wang, Weiyi He, Pengfei He, Shenglai Zeng, Zhen Xiang, Yue Xing, Jiliang Tang, arXiv:2502.13172Unveiling privacy risks in llm agent memory. 2025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
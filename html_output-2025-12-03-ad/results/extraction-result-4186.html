<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4186 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4186</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4186</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-265609385</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.01291v1.pdf" target="_blank">Opportunities for retrieval and tool augmented large language models in scientific facilities</a></p>
                <p><strong>Paper Abstract:</strong> Upgrades to advanced scientific user facilities such as next-generation x-ray light sources, nanoscience centers, and neutron facilities are revolutionizing our understanding of materials across the spectrum of the physical sciences, from life sciences to microelectronics. However, these facility and instrument upgrades come with a significant increase in complexity. Driven by more exacting scientific needs, instruments and experiments become more intricate each year. This increased operational complexity makes it ever more challenging for domain scientists to design experiments that effectively leverage the capabilities of and operate on these advanced instruments. Large language models (LLMs) can perform complex information retrieval, assist in knowledge-intensive tasks across applications, and provide guidance on tool usage. Using x-ray light sources, leadership computing, and nanoscience centers as representative examples, we describe preliminary experiments with a Context-Aware Language Model for Science (CALMS) to assist scientists with instrument operations and complex experimentation. With the ability to retrieve relevant information from facility documentation, CALMS can answer simple questions on scientific capabilities and other operational procedures. With the ability to interface with software tools and experimental hardware, CALMS can conversationally operate scientific instruments. By making information more accessible and acting on user needs, LLMs could expand and diversify scientific facilities’ users and accelerate scientific output.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4186.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4186.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CALMS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context-Aware Language Model for Science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval- and tool-augmented LLM agent developed in this work to help scientists with instrument operations, experiment design, document retrieval, conversational memory, and calling instrument/control APIs to execute experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CALMS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CALMS is a framework that composes (1) a large language model, (2) conversational history (moving window of prior turns), (3) semantic search over preprocessed document stores (embedding-based retrieval of top-N context chunks), and (4) callable experimental/software tools (described to the model) using a ReAct/Chain-of-Thought style workflow. Prompts include retrieved context and recent conversation; a parser recognizes model requests to call tools (e.g., Materials Project API, SPEC instrument control) and inserts tool outputs back into the prompt so the model continues reasoning and can issue further tool calls. Implementations compared GPT-3.5 Turbo and Vicuna models and executed an end-to-end task (set diffractometer to a Bragg peak) by chaining a Materials Project lattice-lookup tool and a SPEC motor-move tool.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo; Vicuna-13b-v1.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B; 13B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science, experimental physics (synchrotron/X-ray experiments), nanoscience, computational resources</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>CALMS as implemented was not used to extract quantitative scientific 'laws' from literature; limitations observed include LLM hallucinations without contextual retrieval, limited context-window size requiring chunking/embedding retrieval, open-source models (Vicuna) performing worse than GPT-3.5 on completeness and tool-call syntax adherence, and occasional inability of open-source models to follow structured JSON inputs for tool calls.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Direct comparisons were made between GPT-3.5 Turbo and an open-source Vicuna-13b model on Q&A, context retrieval, and tool usage tasks; GPT-3.5 performed better on completeness and reliably executed the automated instrument workflow, while Vicuna could not consistently execute the structured tool-call workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Opportunities for retrieval and tool augmented large language models in scientific facilities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4186.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4186.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (low-data discovery reference)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (as used in low-data discovery in chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work noting that generative LLMs can perform few-shot learning for tasks such as materials property prediction and inverse design, indicating potential to discover quantitative relations from small datasets or domain text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is GPT-3 all you need for low-data discovery in chemistry?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-3 (few-shot discovery examples, referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited in this paper as an example where LLMs (GPT-3) were shown to be fast learners requiring few training examples to gain domain expertise for materials property prediction and inverse design; mentioned as evidence that LLMs can support discovery tasks from small data or domain knowledge. The paper itself does not provide implementation details for that referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>chemistry, materials science</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>empirical property-prediction / inverse-design relationships (as characterized in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Mentioned as few-shot/fine-tuning and prompt-based use of an LLM to perform property prediction/inverse design (no detailed extraction pipeline described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>The present paper only cites this work as an example; it does not report specifics. More generally in this paper, authors emphasize LLM hallucination risk and the need for domain context/retrieval to avoid errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Opportunities for retrieval and tool augmented large language models in scientific facilities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4186.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4186.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow: Augmenting large language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific LLM agent that integrates external chemistry tools to perform tasks in materials design, organic synthesis, and drug discovery, combining planning (Chain-of-Thought) and tool utilization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemcrow</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChemCrow (referenced domain-specific agent)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as an example of a domain-specific LLM agent that accomplishes chemistry-related tasks by integrating multiple tools and combining planning via Chain-of-Thought reasoning and tool calls; cited to illustrate capabilities of tool-augmented LLM agents in scientific discovery contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>chemistry, materials science, drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>not specified in this paper (agent aimed at task completion in chemistry; may enable discovery of relationships via tool workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Tool integration plus LLM planning/CoT prompting (as described at high level in this paper); no details on extracting quantitative laws from literature provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Mentioned as illustrative prior work; this paper highlights general LLM failure modes (hallucination) and the need for retrieval/context to ensure faithfulness, which are relevant to agents like ChemCrow.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Opportunities for retrieval and tool augmented large language models in scientific facilities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4186.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4186.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zaki et al. QA dataset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset and evaluation designed to measure GPT-3.5/GPT-4 performance on materials science questions; cited as related work on evaluating LLMs' domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Materials-science QA benchmark (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a prior effort to evaluate LLMs' ability to answer domain-specific scientific questions; the paper notes that such evaluations often do not measure improvement when domain-specific context retrieval is added.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>not specified (benchmark focuses on QA/knowledge rather than automated extraction of quantitative laws)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Dataset for QA evaluation (textual question-answering); not described here as an extraction pipeline for quantitative laws.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Benchmarking LLM answers against annotated answers (as reported by that referenced work) — mentioned in this paper at a high level.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>The present paper notes that such benchmarks often evaluate LLMs without considering gains from domain-specific context retrieval; broader concerns about hallucination and context-window limits also apply.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Opportunities for retrieval and tool augmented large language models in scientific facilities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4186.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4186.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tool-augmented LLMs (Toolformer / Parisi et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tool-augmented language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General class of methods that augment LLMs with external tools/APIs to extend capabilities beyond text generation, cited here as foundational for instrument-control workflows and domain agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tool augmented language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Tool-augmented LLMs (general class; e.g., Toolformer)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Discussed as prior work motivating the use of external tools/APIs described to the model (tool name, I/O specification) and leveraged by the LLM to perform actions; the CALMS implementation uses this paradigm (ReAct + structured tool calls). The paper cites high-level design principles (tool description in prompts, parser to interpret tool calls, insertion of tool outputs back into prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / cross-domain (applied here to experimental controls and materials databases)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Tool-call framework and prompt engineering for structured interaction; not described here as a pipeline for extracting quantitative laws from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper highlights general challenges for tool-augmented LLMs: requirement for precise tool specs, LLM adherence to structured syntax (some open-source models failed to follow JSON constraints), and faithfulness/hallucination in absence of context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Opportunities for retrieval and tool augmented large language models in scientific facilities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Is GPT-3 all you need for low-data discovery in chemistry? <em>(Rating: 2)</em></li>
                <li>Chemcrow <em>(Rating: 2)</em></li>
                <li>A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models. <em>(Rating: 2)</em></li>
                <li>Tool augmented language models. <em>(Rating: 1)</em></li>
                <li>Domain-specific ChatBots for Science using Embeddings <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4186",
    "paper_id": "paper-265609385",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "CALMS",
            "name_full": "Context-Aware Language Model for Science",
            "brief_description": "A retrieval- and tool-augmented LLM agent developed in this work to help scientists with instrument operations, experiment design, document retrieval, conversational memory, and calling instrument/control APIs to execute experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CALMS",
            "system_description": "CALMS is a framework that composes (1) a large language model, (2) conversational history (moving window of prior turns), (3) semantic search over preprocessed document stores (embedding-based retrieval of top-N context chunks), and (4) callable experimental/software tools (described to the model) using a ReAct/Chain-of-Thought style workflow. Prompts include retrieved context and recent conversation; a parser recognizes model requests to call tools (e.g., Materials Project API, SPEC instrument control) and inserts tool outputs back into the prompt so the model continues reasoning and can issue further tool calls. Implementations compared GPT-3.5 Turbo and Vicuna models and executed an end-to-end task (set diffractometer to a Bragg peak) by chaining a Materials Project lattice-lookup tool and a SPEC motor-move tool.",
            "model_name": "GPT-3.5 Turbo; Vicuna-13b-v1.5",
            "model_size": "175B; 13B",
            "scientific_domain": "materials science, experimental physics (synchrotron/X-ray experiments), nanoscience, computational resources",
            "number_of_papers": null,
            "law_type": null,
            "law_examples": null,
            "extraction_method": null,
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "CALMS as implemented was not used to extract quantitative scientific 'laws' from literature; limitations observed include LLM hallucinations without contextual retrieval, limited context-window size requiring chunking/embedding retrieval, open-source models (Vicuna) performing worse than GPT-3.5 on completeness and tool-call syntax adherence, and occasional inability of open-source models to follow structured JSON inputs for tool calls.",
            "comparison_baseline": "Direct comparisons were made between GPT-3.5 Turbo and an open-source Vicuna-13b model on Q&A, context retrieval, and tool usage tasks; GPT-3.5 performed better on completeness and reliably executed the automated instrument workflow, while Vicuna could not consistently execute the structured tool-call workflow.",
            "uuid": "e4186.0",
            "source_info": {
                "paper_title": "Opportunities for retrieval and tool augmented large language models in scientific facilities",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GPT-3 (low-data discovery reference)",
            "name_full": "GPT-3 (as used in low-data discovery in chemistry)",
            "brief_description": "Referenced work noting that generative LLMs can perform few-shot learning for tasks such as materials property prediction and inverse design, indicating potential to discover quantitative relations from small datasets or domain text.",
            "citation_title": "Is GPT-3 all you need for low-data discovery in chemistry?",
            "mention_or_use": "mention",
            "system_name": "GPT-3 (few-shot discovery examples, referenced)",
            "system_description": "Cited in this paper as an example where LLMs (GPT-3) were shown to be fast learners requiring few training examples to gain domain expertise for materials property prediction and inverse design; mentioned as evidence that LLMs can support discovery tasks from small data or domain knowledge. The paper itself does not provide implementation details for that referenced work.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "chemistry, materials science",
            "number_of_papers": null,
            "law_type": "empirical property-prediction / inverse-design relationships (as characterized in the cited work)",
            "law_examples": null,
            "extraction_method": "Mentioned as few-shot/fine-tuning and prompt-based use of an LLM to perform property prediction/inverse design (no detailed extraction pipeline described in this paper).",
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "The present paper only cites this work as an example; it does not report specifics. More generally in this paper, authors emphasize LLM hallucination risk and the need for domain context/retrieval to avoid errors.",
            "comparison_baseline": null,
            "uuid": "e4186.1",
            "source_info": {
                "paper_title": "Opportunities for retrieval and tool augmented large language models in scientific facilities",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "ChemCrow",
            "name_full": "ChemCrow: Augmenting large language models with chemistry tools",
            "brief_description": "A domain-specific LLM agent that integrates external chemistry tools to perform tasks in materials design, organic synthesis, and drug discovery, combining planning (Chain-of-Thought) and tool utilization.",
            "citation_title": "Chemcrow",
            "mention_or_use": "mention",
            "system_name": "ChemCrow (referenced domain-specific agent)",
            "system_description": "Referenced as an example of a domain-specific LLM agent that accomplishes chemistry-related tasks by integrating multiple tools and combining planning via Chain-of-Thought reasoning and tool calls; cited to illustrate capabilities of tool-augmented LLM agents in scientific discovery contexts.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "chemistry, materials science, drug discovery",
            "number_of_papers": null,
            "law_type": "not specified in this paper (agent aimed at task completion in chemistry; may enable discovery of relationships via tool workflows)",
            "law_examples": null,
            "extraction_method": "Tool integration plus LLM planning/CoT prompting (as described at high level in this paper); no details on extracting quantitative laws from literature provided here.",
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Mentioned as illustrative prior work; this paper highlights general LLM failure modes (hallucination) and the need for retrieval/context to ensure faithfulness, which are relevant to agents like ChemCrow.",
            "comparison_baseline": null,
            "uuid": "e4186.2",
            "source_info": {
                "paper_title": "Opportunities for retrieval and tool augmented large language models in scientific facilities",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Zaki et al. QA dataset",
            "name_full": "A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models",
            "brief_description": "A dataset and evaluation designed to measure GPT-3.5/GPT-4 performance on materials science questions; cited as related work on evaluating LLMs' domain knowledge.",
            "citation_title": "A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models.",
            "mention_or_use": "mention",
            "system_name": "Materials-science QA benchmark (referenced)",
            "system_description": "Cited as a prior effort to evaluate LLMs' ability to answer domain-specific scientific questions; the paper notes that such evaluations often do not measure improvement when domain-specific context retrieval is added.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "materials science",
            "number_of_papers": null,
            "law_type": "not specified (benchmark focuses on QA/knowledge rather than automated extraction of quantitative laws)",
            "law_examples": null,
            "extraction_method": "Dataset for QA evaluation (textual question-answering); not described here as an extraction pipeline for quantitative laws.",
            "validation_approach": "Benchmarking LLM answers against annotated answers (as reported by that referenced work) — mentioned in this paper at a high level.",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "The present paper notes that such benchmarks often evaluate LLMs without considering gains from domain-specific context retrieval; broader concerns about hallucination and context-window limits also apply.",
            "comparison_baseline": null,
            "uuid": "e4186.3",
            "source_info": {
                "paper_title": "Opportunities for retrieval and tool augmented large language models in scientific facilities",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Tool-augmented LLMs (Toolformer / Parisi et al.)",
            "name_full": "Tool-augmented language models",
            "brief_description": "General class of methods that augment LLMs with external tools/APIs to extend capabilities beyond text generation, cited here as foundational for instrument-control workflows and domain agents.",
            "citation_title": "Tool augmented language models.",
            "mention_or_use": "mention",
            "system_name": "Tool-augmented LLMs (general class; e.g., Toolformer)",
            "system_description": "Discussed as prior work motivating the use of external tools/APIs described to the model (tool name, I/O specification) and leveraged by the LLM to perform actions; the CALMS implementation uses this paradigm (ReAct + structured tool calls). The paper cites high-level design principles (tool description in prompts, parser to interpret tool calls, insertion of tool outputs back into prompt).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "general / cross-domain (applied here to experimental controls and materials databases)",
            "number_of_papers": null,
            "law_type": null,
            "law_examples": null,
            "extraction_method": "Tool-call framework and prompt engineering for structured interaction; not described here as a pipeline for extracting quantitative laws from literature.",
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Paper highlights general challenges for tool-augmented LLMs: requirement for precise tool specs, LLM adherence to structured syntax (some open-source models failed to follow JSON constraints), and faithfulness/hallucination in absence of context.",
            "comparison_baseline": null,
            "uuid": "e4186.4",
            "source_info": {
                "paper_title": "Opportunities for retrieval and tool augmented large language models in scientific facilities",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Is GPT-3 all you need for low-data discovery in chemistry?",
            "rating": 2,
            "sanitized_title": "is_gpt3_all_you_need_for_lowdata_discovery_in_chemistry"
        },
        {
            "paper_title": "Chemcrow",
            "rating": 2
        },
        {
            "paper_title": "A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models.",
            "rating": 2,
            "sanitized_title": "a_question_answering_dataset_for_investigating_materials_science_knowledge_of_large_language_models"
        },
        {
            "paper_title": "Tool augmented language models.",
            "rating": 1,
            "sanitized_title": "tool_augmented_language_models"
        },
        {
            "paper_title": "Domain-specific ChatBots for Science using Embeddings",
            "rating": 1,
            "sanitized_title": "domainspecific_chatbots_for_science_using_embeddings"
        }
    ],
    "cost": 0.0131795,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Opportunities for Retrieval and Tool Augmented Large Language Models in Scientific Facilities</p>
<p>Michael H Prince 
Advanced Photon Source
Argonne National Laboratory
ILUSA</p>
<p>Henry Chan 
Center for Nanoscale Materials
Argonne National Laboratory
ILUSA</p>
<p>Aikaterini Vriza 
Center for Nanoscale Materials
Argonne National Laboratory
ILUSA</p>
<p>Tao Zhou 
Center for Nanoscale Materials
Argonne National Laboratory
ILUSA</p>
<p>Varuni K Sastry 
Argonne Leadership Computing Facility
Argonne National Laboratory
ILUSA</p>
<p>Matthew T Dearing 
Business and Information Systems
Argonne National Laboratory
ILUSA</p>
<p>Ross J Harder 
Advanced Photon Source
Argonne National Laboratory
ILUSA</p>
<p>Rama K Vasudevan 
Center for Nanophase Materials
Oak Ridge National Laboratory
Oak RidgeTNUSA</p>
<p>Mathew J Cherukara mcherukara@anl.gov 
Advanced Photon Source
Argonne National Laboratory
ILUSA</p>
<p>Opportunities for Retrieval and Tool Augmented Large Language Models in Scientific Facilities
98861111E1AFCF16D91C64CECE06B45CThe submitted manuscript has been created
Upgrades to advanced scientific user facilities such as next-generation x-ray light sources, nanoscience centers, and neutron facilities are revolutionizing our understanding of materials across the spectrum of the physical sciences, from life sciences to microelectronics.However, these facility and instrument upgrades come with a significant increase in complexity.Driven by more exacting scientific needs, instruments and experiments become more intricate each year.This increased operational complexity makes it ever more challenging for domain scientists to design experiments that effectively leverage the capabilities of and operate on these advanced instruments.Large language models (LLMs) can perform complex information retrieval, assist in knowledge-intensive tasks across applications, and provide guidance on tool usage.Using x-ray light sources, leadership computing, and nanoscience centers as representative examples, we describe preliminary experiments with a Context-Aware Language Model for Science (CALMS) to assist scientists with instrument operations and complex experimentation.With the ability to retrieve relevant information from facility documentation, CALMS can answer simple questions on scientific capabilities and other operational procedures.With the ability to interface with software tools and experimental hardware, CALMS can conversationally operate scientific instruments.By making information more accessible and acting on user needs, LLMs could expand and diversify scientific facilities' users and accelerate scientific output.</p>
<p>Introduction</p>
<p>Conversational agents such as ChatGPT built on large language models (LLMs) are upending various industries and professions 1 .Their impact spans education 2,3 , talent acquisition 4 , and many other sectors.A report by McKinsey and Company estimates that generative AI, including LLMs, could add trillions of dollars annually to worldwide productivity 5 .</p>
<p>LLMs also have the potential to revolutionize every aspect of scientific discovery, including accelerating literature search, aiding in experimental design, providing data summaries, and publication writing and editing 6 .LLMs have also been shown to be fast learners, requiring only a few training examples to quickly gain domain expertise, e.g., for materials property prediction and inverse design 7 .</p>
<p>At scientific user facilities, such agents could significantly contribute to nearly all facets of operating advanced scientific instruments and conducting experiments.These agents could help new users navigate different aspects of the facility, such as the proposal submission process, safety policies, and operating practices.They could also help users design suitable experiments to address their scientific inquiries, including selecting the appropriate instrument, determining the necessary modalities, and advising on the sample preparation.Additionally, during the experiment, they could guide basic instrument operation.Despite their potential, LLMs suffer from several dangerous limitations, including the tendency to 'hallucinate' answers when faced with questions not part of their training.</p>
<p>We describe preliminary experiments with retrieval 8 and tool-augmented 9 LLMs tailored for scientists' needs.Scientific context-aware LLMs leverage existing data stores to augment their capabilities, allowing them to perform better in domains that are not part of the pre-training data, thereby eliminating the need to perform any fine-tuning on the domain-specific information that can be computationally expensive and time-consuming.In this work, we describe using LLMs to create an agent with knowledge of scientific user facilities.We describe the initial results of assisting scientists through basic experiment design, instrument operation, and direct experimental tool control.</p>
<p>Related work</p>
<p>LLMs have demonstrated strong performance as autonomous agents across multiple science domains 10 .The key components that make the LLM function as the agent's brain are memory, planning, and tool use.Recently, Yager discussed the importance of memory by building domain-specific chatbots for physical science research.Their demonstration involves the creation of a context window using text chunks from scientific documents in PDF format.When the LLM is composing its reply, it uses this context to formulate the answer 11 .</p>
<p>ChemCrow is another example of a domain-specific LLM agent designed to accomplish tasks related to materials design, organic synthesis, and drug discovery by integrating several chemistry tools.Their workflow combines planning via Chain of Thought reasoning (CoT) and tool utilization 12 .In another recent work, researchers built an Intelligent Agent based on GPT, capable of searching hardware documentation and controlling multi-instrument systems to conduct experiments.An important consideration in this type of research is the evaluation of the chatbots or LLMs for scientific applications.In this context, Zaki et al. developed a question-answering dataset for measuring the performance of GPT-3.5 and GPT-4.However, the performance is measured without considering the improvement when an LLM is provided with additional domain-specific scientific context 13 .Jablonka et al. described results from a hackathon applying LLMs to various scientific tasks. 14</p>
<p>Early Experiments</p>
<p>Overview of CALMS</p>
<p>The core of CALMS consists of four components: a large language model (LLM) 15 , conversational history that allows follow-on queries, semantic search over document stores to retrieve the most relevant context given a question from the user 16 , and instrument tools that the LLM can use when instructed to by the user 17 .Figure 1 shows a schematic of the structure of CALMS.When provided with appropriate context and tools, CALMs can either answer complex technical questions that the LLM has no prior knowledge of from its training 18 or execute an experiment or perform a computation using the appropriate tools 17 .We describe each component in detail in the following sections.We compare responses from two state-of-the-art LLMs, OpenAI's GPT-3.5 Turbo and an open-source model Vicuna, over questions related to experimental design, operation, and ability to drive an instrument successfully.We note that the CALMS framework is independent of the LLM, and other open-source or closed-source LLMs can be swapped without changing the framework.</p>
<p>GPT-3.5</p>
<p>The Generative Pretrained Transformer 3.5 (GPT-3.5) is a near state-of-the-art large language model developed by OpenAI and released in late 2022.Similar to its predecessors and successor, GPT-4, the model architecture is based on the Transformer 19 , which has demonstrated effectiveness in various natural language processing tasks, including generating text, answering questions, writing content with human-like creativity, and even generating computer programming code.These outcomes result from predictive inference based on an input sequence of text processed through the language patterns the model learned from a vast training set.</p>
<p>The most performant of the pre-GPT-4 class of models, GPT-3.5 Turbo, was trained with 175 billion parameters and is available at the time of this writing with context window sizes of 4,096 or 16,384 tokens.The context window corresponds to the maximum number of tokens (i.e., words, punctuation, or even sub-strings of words) the model can process during a single inference procedure.The content included in this window represents a simple type of memory that can be leveraged to provide background as effective guidance toward predicting subsequent words.This feature can significantly impact the quality and relevancy of the model output and has led to the burgeoning field of "prompt engineering" to develop and implement best practices.</p>
<p>While the model is proprietary, Microsoft makes it available through the Azure cloud with an enterprise subscription.Hosting of the model resides on a dedicated and restricted instance, so data remains local, is not retained by default, and is not shared with third parties.This approach alleviates some challenges with using public models, such as ChatGPT from OpenAI.</p>
<p>The interaction with GPT-3.5 starts with an API call authenticated using a unique key.This key is essential for accessing OpenAI's capabilities securely and ensuring that only authorized users can make requests.Before making the API call, specific parameters, i.e., temperature and top_p, guide the model's response generation.The temperature parameter controls the randomness of the output and takes values between 0-1.A higher temperature leads to more creative output, while a lower temperature results in more deterministic outputs.The top_k parameter limits the model's choices to the 'k' most likely next words at each step of the generation process.The top_p parameter is an alternative to the temperature that focuses on a subset of predictions that cumulatively add up to the probability 'p'.After these parameters are set and the API call is made, the model can be used either for chat completion or embeddings generation.For chat completion tasks, the model generates a continuation of the provided text input based on the context and parameter set.For the embeddings generation task, the model processes the input text to create a vector representation that captures the semantic meaning of the text.These embeddings can be used for various applications like text similarity assessment or semantic search.</p>
<p>Open-source LLMs</p>
<p>LLMs have transformed the AI field in recent years with the advent of large and complex models like GPT-3 and GPT-4.Though their performance and capabilities are unparalleled, they often require very large compute resources, thus limiting the number of individuals and enterprises that can develop and maintain systems of this scale.Moreover, with limited or no transparency about the model specifications, data, and development methods, these "closed" source LLMs raise concerns regarding adherence to AI ethical standards (including fairness, morality, accountability, and misuse, among others).Open-source LLMs alleviate these concerns by democratizing the AI community with free and unrestricted accessibility to models and information driven by platforms and communities with a transparent and collaborative approach.Open-source LLMs also provide the advantage of being accessible to modification to tailor it to a specific task.HuggingFace is such a resource that provides a platform where researchers, academics, and data scientists can share datasets. 20HuggingFace also provides the necessary tools within their Transformers library to fine-tune and deploy ML workflows for open-source models, including LLMs.With a wide range of modalities of models and data available, it also has the added advantage of seamlessly switching and testing between different models with minimal code changes.</p>
<p>HuggingFace also provides the "Open LLM Leaderboard evaluation suite" that evaluates and compares the performance of different LLMs based on a relative Elo rating strategy.Traditionally, the Elo rating system has been a method used in the field of chess to calculate the relative skill levels of players based on the outcome of the games.A similar strategy is applied to the model responses, where human annotators rate responses for helpfulness and truthfulness in a pairwise setting. 21At the time of finalizing the paper, "lmsys/vicuna-13b-v1.5" was ranked among the top five pertained models with an average score of 61.63.This model was obtained by finetuning Llama 2 on supervised instruction downstream tasks trained on conversations collected from ShareGPT with a sequence length of 2048. 22n the following sections, we compare responses from Vicuna 1.5-16k against GPT-3.5 across a series of tasks.Our evaluation encompasses tasks including general comprehension of user questions, domain-specific context retrieval, quality of the answers, and utilization of tools.</p>
<p>Context retrieval through semantic search</p>
<p>One of the prominent use cases of LLMs is context retrieval through semantic search, which is a process that involves extracting relevant information from a large text body based on the meaning and context of the query instead of relying on keyword matching.Currently, Claude has the largest context window of 100k tokens, corresponding to around 75,000 words.</p>
<p>Two approaches are possible to extend the context retrieval capabilities of models beyond their training corpus.First, fine-tuning the model with domain-specific training data requires the careful curation of data and often necessitates significant up-front investment either in local GPU resources (e.g., Llama was trained on 2048 GPUs) 23 in the case of open-source or in cloud compute costs in the case of closed-source models.A great deal of expertise in language modeling, distributed training, transformers, and data curation is also typically required to fine-tune these models successfully.However, this approach will likely provide the best results when a sufficiently large corpus of data for fine-tuning is available.</p>
<p>A second approach to enhance the capabilities of LLMs is to leave the model weights unchanged but provide appropriate additional context to help answer user queries.This strategy is typically cost-efficient, and a plethora of frameworks now exist to enable facile interaction of the LLM with document stores.A remaining challenge, however, is that even the largest models, e.g., GPT-4-128k, can only accept input text up to 128k tokens, corresponding to approximately 300 pages of text.While future models might allow even larger input sizes, the current limitations prevent passing in all relevant documentation 24 .To address this gap, available documentation is pre-processed for context retrieval, which is performed via a lightweight embedding model.All technical documents are split into token windows of a pre-determined size, and their embeddings are stored in a vector database, e.g., ChromaDB, collection 25 .When a question is sent, the user's text is embedded, and the top N = 4 text chunks closest to the user input are retrieved from the store.These context chunks are then included in the prompt provided to the conversational LLM.</p>
<p>Conversational memory</p>
<p>At their core, LLMs are probabilistic models that learn sentence or paragraph completion.In other words, given a sequence of words, they are trained to predict the most likely completions of that sentence or paragraph.Models that have been fine-tuned for chat have an additional training step that teaches them how to complete conversations, either an unsupervised approach by learning from human-provided conversations or, more popularly, through reinforcement learning with human feedback (RLHF). 26Such fine-tuned chat models can complete human-like conversations, and we leverage this ability by providing the conversational history as part of the subsequent prompt.As previously discussed, due to the limited amount of text that can be provided in the prompt, reproducing the entire conversational history is often not possible.In essence, the problem is how to provide the LLM with short-term memory to answer questions more effectively when faced with a limited context window.Several strategies have been proposed to address this; one option that CALMS adopted is to use a moving window over the conversation history to include only the last K = 6 conversations between the user and the LLM.Supplementary Figure 1A shows the prompt to the LLM with context and memory.</p>
<p>Software and experimental tools</p>
<p>To further augment the use of LLMs, current research is exploring if the LLMs can go beyond text and reason about the physical world.For example, work from Google and Microsoft have demonstrated using NLP to achieve robotic tasks. 27,28This process involves building a series of design principles that include high-level robot APIs or function libraries, special prompting structures, and human feedback via text.</p>
<p>Building upon this strategy, the current implementation of CALMS incorporates capabilities such as calling the Materials Project API 29 , an instrument control software (spec TM ) 30 , and is based on Chain-of-Thought prompting and the ReAct framework. 31,32These functions are provided as a list of tool names, including the description of the way they interact with scientific equipment and details about the user inputs and outputs.The tool is then instructed to answer a user-provided prompt by leveraging the available tools.A parser interprets the LLM output for tool call requests.If a tool is called, then the parser inserts the output into the prompt, and the model continues generating actions until a response is provided.Supplementary Figure 1B shows the prompt to the LLM with the tool description.</p>
<p>In effect, using conversational memory, context-retrieval, and specified tools creates a dynamic pre-prompt that allows the LLM to parse and respond to user input with a detailed understanding of the context behind those queries.</p>
<p>AI-assisted experimental design</p>
<p>We first explore the ability of context-aware LLMs, such as those implemented in CALMS, to respond to highly technical questions typical of those encountered by new scientific facility users.CALMS is queried with questions concerning user facilities, including the Advanced Photon Source (APS), Center for Nanoscale Materials (CNM), Argonne Leadership Computing Facility (ALCF), and Center for Nanophase Materials Sciences (CNMS).A summary of responses with Vicuna and GPT-3.5 Turbo with and without context retrieval is shown in Figure 2. The AI response is first graded by their helpfulness (denoted Rel in Fig. 2 for relevance), 'yes' for giving a relevant answer to the question, and 'no' for an irrelevant answer.Next, their truthfulness is evaluated by checking if hallucinations (denoted Hal) appear in the response.While we appreciate that relevance and hallucinations are not correlated, a relevant response can be hallucinated, and an irrelevant response can be entirely truth-based.Finally, the completeness (denoted Com) of the AI response is graded with a score of 0-5.Details on the scoring are included in the Methods section.A complete list of the queries and AI responses is found in the data availability section, with highlighted key differences.We caution that the reader might observe slightly different results because the LLMs outputs are non-deterministic.Given an accurate context, CALMS can always provide relevant answers to questions specific to each user facility.Rarely was hallucination observed.In general, GPT-3.5 Turbo tends to provide a more complete answer than Vicuna, scoring multiple 5s in the completeness grading.A score of 5 indicates that the users are provided with all the necessary information to select the appropriate tool to complete their task.</p>
<p>Without appropriate context, the language models have a strong tendency to hallucinate.As a representative example, we queried the models on the name of the automated image simulation framework developed at the CNM.Both Vicuna and GPT-3.5 Turbo provided the user with a made-up tool called ImageSim (an apparent portmanteau for image and simulation) (see Figure 3).We note that both models, with context, were able to retrieve the correct answer, which is a tool called ingrained. 33Without context, irrelevant answers have been observed, and occasionally, the models decline to answer the question, citing insufficient knowledge (see for example Supplementary Figure 2).Supplementary Figures 3 and 4 show further examples illustrating the importance of context-aware responses.</p>
<p>AI-assisted experiments</p>
<p>We further queried the models' ability to provide operational assistance on advanced scientific and computational resources at the abovementioned user facilities.A summary of responses with Vicuna and GPT-3.5 Turbo with and without context retrieval is shown in Figure 4. Similar behavior was observed compared to what was noted for the experimental design queries.Without context, both Vicuna and GPT-3.5 Turbo tend to hallucinate.Even when they are being truthful (i.e., not hallucinating), the answers they provide are typically not directly helpful.For instance, when queried about the procedure to start a tomographic scan, Vicuna, without context, responded with a truthful answer concerning the computational tomography (CT) scan in a hospital, which is unrelated to the tomographic scan performed at the APS (Figure 5).In this case, the truthful yet unhelpful answer Vicuna provided was due to the LLM mistaking the tool being queried with a different subject well-known in the public domain.GTP-3.5 Turbo realized the nuances in the question and suggested the user be more specific about the type of tomographic scan they are inquiring about.It then provided a general answer about the need to submit a user proposal to perform a tomography experiment.However, with context, both Vicuna and GPT-3.5 Turbo provide consistently relevant and truthful answers, with GTP-3.5 Turbo again faring better on the score of completeness than Vicuna.Still using the tomographic scan procedure query as an example, Vicuna responded with a short answer regarding the use of the tomoscan command, while GTP-3.5 Turbo's answer included solutions with the graphic user and the command line interface with a reference to the relevant documentation (Figure 5).Supplementary Figures 5, 6, and 7 show further instances of the importance of context-aware responses.</p>
<p>Automated execution</p>
<p>Through the ReAct framework 32 and API calls, the CALMS framework could be configured to drive experiments in an agentic manner.To demonstrate this capability, a test was conducted on a real-world diffractometer simulating a common user operation.</p>
<p>For a user to take a diffraction measurement, an area detector, usually mounted to a robotic arm, must be moved into a set position relative to the sample and incoming x-ray beam.This angle is determined by the intensity of the beam, the material being observed, and the peak position selected by the user.To do this process manually, the user must first look up lattice information via a materials database.Then, the user must copy over the lattice information and the selected peak into the beamline interface, controlled by SPEC, to move the motors into position.</p>
<p>To automate this process, two LLM calls were implemented: GetLatticeConstants and SetDiffractometmer.For the LLM to successfully perform the action, it must first input the material into a call that queries the Materials Project API to return the lattice constants to the model.Next, it must combine the output of this call with the peak positions from the user input to execute a second call.Figure 6 describes in detail the sequence and parameters of the calls.In the example shown, a user requests the diffractometer to be set to the 012 Bragg peak of WSe2.Within the CALMS framework, the LLM parses the query and recognizes the material and Bragg peak.It then uses a tool to call The Materials Project 29 to obtain the lattice information for WSe2.It then passes that information to a second tool that uses the Spec software to calculate the motor positions and then moves the diffractometer to that position.</p>
<p>These calls were implemented via LangChain's "Structured input ReAct" framework.This library API allows developers to define the parameters and types for multi-parameter input and requires the model to use valid JSON to perform a call.In our tests, we observed that the opensource models could not consistently follow this syntax.All results shown were run with OpenAI's GPT-3.5 model.We could execute this sequence of commands via a single user prompt on beamline 34 at the APS.A video of the commands being run on the beamline is provided in the supplemental material of this paper and can also be viewed online.</p>
<p>Summary and Outlook</p>
<p>In summary, we described the development of a Context-Aware Language Model for Science (CALMS) and exemplified the promising potential of leveraging LLMs for accelerating scientific experiments and discovery.Preliminary results obtained using CALMS backed by GPT-3.5 and Vicuna illustrate the applicability of LLMs in experimental design and operation guidance, leveraging the zero-shot capability of LLMs via prompt engineering 35 .The results emphasize the importance of context in addressing the typical issue of hallucination in LLMs.CALMS powered by GPT-3.5 and Vicuna both performed reasonably well in the Q&amp;A tasks, but the results reveal limitations in the open-source models compared to state-of-the-art closed-source models such as GPT-3.5 Turbo.This gap is particularly pronounced in tool usage, where we could not execute the experimental workflow with the open-source model Vicuna.With the investment and extraordinary rate of progress in open-source models and fine-tuning methods, we expect the performance gap to the closed-source models to narrow.</p>
<p>Traditionally, model performance over the years has shown power law scaling with an increase in the number of model parameters, dataset size, and compute budget.However, emerging research demonstrates that it is equally important to scale the training size with an increase in model parameter size to obtain optimal training. 36Additionally, with instruction fine-tuning and RLHF, we see that the even smaller models of the size 1.3B parameters can equal the performance of the 175B GPT-3-base model. 37With several innovations like SELF-Instruct 38 (that can enhance the model performance by 33% over the GPT3 base model with fine tuning on the self-generated instruction without any human intervention) and Chain-of-Thought prompting 15,39 , the broad consensus is that these performance gains will continue to push the state of the art.With such anticipated advanced model performance, we envision future capabilities could include leveraging decades of information recorded in e-logs as well as the ability to extract code or commands and execute experimental workflows fully autonomously.</p>
<p>Methods</p>
<p>Scoring methodology: 1/1 pt is automatically given if the response is rated 'yes' on relevance and 'no' on hallucination.2/2 pts is given if the instructions can be executed by the user without error, 1/2 if minor errors are present that are easily fixable, and 0/2 if major errors are present in the response.2/2 pts is assigned if the user is given all the information they need for their request, 1/2 if minor details are missing, and 0/2 if crucial details are missing that affect the execution.The maximum score is 5 pts.</p>
<p>Code and Data Availability</p>
<p>All code, models, and data will be made publicly available on GitHub with the published manuscript.</p>
<p>Figure 1 :
1
Figure 1: Overview of CALMS: CALMS uses a large language model in conjunction with conversational memory, document stores, and experimental tools to answer user queries or take action to drive an instrument.</p>
<p>Figure 2 :
2
Figure 2: Scoring of experimental design questions with and without context.We score the models on relevance, absence of hallucination, and completeness of response.</p>
<p>Figure 3 :
3
Figure 3: Demonstration of AI-assisted experimental design specific to the CNM user facility: (top) initial question from a user, (bottom-left) response from AI without context, (bottom-right) response from AI with context.Major differences between the responses are highlighted (in yellow) to illustrate the importance of context.</p>
<p>Figure 4 :
4
Figure 4: Scoring of experimental operations assistance questions with and without context.As before, we score the models on relevance, absence of hallucination, and completeness of response.</p>
<p>Figure 5 :
5
Figure 5: Demonstration of AI-powered operational assistance specific to the APS user facility: (top) initial query from a user, (bottom-left) response from AI without context, (bottom-right) response from AI with context.Major differences between the responses are highlighted (in yellow) to illustrate the importance of context.</p>
<p>34
34</p>
<p>Figure 6 :
6
Figure 6: Workflow demonstrating the execution of a robot motor move based on a user query to CALMS.CALMS parses the user query to extract the material and Bragg peak, queries Materials Project for the lattice constants, and then calculates the position of and moves the beamline diffractometer to the Bragg peak requested by the user.A video of the demonstration is available at https://danielzt12.github.io/latest_news/2023/11/20/operating-scientific-instruments-with-LLMs.html</p>
<p>AcknowledgmentsWork performed at the Center for Nanoscale Materials and Advanced Photon Source, both U.S. Department of Energy Office of Science User Facilities, was supported by the U.S. DOE, Office of Basic Energy Sciences, under Contract No. DE-AC02-06CH11357.This research used resources of the Argonne Leadership Computing Facility, a U.S. Department of Energy (DOE) Office of Science user facility at Argonne National Laboratory and is based on research supported by the U.S. DOE Office of Science-Advanced Scientific Computing Research Program, under Contract No. DE-AC02-06CH11357.M.J.C and H.C also acknowledge support from the U.S. Department of Energy, Office of Science, Office of Basic Energy Sciences Data, Artificial Intelligence, and Machine Learning at DOE Scientific User Facilities program under Award Number 34532.by UChicago Argonne, LLC, Operator of Argonne National Laboratory ("Argonne").Argonne, a U.S. Department of Energy Office of Science laboratory, is operated under Contract No. DE-AC02-06CH11357.The U.S. Government retains for itself, and others acting on its behalf, a paid-up nonexclusive, irrevocable worldwide license in said article to reproduce, prepare derivative works, distribute copies to the public, and perform publicly and display publicly, by or on behalf of the Government.The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan.http://energy.gov/downloads/doe-public-access-plan
References 1 Prepare for truly useful large language models. https://doi.org:10.1038/s41551-023-01012-6Nature Biomedical Engineering. 72023</p>
<p>Chegg Embraced AI. Wired, ChatGPT Ate Its Lunch Anyway. 2023</p>
<p>ChatGPT for good? On opportunities and challenges of large language models for education. E Kasneci, https://doi.org:10.1016/j.lindif.2023.102274Learning and Individual Differences. 1032023</p>
<p>B Clavié, A Ciceu, F Naylor, G Soulié, T Brightwell, International Conference on Applications of Natural Language to Information Systems. Springer</p>
<p>The economic potential of generative AI: The next productivity frontier. M Company, 2023</p>
<p>Scientists used ChatGPT to generate an entire paper from scratch-but is it any good?. G Conroy, Nature. 6192023</p>
<p>Is GPT-3 all you need for low-data discovery in chemistry?. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, 2023</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, Advances in Neural Information Processing Systems. 332020</p>
<p>A Parisi, Y Zhao, N Fiedel, Talm, arXiv:2205.12255Tool augmented language models. 2022arXiv preprint</p>
<p>The future of chemistry is language. A D White, Nature Reviews Chemistry. 2023</p>
<p>Domain-specific ChatBots for Science using Embeddings. K G Yager, arXiv:2306.100672023arXiv preprint</p>
<p>A M Bran, S Cox, A D White, P Schwaller, Chemcrow, arXiv:2304.05376Augmenting largelanguage models with chemistry tools. 2023arXiv preprint</p>
<p>M Zaki, N Krishnan, Mascqa, arXiv:2308.09115A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models. 2023arXiv preprint</p>
<p>14 examples of how LLMs can transform materials science and chemistry: a reflection on a large language model hackathon. K M Jablonka, https://doi.org:10.1039/D3DD00113JDigital Discovery. 22023</p>
<p>W.-L A L Chiang, Zhuohan, Zi Lin, Ying Sheng, Zhanghao Wu, Zhang, Hao, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, Vicuna, An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality. 2023</p>
<p>N Reimers, I Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bertnetworks. 2019arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, arXiv:2302.047612023arXiv preprint</p>
<p>H He, H Zhang, D Roth, arXiv:2301.00303Rethinking with retrieval: Faithful large language model inference. 2022arXiv preprint</p>
<p>Advances in neural information processing systems. A Vaswani, 201730Attention is all you need</p>
<p>. HuggingFace. 2023</p>
<p>Can foundation models label data like humans?. Huggingface, 2023</p>
<p>Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. L Zheng, arXiv:2306.056852023arXiv preprint</p>
<p>Llama: Open and efficient foundation language models. H Touvron, arXiv:2302.139712023arXiv preprint</p>
<p>New models and developer products announced at DevDay. Openai, 2023</p>
<p>. Chroma, Chroma, 2023</p>
<p>Fine-tuning language models from human preferences. D M Ziegler, arXiv:1909.085932019arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. D Driess, arXiv:2303.033782023arXiv preprint</p>
<p>Chatgpt for robotics: Design principles and model abilities. S Vemprala, R Bonatti, A Bucker, A Kapoor, Microsoft Auton. Syst. Robot. Res. 2202023</p>
<p>Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. A Jain, APL materials. 12013</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. C S Software, J Wei, spec Software for Diffraction. 202235</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, arXiv:2210.036292022arXiv preprint</p>
<p>Ingrained: An Automated Framework for Fusing Atomic-Scale Image Simulations into Experiments. E Schwenker, Small. 1821029602022</p>
<p>T Zhou, Augmenting Scientific Instrumentation with LLMs. 2023</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 352022</p>
<p>Training compute-optimal large language models. J Hoffmann, arXiv:2203.155562022arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, Advances in Neural Information Processing Systems. 352022</p>
<p>Self-instruct: Aligning language model with self generated instructions. Y Wang, arXiv:2212.105602022arXiv preprint</p>
<p>Large language models can self-improve. J Huang, arXiv:2210.116102022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
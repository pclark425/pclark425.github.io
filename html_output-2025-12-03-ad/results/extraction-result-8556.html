<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8556 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8556</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8556</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-270878452</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.01863v1.pdf" target="_blank">VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs</a></p>
                <p><strong>Paper Abstract:</strong> Vision language models (VLMs) are an exciting emerging class of language models (LMs) that have merged classic LM capabilities with those of image processing systems. However, the ways that these capabilities combine are not always intuitive and warrant direct investigation. One understudied capability in VLMs is visual spatial planning -- the ability to comprehend the spatial arrangements of objects and devise action plans to achieve desired outcomes in visual scenes. In our study, we introduce VSP, a benchmark that 1) evaluates the spatial planning capability in these models in general, and 2) breaks down the visual planning task into finer-grained sub-tasks, including perception and reasoning, and measure the LMs capabilities in these sub-tasks. Our evaluation shows that both open-source and private VLMs fail to generate effective plans for even simple spatial planning tasks. Evaluations on the fine-grained analytical tasks further reveal fundamental deficiencies in the models' visual perception and bottlenecks in reasoning abilities, explaining their worse performance in the general spatial planning tasks. Our work illuminates future directions for improving VLMs' abilities in spatial planning. Our benchmark is publicly available at https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8556.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8556.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.0-Pro-Vision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A private multimodal vision-language model evaluated in this paper on visual spatial planning (maze navigation and blocksworld); tested zero-shot, with in-context examples, and with textual-input ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.0-Pro-Vision</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Private multimodal VLM from the Gemini family with integrated vision and language capabilities; used here as a representative state-of-the-art VLM for image understanding and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Maze Navigation; Blocks World (Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based path-finding (2D grid maze) and block-manipulation (stacked blocks planning) requiring 2D spatial perception and sequential action planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>VSP benchmark: interleaved image + text prompts. Main tasks: generate a safe action sequence (L/R/U/D) to reach the goal in an observable maze, or produce a shortest moving plan to reach a target block configuration in Blocks World. Evaluations run zero-shot (primary), with 1/2/4-shot in-context examples, with textual-only ablations (images replaced by exact textual descriptions/tables), and fine-tuning (open-source models only). Automatic script-based scoring allowed multiple valid solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No specialized symbolic engine; prompted to perform stepwise analysis and verification (prompt encourages step-by-step checking of each action). Evaluated with in-context examples (0/1/2/4-shot). Paper also used text-replacement to simulate perfect perception; no explicit chain-of-thought or external planner was required by the benchmark (although prompts encourage analytic steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Decomposed-task accuracies (Table 3): Maze T1=0.58, T2=0.56, T3=0.33, T4=0.49; Blocks World T1=0.86, T2=0.51, T3=0.54, T4=0.55. Main-task success rates: described as substantially below perfect and dropping with difficulty; Gemini often around ~50% on easier settings and much lower as difficulty rises (paper reports poor performance on some simple 3x3 or 1-step tasks). All evaluations zero-shot unless noted.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Partial: Gemini shows nontrivial performance on perception (T1/T2) and moderate reasoning (T4) but fails on environment-perception (T3) in some maze cases. Ablation replacing images with exact textual descriptions improved performance, indicating failures are dominated by perception rather than pure reasoning. In-context examples yielded small or inconsistent gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against other private models (GPT-Vision, Claude-3, GPT-4o) and open-source models (LLaVA, InternLM, InternLM-VL, InstructBLIP, SPHINX). Gemini's performance is middling among private models (worse than GPT-4o/GPT-Vision on many tasks). Open-source models perform close to random in many tasks. Fine-tuned open-source models improved substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails on some simple spatial planning cases (3x3 mazes, single-step block moves). Performance decays quickly with map size/path length. Perception bottleneck (visual input) is a major failure mode; textual replacement improves scores but does not fully close gaps, indicating remaining reasoning limitations. Context-length and multi-image handling are challenges for open-source analogs noted in analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8556.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8556.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-Vision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT Vision (GPT-4/Turbo with vision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A private multimodal GPT-based vision-language model (referred to as GPT-Vision/GPT-4V in the paper) evaluated on VSP's mazes and blocksworld; shows relatively strong perception and reasoning among evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Vision (turbo-2024-04-09 with vision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's multimodal GPT variant with integrated vision capabilities (used here via a vision-enabled turbo model). Known for strong text understanding and applied vision reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Maze Navigation; Blocks World (Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based navigation and stacked-block planning requiring accurate visual perception of 2D layouts and sequential planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same VSP setup: interleaved image+text prompts; zero-shot primary evaluation; additionally in-context examples (1/2/4-shot) and text-replacement ablations were used.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt instructs stepwise interpretation and verification. Evaluated with in-context examples; no external symbolic planner used. Textual-input ablations used to measure perception vs reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Decomposed-task accuracies (Table 3): Maze T1=0.56, T2=0.27, T3=0.46, T4=0.56; Blocks World T1=0.73, T2=0.80, T3=0.70, T4=0.71. Paper notes GPT-Vision can exceed ~50% success on easy 3x3 mazes but performance falls to ~10% on 8x8 mazes in main task (example illustrating steep decay).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Shows stronger evidence than many models: relatively high accuracy on Blocks World spatial relations (T2) and reasoning (T4). Textual-input ablation improves performance across tasks, indicating some remaining perception limitations. In-context examples produce small improvements but not dramatic.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Among the private models assessed, GPT-Vision is competitive; GPT-4o often outperforms it on some tasks. Outperforms most open-source models by large margins on VSP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Sharp performance drop with increased maze size and number of required steps. Visual perception of image inputs remains a bottleneck (text-replacement experiments increase accuracy). Some tasks (e.g., maze T2) show low scores indicating inconsistent spatial relation extraction from images.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8556.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8556.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3 (claude-3-sonnet-20240229)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's Claude-3 VLM variant evaluated in the paper; shows mixed performance across perception and reasoning sub-tasks in VSP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>claude-3-sonnet-20240229</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic's Claude-3 family VLM used here as a vision+language baseline; described in the paper as strong at advanced reasoning and vision analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Maze Navigation; Blocks World (Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid navigation and block stacking planning requiring environment perception and sequential reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>VSP benchmark with interleaved images and textual prompts; evaluated zero-shot, with in-context examples and text-input ablations as in other models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt-driven reasoning with explicit stepwise checks; in-context examples tested; no external tool use described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Decomposed-task accuracies (Table 3): Maze T1=0.45, T2=0.67, T3=0.32, T4=0.61; Blocks World T1=0.43, T2=0.53, T3=0.49, T4=0.66.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Moderate evidence: relatively strong on maze T2 (spatial relation perception) and maze T4 (spatial reasoning). Textual ablations showed performance gains for many models, indicating perception remains critical.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs comparably to Gemini on several subtasks; typically behind GPT-4o but competitive with other private models. Outperforms most open-source models on several sub-tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Perception failures on environment-perception questions (T3) and general vulnerability to increased difficulty (map size and plan length). No human baseline numeric comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8556.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8556.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (gpt-4o-2024-05-13)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Latest multimodal GPT variant evaluated in the benchmark; achieves the best overall performance across many VSP tasks but still exhibits substantial failure modes at higher difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-2024-05-13</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A recent multimodal large language model (GPT-4o) supporting combinations of text, image, and audio; used here for VSP evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Maze Navigation; Blocks World (Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-navigation and block manipulation requiring 2D spatial perception and sequential planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>VSP interleaved image + text prompts; zero-shot primary evaluation; in-context examples and text-input ablations applied.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompted step-by-step analysis; tested with in-context examples. No explicit external search/planning added by authors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Decomposed-task accuracies (Table 3): Maze T1=0.58, T2=0.67, T3=0.58, T4=0.74; Blocks World T1=0.95, T2=0.90, T3=0.90, T4=0.76. Paper states GPT-4o performs best across tasks but still makes frequent mistakes under higher difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Strongest of the evaluated models: high accuracy on blocks world perception and relation tasks and relatively high maze reasoning (T4). Textual-input ablations still boost performance, indicating perception remains a contributor to errors but GPT-4o shows better integrated perception+reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms other private models (Gemini, GPT-Vision, Claude-3) and considerably outperforms open-source baselines. Still below human-level for reliably solving larger/harder instances.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not immune to rapid decay with difficulty; still fails on many larger mazes and longer block-moving plans. No direct human baseline reported; qualitative statement that tasks would be simple for humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8556.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8556.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-V1.6-VICUNA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source VLM based on LLaMA/Vicuna instruction-tuned with CLIP-based visual encoder; evaluated zero-shot and after fine-tuning, showing large improvement when trained on VSP data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAVA-V1.6-VICUNA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source vision-language model that projects images into text embedding space via CLIP and fine-tunes LLaMA/Vicuna; has a 7B checkpoint used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Maze Navigation; Blocks World (Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid-based navigation and block-stacking planning requiring image perception and sequential reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot evaluation with interleaved images+text; also LoRA fine-tuning on 10k VSP training points (open-source fine-tuning experiment). Trained/evaluated on both perception subtasks (T1-T3) and reasoning (T4) and main tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard prompt-driven reasoning; fine-tuning via LoRA on VSP training data improves capabilities. No explicit chain-of-thought prompting used in core zero-shot evaluations (in-context examples tested separately for other models).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot decomposed-task accuracies (Table 3): Maze T1=0.49, T2=0.27, T3=0.21, T4=0.54; Blocks World T1=0.22, T2=0.21, T3=0.24, T4=0.55. After fine-tuning (Table 5): large gains reported (example highlights LLaVA fine-tune yielding near-perfect in some Blocks World metrics and big improvements in Maze T2/T4).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Zero-shot performance close to random on many tasks indicates poor out-of-the-box spatial reasoning from images; fine-tuning on VSP substantially improves performance, indicating the model capacity can learn the perceptual-to-reasoning mapping with data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms private models in zero-shot but shows ability to improve via task-specific fine-tuning more than InternLM in the experiments. Open-source limitations (context length, multi-image handling) noted as causes of poor zero-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Zero-shot performance often close to random; limited context window and tokenized image embeddings (image tokens consume much of context) restrict multi-image tasks. Improvements require task-specific fine-tuning on many examples (10k used).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8556.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8556.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternLM-XComposer2 (internlm-xcomposer2-7b / internlm-xcomposer2-vl-7b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source vision-language family focused on free-form text-image composition evaluated here in two variants; shows limited zero-shot performance but can improve with fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>internlm-xcomposer2-7b / internlm-xcomposer2-vl-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source VLM family aimed at compositional text-image understanding; experiments used both general and vision-focused checkpoints (7B variants referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Maze Navigation; Blocks World (Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid navigation and stacked-block reconfiguration tasks requiring 2D visual perception and sequential planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Evaluated zero-shot with image+text prompts; fine-tuning (LoRA) on VSP training data performed in separate experiments for open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt-based reasoning; fine-tuning for task adaptation. No external planner; in-context example experiments reported for private models but main internLM analysis centered on zero-shot and fine-tune.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot decomposed-task accuracies (Table 3): InternLM T1=0.48, T2=0.27, T3=0.29, T4=0.58 (maze) and Blocks World T1=0.25, T2=0.32, T3=0.26, T4=0.53. Fine-tuning (Table 5) yields variable improvements (some tasks improved notably, others less so).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Zero-shot performance near low-to-moderate levels; fine-tuning yields measurable improvements, indicating capacity exists but generalization and perception are limited out-of-the-box.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs similarly to other open-source baselines and much worse than private models in zero-shot. LLaVA showed larger fine-tuning gains in the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Context length and multi-image input constraints hinder zero-shot performance; requires fine-tuning to reach competitive performance on VSP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8556.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8556.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternLM-VL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternLM-XComposer2-VL (internlm-xcomposer2-vl-7b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-focused variant of InternLM-XComposer2 (7B) evaluated on VSP; shows lower zero-shot performance than some other open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>internlm-xcomposer2-vl-7b</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-focused checkpoint of InternLM-XComposer2 family (7B) intended for free-form text-image composition and comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Maze Navigation; Blocks World (Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid and stacked-block planning tasks requiring image-based perception and sequential reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot evaluations on VSP with interleaved image and text prompts; fine-tuning experiments considered for open-source family (internLM variants included in fine-tuning comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt-driven reasoning; no additional symbolic planner. Fine-tuning via LoRA for open-source experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot decomposed-task accuracies (Table 3): Maze T1=0.41, T2=0.20, T3=0.17, T4=0.47; Blocks World T1=0.22, T2=0.20, T3=0.20, T4=0.53.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Generally low zero-shot performance indicates limited effective spatial reasoning from images; no strong evidence of robust reasoning without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs worse than LLaVA and InternLM-general in many zero-shot subtasks; fine-tuning can mitigate but authors observed varying improvement magnitudes across architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Poor zero-shot handling of multi-image interleaved prompts and context-length constraints; perception bottlenecks evident in text-replacement ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8556.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8556.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructBLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructBLIP (blip2-t5-instruct-flant5xxl)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned BLIP-2 based VLM assessed on the VSP benchmark, showing low-to-moderate zero-shot performance on spatial perception and reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>blip2-t5-instruct-flant5xxl (InstructBLIP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A BLIP-2 based vision-language model instruction-tuned for general VQA and vision-language tasks; used here as an open-source baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Maze Navigation; Blocks World (Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid navigation and block rearrangement requiring image perception and sequential planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot VSP evaluations with image+text prompts; no dedicated fine-tuning for InstructBLIP reported in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt-driven reasoning; standard VQA-style instruction tuning. No chain-of-thought or planner augmentation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot decomposed-task accuracies (Table 3): Maze T1=0.44, T2=0.23, T3=0.21, T4=0.37; Blocks World T1=0.21, T2=0.16, T3=0.22, T4=0.47.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Low scores on spatial relation and environment-perception tasks indicate limited spatial reasoning in VSP zero-shot setting; no fine-tuning results reported that would show learning capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms private models and some open-source peers like SPHINX on certain subtasks; much lower than GPT-4o/GPT-Vision on VSP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Perception and multi-image context handling limit performance; often close to random on harder difficulties.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8556.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8556.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPHINX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPHINX-v2-1k</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source VLM variant that unfreezes the LLM during pretraining to enhance cross-modal alignment; evaluated on VSP and performs modestly on some perception/reasoning subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SPHINX-v2-1k</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VLM that jointly mixes weights/tasks/visual embeddings to improve multimodal alignment; used as an open-source baseline in the VSP evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Maze Navigation; Blocks World (Blocksworld)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid maze and stacked-block planning requiring image-based perception and sequential action planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot VSP evaluation with interleaved images+text prompts; no fine-tuning results highlighted for SPHINX in main tables.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompted stepwise reasoning per VSP prompt template; no external planning or symbolic modules described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot decomposed-task accuracies (Table 3): Maze T1=0.56, T2=0.28, T3=0.32, T4=0.59; Blocks World T1=0.24, T2=0.33, T3=0.27, T4=0.58.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Moderate T1/T4 scores in maze indicate some competence at single-object perception and simulated reasoning steps, but environment-perception (T3) and many block subtasks remain weak.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Better than some open-source baselines on certain maze tasks but well below private models; overall open-source family underperforms private multimodal models in zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Perception limitations on environment-level descriptions and sensitivity to increasing difficulty (map size/plan length).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning <em>(Rating: 2)</em></li>
                <li>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning <em>(Rating: 2)</em></li>
                <li>Blocksworld revisited: Learning and reasoning to generate event-sequences from image pairs <em>(Rating: 2)</em></li>
                <li>On the planning abilities of large language models -a critical investigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8556",
    "paper_id": "paper-270878452",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "Gemini",
            "name_full": "Gemini-1.0-Pro-Vision",
            "brief_description": "A private multimodal vision-language model evaluated in this paper on visual spatial planning (maze navigation and blocksworld); tested zero-shot, with in-context examples, and with textual-input ablations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-1.0-Pro-Vision",
            "model_description": "Private multimodal VLM from the Gemini family with integrated vision and language capabilities; used here as a representative state-of-the-art VLM for image understanding and reasoning.",
            "model_size": null,
            "puzzle_name": "Maze Navigation; Blocks World (Blocksworld)",
            "puzzle_type": "Grid-based path-finding (2D grid maze) and block-manipulation (stacked blocks planning) requiring 2D spatial perception and sequential action planning.",
            "task_setup": "VSP benchmark: interleaved image + text prompts. Main tasks: generate a safe action sequence (L/R/U/D) to reach the goal in an observable maze, or produce a shortest moving plan to reach a target block configuration in Blocks World. Evaluations run zero-shot (primary), with 1/2/4-shot in-context examples, with textual-only ablations (images replaced by exact textual descriptions/tables), and fine-tuning (open-source models only). Automatic script-based scoring allowed multiple valid solutions.",
            "mechanisms_or_strategies": "No specialized symbolic engine; prompted to perform stepwise analysis and verification (prompt encourages step-by-step checking of each action). Evaluated with in-context examples (0/1/2/4-shot). Paper also used text-replacement to simulate perfect perception; no explicit chain-of-thought or external planner was required by the benchmark (although prompts encourage analytic steps).",
            "performance_metrics": "Decomposed-task accuracies (Table 3): Maze T1=0.58, T2=0.56, T3=0.33, T4=0.49; Blocks World T1=0.86, T2=0.51, T3=0.54, T4=0.55. Main-task success rates: described as substantially below perfect and dropping with difficulty; Gemini often around ~50% on easier settings and much lower as difficulty rises (paper reports poor performance on some simple 3x3 or 1-step tasks). All evaluations zero-shot unless noted.",
            "evidence_of_spatial_reasoning": "Partial: Gemini shows nontrivial performance on perception (T1/T2) and moderate reasoning (T4) but fails on environment-perception (T3) in some maze cases. Ablation replacing images with exact textual descriptions improved performance, indicating failures are dominated by perception rather than pure reasoning. In-context examples yielded small or inconsistent gains.",
            "comparisons": "Compared against other private models (GPT-Vision, Claude-3, GPT-4o) and open-source models (LLaVA, InternLM, InternLM-VL, InstructBLIP, SPHINX). Gemini's performance is middling among private models (worse than GPT-4o/GPT-Vision on many tasks). Open-source models perform close to random in many tasks. Fine-tuned open-source models improved substantially.",
            "limitations_or_failure_cases": "Fails on some simple spatial planning cases (3x3 mazes, single-step block moves). Performance decays quickly with map size/path length. Perception bottleneck (visual input) is a major failure mode; textual replacement improves scores but does not fully close gaps, indicating remaining reasoning limitations. Context-length and multi-image handling are challenges for open-source analogs noted in analysis.",
            "uuid": "e8556.0",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-Vision",
            "name_full": "GPT Vision (GPT-4/Turbo with vision)",
            "brief_description": "A private multimodal GPT-based vision-language model (referred to as GPT-Vision/GPT-4V in the paper) evaluated on VSP's mazes and blocksworld; shows relatively strong perception and reasoning among evaluated models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-Vision (turbo-2024-04-09 with vision)",
            "model_description": "OpenAI's multimodal GPT variant with integrated vision capabilities (used here via a vision-enabled turbo model). Known for strong text understanding and applied vision reasoning.",
            "model_size": null,
            "puzzle_name": "Maze Navigation; Blocks World (Blocksworld)",
            "puzzle_type": "Grid-based navigation and stacked-block planning requiring accurate visual perception of 2D layouts and sequential planning.",
            "task_setup": "Same VSP setup: interleaved image+text prompts; zero-shot primary evaluation; additionally in-context examples (1/2/4-shot) and text-replacement ablations were used.",
            "mechanisms_or_strategies": "Prompt instructs stepwise interpretation and verification. Evaluated with in-context examples; no external symbolic planner used. Textual-input ablations used to measure perception vs reasoning.",
            "performance_metrics": "Decomposed-task accuracies (Table 3): Maze T1=0.56, T2=0.27, T3=0.46, T4=0.56; Blocks World T1=0.73, T2=0.80, T3=0.70, T4=0.71. Paper notes GPT-Vision can exceed ~50% success on easy 3x3 mazes but performance falls to ~10% on 8x8 mazes in main task (example illustrating steep decay).",
            "evidence_of_spatial_reasoning": "Shows stronger evidence than many models: relatively high accuracy on Blocks World spatial relations (T2) and reasoning (T4). Textual-input ablation improves performance across tasks, indicating some remaining perception limitations. In-context examples produce small improvements but not dramatic.",
            "comparisons": "Among the private models assessed, GPT-Vision is competitive; GPT-4o often outperforms it on some tasks. Outperforms most open-source models by large margins on VSP tasks.",
            "limitations_or_failure_cases": "Sharp performance drop with increased maze size and number of required steps. Visual perception of image inputs remains a bottleneck (text-replacement experiments increase accuracy). Some tasks (e.g., maze T2) show low scores indicating inconsistent spatial relation extraction from images.",
            "uuid": "e8556.1",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Claude-3",
            "name_full": "Claude-3 (claude-3-sonnet-20240229)",
            "brief_description": "Anthropic's Claude-3 VLM variant evaluated in the paper; shows mixed performance across perception and reasoning sub-tasks in VSP.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "claude-3-sonnet-20240229",
            "model_description": "Anthropic's Claude-3 family VLM used here as a vision+language baseline; described in the paper as strong at advanced reasoning and vision analysis.",
            "model_size": null,
            "puzzle_name": "Maze Navigation; Blocks World (Blocksworld)",
            "puzzle_type": "2D grid navigation and block stacking planning requiring environment perception and sequential reasoning.",
            "task_setup": "VSP benchmark with interleaved images and textual prompts; evaluated zero-shot, with in-context examples and text-input ablations as in other models.",
            "mechanisms_or_strategies": "Prompt-driven reasoning with explicit stepwise checks; in-context examples tested; no external tool use described.",
            "performance_metrics": "Decomposed-task accuracies (Table 3): Maze T1=0.45, T2=0.67, T3=0.32, T4=0.61; Blocks World T1=0.43, T2=0.53, T3=0.49, T4=0.66.",
            "evidence_of_spatial_reasoning": "Moderate evidence: relatively strong on maze T2 (spatial relation perception) and maze T4 (spatial reasoning). Textual ablations showed performance gains for many models, indicating perception remains critical.",
            "comparisons": "Performs comparably to Gemini on several subtasks; typically behind GPT-4o but competitive with other private models. Outperforms most open-source models on several sub-tasks.",
            "limitations_or_failure_cases": "Perception failures on environment-perception questions (T3) and general vulnerability to increased difficulty (map size and plan length). No human baseline numeric comparisons provided.",
            "uuid": "e8556.2",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (gpt-4o-2024-05-13)",
            "brief_description": "Latest multimodal GPT variant evaluated in the benchmark; achieves the best overall performance across many VSP tasks but still exhibits substantial failure modes at higher difficulty.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4o-2024-05-13",
            "model_description": "A recent multimodal large language model (GPT-4o) supporting combinations of text, image, and audio; used here for VSP evaluation.",
            "model_size": null,
            "puzzle_name": "Maze Navigation; Blocks World (Blocksworld)",
            "puzzle_type": "Grid-navigation and block manipulation requiring 2D spatial perception and sequential planning.",
            "task_setup": "VSP interleaved image + text prompts; zero-shot primary evaluation; in-context examples and text-input ablations applied.",
            "mechanisms_or_strategies": "Prompted step-by-step analysis; tested with in-context examples. No explicit external search/planning added by authors.",
            "performance_metrics": "Decomposed-task accuracies (Table 3): Maze T1=0.58, T2=0.67, T3=0.58, T4=0.74; Blocks World T1=0.95, T2=0.90, T3=0.90, T4=0.76. Paper states GPT-4o performs best across tasks but still makes frequent mistakes under higher difficulty.",
            "evidence_of_spatial_reasoning": "Strongest of the evaluated models: high accuracy on blocks world perception and relation tasks and relatively high maze reasoning (T4). Textual-input ablations still boost performance, indicating perception remains a contributor to errors but GPT-4o shows better integrated perception+reasoning.",
            "comparisons": "Outperforms other private models (Gemini, GPT-Vision, Claude-3) and considerably outperforms open-source baselines. Still below human-level for reliably solving larger/harder instances.",
            "limitations_or_failure_cases": "Not immune to rapid decay with difficulty; still fails on many larger mazes and longer block-moving plans. No direct human baseline reported; qualitative statement that tasks would be simple for humans.",
            "uuid": "e8556.3",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LLaVA",
            "name_full": "LLaVA-V1.6-VICUNA-7B",
            "brief_description": "Open-source VLM based on LLaMA/Vicuna instruction-tuned with CLIP-based visual encoder; evaluated zero-shot and after fine-tuning, showing large improvement when trained on VSP data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLAVA-V1.6-VICUNA-7B",
            "model_description": "Open-source vision-language model that projects images into text embedding space via CLIP and fine-tunes LLaMA/Vicuna; has a 7B checkpoint used in experiments.",
            "model_size": "7B",
            "puzzle_name": "Maze Navigation; Blocks World (Blocksworld)",
            "puzzle_type": "2D grid-based navigation and block-stacking planning requiring image perception and sequential reasoning.",
            "task_setup": "Zero-shot evaluation with interleaved images+text; also LoRA fine-tuning on 10k VSP training points (open-source fine-tuning experiment). Trained/evaluated on both perception subtasks (T1-T3) and reasoning (T4) and main tasks.",
            "mechanisms_or_strategies": "Standard prompt-driven reasoning; fine-tuning via LoRA on VSP training data improves capabilities. No explicit chain-of-thought prompting used in core zero-shot evaluations (in-context examples tested separately for other models).",
            "performance_metrics": "Zero-shot decomposed-task accuracies (Table 3): Maze T1=0.49, T2=0.27, T3=0.21, T4=0.54; Blocks World T1=0.22, T2=0.21, T3=0.24, T4=0.55. After fine-tuning (Table 5): large gains reported (example highlights LLaVA fine-tune yielding near-perfect in some Blocks World metrics and big improvements in Maze T2/T4).",
            "evidence_of_spatial_reasoning": "Zero-shot performance close to random on many tasks indicates poor out-of-the-box spatial reasoning from images; fine-tuning on VSP substantially improves performance, indicating the model capacity can learn the perceptual-to-reasoning mapping with data.",
            "comparisons": "Underperforms private models in zero-shot but shows ability to improve via task-specific fine-tuning more than InternLM in the experiments. Open-source limitations (context length, multi-image handling) noted as causes of poor zero-shot performance.",
            "limitations_or_failure_cases": "Zero-shot performance often close to random; limited context window and tokenized image embeddings (image tokens consume much of context) restrict multi-image tasks. Improvements require task-specific fine-tuning on many examples (10k used).",
            "uuid": "e8556.4",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "InternLM",
            "name_full": "InternLM-XComposer2 (internlm-xcomposer2-7b / internlm-xcomposer2-vl-7b)",
            "brief_description": "Open-source vision-language family focused on free-form text-image composition evaluated here in two variants; shows limited zero-shot performance but can improve with fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "internlm-xcomposer2-7b / internlm-xcomposer2-vl-7b",
            "model_description": "Open-source VLM family aimed at compositional text-image understanding; experiments used both general and vision-focused checkpoints (7B variants referenced).",
            "model_size": "7B",
            "puzzle_name": "Maze Navigation; Blocks World (Blocksworld)",
            "puzzle_type": "Grid navigation and stacked-block reconfiguration tasks requiring 2D visual perception and sequential planning.",
            "task_setup": "Evaluated zero-shot with image+text prompts; fine-tuning (LoRA) on VSP training data performed in separate experiments for open-source models.",
            "mechanisms_or_strategies": "Prompt-based reasoning; fine-tuning for task adaptation. No external planner; in-context example experiments reported for private models but main internLM analysis centered on zero-shot and fine-tune.",
            "performance_metrics": "Zero-shot decomposed-task accuracies (Table 3): InternLM T1=0.48, T2=0.27, T3=0.29, T4=0.58 (maze) and Blocks World T1=0.25, T2=0.32, T3=0.26, T4=0.53. Fine-tuning (Table 5) yields variable improvements (some tasks improved notably, others less so).",
            "evidence_of_spatial_reasoning": "Zero-shot performance near low-to-moderate levels; fine-tuning yields measurable improvements, indicating capacity exists but generalization and perception are limited out-of-the-box.",
            "comparisons": "Performs similarly to other open-source baselines and much worse than private models in zero-shot. LLaVA showed larger fine-tuning gains in the authors' experiments.",
            "limitations_or_failure_cases": "Context length and multi-image input constraints hinder zero-shot performance; requires fine-tuning to reach competitive performance on VSP tasks.",
            "uuid": "e8556.5",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "InternLM-VL",
            "name_full": "InternLM-XComposer2-VL (internlm-xcomposer2-vl-7b)",
            "brief_description": "A vision-focused variant of InternLM-XComposer2 (7B) evaluated on VSP; shows lower zero-shot performance than some other open-source models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "internlm-xcomposer2-vl-7b",
            "model_description": "Vision-focused checkpoint of InternLM-XComposer2 family (7B) intended for free-form text-image composition and comprehension.",
            "model_size": "7B",
            "puzzle_name": "Maze Navigation; Blocks World (Blocksworld)",
            "puzzle_type": "2D grid and stacked-block planning tasks requiring image-based perception and sequential reasoning.",
            "task_setup": "Zero-shot evaluations on VSP with interleaved image and text prompts; fine-tuning experiments considered for open-source family (internLM variants included in fine-tuning comparisons).",
            "mechanisms_or_strategies": "Prompt-driven reasoning; no additional symbolic planner. Fine-tuning via LoRA for open-source experiments.",
            "performance_metrics": "Zero-shot decomposed-task accuracies (Table 3): Maze T1=0.41, T2=0.20, T3=0.17, T4=0.47; Blocks World T1=0.22, T2=0.20, T3=0.20, T4=0.53.",
            "evidence_of_spatial_reasoning": "Generally low zero-shot performance indicates limited effective spatial reasoning from images; no strong evidence of robust reasoning without task-specific fine-tuning.",
            "comparisons": "Performs worse than LLaVA and InternLM-general in many zero-shot subtasks; fine-tuning can mitigate but authors observed varying improvement magnitudes across architectures.",
            "limitations_or_failure_cases": "Poor zero-shot handling of multi-image interleaved prompts and context-length constraints; perception bottlenecks evident in text-replacement ablations.",
            "uuid": "e8556.6",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "InstructBLIP",
            "name_full": "InstructBLIP (blip2-t5-instruct-flant5xxl)",
            "brief_description": "An instruction-tuned BLIP-2 based VLM assessed on the VSP benchmark, showing low-to-moderate zero-shot performance on spatial perception and reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "blip2-t5-instruct-flant5xxl (InstructBLIP)",
            "model_description": "A BLIP-2 based vision-language model instruction-tuned for general VQA and vision-language tasks; used here as an open-source baseline.",
            "model_size": null,
            "puzzle_name": "Maze Navigation; Blocks World (Blocksworld)",
            "puzzle_type": "2D grid navigation and block rearrangement requiring image perception and sequential planning.",
            "task_setup": "Zero-shot VSP evaluations with image+text prompts; no dedicated fine-tuning for InstructBLIP reported in main experiments.",
            "mechanisms_or_strategies": "Prompt-driven reasoning; standard VQA-style instruction tuning. No chain-of-thought or planner augmentation reported.",
            "performance_metrics": "Zero-shot decomposed-task accuracies (Table 3): Maze T1=0.44, T2=0.23, T3=0.21, T4=0.37; Blocks World T1=0.21, T2=0.16, T3=0.22, T4=0.47.",
            "evidence_of_spatial_reasoning": "Low scores on spatial relation and environment-perception tasks indicate limited spatial reasoning in VSP zero-shot setting; no fine-tuning results reported that would show learning capacity.",
            "comparisons": "Underperforms private models and some open-source peers like SPHINX on certain subtasks; much lower than GPT-4o/GPT-Vision on VSP tasks.",
            "limitations_or_failure_cases": "Perception and multi-image context handling limit performance; often close to random on harder difficulties.",
            "uuid": "e8556.7",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "SPHINX",
            "name_full": "SPHINX-v2-1k",
            "brief_description": "An open-source VLM variant that unfreezes the LLM during pretraining to enhance cross-modal alignment; evaluated on VSP and performs modestly on some perception/reasoning subtasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SPHINX-v2-1k",
            "model_description": "VLM that jointly mixes weights/tasks/visual embeddings to improve multimodal alignment; used as an open-source baseline in the VSP evaluations.",
            "model_size": null,
            "puzzle_name": "Maze Navigation; Blocks World (Blocksworld)",
            "puzzle_type": "2D grid maze and stacked-block planning requiring image-based perception and sequential action planning.",
            "task_setup": "Zero-shot VSP evaluation with interleaved images+text prompts; no fine-tuning results highlighted for SPHINX in main tables.",
            "mechanisms_or_strategies": "Prompted stepwise reasoning per VSP prompt template; no external planning or symbolic modules described.",
            "performance_metrics": "Zero-shot decomposed-task accuracies (Table 3): Maze T1=0.56, T2=0.28, T3=0.32, T4=0.59; Blocks World T1=0.24, T2=0.33, T3=0.27, T4=0.58.",
            "evidence_of_spatial_reasoning": "Moderate T1/T4 scores in maze indicate some competence at single-object perception and simulated reasoning steps, but environment-perception (T3) and many block subtasks remain weak.",
            "comparisons": "Better than some open-source baselines on certain maze tasks but well below private models; overall open-source family underperforms private multimodal models in zero-shot.",
            "limitations_or_failure_cases": "Perception limitations on environment-level descriptions and sensitivity to increasing difficulty (map size/plan length).",
            "uuid": "e8556.8",
            "source_info": {
                "paper_title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning",
            "rating": 2
        },
        {
            "paper_title": "Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning",
            "rating": 2
        },
        {
            "paper_title": "Blocksworld revisited: Learning and reasoning to generate event-sequences from image pairs",
            "rating": 2
        },
        {
            "paper_title": "On the planning abilities of large language models -a critical investigation",
            "rating": 1
        }
    ],
    "cost": 0.0180975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs
2 Jul 2024</p>
<p>Qiucheng Wu qiucheng@ucsb.edu 
Handong Zhao 
Adobe Research</p>
<p>Michael Saxon 
Trung Bui 
Adobe Research</p>
<p>William Yang Wang 
Yang Zhang 
MIT-IBM Watson AI Lab</p>
<p>Shiyu Chang 
Uc Santa Barbara 
VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for VLMs
2 Jul 20244F4B059B9FCFF061BF9CA5582A269EC4arXiv:2407.01863v1[cs.CL]
Vision language models (VLMs) are an exciting emerging class of language models (LMs) that have merged classic LM capabilities with those of image processing systems.However, the ways that these capabilities combine are not always intuitive and warrant direct investigation.One understudied capability in VLMs is visual spatial planning-the ability to comprehend the spatial arrangements of objects and devise action plans to achieve desired outcomes in visual scenes.In our study, we introduce VSP, a benchmark that 1) evaluates the spatial planning capability in these models in general, and 2) breaks down the visual planning task into finer-grained sub-tasks, including perception and reasoning, and measure the LMs capabilities in these sub-tasks.Our evaluation shows that both open-source and private VLMs fail to generate effective plans for even simple spatial planning tasks.Evaluations on the fine-grained analytical tasks further reveal fundamental deficiencies in the models' visual perception and bottlenecks in reasoning abilities, explaining their worse performance in the general spatial planning tasks.Our work illuminates future directions for improving VLMs' abilities in spatial planning.Our benchmark is publicly available at https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning.Preprint.Under review.</p>
<p>Introduction</p>
<p>The rapid advancement of large language models has driven considerable growth in their capabilities to produce fluent text in many domains, generating outputs exhibiting potential "reasoning" and "understanding" abilities [1,2,3,4].Recently, vision language models (VLMs) have advanced on LMs through additional training on native image inputs, to achieve impressive performance generating text describing and relating to input images [5,6,7,8,9], with applications in image captioning, visual question answering, visual reasoning, and others [10,11,12,13].The swift evolution of VLMs has enabled them to tackle increasingly sophisticated tasks that require multiple emerging abilities in complex scenarios.However, as model capabilities and deployment needs advance, the challenges in usefully evaluating them grow in kind.</p>
<p>Planning is a fundamental capability in intelligent systems that is particularly contested in LMs [14], and is understudied in VLMs.Visual spatial planning refers to the task of comprehending the spatial arrangement of objects in a scene and designing action plans to achieve a desired outcome.For example, the classical maze problem can be considered a visual planning task, where an agent is given an input image describing the maze environment and is asked to produce a viable path to navigate the player from the starting position to the goal.This task requires two capabilities: image perception, which enables the agent to understand the objects, environment and spatial relations present in the image, and reasoning, which enables the agent to perform strategic decision-making.</p>
<p>Visual spatial planning is an important capability in many potential applications for VLMs, such as navigating in complex environments with autonomous driving [15,16] or manipulating objects with robotic hands [17,18].Though there have been increasingly more benchmarks to evaluate the vision processing capabilities of VLMs, few current benchmarks systematically evaluate their capability to perform visual spatial planning tasks.As shown in Table 1, existing benchmarks mostly focus on VLMs' ability to understand image content and perform visual logic reasoning [19,20,21]; however, they often overlook the ability to comprehend the spatial arrangements of entities within images and to devise spatial action plans based on practical restrictions in the visual environment.As a result, two research questions are left unanswered:  How performant are VLMs in performing visual planning tasks? What are the bottleneck capabilities, e.g., perception or reasoning, that limit the performance of VLMs in the visual planning tasks?</p>
<p>To this end, we introduce Visual Spatial Planning (VSP), a benchmark specifically designed to evaluate the spatial planning capabilities of VLMs.As illustrated in Figure 1 and Figure 2, the VSP benchmark is developed from classical maze navigation and block-moving games, where the entire environment is fully observable in the input images.In this benchmark, the VLMs are required to interpret the visual inputs, deduce the consequences of each action, and execute the designated tasks accordingly.To comprehensively evaluate the fine-grained capabilities needed for the visual spatial planning, our VSP includes 4.4K questions in 10 meticulously designed tasks that feature both simulated and photo-realistic visual environments.In addition to testing end-to-end spatial planning performance, these tasks further evaluate essential individual capabilities needed for performing visual planning, such as image perception and reasoning.</p>
<p>We apply the VSP benchmark to evaluate existing state-of-the-art VLMs, including both opensource and private models.Surprisingly, we find that even the most competitive VLMs sometimes struggle in performing the simplest visual planning tasks, such as a 3x3 maze problem or an one-step block-moving task.Our fine-grained capability analysis further reveals that existing VLMs have flaws in reasoning and bigger bottlenecks in perception.We believe the VSP benchmark highlights critical weaknesses in current VLMs and sheds light on future directions for enhancing their spatial understanding and planning capabilities.</p>
<p>Related Work 2.1 General planning in LMs</p>
<p>Planning has been a central focus of research in AI.Traditional work in AI planning includes using formal languages to represent and solve planning problems [24], and developing algorithms like dynamic programming and reinforcement learning to explore environments and formulate viable plans [25,26].While these works mostly focus on predefined and restricted environments, recently, with the advancement of LMs, it has become intriguing to study whether LMs, with the potential to be general intelligent agents, can perform planning in different settings and environments [27,28,29].Many works explore the best ways to activate the planning capabilities of LMs, including divide and conquer [30,31,32,33], grounding outputs in admissible actions [34,35], retrospecting and refining [36,37], and leveraging external tools [38,39].Meanwhile, with the increasing capabilities of LMs, growing research efforts are now dedicated to benchmark their planning capabilities in various complex environments [40,41,42].</p>
<p>Main Task</p>
<p>Sub-Tasks
Spatial</p>
<p>Spatial and visual planning in LMs</p>
<p>Many general planning tasks in LMs involve understanding visual environments and comprehending spatial information.In robotics and embodied agent studies, LMs play a crucial role in grounding visual entities with references in open-domain instructions and formulating plans based on spatial constraints.Consequently, they are increasingly used in physically grounded scenarios such as object rearrangement [17,18], cooking [43,44], and navigation [35,34].LMs are also used in AIGC to propose spatial arrangements of entities following instructions [45].While realistic planning tasks align with real needs, their complexity and expansive action spaces limit the analysis of LMs' detailed planning capabilities.Therefore, research also focuses on LMs' planning in simulated environments and games.For example, mystery blocksworld is a dynamically generated set of blocksworld tasks to test generalization in LMs [14].Additionally, many text games have been introduced to test LMs' abilities in spatial understanding and imagination [46,40,47,48].However, most of these studies transform visual information into text inputs, thus not directly measuring LMs' visual abilities.</p>
<p>Benchmarks for VLMs</p>
<p>VLMs have inherited and advanced many intriguing features from text-only LMs [47,49].Benchmarks for VLMs have rapidly emerged to evaluate performance in areas such as image content understanding [19,50], perception [51,52], knowledge [20,21,53], and reasoning [19,20,54].Recently, there are also emerging benchmarks focusing on the capability of understanding multiple images in long context and complex realistic environments [55,56].While these benchmarks successfully quantify VLMs' abilities in many fields, their capabilities in spatial understanding and reaction are relatively under-explored.Some benchmarks cover spatial relations understanding [22,23], but often overlook the ability to devise complex spatial action plans based on visual environment constraints.We focus on visual spatial planning -the ability to comprehend spatial arrangements of objects and devise action plans to achieve specific outcomes.We fill the gap in benchmarking VLM abilities for visual spatial planning and highlight future directions for improving VLMs towards models with general intelligence.</p>
<p>3 The Visual Spatial Planning Benchmark</p>
<p>Overview of the Benchmark</p>
<p>In this benchmark, our objectives are two-fold:  quantify the visual spatial planning capabilities of current VLMs; and  uncover current capability bottlenecks that limit the effectiveness of VLMs in visual spatial planning tasks.While the first objective can be achieved through direct measurements on corresponding tasks, the second objective requires more careful benchmark design.Specifically, performing spatial planning in visual environments requires a series of cohesive steps.For example, to generate an accurate path to navigate a player to a goal, an agent needs to be able to correctly view and understand the visual map, reason to find which actions are safe or dangerous, and come up with a detailed plan to achieve the goal.Each of these steps could be challenging for a developing VLM, and understanding which of these subtasks challenge them most will drive future improvement.To this end, we propose the Visual Spatial Planning (VSP) benchmark, with the objective of measuring and diagnosing the capabilities of VLMs in producing accurate spatial plans in visual environments.The VSP benchmark consists of two scenarios:  the simulated Maze Navigation scenario, whose main task is to move a game character through a maze, and  the photo-realistic Blocks World scenario, whose main task is to move blocks from a starting configuration to a goal configuration.In each scenario, in addition to the main task, VSP introduces four sub-tasks that focus on the individual capabilities needed for the main task:</p>
<p>Main Task</p>
<p>Sub-Tasks</p>
<p> T1.Single Object Perception -Determine the characteristics of a single object;</p>
<p> T2.Spatial Relation Perception -Determine the relative positions of two objects;</p>
<p> T3.Environment Perception -Find textual descriptions that describe the visual environment;</p>
<p> T4.Spatial Reasoning -Determine the consequence of a series of actions or moves.</p>
<p>The sub-task details are designed specific to each scenario.Furthermore, to demonstrate the model's performance under different levels of environmental complexity, we establish progressive difficulty settings for each task, which are measured by parameters such as map size, minimum required number of actions, etc.We provide the details of task statistics, i.e., total number of problems, in appendix A. In what follows, we will introduce each scenario in detail, as well as the data curation and the task creation processes.</p>
<p>The Maze Navigation Scenario</p>
<p>The maze navigation scenario is inspired by the popular implementation [57] of a fully observable path-finding problem.As depicted in Figure 1 left, it simulates a classical grid world environment with a designated start and goal position, where part of the grids contain obstacles (the "holes") and cannot be passed through.</p>
<p>The main spatial planning task and the four sub-tasks are defined as follows:</p>
<p> Main Task (Spatial Planning) -Generate a safe path to navigate from the start grid to the goal;</p>
<p> T1 (Single Object Perception) -Determine if a specified grid is safe;</p>
<p> T2 (Spatial Relation Perception) -Find spatial relations between the player and the goal;</p>
<p> T3 (Environment Perception) -Find the textual description that fits the visual environment;</p>
<p> T4 (Spatial Reasoning) -Determine the consequence of a given action series.</p>
<p>An example of input image and questions is demonstrated in Figure 1.Each task is equipped with progressive adjusted difficulty settings to evaluate the model's capability under various circumstances.For Main Task and T1-T3, the difficulties are measured by the size of the map, ranging from 3x3 to 8x8, where a larger map introduces more challenges in correctly perceiving objects and planning accordingly.For task T4, since a longer path naturally introduces more challenges for reasoning, we adopt path length ranging from 1 to 9 as the difficulty measure.Please refer to Appendix A for the complete example of the question and answer in each task.</p>
<p>The Blocksword Scenario</p>
<p>Blocksworld is a widely-adopted planning problem [42,58,59].As depicted in Figure 2 left, in this scenario, the agent is given images containing sets of blocks in unique colors.These blocks are stacked vertically, forming multiple stacks on the table.The agent is asked to turn the blocks from initial state to target state through a series of moving actions.For each action, the agent can only move the top block of any stack, providing it is moved to either the table or the top of another stack.</p>
<p>Similarly, the main spatial planning task and the four sub-tasks are defined as follows:</p>
<p> Main Task (Spatial Planning) -Form a moving plan to achieve the target state of block arrangement;</p>
<p> T1 (Single Object Perception) -Determine the color of the block at a specific position;</p>
<p> T2 (Spatial Relation Perception) -Determine the spatial relation between two blocks;</p>
<p> T3 (Environment Perception) -Find the text representation that fits the visual environment;</p>
<p> T4 (Spatial Reasoning) -Determine the consequence of a given moving plan.</p>
<p>An example of input image and questions is demonstrated in Figure 2. Similar to the maze navigation scenario, each task is equipped with progressive adjusted difficulty.Specifically, in Main Task and T4, the difficulties are measured by the number of actions involved, ranging from 1 to 7, which quantifies the complexity of the action plan.On the other hand, for tasks T1-T3, which focus on perception, the difficulty is measured by the number of blocks presented in the image, ranging from 3 to 5. Please refer to Appendix A for the complete example of the questions in each task.First, in the left panel of Figure 3, we prepare the input images used for each task and scenario.In the maze navigation scenario, we generate input maps using the OpenAI Gym package [57], with modifications to ensure that the positions of the player, the goal, and the holes are all randomly generated.In the blocksworld scenario, we sample pairs of images from the BIRD dataset [59], ensuring there is at least one viable plan to move the blocks from the initial state to the target state.The images are prepared conditional on different levels of difficulty.</p>
<p>Benchmark Creation Process</p>
<p>Second, in the center panel of Figure 3, we formulate input prompts for each task.The prompt consists of interleaved text and images to provide sufficient information.For example, for maze navigation, we include images to show the appearance of elements in the map and provide example maps to better illustrate how the models should interpret the map.We invite native speakers to refine Gemini [7] 0.31 0.26 0.15 0.06 0.14 0.10 0.10 0.14 0.00 0.01 0.13 GPT-Vision [5] 0.55 0.36 0.27 0.13 0.17 0.10 0.50 0.17 0.03 0.00 0.23 Claude-3 [60] 0.52 0.33 0.16 0.15 0.16 0.09 0.12 0.03 0.00 0.00 0.16 GPT-4o [61] 0.68 0.58 0.35 0.24 0.18 0.23 0.71 0.33 0.12 0.03 0.35</p>
<p>LLaVA [6] 0.03 0.03 0.02 0.08 0.09 0.04 0.04 0.01 0.00 0.00 0.04 InternLM [62] 0.27 0.16 0.06 0.05 0.04 0.07 0.10 0.03 0.00 0.00 0.08 InternLM-VL [62] 0.15 0.14 0.08 0.04 0.02 0.05 0.02 0.00 0.00 0.00 0.05 InstructBLIP [63] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 SPHINX [64] 0.11 0.08 0.05 0.02 0.04 0.03 0.07 0.06 0.01 0.00 0.05 the prompts so that they accurately describe the task requirements.These prompts are demonstrated in Appendix A.</p>
<p>Finally, in the right panel of Figure 3, we evaluate the performance of VLMs under each task.It is worth noting that the answer for each task is often not unique.For example, in the blocksworld scenario, there can be many ways to move the blocks to reach the target state.As such, we develop scripts to automatically evaluate the answers for each task.</p>
<p>In addition to the steps above, some tasks require extra steps to construct meaningful questions, candidates, and answers, such as prompt design or example filtering.For example, in task 5 of the blocksworld scenario, the input actions must cover various valid/invalid movements, requiring filtering and balancing.The detailed steps we followed to create each task set are provided in Appendix A. We release all images, texts, and scripts to facilitate replication and scaling.</p>
<p>Experiments</p>
<p>In this section, we present evaluation results of state-of-the-art VLMs under our main tasks and sub-tasks.Our goal is to answer the following research questions:  How well can state-of-the-art VLMs perform in the visual spatial planning tasks? What are the bottleneck capabilities that limit the VLMs in visual spatial planning tasks?</p>
<p>Baselines</p>
<p>We evaluate various representative VLMs including both private and open-source models.</p>
<p>We cover the following private models:  Gemini [7] has demonstrated remarkable capabilities in image understanding and reasoning.We adopt Gemini-1.0-Pro-Vision in our experiments 1 . GPT-4 Turbo with vision [5] inherent strong text understanding capabilities from GPT-4 and is equipped with vision capabilities.We use turbo-2024-04-09 for evaluation. Claude-3 [60] is a family of VLMs strong at advanced reasoning and vision analysis.We adopt claude-3-sonnet-20240229, the default model used in chat interface and has comparable speed &amp; cost with GPT Vision. GPT-4o [61] is a recently released multimodal LM with one of the most advanced abilities in processing combination of text, audio, and image outputs.We adopt gpt-4o-2024-05-13 in experiments.</p>
<p>We cover the following open-source models:  LLaVA [6] performs instruction tuning on LLaMA and projects input image into text embedding space through pre-trained CLIP visual encoder [65].We adopt LLAVA-V1.6-VICUNA-7Bfor evaluation. InternLM-XComposer2 [62] enhances ability to understand free-form text-image composition and surpasses GPT-4V in several tasks.The latest released checkpoints include internlm-xcomposer2-7b and internlm-xcomposer2-vl-7b, with the former focusing on general text-image composition and the latter focusing on VL bench-This is a 4x4 map.</p>
<p>The player is at: row 1, column 1; The hole(s) are at: row 1, column 2; row 4, column 1;</p>
<p>The goal is at: row 4, column 4.  marks.We adopt both for evaluation. InstructBLIP [63] is a popular VLM based on pre-trained BLIP-2 [66] model.We adopt blip2-t5-instruct-flant5xxl for evaluation. SPHINX [64] unfreezes the LLM during pre-training to enhance cross-model alignment.We adopt SPHINX-v2-1k for evaluation.Additionally, we attempt to perform measurements on the latest CogVLM2 [67] fine-tuned on LLaMA-3 [68].However, since its current codebase only supports single-image input, we do not include its results.For all open-source models, we use their public released checkpoints, codes, and hyperparameter choices.
| | Col 1 | Col 2 | Col 3 | Col 4 | | Row 1 | @ | # | _ | _ | | Row 2 | _ | _ | _ | _ | | Row 3 | _ | _ | _ | _ | | Row 4 | # | _ | _ | * | -Stack</p>
<p>Main Task (Spatial Planning) Evaluation</p>
<p>First, we present the main task evaluation results for both the maze and the Blocksword scenarios, which reflect the general spatial planning capabilities of existing VLMs.All the evaluation in this section is conducted under zero-shot setting without any fine-tuning or in-context learning.</p>
<p>Evaluations with in-context learning and fine-tuning are presented in Sections 4.5 and 4.6.</p>
<p>The performance is demonstrated in Table 2.Each column represents a difficulty level, which is measured by the size of the map (3 represents 3x3 maps) in Maze Navigation and by the minimum number of steps in Blocksword.From the table, we summarize our findings as follows:</p>
<p>VLMs have considerable room for improvement in spatial planning tasks.We observe that both private and open-source models exhibit sub-optimal performance in various scenarios.In particular, open-source models face significant challenges and rarely succeed in these tasks.Besides, even the most capable private models could frequently make mistakes on relatively simple tasks, such as those involving a 3x3 size map or a single-step block moving task.Considering that these tasks would be simple for humans, the VSP benchmark poses a substantial challenge to VLMs, illustrating that current VLMs have considerable potential for improvement in spatial planning tasks.</p>
<p>Quick performance decay as difficulty increases.We observe a significant drop in the success rates of VLMs as task difficulty escalates.For example, GPT-Vision may achieve a success rate of over 50% on 3x3 size maps, but this plummets to just 10% on 8x8 maps.Analyzing the impact of increased difficulty, we identify two major challenges for the models: First, increasing size of the map in maze navigation scenario could make it difficult for the model to accurately perceive the positions of elements within the map.Second, the increase in both map size and the number of steps required for moving blocks heightens the challenge for the model to reason deeply through the entire path and devise a complete, viable solution.In the following experiments, we focus on these two factors and provide in-depth analysis with subsequent tasks.</p>
<p>Challenges in open-source models.Finally, we note that open-source models often face challenges when evaluating on these tasks.We identify two main factors. Context length: Open-source models typically have significantly shorter context windows compared to private models.Besides, image embeddings can occupy many tokens.Thus, these models may not have enough capacity to understand the complete inputs.For example, LLAVA-V1.6-VICUNA-7B is trained with a maximum context window of 2048 tokens, while each image consumes 576 tokens.Consequently, when fed with multiple images and relatively long texts in our tasks, the total token length may surpass training, resulting in poor performance. Multiple image input: Our tasks require the model to understand multiple images interleaved with text inputs, whereas many open-source models are only trained with single-image inputs, with the image positioned at the start of the input.To further explore their potential in our tasks, we assess their performance after training on our inputs in Section 4.6.</p>
<p>Meanwhile, we suggest that future open-source models could consider increasing their context length and reducing restrictions on input formats to address complex and realistic tasks effectively.From the previous observation, we identify that spatial perception and reasoning could be two important capabilities for an agent to successfully perform visual spatial planning.Next, we evaluate the perception and reasoning abilities through the remaining tasks T1-T4.Similar to previous setting, all the evaluation is conducted under zero-shot settings.</p>
<p>The performance results are presented in Table 3.We observe that the recent GPT-4o and GPT-Vision achieve good performance across a series of tasks, demonstrating a decent capability in perception and reasoning.However, the overall performance of private models hovers around 50% accuracy, which is far from satisfactory for agents requiring spatial intelligence.Furthermore, the performance of open-source models is mostly close to random guessing on these tasks, indicating a significant gap compared to private models.Besides, we note that tasks T1-T3 focus on perception abilities, and task T4 involves the ability of both understanding input images and perform reasoning.We perform further analysis to disentangle these two abilities in Section 4.4.</p>
<p>The Effects of Visual Input Perception and Reasoning</p>
<p>Previous analysis shows that even current state-of-the-art models have clear deficiencies in various aspects of visual spatial planning.In this study, we focus on disentangling the effects of perception and reasoning by exploring the performance gain assuming the model had perfect perception.</p>
<p>The key strategy of this study is to create a scenario where the model has already acquired all the necessary information that would typically be obtained through visual perception.To this end, for every input image, we produce the corresponding textual inputs and replace those images, as demonstrated in Figure 4.For the maze navigation scenario, we use either pure text descriptions or tables to depict the image.For the blocks world scenario, we use pure text descriptions.We do not use tables for the blocks world scenario because the number of blocks in each horizontal stack is usually unequal, making it difficult to form a complete table.Please refer to Appendix B for a complete example with pure text or table input.</p>
<p>The results are shown in Figure 5.We observe a clear performance improvement when using textual input across every task.This suggests image perception presents significant challenges for VLMs, and poor perception ability is a key factor in the inferior performance observed in previous tasks.Meanwhile, we observe that even with textual input, Gemini still cannot achieve decent performance on tasks that require reasoning.This indicates deficiencies in its reasoning capabilities as well.</p>
<p>In-context Learning in Visual Spatial Planning</p>
<p>In-context learning is a widely-adopted method to enhance LM's reasoning ability [4].In this analysis, we study if it boosts the visual spatial planning capabilities.We included varying numbers 0 .97 0 .90 0 .9 1 0 .7 7 0 .9 3 0 .9 5 0 .9 2 0 .7 0
GPT-V T1 Gemini GPT-V T2 Gemini GPT-V T4 Gemini GPT-V</p>
<p>Maze Navigation Scenario</p>
<p>Image Text Table
GPT-V T1 Gemini GPT-V T2 Gemini GPT-V T4 Gemini GPT-V</p>
<p>Fine-tuning in VSP Tasks</p>
<p>Finally, we assess the capabilities of the open-source model through dedicated training for each task.</p>
<p>In these tasks, the model is trained on 10k data points.We use the default hyperparameters provided in the official repo.The results, shown in Table 5, demonstrate clear performance improvements for both models across a series of tasks, highlighting their potential in spatial planning.Additionally, we observe that LLaVA shows greater improvement compared to InternLM, suggesting that different model architectures may exhibit varying levels of efficacy in spatial planning capabilities.</p>
<p>Conclusion</p>
<p>We present VSP, a benchmark measuring and diagnosing the visual spatial planning capabilities in VLMs.The VSP quantifies the model's performance through a series of carefully designed tasks, with main tasks focusing on the general spatial planning abilities and sub-tasks focusing on the individual capabilities needed for the main task.Experiments on current models show that both private models and open-source models fail to generate effective plans for even simple spatial planning tasks, and further analyses expose their bottlenecks in spatial perception and reasoning abilities.Our work illuminates future directions for improving VLMs' abilities in spatial planning.</p>
<p>A Details of Benchmarks A.1 Additional Task Implementation Process and Statistics</p>
<p>In Section 3.4, we described our general benchmark creation process.Our task creation process can be divided into three stages: (a) preparing the input image; (b) formulating task prompts; (c) developing scripts for auto evaluation.Additionally, some specific tasks require extra steps to implement, such as question and answer generation.In what follows, we describe these steps in detail.</p>
<p>Maze Navigation, Main Task In this task, agents need to find a safe path to navigate from the start grid to the goal.We adjust the map generation mechanisms to ensure the positions of the start grid, the goal, and the holes are all randomly generated, while ensuring there is at least one viable safe path from the start grid to the goal.For each grid, the probability that it contains the hole is 20%.</p>
<p>Maze Navigation, T1 In this task, agents need to identify whether a specific grid is safe (i.e., whether it contains a hole or not).We randomly sample a row number and a column number and ask the safety question for this randomly chosen grid in each problem of this task.Additionally, to prevent the model from patterned guessing and achieving falsely high ratings (e.g., answering "not safe" for all images and obtaining high accuracy scores), we regenerate the map for this task to ensure that the safe and unsafe grids each comprise around 50% of the total grids in a single map.</p>
<p>Maze Navigation, T3 In this task, agents need to find the correct textual description that fits the visual environment.For each problem, we prepare four textual description candidates.One candidate is the correct answer, one candidate has the correct size but an incorrect map arrangement, and the other two candidates have the wrong size.The candidates are shuffled to prevent the model from making random guesses.</p>
<p>Maze Navigation, T4 In this task, agents need to determine if the given action series is safe or not.Similarly, to prevent the models from achieving falsely high ratings through guessing, we generate the action series to ensure that around 50% of them are safe and the other 50% are not.For the unsafe paths, the particular step in which the player steps into a hole is also randomly chosen.</p>
<p>Blocks World, T2 In this task, agents need to determine the spatial relation between two designated blocks.In addition to the directional relation ("above" and "below"), we note that it is important for agents to recognize if two blocks are at the same stack or not.Therefore, we design the following four candidates for this question: (A) The first block is directly above the second block, and they are in the same stack; (B) The first block is directly below the second block, and they are in the same stack; (C) The two blocks are at different stacks; (D) At least one of the mentioned blocks do not exist in the presented image.</p>
<p>Blocks World, T4 In this task, agents need to determine the consequence of a given moving plan.Specifically, some invalid moving plans contain actions that cannot be executed in the given scenario, such as trying to move a block that is covered by another block.To prevent guessing in this task, similar to the maze navigation scenario, we generate the action plans to ensure that half of the plan candidates are executable.For the plans that cannot be executed, there can be two types of errors: first, the plan may include steps that involve moving a block from or to an invalid position; second, the plan may try to move a block that does not exist.We randomly generate these errors in inputs.</p>
<p>Statistics</p>
<p>The VSP benchmark consists of 10 tasks in two scenarios.For each task, the problems are designed with different difficulty levels.Specifically, each difficulty level consists of 100 problems.In total, the VSP benchmark includes 4.4k questions.</p>
<p>A.2 Complete Prompt</p>
<p>In this subsection, we provide the complete prompts for each task.Generally, the prompts consist of a general task description at the beginning and a specific question at the end.The prompts interleave text and images in a pattern similar to a human-readable manual with reference figures.</p>
<p>Prompt for Maze Navigation scenario, Main task (Spatial Planning):</p>
<p>As a professional maze solver, your task is to analyze a grid-based map and devise an action plan that enables a player to reach the goal from the starting point without falling into any holes, using the fewest possible moves.Since coding is not within your skill set, your approach relies on logical reasoning of the map.</p>
<h2>Game Setup -The game presents a fully observable grid-based map.</h2>
<p>-The player starts at a specified grid square, with the goal located elsewhere on the map.</p>
<p>-Each grid square is either safe or contains a hole.</p>
<p>-Your goal is to guide the player to the goal while avoiding holes.</p>
<p>The following figure shows how the player, the holes (non-safe grid), the lands (safe grids), and the goals look like.</p>
<h2>Moving Rules -The action plan involves a series of moves: 'L' (left), 'R' (right), 'U' (up), or 'D' (down).</h2>
<p>-Each move transfers the player to the adjacent square in that direction, provided it is a safe square.The player cannot move more than one square at a time.</p>
<p>-Moving off the edge of the map has no effect.The player will remain at the same square.</p>
<p>-DO NOT MOVE INTO A HOLE! Falling into a hole results in defeat.</p>
<p>-Locating at the grid containing the goal results in victory.</p>
<p>We provide an example to further illustrate the rules.</p>
<p>In this provided example: -The player is at Row 1, Column 1; -The goal is at Row 4, Column 4; -There are two holes: one at Row 1, Column 2, and another at Row 4, Column 1.</p>
<p>-The player can move DOWN.This is because moving down brings them to Row 2, Column 1, and this cell is safe (without holes).</p>
<p>-Moving UP has no effects.This is because the player is already in the topmost row.</p>
<p>-Similarly, moving LEFT has no effects because the player is already in the left-most column.</p>
<p>-Moving RIGHT places the player at Row 1, Column 2. Since there is a hole at this grid, this move results in a loss.</p>
<h2>Procedure and Output Now you will solve the given maze.To solve it, please generate text exactly follow the following steps: 1.First, interpret map.List where the player is at now, where is the goal, and where are the holes.4. If succeed, output an aggregated plan using "Action plan: <PLAN>", where <PLAN> is a string concatenated action in each step.For example, "Action plan: L,L,R,U,D" meaning an action plan of left, left, right, up, and down.Double check the final action plan is consistent with the previous analysis.Do not output any extra content after the above aggregated output.</h2>
<p>Please generate the action plan for the following maze:</p>
<p>Prompt for Maze Navigation scenario, Task 1 (Single Object Perception):</p>
<p>In this task, you will analyze a maze to determine if there is a hole in a specific position.</p>
<p>The following figure illustrates the appearances of the player, holes, lands, and the goal within the maze.You will need to focus on the appearance of the hole.</p>
<p>is an example to illustrate how to analyze and answer the question:</p>
<p>Example question: Is there a hole in row 3, column 3?</p>
<p>In this example:</p>
<p>-We check the position in row 3, column 3.</p>
<p>-According to the image, it is a land square.It does not contain a hole.</p>
<p>-Therefore, you will output "<Output> No".</p>
<p>Your output should be: "<Output> No" or "<Output> Yes", depending on whether there is a hole at the specified position.Now you will analyze the following maze and answer the question: Is there a hole in row 2, column 1?</p>
<p>Prompt for Maze Navigation scenario, Task 3 (Environment Perception):</p>
<p>In this task, you will analyze a maze presented in an image.Later, you will be presented with four choices, each offering a textual representation of a candidate maze.You will need to choose the representation that exactly reflects the contents of the given image.</p>
<p>The following figure illustrates the appearances of the player, holes, lands, and the goal within the maze in the image.</p>
<p>This is how the player, the holes (non-safe grid), the lands (safe grids), and the goals look like in a map:</p>
<p>-The player is represented as "@" -The hole is represented as "#" -The safe grid is represented as "_" -The goal is represented as "*" -If the player is at the goal (at this case the game is solved), that grid is represented as "%"</p>
<p>We provide an example to illustrate how to interpret the input, candidates, and answer the question.Here is the image input:</p>
<p>Here are the textual candidates:
(A) | | Col 1 | Col 2 | Col 3 | | Row 1 | # | _ | _ | | Row 2 | # | @ | # | | Row 3 | _ | * | _ | (B) | | Col 1 | Col 2 | Col 3 | Col 4 | Col 5 | | Row 1 | _ | _ | _ | _ | _ | | Row 2 | _ | # | _ | _ | _ | | Row 3 | _ | # | * | _ | # | | Row 4 | _ | @ | _ | _ | _ | | Row 5 | _ | _ | _ | # | _ | (C) | | Col 1 | Col 2 | Col 3 | Col 4 | | Row 1 | @ | # | _ | _ | | Row 2 | _ | _ | _ | _ | | Row 3 | _ | _ | _ | _ | | Row 4 | # | _ | _ | * | (D) | | Col 1 | Col 2 | Col 3 | Col 4 | | Row 1 | _ | _ | _ | _ | | Row 2 | * | _ | _ | _ | | Row 3 | @ | _ | # | _ | | Row 4 | _ | _ | _ | # |
Here is an example of how to analyze and answer the question:
(continue in next page) (Continued)
-First, we focus on the difference of the maze shape between the candidates and the input image.</p>
<p>-We begin by examining the input image.It is a 4-by-4 maze.We then review the candidates.Candidate A is a 3-by-3 maze.Therefore, it is not the correct answer.Similarly, Candidate B is a 5-by-5 maze, which also cannot be correct.Both Candidate C and Candidate D are 4-by-4 mazes.Now we only need to choose from them.</p>
<p>-For the remaining candidates, we compare the positions of the players, goals, and the holes in the maze.</p>
<p>-We first check the input image.What is the position of the player in the image?The player is in row 1, column 1.We then check the remaining candidates.For Candidate C, the textual representation indicates the player is also at row 1, column 1, matching the input image.For Candidate D, the player is located at row 3, column 1.Hence, Candidate D is not the correct answer.</p>
<p>-We double check the remaining Candidate C, and it correctly shows the position of the player, holes, and the goal.It is therefore the correct answer.<Answer> C</p>
<p>Your output should consist of two parts: 1.First, analysis the input image and candidates similar to the reasoning process above.</p>
<ol>
<li>Following the reasoning process, output answer as "<Answer> <Choice>", where "<Choice>" is one of A,B,C,D.Important: Note that there will be only one correct answer.If you find no answer or multiple answers, you must go back and recheck your reasoning process.You are not allowed to provide 0 or more than 1 answer.Now answer the question below.Here is the image input:</li>
</ol>
<p>Here are the textual candidates: <CANDIDATES> Prompt for Maze Navigation scenario, Task 4 (Spatial Reasoning):</p>
<p>You are a maze-solving agent playing a pixelated maze video game.Mazes are presented on grid maps, where each tile can be empty land, or contain a player, hole, or goal.Each of the above tile types are represented as square pixel art images.</p>
<p>In this task, you will analyze a grid-based map and determine if a provided action plan is safe.A safe action plan avoids stepping into holes in the map.</p>
<p>The following figure illustrates the appearances of the player, holes, lands, and the goal within the maze.</p>
<h2>Moving Rules -The action plan involves a series of moves: 'L' (left), 'R' (right), 'U' (up), or 'D' (down).</h2>
<p>-Each move transfers the player to the adjacent square in that direction, provided it is a safe square.The player cannot move more than one square at a time.</p>
<p>-Moving off the edge of the map has no effect.The player will remain at the same square.</p>
<p>-DO NOT MOVE INTO A HOLE! Falling into a hole results in defeat.</p>
<p>-Locating at the grid containing the goal results in victory.</p>
<p>We provide an example to further illustrate the rules.</p>
<p>(continue in next page)</p>
<p>Prompt for Block World scenario, Main Task (Spatial Planning):</p>
<p>You are a robot that sorts and organizes colored blocks by adding and removing them to stacks.You can move them between stacks to produce a desired end state.</p>
<p>In this task, you will see two photos of blocks.These photos show the beginning and end state of the blocks.Your task is to find a shortest movement plan to transit from the beginning state to the end state.Since coding is not within your skill set, your approach relies on logical reasoning of the map.</p>
<h2>Game Setup -The stacks of blocks are presented in images.You must view and interpret the image in order to determine which blocks are in which stack and determine how to move them.</h2>
<p>-Each block has a unique color (blue, yellow, purple, orange, red, green).</p>
<p>-Blocks are stacked vertically in a stack, forming multiple stacks.All stacks are on the table.</p>
<p>-In a single move, you can only move the top block of any pile.Attempting to move lower blocks is considered an invalid move.</p>
<p>-You can either (a) move the top block to the top of another stack, or (b) place the top block on the table, creating a new stack with just one block.</p>
<p>We provide an example to further illustrate the rules:</p>
<p>This example features four blocks arranged in three stacks:</p>
<ul>
<li><em>Important Note</em>*: The order of the stacks doesn't matter in this game.Two images are considered equivalent as long as the stacks contain the same blocks, regardless of the order in which the stacks appear.For example, an image with stack A on the left and stack B on the right is equivalent to an image with stack B on the left and stack A on the right.</li>
</ul>
<h2>Procedure and Output</h2>
<p>Your output should follow this format: 1.First, analyze the starting and ending configurations, including the number of stacks and the blocks in each stack (similar to the example above).2.Then, list the moves in a step-by-step manner using the format move(SOURCE, TARGET).Remember, "SOURCE" refers to the block being moved (always the top block of a stack), and "TARGET" refers to the destination (another stack or the table The end state is:</p>
<p>Prompt for Block World scenario, Task 1 (Single Object Perception):</p>
<p>In this task, you will see a photo of blocks.You will analyze the block configuration and then answer a question regarding the color of blocks in a specific place.</p>
<h2>Game Setup -Each block has a unique color (blue, yellow, purple, orange, red, green).</h2>
<p>-Blocks are stacked vertically in a stack, forming multiple stacks.</p>
<p>-In the questions, the position of the blocks is represented as "Stack s, Level l".The stack number is counted from left to right, and the level number is counted from bottom to top.</p>
<p>We provide an example to further illustrate the setting:</p>
<p>In this example, there are four blocks in three stacks.From left to right:</p>
<p>-Stack 1 has one level.Level 1 contains a purple block.</p>
<p>-Stack 2 has one level.Level 1 contains a blue block.</p>
<p>-Stack 3 has one level.From bottom to top: level 1 has an orange block, and level 2 has a red block.As such, for the question "What is the color of the block at stack 3, level 1?", the correct answer is "<Output> orange".</p>
<h2>Procedure and Output Your output should follow this format: 1.First, analyze the block configuration; 2.Then, answer the question with the format <Output> <Color>, where <Color> is one of (blue, yellow, purple, orange, red, green).For example, "<Output> red".</h2>
<p>Now please answer the following question based on the given image below:</p>
<p>What is the color of the block at stack 1, level 2?</p>
<p>Prompt for Block World scenario, Task 3 (Environment Perception):</p>
<p>In this task, you will analyze an image containing several stacks of blocks.Later, you will be presented with four choices, each offering a textual representation of a block configuration.You will need to choose the configuration that exactly reflects the contents of the given image.## Game Setup -Each block has a unique color (blue, yellow, purple, orange, red, green).</p>
<p>-Blocks are stacked vertically in a stack, forming multiple stacks.This is an image input example:</p>
<p>This example features four blocks arranged in three stacks:</p>
<ul>
<li>We can analyze which text representation exactly reflects the configurations in the image accordingly.In this example:</li>
</ul>
<p>-The input image has 3 stacks, while Candidate A only has 2 stacks.Therefore, Candidate A is not the correct answer.</p>
<p>-Similarly, Candidate C has 4 stacks, which also cannot be correct.</p>
<p>-For Candidate B, the blocks in each stack match what's shown in the image.This is the correct answer.</p>
<p>-For Candidate D, the blocks in each stack do not match the image.For example, stack 1 in the image has a purple block, and there is no any purple block in Candidate D. So this is incorrect.</p>
<p>-Therefore, the final answer is B. ## Procedure and Output Your output should follow this format: 1.First, analyze the block configuration in the image and candidates as shown above; 2.Then, answer the question with the format <Output> <Choice>, where <Choice> is one of A,B,C,D.For example, "<Output> A".</p>
<p>Now please choose the correct textual representation based on the given image below:</p>
<p>Prompt for Maze Navigation scenario, Task 4 (Spatial Reasoning):</p>
<p>You are a robot that sorts and organizes colored blocks by adding and removing them to stacks.You can move them between stacks to produce a desired end state.In this task, you will see a photo of blocks.This photo shows the beginning state of the blocks.You will see a photo of blocks.This photo shows the beginning state of the blocks.Meanwhile, you will be provided an action sequence about moving blocks.Your task is to determine if the provided action plan can be successfully executed.## Game Setup -The block configuration is presented in the image.You must view and interpret the image in order to determine which blocks are in which stack and determine the consequence of moving.</p>
<p>-Each block has a unique color (blue, yellow, purple, orange, red, green).</p>
<p>-Blocks are stacked vertically in a stack, forming multiple stacks.</p>
<p>-A valid action can only move the top block of any stacks.Attempting to move lower blocks is considered an invalid move.</p>
<p>-For the destination, a valid move can either (a) move the top block to the top of another stack, or (b) place the top block on the table, creating a new stack with just one block.</p>
<p>We provide an example to further illustrate the rules:</p>
<p>The sequence of actions provided is:</p>
<p>B Prompt for textual input</p>
<p>In Section 4.4, we described the procedure of using textual representation instead of visual input.Below, we use the main task in the maze navigation scenario as an example to show the complete prompt after making the replacement.</p>
<p>Prompt for Maze Navigation scenario, Main task (Spatial Planning, Textual Input):</p>
<p>As a professional maze solver, your task is to analyze a grid-based map and devise an action plan that enables a player to reach the goal from the starting point without falling into any holes, using the fewest possible moves.Since coding is not within your skill set, your approach relies on logical reasoning of the map.</p>
<h2>Game Setup -The game presents a fully observable grid-based map.</h2>
<p>-The player starts at a specified grid square, with the goal located elsewhere on the map.</p>
<p>-Each grid square is either safe or contains a hole.</p>
<p>-Your goal is to guide the player to the goal while avoiding holes.</p>
<h2>Moving Rules -The action plan involves a series of moves: 'L' (left), 'R' (right), 'U' (up), or 'D' (down).</h2>
<p>-Each move transfers the player to the adjacent square in that direction, provided it is a safe square.The player cannot move more than one square at a time.</p>
<p>-Moving off the edge of the map has no effect.The player will remain at the same square.</p>
<p>-DO NOT MOVE INTO A HOLE! Falling into a hole results in defeat.</p>
<p>-Locating at the grid containing the goal results in victory.</p>
<p>We provide an example to further illustrate the rules.</p>
<p>Example Input: This is a 4x4 map.</p>
<p>The player is at: row 1, column 1; The hole(s) are at: row 1, column 2; row 4, column 1;</p>
<p>The goal is at: row 4, column 4.</p>
<p>In this provided example: -The player is at Row Please generate the action plan for the following maze: This is a 3x3 map.</p>
<p>The player is at: row 3, column 2; There is no holes in this map;</p>
<p>The goal is at: Row 1, Column 2.</p>
<p>C Prompt with in-context examples</p>
<p>In Section 4.4, we described the procedure of including in-context example in the test.Below, we use the main task in the maze navigation scenario as an example to show the complete prompt after adding in-context examples.</p>
<p>Prompt for Maze Navigation scenario, Main task (Spatial Planning, in-context examples):</p>
<p>As a professional maze solver, your task is to analyze a grid-based map and devise an action plan that enables a player to reach the goal from the starting point without falling into any holes, using the fewest possible moves.Since coding is not within your skill set, your approach relies on logical reasoning of the map.</p>
<h2>Game Setup -The game presents a fully observable grid-based map.</h2>
<p>-The player starts at a specified grid square, with the goal located elsewhere on the map.</p>
<p>-Each grid square is either safe or contains a hole.</p>
<p>-Your goal is to guide the player to the while avoiding holes.</p>
<p>The following figure shows how the player, the holes (non-safe grid), the lands (safe grids), and the goals look like.</p>
<h2>Moving Rules -The action plan involves a series of moves: 'L' (left), 'R' (right), 'U' (up), or 'D' (down).</h2>
<p>-Each move transfers the player to the adjacent square in that direction, provided it is a safe square.The player cannot move more than one square at a time.</p>
<p>-Moving off the edge of the map has no effect.The player will remain at the same square.</p>
<p>-DO NOT MOVE INTO A HOLE! Falling into a hole results in defeat.</p>
<p>-Locating at the grid containing the goal results in victory.</p>
<p>We provide an example to further illustrate the rules.</p>
<p>In this provided example: -The player is at Row 1, Column 1; -The goal is at Row 4, Column 4; -There are two holes: one at Row 1, Column 2, and another at Row 4, Column 1.</p>
<p>-The player can move DOWN.This is because moving down brings them to Row 2, Column 1, and this cell is safe (without holes).</p>
<p>-Moving UP has no effects.This is because the player is already in the topmost row.</p>
<p>-Similarly, moving LEFT has no effects because the player is already in the left-most column.</p>
<p>-Moving RIGHT places the player at Row 1, Column 2. Since there is a hole at this grid, this move results in a loss.</p>
<p>(continue in next page)</p>
<p>D Training Details</p>
<p>In this section, we describe the training details when we fine-tune LLaVA and InternLM-XComposer for our designed tasks.We perform LoRA fine-tuning, and we stick with the default hyperparameter settings in their official repo.The detailed hyperparameter choices are shown in Table 6.</p>
<p>E Complete Task Performance Results with different difficulty levels</p>
<p>In this section, we present the experimental results of models across different difficulty levels.The results are shown in Table 7.As expected, we observe that as difficulty increases, all models perform progressively worse, with some performing close to random guessing at higher difficulty levels (e.g., Task 1 in Maze Navigation scenario).We also observe that GPT-4o, the most recently released model, performs the best across different tasks, although it still frequently makes mistakes under different difficulty levels.This suggests a current bottleneck in state-of-the-art VLMs.</p>
<p>F Benchmark Documentation and Intended Users</p>
<p>The VSP benchmark is designed to evaluate VLM's capability in visual spatial planning.Visual spatial planning refers to the ability to comprehend the spatial arrangements of objects and devise action plans to achieve desired outcomes in visual scenes.The VSP benchmark consists of two scenarios:  the simulated Maze Navigation scenario, whose main task is to move a game character through a maze, and  the photo-realistic Blocks World scenario, whose main task is to move blocks from a starting configuration to a goal configuration.In each scenario, in addition to the main task, VSP introduces four sub-tasks that focus on the individual capabilities needed for the main task:</p>
<p> T1.Single Object Perception -Determine the characteristics of a single object;  T2.Spatial Relation Perception -Determine the relative positions of two objects;  T3.Environment Perception -Find textual descriptions that describe the visual environment;  T4.Spatial Reasoning -Determine the consequence of a series of actions or moves.</p>
<p>The VSP benchmark consists of 10 tasks in two scenarios.For each task, the problems are designed with different difficulty levels.Specifically, each difficulty level consists of 100 problems.In total, the VSP benchmark includes 4.4k questions.To implement the two scenarios, we utilize and enhance existing resources from two aspects.For the maze navigation scenario, we leverage OpenAI's gym [57] engine to generate input images.For the blocks world scenario, we sample input images from the BIRD dataset [59].The BIRD dataset is originally built to test an RL model's capability in understanding visual block configurations and performing sequential actions to reach the target state.We enhance it by designing auxiliary tasks (T1-T4), corresponding textual descriptions, and text prompts necessary for the input of VLMs.Additionally, we implement auto-evaluation scripts</p>
<p>Figure 1 :
1
Figure 1: Overview of the Maze Navigation scenario.</p>
<p>Figure 2 :
2
Figure 2: Overview of the Blocks World scenario.</p>
<p>Figure 3 :
3
Figure 3: Benchmark creation process.Left: We prepare input images that fulfill the task requirements with different difficulties.Mid: We formulate input prompts for each task.The input prompts consists of interleaved texts and images.Right: We develop automatic evaluation process for each task.</p>
<p>Figure 3
3
Figure 3 demonstrates our 3-stage general process for benchmark creation.</p>
<p>Figure 4 :
4
Figure 4: The visual and corresponding textual inputs.</p>
<p>Figure 5 :
5
Figure 5: Performance comparison with the visual/textual input.When the environment is described by text instead of image, the performance increases significantly.</p>
<p>Stack 1 :
1
Purple block (alone) -Stack 2: Blue block (alone) -Stack 3: From bottom to top: Orange block, Red block You can only move the top block of each stack: the purple block, the blue block, and the red block.The orange block is stuck underneath the red block and cannot be moved directly.Each move can place the block on another stack or on the table (creating a new stack of one).For instance, you could move the red block to either the blue stack or the table.</p>
<p>Spatial Planning <Question> Please find a moving plan to transit transit from the beginning state to the end state. <Answer> move(purple, green) move(red, table) move(blue, table) move(yellow, red) move(blue, yellow) Object Perception Please indicate the color of the block at stack 2, level 3? (Stack is counted from left to right; level of blocks is counted from bottom to top) Spatial Relation Perception Please indicate the spatial relation between the yellow and the red block. (A) The red block is above the yellow block (B) ... Environment Perception Please select the best text representation of the given initial state: (A) <Textual Description> (B)  Reasoning Please determine whether the given moving plan can be executed: move(red, table) move(purple, green) Initial State Target State</p>
<p>Table 2 :
2
Zero-shot success rates for the spatial planning task, at various difficulty levels.Maze navigation difficulty levels represent the maze's square grid length.Blocksworld difficulty levels correspond to the minimum number of steps to a solution.Results better than 30% are bolded.
MAZE NAVIGATIONBLOCKSWORDOVERALLDifficulty level3456781357</p>
<p>Text Input Vision Input Table Input Vision Input Pure Text Input Maze Navigation Scenario Blocks World Scenario</p>
<p>with purple block -Stack with blue block -Stack with orange block, red block, from bottom to top Pure</p>
<p>Table 3 :
3
Decomposed Capability Analysis.Similar to the spatial planning task, each task consists of test with different difficulties.Results better than 70% are bolded.Please refer to Appendix E for the complete evaluation results for different difficulties.
MAZE NAVIGATIONBLOCKSWORDTaskT1T2T3T4T1T2T3T4Random Guess0.50.25 0.250.50.17 0.25 0.250.5Gemini [7]0.58 0.56 0.33 0.49 0.86 0.51 0.54 0.55GPT-Vision [5]0.56 0.27 0.46 0.56 0.73 0.80 0.70 0.71Claude-3 [60]0.45 0.67 0.32 0.61 0.43 0.53 0.49 0.66GPT-4o [61]0.58 0.67 0.58 0.74 0.95 0.90 0.90 0.76LLaVA [6]0.49 0.27 0.21 0.54 0.22 0.21 0.24 0.55InternLM [62]0.48 0.27 0.29 0.58 0.25 0.32 0.26 0.53InternLM-VL [62]0.41 0.20 0.17 0.47 0.22 0.20 0.20 0.53InstructBLIP [63]0.44 0.23 0.21 0.37 0.21 0.16 0.22 0.47SPHINX [64]0.56 0.28 0.32 0.59 0.24 0.33 0.27 0.584.3 The Perception and Reasoning Sub-tasks Evaluation</p>
<p>Table 4 :
4
Effects of providing in-context examples.
MAZE NAVIGATIONBLOCKSWORDTaskT1T2T3T4MainT1T2T3T4MainGemini, 0-shot0.58 0.56 0.33 0.490.170.86 0.51 0.54 0.550.03Gemini, 1-shot0.50 0.66 0.31 0.480.200.91 0.68 0.71 0.590.03Gemini, 2-shot0.53 0.68 0.31 0.510.210.90 0.76 0.70 0.610.03Gemini, 4-shot0.53 0.67 0.35 0.530.190.91 0.64 0.69 0.620.06GPT-Vision, 0-shot0.56 0.27 0.46 0.560.260.73 0.80 0.70 0.710.10GPT-Vision, 1-shot0.55 0.50 0.47 0.570.280.89 0.84 0.94 0.730.11GPT-Vision, 2-shot0.55 0.63 0.50 0.560.300.90 0.83 0.95 0.710.16GPT-Vision, 4-shot0.54 0.69 0.54 0.560.290.90 0.79 0.96 0.73-</p>
<p>Table 5 :
5
Fine-tuning results for open-source models.
MAZE NAVIGATIONBLOCKS OF WORLDModelSettingT1T2T3T4MainT1T2T3T4MainLLaVAzero-shot 0.49 0.27 0.21 0.54 fine-tune 0.53 0.99 0.51 0.930.05 0.600.22 0.21 0.24 0.55 1.00 1.00 1.00 1.000.01 0.97InternLMzero-shot 0.48 0.27 0.29 0.58 fine-tune 0.52 0.59 0.91 0.590.11 0.170.25 0.32 0.26 0.53 0.29 0.44 0.69 0.620.00 0.09
of examples for Gemini and GPT-Vision (refer to Appendix C for the input examples).The result is shown in Table4.There are two key observations: First, in-context examples make some potential contributions, but they are not significant.Introducing examples only benefits in several sparse cases, such as T2 in maze navigation and T3 in blocksworld.Second, scaling in-context examples generally does not help, as illustrated by the saturated performance in each task.</p>
<ol>
<li>Then, generate an action plan to navigate to the goal step by step.At each step, you should check: (a) Where the current move leads the player to (the row and column); (b) What is in that grid.Is it a hole?Is it the goal?Is it an empty space?(c) Determine if that is a safe action.If not, correct it and re-generate the action plan.3. Next, verify if the steps successfully navigate the player to the goal without falling into the hole.If not, restart from step 2 and re-generate this step.</li>
</ol>
<p>(continue in next page) (Continued)</p>
<p>Now please generate moving plan.The beginning state is:
(Continue)).## Example Output<Analysis>Starting state: there are three stacks:-Stack 1: Purple block (alone)-Stack 2: Blue block (alone)-Stack 3: From bottom to top: Orange block, Red blockEnding state: there are three stacks:-Stack 1: Purple block (alone)-Stack 2: From bottom to top: Orange block, Blue block-Stack 3: Red block (alone)<Output>1. move(red,table)2. move(blue,orange)(continue in next page)</p>
<p>Stack 1: Purple block (alone) -Stack 2: Blue block (alone) -Stack 3: From bottom to top: Orange block, Red block Here are examples of textual representations: (A) -Stack with red block, yellow block, from bottom to top -Stack with orange block, purple block, green block, from bottom to top
(B)-Stack with purple block-Stack with blue block-Stack with orange block, red block, from bottom to top(C)-Stack with orange block-Stack with purple block-Stack with blue block-Stack with green block, yellow block, from bottom to top(D)-Stack with green block-Stack with yellow block, blue block, from bottom to top-Stack with red block, orange block, from bottom to top</p>
<p>The player can move DOWN.This is because moving down brings them to Row 2, Column 1, and this cell is safe (without holes).-MovingUPhasno effects.This is because the player is already in the topmost row.-Similarly,movingLEFT has no effects because the player is already in the left-most column.-MovingRIGHT places the player at Row 1, Column 2. Since there is a hole at this grid, this move results in a loss.## Procedure and Output Now you will solve the given maze.To solve it, please generate text exactly follow the following steps: 1.First, interpret map.List where the player is at now, where is the goal, and where are the holes.2.Then, generate an action plan to navigate to the goal step by step.At each step, you should check: (a) Where the current move leads the player to (the row and column); (b) What is in that grid.Is it a hole?Is it the goal?Is it an empty space?(c) Determine if that is a safe action.If not, correct it and re-generate the action plan.3. Next, verify if the steps successfully navigate the player to the goal without falling into the hole.If not, restart from step 2 and re-generate this step.4. If succeed, output an aggregated plan using "Action plan: <PLAN>", where <PLAN> is a string concatenated action in each step.For example, "Action plan: L,L,R,U,D" meaning an action plan of left, left, right, up, and down.Double check the final action plan is consistent with the previous analysis.Do not output any extra content after the above aggregated output.</p>
<p>1, Column 1; -The goal is at Row 4, Column 4; -There are two holes: one at Row 1, Column 2, and another at Row 4, Column 1. -</p>
<p>Table 6 :
6
Training details on LLaVA and InternLM-XComposer.Value
Learning rate2e-4SchedulerCosineEpoch1LLaVATraining data10kBatch size32Pretrained Checkpoint llava-v1.6-vicuna-7bLearning rate5e-5SchedulerCosineEpoch1InternLMTraining data10kBatch size8Pretrained Checkpoint internlm-xcomposer2-7b
The latest Gemini-1.5-Pro-Vision currently has a daily request limits of 50. Therefore, we did not include its evaluation.
Prompt for Maze Navigation scenario, Task 2 (Spatial Relation Perception):In this task, you will analyze a maze to determine the relative positions of the player and the goal.The following figure illustrates the appearances of the player, holes, lands, and the goal within the maze.You will need to focus on the player and the goal.To describe their relative positions, use the directional indicators from "Above", "Below", "Left", "Right".We provide an example to illustrate how to interpret and describe these positions:In this example: -We focus on the position of the player and the goal.-Rows: The player is at row 1, and the goal is at row 4. Here, the row number is from top to bottom.Comparing player (row=1) with goal (row=4), player is counted first.Therefore, the player is positioned above the target.-Columns: The player is at column 1, and the goal is at column 4. Here, the column number is from left to right.Comparing player (column=1) with goal (column=4).Therefore, the player is to the left of the target.-Remember that we should answer the player's position with respect to the goal, not the opposite.Therefore, we answer "Above,Left".Your output should be two parts:1. Analyze the rows and columns of the player and the goal like shown above.2. Following your analysis, output answer as "<Output> <Position>".For example, "<Output> Above,Left" means the player is above and to the left of the goal, and "<Output> Below" means the player is below the goal.Note that you should not output "Left" or "Right" if the player and the goal are at the same column, and similarly, you should not output "Above" or "Below" if the player and the goal are at the same row.Now you will analyze the following maze and determine the relative position of the player in relation to the goal.-The player can move DOWN.This is because moving down brings them to Row 2, Column 1, and this cell is safe (without holes).-Moving UP has no effects.This is because the player is already in the topmost row.-Similarly, moving LEFT has no effects because the player is already in the left-most column.-Moving RIGHT places the player at Row 1, Column 2. Since there is a hole at this grid, this move results in a loss.## Example: <Interpret>The player is at row 1, column 1, and the goal is at row 2, column 2. There are 2 holes.They are at: row 3, column 2; row 3, column 3. <Action Plan> -Moving Right (R).The player is now at row 1, column 2. This grid is safe.-Moving Down (D).The player is now at row 2, column 2. This grid is the goal, so we stop here.for each task, aiming to provide a convenient testbed for current VLMs.All the images and texts in this benchmark do not contain any personally identifiable information or offensive content.All the content in this benchmark can be accessed, reviewed, and downloaded via https://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning.As the authors of this benchmark, we assume full responsibility for any rights violations related to this benchmark.The benchmark is licensed under the MIT license.We will consistently monitor issues and pull requests for better maintenance.Additionally, we also release the test scripts to replicate our results.G LimitationIt is important to note that our proposed VSP benchmark also has limitations.First, as a VLM benchmark specifically tailored for visual spatial planning capability, the VSP does not measure a VLM's abilities in other important aspects, such as semantic understanding, factual knowledge, etc.We emphasize that VSP is not a comprehensive benchmark for VLMs, but rather a benchmark focusing on an important capability that has been mostly overlooked by existing benchmarks.Second, we also note that the appearance of objects in the image may influence models' performance.Specifically, a model might find it easier to recognize objects against a darker background in the blocks world scenario, and vice versa.The current measure is based on a single kind of object appearance, which might favor some particular models trained on similar images.An ideal measurement would assess the average performance on images with the same content but a variety of different appearances.That said, with the detailed prompt description and sufficient information provided in the image, we believe the current version of the VSP benchmark already demonstrates the deficiencies of current state-ofthe-art models in visual spatial planning.In future work, we plan to incorporate appearance/style variations in the input images for a more thorough model ability quantification.
Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothe Lachaux, Baptiste Lacroix, Naman Rozire, Eric Goyal, Faisal Hambro, Azhar, abs/2302.139712023ArXiv preprint</p>
<p>Deepseek llm: Scaling open-source language models with longtermism. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, abs/2401.029542024ArXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, abs/2401.040882024Mixtral of experts. ArXiv preprint</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Hugo Larochelle, Marc ' , Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, Hsuan-Tien Lin, NeurIPS2020. 2020. December 6-12, 2020, virtual, 2020</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, abs/2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023831ArXiv preprint</p>
<p>Advances in neural information processing systems. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, 2024. 1, 6, 836Visual instruction tuning</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, abs/2312.118052023831ArXiv preprint</p>
<p>Openflamingo: An opensource framework for training large autoregressive vision-language models. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Yonatan Kalyani Marathe, Samir Bitton, Shiori Gadre, Sagawa, abs/2308.013902023ArXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in neural information processing systems. 202235</p>
<p>Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, abs/2404.160062024ArXiv preprint</p>
<p>Exploring diverse in-context configurations for image captioning. Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, Xin Geng, Advances in Neural Information Processing Systems. 3612024</p>
<p>Prompting large language models with answer heuristics for knowledge-based visual question answering. Zhenwei Shao, Zhou Yu, Meng Wang, Jun Yu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models. Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, Sibei Yang, Advances in Neural Information Processing Systems. 202336</p>
<p>On the planning abilities of large language models -a critical investigation. Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc2023363</p>
<p>Drivevlm: The convergence of autonomous driving and large vision-language models. Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Chenxu Hu, Yang Wang, Kun Zhan, Peng Jia, Xianpeng Lang, Hang Zhao, abs/2402.122892024ArXiv preprint</p>
<p>Dolphins: Multimodal language model for driving. Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, Chaowei Xiao, preprint, abs/2312.004382023</p>
<p>Lgmcts: Language-guided monte-carlo tree search for executable semantic object rearrangement. Haonan Chang, Kai Gao, Kowndinya Boyalakuntla, Alex Lee, Baichuan Huang, Udhaya Harish, Jinjin Kumar, Abdeslam Yu, Boularias, abs/2309.1582120233ArXiv preprint</p>
<p>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao, abs/2311.1784220233ArXiv preprint</p>
<p>Mme: A comprehensive evaluation benchmark for multimodal large language models. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, Rongrong Ji, abs/2306.1339420233ArXiv preprint</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, Proceedings of CVPR. CVPR202423</p>
<p>Measuring multimodal mathematical reasoning with math-vision dataset. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, Hongsheng Li, abs/2402.1480420243ArXiv preprint</p>
<p>Seedbench: Benchmarking multimodal llms with generative comprehension. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan, abs/2307.1612520233ArXiv preprint</p>
<p>Mm-vet: Evaluating large multimodal models for integrated capabilities. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang, abs/2308.0249020233ArXiv preprint</p>
<p>Pddl| the planning domain definition language. Constructions Aeronautiques, Adele Howe, Craig Knoblock, Drew Isi, Ashwin Mcdermott, Manuela Ram, Daniel Veloso, David Wilkins Weld, Anthony Sri, Dave Barrett, Christianson, Tech. Rep. 21998Technical Report</p>
<p>Deep learning for real-time atari game play using offline monte-carlo tree search planning. Xiaoxiao Guo, P Satinder, Honglak Singh, Richard L Lee, Xiaoshi Lewis, Wang, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D Lawrence, Kilian Q Weinberger, Montreal, Quebec, Canada2014. December 8-13 2014. 2014</p>
<p>Planning by incremental dynamic programming. Richard S Sutton, Machine learning proceedings 1991. Elsevier1991</p>
<p>Can large language models reason and plan?. Subbarao Kambhampati, Annals of the New York Academy of Sciences. 153412024</p>
<p>Llms can't plan, but can help planning in llm-modulo frameworks. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, Anil Murthy, abs/2402.018172024ArXiv preprint</p>
<p>On the self-verification limitations of large language models on reasoning and planning tasks. Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati, abs/2402.081152024ArXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, Advances in Neural Information Processing Systems. 3622024</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, abs/2210.036292022ArXiv preprint</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, abs/2204.01691ArXiv preprint. 32022</p>
<p>Saycanpay: Heuristic planning with large language models using learnable domain knowledge. Rishi Hazra, Pedro Zuidberg Dos Martires, Luc De, Raedt , Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024383</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 202336</p>
<p>Tptu: Task planning and tool usage of large language model-based ai agents. Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, Rui Zhao, abs/2308.034272023ArXiv preprint</p>
<p>Smartplay: A benchmark for llms as intelligent agents. Yue Wu, Xuan Tang, Tom M Mitchell, Yuanzhi Li, abs/2310.0155720233ArXiv preprint</p>
<p>Travelplanner: A benchmark for real-world planning with language agents. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su, abs/2402.01622ArXiv preprint. 2024</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, abs/2206.104982022ArXiv preprint</p>
<p>Copal: Corrective planning of robot actions with large language models. Frank Joublin, Antonello Ceravola, Pavel Smirnov, Felix Ocker, Joerg Deigmoeller, Anna Belardinelli, Chao Wang, Stephan Hasler, Daniel Tanneberg, Michael Gienger, abs/2310.072632023ArXiv preprint</p>
<p>From cooking recipes to robot task trees-improving planning correctness and task efficiency by leveraging llms with a knowledge network. Md Sadman, Sakib , Yu Sun, abs/2309.09181ArXiv preprint. 2023</p>
<p>Layoutgpt: Compositional visual planning and generation with large language models. Weixi Feng, Wanrong Zhu, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin , Eric Wang, William Yang, Wang , Advances in Neural Information Processing Systems. 202436</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Ct, Yonatan Bisk, Adam Trischler, Matthew J Hausknecht, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaMay 3-7, 2021. 2021OpenReview.net</p>
<p>The dawn of lmms: Preliminary explorations with gpt-4v (ision). Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, ArXiv preprint, abs/2309.174212023</p>
<p>Can large language models be good path planners? a benchmark and investigation on spatial-temporal reasoning. Mohamed Aghzal, Erion Plaku, Ziyu Yao, abs/2310.032492023ArXiv preprint</p>
<p>Gemini vs gpt-4v: A preliminary comparison and combination of vision-language models through qualitative cases. Zhangyang Qi, Ye Fang, Mengchen Zhang, Zeyi Sun, Tong Wu, Ziwei Liu, Dahua Lin, Jiaqi Wang, Hengshuang Zhao, abs/2312.150112023ArXiv preprint</p>
<p>Visually dehallucinative instruction generation: Know what you don't know. Sungguk Cha, Jusung Lee, Younghyun Lee, Cheoljong Yang, abs/2402.097172024ArXiv preprint</p>
<p>Mllm-bench, evaluating multi-modal llms using gpt-4v. Wentao Ge, Shunian Chen, Guiming Chen, Junying Chen, Zhihong Chen, Shuo Yan, Chenghao Zhu, Ziyue Lin, Wenya Xie, Xidong Wang, abs/2311.139512023ArXiv preprint</p>
<p>Eyes wide shut? exploring the visual shortcomings of multimodal llms. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann Lecun, Saining Xie, abs/2401.062092024ArXiv preprint</p>
<p>Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, Mathvista, arXiv-2310Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. arXiv e-prints. 2023</p>
<p>Hallusionbench: You see what you think? or you think what you see? an imagecontext reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models. Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, Tianyi Zhou, abs/2310.145662023ArXiv preprint</p>
<p>Milebench: Benchmarking mllms in long context. Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, Benyou Wang, ArXiv preprint, abs/2404.18532, 2024</p>
<p>Multimodal needle in a haystack: Benchmarking longcontext capability of multimodal large language models. Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, Hao Wang, abs/2406.112302024ArXiv preprint</p>
<p>. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, abs/1606.015402016530Openai gym. ArXiv preprint</p>
<p>Reasoning with language model is planning with world model. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, abs/2305.14992ArXiv preprint. 2023</p>
<p>Blocksworld revisited: Learning and reasoning to generate event-sequences from image pairs. Tejas Gokhale, Shailaja Sampat, Zhiyuan Fang, Yezhou Yang, Chitta Baral, 1905.12042. 201930ArXiv preprint</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Ai Anthropic, 2024831Claude-3 Model Card</p>
<p>. Gpt-4o, 831</p>
<p>Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, abs/2401.164202024ArXiv preprint</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, Steven Hoi, Advances in Neural Information Processing Systems. 3662024</p>
<p>Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, abs/2311.0757520237ArXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. Marina Meila, Tong Zhang, the 38th International Conference on Machine Learning, ICML 2021PMLR18-24 July 2021. 2021139of Proceedings of Machine Learning Research</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023</p>
<p>Cogvlm: Visual expert for pretrained language models. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, abs/2311.030792023ArXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
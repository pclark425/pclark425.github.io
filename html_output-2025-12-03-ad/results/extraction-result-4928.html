<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4928 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4928</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4928</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-00cccb9065f0a59e845d5b4d360ce31cf25036be</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/00cccb9065f0a59e845d5b4d360ce31cf25036be" target="_blank">Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a cascade pipeline that follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the"answer consistency"of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can achieve performance comparable to using solely the stronger LLM but require only 40% of its cost.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4928.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4928.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoT-1D/2D-Vote</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture of Thought (one- or two-demo) Vote-based cascade</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vote-based cascade decision method that samples answers from a weaker LLM using a mixture of two thought representations (Chain-of-Thought and Program-of-Thought) and accepts the majority-voted answer when agreement exceeds a threshold; used to detect easy questions and avoid calling a stronger LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (weaker) with GPT-4 (stronger) in cascade</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-turbo (chat API model, 4k context) used as the cheaper weaker LLM to produce multiple sampled answers; GPT-4 (8k context) used as the stronger LLM for fallback/baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Mixture of Thought (MoT) Vote</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Sample K answers from the weaker LLM across two thought representations (CoT and PoT); form a union of samples and compute agreement score s = fraction of samples equal to majority answer; accept majority answer if s >= threshold τ, otherwise route to stronger LLM. Diversity is achieved by mixing CoT and PoT prompts (and optionally different demonstration sets).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Average over six reasoning datasets (GSM8k, ASDIV, TabMWP, DATE, Navigate, CREPE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical, symbolic, and causal reasoning benchmarks used to evaluate multi-step reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy ≈ 0.929 (paper: "~0.929") while requiring ~40% of GPT-4-CoT-SC cost; on CREPE MoT variants achieved 0.885 accuracy at ~47% cost (outperforming GPT-4-CoT-SC 0.871).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>GPT-4-CoT-SC baseline accuracy 0.931 (cost 1.0); CoT-2D-Vote (single-representation, diverse demos) achieved avg 0.924 at ~57% cost; CoT-1D-Vote and PoT-1D-Vote (single-source sampling) generally underperform MoT variants at comparable costs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mixing thought representations (MoT) produces more diverse opinions from the weaker LLM, yielding a larger consistency gap between easy and hard questions and enabling more accurate cascade decisions; MoT variants matched or nearly matched GPT-4-CoT-SC accuracy while drastically reducing cost (~40% of GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On DATE (many test questions very similar to demonstrations), MoT shows less advantage; pushing vote threshold extremely high can be counterproductive (marginal benefits) because small hallucinations can cause unnecessary routing to the stronger LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4928.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4928.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-1D/2D-Vote</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (single- or two-demo) Vote-based cascade</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vote-based cascade decision using only Chain-of-Thought sampling from the weaker LLM, with variants sampling from one prompt (1D) or two different demonstration sets (2D).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (weaker) with GPT-4 (stronger) in cascade</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-turbo used to produce K sampled CoT reasoning paths; GPT-4 used as fallback/baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought Vote</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Sample K CoT reasoning paths with the same thought representation; compute majority-vote agreement s (fraction of samples equal to majority answer). For 2D variants, samples are drawn from two different sets of CoT demonstrations to increase sampling source diversity while keeping representation identical.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same six reasoning datasets (GSM8k, ASDIV, TabMWP, DATE, Navigate, CREPE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning benchmarks covering math, symbolic, and causal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example averages: CoT-2D-Vote achieved avg ≈ 0.924 accuracy at ~57% relative cost (paper reports CoT-2D-Vote: 0.924 avg, cost 0.57 in an example); CoT-1D-Vote underperforms CoT-2D-Vote and MoT variants at comparable cost settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>MoT variants achieved ≈0.929 avg accuracy at ~40% cost and tended to outperform CoT-1D/2D-Vote at similar costs; GPT-4-CoT-SC baseline accuracy ≈0.931.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling from different demonstrations (2D) improves CoT's ability to distinguish easy vs hard questions compared to single-prompt sampling (1D), but mixing representations (MoT) yields larger gains than only varying demonstrations under CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT-1D-Vote resulted in the smallest consistency gap between easy and hard questions (poorer discrimination); CoT errors tend to be similar across different CoT demonstration sets, leading to false confidence (high agreement on the same wrong answer).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4928.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4928.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoT-1D/2D-Vote</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-of-Thought (single- or two-demo) Vote-based cascade</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vote-based cascade decision using Program-of-Thought (code-based) prompts for sampling answers from the weaker LLM, with single- and two-demo variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (weaker) with GPT-4 (stronger) in cascade</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-turbo prompted to emit program/code (PoT) which is executed to obtain answers; samples are drawn and majority-voted; GPT-4 used for fallback.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Program-of-Thought Vote</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Sample K programmatic reasoning paths (PoT) from the weaker LLM; execute generated code to get answers; compute majority-vote agreement s. 2D variants use two sets of PoT demonstrations to diversify sampling sources.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same six reasoning datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks requiring multi-step calculation and symbolic reasoning; PoT uses code execution to reduce numeric errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>PoT-only baselines (weaker model) had lower average accuracy than CoT-only and MoT approaches in many settings; in cascades PoT-2D-Vote improved over PoT-1D-Vote but generally underperformed MoT variants. Example average performance: PoT-only (GPT-3-PoT-SC) average ≈ 0.807 (without cascade) while cascaded PoT-2D-Vote reaches improved scores depending on threshold but not as high as MoT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>MoT variants outperform PoT-only and PoT-vote counterparts at comparable costs; PoT sometimes solved specific cases CoT missed (case studies show PoT solved some examples CoT failed), contributing to MoT's advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PoT makes different kinds of mistakes than CoT (often solving some questions CoT fails or making different wrong answers), so combining PoT with CoT in MoT yields complementary opinions that improve cascade decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>PoT alone does not uniformly outperform CoT; on some datasets PoT-1D underperforms CoT variants. Execution of incorrect code can produce large incorrect outputs (case studies).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4928.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4928.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoT-Verify</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture of Thought Verification-based cascade</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A verification-based cascade that accepts a weaker LLM's answer only when the most-consistent answers from two different prompt settings (e.g., CoT vs PoT) match exactly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (weaker) with GPT-4 (stronger) in cascade</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Weaker LLM sampled under two settings (e.g., CoT and PoT on same demos or different demos); the two majority-voted answers (one per prompt) are compared and accepted only if identical; otherwise route to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>MoT-Verify</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Get two sets of majority-voted answers from distinct prompt settings (different representations and/or different demonstrations); set s = 1 if the two majority answers are equal; accept only when s == 1. Diversity arises from using different representations for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same six reasoning datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning benchmarks requiring robust correctness detection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MoT-1D-Verify reported average accuracy 0.947 at cost ≈0.401 (example from tables); MoT-2D-Verify reported avg 0.949 at cost ≈0.403 in one reported summary; both approach accuracies comparable to or slightly below GPT-4-CoT-SC while using substantially less cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Compare to CoT-2D-Verify (0.926 acc at cost 0.322) and PoT-2D-Verify (0.909 acc at cost 0.311); MoT-Verify variants outperform single-representation verification variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Verification-based MoT yields high accuracy without threshold tuning and can be particularly effective at balancing cost and performance because agreement across representations is a strong signal of correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Verification is less flexible than vote-based thresholding and can reject plausible correct answers if representations disagree due to small hallucinations; on some datasets (e.g., DATE) MoT advantage is reduced.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4928.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4928.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 CoT-SC (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 with Chain-of-Thought self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Strong LLM baseline that uses Chain-of-Thought prompting combined with self-consistency (sampling multiple CoT paths and taking the most consistent answer) to achieve high multi-step reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 (8k context) used as the stronger, more expensive LLM in experiments; used CoT prompting with self-consistency (SC) sampling K=3 in the paper's setup.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought Self-Consistency (CoT-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate multiple CoT reasoning paths (self-consistency) and select the most frequent final answer; single-representation approach that marginalizes over sampled CoT paths.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same six reasoning datasets (used as stronger baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning benchmarks used to measure upper-bound accuracy when always invoking the strong LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported baseline GPT-4-CoT-SC average accuracy ≈ 0.931 (cost normalized to 1.0 in comparisons); per-dataset examples: GSM8k 0.958, ASDIV 0.933, TabMWP 0.961, DATE 0.977 (greedy) / 0.975 (CoT-SC), etc. (paper reports detailed per-dataset tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>MoT cascade variants achieved comparable avg accuracy (~0.929) while reducing cost to ~40% of GPT-4-CoT-SC; CoT/PoT cascades had varying performance below MoT at similar costs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 CoT-SC remains a strong single-representation baseline; however, cascades that leverage a cheaper weaker LLM with diverse sampling (MoT) can approach GPT-4's accuracy at far lower cost.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>In a few individual datasets MoT even outperformed GPT-4-CoT-SC (e.g., CREPE where MoT got 0.885 vs GPT-4-CoT-SC 0.871), indicating that strategic cascades with diverse weaker-LM sampling can occasionally surpass the strong model's SC performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4928.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4928.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>External verifiers (Finetuned / LLM-Q/A)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>External text-based verifiers: finetuned small LMs and LLM-as-verifier prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that decide routing by evaluating the question (and optionally the weaker LLM's answer) using a finetuned small model (RoBERTa) or a prompted LLM verifier; these use textual descriptions rather than multiple reasoning-path agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Finetuned RoBERTa (verifier) and GPT-3.5-turbo used as verifier (LLM-Q / LLM-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RoBERTa-base fine-tuned to predict whether to route to stronger LLM based on question or question+answer; GPT-3.5-turbo prompted to judge question difficulty or answer correctness as a cheaper LLM verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Text-based external verification (Finetuned-Q, Finetuned-QA, LLM-Q, LLM-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Train or prompt an external verifier to judge, from the textual question and/or the weaker LLM's answer, whether the weaker answer should be accepted or forwarded; does not rely on sampling multiple reasoning paths for consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8k, DATE (where available), CREPE (experiments performed on selected datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-step reasoning tasks; external verifiers trained on generated weaker-LM outputs to predict accept/reject.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>External verifiers did not reach GPT-4-CoT-SC accuracy; on GSM8k the best external verifier accuracy reported was 0.892 (far below GPT-4-CoT-SC 0.958) and below the cascade MoT approaches (e.g., ~0.951 on GSM8k for MoT in examples). Fine-tuned verifiers dev/dev accuracies reported (dev) : GSM8k Finetuned-Q 0.592, Finetuned-QA 0.615; CREPE Finetuned-Q 0.749, Finetuned-QA 0.812 (dev).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>MoT cascade approaches substantially outperformed external verifiers at similar or lower cost; text-only verifiers underperformed because deciding difficulty/correctness from literal text is intrinsically challenging for complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Answer-consistency based methods (sampling diverse reasoning paths and checking agreement) outperform external text-based verifiers for routing decisions in reasoning tasks; external verifiers are insufficiently accurate for complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Finetuned verifiers show modest improvement over trivial baselines but still substantially lower accuracy than consistency-based cascades; sometimes LLM-QA (verifier that also sees answers) trusts the weaker LLM more and so costs less but still yields lower ultimate accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>FrugalGPT: How to use large language models while reducing cost and improving performance <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4928",
    "paper_id": "paper-00cccb9065f0a59e845d5b4d360ce31cf25036be",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "MoT-1D/2D-Vote",
            "name_full": "Mixture of Thought (one- or two-demo) Vote-based cascade",
            "brief_description": "A vote-based cascade decision method that samples answers from a weaker LLM using a mixture of two thought representations (Chain-of-Thought and Program-of-Thought) and accepts the majority-voted answer when agreement exceeds a threshold; used to detect easy questions and avoid calling a stronger LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (weaker) with GPT-4 (stronger) in cascade",
            "model_description": "GPT-3.5-turbo (chat API model, 4k context) used as the cheaper weaker LLM to produce multiple sampled answers; GPT-4 (8k context) used as the stronger LLM for fallback/baseline.",
            "reasoning_method_name": "Mixture of Thought (MoT) Vote",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Sample K answers from the weaker LLM across two thought representations (CoT and PoT); form a union of samples and compute agreement score s = fraction of samples equal to majority answer; accept majority answer if s &gt;= threshold τ, otherwise route to stronger LLM. Diversity is achieved by mixing CoT and PoT prompts (and optionally different demonstration sets).",
            "task_name": "Average over six reasoning datasets (GSM8k, ASDIV, TabMWP, DATE, Navigate, CREPE)",
            "task_description": "Mathematical, symbolic, and causal reasoning benchmarks used to evaluate multi-step reasoning performance.",
            "performance": "Average accuracy ≈ 0.929 (paper: \"~0.929\") while requiring ~40% of GPT-4-CoT-SC cost; on CREPE MoT variants achieved 0.885 accuracy at ~47% cost (outperforming GPT-4-CoT-SC 0.871).",
            "comparison_with_other_method": true,
            "performance_other_method": "GPT-4-CoT-SC baseline accuracy 0.931 (cost 1.0); CoT-2D-Vote (single-representation, diverse demos) achieved avg 0.924 at ~57% cost; CoT-1D-Vote and PoT-1D-Vote (single-source sampling) generally underperform MoT variants at comparable costs.",
            "key_findings": "Mixing thought representations (MoT) produces more diverse opinions from the weaker LLM, yielding a larger consistency gap between easy and hard questions and enabling more accurate cascade decisions; MoT variants matched or nearly matched GPT-4-CoT-SC accuracy while drastically reducing cost (~40% of GPT-4).",
            "counter_examples_or_negative_results": "On DATE (many test questions very similar to demonstrations), MoT shows less advantage; pushing vote threshold extremely high can be counterproductive (marginal benefits) because small hallucinations can cause unnecessary routing to the stronger LLM.",
            "uuid": "e4928.0",
            "source_info": {
                "paper_title": "Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CoT-1D/2D-Vote",
            "name_full": "Chain-of-Thought (single- or two-demo) Vote-based cascade",
            "brief_description": "Vote-based cascade decision using only Chain-of-Thought sampling from the weaker LLM, with variants sampling from one prompt (1D) or two different demonstration sets (2D).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (weaker) with GPT-4 (stronger) in cascade",
            "model_description": "GPT-3.5-turbo used to produce K sampled CoT reasoning paths; GPT-4 used as fallback/baseline.",
            "reasoning_method_name": "Chain-of-Thought Vote",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Sample K CoT reasoning paths with the same thought representation; compute majority-vote agreement s (fraction of samples equal to majority answer). For 2D variants, samples are drawn from two different sets of CoT demonstrations to increase sampling source diversity while keeping representation identical.",
            "task_name": "Same six reasoning datasets (GSM8k, ASDIV, TabMWP, DATE, Navigate, CREPE)",
            "task_description": "Multi-step reasoning benchmarks covering math, symbolic, and causal reasoning.",
            "performance": "Example averages: CoT-2D-Vote achieved avg ≈ 0.924 accuracy at ~57% relative cost (paper reports CoT-2D-Vote: 0.924 avg, cost 0.57 in an example); CoT-1D-Vote underperforms CoT-2D-Vote and MoT variants at comparable cost settings.",
            "comparison_with_other_method": true,
            "performance_other_method": "MoT variants achieved ≈0.929 avg accuracy at ~40% cost and tended to outperform CoT-1D/2D-Vote at similar costs; GPT-4-CoT-SC baseline accuracy ≈0.931.",
            "key_findings": "Sampling from different demonstrations (2D) improves CoT's ability to distinguish easy vs hard questions compared to single-prompt sampling (1D), but mixing representations (MoT) yields larger gains than only varying demonstrations under CoT.",
            "counter_examples_or_negative_results": "CoT-1D-Vote resulted in the smallest consistency gap between easy and hard questions (poorer discrimination); CoT errors tend to be similar across different CoT demonstration sets, leading to false confidence (high agreement on the same wrong answer).",
            "uuid": "e4928.1",
            "source_info": {
                "paper_title": "Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PoT-1D/2D-Vote",
            "name_full": "Program-of-Thought (single- or two-demo) Vote-based cascade",
            "brief_description": "Vote-based cascade decision using Program-of-Thought (code-based) prompts for sampling answers from the weaker LLM, with single- and two-demo variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (weaker) with GPT-4 (stronger) in cascade",
            "model_description": "GPT-3.5-turbo prompted to emit program/code (PoT) which is executed to obtain answers; samples are drawn and majority-voted; GPT-4 used for fallback.",
            "reasoning_method_name": "Program-of-Thought Vote",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Sample K programmatic reasoning paths (PoT) from the weaker LLM; execute generated code to get answers; compute majority-vote agreement s. 2D variants use two sets of PoT demonstrations to diversify sampling sources.",
            "task_name": "Same six reasoning datasets",
            "task_description": "Benchmarks requiring multi-step calculation and symbolic reasoning; PoT uses code execution to reduce numeric errors.",
            "performance": "PoT-only baselines (weaker model) had lower average accuracy than CoT-only and MoT approaches in many settings; in cascades PoT-2D-Vote improved over PoT-1D-Vote but generally underperformed MoT variants. Example average performance: PoT-only (GPT-3-PoT-SC) average ≈ 0.807 (without cascade) while cascaded PoT-2D-Vote reaches improved scores depending on threshold but not as high as MoT.",
            "comparison_with_other_method": true,
            "performance_other_method": "MoT variants outperform PoT-only and PoT-vote counterparts at comparable costs; PoT sometimes solved specific cases CoT missed (case studies show PoT solved some examples CoT failed), contributing to MoT's advantage.",
            "key_findings": "PoT makes different kinds of mistakes than CoT (often solving some questions CoT fails or making different wrong answers), so combining PoT with CoT in MoT yields complementary opinions that improve cascade decision-making.",
            "counter_examples_or_negative_results": "PoT alone does not uniformly outperform CoT; on some datasets PoT-1D underperforms CoT variants. Execution of incorrect code can produce large incorrect outputs (case studies).",
            "uuid": "e4928.2",
            "source_info": {
                "paper_title": "Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MoT-Verify",
            "name_full": "Mixture of Thought Verification-based cascade",
            "brief_description": "A verification-based cascade that accepts a weaker LLM's answer only when the most-consistent answers from two different prompt settings (e.g., CoT vs PoT) match exactly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (weaker) with GPT-4 (stronger) in cascade",
            "model_description": "Weaker LLM sampled under two settings (e.g., CoT and PoT on same demos or different demos); the two majority-voted answers (one per prompt) are compared and accepted only if identical; otherwise route to GPT-4.",
            "reasoning_method_name": "MoT-Verify",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Get two sets of majority-voted answers from distinct prompt settings (different representations and/or different demonstrations); set s = 1 if the two majority answers are equal; accept only when s == 1. Diversity arises from using different representations for verification.",
            "task_name": "Same six reasoning datasets",
            "task_description": "Multi-step reasoning benchmarks requiring robust correctness detection.",
            "performance": "MoT-1D-Verify reported average accuracy 0.947 at cost ≈0.401 (example from tables); MoT-2D-Verify reported avg 0.949 at cost ≈0.403 in one reported summary; both approach accuracies comparable to or slightly below GPT-4-CoT-SC while using substantially less cost.",
            "comparison_with_other_method": true,
            "performance_other_method": "Compare to CoT-2D-Verify (0.926 acc at cost 0.322) and PoT-2D-Verify (0.909 acc at cost 0.311); MoT-Verify variants outperform single-representation verification variants.",
            "key_findings": "Verification-based MoT yields high accuracy without threshold tuning and can be particularly effective at balancing cost and performance because agreement across representations is a strong signal of correctness.",
            "counter_examples_or_negative_results": "Verification is less flexible than vote-based thresholding and can reject plausible correct answers if representations disagree due to small hallucinations; on some datasets (e.g., DATE) MoT advantage is reduced.",
            "uuid": "e4928.3",
            "source_info": {
                "paper_title": "Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 CoT-SC (baseline)",
            "name_full": "GPT-4 with Chain-of-Thought self-consistency",
            "brief_description": "Strong LLM baseline that uses Chain-of-Thought prompting combined with self-consistency (sampling multiple CoT paths and taking the most consistent answer) to achieve high multi-step reasoning accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's GPT-4 (8k context) used as the stronger, more expensive LLM in experiments; used CoT prompting with self-consistency (SC) sampling K=3 in the paper's setup.",
            "reasoning_method_name": "Chain-of-Thought Self-Consistency (CoT-SC)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Generate multiple CoT reasoning paths (self-consistency) and select the most frequent final answer; single-representation approach that marginalizes over sampled CoT paths.",
            "task_name": "Same six reasoning datasets (used as stronger baseline)",
            "task_description": "Multi-step reasoning benchmarks used to measure upper-bound accuracy when always invoking the strong LLM.",
            "performance": "Reported baseline GPT-4-CoT-SC average accuracy ≈ 0.931 (cost normalized to 1.0 in comparisons); per-dataset examples: GSM8k 0.958, ASDIV 0.933, TabMWP 0.961, DATE 0.977 (greedy) / 0.975 (CoT-SC), etc. (paper reports detailed per-dataset tables).",
            "comparison_with_other_method": true,
            "performance_other_method": "MoT cascade variants achieved comparable avg accuracy (~0.929) while reducing cost to ~40% of GPT-4-CoT-SC; CoT/PoT cascades had varying performance below MoT at similar costs.",
            "key_findings": "GPT-4 CoT-SC remains a strong single-representation baseline; however, cascades that leverage a cheaper weaker LLM with diverse sampling (MoT) can approach GPT-4's accuracy at far lower cost.",
            "counter_examples_or_negative_results": "In a few individual datasets MoT even outperformed GPT-4-CoT-SC (e.g., CREPE where MoT got 0.885 vs GPT-4-CoT-SC 0.871), indicating that strategic cascades with diverse weaker-LM sampling can occasionally surpass the strong model's SC performance.",
            "uuid": "e4928.4",
            "source_info": {
                "paper_title": "Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "External verifiers (Finetuned / LLM-Q/A)",
            "name_full": "External text-based verifiers: finetuned small LMs and LLM-as-verifier prompts",
            "brief_description": "Methods that decide routing by evaluating the question (and optionally the weaker LLM's answer) using a finetuned small model (RoBERTa) or a prompted LLM verifier; these use textual descriptions rather than multiple reasoning-path agreement.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Finetuned RoBERTa (verifier) and GPT-3.5-turbo used as verifier (LLM-Q / LLM-QA)",
            "model_description": "RoBERTa-base fine-tuned to predict whether to route to stronger LLM based on question or question+answer; GPT-3.5-turbo prompted to judge question difficulty or answer correctness as a cheaper LLM verifier.",
            "reasoning_method_name": "Text-based external verification (Finetuned-Q, Finetuned-QA, LLM-Q, LLM-QA)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "Train or prompt an external verifier to judge, from the textual question and/or the weaker LLM's answer, whether the weaker answer should be accepted or forwarded; does not rely on sampling multiple reasoning paths for consistency.",
            "task_name": "GSM8k, DATE (where available), CREPE (experiments performed on selected datasets)",
            "task_description": "Same multi-step reasoning tasks; external verifiers trained on generated weaker-LM outputs to predict accept/reject.",
            "performance": "External verifiers did not reach GPT-4-CoT-SC accuracy; on GSM8k the best external verifier accuracy reported was 0.892 (far below GPT-4-CoT-SC 0.958) and below the cascade MoT approaches (e.g., ~0.951 on GSM8k for MoT in examples). Fine-tuned verifiers dev/dev accuracies reported (dev) : GSM8k Finetuned-Q 0.592, Finetuned-QA 0.615; CREPE Finetuned-Q 0.749, Finetuned-QA 0.812 (dev).",
            "comparison_with_other_method": true,
            "performance_other_method": "MoT cascade approaches substantially outperformed external verifiers at similar or lower cost; text-only verifiers underperformed because deciding difficulty/correctness from literal text is intrinsically challenging for complex reasoning.",
            "key_findings": "Answer-consistency based methods (sampling diverse reasoning paths and checking agreement) outperform external text-based verifiers for routing decisions in reasoning tasks; external verifiers are insufficiently accurate for complex reasoning.",
            "counter_examples_or_negative_results": "Finetuned verifiers show modest improvement over trivial baselines but still substantially lower accuracy than consistency-based cascades; sometimes LLM-QA (verifier that also sees answers) trusts the weaker LLM more and so costs less but still yields lower ultimate accuracy.",
            "uuid": "e4928.5",
            "source_info": {
                "paper_title": "Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "FrugalGPT: How to use large language models while reducing cost and improving performance",
            "rating": 2
        },
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 1
        }
    ],
    "cost": 0.01876775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Model Cascades with Mixture of Thought Representations for CostEFFICIENT REASONING</h1>
<p>Murong Yue ${ }^{1}$, Jie Zhao ${ }^{2}$, Min Zhang ${ }^{3}$, Liang Du ${ }^{2}$, Ziyu Yao ${ }^{1}$<br>${ }^{1}$ George Mason University ${ }^{2}$ Microsoft ${ }^{3}$ Virginia Tech<br>${ }^{1}{$ myue, ziyuyao}@gmu.edu ${ }^{2}$ {zhaojie, liang.du}@microsoft.com<br>${ }^{3}{$ minzhang23}@vt.edu</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought (Wei et al., 2022) and Program-of-Thought (Chen et al., 2022; Gao et al., 2023)). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can achieve performance comparable to using solely the stronger LLM but require only $40 \%$ of its cost. Our codes are available at https://github.com/MurongYue/LLM_MoT_cascade.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in reasoning tasks (Rae et al., 2021; Lewkowycz et al., 2022; Zhong et al., 2023). Because of the intensive computing resources required for training and hosting the LLMs for inference, many such LLMs are only accessible via paid API services, thus leading to high monetary costs. In this work, we are motivated to study strategies for reducing the costs of using LLMs while not sacrificing task performance, particularly for LLMs' applications to reasoning tasks.</p>
<p>Different types and versions of LLMs often come with different capabilities and costs. Typically, LLMs with better performance (termed "stronger LLMs") are more expensive than those with relatively worse overall performance (termed "weaker LLMs"). For example, GPT-4 (OpenAI, 2023) is 30 times more expensive than GPT-3.5-turbo for the output tokens. ${ }^{1}$ It thus implies a promising solution to cost-saving. That is, simple questions could be answered by the weaker but more affordable LLM, whereas only the difficult questions need to be tackled by the more expensive, stronger LLM. Drawing inspirations from here, Chen et al. (2023a) explored the idea of "LLM cascades", where a question is always first answered by a weaker LLM, and then optionally routed to a stronger LLM when the the weaker LLM's answer is not accepted (Figure 1). To decide this routing, this work suggested fine-tuning a smaller LLM to score each question along with its answer produced by the weaker LLM. While this approach could work for some tasks, in practice, we observed that it did not yield satisfying performance for intricate reasoning tasks. Intuitively, it is very challenging to evaluate the difficulty and the answer correctness of a reasoning question solely based on its literal expression, even with a large enough LLM, since the errors could be nuanced despite the reasoning paths appearing promising (Madaan et al., 2023).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In this work, we proposed to devise this routing decision-maker from a different angle, i.e., the “answer consistency” of the weaker LLM (Wang et al., 2023). This is inspired by the observation that answers from the weaker LLM tend to be consistent in multiple sampling paths when the question is easy, but inconsistent when the question is hard. To implement this idea, we proposed two types of methods, a vote-based method that examines if the agreement of multiple answer samples on the majority-voted answer surpasses a pre-defined confidence threshold, and a verification-based method that checks if the majority-voted answers sampled from different prompts are consistent. To realize the two methods in reasoning tasks, we further investigated multiple strategies for answer sampling, including sampling from a single set versus two sets of task demonstrations. In particular, we proposed to leverage a “mixture of thought (MoT) representations”, which samples answers from both Chain-of-Thought (Wei et al., 2022, CoT) and Program-of-Thought (Chen et al., 2022; Gao et al., 2023, PoT) prompts, emulating how experts can provide diverse perspectives to the same question. This follows the same spirit of ensembling (Rokach, 2010), but is applied to developing LLM cascades for the first time. By pairing different sampling strategies with the two answer consistency checking methods (i.e., vote and verification), we end up with ten approaches to implementing the LLM cascade.</p>
<p>To evaluate the proposed approaches, we conducted experiments on six reasoning datasets, covering mathematical, symbolic, and causal reasoning tasks, using GPT-3.5-turbo as the weaker LLM and GPT-4 as the stronger one. The experimental results demonstrated the effectiveness of LLM cascades based on answering consistency. That is, different approaches that we proposed for LLM cascades can generally achieve performance comparable to or even better than fully using the stronger LLM, while they require only half or less relative cost to the latter. In particular, our approaches based on a mixture of thought representations achieved comparable task performance with only 40% of the cost of GPT-4. Our results also underscored the effectiveness of sampling answers from diverse prompt settings, such as sampling from different task demonstrations or different thought representations. Our further analysis revealed that different prompt settings can often provide different opinions for the more complex questions while tending to be consistent for easier ones, which allows us to distinguish questions at different difficulty levels more accurately for cascade decision-making. Finally, we also compared our consistency-based approaches with fine-tuned smaller LLMs (Chen et al., 2023a) as well as other variants that make the routing decisions based on the literal expressions of the question and its answer. Our approaches exhibited strong advantages over all of them.</p>
<h2>2 LLM CASCADES FOR COST-EFFICIENT REASONING</h2>
<h3>2.1 OVERVIEW OF LLM CASCADES</h3>
<p>We leverage a cascade of LLMs to save the cost of in-context LLM reasoning, as illustrated in Figure 1. Specifically, we assume two LLMs. The weaker LLM (denoted as <em>LLM</em><strong>) yields relatively worse performance but is less costly, whereas the stronger LLM (denoted as <em>LLM*<em>) enjoys better task performance but is more expensive. Given a question </em>Q<em>, the LLM cascade first employs the weaker LLM to obtain an initial answer </em>A</em></strong>. This answer, along with other metadata produced by the weaker LLM, will then be fed to a cascade decision maker to decide whether the answer can be accepted as the final one. If the answer is rejected, the stronger LLM should be invoked to provide a more reliable answer <em>A</em>**. As a consequence, the total cost of answering the question becomes</p>
<p>$$C = C^w + C^d + \mathbb{1}_{reject} \, C^s,$$</p>
<p>where <em>C</em><em> and </em>C<strong> indicate the costs from calling the weaker and the stronger LLMs, respectively, *C</strong> denotes any cost involved in the LLM cascade decision-making process, and <em>$\mathbb{1}_{reject} = 1$</em> holds if and only if the decision maker rejects the answer.</p>
<p>Both LLMs solve the question via few-shot in-context learning, e.g., for the weaker LLM, an answer <em>A</em><em> is produced by sampling from </em>PLLM=<em> {</em>A<em><em> | E1 || E2 || . . . || EM || Q</em>}, where </em>E1 || E2 || . . . || EM || Q<em> denotes a concatenation of </em>M<em> task demonstrations and the input question </em>Q<em>, forming the “prompt input” to the LLM (Brown et al., 2020). As </em>M* task examples are used to demonstrate the task,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of LLM cascade chaining a weaker but cheaper LLM with a stronger but more costly one.</p>
<p>it indicates a " $M$-shot in-context learning" of LLM. For reasoning tasks, in practice, a LLM will typically be prompted to elaborate on its reasoning process via "thought representations", such as Chain-of-Thought (Wei et al., 2022, CoT) and Program-of-Thought (Chen et al., 2022; Gao et al., 2023, PoT), where the reasoning process is described step by step via natural language and programming language, respectively. The answer (e.g., a numerical result of the mathematical calculation) can then be extracted from the texts (for CoT ) or obtained by executing the code (for PoT).</p>
<h1>2.2 ANSWER CONSISTENCY-BASED CASCADE DECISION-MAKING</h1>
<p>The core of our LLM cascade is the decision maker, which takes in the output from the weaker LLM, and then decides whether to route to the stronger LLM or not. An ideal cascade decision maker should call the stronger LLM only when the answer by the weaker LLM is wrong, such that the total cost $C$ can be minimized without degrading the overall task performance (compared with using the stronger LLM all the time). To this end, we propose two methodologies based on the "answer consistency" of the weaker LLM, which we elaborate on below.</p>
<p>Answer Consistency and Sources of Sampling Answer consistency has been found helpful for improving the LLM performance in reasoning tasks (Wang et al., 2023). Instead of greedily decoding one answer for each question, Wang et al. (2023) sampled a diverse set of reasoning paths (or thought processes) and then selected the most consistent answer by marginalizing out the sampled paths. Drawing inspiration from the prior work, we make the following hypothesis: When the weaker LLM samples highly consistent answers for a given question, it reveals a high "confidence" in solving this question and its most consistent answer is likely to be correct; in this case, there is thus no need to invoke the stronger LLM.</p>
<p>To realize this intuition, we generalize from Wang et al. (2023) and consider three sources of sampling consistency:</p>
<ul>
<li>In-distribution sampling: As Wang et al. (2023), we consider sampling multiple answers given the same prompt input to the weaker LLM. In practice, this can be achieved by setting a non-zero temperature for the weaker LLM.</li>
<li>Sampling from different in-context demonstrations: We further consider sampling answers from two sets of task demonstrations under the same thought representation. For example, to demonstrate the CoT process in mathematical reasoning tasks, Wei et al. (2022) annotated eight math examples as the demonstrations and performed 8 -shot in-context learning. We additionally annotated another eight examples as the second set of demonstrations, which allowed us to further diversify the sources of answer sampling.</li>
<li>Sampling from different thought representations: While existing literature typically investigated either CoT or PoT independently, in this work, we propose to leverage the synergy of both thought representations in a single task. We hypothesize that an LLM obtains truly high confidence in its problem-solving, only when it is able to produce a consistent answer agnostic to how the intermediate steps are represented. Therefore, we propose to sample the weaker LLM answers from a "mixture of thought (MoT) representations", which includes both CoT and PoT prompts.</li>
</ul>
<p>Below, we introduce our methodologies for consistency checking based on the answer samples.
Method 1: Vote-based decision-making The first method calculates the consistency of the weaker LLM's answer samples by voting. Formally, for a single prompt, we denote the set of answers produced by the weaker LLM for each question $Q$ as $\left(A_{1}^{w}, A_{2}^{w}, \ldots, A_{K}^{w}\right)$, where $K$ is the pre-defined number of samples. When sampling from two different prompts, we denote $\left(A_{11}^{w}, A_{12}^{w}, \ldots, A_{1 K_{1}}^{w}\right)$ and $\left(A_{21}^{w}, A_{22}^{w}, \ldots, A_{2 K_{2}}^{w}\right)$ as the answer samples produced by each of them, where $K_{1}$ and $K_{2}$ represent the pre-defined sample size for each prompt setting, respectively. Note that for this method, we do not distinguish answers sampled with a single prompt or multiple prompts (e.g., samples from different prompts have exactly equal weights when voting). The most consistent answer can then be selected as the one that most samples agree with, and this answer will also be regarded as the final answer $A^{w}$ by the weaker LLM. The decision maker measures the weaker LLM's consistency via the agreement score</p>
<p>$$
s=\frac{\sum_{i=1}^{K} \mathbb{1}<em i="i">{A</em>}^{w}=A^{w}}}{K} \text { or } s=\frac{\sum_{i=1}^{K_{1}} \mathbb{1<em i="i">{A</em>}^{w}=A^{w}}+\sum_{i=1}^{K_{2}} \mathbb{1<em 2="2" i="i">{A</em>
$$}^{w}=A^{w}}}{K_{1}+K_{2}</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An overview of our approaches ( 6 vote-based and 4 verification-based). We use $\square$ to represent the answers from PoT and $\square$ to represent the answers from CoT. Demo $_{i}$ is the $i$-th set of demonstrations.</p>
<p>The larger the $s$, the more consistent the weaker LLM's answer samples. In conjunction with a pre-defined threshold value $\tau$, the decision maker accepts the weaker LLM's most consistent answer $A^{w}$ when $s \geq \tau$ and rejects it otherwise. As a result, the total cost of answering a question (Eq 1) can vary depending on the threshold.</p>
<p>Method 2: Verification-based decision-making In the case of producing samples from two different prompt settings (i.e., different demonstrations or thought representations), we propose the second method, which compares the most consistent answers produced by each prompt as the answer verification. As previously mentioned, we could obtain two sets of answers from distinct prompts: $\left(A_{11}^{w}, A_{12}^{w}, \ldots, A_{1 K_{1}}^{w}\right)$ and $\left(A_{21}^{w}, A_{22}^{w}, \ldots, A_{2 K_{2}}^{w}\right)$. Our method then verifies the most consistent answers within each prompt, denoted as $A_{1}^{w \prime}$ and $A_{2}^{w \prime}$ respectively, as follows:</p>
<p>$$
s=\mathbb{1}<em 1="1">{A</em>
$$}^{w \prime}=A_{2}^{w \prime}</p>
<p>Only when $s$ equals 1 , i.e., when the two answers are the same, the weaker LLM's answer will be accepted by the decision maker. In this case, the final answer of the weaker LLM will be the same as the two most consistent answers, i.e., $A^{w}=A_{1}^{w \prime}=A_{2}^{w \prime}$.</p>
<p>In comparison, these two methods have different applicable scenarios. The vote-based method is well-suited for scenarios with pre-defined cost constraints. In such cases, we have the flexibility to tune the threshold to ensure it aligns with the constraint. On the other hand, the verification-based method is capable of producing relatively optimal results without the need for threshold tuning, although it lacks flexibility. We will systematically compare the two methods in experiments.</p>
<h1>2.3 LLM CASCADEs FOR REASONING TASKS</h1>
<p>We instantiate the proposed two methods in LLM reasoning tasks with different sampling sources, resulting in 10 approaches, as summarized in Figure 2. Specifically, 6 approaches adopt vote-based decision-making: CoT-1D-Vote collects $K$ answers sampled from prompting the weaker LLM with the CoT representation, and then calculates the answer consistency for decision-making, following Eq 2. Similarly, PoT-1D-Vote bases its decision-making on answers sampled from a PoT prompt. To diversify the sources of the answers, for each thought representation, we further consider sampling from two sets of CoT or PoT demonstrations, resulting in CoT-2D-Vote and PoT-2D-Vote, respectively. Finally, the vote-based approaches also include two variants leveraging a mixture of thought (MoT) representations. For MoT-1D-Vote, $K_{1}$ answers are sampled from the CoT prompt and another $K_{2}$ from the PoT prompt, and a union set of their answers are then used to compute the consistency score $s$. For MoT-2D-Vote, the procedure is similar, except that the CoT and the PoT prompts are annotated from two sets of demonstration examples.</p>
<p>The verification-based approaches assume answer samples from two different prompts. We instantiate 4 variants, including CoT-2D-Verify, where we prompt the weaker LLM with</p>
<p>two sets of CoT demonstrations, resulting in two answer sets for decision-making (Eq 3); PoT-2D-Verify, where we similarly prompt the weaker LLM with two sets of PoT demonstrations; MoT-1D-Verify, where we consider two sets of answers from two thought representation prompts (but on the same set of task demonstration examples); and MoT-2D-Verify, which additionally employs different sets of demonstrations when prompting the weaker LLM with different thought representations.</p>
<p>Cost-Comparable Sample Size Configuration To fairly compare all approaches in terms of their effectiveness in identifying easy (or correct) vs. hard (or correct) questions, we aim to configure their costs to be comparable. Since we use the same prompt to the stronger LLM, $C^{s}$ is agnostic to the specific approach. However, different approaches may require varying costs in calling the weaker LLM (i.e., $C^{w}$ ). For example, even with the same total sampling size (i.e., $K=K_{1}+$ $K_{2}$ ), CoT-1D-Vote and CoT-2D-Vote result in different input token usages because the former prompt the LLM once but the latter prompt it twice, doubling the token usages. Therefore, we unify the $C^{w}$ by a coarse-grained cost alignment. Our analysis suggests different configurations of $K$ for different approaches. We refer readers to Appendix A for more details. Since our configuration can only yield comparable costs due to certain simplifications, rather than exact costs, we still report the actual token cost for each approach.</p>
<h1>3 EXPERIMENT</h1>
<h3>3.1 EXPERIMENTAL SETTING</h3>
<p>We evaluate our LLM cascade approaches on six datasets, covering (1) mathematical reasoning, including GSM8k (Cobbe et al., 2021), ASDIV (Ling et al., 2017), and TabMWP (Lu et al., 2023); (2) symbolic reasoning from BIG-Bench Hard (bench authors, 2023), including DATE and Navigate; and (3) causal reasoning, including CREPE (Zhang et al., 2023). In our pipeline, we leverage the GPT-3.5-turbo (4k context) as the weaker LLM and the GPT-4 (8k context) with CoT selfconsistency (Wang et al., 2023, SC) as the stronger LLM. Throughout our experiments, we set the number of task demonstrations as $M=8$. We use the same demonstration examples as prior work (Chen et al., 2022; Wei et al., 2022). When additional task demonstrations are needed (e.g., for 2D approaches), we randomly sample examples from the training data and manually annotate them with thought representations. We set the number of sampling paths as $K=20$ for GPT-3.5turbo and $K=3$ for GPT-4. The sampling temperature by default is 0.4 for both LLMs. The metrics we use are the task accuracy and the relative cost compared with the cost of GPT-4 with CoT SC (denoted as GPT-4-CoT-SC). A lower relative cost and higher accuracy indicate better performance. We also compared our approaches with baselines using only the weaker LLM (i.e., GPT-3.5-CoT-SC, GPT-3.5-PoT-SC) or only the strong LLM in different ways (i.e., GPT-4-CoT-Greedy, GPT-4-PoT-Greedy, GPT-4-CoT-SC, GPT-4-PoT-SC). Our experimental details can be found in Appendix B and reproducible prompts in Appendix L.</p>
<h3>3.2 MAIN RESULTS</h3>
<p>Figure 3 illustrates the performance of our proposed approaches. For Vote-based approaches, we draw curves by changing the pre-defined threshold $\tau$ varying from 0.4 to 1 . A high value of threshold signifies a more rigorous criterion for trusting the answers from the weaker LLM, making more examples transferred to the stronger LLM. Our observations are as follows:</p>
<p>Our pipeline achieves comparable task performance with significantly reduced costs. On average, all of our cascade variants (Vote or Verify) demonstrate significant cost efficiency. In particular, as shown in the average plot, the four MoT variants achieve comparable task performance $(\sim 0.929$ accuracy) to GPT-4-CoT-SC (0.931) while demanding only $40 \%$ of its cost. On CREPE, MoT variants even outperform GPT-4-CoT-SC (0.885 vs. 0.871 ) at $47 \%$ of its cost. Our approaches based on CoT and PoT also exhibit the capability to save costs while maintaining the overall task performance. For example, CoT-2D-Vote achieved 0.924 task accuracy on average but demanded only $57 \%$ relative cost. These observations suggest the effectiveness of our cascade decision maker via checking the answer consistency of the weaker LLM.</p>
<p>Sampling from diverse prompt settings helps cascade decision-making. Our results show that variants involving diverse sources of sampling, such as CoT/PoT-2D-Vote and MoT-1D/2D-Vote, can more precisely distinguish between easy and hard reasoning questions,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Main experiment results over six reasoning datasets. The bottom figure represents the average performance. The exact numerical results are included in Appendix C.
compared with their counterparts sampling from single sources, i.e., CoT/PoT-1D-Vote. For example, between CoT-2D-Vote and CoT-1D-Vote, the former outperforms the latter by $1.4 \%$ absolute accuracy under the same relative cost of 0.4 on average.</p>
<p>Mixing thought representations is particularly effective. Furthermore, we find that mixing the two thought representations (i.e., MoT-1D/2D-Vote) outperforms decision-making using either of them (i.e., CoT-1D/2D-vote and PoT-1D/2D-vote). This is illustrated by the gap in the average plot and is consistent on most datasets except DATE, where many test questions are very similar to the demonstration examples. Intuitively, this is because different thought representations can bring in more diverse "opinions" of the weaker LLM on the same input question, resembling how a group of experts with diverse perspectives could contribute to more effective results in collaborative work. It can also be viewed as "ensembling" LLMs, which utilizes the intuition that variants of the same model typically share few mistakes (Rokach, 2010). We provide a further investigation of this effect in Section 3.3. We also note that when using MoT, no obvious difference is perceived between using one set (i.e., MoT-1D-Vote) or two sets (i.e., MoT-2D-Vote) of task demonstrations. This result reveals that tuning the thought representations is more helpful for measuring an LLM's (un)certainty on its answer than tuning the task demonstrations.</p>
<p>Increasing the threshold yields marginal benefits for MoT-1D/2D-Vote. As costs increase, the curves of MoT-1D/2D-Vote flatten out, showing that pushing the threshold to exceedingly high is unnecessary. This is because even for an easy question, the weaker LLM may still have hallucinations in a small set of answers. Setting the threshold too high can lead to the decision-making being influenced by them. It may result in easy questions being sent to the stronger LLM incorrectly, thus driving up the overall cost. In our practice, a threshold that can balance cost and accuracy typically falls between 0.5 and 0.6 . When it comes to MoT-1D/2D-Verify, we can strike a balance between cost and accuracy as the verification method can tolerate a few hallucinations.</p>
<h1>3.3 Analysis on Mixture of Thought Representations</h1>
<p>To understand the effect of MoT, we analyze three vote-based approaches. We first group questions into "easy" and "hard" based on whether the weaker LLM can answer them correctly (i.e., whether the majority-voted answer $A^{w}$ is correct or not). For all easy/hard questions, we then calculate the average consistency score following Eq 2. We present the average consistency scores for each approach, categorizing easy and hard questions based on whether the weaker LLM can answer them correctly (i.e., $A w$ being correct). A higher average consistency score indicates our decision-maker places greater trust in the answer from the weaker LLM. Hence, an effective cascade decision-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Average consistency scores (Top) and case studies (Bottom; simplified for presentation purposes) of various approaches showing the advantage of MoT. Shades highlight similar mistakes by CoT even based on different task demonstrations, and dissimilar mistakes by PoT.
maker should reveal relatively higher consistency scores for easy questions and lower ones for hard questions, leading to a larger "gap" between them.
We show the results in Figure 4 (Top). It is observed that all approaches lead to higher consistency scores on the easy questions than those on the hard questions, which explains their overall effectiveness for cascade decision-making. Because of involving two different thought representations, MoT-1D-Vote tends to have a lower consistency score compared with the two CoT approaches. However, it still ends up with a larger "gap" in the consistency scores for the easy and the hard questions, which is particularly prominent on the Navigate dataset where it gives the best performance gain (Figure 3). In contrast, CoT-1D-Vote results in the smallest score gap, indicating its weakness in distinguishing between the easy and the hard questions, particularly for Navigate. This weakness is mitigated by diversifying the prompting with two sets of task demonstrations (i.e., CoT-2D-Vote), but it still underperforms mixing the thought representations.</p>
<p>Finally, in Figure 4 (Bottom) we show that when CoT cannot answer a hard question, prompting the weaker LLM with another set of task demonstrations (but still under the CoT representation) often yields the same mistaken answer, which thus results in a high consistency score. On the contrary, PoT tends to make mistakes in a very different way and result in a different mistaken answer, which explains the low consistency score of MoT-1D-Vote. More cases are shown in Appendix D.</p>
<h1>3.4 Robustness Evaluation</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Robustness analysis with varying temperature $T$ and sample size $K$.</p>
<p>We further analyze if our results are sensitive to the change of the sampling temperature $T$ or the sample size $K$. We select datasets for each type of reasoning task and conduct experiments with CoT-2D-Vote and MoT-1D-Vote. Our results are shown in Figure 5. We first look into the effect when increasing the sampling temperature $T$ from 0.4 (our default setting) to 0.8. For both approaches, increasing their temperature yields comparable or slightly better performance. This is owing to the increased answer diversity when the temperature gets higher. However, in any case, MoT-1D-Vote consistently outperforms CoT-2D-Vote. Increasing the sample size $K$ from 20 to 40 , on the other hand, leads to a rightward shift of the curves, implying that it requires higher cost to achieve the same task accuracy. This can be explained by the higher cost of the larger sample size, whereas increasing the sample size does not contribute to the detection of easy vs. hard questions. Like in the results of varying temperatures, MoT robustly outperforms CoT in any case.</p>
<h1>3.5 COMPARISON TO EXTERNAL TEXT-BASED VERIFIERS</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Comparison with external verifiers showing the advantages of our approaches based on answer consistency. We do not report finetuned verifiers for DATE as it does not have a training set.</p>
<p>As mentioned, prior work implemented the LLM cascade decision-maker by training an external verifier, which scores a question and its answer (from the weaker LLM) based on their literal descriptions (Chen et al., 2023a; Šakota et al., 2023). Related to the above work, Chen et al. (2023b) and Madaan et al. (2023) also showed the promise of prompting LLMs to evaluate their own responses. To perform a general comparison with such external verifiers, we conducted experiments on GSM8k, DATE, and CREPE with the following baselines: Finetuned-Q, which is a RoBERTa base model (Liu et al., 2020) fine-tuned to decide whether a question should be routed to the stronger LLM based on its description; Finetuned-QA, which works similarly as Finetuned-Q but additionally takes the majority-voted answer from the weaker LLM (GPT-3.5-CoT-SC) as input; LLM-Q, where we instead prompt GPT-3.5-turbo as the verifier to judge based on the question description; and LLM-QA, which similarly employs GPT-3.5-turbo to decide upon the question and the weaker LLM's majority-voted answer. We leave details of the baselines in Appendix E.</p>
<p>The results in Figure 6 show that incorporating the external verifiers cannot achieve comparable accuracy with GPT-4-CoT-SC. For example, on the GSM8k dataset, the highest accuracy with the external verifiers is 0.892 , which is way lower than the accuracy ( 0.958 ) of GPT-4-CoT-SC and the accuracy ( 0.951 ) of our approaches. They also show lower task accuracies than our approaches under the same cost. It indicates that the external verifiers cannot yield satisfying results in complex reasoning tasks, which can be due to the intrinsic challenge of deciding question difficulty and answer correctness solely based on their textual descriptions. For the calibration analysis, please refer to the appendix I.</p>
<h3>3.6 ADDITIONAL STUDIES</h3>
<p>How weak can the weaker LLM be? We evaluate our approaches when adopting LLAMA2-13B (Touvron et al., 2023) as the weaker LLM and GPT-4 as the stronger LLM. The results are shown in the Appendix F. On the DATE dataset, our approaches still works. However, LLAMA2-13B as the weaker LLM doesn't yield ideal results on GSM8k and CREPE. That is because most of the questions in GSM8k and CREPE are excessively complex for LLAMA2-13B. Therefore, LLAMA213 B often fails to answer the questions consistently across multiple samplings. Hence, the choice of a weaker LLM should be contingent on the task's level of difficulty. When the current weaker LLM struggles with the task, it is advisable to consider switching to a more powerful LLM.</p>
<p>Can the stronger LLM benefit from the weaker LLM hints? In our LLM cascade, all questions will obtain answers from the weaker LLM, no matter if they will be sent to the stronger LLM</p>
<p>or not. Therefore, an interesting question is whether the answer produced by the weaker LLM (correct or incorrect) can provide "hints" to enhance the stronger LLM. To answer this question, we experimented with MoT-1D-Verify, where we additionally passed the two inconsistent answers $A_{1}^{u r}$ and $A_{2}^{u r}$ as hints to the stronger LLM following the format of prior work (Zheng et al., 2023). We observed that the hints can only yield slight improvement on DATE, but greatly hurt the model performance on GSM8k and CREPE. Therefore, we conclude that hints from the weaker LLM, when it is uncertain about the question, do not help the stronger LLM. Details are in Appendix G.</p>
<p>Can our method generalize to factual-based tasks? We also explored whether our method can be generalized to factual-based reasoning tasks in Appendix J.</p>
<h1>4 Related Work</h1>
<p>Earlier research has delved into techniques to enhance the cost efficiency of LLMs, including quantization, pruning, and decoding methods(Bai et al., 2022; Kurtic et al., 2023; Leviathan et al., 2022). While many LLMs are closed-source, certain studies have concentrated on how to utilize the API efficiently. Chen et al. (2023a) proposed an LLM cascade, which sends a query to a list of LLMs sequentially if the answers provided by the prior LLMs are regarded unacceptable. A concurrent effort is made by Šakota et al. (2023), where the proposed approach determines which LLM to invoke prior to sending the query to any LLM. Both of these approaches employed external verifiers fine-tuned from smaller LMs and therefore demanded substantial training data and additional model learning. In our experiments, we showed the ineffectiveness of these approaches when they were used to save costs for complex reasoning tasks, which echos the discovery of the shortcomings of LLM fine-tuning in recent work (Ni et al., 2023). In contrast, we proposed a novel solution for LLM cascades based on the answer consistency of the weaker LLM, which is training-free and is not limited to a specific weaker LLM.</p>
<p>Reasoning with LLMs LLMs have demonstrated strong capability in solving reasoning tasks (Rae et al., 2021; Lewkowycz et al., 2022; Zhong et al., 2023). The standard few-shot incontext learning for LLMs is to feed the question to the LLM and let it generate the answer directly (Brown et al., 2020). Wei et al. (2022) then proposed the Chain-of-Thought approach, showing that prompting LLMs to think step by step can significantly improve their performance in solving reasoning problems, as it activates the built-in reasoning capabilities of LLMs (Madaan \&amp; Yazdanbakhsh, 2023). To further facilitate the mathematical steps in a reasoning task, Chen et al. (2022) and Gao et al. (2023) proposed the Program-of-Thought prompting, using code as the intermediate step and executing it with an external interpreter to entangle reasoning and numerical calculation. While most existing works studied CoT and PoT independently as alternative approaches, several other works have delved into the synergy between them, e.g., integrating code into CoT (Cheng et al., 2023b; He-Yueya et al., 2023), ensembling algebraic and code representations (Imani et al., 2023; Yue et al., 2023), or performing automatic selection between CoT and PoT (Zhao et al., 2023). Similar to these works, our approaches employ CoT and PoT jointly; however, unlike them, we leverage this synergy for the novel application of cost-saving for LLM reasoning. On the other hand, like our LLM cascades, there are also recent works building pipelines chaining multiple LLMs (Varshney \&amp; Baral, 2022; Xiong et al., 2023a; Li et al., 2023; Lin et al., 2023a), but these works have a different focus on improving the task accuracy, rather than reducing the LLM costs.</p>
<p>Uncertainty of LLMs Our approach involves measuring the uncertainty of the results of LLMs. Multiple concurrent works have investigated using the voting score to evaluate the answer's uncertainty (Xiong et al., 2023b; Si et al., 2023; Cai et al., 2023). However, they dismissed the potential application for different representations of intermediate steps in the evaluation.</p>
<h2>5 CONCLUSION</h2>
<p>We introduce a simple yet efficient and universally applicable economical pipeline to dynamically decide the LLMs in reasoning tasks, so as to save the token costs. Our approach based on checking the answer consistency of the weaker LLM is novel and effective. Our discoveries emphasize that leveraging prompts with a mixture of thought representations in weaker LLM achieves the best performance as it introduces diverse answers. Compared with fully employing the stronger LLM, our pipeline requires approximately $40 \%$ of expenses to achieve a comparable result. Future works are listed in Appendix K.</p>
<h1>6 ETHICS STATEMENT</h1>
<p>Our research endeavors to build a cost-efficient pipeline in solving reasoning problems in the LLM era. The significance of this work holds the potential to benefit a diverse range of organizations, particularly those with limited financial resources such as local businesses, educational institutions, and non-profit organizations. By devising cost-effective solutions, we empower these resourceconstrained entities to harness the reasoning ability of LLMs cheaply, thereby fostering fairness and inclusivity within the NLP community. This aligns with the broader goal of ensuring that advancements in NLP do not remain exclusive to a select few, but are accessible to a wider audience, regardless of their financial constraints.</p>
<p>Moreover, for these industry giants, the cost savings facilitated by our findings can also be substantial, particularly when facing extremely large throughputs. By optimizing cost-efficiency, our research contributes to not only economic savings but also environmental sustainability by mitigating the carbon footprint associated with running large-scale computations.</p>
<h2>7 REPRODUCIbility STATEMENT</h2>
<p>Our pipeline is simple to implement and reproducible. We have documented all the experimental details, both in the main text and the appendix sections. While we cannot include the complete text of every prompt due to their excessive length, we do provide examples of each prompt in Appendix L, facilitating the reader's comprehension of the style employed. All of our implementations (including the complete prompt scripts, the code for training external verifiers, the code for approach evaluation, etc.) are publicly available at https://github.com/MurongYue/LLM_MoT_cascade.</p>
<h2>8 ACKNOWLEDGEMENT</h2>
<p>This project was supported by Microsoft. It was also partially supported by resources provided by the Office of Research Computing at George Mason University (URL: https://orc.gmu. edu) and funded in part by grants from the National Science Foundation (Awards Number 1625039 and 2018631). Murong and Ziyu appreciate the funding support from George Mason College of Engineering and Computing.</p>
<h2>REFERENCES</h2>
<p>Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, and Michael Lyu. Towards efficient posttraining quantization of pre-trained language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=tvDRmAxGIjw.</p>
<p>BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=uyTL5Bvosj.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Zefan Cai, Baobao Chang, and Wenjuan Han. Human-in-the-loop through chain-of-thought. arXiv preprint arXiv:2306.07932, 2023.</p>
<p>Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance. arXiv preprint arXiv:2305.05176, 2023a.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023b.</p>
<p>Zhoujun Cheng, Jungo Kasai, and Tao Yu. Batch prompting: Efficient inference with large language model apis. arXiv preprint arXiv:2301.08721, 2023a.</p>
<p>Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. Binding language models in symbolic languages. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id=1H1PV42cbF.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021 .</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: Program-aided language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 10764-10799. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/gao23f.html.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL), 2021.</p>
<p>Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 14953-14962, June 2023.</p>
<p>Joy He-Yueya, Gabriel Poesia, Rose E Wang, and Noah D Goodman. Solving math word problems by combining language models with symbolic solvers. arXiv preprint arXiv:2304.09102, 2023.</p>
<p>Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398, 2023.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know, 2022.</p>
<p>Eldar Kurtic, Elias Frantar, and Dan Alistarh. Ziplm: Hardware-aware structured pruning of language models. arXiv preprint arXiv:2302.04089, 2023.</p>
<p>Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, 2022. URL https://api. semanticscholar.org/CorpusID:254096365.</p>
<p>Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=IFXTZERXdM7.</p>
<p>Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1228612312, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.acl-long.687. URL https://aclanthology.org/2023.acl-long.687.</p>
<p>Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https://openreview.net/forum?id= Rzk3GP1HN7.</p>
<p>Jianzhe Lin, Maurice Diesendruck, Liang Du, and Robin Abraham. Batchprompt: Accomplish more with less, 2023b.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 158-167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Ro{bert}a: A robustly optimized {bert} pretraining approach, 2020. URL https://openreview.net/forum?id=SyxS0T4tvS.</p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In International Conference on Learning Representations (ICLR), 2023.</p>
<p>Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought it takes two to tango, 2023. URL https://openreview.net/forum?id=z9fXRC5XdT.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.</p>
<p>Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. Learning math reasoning from self-sampled correct and partiallycorrect solutions. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=4D4TSJE6-K.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.</p>
<p>Lior Rokach. Ensemble-based classifiers. Artificial intelligence review, 33:1-39, 2010.
Marija Šakota, Maxime Peyrard, and Robert West. Fly-swat or cannon? cost-effective language model choice via meta-modeling. arXiv preprint arXiv:2308.06077, 2023.</p>
<p>Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber, and Lijuan Wang. Prompting GPT-3 to be reliable. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= 98p5x51L5af.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Neeraj Varshney and Chitta Baral. Model cascading: Towards jointly improving efficiency and accuracy of NLP systems. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11007-11021, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.756. URL https://aclanthology. org/2022.emnlp-main. 756 .</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMdrw.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMeSB_J.</p>
<p>Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. Diving into the inter-consistency of large language models: An insightful analysis through debate. arXiv preprint arXiv:2305.11595, 2023a.</p>
<p>Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063, 2023b.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.</p>
<p>Li Zhang, Hainiu Xu, Yue Yang, Shuyan Zhou, Weiqiu You, Manni Arora, and Chris Callisonburch. Causal reasoning of entities and events in procedural texts. In Findings of the Association for Computational Linguistics: EACL 2023, pp. 415-431, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023. findings-eacl. 31 .</p>
<p>Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with large language models for reasoning. arXiv preprint arXiv:2305.14333, 2023.</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023.</p>
<p>Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.</p>
<h1>A COST ANALYSIS OF LLM CASCADE APPROACHES</h1>
<p>In Section 2.3, we introduced ten approaches to implement the LLM cascade decision maker. As we are interested in saving the cost of LLM usage, we first analyze the token usage of each approach. While different thought representations of the same task example could induce different token usages, the difference is very hard to quantify and also depends on the specific reasoning task. For example, for GSM8k (Cobbe et al., 2021), the PoT representation of the same example is typically shorter than its CoT counterpart, while a reversed comparison is observed for DATE and Navigate (bench authors, 2023). In addition, different demonstration examples may also consist of different numbers of tokens, which is hard to quantify as well. To provide a unified analysis, we assume that every demonstration example for any task, regardless of the thought representation in use, consumes $N_{t o k}$ tokens.</p>
<p>Based on our hypothesis, we conducted a cost analysis, with results summarized in Table 1. To give an example, consider comparing the token costs of CoT-1D-Vote and CoT-2D-Vote. For CoT-1D-Vote, the cost of calling weaker LLM is $\left(M C_{t}^{w}+K C_{o}^{w}\right) \times N_{t o k}$; for CoT-2D-Vote, it is $\left(2 M C_{t}^{w}+K_{1} C_{o}^{w}+K_{2} C_{o}^{w}\right) \times N_{t o k}$. Here, $C_{t}^{w}$ and $C_{o}^{w}$ are the input and output per-token cost for the weaker LLM, respectively, and $M$ is the number of demonstrations in each prompt. In practice, we set $K_{1}=K_{2}=K_{2 D} / 2$, where $K_{2 D}$ is the total sample size of two sets of demonstration examples, and the total cost for CoT-2D-Vote can then be rewritten to $\left(2 M C_{t}^{w}+K_{2 D} C_{o}^{w}\right)$. To keep the costs of CoT-1D-Vote and CoT-2D-Vote consistent, we set $K_{2 D}=K_{1 D}-M \times \frac{C_{t}^{w}}{C_{o}^{w}}$. Similarly, we can get the total sample size of different representations $K_{M o T}$. The analysis guides us in configuring each approach to be "cost-comparable" (i.e., leading to similar $C^{w}$ 's). Finally, we note that we do not change the number of task demonstrations (i.e., $M$ ) because LLMs are very sensitive to this configuration.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Certainty Value (s) Definition</th>
<th style="text-align: center;">Cost Estimation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CoT-1D-Vote</td>
<td style="text-align: center;">Agreement of $K$ CoT samples on the majority-voted answer $A^{w}, s=$ $\sum_{i=t}^{K} \mathbb{1}<em i="i">{A</em> / K$}^{w}=A^{w}</td>
<td style="text-align: center;">$\left(M C_{t}^{w}+K_{1 D} C_{o}^{w}\right) \times N_{t o k}$</td>
</tr>
<tr>
<td style="text-align: center;">PoT-1D-Vote</td>
<td style="text-align: center;">Agreement of $K$ PoT samples on the majority-voted answer $A^{w}, s=$ $\sum_{i=t}^{K} \mathbb{1}<em i="i">{A</em> / K$}^{w}=A^{w}</td>
<td style="text-align: center;">$\left(M C_{t}^{w}+K_{1 D} C_{o}^{w}\right) \times N_{t o k}$</td>
</tr>
<tr>
<td style="text-align: center;">MoT-1D-Vote</td>
<td style="text-align: center;">Agreement of $K_{M o T} / 2$ CoT and $K_{M o T} / 2$ PoT samples on the majority-voted answer $A^{w}, s=\sum_{i=t}^{K_{M o T}} \mathbb{1}<em i="i">{A</em>$}^{w}=A^{w}} / K_{M o T</td>
<td style="text-align: center;">$2 \times\left(M C_{t}^{w}+\frac{K_{M o T}}{2} C_{o}^{w}\right) \times N_{t o k}$</td>
</tr>
<tr>
<td style="text-align: center;">CoT-2D-Vote</td>
<td style="text-align: center;">Agreement of $K_{2 D} / 2$ CoT samples over 2 sets of task demonstrations on the majority-voted answer $A^{w}, s=\sum_{i=t}^{K_{2 D}} \mathbb{1}<em i="i">{A</em>$}^{w}=A^{w}} / K_{2 D</td>
<td style="text-align: center;">$2 \times\left(M C_{t}^{w}+\frac{K_{2 D}}{2} C_{o}^{w}\right) \times N_{t o k}$</td>
</tr>
<tr>
<td style="text-align: center;">PoT-2D-Vote</td>
<td style="text-align: center;">Agreement of $K_{2 D} / 2$ PoT samples over 2 sets of task demonstrations on the majority-voted answer $A^{w}, s=\sum_{i=t}^{K_{2 D}} \mathbb{1}<em i="i">{A</em>$}^{w}=A^{w}} / K_{2 D</td>
<td style="text-align: center;">$2 \times\left(M C_{t}^{w}+\frac{K_{2 D}}{2} C_{o}^{w}\right) \times N_{t o k}$</td>
</tr>
<tr>
<td style="text-align: center;">MoT-2D-Vote</td>
<td style="text-align: center;">Agreement of $K_{M o T} / 2$ CoT samples on one set of task demos and $K_{M o T} / 2$ PoT samples on another set of task demos on their majority-voted answer $A^{*}$, $s=\sum_{i=t}^{K_{M o T}} \mathbb{1}<em i="i">{A</em>$}^{w}=A^{w}} / K_{M o T</td>
<td style="text-align: center;">$2 \times\left(M C_{t}^{w}+\frac{K_{M o T}}{2} C_{o}^{w}\right) \times N_{t o k}$</td>
</tr>
<tr>
<td style="text-align: center;">CoT-2D-Verify</td>
<td style="text-align: center;">Agreement of the two majority-voted answers $A_{1}^{w}$ and $A_{2}^{w}$ over 2 sets of CoT task demonstrations with $K_{2 D}$ examples, $s=\mathbb{1}<em 2="2">{A</em>$}^{w^{\prime}}=A_{2}^{w^{\prime}}</td>
<td style="text-align: center;">$2 \times\left(M C_{t}^{w}+\frac{K_{2 D}}{2} C_{o}^{w}\right) \times N_{t o k}$</td>
</tr>
<tr>
<td style="text-align: center;">PoT-2D-Verify</td>
<td style="text-align: center;">Agreement of the two majority-voted answers $A_{1}^{w}$ and $A_{2}^{w}$ over 2 sets of PoT task demonstrations with $K_{2 D}$ examples, $s=\mathbb{1}<em 2="2">{A</em>$}^{w^{\prime}}=A_{2}^{w^{\prime}}</td>
<td style="text-align: center;">$2 \times\left(M C_{t}^{w}+\frac{K_{2 D}}{2} C_{o}^{w}\right) \times N_{t o k}$</td>
</tr>
<tr>
<td style="text-align: center;">MoT-2D-Verify</td>
<td style="text-align: center;">Agreement of the two majority-voted answers $A_{1}^{w}$ and $A_{2}^{w}$ over one set of CoT and another set of PoT task demonstrations with $K_{M o T} / 2$ examples, $s=$ $\mathbb{1}<em 2="2">{A</em>$}^{w^{\prime}}=A_{2}^{w^{\prime}}</td>
<td style="text-align: center;">$2 \times\left(M C_{t}^{w}+\frac{K_{M o T}}{2} C_{o}^{w}\right) \times N_{t o k}$</td>
</tr>
<tr>
<td style="text-align: center;">MoT-1D-Verify</td>
<td style="text-align: center;">Agreement of the two majority-voted answers $A_{1}^{w}$ and $A_{2}^{w}$ for CoT and PoT over the same set of task demonstrations with $K_{M o T} / 2$ examples, $s=$ $\mathbb{1}<em 2="2">{A</em>$}^{w^{\prime}}=A_{2}^{w^{\prime}}</td>
<td style="text-align: center;">$2 \times\left(M C_{t}^{w}+\frac{K_{M o T}}{2} C_{o}^{w}\right) \times N_{t o k}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Investigated approaches and cost calculations. For simplicity, we assume $M$-shot demonstration prompting and that every demonstration example has $N_{t o k}$ tokens. $C_{t}^{w}$ is the input price per token and $C_{o}^{w}$ is the output price per token. To keep consistent costs of using the weaker LLM, we set $K_{M o T}=K_{2 D}=K_{1 D}-M \times \frac{C_{o}^{w}}{C_{o}^{w}}$.</p>
<h2>B MAIN EXPERIMENTAL DETAILS</h2>
<p>Implementation Details We run each approach two times to reduce variance and report the average results. Since DATE and Navigate do not have a training set, before experiments we sampled 8 shots of examples randomly and annotated them to be the second set of task demonstrations (for 2D</p>
<p>approaches). The remaining examples are used consistently across all experiments as the test set. We adopt "Python code, return ans" in PoT prompt to let the LLM generate the Python code (Chen et al., 2022). The interpreter we used for executing PoT is Python 3.10, with some packages, such as DateTime ${ }^{2}$ to facilitate the execution of the generated code.</p>
<p>Metrics Details We evaluate the methods based on task accuracy and cost efficiency, with a lower cost and higher accuracy indicating better performance. For accuracy, when the answer is a string, we use exact matching. When the answer is a number, we relax the evaluation criteria due to potential variations in the exact computations carried out by the external interpreter. Following prior work (Chen et al., 2022), we adopt the tolerance to 0.001 . For cost efficiency, we calculate the actual token number based on the tiktoken ${ }^{3}$ and the total cost ${ }^{4}$ for each method and get the relative cost by comparing it with the total cost of GPT-4-CoT-SC.</p>
<h1>C Main Result Tables</h1>
<p>In this section, we show the exact numerical results that are reported in Figure 3. The GSM8k result is in Table 2. The ASDIV result is in Table 3. The TabMWP result is in Table 4. The DATE result is in Table 5. The Navigation result is in Table 6. The CREPE result is in Table 7. The average results are in Table 8.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Voting Method</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-2D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-2D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-2d-Vote</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Threshold</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
</tr>
<tr>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.875</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.832</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">0.897</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.195</td>
</tr>
<tr>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.896</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.241</td>
</tr>
<tr>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.907</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">0.943</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.942</td>
<td style="text-align: center;">0.334</td>
</tr>
<tr>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.381</td>
</tr>
<tr>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.336</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.425</td>
</tr>
<tr>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.425</td>
</tr>
<tr>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.438</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.514</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.528</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">0.530</td>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.938</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">0.598</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.600</td>
</tr>
<tr>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.699</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.691</td>
</tr>
<tr>
<td style="text-align: center;">Verify Method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-1D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Without Cascade</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3-CoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3-PoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 CoT-Greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 PoT-Greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 CoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 PoT-SC</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.842</td>
<td style="text-align: center;">0.111</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.078</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.789</td>
<td style="text-align: center;">0.942</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.745</td>
</tr>
</tbody>
</table>
<p>Table 2: Exact numerical results on GSM8k.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Voting Method</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-2D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-2D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-2d-Vote</td>
</tr>
<tr>
<td style="text-align: center;">Threshold</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.173</td>
<td style="text-align: center;">0.911</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.916</td>
</tr>
<tr>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.908</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.190</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.218</td>
<td style="text-align: center;">0.920</td>
</tr>
<tr>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.931</td>
</tr>
<tr>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.914</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.887</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.893</td>
<td style="text-align: center;">0.239</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.897</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.347</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.396</td>
<td style="text-align: center;">0.934</td>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.348</td>
<td style="text-align: center;">0.916</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.934</td>
</tr>
<tr>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.934</td>
</tr>
<tr>
<td style="text-align: center;">Verify Method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-1D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.272</td>
<td style="text-align: center;">0.908</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Without Cascade</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3-CoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3-PoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 CoT-Greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 PoT-Greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 CoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 PoT-SC</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.887</td>
<td style="text-align: center;">0.115</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.106</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.742</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.943</td>
</tr>
</tbody>
</table>
<p>Table 3: Exact numerical results on ASDIV.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Voting Method</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-2D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-2D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-2d-Vote</td>
</tr>
<tr>
<td style="text-align: center;">Threshold</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">0.155</td>
<td style="text-align: center;">0.912</td>
</tr>
<tr>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.880</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">0.177</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.897</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.924</td>
</tr>
<tr>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.899</td>
<td style="text-align: center;">0.195</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.916</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.952</td>
</tr>
<tr>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.899</td>
<td style="text-align: center;">0.216</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.952</td>
</tr>
<tr>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.952</td>
</tr>
<tr>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.915</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.952</td>
</tr>
<tr>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.957</td>
<td style="text-align: center;">0.484</td>
<td style="text-align: center;">0.956</td>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.956</td>
</tr>
<tr>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.601</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.955</td>
</tr>
<tr>
<td style="text-align: center;">Verify Method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-1D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.940</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Without Cascade</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3-CoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3-PoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 CoT-Greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 PoT-Greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 CoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 PoT-SC</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.844</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.072</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.819</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.728</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.941</td>
</tr>
</tbody>
</table>
<p>Table 4: Exact numerical results on TabMWP.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Voting Method</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-2D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-2D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-2d-Vote</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Threshold</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
</tr>
<tr>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.737</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.793</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.775</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.829</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">0.812</td>
<td style="text-align: center;">0.197</td>
</tr>
<tr>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.806</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.844</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.826</td>
<td style="text-align: center;">0.225</td>
</tr>
<tr>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.367</td>
</tr>
<tr>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.779</td>
<td style="text-align: center;">0.289</td>
<td style="text-align: center;">0.828</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.425</td>
</tr>
<tr>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.795</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.837</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.469</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.484</td>
</tr>
<tr>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.469</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.484</td>
</tr>
<tr>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.502</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.583</td>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.637</td>
<td style="text-align: center;">0.891</td>
<td style="text-align: center;">0.654</td>
</tr>
<tr>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.744</td>
</tr>
<tr>
<td style="text-align: center;">Verify Method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-1D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">0.456</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.875</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Without Cascade</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3-CoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3-PoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 CoT-Greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 PoT-Greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 CoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 PoT-SC</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.677</td>
<td style="text-align: center;">0.107</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">1.031</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">1.402</td>
</tr>
</tbody>
</table>
<p>Table 5: Exact numerical results on DATE.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Voting Method</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-2D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-2D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-2d-Vote</td>
</tr>
<tr>
<td style="text-align: center;">Threshold</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.132</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.213</td>
<td style="text-align: center;">0.898</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.145</td>
<td style="text-align: center;">0.880</td>
</tr>
<tr>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.134</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.898</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.883</td>
</tr>
<tr>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.140</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.308</td>
<td style="text-align: center;">0.964</td>
</tr>
<tr>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.885</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.966</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.966</td>
</tr>
<tr>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.866</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.908</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.967</td>
</tr>
<tr>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.159</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.908</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.967</td>
</tr>
<tr>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.971</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.970</td>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.548</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.339</td>
<td style="text-align: center;">0.935</td>
<td style="text-align: center;">0.649</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.972</td>
</tr>
<tr>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.942</td>
<td style="text-align: center;">0.743</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">0.973</td>
</tr>
<tr>
<td style="text-align: center;">Verify Method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-1D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.879</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.966</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.966</td>
<td style="text-align: center;">0.398</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Without Cascade</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3-CoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3-PoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 CoT-Greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 PoT-Greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 CoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 PoT-SC</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.856</td>
<td style="text-align: center;">0.128</td>
<td style="text-align: center;">0.859</td>
<td style="text-align: center;">0.154</td>
<td style="text-align: center;">0.977</td>
<td style="text-align: center;">0.701</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.966</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.878</td>
</tr>
</tbody>
</table>
<p>Table 6: Exact numerical results on Navigate.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Voting Method</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-2D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-2D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-2d-Vote</td>
</tr>
<tr>
<td style="text-align: center;">Threshold</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.838</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.838</td>
<td style="text-align: center;">0.461</td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.866</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.874</td>
</tr>
<tr>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.842</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.855</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.878</td>
</tr>
<tr>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.865</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.870</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.885</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">0.885</td>
</tr>
<tr>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.852</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.865</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.882</td>
</tr>
<tr>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.856</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.573</td>
<td style="text-align: center;">0.881</td>
</tr>
<tr>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.863</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.879</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.880</td>
</tr>
<tr>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.853</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">0.879</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.878</td>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.534</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.851</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.873</td>
</tr>
<tr>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.803</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.621</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.871</td>
</tr>
<tr>
<td style="text-align: center;">Verify Method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-1D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.534</td>
<td style="text-align: center;">0.883</td>
<td style="text-align: center;">0.536</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Without Cascade</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3-CoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3-PoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 CoT-Greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 PoT-Greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 CoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 PoT-SC</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.577</td>
<td style="text-align: center;">0.876</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.882</td>
</tr>
</tbody>
</table>
<p>Table 7: Exact numerical results on CREPE.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Voting Method</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-2D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-2D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-1D-Vote</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-2d-Vote</td>
</tr>
<tr>
<td style="text-align: center;">Threshold</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.202</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.861</td>
<td style="text-align: center;">0.195</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.883</td>
</tr>
<tr>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.195</td>
<td style="text-align: center;">0.858</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.216</td>
<td style="text-align: center;">0.891</td>
</tr>
<tr>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.216</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.924</td>
</tr>
<tr>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.871</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.896</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">0.925</td>
</tr>
<tr>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.348</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.928</td>
</tr>
<tr>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.884</td>
<td style="text-align: center;">0.289</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.927</td>
</tr>
<tr>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.336</td>
<td style="text-align: center;">0.893</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.915</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.930</td>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.904</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.921</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.920</td>
<td style="text-align: center;">0.530</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.576</td>
<td style="text-align: center;">0.930</td>
</tr>
<tr>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.918</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">0.611</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.663</td>
<td style="text-align: center;">0.930</td>
</tr>
<tr>
<td style="text-align: center;">Verify Method</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-1D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MoT-2D-Verify</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.894</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.398</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Without Cascade</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3-CoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3-PoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 CoT-Greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 PoT-Greedy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 CoT-SC</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4 PoT-SC</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Cost</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">0.116</td>
<td style="text-align: center;">0.807</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.743</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.913</td>
</tr>
</tbody>
</table>
<p>Table 8: Exact numerical results on average across all datasets.</p>
<h1>D CASE STUDY</h1>
<p>In this section, we perform a case study to learn why MoT can achieve better performance. When CoT makes a mistake, prompting the LLM with another set of CoT task demonstrations often leads to the same mistake. In contrast, PoT more frequently either directly gives a correct answer or makes a different mistake than CoT's. Among those cases, $43 \%$ fall into the former category that PoT can solve the question, while the remaining $57 \%$ fall into the latter that PoT can make a different mistake. Here we use CoT1, CoT2, and PoT1 to represent the different prompts. CoT1 and CoT2 share the same representation but different demonstrations while CoT1 and PoT1 share the same demonstration but different representations.</p>
<h2>D. 1 CASEs PoT CAN SOLVE</h2>
<p>Here we show cases satisfying that PoT can solve the question. The first case shows that the PoT can provide different reasoning paths to avoid the same mistake with CoT and the second case shows that the PoT can avoid the computation errors.</p>
<h2>D.1.1 CASE 1</h2>
<p>Question: Juan and his brother Carlos are selling lemonade. For each gallon they make it costs 3 for lemons and 2 for sugar. They sell each glass for 0.50 and get 20 glasses per gallon. If they made 25 in profit, how much did they spend on lemons?</p>
<p>Gold Answer: Each gallon sells for a total of 10 because $20 \times 0.5=10$
Each gallon costs 5 to make because $3+2=5$
They make 5 off each gallon because $10-5=5$
They made and sold 5 gallons because $25 / 5=5$
They spent 15 on lemons because $5 \times 3=15$
CoT1: Answer: They make 10 on each gallon of lemonade because $20 \times 0.5=10$
They made 2.5 gallons of lemonade because $25 / 10=2.5$
They spent 7.5 on lemons because $2.5 \times 3=7.5$
They spent 5 on sugar because $2.5 \times 2=5$
They spent 12.5 in total because $7.5+5=12.5$
ans $=7.5$
CoT2: Answer: They make 10 per gallon because $20 \times 0.5=10$
They sold 2.5 gallons because $25 / 10=2.5$
They spent 7.5 on sugar because $2.5 \times 2 \times 1.5=7.5$
They spent 7.5 on lemons because $2.5 \times 3=7.5$
ans $=7.5$</p>
<h2>PoT1:</h2>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Python code, return ans
cost_per_gallon = 3 + 2
num_of_glasses_per_gallon = 20
profit_per_gallon = num_of_glasses_per_gallon <span class="gs">* 0.5 -\</span>
<span class="gs">cost_per_gallon</span>
<span class="gs">total_profit = 25</span>
<span class="gs">total_gallons = total_profit / profit_per_gallon</span>
<span class="gs">cost_of_lemons = total_gallons *</span> 3
ans = cost_of_lemons
<span class="gh">#</span>ans = 15.0(This line is not generated by LLM.)
</code></pre></div>

<h2>D.1.2 CASE 2</h2>
<p>Question: Robert wants to buy a bookshelf and a sofa and has received two offers. Furniture United's offer includes a 1,350 advance payment and 6 monthly installments of 350 each. Cozy Homes' offer includes a 1, 100 advance payment and 9 monthly installments of 250 each. What is the cost difference between the two offers?</p>
<p>Gold Answer: Furniture United's offer is $1,350+(6 \times 350)=3,450$ Cozy Homes' offer is $1,100+(9 \times 250)=3,350$. The difference is $3,450-3,350=100$</p>
<p>CoT1: Answer: Furniture United's total cost would be $1350+6 \times 350=3150$. Cozy Homes' total cost would be $1100+9 \times 250=3350$. The cost difference between the two offers is $3350-3150=$ 200. ans $=200$</p>
<p>CoT2: Answer: The total cost of Furniture United's offer is $1350+(6 \times 350)=3,150$. The total cost of Cozy Homes' offer is $1100+(9 \times 250)=3,350$. The cost difference between the two offers is $3350-3150=200$. ans $=200$.</p>
<h1>PoT1:</h1>
<div class="codehilite"><pre><span></span><code>furniture_united_advance = 1350
furniture_united_monthly_installment = 350
cozy_homes_advance = 1100
cozy_homes_monthly_installment = 250
furniture_united_total_cost = furniture_united_advance + \
(6 <span class="gs">* furniture_united_monthly_installment)</span>
<span class="gs">cozy_homes_total_cost = cozy_homes_advance + \</span>
<span class="gs">(9 *</span> cozy_homes_monthly_installment)
ans = furniture_united_total_cost - cozy_homes_total_cost
<span class="gh">#</span> ans = 100.0(This line is not generated by LLM.)
</code></pre></div>

<h2>D. 2 CASEs PoT CANNOT Solve</h2>
<p>In this section, we show some cases that PoT cannot solve but will make a different mistake. In case 1, the CoT forgot to calculate the first 40-yard line but the PoT calculated the first 40-yard but with an incorrect grounding value. In case 2, the CoT forgot to remove the lowest score. The PoT removed the lowest score but grounded an incorrect number of total tests. We can see that although both the CoT and PoT make mistakes, the reasons why they make mistakes are different, making the final results different.</p>
<h2>D.2.1 CASE 1</h2>
<p>Question: Blake and Kelly are having a contest to see who can run the most in 15 minutes. They decide to do it on a football field that is 100 yards long. Blake runs back and forth 15 times. Kelly runs back and forth once, and then decides that she doesn't want to run next to Blake, so she starts to run to the 40 -yard line and back. She does this 34 times. How much farther does the winner run than the loser?
Gold Answer: Blake runs 200 yards for each back and forth because $100 \times 2=200$ He runs 3000 yards in total because $200 \times 15=3000$ Kelly runs 200 yards at first because $100 \times 2=200$ She runs 80 yards each time she does a back and forth to the 40 -yard line because $40 \times 2=80$ She runs 2720 from these back and forth because $34 \times 80=2720$ She runs 2920 in total because $2720+200=2920$ Blake won because $3000&gt;2920$ The winner ran 80 yards more than the loser because 3000 - 2920 $=80$
CoT1: Answer: Blake runs back and forth 15 times, so she runs 15<em>100</em>2=3000 yards. Kelly runs to the 40 -yard line and back 34 times, so she runs $34 * 80=2720$ yards. Thus, Blake runs 3000$2720=280$ yards farther than Kelly. ans $=280$
CoT2: Answer: Blake runs a total of $15 \times 100 \times 2=3000$ yards. Kelly runs $34 \times(40 \times 2)=2720$ yards. Blake runs $3000-2720=280$ yards farther than Kelly. ans $=280$.</p>
<h2>PoT1:</h2>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Python code, return ans
length_of_football_field = 100
num_of_laps_for_blake = 15
num_of_laps_for_kelly = 34
distance_covered_by_blake = length_of_football_field <span class="gs">* \</span>
<span class="gs">num_of_laps_for_blake *</span> 2
distance_covered_by_kelly = (length_of_football_field *\
</code></pre></div>

<div class="codehilite"><pre><span></span><code>num_of_laps_for_kelly <span class="gs">* 2) + (40 *</span> num_of_laps_for_kelly * 2)
ans = abs(distance_covered_by_blake - distance_covered_by_kelly)
<span class="gh">#</span> ans = 6520.0(This line is not generated by LLM.)
</code></pre></div>

<h1>D.2.2 CASE 2</h1>
<p>Question: Brinley is in Mr. Bert's math class. Mr. Bert gives six tests each semester. He allows the students to remove the lowest score from the average each semester. If Brinley has scores of 89, 71, 92,100 and 86 on the first five tests, what score does she need on the sixth test to get an average of 93 ?</p>
<p>Gold Answer: First, identify the lowest score, which is 71, and remove it. Since Brinley wants a score of 93 , we know her total score needs to be 93 (average) * 5 (tests) $=465$ points So far, Brinley has earned $89+92+100+86=367$ points The final test must be $465-367=98$ points.</p>
<p>CoT1: First, we need to find Brinley's current average by adding up her five test scores and dividing by 5: $(89+71+92+100+86) / 5=87.6$ We know that Brinley wants her average to be 93 , which means she needs to have a total of 6 test scores (since one will be dropped). The total of these 6 scores needs to be $93 \times 6=558$ To find out what score Brinley needs on the sixth test, we can subtract the total of her first five scores from the desired total: $558-(89+71+92+100+86)=$ 120 So Brinley needs to get a score of 120 on her sixth test to have an average of 93 after dropping her lowest score. ans $=120$</p>
<p>CoT2: Answer: Brinley's first five test scores add up to 438. To have an average of 93, Brinley needs a total of $93 \times 6=558$ points for all six tests. To get 558 points after five tests, Brinley needs to score $558-438=120$ points on the sixth test. ans $=120$</p>
<h2>PoT1:</h2>
<div class="codehilite"><pre><span></span><code><span class="gh">#</span> Python code, return ans
num_of_tests = 6
total_of_first_five_tests_scores = 89 + 71 + 92 + 100 + 86
desired_average = 93
lowest_score = min(89, 71, 92, 100, 86)
sum_of_best_five_scores = total_of_first_five_tests_scores -\
lowest_score
ans = (desired_average * num_of_tests) - sum_of_best_five_scores
<span class="gh">#</span> ans = 191.0(This line is not generated by LLM.)
</code></pre></div>

<h2>E IMPLEMENTATION DETAILS OF EXTERNAL VERIFIERS</h2>
<p>Throughout these experiments, our choice of the weaker LLM remains GPT-3.5-turbo. We only perform our experiment with CoT representation because the finetuned model we utilize is not suitable for understanding Python code.</p>
<p>The input to the verifier can be either the question alone or a concatenation of both the question and the answer produced by the weaker LLM. The question texts include the context, e.g., the goal and context in CREPE. The answer texts contain both intermediate thoughts and the final answer (e.g., an integer in mathematical reasoning). We have multiple generation results from the weaker LLM. To avoid excessive input length, we only employ the majority-voted answer and its intermediate thoughts in the test time. Specifically, in the test time, we extract the majority-voted answer from two CoT prompts with different demonstrations as this setting shows better performance in our method, and then randomly select one collection of intermediate thoughts that result in this majority-voted answer.</p>
<p>We still use task accuracy and relative cost as the metric. Note that only the question as input doesn't need the answers from the weaker LLM necessarily. Therefore, the total cost $C$ (Eq 1) for only the question as input becomes:</p>
<p>$$
C=C^{d}+\left(1-\mathbb{1}<em _reject="{reject" _text="\text">{\text {reject }}\right) C^{w}+\mathbb{1}</em>
$$}} C^{s</p>
<p>To use an LLM as the verifier, we have designed two distinct prompts: one for "question difficulty prediction", which solely relies on the questions (LLM-Q), and another for "answer evalua-</p>
<p>tion", which incorporates both questions and answers (LLM-QA). The difficulty prediction (LLM-Q) prompt entails the LLM's determination of whether a question necessitates redirection to the stronger LLM, based solely on the question's complexity. If the LLM deems a question challenging for difficulty prediction, it triggers forwarding to the stronger LLM. The answer evaluation (LLM-QA) prompt is to give the LLM both the question and the answer. We then let the LLM rethink the solving steps and generate feedback to evaluate whether the solution is correct. If not, the question will be transferred to the stronger LLM. Both prompting methods are implemented with $M=8$ shots of demonstrations, and we keep the number of "easy" and "hard" questions or the number of "correct" and "incorrect" answers balanced. We set the sampling path as $K=20$ and temperature as $T=0.4$ to sample the major answer and then use the self-consistency (SC) to decide the final "difficulty" or "correctness of the answers". The reproducible prompts are in the Appendix L. Notably, the cost for decision $C^{d}$ in Eq. 1 is no longer 0 with an external verifier. For the LLM as a verifier, we use GPT-3.5-turbo as it's much cheaper than GPT-4. We count the number of tokens used for the approaches LLM-Q and LLM-QA.
For finetuning a smaller LLM (RoBERTa-base of Liu et al. (2020)) as the verifier, we similarly tested two variants, when the input contains only the question (Finetuned-Q) or when it includes both the question and the answers (Finetuned-QA), in which case we use a separation token to split them. The training data for fine-tuning the models come from the original training split of each benchmark dataset. Since DATE does not have a training split, we only performed experiments on GSM8k and CREPE. In each training instance, we have $K=20$ generated responses and we label all of them. We label the generated responses where GPT-3.5-CoT-SC correctly predicts the answer as positive. Notably, the number of total training cases is the number of sampling paths $K=20$ multiplied by the number of questions. To address the data imbalance issue (i.e., most of the training cases are positive), we perform simple under-sampling over the GSM8k. For CREPE, the resulting training set is relatively smaller, so we did not perform the under-sampling.
In the training process, we fine-tune RoBERTa-base with the learning rate $1 \times 10^{-5}$ and batch size 32. We randomly split $10 \%$ of the training data to be the development set. The performance of the best accuracy over the development set is shown in Table 9. One notable approach, predicting "all positive", is to assign a positive label to every example in the development set. We could learn that the external verifier could achieve better performance than "all positive" but not significant enough.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Dev</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">All positive</td>
<td style="text-align: left;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">GSM8k</td>
<td style="text-align: left;">Based on Question</td>
<td style="text-align: left;">0.592</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Based on Question and Answer</td>
<td style="text-align: left;">0.615</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">All positive</td>
<td style="text-align: left;">0.715</td>
</tr>
<tr>
<td style="text-align: left;">CREPE</td>
<td style="text-align: left;">Based on Question</td>
<td style="text-align: left;">0.749</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Based on Question and Answer</td>
<td style="text-align: left;">0.812</td>
</tr>
</tbody>
</table>
<p>Table 9: Performance of finetuning RoBERTa as verifier over the dev set</p>
<p>Table 10 shows the exact numerical results of those presented in Figure 6. We can learn that the external verifier could boost the performance but not that significant. Comparing LLM-Q with LLM-QA, an interesting observation is that the latter costs less than the former. This may seem counter-intuitive at first glance because LLM-QA includes a much longer input (a concatenation of question and answer) than LLM-Q (only question). However, we found out that for both approaches, their final costs are mainly determined by the number of cases transferred to the stronger LLM. That is, when including both the question and the answer as input, LLM-QA tends to trust the weaker LLM more frequently, leading to fewer cases being routed to the stronger LLM and hence the lower cost. However, both approaches do not yield accuracy as high as our MoT-based approaches.</p>
<h1>F LLAMA2-13B as the WeAKer LLM</h1>
<p>Although GPT-3.5-turbo is considered to be weaker compared to GPT-4, it is still more powerful and expensive than most of the open-source LLMs. In this section, we tested the performance of</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://pypi.org/project/DATETime/
${ }^{3}$ https://platform.openai.com/tokenizer
${ }^{4}$ https://openai.com/pricing&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8465 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8465</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8465</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-250451569</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2207.05608v1.pdf" target="_blank">Inner Monologue: Embodied Reasoning through Planning with Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8465.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8465.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inner Monologue</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inner Monologue: Embodied Reasoning through Planning with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based embodied agent framework that closes the loop between planning and environment feedback by continuously injecting textual observations (object lists, success/failure signals, task-progress scene descriptions, and human answers) into the LLM prompt so the model can replan, retry, and ask questions during multi-step physical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Inner Monologue</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM planner that interleaves action generation with grounded textual feedback from perception and human responses. The system uses pre-trained LLMs to decompose high-level instructions into low-level skills, and after each skill execution it appends textual feedback (object recognitions, success detection, scene-progress descriptions, or human answers) into the prompt so the LLM can re-evaluate and replan. Supports chain-of-thought style 'Robot thought' summaries and active queries to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>InstructGPT (1.3B) / PaLM (540B)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Pretrained autoregressive transformer language models used without further fine-tuning; InstructGPT (1.3B) is a human-feedback fine-tuned GPT-3 variant used for simulated and tabletop experiments; PaLM (540B) is a very-large pretrained model used for kitchen mobile manipulation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Embodied manipulation and mobile-manipulation tasks (Simulated Tabletop Rearrangement; Real-World Tabletop Rearrangement; Real-World Kitchen Mobile Manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Long-horizon, vision-based robotic tasks (block/bowl rearrangement, stacking, object sorting, kitchen manipulation including drawers and navigation). Challenges include partial observability, occlusion, low-level policy noise, long horizons, and adversarial disturbances that force retries/replanning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>prompt-augmented working/episodic memory (scratchpad-style task-progress memory + success history + visible/occluded object lists)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>All memory is represented as text appended to the LLM prompt: e.g., lists of currently visible objects and previously visible (occluded) objects, lists of achieved sub-goals (task-progress Scene), success/failure lines from success detectors, and optional chain-of-thought 'Robot thought' summaries. For the kitchen domain, success detection is implemented by separate 'foresight' and 'hindsight' learned models whose outputs are converted to textual '[success: yes/no]' feedback and appended. There is no separate external retrieval database beyond the prompt; memory is the concatenated prompt history and structured scene/status lines.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Concatenate the textual feedback to the LLM input prompt after each executed skill (i.e., prompt extension). The LLM conditions next planned steps on the full prompt history (including prior actions, success outputs, and scene descriptions). The LLM may also generate chain-of-thought summaries which are appended and used for later reasoning, and it can ask human queries whose responses are inserted back into the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>The paper ablates and compares variants that differ only in which textual feedback is provided: Object-only, Object+Success, Object+Scene (task-progress), and combinations. It compares Inner Monologue variants against baselines including a multi-task CLIPort policy in simulation and SayCan in the kitchen domain. Key experimental comparisons: (1) Simulation: Object+Scene outperforms Object-only and Object+Success variants; chain-of-thought improves Object+Scene consistency. (2) Real tabletop: Object+Success closed-loop feedback enables recovery from noisy pick/place; ablation shows occluded objects (lack of scene description) causes failures and lack of success detection causes failures to retry. (3) Real kitchen: Inner Monologue (with Success and optionally Object feedback) substantially outperforms SayCan when adversarial disturbances force skill failures because Inner Monologue can retry/replan using LLM-informed feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Simulation: Object + Scene (task-progress scene descriptions appended to prompt), augmented with chain-of-thought summaries, gave the best performance; Real tabletop: Object + Success (closed-loop success detection plus object lists) are complementary and necessary for recovery from policy noise and occlusion; Real kitchen: Success feedback (foresight + hindsight models) combined with object feedback yields the largest robustness gain under adversarial disturbances. Overall recommendation: maintain explicit task-progress (achieved sub-goals) and success/failure history in the prompt and allow chain-of-thought summaries and human queries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported memory- and feedback-related failure modes include (1) false negatives in success detectors causing unnecessary retries, (2) false positives causing incorrect belief of progress, (3) LLM sometimes ignoring feedback and proposing actions for absent objects, (4) initial occlusions causing objects to be missing from initial scene description so planner omits required steps, and (5) overall bottleneck imposed by low-level control policies (LLM reasoning cannot overcome policy incapacity). The paper notes variability in emergent capabilities depending on prompt examples and LLM capability.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>The paper concludes that closed-loop textual feedback (both structured passive scene descriptions listing visible/occluded objects and task-progress, and success/failure signals after each primitive) significantly improves LLM planning robustness, especially under disturbances. They recommend (a) appending structured feedback to the prompt after each action, (b) using chain-of-thought summarization for complex task-progress reasoning, (c) providing success detectors to allow automatic retries, (d) allowing active human queries when ambiguous, and (e) representing uncertainty in textual feedback and integrating additional modules for safety. They also recommend that future systems aggregate potentially inaccurate sources of information (e.g., by describing uncertainty in text) and aim for automated scene description to remove reliance on humans.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>This paper studies memory-like mechanisms implemented as prompt-augmented feedback in embodied tasks; it does not study traditional text-game benchmarks (e.g., TextWorld, Jericho) nor specialized memory-augmented agent architectures (e.g., external key-value stores, retrieval-augmented memory) in textual games. Performance is reported qualitatively in the main text and with numeric results in tables (not reproduced in the text excerpt), but the paper emphasizes qualitative ablations between feedback variants rather than introducing a separate external memory module.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Inner Monologue: Embodied Reasoning through Planning with Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Socratic Models: Composing zero-shot multimodal reasoning with language <em>(Rating: 2)</em></li>
                <li>Do as I can and not as I say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>CLIPort: What and where pathways for robotic manipulation <em>(Rating: 1)</em></li>
                <li>SayCan <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8465",
    "paper_id": "paper-250451569",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "Inner Monologue",
            "name_full": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
            "brief_description": "An LLM-based embodied agent framework that closes the loop between planning and environment feedback by continuously injecting textual observations (object lists, success/failure signals, task-progress scene descriptions, and human answers) into the LLM prompt so the model can replan, retry, and ask questions during multi-step physical tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Inner Monologue",
            "agent_description": "LLM planner that interleaves action generation with grounded textual feedback from perception and human responses. The system uses pre-trained LLMs to decompose high-level instructions into low-level skills, and after each skill execution it appends textual feedback (object recognitions, success detection, scene-progress descriptions, or human answers) into the prompt so the LLM can re-evaluate and replan. Supports chain-of-thought style 'Robot thought' summaries and active queries to humans.",
            "llm_model_name": "InstructGPT (1.3B) / PaLM (540B)",
            "llm_model_description": "Pretrained autoregressive transformer language models used without further fine-tuning; InstructGPT (1.3B) is a human-feedback fine-tuned GPT-3 variant used for simulated and tabletop experiments; PaLM (540B) is a very-large pretrained model used for kitchen mobile manipulation experiments.",
            "benchmark_name": "Embodied manipulation and mobile-manipulation tasks (Simulated Tabletop Rearrangement; Real-World Tabletop Rearrangement; Real-World Kitchen Mobile Manipulation)",
            "benchmark_description": "Long-horizon, vision-based robotic tasks (block/bowl rearrangement, stacking, object sorting, kitchen manipulation including drawers and navigation). Challenges include partial observability, occlusion, low-level policy noise, long horizons, and adversarial disturbances that force retries/replanning.",
            "memory_used": true,
            "memory_type": "prompt-augmented working/episodic memory (scratchpad-style task-progress memory + success history + visible/occluded object lists)",
            "memory_architecture": "All memory is represented as text appended to the LLM prompt: e.g., lists of currently visible objects and previously visible (occluded) objects, lists of achieved sub-goals (task-progress Scene), success/failure lines from success detectors, and optional chain-of-thought 'Robot thought' summaries. For the kitchen domain, success detection is implemented by separate 'foresight' and 'hindsight' learned models whose outputs are converted to textual '[success: yes/no]' feedback and appended. There is no separate external retrieval database beyond the prompt; memory is the concatenated prompt history and structured scene/status lines.",
            "memory_integration_strategy": "Concatenate the textual feedback to the LLM input prompt after each executed skill (i.e., prompt extension). The LLM conditions next planned steps on the full prompt history (including prior actions, success outputs, and scene descriptions). The LLM may also generate chain-of-thought summaries which are appended and used for later reasoning, and it can ask human queries whose responses are inserted back into the prompt.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "The paper ablates and compares variants that differ only in which textual feedback is provided: Object-only, Object+Success, Object+Scene (task-progress), and combinations. It compares Inner Monologue variants against baselines including a multi-task CLIPort policy in simulation and SayCan in the kitchen domain. Key experimental comparisons: (1) Simulation: Object+Scene outperforms Object-only and Object+Success variants; chain-of-thought improves Object+Scene consistency. (2) Real tabletop: Object+Success closed-loop feedback enables recovery from noisy pick/place; ablation shows occluded objects (lack of scene description) causes failures and lack of success detection causes failures to retry. (3) Real kitchen: Inner Monologue (with Success and optionally Object feedback) substantially outperforms SayCan when adversarial disturbances force skill failures because Inner Monologue can retry/replan using LLM-informed feedback.",
            "best_memory_strategy": "Simulation: Object + Scene (task-progress scene descriptions appended to prompt), augmented with chain-of-thought summaries, gave the best performance; Real tabletop: Object + Success (closed-loop success detection plus object lists) are complementary and necessary for recovery from policy noise and occlusion; Real kitchen: Success feedback (foresight + hindsight models) combined with object feedback yields the largest robustness gain under adversarial disturbances. Overall recommendation: maintain explicit task-progress (achieved sub-goals) and success/failure history in the prompt and allow chain-of-thought summaries and human queries.",
            "limitations_or_failure_cases": "Reported memory- and feedback-related failure modes include (1) false negatives in success detectors causing unnecessary retries, (2) false positives causing incorrect belief of progress, (3) LLM sometimes ignoring feedback and proposing actions for absent objects, (4) initial occlusions causing objects to be missing from initial scene description so planner omits required steps, and (5) overall bottleneck imposed by low-level control policies (LLM reasoning cannot overcome policy incapacity). The paper notes variability in emergent capabilities depending on prompt examples and LLM capability.",
            "recommendations_or_conclusions": "The paper concludes that closed-loop textual feedback (both structured passive scene descriptions listing visible/occluded objects and task-progress, and success/failure signals after each primitive) significantly improves LLM planning robustness, especially under disturbances. They recommend (a) appending structured feedback to the prompt after each action, (b) using chain-of-thought summarization for complex task-progress reasoning, (c) providing success detectors to allow automatic retries, (d) allowing active human queries when ambiguous, and (e) representing uncertainty in textual feedback and integrating additional modules for safety. They also recommend that future systems aggregate potentially inaccurate sources of information (e.g., by describing uncertainty in text) and aim for automated scene description to remove reliance on humans.",
            "notes": "This paper studies memory-like mechanisms implemented as prompt-augmented feedback in embodied tasks; it does not study traditional text-game benchmarks (e.g., TextWorld, Jericho) nor specialized memory-augmented agent architectures (e.g., external key-value stores, retrieval-augmented memory) in textual games. Performance is reported qualitatively in the main text and with numeric results in tables (not reproduced in the text excerpt), but the paper emphasizes qualitative ablations between feedback variants rather than introducing a separate external memory module.",
            "uuid": "e8465.0",
            "source_info": {
                "paper_title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Socratic Models: Composing zero-shot multimodal reasoning with language",
            "rating": 2
        },
        {
            "paper_title": "Do as I can and not as I say: Grounding language in robotic affordances",
            "rating": 2
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "CLIPort: What and where pathways for robotic manipulation",
            "rating": 1
        },
        {
            "paper_title": "SayCan",
            "rating": 2
        }
    ],
    "cost": 0.012466499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Inner Monologue: Embodied Reasoning through Planning with Language Models</p>
<p>Wenlong Huang 
Fei Xia 
Ted Xiao 
Harris Chan 
Jacky Liang 
Pete Florence 
Andy Zeng 
Jonathan Tompson 
Igor Mordatch 
Yevgen Chebotar 
Pierre Sermanet 
Noah Brown 
Tomas Jackson 
Linda Luu 
Sergey Levine 
Karol Hausman 
Brian 
Inner Monologue: Embodied Reasoning through Planning with Language Models
Ichter Robotics at Google, † equal contribution and alphabetically listed Project website: https://innermonologue.github.io
Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them -answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.</p>
<p>Introduction</p>
<p>Intelligent and flexible embodied interaction requires robots to be able to deploy large repertoires of basic behaviors in appropriate ways, sequence these behaviors as needed for long horizon tasks, and also recognize when to switch to a different approach if a particular behavior or plan is unsuccessful. High-level planning, perceptual feedback, and low-level control are just a few of the sub-tasks that would need to be seamlessly combined together to perform the sort of reasoning required for an embodied agent, such as a robot, to intelligently act in the world. While conventionally these challenges have been approached from the perspective of planning (e.g., TAMP [1]) or hierarchical learning (e.g., HRL [2]), effective high-level reasoning about complex tasks also requires semantic knowledge and understanding of the world.</p>
<p>One of the remarkable observations in recent machine learning research is that large language models (LLMs) can not only generate fluent textual descriptions, but also appear to have rich internalized knowledge about the world [3,4,5,6,7]. When appropriately conditioned (e.g., prompted), they can even carry out some degree of deduction and respond to questions that appear to require reasoning and inference [8,9,10,11,12,13]. This raises an intriguing possibility: beyond their ability to interpret natural language instructions, can language models further serve as reasoning models that combine multiple sources of feedback and become interactive problem solvers for embodied tasks, such as robotic manipulation?</p>
<p>Prior studies show that language helps humans internalize our knowledge and perform complex relational reasoning through thinking in language [14,15,16,17,18]. Imagine the "inner monologue" that happens when a person tries to solve some task: "I have to unlock the door; let me try to pick up the key and put it in the lock... no, wait, it doesn't fit, I'll try another one... that one worked, now I can turn the key." The thought process in this case involves choices about the best immediate action to solve the high-level task ("pick up the key"), observations about the outcomes of attempted actions ("it doesn't fit"), and corrective actions that are taken in response to these observations ("I'll try another one"). Inspired by the human thought process, we propose that such an inner monologue is a natural framework for incorporating feedback for LLMs.</p>
<p>Our work studies these questions by combining LLMs with various sources of textual feedback, only utilizing few-shot prompting without any additional training. We observe that similarly to recent work [19], natural language provides a universal and interpretable interface for such grounding of model communication and allows them to incorporate their conclusions in an overarching inner monologue driven by a language model. While prior work has investigated using language models as planners [20,21] or incorporating  Figure 1: Inner Monologue enables grounded closed-loop feedback for robot planning with large language models by leveraging a collection of perception models (e.g., scene descriptors and success detectors) in tandem with pretrained language-conditioned robot skills. Experiments show our system can reason and replan to accomplish complex long-horizon tasks for (a) mobile manipulation and (b,c) tabletop manipulation in both simulated and real settings. multimodal-informed perception through language [19], to the best of our knowledge no work has studied the critical link of not only planning with language, but also informing embodied feedback with language, which we investigate in this work.</p>
<p>Specifically, we study methods and sources of feedback for closing the agent-environment loop via an inner monologue and their impact on downstream execution success and new capabilities arising from such interaction. In particular, we combine multiple perception models that perform various tasks such as language-conditioned semantic classification or language-based scene description, together with feedback provided by a human user that the robot is cooperating with. To execute the commands given by a user, the actions are chosen from a set of pre-trained robotic manipulation skills together with their textual descriptions that can be invoked by a language model. Our proposed system Inner Monologue chains together these various components (perception models, robotic skills, and human feedback) in a shared language prompt, enabling it to successfully perform user instructions.</p>
<p>Finally, we show that Inner Monologue, without requiring additional training beyond a frozen language model and pre-trained robotic skills, can accomplish complex, long-horizon, and unseen tasks in simulation as well as on two real-world robotic platforms. Notably, we show that it can efficiently retry under observed stochastic failure, replan under systematic infeasibility, or request human feedback for ambiguous queries, resulting in significantly improved performance in dynamical environments. As a demonstration of the versatility of LLMs and grounded closed-loop feedback, we additionally show several surprising capabilities emerging from the inner monologue formulation, including continued adaptation to new instructions, self-proposed goals, interactive scene understanding, multilingual interactions, and more.</p>
<p>Related Work</p>
<p>Task and Motion Planning. Task and motion planning [22,23] requires simultaneously solving a high-level, discrete task planning problem [24,25,26], and a low-level, continuous motion planning problem [27]. Traditionally, this problem has been solved through optimization [28,29] or symbolic reasoning [24,26], but more recently machine learning has been applied to aspects of the problem via learned representations, learned task-primitives, and more [30,31,32,33,34,35,36,37,38]. Some works utilize language for planning and grounding [39,40,41,42,43,44]. Others have approached the problem through hierarchical learning [45,46,34,47,48,49,50]. In this work, we leverage pre-trained LLMs and their semantic knowledge, along with trained low-level skills, to find feasible plans.</p>
<p>Task Planning with Language Models. Various prior works have explored using language as a space for planning [51,52,20,53,21,19]. Similar to ours are recent task planning approaches that leverage pre-trained autoregressive LLMs to decompose abstract, high-level instructions into a sequence of low-level steps executable by an agent [20,21] in a zero-shot manner. Specifically, Huang et al. [20] prompt GPT-3 [9] and Codex [54] to generate action plans for embodied agents, where each action step is semantically translated to an admissible action with a Sentence-RoBERTa model [55,56]. SayCan [21] instead grounds the actions by mul-tiplying each candidate action's probability under FLAN [57] with the action's value function, which serves as a proxy for affordance [34]. However, both approaches effectively produce the plan while assuming that each proposed step is executed successfully by the agent. As a result, these approaches may not be robust in handling intermediate failures in dynamic environments or with poor lower level policies. We explore in Inner Monologue ways to incorporate grounded feedback from the environment into the LLM as we produce each step in the plan.</p>
<p>Fusing Vision, Language, and Control in Robotics. Various works have investigated strategies for the challenging problem of fusing vision, language, and control [58,59,60,61,62,63,64]. While pretrained LLMs typically train only on text data, pretrained visual-language models (e.g., CLIP [65]) are trained on joint image(s) and corresponding text captions via variants of masked language modeling (MLM) objective [66,67,68,69], contrastive loss [70,71,65] or other supervised objectives [72,73]. CLIP has been employed in several robotics and embodied settings in zero-shot manner [74], or combined with Transporter networks [75] as in CLIPort [76]. Socratic Models [19] combines several foundation models (e.g., GPT-3 [9], ViLD [77]) and language-conditioned policies, using language as the common interface, and demonstrated manipulating objects in a simulated vision-based robotic manipulation environment.</p>
<p>Leveraging Embodied Language Feedback with Inner Monologue</p>
<p>In this section, we introduce how LLMs can act as interactive problem solvers and incorporate embodied environment observations into grounded planning through a process we refer to as Inner Monologue.</p>
<p>Problem Statement</p>
<p>Our setting consists of an embodied robotic agent attempting to perform a high-level natural language instruction i. This robotic agent is only capable of executing short-horizon skills from a library of previously trained policies π k ∈Π with short language descriptions k , which may be trained with reinforcement learning or behavioral cloning. The "planner," which is a pretrained LLM [20,21], attempts to find a sequence of skills to accomplish the instruction. To observe the environment, the planner has access to textual feedback o from the environment that can be appended to the instruction or requested by the planner. The observation o may be success detection, object detection, scene description, visual-question answering, or even human feedback. Our work studies to what extent the LLM planner is able to reason over and utilize such feedback to "close the loop" with the environment and improve planning.</p>
<p>Inner Monologue</p>
<p>We formulate an "inner monologue" by continually injecting information from the various sources of feedback into the LLM planning language prompts as the robot interacts with the environment. While LLMs have demonstrated exceptional planning capabilities for embodied control tasks [20], prior works have found it crucial to ground LLM predictions with external components such as affordance functions [21] in order to produce useful plans that are executable by robots. However, LLMs used in this context have thus far remained one-directional -providing a list of skills, without making corrections or leveraging opportunities to replan accordingly. In contrast, Inner Monologue studies settings where grounded environment feedback is provided directly to the LLM in a closed-loop fashion. This promotes improved LLM reasoning in complex long-horizon settings, even before any external affordance-based grounding methods are applied.</p>
<p>Our analysis assumes textual feedback is provided to the planner, but does not assume a single specific method of fusing LLM planning with low-level robotic control or a specific method of extracting environment feedback into language. Rather than focusing on a particular algorithmic implementation, our aim is to provide a case study on the value of incorporating different types of feedback into closed-loop LLM-based planning. Thus, Inner Monologue in Sec 4 utilizes language feedback within separate systems that incorporate different LLMs, different methods of fusing planning with control, different environments and tasks, and different methods of acquiring control policies. We note that in our specific implementations of Inner Monologue, we use pre-trained LLMs for planning that are not finetuned, but rather evaluated solely with few-shot prompting; the full prompts can be found in the Appendix.</p>
<p>Sources of Feedback</p>
<p>In theory any type of environment feedback can inform the LLM planner, as long as it can be expressed through language. We focus on the specific forms of feedback shown in Fig 2, which can be broken down into task-specific feedback, such as success detection, and scene-specific feedback (either "passive" or "active"), </p>
<p>Success Detection.</p>
<p>Semantic success detection is a binary classification problem of whether the low-level skill π k has succeeded. Engineered success detectors can operate on ground-truth state in simulation, while learned success detectors can be trained on real examples of successes and failures in the real world [78,79,80,81,82]. We use the output of success detectors in language form, which we refer to as Success feedback.</p>
<p>Passive Scene Description. While there are many ways to describe the semantics contained within a scene, we use the term Passive Scene Description to broadly describe sources of scene feedback that are consistently provided and follow some structure. Passive Scene Description covers all sources of environment grounding feedback that are automatically provided and injected into the LLM prompt without any active prompting or querying by the LLM planner. One common type of such feedback is object recognition [83,84,85,86] -we refer to the textual outputs of such object recognizers as Object feedback. We also demonstrate the use of a task-progress scene description in the simulated tabletop rearrangement environment, to which we refer as Scene feedback.</p>
<p>Active Scene Description. As the proactive counterpart to Passive Scene Description, Active Scene Description encompasses sources of feedback that are provided directly in response to active queries by the LLM planner. In this case, the LLM can directly ask a question about the scene, and this question can be answered either by a person, or by another pretrained model, such as a Visual Question Answering (VQA) model [87,88,89,90]. While the previous types of feedback are strictly structured and narrow in their scope, in the Active Scene Description setting the LLM can receive unstructured answers to open-ended questions, allowing it to actively gather information relevant to the scene, the task, or even preferences of the user (in the case of human-provided response). The combined output we send to the LLM planner includes both the LLM-generated question along with the response. As we aim to investigate whether and how a LLM planner can incorporate such feedback and wish to study both structured VQA-style human feedback as well as unstructured human preferences feedback, we only consider human-provided response in this work, which we refer to as Human feedback.</p>
<p>Experimental Results</p>
<p>In order to study how different sources of environment feedback can support a rich inner monologue that enables complex robotic control, we analyze diverse long-horizon manipulation and navigation tasks in simulation and in the real world. As Inner Monologue is not dependent on a specific LLM or a type of grounding feedback, we study different Inner Monologue implementations in three environments with different LLM planning methods and different sources of feedback from the environment. Below, we show results for a tabletop manipulation environment in sim (Sec 4.1) and real (Sec 4.2) as well as a mobile manipulation environment in real (Sec 4.3). For more details about the experiment setup and results, please refer to the Appendix.</p>
<p>Simulated Tabletop Rearrangement</p>
<p>We experiment with vision-based block manipulation tasks in a Ravens-based [75]   . Each domain uses different prompts and different feedback models. Sharing across the domains is the same Inner Monologue formulation that uses a pre-trained langauge model to take in a human instruction and decompose it into a sequence of actionable steps by the agent, while accounting for injected embodied feedback from different models, such as object recognizers and success detectors. In real-world kitchen mobile manipulation domain (bottom), we additionally ground the actions using pre-trained affordance functions built in [21], which do not communicate back to the language model. a number of blocks and bowls on a table, a robotic arm containing a gripper is tasked with rearranging these objects in some desired configuration, specified by natural language (e.g., "putting the blocks in the bowls with matching colors"). We evaluate each method on four seen tasks and four unseen tasks, where seen tasks may be used for training (in the case of supervised baseline) or used as few-shot prompting for LLM planner.</p>
<p>This instantiation of Inner Monologue uses (i) InstructGPT [9,91] as the LLM for multi-step planning [20,21], (ii) scripted modules to provide language feedback in the form of object recognition (Object), success detection (Success), and task-orogress scene description (Scene), and (iii) a pre-trained language-conditioned pick-and-place primitive (similar to CLIPort [76] and Transporter Nets [75]). Object feedback informs the LLM planner about the objects present in the scene, and the variant using only Object feedback is similar to the demonstrated example in [19] in this environment. Success feedback informs the planner about success/failure of the most recent action. However, in the presence of many objects and test-time disturbances, the complex combinatorial state space requires the planner to additionally reason about the overall task progress (e.g., if the goal is to stack multiple blocks, the unfinished tower of blocks may be knocked over by the robot). Therefore, task-progress scene description (Scene) describes the semantic sub-goals inferred by the LLM towards completing the high-level instruction that are achieved by the agent so far. For the variant that uses Object + Scene feedback, due to the additional reasoning complexity, we find that adding chain-of-thought [10,12,13] can improve the consistency between inferred goals and achieved goals.</p>
<p>We additionally compare to a multi-task CLIPort policy directly trained on long-horizon task instructions (i.e., without using LLM for planning). Because CLIPort is a single-step policy and does not terminate spontaneously during policy rollout, we report CLIPort evaluations with oracle termination (i.e., repeat until oracle indicates task completion) and fixed-step termination (i.e., repeat for k steps). While Inner Monologue terminates when the LLM stops generating new steps, we similarly set the maximum number of steps to be k for practical considerations. We use k =15. To simulate real-world disturbances and evaluate the system's robustness to disturbances, we add Gaussian noise to multiple levels of the system at test time: N (0,3) for pixel observation, N (0,2.5) for policy primitive (i.e., pick-place pixel heatmaps), N (0,0.02m) for place locations.</p>
<p>Results are shown in Table 1, and example prompt can be found in   Analysis. As shown in Table 1, all variants of Inner Monologue perform well on seen tasks, and by leveraging rich semantic knowledge in the pre-trained LLM, the performance can be directly translated to unseen tasks without further training. Furthermore, aligned with our hypothesis, Inner Monologue with Object + Scene performs the best because of its ability to keep track of all goal conditions and currently achieved goals. Finally, we observe that non-hierarchical and solitary systems such as CLIPort (i) struggle at generalizing to unseen long-horizon tasks under test-time disturbances, and (ii) on training tasks, an oracle is also often required to indicate task completion for good performance.</p>
<p>Real-World Tabletop Rearrangement</p>
<p>We evaluate Inner Monologue on a real-world robot platform designed to resemble the simulation experiments presented in Sec 4.1 using motion primitives for tabletop pick and place. The setup consists of a UR5e robot arm equipped with a wrist-mounted Intel RealSense RGB-D camera overlooking a workspace of diverse objects -from toy blocks to food items to condiments (shown in Fig 3). We use an instantiation of Inner Monologue using (i) InstructGPT [9,91] as the LLM for multi-step planning, (ii) a pretrained open-vocabulary object recognition with MDETR [92] to generate a list of currently visible objects and list of previously visible objects that are no longer visible (Object), (iii) heuristics on the object bounding box predictions from MDETR for Success Detection (Success), and (iv) a zero-shot pick and place policy that uses an LLM to parse target objects from a language command (e.g., given by the planner) and then executes scripted suction-based picking and placing primitives at the center of the target objects' bounding boxes. Aside from the pretraining of the LLM and MDETR (which are available out-of-the-box), the system does not require any model finetuning to perform pick and place tasks with new objects.</p>
<p>We investigate two tasks: (i) a simple 3-block stacking task where 2 blocks are already pre-stacked, and (ii) a more complex long-horizon sorting task to place food in one plate and condiments in another (where categorizing food versus condiments is autonomously done by the LLM planner). Since default pick and place performance is generally quite high on the system, we artificially inject Gaussian noise into the policy actions (i.e., add standard deviation σ=4mm clipped at 2σ) to stress test recovery from failures via replanning with grounded closed-loop feedback. Results are presented in Table 2. Note that the system is also subject to noisy object and success detections due to the additional challenge of real-world perception and clutter.   [19]). The partial 3-block stacking task highlights an immediate failure mode of this baseline, where the initial scene description struggles to capture a complete representation of the scene (due to clutter and occlusion) to provide as input to the multi-step planner. As a result, the system only executes one pick and place action -and cannot recover from mistakes. To address these shortcomings, Inner Monologue (Object + Success) leverages closed-loop scene description and success detection after each step, which allows it to successfully replan and recover from policy mistakes.</p>
<p>Additional ablations with Inner Monologue also show that (i) common failures induced by lack of closed-loop scene description are largely due to initially occluded objects not being part of the LLM generated plans, and (ii) failures induced by lack of success detection come from not retrying pick and place actions that have failed due to policy noise. Overall, we observe that both components are complementary and important in maintaining robust recovery modes for real rearrangement tasks.</p>
<p>Real-World Mobile Manipulator in a Kitchen Setting</p>
<p>We implement Inner Monologue in a robotic system using the kitchen environment and task definitions described in SayCan [21]. The Everyday Robots robot, a mobile manipulator with RGB observations, is placed in an office kitchen to interact with common objects using concurrent [93] continuous closed-loop control.</p>
<p>The baseline, SayCan [21], is a method that plans and acts in diverse real world scenarios by combining an LLM with value functions of underlying control policies. While SayCan creates plans that are grounded by the affordances of value functions, the LLM predictions in isolation are never given any closed-loop feedback.</p>
<p>We use an instantiation of Inner Monologue that uses (i) PALM [8] as the LLM for multi-step planning, (ii) value functions from pre-trained control policies for affordance grounding [21], (iii) a learned visual classification model for success detection feedback (Success), (iv) human-provided object recognition feedback (Object), and (v) pre-trained control policies for relevant skills in the scene. We also perform a case study where we allow the agent to ask questions and source Human feedback directly; results are shown in Fig 5a and the Appendix.</p>
<p>We evaluate methods on 120 evaluations over three task families: 1) four manipulation tasks, 2) two dexterous manipulation tasks utilizing drawers, and 3) two long-horizon combined manipulation and navigation tasks. In order to better study how Inner Monologue improves reasoning in especially challenging scenarios, we consider an experiment variant where we add adversarial disturbances during control policy executions that cause skill policy rollouts to fail. These disturbances may be fairly simple and just require the policy to try again, or it may complex enough that the policy needs to replan and select a completely new skill. While these failures occur naturally even without perturbances, the adversarial disturbances creates a consistent comparison between methods that requires retrying or replanning to accomplish the original instruction.   Analysis. The results of real robot experiments are shown in Table 3. Without adversarial disturbances, the baseline method SayCan performs reasonably on all three task families, yet incorporating LLM-informed feedback on skill success/failure and presence of objects allows the Inner Monologue to effectively retry or replan under natural failures, providing further improvement to the baseline. The most notable difference is in the cases with adversarial disturbances when a policy failure is forced to occur. Without any LLM-informed feedback SayCan has success rate close to 0% since it is does not have explicitly high-level retry behavior. Inner Monologue significantly outperforms SayCan because of its ability to invoke appropriate recovery  </p>
<p>Emergent Capabilities</p>
<p>Although LLMs can generate fluent continuation from the prompted examples, we surprisingly find that, when informed with environment feedback, Inner Monologue demonstrates many impressive reasoning and replanning behaviors beyond the examples given in the prompt. Using a pre-trained LLM as the backbone, the method also inherits many of the appealing properties from its versatility and general-purpose language understanding. In this section, we demonstrate a few of these emergent capabilities.</p>
<p>Continued Adaptation to New Instructions. Although not explicitly prompted, the LLM planner can react to human interaction that changes the high-level goal mid-task. Fig 5a demonstrates a challenging case, where Human feedback changes the goal during the plan execution, and then changes the goal yet again by saying "finish the previous task". We can see that the planner incorporates the feedback correctly by switching tasks twice. In another instance, despite not being explicitly prompted to terminate after a human says "please stop", the LLM planner generalizes to this scenario and predicts a "done" action.</p>
<p>Self-Proposing Goals under Infeasibility. Instead of mindlessly following human-given instructions, Inner Monologue can also act as an interactive problem solver by proposing alternative goals to achieve when the previous goal becomes infeasible. In Fig 5b, to solve the task "put any two blocks inside the purple bowl", Inner Monologue first attempts an action of picking up the purple block -the action fails as the purple block is intentionally made to be too heavy for the robot. After a hint "the purple block is too heavy", it proposes to "find a lighter block" and successfully solves the task in the end.</p>
<p>Multilingual Interaction. Pre-trained LLMs are known to be able to translate from one language to another, without any finetuning. We observe that such multilingual understanding also transfers to the embodied settings considered in this work. Specifically, in Fig 5c, the human-provided new instruction is written in Chinese, but the LLM can correctly interpret it, re-narrate it as a concrete goal to execute in English, and accordingly replan its future actions. Occasionally, we find that this capability even extends to symbols and emojis.</p>
<p>Interactive Scene Understanding. We also observe that Inner Monologue demonstrates interactive understanding of the scene using the past actions and environment feedback as context. In Fig 5d, after a task instruction has been executed, we turn to ask questions about the scene, again a structure that has not appeared in the prompt. Surprisingly, we find that it can often correctly answer these questions that require temporal and embodied reasoning.</p>
<p>Robustness to Feedback Order. In the main experiments of the paper, we prompted the language model following certain conventions. For instance, in the simulated tabletop domain, the convention is [Robot action, Scene, and Robot thought]. In practice, we find that the LLM planner is robust to occasionally swapping the order of feedback. In Appendix Fig 9a, a new human instruction is injected in the middle of the plan execution, but this structure has not been seen in the example prompts. Yet the planner recognizes the change and generates a new "Robot thought: Goal state is. . . " statement allowing it to solve the new task.</p>
<p>Robustness to Typos. Inherited from the LLM backbone, our approach is robust to typos in human instruction, as seen in Appendix Fig 9b. Despite the appealing findings about these emergent capabilities, we observe that they are of varying levels of consistency when no similar examples have been provided in the prompt, likely limited by the current capabilities of the language models. However, we believe that further investigations into these behaviors and addressing their limitations would each lead to exciting future directions.</p>
<p>Limitations</p>
<p>Limiting assumptions and failure modes. In Sec 4.1 and Sec 4.3, we assume access to oracle scene descriptors in the form of human observers or scripted systems to provide textual description back to the LLM planner. We study the viability of learned systems scene desription and object recognition in Appendix Table 5. As for failure modes, Inner Monologue may fail due to several sources of errors: (1) success detections, (2) LLM planning errors, and (3) control errors. False negative predictions from the success detector lead to additional retry attempts, while false positive predictions add adversarial partial observability to the environment. In some instances, we found that the LLM planners ignored the environment feedback and still proposed policy skills involving objects not present in the scene.</p>
<p>Limitations of results and future work. The performance of low-level control policies limits not only overall high-level instruction completion performance, but also limits the scope of tasks that the LLM is able to reason over: no matter how much the LLM reasoning improves, it can still be bottlenecked by what low-level control policies are able to achieve. Future work remains open on several fronts. First, with advances in image/video captioning and visual-question answering, a fully automated system of Inner Monologue can be implemented without a human in the loop as an oracle. Second, improvements can be made on how to aggregate potentially inaccurate sources of information, such as using text to describe the uncertainty of the feedback modules, or including additional feedback modules for safety and ethics for the proposed plans.</p>
<p>Conclusion</p>
<p>In this work, we investigated the role that environment feedback plays for LLMs reasoning in tasks involving embodied robotic planning and interaction. We presented a general formulation Inner Monologue that combines different sources of environment feedback with methods fusing LLM planning with robotic control policies. Then, we studied how these methods scaled to three different robotic manipulation settings in simulation and the real world. We found that environment feedback significantly improves high-level instruction completion, especially in challenging scenarios with adversarial disturbances. Finally, we analyze emergent capabilities of Inner Monologue that highlight how closed-loop language feedback enables replanning even in complex unseen settings.</p>
<p>A Inner Monologue Implementation Details</p>
<p>We study three different implementations of Inner Monologue for each of the experimental settings. While each version incorporates textual environment feedback into planning, there are differences in the internal components of each system, as seen in Table 4.  Low-level Policies We use a CLIP-based [65] Transporter Net [75] (inspired by CLIPort [76]) as the pick-and-place primitive that can be invoked by the LLM planner. The policy is trained on 20000 pre-collected demonstrations, where each demonstration contains 1) language instruction of the format "pick up [x] and place it on [y]", 2) top-down view of RGB-D observation of the current environment, 3) expert pick coordinates, and 4) expert place coordinates. The expert pick and place coordinates are obtained by accessing ground-truth object pose in the simulator. Unlike the evaluated settings in [76], the demonstrations cover all objects because the focus of this work is not on generalization across object instances but on novel long-horizon behaviors. The policy outputs a 3D pick location and a 3D place location, and a scripted pick-and-place motion is executed following the predicted locations.</p>
<p>Environment Feedback: Object Recognition</p>
<p>We provide the list of objects present in the scene at the start of each episode for the language model (without bounding boxes or coordinates because spatial manipulation is handled by low-level policies). Although it has been shown in [19] that this can be obtained by off-the-shelf pre-trained open-vocabulary object detection models such as ViLD [77] and MDETR [92], we obtain the list of objects by accessing simulator state for simplicity.</p>
<p>Environment Feedback: Success Detection For Object + Success method, we provide textual feedback of low-level policy success detection results after each policy execution. The success detector is built using heuristics based on ground-truth object poses: a pick-and-place execution is successful if picked object has a 2D Euclidean distance &lt;4 cm from the place object and its height is greater than the place object. If the place object is a location (e.g., top side of the table) rather than a block or a bowl, the height requirement is not enforced.</p>
<p>Environment Feedback: Passive Scene Description For Object + Scene method, we provide task-progress scene description as a list of achieved sub-goals after each pick-and-place execution. At the start of the action plan, the language model first generates a list of desired sub-goals given the high-level instruction. The sub-goals are of the format "[x] is on top of [y]". After each pick-and-place execution, we iterate through all desired sub-goals inferred by the language model, and check which are satisfied using the success detector described above. The full list of currently satisfied sub-goals are appended to the language model prompt. Additionally, we allow the language model to generate chain-of-thought summarization following the achieved sub-goals (i.e., "Robot thought: ..."), which we find to be useful empirically.</p>
<p>A.2 Inner Monologue for Real-World Tabletop Rearrangement</p>
<p>Large Language Model We use InstructGPT [91], a 1.3B parameter language model fine-tuned from GPT-3 [9] with human feedback, accessed through OpenAI API.</p>
<p>Low-level Policies</p>
<p>We use a single low-level policy for the real tabletop rearrangement environment that is responsible for performing object-centric pick and place actions as instructed by the language model. The policy takes as input 1) the bounding boxes of all the objects in the scene, 2) the names of the object to pick and the target object for place, 3) the depth image corresponding to the RGB image from which object detection was done, 4) camera intrinsics, and 5) camera pose. It then outputs a 3D pick position and a 3D place location for the robot to follow.</p>
<p>The pick position is the 3D coordinate of the center of the object bounding box in the robot base frame. This is obtained by first deprojecting the depth image pixel corresponding to the center of the pick object bounding box with the given camera intrinsics, then transforming this point from the camera frame to the robot frame with the given camera pose. Camera pose is known because it is a wrist camera, and we register the delta pose from the mounted camera to the robot end-effector.</p>
<p>For the block stacking task, the place location is obtained the same way as the pick position. For the object sorting task, the place location is chosen to be a point in the target plate's bounding box that is farthest way from the bounding boxes of other objects. This is done to prevent undesired stacking behavior when placing objects into the plate, which may cause the object to roll off and fall off the table.</p>
<p>To perform the pick and place motions, the robot moves to a position 15 cm above the intended pick or place position, and then it slowly lowers the end-effector until a 5 N contact force is detected. This is more robust than moving directly to the height obtained from the depth image due to depth sensor noises. We set a threshold for how low the end-effector can go for the object sorting task so picking inaccuracies do not result in accidentally picking up the plate.</p>
<p>We add planar translation noise to the pick position by sampling the magnitude from a zero-mean Normal distribution and then sampling an angle from a uniform distribution on the unit circle. The standard deviation for the Normal distribution is σ =1.5 cm for the block stacking task, σ =0.7 cm for the object sorting task, and the samples are capped at 1.5σ.</p>
<p>Object Recognition Implementation Object detection is done by MDETR [92], an open-vocabulary object detection model. It takes as input an RGB image with an image caption, and the model tries to identify the objects described in the caption. To form the caption for our tasks, we join the list of all possible object names together, separated by periods. Note that not all provided objects will be detected due to occlusions. MDETR detections are not always robust or consistent. For example, the detected object bounding box may some times be bigger or smaller than the object, or its center may be far away from the object center. Its performance also drops when objects are in close clutter. These issues with perception further motivate the need for success detection and multi-step environment feedback, so the high-level language model planner can be robust to these inaccuracies.</p>
<p>Environment Feedback: Success Detection In the real tabletop rearrangement environment, success detection is done by comparing the 2D position of the center of the detected object bounding box after pick and place action with the intended 2D place position. By 2D we mean the x-y spatial coordinates in the base frame of the robot, not the image coordinates. These are obtained by deprojecting the detected bounding box centers from the depth image and converting the resulting 3D point from camera frame to base frame. The detector returns success if the difference between the two positions is less than a threshold. For the block stacking task, the threshold is 3 cm, and for the object sorting task it is 10 cm. While 3 cm is larger than half of the dimension of the blocks we use (4 cm), it is more robust for our case because the center of the detected object bounding box is often not the center of the block. The threshold for object sorting is much higher, because for some objects, once they are placed in the plate, they may roll around until contact with the plate boundary.</p>
<p>Environment Feedback: Object Recognition For the block stacking task, the scene description contains a list of currently visible objects and a list of previously visible objects that are no longer visible. We may add new objects to the currently visible objects list if, after a robot action, MDETR detects an object that we have not seen before. This happens for the block stacking task as there is an initial partially constructed tower of two blocks, the bottom of which is initially occluded.</p>
<p>For the object sorting task, the scene description contains a list of currently visible objects and a list of objects that the robot has successfully moved into a plate. Like the block stacking task, some objects are initially occluded, so later actions may reveal them and they will be added to the visible objects list. The successfully moved object list is needed so that the planner does not stop before task completion and that it stops on task completion.</p>
<p>A.3 Inner Monologue for Real-World Mobile Manipulation in a Kitchen Setting</p>
<p>Large Language Model We use PALM [8], a 540B parameter language model trained on a large datasets that include high-quality web documents, books, Wikipedia, conversations, and GitHub code.</p>
<p>Low-level Policies Following the implementation described in SayCan [21], we use a combination of learned and scripted control policies for navigation and manipulation. The learned manipulation policies responsible for counter picking, drawer opening and closing, drawer picking, and countertop object manipulation are Behavior Cloning (BC) policies trained on 68000 teleoperated demonstrations and 12000 autonomous successes that were collected over the course of 11 months using a fleet of 10 robots. The teleoperators use VR headset controllers to track the motion of their hand, which is then mapped onto the robot's end-effector pose. The scripted navigation policies utilize a ground-truth map along with a learned perception module to navigate between different points in the environment. The scripted manipulation policy is solely responsible for countertop placing when preceded by a navigation policy, and follows pre-computed motions. The Value Functions used by SayCan for affordance grounding are provided by the Q-networks of trained RL agents; we follow the RL training setup described in [21]. While we utilize human-provided object recognition to gauge the role of maximally accurate Object feedback, we also study the feasibility of using learned object recognition models to provide fully automated Object feedback. We find that two such models, ViLD [77] and MDETR [92], perform worse than humans but still quite resonably at providing Object feedback, even without domain-specific data for fine-tuning. As advances in computer vision improve object detection models that can transfer zero-shot or few-shot to novel environments like our kitchen environment, we expect that fully automated Object feedback will increasingly approach human-provided accuracy.  Table 5: Comparison of ViLD [77] and MDETR [92], two open-vocabulary object detection models. We evaluate these two pretrained models on 10 representative kitchen mobile manipulation episodes. ViLD has strong overall accuracy, but still fails to detect objects 28.0% of the time. Figure 6: An example of ViLD object detection segmentation mask and bounding box predictions.</p>
<p>Environment Feedback: Success Detection We trained a "foresight" success detector on offline data collected from a combination of teleoperated demonstrations and autonomous roll-outs performing the low-level tasks. The input to the model consists of: (1) o 0 , the initial image observation, (2) o f , the final image observation after the policy chose to terminate its chosen skill execution, and (3) l k , the low-level skill which the agent was trying to execute (e.g., "Pick coke can"). The model uses image encoders from CLIP [65] to embed o 0 and o f , concatenate them, and fuse these representations with a fusion MLP. This image embedding is concatenated with the text embedding obtained by the CLIP text encoder, then passed through another MLP. The output of the model is a scalar denoting the probability of the agent succeeding at the specified task (Fig 7a). The model is trained with the binary cross entropy loss with respect to the ground truth binary label. We fine tune the CLIP model as well during the training. At inference time within Inner Monologue, we output the text "[success: no]" when the probability is below a certain threshold.</p>
<p>To reduce false positive predictions, we trained a separate "hindsight" success predictor model. Given the first and last observation, the model outputs a probability distribution over all the possible skills. We used this model to disambiguate between possible tasks that the agent may have achieved. To train this model, we use the symmetric contrastive loss as used in CLIP (Fig 7b). At inference time, similar to the CLIP model, we take a dot product between the image-fusion embedding and the text embeddings and apply softmax with a learned temperature.</p>
<p>We combine the foresight and hindsight model by first thresholding the probability from the foresight model by some value τ. If the probability is above, then we run the hindsight model and only predict success if the argmax across all skills is indeed the foresight task.  Environment Feedback: Active Scene Description We perform a case study where we allow the LLM agent to ask questions and source Human feedback directly. After each LLM planning step, the LLM chooses between and continue or and ask: for the and continue option the LLM proceeds to the planning the next action to execute, and for the and ask option the LLM proceeds to propose a question to ask the human. The questions proposed by the LLM do not follow any pre-determined format or scope, so human-provided answers are similarly open-ended. These answers can range from confirming factual observations about the scene to providing refinements or changes to the high-level plan. Some examples can be found in Sec D.</p>
<p>B Experiment Details</p>
<p>B.1 Simulated Tabletop Rearrangement Environment</p>
<p>There are a total of 8 tasks, listed in Table 1. For all tasks, up to 4 blocks and up to 3 bowls are randomly initialized with different colors (chosen from 10 different colors) and initial locations. A 15 cm distance is enforced between any two objects to avoid collision and penetration at initialization. For tasks that involve a specific object instance or location in the task instruction (e.g., "Put all the blocks in the [x] bowl"), a random task instruction is sampled using the available objects in the scene or all available locations. There are a total of a total of 9 locations (e.g., "top side", "bottom right corner", etc). To allow for automatic evaluations, a binary reward function is defined for each task using ground-truth state of the objects.</p>
<p>CLIPort Baseline</p>
<p>We additionally train a multi-task CLIPort [76] policy on 4 of the total 8 tasks as an additional baseline (see Table 1 for train/test split). The goal is to evaluate whether solitary language-conditioned policies can perform well on long-horizon tasks and generalize to new task instructions. Note that this is a different policy as the low-level primitive used in Inner Monologue, though sharing the same architecture. The policy also shares the same training details as the low-level policy used in Inner Monologue but is trained on 20000 demonstrations across 4 training tasks.</p>
<p>B.2 Real Tabletop Rearrangement</p>
<p>There are two tasks in the real tabletop rearrangement domain -a block stacking task and an object sorting task.</p>
<p>In the block stacking task, the initial state consists of three 4 cm cubes, two of which have been already stacked. The robot needs to complete stacking the full tower either by putting the third block onto the partially stacked two, or by putting the two partially stacked blocks onto the third block. Due to occlusions, the robot only perceives two blocks in the beginning, so without scene description feedback, the planner has a 50% chance of completing the task.</p>
<p>For the object sorting task, the initial state consists of three fruits, three bottles, and three plates. The fruits and bottles are placed in a clutter in one of the three plates, and the robot is asked to sort them by putting fruits on one of the remaining plates, and bottles on the other. Due to occlusions and reduced object detection performance in clutter, some objects may not be detected in the initial clutter, so scene description feedback is still needed. The three bottles used are a ketchup bottle, a mustard bottle, and a soft drink bottle. The three fruits used are an apple, a strawberry, and a plum. Note these are toy objects made out of plastic, so they are smaller and lighter than their real-world counterparts, and they are also rigid, allowing for robust suction grasps. They have realistic visual appearances, which allow MDETR detections.</p>
<p>B.3 Real Kitchen Mobile Manipulation</p>
<p>We follow the environment and open-ended task setup described in SayCan [21]. An Everyday Robots robot, a mobile manipulator robot with RGB observations, is placed in a kitchen with multiple named locations that contain a set of 15 household items. Given an unstructured human instruction, the robot must complete the instruction successfully by executing a set of navigation and manipulation skills, which are vision-based concurrent [93] continuous closed-loop policies. Fig 8 presents a high-level overview of the environment. The full list of tasks are shown in Table 6. </p>
<p>Task Instructions</p>
<p>"pick up a soda" Manipulation "pick up the jalapeno or kettle chips" "pick up a snack" "pick up a caffeinated drink"</p>
<p>Mobile Manipulation "can you throw away the soda on the table" "I spilled my coke, can you bring me something to clean it up"</p>
<p>Drawer Manipulation "leave the top drawer open" "put a coke in the top drawer" Table 6: List of all instructions in Kitchen Mobile Manipulation tasks. We evaluate the algorithm on 3 families of instructions on a mock kitchen scene.</p>
<p>C Additional Results</p>
<p>We show additional results of emergent capability in Fig 9. Please refer to Sec 4.4 for context.  Figure 9: Additional emergent capabilities in Inner Monologue. While we mostly use a fixed feedback order for each domain, we find that Inner Monologue can seamlessly handle different feedback injected into the prompt, as opposed to the normal case of feedback being provided after the prompt and after planning steps (left). Additionally, it is also robust to typos in the instruction, an ability likely inherited from the backbone LLM (right). </p>
<p>Figure 2 :
2Various types of textual feedback. Success Detection gives task-specific task completion information, Passive Scene Description gives structured semantic scene information at every planning step, and Active Scene Description gives unstructured semantic information only when queried by the LLM planner.which describes the scene. Specific instantiations and implementation details of each type of feedback can be found in Sec 4.1, Sec 4.2, and Sec 4.3 respectively for each domain.</p>
<p>Figure 3 :
3Different instantiations of Inner Monologue in three distinct domains -simulated tabletop rearrangement (top), real-world tabletop rearrangement (middle), and real-world kitchen mobile manipulation (bottom)</p>
<p>Fig 3 .
3</p>
<p>+LLM</p>
<p>Figure 4 :
4Failure causes on 120 evaluations. When disturbances are added (red), only the Inner Monologue variants consistently complete the instructions.</p>
<p>Figure 5 :
5Informing LLM with embodied feedback enables many emergent capabilities, all of which are achieved without similar prompted examples. For instance, Inner Monologue can continually adapt to new instructions given by humans, propose new goals to achieve when faced with infeasibility for the previous plan, interact with humans in different natural languages, and answer questions about the current scene given past actions and feedback.modes depending on the environment feedback. In-depth analysis on the failure causes indicates that Success and Object feedback can effectively reduce LLM planning failures and thus overall failure rate, albeit at the cost of introducing new failure modes to the system.</p>
<p>( a )
aForesight success detector (b) Hindsight success detector</p>
<p>Figure 7 :
7Success Detection architecture used for the Kitchen Environment. (Left) The foresight model predicts whether a given instruction was successfully achieved between the first and last image. (Right) The hindsight model fine-tuned via contrastive loss as used in CLIP [65]. At inference time, the model is used infer among the possible instructions which one was achieved.</p>
<p>Figure 8 :
8The experiments were performed in a mock office kitchen mirroring this setup (a), with 5 predicted locations and 15 objects (b). The robot is a mobile manipulator with a 7 degree-of-freedom arm and a gripper with policies trained from an RGB observation (c).</p>
<p>simulation environment to evaluate our method against several baselines and ablate across varying amounts of textual feedback. Given Human: move all the blocks into mismatching bowls. Scene: There is a yellow block, yellow bowl, blue block, blue bowl, red block, red bowl. Robot: My goal is ['yellow block in blue bowl', 'red block in yellow bowl', 'blue block in red bowl']. Robot: Pick up yellow block and place it in blue bowl. Scene: You achieved ['yellow block in blue bowl'] Robot: I need red block in yellow bowl, blue block in red bowl Robot: Pick up red block and place it in yellow bowl.Current Color Scheme (feel free to change it): 
scene descriptor, unimportant prompt, success detector, 
human, chain-of-thought/robot-question, robot action </p>
<p>Task-Progress 
Scene Description </p>
<p>Achieved </p>
<p>Achieved </p>
<p>Success Detection 
Object Recognition </p>
<p>Human: Sort fruits and condiments in different plates 
Scene: objects = [apple, egg, plum, pizza, cookie, salt, 
mustard, ketchup, lemon] 
Scene: occluded objects = [] 
Robot: robot.pickplace(strawberry, green plate) 
Robot: robot.pickplace(salt shaker, blue plate) 
Success Detector: False. 
Robot: robot.pickplace(salt shaker, blue plate) 
... 
Robot: robot.stop() 
Success Detection </p>
<p>Object Recognition 
(w/ Potential Occlusion) </p>
<p>Action: put 
the egg on 
the blue plate </p>
<p>True 
False </p>
<p>Initial </p>
<p>Action: put 
the yellow 
block on 
the blue 
bowl </p>
<p>True 
False </p>
<p>Initial </p>
<p>Human: Bring me a drink from the table 
Robot: go to the table 
Scene: I see a coke and lime soda 
Robot: What kind of drink would you like? 
Human: Something with caffeine 
Robot: Pick up coke 
Success Detector: False 
Robot: Pick up coke 
Robot: Bring it to you 
Robot: Put down the coke 
Robot: Done 
Success Detection 
Object Recognition </p>
<p>Action: 
pick up 
the coke </p>
<p>True 
False </p>
<p>Initial </p>
<p>Large Language 
Model </p>
<p>… </p>
<p>Pick up water </p>
<p>Pick up chocolate bar </p>
<p>Pick up coke </p>
<p>Open drawer </p>
<p>Affordance 
Functions </p>
<p>Affordance Grounding </p>
<p>Tabletop 
Rearrangement (Sim) </p>
<p>Tabletop 
Rearrangement (Real) </p>
<p>Kitchen Mobile 
Manipulation (Real) </p>
<p>Table 1 :
1Success rates for various methods, averaged across 50 episodes in Ravens-based environment with test-time disturbances. CLIPort + oracle indicates that CLIPort was provided a "termination" oracle. Although CLIPort can receive visual feedback from the environment, we show that LLM-informed feedback can effectively enable the planner to retry/replan in the presence of failures, while enjoying the generalization benefits of LLMs to unseen tasks.+Inner Monologue </p>
<p>Table 2 :
2Inner Monologue (with object recognition and success detection feedback) on a real pick and place robot exceeds the performance of baseline alternatives, as measured by average task success rates over 10 runs. For both tasks, we observe significant improvement in Inner Monologue compared to the baseline, with the two embodied feedback being complementary to each other.Analysis. We compare different variants of Inner Monologue with different LLM-informed closed-loop feedback, as well as an open-loop variant that only runs object recognition once at the beginning of the task (similar to the system demonstrated in</p>
<p>Table 3 :
3Averaged success rate across 120 evaluations on several task families in our real-world mobile manipulation environment. We consider a standard setting and adversarial setting with external human disturbances. In all cases, LLM-informed embodied feedback is shown to be effective in improving robustness of the system, especially when low-level policies are prone to failures.Disturbance </p>
<p>SayCan </p>
<p>IM 
w/ Success </p>
<p>IM 
w/ Success 
+ Object </p>
<p>SayCan </p>
<p>IM 
w/ Success </p>
<p>IM 
w/ Success 
+ Object </p>
<p>Table 4 :
4Comparison between different versions of Inner Monologue implemented in three different environments.A.1 Inner Monologue for Simulated Tabletop Rearrangement </p>
<p>Large Language Model We use InstructGPT [91], a 1.3B parameter language model fine-tuned from 
GPT-3 [9] with human feedback, accessed through OpenAI API. </p>
<p>Scene: There is a red bowl, yellow bowl, blue bowl, red block, yellow block, blue block. Task: Put all the blocks in bowls with matching colors. … Robot action: Pick the yellow block and place it in the yellow bowl. Scene: Completed ['Red block in red bowl.', 'Yellow block in yellow bowl.'] Robot thought: Blue block is not in a bowl with a matching color. Human: I changed my mind. Can you put all the blocks in the red bowl? Robot thought: Goal state is ["Red block in red bowl.", "Yellow block in red bowl.", "Blue block in red bowl."] Robot action: Pick the blue block and place it in the red bowl. Scene: Completed ['Red block in red bowl.', 'Blue block in red bowl.'] Robot thought: Yellow block is not in the red bowl. … Current Color Scheme (feel free to change it): scene descriptor, unimportant prompt, success detector, human, chain-of-thought/robot-question, robot action Replan (a) Robustness to Feedback Order Scene: There is a yellow block, blue block, yellow bowl, blue bowl on the table.Human: Put the blocks in the bowls with mismatching colors. Robot thought: Goal state is ["Yellow block is in the blue bowl.", "Blue block is in the yellow bowl."] Robot action: Pick the yellow block and place it in the blue bowl. Scene: Completed ["Yellow block is in the blue bowl."] Robot thought: Blue block is not in the yellow bowl. Human: Actully can you put the bloks in the maching bwls? Robot thought: Goal state is ["Yellow block is in the yellow bowl.", "Blue block is in the blue bowl."] Robot action: Pick the yellow block and place it in the yellow bowl. Scene: Completed ["Yellow block is in the yellow bowl."] Robot thought: Blue block is not in the blue bowl. Robot action: Pick the blue block and place it in the blue bowl. Scene: Completed ["Yellow block is in the yellow bowl.", "Blue block is in the blue bowl."] Robot thought: I'm done.Current Color Scheme (feel 
free to change it): 
scene descriptor, unimportant 
prompt, success detector, 
human, 
chain-of-thought/robot-questio 
n, robot action </p>
<p>(b) Robustness to Typos </p>
<p>D Prompts
PromptsListing 1: Prompt in Inner Monologue on Simulated Tabletop Rearrangement =============== Scene: There is a cyan, yellow, brown block. Human: Move all the blocks to the top left corner. Robot thought: Goal state is ["Cyan block is on the top left corner.", "Yellow block is on the top left corner.", "Brown block is on the top left corner."]
AcknowledgmentsThe authors would like to thank Kanishka Rao and Vincent Vanhoucke for valuable feedback and discussions. In addition, the authors would like to acknowledge the large team who built[21], upon which we construct our Kitchen Mobile Manipulation experiments.Successful action: TrueScene: Visible objects are ["red marker", "red pen", "blue box", "green bowl"] Scene:Occluded
Integrated task and motion planning in belief space. L P Kaelbling, T Lozano-Pérez, The International Journal of Robotics Research. 329L. P. Kaelbling and T. Lozano-Pérez. Integrated task and motion planning in belief space. The International Journal of Robotics Research, 32(9-10):1194-1227, 2013.</p>
<p>Recent advances in hierarchical reinforcement learning. Discrete event dynamic systems. A G Barto, S Mahadevan, 13A. G. Barto and S. Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete event dynamic systems, 13(1):41-77, 2003.</p>
<p>F Petroni, T Rocktäschel, P Lewis, A Bakhtin, Y Wu, A H Miller, S Riedel, arXiv:1909.01066Language models as knowledge bases. arXiv preprintF. Petroni, T. Rocktäschel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller, and S. Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.</p>
<p>How can we know what language models know?. Z Jiang, F F Xu, J Araki, G Neubig, Transactions of the Association for Computational Linguistics. 8Z. Jiang, F. F. Xu, J. Araki, and G. Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438, 2020.</p>
<p>Commonsense knowledge mining from pretrained models. J Davison, J Feldman, A M Rush, Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)J. Davison, J. Feldman, and A. M. Rush. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th in- ternational joint conference on natural language processing (EMNLP-IJCNLP), pages 1173-1178, 2019.</p>
<p>olmpics-on what language model pre-training captures. A Talmor, Y Elazar, Y Goldberg, J Berant, Transactions of the Association for Computational Linguistics. 8A. Talmor, Y. Elazar, Y. Goldberg, and J. Berant. olmpics-on what language model pre-training captures. Transactions of the Association for Computational Linguistics, 8:743-758, 2020.</p>
<p>How much knowledge can you pack into the parameters of a language model. A Roberts, C Raffel, N Shazeer, arXiv:2002.08910arXiv preprintA. Roberts, C. Raffel, and N. Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:2002.08910, 2020.</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. arXiv preprintA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 33T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Chain of thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, E Chi, Q Le, D Zhou, arXiv:2201.11903arXiv preprintJ. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, arXiv:2205.11916arXiv preprintT. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Can language models learn from explanations in context?. A K Lampinen, I Dasgupta, S C Chan, K Matthewson, M H Tessler, A Creswell, J L Mcclelland, J X Wang, F Hill, arXiv:2204.02329arXiv preprintA. K. Lampinen, I. Dasgupta, S. C. Chan, K. Matthewson, M. H. Tessler, A. Creswell, J. L. McClelland, J. X. Wang, and F. Hill. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022.</p>
<p>M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. arXiv preprintM. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>Thought and language. L S Vygotsky, MIT pressL. S. Vygotsky. Thought and language. MIT press, 2012.</p>
<p>Thinking in language?: evolution and a modularist possibility. P Carruthers, Cambridge University PressP. Carruthers. Thinking in language?: evolution and a modularist possibility. Cambridge University Press, 1998.</p>
<p>Tool and symbol in child development. The vygotsky reader. L Vygotsky, L. Vygotsky. Tool and symbol in child development. The vygotsky reader, 1994.</p>
<p>Play and its role in the mental development of the child. L S Vygotsky, Soviet psychology. 53L. S. Vygotsky. Play and its role in the mental development of the child. Soviet psychology, 5(3): 6-18, 1967.</p>
<p>Vygotskian autotelic artificial intelligence: Language and culture internalization for human-like ai. C Colas, T Karch, C Moulin-Frier, P.-Y Oudeyer, arXiv:2206.01134arXiv preprintC. Colas, T. Karch, C. Moulin-Frier, and P.-Y. Oudeyer. Vygotskian autotelic artificial intelligence: Language and culture internalization for human-like ai. arXiv preprint arXiv:2206.01134, 2022.</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. A Zeng, A Wong, S Welker, K Choromanski, F Tombari, A Purohit, M Ryoo, V Sindhwani, J Lee, V Vanhoucke, arXiv:2204.00598arXiv preprintA. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, PMLRInternational Conference on Machine Learning. W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning. PMLR, 2022.</p>
<p>Do as i can and not as i say: Grounding language in robotic affordances. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, D Ho, J Hsu, J Ibarz, B Ichter, A Irpan, E Jang, R J Ruano, K Jeffrey, S Jesmonth, N Joshi, R Julian, D Kalashnikov, Y Kuang, K.-H Lee, S Levine, Y Lu, L Luu, C Parada, P Pastor, J Quiambao, K Rao, J Rettinghouse, D Reyes, P Sermanet, N Sievers, C Tan, A Toshev, V Vanhoucke, F Xia, T Xiao, P Xu, S Xu, M Yan, arXiv:2204.01691In arXiv preprintM. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, and M. Yan. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691, 2022.</p>
<p>Hierarchical planning in the now. L P Kaelbling, T Lozano-Pérez, Workshops at the Twenty-Fourth AAAI Conference on Artificial Intelligence. L. P. Kaelbling and T. Lozano-Pérez. Hierarchical planning in the now. In Workshops at the Twenty-Fourth AAAI Conference on Artificial Intelligence, 2010.</p>
<p>Combined task and motion planning through an extensible planner-independent interface layer. S Srivastava, E Fang, L Riano, R Chitnis, S Russell, P Abbeel, 2014 IEEE international conference on robotics and automation (ICRA). S. Srivastava, E. Fang, L. Riano, R. Chitnis, S. Russell, and P. Abbeel. Combined task and motion planning through an extensible planner-independent interface layer. In 2014 IEEE international conference on robotics and automation (ICRA), 2014.</p>
<p>Strips: A new approach to the application of theorem proving to problem solving. R E Fikes, N J Nilsson, Artificial intelligence. R. E. Fikes and N. J. Nilsson. Strips: A new approach to the application of theorem proving to problem solving. Artificial intelligence, 1971.</p>
<p>A structure for plans and behavior. E D Sacerdoti, SRI International. Menlo Park California Artificial Intelligence CenterTechnical reportE. D. Sacerdoti. A structure for plans and behavior. Technical report, SRI International, Menlo Park California Artificial Intelligence Center, 1975.</p>
<p>Shop: Simple hierarchical ordered planner. D Nau, Y Cao, A Lotem, H Munoz-Avila, Proceedings of the 16th international joint conference on Artificial intelligence. the 16th international joint conference on Artificial intelligenceD. Nau, Y. Cao, A. Lotem, and H. Munoz-Avila. Shop: Simple hierarchical ordered planner. In Proceedings of the 16th international joint conference on Artificial intelligence, 1999.</p>
<p>Planning algorithms. S M Lavalle, Cambridge university pressS. M. LaValle. Planning algorithms. Cambridge university press, 2006.</p>
<p>Logic-geometric programming: An optimization-based approach to combined task and motion planning. M Toussaint, Twenty-Fourth International Joint Conference on Artificial Intelligence. M. Toussaint. Logic-geometric programming: An optimization-based approach to combined task and motion planning. In Twenty-Fourth International Joint Conference on Artificial Intelligence, 2015.</p>
<p>Differentiable physics and stable modes for tool-use and manipulation planning. M A Toussaint, K R Allen, K A Smith, J B Tenenbaum, Robotics: Science and Systems Foundation. M. A. Toussaint, K. R. Allen, K. A. Smith, and J. B. Tenenbaum. Differentiable physics and stable modes for tool-use and manipulation planning. Robotics: Science and Systems Foundation, 2018.</p>
<p>Search on the replay buffer: Bridging planning and reinforcement learning. B Eysenbach, R R Salakhutdinov, S Levine, Advances in Neural Information Processing Systems. B. Eysenbach, R. R. Salakhutdinov, and S. Levine. Search on the replay buffer: Bridging planning and reinforcement learning. Advances in Neural Information Processing Systems, 2019.</p>
<p>Neural task programming: Learning to generalize across hierarchical tasks. D Xu, S Nair, Y Zhu, J Gao, A Garg, L Fei-Fei, S Savarese, 2018 IEEE International Conference on Robotics and Automation (ICRA). D. Xu, S. Nair, Y. Zhu, J. Gao, A. Garg, L. Fei-Fei, and S. Savarese. Neural task programming: Learning to generalize across hierarchical tasks. In 2018 IEEE International Conference on Robotics and Automation (ICRA), 2018.</p>
<p>Regression planning networks. D Xu, R Martín-Martín, D.-A Huang, Y Zhu, S Savarese, L F Fei-Fei, Advances in Neural Information Processing Systems. 32D. Xu, R. Martín-Martín, D.-A. Huang, Y. Zhu, S. Savarese, and L. F. Fei-Fei. Regression planning networks. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Inventing relational state and action abstractions for effective and efficient bilevel planning. T Silver, R Chitnis, N Kumar, W Mcclinton, T Lozano-Perez, L P Kaelbling, J Tenenbaum, arXiv:2203.09634arXiv preprintT. Silver, R. Chitnis, N. Kumar, W. McClinton, T. Lozano-Perez, L. P. Kaelbling, and J. Tenenbaum. Inventing relational state and action abstractions for effective and efficient bilevel planning. arXiv preprint arXiv:2203.09634, 2022.</p>
<p>Value function spaces: Skill-centric state abstractions for long-horizon reasoning. D Shah, P Xu, Y Lu, T Xiao, A Toshev, S Levine, B Ichter, D. Shah, P. Xu, Y. Lu, T. Xiao, A. Toshev, S. Levine, and B. Ichter. Value function spaces: Skill-centric state abstractions for long-horizon reasoning. ICLR, 2022. URL https://openreview.net/pdf?id=vgqS1vkkCbE.</p>
<p>Universal planning networks: Learning generalizable representations for visuomotor control. A Srinivas, A Jabri, P Abbeel, S Levine, C Finn, International Conference on Machine Learning. PMLRA. Srinivas, A. Jabri, P. Abbeel, S. Levine, and C. Finn. Universal planning networks: Learning generalizable representations for visuomotor control. In International Conference on Machine Learning, pages 4732-4741. PMLR, 2018.</p>
<p>Learning plannable representations with causal infogan. T Kurutach, A Tamar, G Yang, S J Russell, P Abbeel, Advances in Neural Information Processing Systems. 31T. Kurutach, A. Tamar, G. Yang, S. J. Russell, and P. Abbeel. Learning plannable representations with causal infogan. Advances in Neural Information Processing Systems, 31, 2018.</p>
<p>Grounding language to autonomously-acquired skills via goal generation. A Akakzia, C Colas, P.-Y Oudeyer, M Chetouani, O Sigaud, International Conference on Learning Representations. A. Akakzia, C. Colas, P.-Y. Oudeyer, M. Chetouani, and O. Sigaud. Grounding language to autonomously-acquired skills via goal generation. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=chPj I5KMHG.</p>
<p>Modeling long-horizon tasks as sequential interaction landscapes. S Pirk, K Hausman, A Toshev, M Khansari, arXiv:2006.04843arXiv preprintS. Pirk, K. Hausman, A. Toshev, and M. Khansari. Modeling long-horizon tasks as sequential interaction landscapes. arXiv preprint arXiv:2006.04843, 2020.</p>
<p>Toward understanding natural language directions. T Kollar, S Tellex, D Roy, N Roy, 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEET. Kollar, S. Tellex, D. Roy, and N. Roy. Toward understanding natural language directions. In 2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI), pages 259-266. IEEE, 2010.</p>
<p>Understanding natural language commands for robotic navigation and mobile manipulation. S Tellex, T Kollar, S Dickerson, M Walter, A Banerjee, S Teller, N Roy, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence25S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, and N. Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 25, pages 1507-1514, 2011.</p>
<p>Interpreting and executing recipes with a cooking robot. M Bollini, S Tellex, T Thompson, N Roy, D Rus, Experimental Robotics. SpringerM. Bollini, S. Tellex, T. Thompson, N. Roy, and D. Rus. Interpreting and executing recipes with a cooking robot. In Experimental Robotics, pages 481-495. Springer, 2013.</p>
<p>Asking for help using inverse semantics. S Tellex, R Knepper, A Li, D Rus, N Roy, S. Tellex, R. Knepper, A. Li, D. Rus, and N. Roy. Asking for help using inverse semantics. 2014.</p>
<p>Grounding verbs of motion in natural language commands to robots. T Kollar, S Tellex, D Roy, N Roy, Experimental robotics. SpringerT. Kollar, S. Tellex, D. Roy, and N. Roy. Grounding verbs of motion in natural language commands to robots. In Experimental robotics, pages 31-47. Springer, 2014.</p>
<p>Learning to map natural language instructions to physical quadcopter control using simulated flight. V Blukis, Y Terme, E Niklasson, R A Knepper, Y Artzi, arXiv:1910.09664arXiv preprintV. Blukis, Y. Terme, E. Niklasson, R. A. Knepper, and Y. Artzi. Learning to map natural language in- structions to physical quadcopter control using simulated flight. arXiv preprint arXiv:1910.09664, 2019.</p>
<p>Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation. S Nair, C Finn, abs/1909.05829ArXiv. S. Nair and C. Finn. Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation. ArXiv, abs/1909.05829, 2020.</p>
<p>Relmogen: Integrating motion generation in reinforcement learning for mobile manipulation. F Xia, C Li, R Martín-Martín, O Litany, A Toshev, S Savarese, 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021F. Xia, C. Li, R. Martín-Martín, O. Litany, A. Toshev, and S. Savarese. Relmogen: Integrating motion generation in reinforcement learning for mobile manipulation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), 2021.</p>
<p>Hrl4in: Hierarchical reinforcement learning for interactive navigation with mobile manipulators. C Li, F Xia, R Martin-Martin, S Savarese, Conference on Robot Learning. C. Li, F. Xia, R. Martin-Martin, and S. Savarese. Hrl4in: Hierarchical reinforcement learning for interactive navigation with mobile manipulators. In Conference on Robot Learning, 2020.</p>
<p>Language as an abstraction for hierarchical deep reinforcement learning. Y Jiang, S Gu, K Murphy, C Finn, NeurIPS. Y. Jiang, S. Gu, K. Murphy, and C. Finn. Language as an abstraction for hierarchical deep reinforcement learning. In NeurIPS, 2019.</p>
<p>D Hafner, K.-H Lee, I Fischer, P Abbeel, arXiv:2206.04114Deep hierarchical planning from pixels. arXiv preprintD. Hafner, K.-H. Lee, I. Fischer, and P. Abbeel. Deep hierarchical planning from pixels. arXiv preprint arXiv:2206.04114, 2022.</p>
<p>Exploration through learned language abstraction. S Mirchandani, S Karamcheti, D Sadigh, Ella, Advances in Neural Information Processing Systems. 34S. Mirchandani, S. Karamcheti, and D. Sadigh. Ella: Exploration through learned language abstraction. Advances in Neural Information Processing Systems, 34:29529-29540, 2021.</p>
<p>Visually-grounded planning without vision: Language models infer detailed plans from high-level instructions. P A Jansen, arXiv:2009.14259arXiv preprintP. A. Jansen. Visually-grounded planning without vision: Language models infer detailed plans from high-level instructions. arXiv preprint arXiv:2009.14259, 2020.</p>
<p>Skill induction and planning with latent language. P Sharma, A Torralba, J Andreas, arXiv:2110.01517arXiv preprintP. Sharma, A. Torralba, and J. Andreas. Skill induction and planning with latent language. arXiv preprint arXiv:2110.01517, 2021.</p>
<p>Pre-trained language models for interactive decision-making. S Li, X Puig, Y Du, C Wang, E Akyurek, A Torralba, J Andreas, I Mordatch, arXiv:2202.01771arXiv preprintS. Li, X. Puig, Y. Du, C. Wang, E. Akyurek, A. Torralba, J. Andreas, and I. Mordatch. Pre-trained language models for interactive decision-making. arXiv preprint arXiv:2202.01771, 2022.</p>
<p>Evaluating large language models trained on code. M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374arXiv preprintM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. arXiv preprintY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>N Reimers, I Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprintN. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, arXiv:2109.01652arXiv preprintJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Prospection: Interpretable plans from language by predicting the future. C Paxton, Y Bisk, J Thomason, A Byravan, D Foxl, 2019 International Conference on Robotics and Automation (ICRA). IEEEC. Paxton, Y. Bisk, J. Thomason, A. Byravan, and D. Foxl. Prospection: Interpretable plans from language by predicting the future. In 2019 International Conference on Robotics and Automation (ICRA), pages 6942-6948. IEEE, 2019.</p>
<p>Language-conditioned imitation learning for robot manipulation tasks. S Stepputtis, J Campbell, M Phielipp, S Lee, C Baral, H Ben Amor, Advances in Neural Information Processing Systems. 33S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. Ben Amor. Language-conditioned imitation learning for robot manipulation tasks. Advances in Neural Information Processing Systems, 33:13139-13150, 2020.</p>
<p>Few-shot object grounding and mapping for natural language robot instruction following. V Blukis, R A Knepper, Y Artzi, arXiv:2011.07384arXiv preprintV. Blukis, R. A. Knepper, and Y. Artzi. Few-shot object grounding and mapping for natural language robot instruction following. arXiv preprint arXiv:2011.07384, 2020.</p>
<p>Language conditioned imitation learning over unstructured data. Robotics: Science and Systems. C Lynch, P Sermanet, C. Lynch and P. Sermanet. Language conditioned imitation learning over unstructured data. Robotics: Science and Systems, 2021. URL https://arxiv.org/abs/2005.07648.</p>
<p>A joint network for grasp detection conditioned on natural language commands. Y Chen, R Xu, Y Lin, P A Vela, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEEY. Chen, R. Xu, Y. Lin, and P. A. Vela. A joint network for grasp detection conditioned on natural language commands. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 4576-4582. IEEE, 2021.</p>
<p>What matters in language conditioned robotic imitation learning. O Mees, L Hermann, W Burgard, arXiv:2204.06252arXiv preprintO. Mees, L. Hermann, and W. Burgard. What matters in language conditioned robotic imitation learning. arXiv preprint arXiv:2204.06252, 2022.</p>
<p>Intra-agent speech permits zero-shot task acquisition. C Yan, F Carnevale, P Georgiev, A Santoro, A Guy, A Muldal, C.-C Hung, J Abramson, T Lillicrap, G Wayne, arXiv:2206.03139arXiv preprintC. Yan, F. Carnevale, P. Georgiev, A. Santoro, A. Guy, A. Muldal, C.-C. Hung, J. Abramson, T. Lillicrap, and G. Wayne. Intra-agent speech permits zero-shot task acquisition. arXiv preprint arXiv:2206.03139, 2022.</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International Conference on Machine Learning. PMLRA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021.</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. J Lu, D Batra, D Parikh, S Lee, Advances in neural information processing systems. 32J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019.</p>
<p>Simvlm: Simple visual language model pretraining with weak supervision. Z Wang, J Yu, A W Yu, Z Dai, Y Tsvetkov, Y Cao, arXiv:2108.10904arXiv preprintZ. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021.</p>
<p>Embodied bert: A transformer model for embodied, language-guided visual task completion. A Suglia, Q Gao, J Thomason, G Thattai, G Sukhatme, arXiv:2108.04927arXiv preprintA. Suglia, Q. Gao, J. Thomason, G. Thattai, and G. Sukhatme. Embodied bert: A transformer model for embodied, language-guided visual task completion. arXiv preprint arXiv:2108.04927, 2021.</p>
<p>Big self-supervised models are strong semi-supervised learners. T Chen, S Kornblith, K Swersky, M Norouzi, G E Hinton, Advances in neural information processing systems. 33T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33:22243-22255, 2020.</p>
<p>Mural: multimodal, multitask retrieval across languages. A Jain, M Guo, K Srinivasan, T Chen, S Kudugunta, C Jia, Y Yang, J Baldridge, arXiv:2109.05125arXiv preprintA. Jain, M. Guo, K. Srinivasan, T. Chen, S. Kudugunta, C. Jia, Y. Yang, and J. Baldridge. Mural: multimodal, multitask retrieval across languages. arXiv preprint arXiv:2109.05125, 2021.</p>
<p>Plate: Visually-grounded planning with transformers in procedural tasks. J Sun, D.-A Huang, B Lu, Y.-H Liu, B Zhou, A Garg, IEEE Robotics and Automation Letters. 72J. Sun, D.-A. Huang, B. Lu, Y.-H. Liu, B. Zhou, and A. Garg. Plate: Visually-grounded planning with transformers in procedural tasks. IEEE Robotics and Automation Letters, 7(2):4924-4930, 2022.</p>
<p>Zero-shot anticipation for instructional activities. F Sener, A Yao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionF. Sener and A. Yao. Zero-shot anticipation for instructional activities. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 862-871, 2019.</p>
<p>Simple but effective: Clip embeddings for embodied ai. A Khandelwal, L Weihs, R Mottaghi, A Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionA. Khandelwal, L. Weihs, R. Mottaghi, and A. Kembhavi. Simple but effective: Clip embeddings for embodied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14829-14838, 2022.</p>
<p>Transporter networks: Rearranging the visual world for robotic manipulation. A Zeng, P Florence, J Tompson, S Welker, J Chien, M Attarian, T Armstrong, I Krasin, D Duong, V Sindhwani, J Lee, Conference on Robot Learning (CoRL). 2020A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin, D. Duong, V. Sindhwani, and J. Lee. Transporter networks: Rearranging the visual world for robotic manipulation. Conference on Robot Learning (CoRL), 2020.</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on Robot Learning. PMLRM. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, pages 894-906. PMLR, 2022.</p>
<p>Open-vocabulary object detection via vision and language knowledge distillation. X Gu, T.-Y Lin, W Kuo, Y Cui, arXiv:2104.13921arXiv preprintX. Gu, T.-Y. Lin, W. Kuo, and Y. Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.</p>
<p>Deep learning for detecting robotic grasps. I Lenz, H Lee, A Saxena, The International Journal of Robotics Research. 344-5I. Lenz, H. Lee, and A. Saxena. Deep learning for detecting robotic grasps. The International Journal of Robotics Research, 34(4-5):705-724, 2015.</p>
<p>Real-world multiobject, multigrasp detection. F.-J Chu, R Xu, P A Vela, IEEE Robotics and Automation Letters. 34F.-J. Chu, R. Xu, and P. A. Vela. Real-world multiobject, multigrasp detection. IEEE Robotics and Automation Letters, 3(4):3355-3362, 2018.</p>
<p>D Kalashnikov, J Varley, Y Chebotar, B Swanson, R Jonschkowski, C Finn, S Levine, K Hausman, arXiv:2104.08212Continuous multi-task robotic reinforcement learning at scale. arXiv preprintD. Kalashnikov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, and K. Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv preprint arXiv:2104.08212, 2021.</p>
<p>Grounding predicates through actions. T Migimatsu, J Bohg, arXiv:2109.14718arXiv preprintT. Migimatsu and J. Bohg. Grounding predicates through actions. arXiv preprint arXiv:2109.14718, 2021.</p>
<p>Can foundation models perform zero-shot task specification for robot manipulation?. Y Cui, S Niekum, A Gupta, V Kumar, A Rajeswaran, Learning for Dynamics and Control Conference. PMLRY. Cui, S. Niekum, A. Gupta, V. Kumar, and A. Rajeswaran. Can foundation models perform zero-shot task specification for robot manipulation? In Learning for Dynamics and Control Conference, pages 893-905. PMLR, 2022.</p>
<p>Recurrent convolutional neural network for object recognition. M Liang, X Hu, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionM. Liang and X. Hu. Recurrent convolutional neural network for object recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3367-3375, 2015.</p>
<p>Faster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, Advances in neural information processing systems. 28S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.</p>
<p>Z Zou, Z Shi, Y Guo, J Ye, arXiv:1905.05055Object detection in 20 years: A survey. arXiv preprintZ. Zou, Z. Shi, Y. Guo, and J. Ye. Object detection in 20 years: A survey. arXiv preprint arXiv:1905.05055, 2019.</p>
<p>Yolov4: Optimal speed and accuracy of object detection. A Bochkovskiy, C.-Y. Wang, H.-Y M Liao, arXiv:2004.10934arXiv preprintA. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020.</p>
<p>Vqa: Visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, C L Zitnick, D Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionS. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433, 2015.</p>
<p>Unified vision-language pre-training for image captioning and vqa. L Zhou, H Palangi, L Zhang, H Hu, J Corso, J Gao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, and J. Gao. Unified vision-language pre-training for image captioning and vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13041-13049, 2020.</p>
<p>Once for all: Train one network and specialize it for efficient deployment. H Cai, C Gan, T Wang, Z Zhang, S Han, International Conference on Learning Representations. H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han. Once for all: Train one network and specialize it for efficient deployment. In International Conference on Learning Representations, 2020. URL https://arxiv.org/pdf/1908.09791.pdf.</p>
<p>Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, arXiv:2204.14198arXiv preprintJ.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, arXiv:2203.02155arXiv preprintL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.</p>
<p>Mdetr-modulated detection for end-to-end multi-modal understanding. A Kamath, M Singh, Y Lecun, G Synnaeve, I Misra, N Carion, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionA. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1780-1790, 2021.</p>
<p>STOP =============== Task: Sort pencils and markers and in different containers Scene: Visible objects are. T Xiao, E Jang, D Kalashnikov, S Levine, J Ibarz, K Hausman, A Herzog, arXiv:2004.06089Robot action: robot.stop() STOP =============== Task: Stack all the blocks. arXiv preprintSuccessful action: False Scene: Visible objects are. red marker", "red pen", "blue box", "green bowl"] Scene: Occluded objects are. Robot action: robot.pick_place("red marker. Successful action: False Scene: Visible objects are. red marker. red pen", "blue box", "green bowl"] Scene: Occluded objects are. Robot action: robot.pick_place("red marker", "green bowl") Successful action: True Scene: Visible objects are. red marker", "red pen", "blue box", "green bowl"] Scene: Occluded objects are. Robot action: robot.pick_place("red pen", "blue boxT. Xiao, E. Jang, D. Kalashnikov, S. Levine, J. Ibarz, K. Hausman, and A. Herzog. Thinking while moving: Deep reinforcement learning with concurrent control. arXiv preprint arXiv:2004.06089, 2020. Scene: Visible objects are ["brown block"] Scene: Occluded objects are ["cyan block", "yellow block"] Robot action: robot.stop() STOP =============== Task: Stack all the blocks. Scene: Visible objects are ["brown block", "purple block"] Scene: Occluded objects are [] Robot action: robot.pick_place("brown block", "purple block") Successful action: False Scene: Visible objects are ["brown block", "purple block"] Scene: Occluded objects are [] Robot action: robot.pick_place("brown block", "purple block") Successful action: True Scene: Visible objects are ["brown block", "orange block"] Scene: Occluded objects are ["purple block"] Robot action: robot.pick_place("orange block", "brown block") Successful action: True Scene: Visible objects are ["orange block"] Scene: Occluded objects are ["purple block", "brown block"] Robot action: robot.stop() STOP =============== Task: Stack all the blocks. Scene: Visible objects are ["brown block", "purple block"] Scene: Occluded objects are [] Robot action: robot.pick_place("brown block", "purple block") Successful action: True Scene: Visible objects are ["brown block"] Scene: Occluded objects are ["purple block"] Robot action: robot.stop() STOP =============== Task: Put objects in their corresponding bowls Scene: Visible objects are ["red block", "green block", "red bowl", "green bowl"] Scene: Occluded objects are [] Robot action: robot.pick_place("red block", "red bowl") Successful action: False Scene: Visible objects are ["red block", "green block", "red bowl", "green bowl"] Scene: Occluded objects are [] Robot action: robot.pick_place("red block", "red bowl") Successful action: True Scene: Visible objects are ["red block", "green block", "red bowl", "green bowl"] Scene: Occluded objects are [] Robot action: robot.pick_place("green block", "green bowl") Successful action: True Scene: Visible objects are ["red block", "green block", "red bowl", "green bowl"] Scene: Occluded objects are [] Robot action: robot.stop() STOP =============== Task: Sort pencils and markers and in different containers Scene: Visible objects are ["red marker", "red pen", "blue box", "green bowl"] Scene: Occluded objects are [] Robot action: robot.pick_place("red marker", "green bowl") Successful action: False Scene: Visible objects are ["red marker", "red pen", "blue box", "green bowl"] Scene: Occluded objects are [] Robot action: robot.pick_place("red marker", "green bowl") Successful action: False Scene: Visible objects are ["red marker", "red pen", "blue box", "green bowl"] Scene: Occluded objects are [] Robot action: robot.pick_place("red marker", "green bowl") Successful action: True Scene: Visible objects are ["red marker", "red pen", "blue box", "green bowl"] Scene: Occluded objects are [] Robot action: robot.pick_place("red pen", "blue box")</p>
<p>Human: pick up the peanuts Robot: 1. pick up the peanuts 2. done. Human: bring me some snack Robot: 1. pick up the peanuts 2. pick up the peanuts. you I'm done. scene: blue chip bag, 7upyou I'm done. Human: pick up the peanuts Robot: 1. pick up the peanuts 2. done. Human: bring me some snack Robot: 1. pick up the peanuts 2. pick up the peanuts[scene: blue chip bag, 7up]</p>
<p>Please keep the lime soda here because I haven't finished it 8. put down the lime soda and continue 9. done. Human: let go of the banana Robot: 1. put down the banana and continue 2. done. Human: bring me something healthy to eat Robot: 1. go to counter and ask: What food items. up the lime soda. Human: I changed my mind. do you have? Human: We have apples, bananas, multigrain chips, and kettle chipsup the lime soda, Human: I changed my mind. Please keep the lime soda here because I haven't finished it 8. put down the lime soda and continue 9. done. Human: let go of the banana Robot: 1. put down the banana and continue 2. done. Human: bring me something healthy to eat Robot: 1. go to counter and ask: What food items do you have? Human: We have apples, bananas, multigrain chips, and kettle chips</p>
<p>Human: I actually don't want to have fruits. Can you bring me something else?. down the appledown the apple, Human: I actually don't want to have fruits. Can you bring me something else?</p>
<p>open the drawer and ask: what items are there in the drawer? Human: There is a coke and rice chips. open the drawer and ask: what items are there in the drawer? Human: There is a coke and rice chips</p>
<p>the table and ask: Did I successfully take out everything from the drawer? Human: The coke is still there. the table and ask: Did I successfully take out everything from the drawer? Human: The coke is still there</p>            </div>
        </div>

    </div>
</body>
</html>
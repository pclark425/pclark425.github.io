<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7225 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7225</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7225</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-264376206</p>
                <p><strong>Paper Title:</strong> Verbal lie detection using Large Language Models</p>
                <p><strong>Paper Abstract:</strong> Human accuracy in detecting deception with intuitive judgments has been proven to not go above the chance level. Therefore, several automatized verbal lie detection techniques employing Machine Learning and Transformer models have been developed to reach higher levels of accuracy. This study is the first to explore the performance of a Large Language Model, FLAN-T5 (small and base sizes), in a lie-detection classification task in three English-language datasets encompassing personal opinions, autobiographical memories, and future intentions. After performing stylometric analysis to describe linguistic differences in the three datasets, we tested the small- and base-sized FLAN-T5 in three Scenarios using 10-fold cross-validation: one with train and test set coming from the same single dataset, one with train set coming from two datasets and the test set coming from the third remaining dataset, one with train and test set coming from all the three datasets. We reached state-of-the-art results in Scenarios 1 and 3, outperforming previous benchmarks. The results revealed also that model performance depended on model size, with larger models exhibiting higher performance. Furthermore, stylometric analysis was performed to carry out explainability analysis, finding that linguistic features associated with the Cognitive Load framework may influence the model’s predictions.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7225.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7225.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5 - Opinion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (small & base) fine-tuned on the Deceptive Opinions dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>FLAN-T5 (instruction-tuned T5 family) was fine-tuned to classify short opinion narratives as truthful or deceptive and evaluated with 10-fold cross-validation; both small and base sizes were tested.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-/task-tuned T5-family encoder-decoder large language model (text-to-text paradigm), used here in 'small' and 'base' sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>small / base</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Verbal lie-detection — Deceptive Opinions dataset</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Deception detection / social cognition</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Binary classification of short written personal opinions as truthful or deceptive; labels derived from participants instructed to produce genuine or fabricated opinions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (mean ± standard deviation across 10 folds)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Paper reports general prior literature that human intuitive judgments typically do not exceed chance (~50%); one cited study reports instructed human accuracy ranging 59–79% (Verschuere et al., 2023). No direct human baseline on this dataset is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Scenario 1 (within-dataset): FLAN-T5 small = 80.64% ± 2.03; FLAN-T5 base = 82.60% ± 3.01. Scenario 3 (aggregated multi-context training/test): FLAN-T5 small = 79.00% ± 2.11; FLAN-T5 base = 82.72% ± 2.39.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Supervised fine-tuning of instruction-tuned FLAN-T5 on labeled training data; evaluated via 10-fold cross-validation (text-to-text classification).</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Prior literature cited in the paper (refs. 5,6 for general near-chance human performance; ref. 10 = Verschuere et al., 2023 for 59–79% when instructed). No direct human baseline on the Deceptive Opinions dataset reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>No statistical test reported comparing LLM performance directly to human baselines; model results reported as mean ± SD across folds. Paper reports outperforming classical ML baselines and prior model baselines (accuracy differences reported), but no p-values comparing to human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Human baseline statements in the paper are drawn from prior literature and are not measured on the same datasets; the LLM substantially outperformed simple baselines (BoW+logistic) and prior transformer baselines reported in literature for this dataset. Limitations include low-stakes experimental data and absence of direct human-vs-model statistical comparison on the same samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Verbal lie detection using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7225.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7225.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5 - Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (small & base) fine-tuned on the Hippocorpus (Memory) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>FLAN-T5 models were fine-tuned to classify autobiographical memory narratives as truthful or deceptive; performance measured with 10-fold cross-validation and reported per model size and scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-/task-tuned T5-family encoder-decoder LLM used in small and base sizes for supervised fine-tuning on deception detection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>small / base</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Verbal lie-detection — Hippocorpus ( autobiographical memory ) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Deception detection / memory and reality monitoring</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Binary classification of autobiographical memory narratives as truthful or fabricated; dataset contains longer, narrative-style memory accounts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (mean ± standard deviation across 10 folds)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Paper cites prior literature indicating human deception-detection accuracy is generally at or near chance; no direct measured human baseline on this Memory dataset provided in the paper (general references: refs. 5,6; an instructed improvement to 59–79% is cited from Verschuere et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Scenario 1 (within-dataset): FLAN-T5 small = 76.87% ± 2.06; FLAN-T5 base = 80.61% ± 1.41. Scenario 3 (aggregated multi-context training/test): FLAN-T5 small = 75.67% ± 1.90; FLAN-T5 base = 79.87% ± 1.60.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Supervised fine-tuning of FLAN-T5 on labeled memory narratives; evaluation via 10-fold cross-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>General human performance references in the paper (refs. 5,6 for near-chance performance; ref. 10 = Verschuere et al., 2023 for 59–79% under instructed conditions). No empirical human baseline on the Hippocorpus dataset is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>No statistical comparison reported between LLM and human performance on these datasets. Model accuracies are reported with mean and SD; the paper reports that model performance significantly exceeded computational baselines (e.g., BoW+logistic) but does not present p-values comparing to human benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The Memory dataset contains longer narratives, and the paper notes a larger performance improvement over simple baselines (reported 32% improvement over logistic baseline for Memory). Human baseline claims are literature-based and not co-measured with the model on the same items; generalization to real-world or high-stakes deception remains limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Verbal lie detection using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7225.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7225.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5 - Intention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (small & base) fine-tuned on the Intention (future intentions) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>FLAN-T5 small and base models were fine-tuned to classify future-intention utterances as truthful or deceptive; performance reported across within-dataset and aggregated scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned T5-family text-to-text LLM (encoder-decoder) fine-tuned in supervised fashion; small & base sizes evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>small / base</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Verbal lie-detection — Intention (future intentions) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Deception detection / planning and intention</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Binary classification of short texts describing future intentions as truthful or fabricated; dataset originally from Kleinberg & Verschuere and used to study verifiable detail and intention deception.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (mean ± standard deviation)</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>Paper states general human accuracy on deception detection is near chance (refs. 5,6) and cites Verschuere et al. (2023) reporting instructed human accuracy of 59–79%; no direct human baseline measured on this Intention dataset is reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Scenario 1 (within-dataset): FLAN-T5 small = 71.46% ± 3.65; FLAN-T5 base = 71.52% ± 2.21. Scenario 3 (aggregated multi-context training/test): FLAN-T5 small = 69.32% ± 3.75; FLAN-T5 base = 72.25% ± 2.86.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Supervised fine-tuning on labeled intention statements; evaluated via 10-fold cross-validation for within-dataset and aggregated scenarios; for cross-domain (Scenario 2) models trained on two datasets and tested on the third (no CV).</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Paper references prior human-performance literature (refs. 5,6 for near-chance; ref. 10 = Verschuere et al., 2023 for 59–79% when instructed). The Intention dataset itself originates from Kleinberg & Verschuere (dataset reference in paper), but no human baseline numbers from that dataset are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>No statistical test reported comparing LLM performance directly to human performance on these datasets. Model metrics are reported as mean ± SD; comparisons are made against computational baselines and prior model baselines (differences reported), but p-values vs human baselines are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>LLM improved over classical baselines (BoW+logistic) and prior transformer-based literature baselines (gains smaller here than for Memory/Opinion). The paper emphasizes domain-specificity: when trained on two domains and tested on an unseen third (Scenario 2), FLAN-T5 performance fell to chance (~50%), indicating limited cross-domain generalization without examples from the target domain. Human baseline statements are literature-derived and not co-measured.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Verbal lie detection using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The use-the-best heuristic facilitates deception detection <em>(Rating: 2)</em></li>
                <li>How humans impair automated deception detection performance <em>(Rating: 2)</em></li>
                <li>Quantifying the narrative flow of imagined versus autobiographical stories <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7225",
    "paper_id": "paper-264376206",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "FLAN-T5 - Opinion",
            "name_full": "FLAN-T5 (small & base) fine-tuned on the Deceptive Opinions dataset",
            "brief_description": "FLAN-T5 (instruction-tuned T5 family) was fine-tuned to classify short opinion narratives as truthful or deceptive and evaluated with 10-fold cross-validation; both small and base sizes were tested.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLAN-T5",
            "model_description": "Instruction-/task-tuned T5-family encoder-decoder large language model (text-to-text paradigm), used here in 'small' and 'base' sizes.",
            "model_size": "small / base",
            "test_name": "Verbal lie-detection — Deceptive Opinions dataset",
            "test_category": "Deception detection / social cognition",
            "test_description": "Binary classification of short written personal opinions as truthful or deceptive; labels derived from participants instructed to produce genuine or fabricated opinions.",
            "evaluation_metric": "Accuracy (mean ± standard deviation across 10 folds)",
            "human_performance": "Paper reports general prior literature that human intuitive judgments typically do not exceed chance (~50%); one cited study reports instructed human accuracy ranging 59–79% (Verschuere et al., 2023). No direct human baseline on this dataset is reported in this paper.",
            "llm_performance": "Scenario 1 (within-dataset): FLAN-T5 small = 80.64% ± 2.03; FLAN-T5 base = 82.60% ± 3.01. Scenario 3 (aggregated multi-context training/test): FLAN-T5 small = 79.00% ± 2.11; FLAN-T5 base = 82.72% ± 2.39.",
            "prompting_method": "Supervised fine-tuning of instruction-tuned FLAN-T5 on labeled training data; evaluated via 10-fold cross-validation (text-to-text classification).",
            "fine_tuned": true,
            "human_data_source": "Prior literature cited in the paper (refs. 5,6 for general near-chance human performance; ref. 10 = Verschuere et al., 2023 for 59–79% when instructed). No direct human baseline on the Deceptive Opinions dataset reported here.",
            "statistical_significance": "No statistical test reported comparing LLM performance directly to human baselines; model results reported as mean ± SD across folds. Paper reports outperforming classical ML baselines and prior model baselines (accuracy differences reported), but no p-values comparing to human performance.",
            "notes": "Human baseline statements in the paper are drawn from prior literature and are not measured on the same datasets; the LLM substantially outperformed simple baselines (BoW+logistic) and prior transformer baselines reported in literature for this dataset. Limitations include low-stakes experimental data and absence of direct human-vs-model statistical comparison on the same samples.",
            "uuid": "e7225.0",
            "source_info": {
                "paper_title": "Verbal lie detection using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "FLAN-T5 - Memory",
            "name_full": "FLAN-T5 (small & base) fine-tuned on the Hippocorpus (Memory) dataset",
            "brief_description": "FLAN-T5 models were fine-tuned to classify autobiographical memory narratives as truthful or deceptive; performance measured with 10-fold cross-validation and reported per model size and scenario.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLAN-T5",
            "model_description": "Instruction-/task-tuned T5-family encoder-decoder LLM used in small and base sizes for supervised fine-tuning on deception detection.",
            "model_size": "small / base",
            "test_name": "Verbal lie-detection — Hippocorpus ( autobiographical memory ) dataset",
            "test_category": "Deception detection / memory and reality monitoring",
            "test_description": "Binary classification of autobiographical memory narratives as truthful or fabricated; dataset contains longer, narrative-style memory accounts.",
            "evaluation_metric": "Accuracy (mean ± standard deviation across 10 folds)",
            "human_performance": "Paper cites prior literature indicating human deception-detection accuracy is generally at or near chance; no direct measured human baseline on this Memory dataset provided in the paper (general references: refs. 5,6; an instructed improvement to 59–79% is cited from Verschuere et al., 2023).",
            "llm_performance": "Scenario 1 (within-dataset): FLAN-T5 small = 76.87% ± 2.06; FLAN-T5 base = 80.61% ± 1.41. Scenario 3 (aggregated multi-context training/test): FLAN-T5 small = 75.67% ± 1.90; FLAN-T5 base = 79.87% ± 1.60.",
            "prompting_method": "Supervised fine-tuning of FLAN-T5 on labeled memory narratives; evaluation via 10-fold cross-validation.",
            "fine_tuned": true,
            "human_data_source": "General human performance references in the paper (refs. 5,6 for near-chance performance; ref. 10 = Verschuere et al., 2023 for 59–79% under instructed conditions). No empirical human baseline on the Hippocorpus dataset is reported in this paper.",
            "statistical_significance": "No statistical comparison reported between LLM and human performance on these datasets. Model accuracies are reported with mean and SD; the paper reports that model performance significantly exceeded computational baselines (e.g., BoW+logistic) but does not present p-values comparing to human benchmarks.",
            "notes": "The Memory dataset contains longer narratives, and the paper notes a larger performance improvement over simple baselines (reported 32% improvement over logistic baseline for Memory). Human baseline claims are literature-based and not co-measured with the model on the same items; generalization to real-world or high-stakes deception remains limited.",
            "uuid": "e7225.1",
            "source_info": {
                "paper_title": "Verbal lie detection using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "FLAN-T5 - Intention",
            "name_full": "FLAN-T5 (small & base) fine-tuned on the Intention (future intentions) dataset",
            "brief_description": "FLAN-T5 small and base models were fine-tuned to classify future-intention utterances as truthful or deceptive; performance reported across within-dataset and aggregated scenarios.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLAN-T5",
            "model_description": "Instruction-tuned T5-family text-to-text LLM (encoder-decoder) fine-tuned in supervised fashion; small & base sizes evaluated.",
            "model_size": "small / base",
            "test_name": "Verbal lie-detection — Intention (future intentions) dataset",
            "test_category": "Deception detection / planning and intention",
            "test_description": "Binary classification of short texts describing future intentions as truthful or fabricated; dataset originally from Kleinberg & Verschuere and used to study verifiable detail and intention deception.",
            "evaluation_metric": "Accuracy (mean ± standard deviation)",
            "human_performance": "Paper states general human accuracy on deception detection is near chance (refs. 5,6) and cites Verschuere et al. (2023) reporting instructed human accuracy of 59–79%; no direct human baseline measured on this Intention dataset is reported in the paper.",
            "llm_performance": "Scenario 1 (within-dataset): FLAN-T5 small = 71.46% ± 3.65; FLAN-T5 base = 71.52% ± 2.21. Scenario 3 (aggregated multi-context training/test): FLAN-T5 small = 69.32% ± 3.75; FLAN-T5 base = 72.25% ± 2.86.",
            "prompting_method": "Supervised fine-tuning on labeled intention statements; evaluated via 10-fold cross-validation for within-dataset and aggregated scenarios; for cross-domain (Scenario 2) models trained on two datasets and tested on the third (no CV).",
            "fine_tuned": true,
            "human_data_source": "Paper references prior human-performance literature (refs. 5,6 for near-chance; ref. 10 = Verschuere et al., 2023 for 59–79% when instructed). The Intention dataset itself originates from Kleinberg & Verschuere (dataset reference in paper), but no human baseline numbers from that dataset are reported here.",
            "statistical_significance": "No statistical test reported comparing LLM performance directly to human performance on these datasets. Model metrics are reported as mean ± SD; comparisons are made against computational baselines and prior model baselines (differences reported), but p-values vs human baselines are not provided.",
            "notes": "LLM improved over classical baselines (BoW+logistic) and prior transformer-based literature baselines (gains smaller here than for Memory/Opinion). The paper emphasizes domain-specificity: when trained on two domains and tested on an unseen third (Scenario 2), FLAN-T5 performance fell to chance (~50%), indicating limited cross-domain generalization without examples from the target domain. Human baseline statements are literature-derived and not co-measured.",
            "uuid": "e7225.2",
            "source_info": {
                "paper_title": "Verbal lie detection using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The use-the-best heuristic facilitates deception detection",
            "rating": 2,
            "sanitized_title": "the_usethebest_heuristic_facilitates_deception_detection"
        },
        {
            "paper_title": "How humans impair automated deception detection performance",
            "rating": 2,
            "sanitized_title": "how_humans_impair_automated_deception_detection_performance"
        },
        {
            "paper_title": "Quantifying the narrative flow of imagined versus autobiographical stories",
            "rating": 1,
            "sanitized_title": "quantifying_the_narrative_flow_of_imagined_versus_autobiographical_stories"
        }
    ],
    "cost": 0.01445325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Verbal lie detection using Large Language Models</p>
<p>Riccardo Loconte riccardo.loconte@imtlucca.it 
Molecular Mind Lab
IMT School for Advanced Studies Lucca
Piazza San Francesco 1955100LuccaLUItaly</p>
<p>Roberto Russo 
Department of Mathematics "Tullio Levi-Civita"
University of Padova
PadovaItaly</p>
<p>Pasquale Capuozzo 
Department of General Psychology
University of Padova
PadovaItaly</p>
<p>Pietro Pietrini 
Molecular Mind Lab
IMT School for Advanced Studies Lucca
Piazza San Francesco 1955100LuccaLUItaly</p>
<p>Giuseppe Sartori 
Department of General Psychology
University of Padova
PadovaItaly</p>
<p>Verbal lie detection using Large Language Models
A36F5F6B9CB738CE959BC4D2E814216410.1038/s41598-023-50214-0Received: 29 June 2023; Accepted: 16 December 2023
Human accuracy in detecting deception with intuitive judgments has been proven to not go above the chance level.Therefore, several automatized verbal lie detection techniques employing Machine Learning and Transformer models have been developed to reach higher levels of accuracy.This study is the first to explore the performance of a Large Language Model, FLAN-T5 (small and base sizes), in a lie-detection classification task in three English-language datasets encompassing personal opinions, autobiographical memories, and future intentions.After performing stylometric analysis to describe linguistic differences in the three datasets, we tested the small-and base-sized FLAN-T5 in three Scenarios using 10-fold cross-validation: one with train and test set coming from the same single dataset, one with train set coming from two datasets and the test set coming from the third remaining dataset, one with train and test set coming from all the three datasets.We reached stateof-the-art results in Scenarios 1 and 3, outperforming previous benchmarks.The results revealed also that model performance depended on model size, with larger models exhibiting higher performance.Furthermore, stylometric analysis was performed to carry out explainability analysis, finding that linguistic features associated with the Cognitive Load framework may influence the model's predictions.Lie detection involves the process of determining the veracity of a given communication.When producing deceptive narratives, liars employ verbal strategies to create false beliefs in the interacting partners and are thus involved in a specific and temporary psychological and emotional state 1 .For this reason, the Undeutsch hypothesis suggests that deceptive narratives differ in form and content from truthful narratives 2 .This topic has always been under constant investigation and development in the field of cognitive psychology, given its significant and promising applications in the forensic and legal setting 3 .Its potential pivotal role is in determining the honesty of witnesses and potential suspects during investigations and legal proceedings, impacting both the investigative information-gathering process and the final decision-making level4.Decades of research have focused on identifying verbal cues for deception and developing effective methods to differentiate between truthful and deceptive narratives, with such verbal cues being, at best, subtle and typically resulting in both naive and expert individuals performing just above chance levels5,6.A potential explanation coming from social psychology for this unsatisfactory human performance is the intrinsic human inclination to the truth bias 7 , i.e., the cognitive heuristic of presumption of honesty, which makes people assume that an interaction partner is truthful unless they have reasons to believe otherwise 8,9 .However, it is worth mentioning that a more recent study challenged this solid result, finding that instructing participants to rely only on the best available cue, such as the detailedness of the story, enabled them to consistently discriminate lies from the truth with accuracy ranging from 59 to 79% 10 .This finding moves the debate on (1) the proper number of cues that judges should combine before providing their veracity judgment -with the suggestion that the use-the-best heuristic approach is the most straightforward and accurate-and thus on (2) the diagnosticity level of this cue.More recently, the issue of verbal lie detection has also been tackled by employing computational techniques, such as stylometry.Stylometry refers to a set of methodologies and tools from computational linguistic and artificial intelligence that allow to conduct quantitative analysis of linguistic features within written texts to uncover distinctive patterns that can infer and characterize authorship or other stylistic attributes[11][12][13].Albeit with some limitations, stylometry has been proven to be effective in the context of lie detection14,15.The main advantage is the possibility of coding and extracting verbal cues independently from human judgment, hence reducing the problem of inter-coder agreement, as researchers using the same technique for the same data will extract the same indices 15 .</p>
<p>The promising results in applying NLP techniques for psychological research suggest the possibility of combining metrics from different psychological frameworks in a new theory-based stylometric analysis, offering the possibility to investigate verbal lie detection from multiple perspectives in one shot.</p>
<p>Related works in the AI field</p>
<p>Previous works from the AI field have applied machine learning and deep learning models in a binary classification task for data-driven verbal deception detection.</p>
<p>Kleinberg and Verschuere 49 developed a database of future intentions to investigate whether combining machine and human judgments may improve accuracy in predicting deception.While finding that human judgment impairs automated deception detection accuracy, the authors implemented two machine learning models (i.e., vanilla random forest) trained respectively on LIWC and Part-of-Speech features (e.g., frequency of names, adjectives, adverbs, verbs) reaching an accuracy of 69% (95% CI: 63-74%) and 64 (95% CI: 58%, 69%), respectively.On the same dataset, Ilias et al. 50evaluated six deep-learning models, including combinations of BERT (and RoBERTa), MultiHead Attention, co-attentions, and Transformers models.The best accuracy reached was 70.61% (± 2.58%) using a BERT with co-attention model.The authors also provided explainaibility analysis to understand how the models reached their decisions using a combination of LIME (a tool used to explain deep learning predictions in more straightforward and understandable terms by showing which specific words of the text influenced the outcome) and LIWC.</p>
<p>Capuozzo et al. 51 developed a new cross-domain and cross-language dataset of opinions, asking Englishspeaking and Italian-speaking participants to provide truthful or deceptive opinions on five different topics.After encoding the texts with FastText word-embedding, they trained Transformers models in multiple scenarios using 10-fold cross-validation, with averaged accuracy ranging from 63% (± 8.7%) in the "within-topic" scenario to a high of 90.1% (± 0.16%) in the "author-based" scenario.</p>
<p>In contrast, Sap et al. 52 developed a new dataset of narratives generated from memories and imagination and used an LLM (GPT-3) to compute a new metric called "sequentiality".Sequentiality is a metric of narrative flow that compares the probability of a sentence with and without its preceding story context.While providing insights into the cognitive processes of storytelling with an innovative computational approach, the authors did not employ a fine-tuning procedure for an LLM to classify different narratives.</p>
<p>The findings in the AI domain indicate that as the model's complexity increases, there is a heightened accuracy in predicting deception from texts.However, this increase in accuracy often comes at the expense of explainability for these predictions.LLMs are currently among the most cutting-edge models capable of handling vast amounts and complexities of linguistic data, and the lack of literature on fine-tuning LLMs for lie-detection tasks provides worthwhile reasons to investigate this area.</p>
<p>Aims and hypotheses of the study</p>
<p>The main objectives and hypothesis of this study are outlined as follows:</p>
<p>• Hypothesis 1a): Fine-tuning an LLM can effectively classify the veracity of short narratives from raw texts, 1b) outperforming classical machine learning and deep learning approaches in verbal lie detection.• Hypothesis 2): Fine-tuning an LLM can effectively classify the veracity of short narratives from raw texts, 1b) outperforming classical machine learning and deep learning approaches in verbal lie detection.• Hypothesis 3): Fine-tuning an LLM can effectively classify the veracity of short narratives from raw texts, 1b) outperforming classical machine learning and deep learning approaches in verbal lie detection.• Hypothesis 4): Fine-tuning an LLM can effectively classify the veracity of short narratives from raw texts, 1b) outperforming classical machine learning and deep learning approaches in verbal lie detection.• Hypothesis 5a): The linguistic style distinguishing truthful from deceptive statements varies across different contexts, 5b) and can be a significant feature for model prediction.</p>
<p>To test Hypothesis 1a, we fine-tuned an open-source LLM, FLAN-T5, using three datasets: personal opinions (the Deceptive Opinions dataset 51 ), autobiographical experiences (the Hippocorpus dataset 52 ) and future intentions (the Intention dataset 49 ).Given the extreme flexibility of LLMs, this approach is hypothesized to detect deception from raw texts above the chance level.To test the advantage of our approach compared to classical machine and deep learning models (Hypothesis 1b), we decided to compare the results with two benchmarks, further described in the Methods and Materials section.</p>
<p>With regards to Hypotheses 2 and 3, according to empirical evidence, classical machine learning models tend to experience a decline in performance when trained and tested on the aforementioned scenarios [53][54][55] .In contrast, LLMs have acquired a comprehensive understanding of language patterns during the pre-training phase.We posit that a fine-tuned LLM is capable of generalizing its learning across various contexts.Related to Hypothesis 4, we believe this generalization ability is further enhanced in larger models, as their size is associated with a more sophisticated representation of language.</p>
<p>Finally, to test Hypothesis 5, we introduced a new theory-based stylometric approach, named DeCLaRatiVE stylometry, to extract linguistic features related to the psychological frameworks of Distancing 29 , Cognitive Load 31 , Reality Monitoring 32 , and Verifiability Approach 40,41 , providing a pragmatic set of criteria to extract features from utterances.We will apply DeCLaRatiVE stylometry to compare truthful and deceptive statements in the three aforementioned datasets in order to explore potential differences in terms of linguistic style.Our hypothesis suggests that the linguistic style distinguishing truthful from deceptive statements may vary across the three datasets, as these types of statements originate from distinct contexts.We also applied the DeCLaRatiVE stylometry technique to provide explainability analysis of the top-performing model.</p>
<p>Methods and materials</p>
<p>Datasets</p>
<p>Three datasets were employed for this study: the Deceptive Opinions dataset 51 , from now on Opinion Dataset, the Hippocorpus dataset 52 , from now on Memory Dataset, and the Intention dataset 49 .For each dataset, participants were required to provide genuine or fabricated statements in three different domains: personal opinions on five different topics (Opinion dataset), autobiographical experiences (Memory dataset), and future intentions (Intention Dataset).Notably, the specific topic within each domain was counterbalanced among liars and truth-tellers.A more detailed description of each dataset is available in Supplementary Information as well as in the method section of each original article.Table 1 displays an example of truthful and deceptive statements about opinions, memories, and intentions.Table 2 reports descriptive statistics for each dataset, both overall and when grouped by truthful and deceptive sets of statements.These statistics include the minimum, maximum, average, and standard deviation of word counts.Word counts were computed after text tokenization using spaCy, a Python library for text processing.Additionally, Table 2 provides Jaccard similarity index values between truthful and deceptive vocabulary sets.Jaccard's index was derived by calculating the intersection (common words) and union (total words) of these Table 1.Truthful and deceptive example statements about opinions, memories, and intentions.In brackets, the topic assigned to the participant in the deceptive condition to fabricate the narrative.</p>
<p>Truthful Deceptive</p>
<p>Opinion (Abortion)</p>
<p>While I am morally torn on the issue, I believe that ultimately it is a woman's body and she should be able to do with it as she pleases.I belive people should not dehumanize the fetus tough, to make themselves feel better.The decision about laws regarding this issue should be left up to the states to decide.To combat this problem, birth control should be easily accessible Abortion is the termination of a life and should not be al-lowed.If a fetus has made it to the point of being able to survive "on its own" outside its mother's body, what right do we have to cut its life short.If the mother's life is in danger, she already chose that she was willing to sacrifice her life to have a child when she consented to procreating Memory (My boyfriend and I went to a concert together and had a great time.We met some of my friends there and really enjoyed ourselves watching the sunset.)</p>
<p>The day started perfectly, with a great drive up to Denver for the show.Me and my boyfriend didn't hit any traffic on the way to Red Rocks, and the weather was beautiful.We met up with my friends at the show, near the top of the theater, and laid down a blanket.The opener came on, and we danced our butts off to the banjoes and mandolins that were playing on-stage.We were so happy to be there.That's when the sunset started.It was so beautiful.The sky was a pastel pink and was beautiful to watch.That's when Phil Lesh came on, and I just about died.It was the happiest moment of my life, seeing him after almost a decade of not seeing him.I was so happy to be there, with my friends and my love.There was nothing that could top that night.We drove home to a sky full of stars and stopped at an overlook to look up at them.I love this place I live.And I love live music.I was so happy Concerts are my most favorite thing, and my boyfriend knew it.That's why, for our anniversary, he got me tickets to see my favorite artist.Not only that, but the tickets were for an outdoor show, which I love much more than being in a crowded stadium.Since he knew I was such a big fan of music, he got tickets for himself, and even a couple of my friends.He is so incredibly nice and considerate to me and what I like to do.I will always remember this event and I will always cherish him.On the day of the concert, I got ready, and he picked me up and we went out to a restaurant beforehand.He is so incredibly romantic.He knew exactly where to take me without asking.We ate, laughed, and had a wonderful dinner date before the big event.We arrived at the concert and the music was so incredibly beautiful.I loved every minute of it.My friends, boyfriend, and I all sat down next to each other.As the music was slowly dying down, I found us all getting lost just staring at the stars.It was such an incredibly unforgettable and beautiful night Intention (Going swimming with my daughter)</p>
<p>We go to a Waterbabies class every week, where my 16-month-old is learning to swim.We do lots of activities in the water, such as learning to blow bubbles, using floats to aid swimming, splashing and learning how to save themselves should they ever fall in.I find this activity important as I enjoy spending time with my daughter and swimming is an important life skill I will be taking my 8-year-old daughter swimming this Saturday.We'll be going early in the morning, as it's generally a lot quieter at that time, and my daughter is always up early watching cartoons anyway (5 am!).I'm trying to teach her how to swim in the deep end before she starts her new school in September as they have swimming lessons there twice a week www.nature.com/scientificreports/two sets 50,56 .The resulting index ranges from 0 to 1, with 0 indicating a completely different vocabulary between the two sets, and 1 indicating a completely identical vocabulary between the two sets.We reported the Jaccard similarity index to provide a measure of similarity or overlap between the word choices of truthful and deceptive statements within the respective datasets.Supplementary Information offers a detailed methodology for calculating the Jaccard similarity index.</p>
<p>FLAN-T5</p>
<p>We adopted FLAN-T5, an LLM developed by Google researchers and freely available through HuggingFace Python's library Transformers (https:// huggi ngface.co/ docs/ trans forme rs/ model_ doc/ flan-t5).HugginFace is a company that provides free access to state-of-the-art LLMs through Python API.Among the available LLMs, we chose FLAN-T5 because of its valuable trade-off between computational load and goodness of the learned representation.FLAN-T5 is the improved version of MT-5, a text-to-text general model capable of solving many NLP tasks (e.g., sentiment analysis, question answering, and machine translation), which has been improved by pre-training 57 .The peculiarity of this model is that every task they were trained on is transformed into a textto-text task.For example, while performing sentiment analysis, the output prediction is the string used in the training set to label the positive or negative sentiment of each phrase rather than a binary integer output (e.g., 0 = positive; 1 = negative).Hence, their power stands in both the generalized representation of natural language learned during the pre-training phase and the possibility of easily adapting the model to a downstream task with little fine-tuning without adjusting its architecture.</p>
<p>DeCLaRatiVE stylometric analysis</p>
<p>This study employed stylometric analysis to achieve two primary objectives.First, we aimed to describe the linguistic features that distinguished the three datasets before initializing the fine-tuning process.Second, we conducted explainability analysis to gain insights into the role of linguistic style that differentiated truthful and deceptive statements in the model's classification process.For this purpose, a new framework that we referred to as DeCLaRatiVE stylometry was adopted, which involved the extraction of 26 linguistic features in conjunction with the psychological frameworks of Distancing 29 , Cognitive Load 30,31 , Reality Monitoring 32,34 , and VErifiability Approach 40,41 .A full list of the 26 linguistic features with a short description is shown in Table 3.This comprehensive approach enabled the analysis of verbal cues of deception from a multidimensional perspective.</p>
<p>Features associated with the CL framework consisted of statistics about the length, readability, and complexity of the text 14,58-60 and were extracted using the Python library TEXTSTAT.Features related to the Distancing and RM framework were computed using LIWC 42,43 , the gold standard software for analyzing word usage.Using the English dictionary, we scored each text along with all the categories present in LIWC-22.LIWC scoring was computed on tokenized text using the English dictionary.The selection of the LIWC categories related to the Distancing and RM framework was guided by previous research on computerized verbal lie-detection 29,49,50,52,56 and a recent metanalysis 14 .RM was also investigated through linguistic concreteness of words 39 .To determine the average level of concreteness for each statement, we utilized the concreteness annotation dataset developed by Brysbaert et al. 61 .For the calculation of concreteness scores, a preprocessing pipeline was applied to textual data using the Python library SpaCy: text was converted to lowercase and tokenized; then stop words were removed, and the remaining content words were lemmatized.These content words were then cross-referenced with the annotated concreteness dataset to assign the respective concreteness value when a match was found.</p>
<p>The concreteness score for each statement was then computed as the average of the concreteness scores for all the content words in that statement.For what concerns verifiable details, they were estimated by the frequency of unique named entities.Named entities were extracted with the NER technique using Python's library SpaCy through the Transformer algorithm for English language (en_core_web_trf, https:// spacy.io/ models/ en# en_ core_ web_ trf).Further details on how the 26 linguistic features were computed are provided in the Supplementary Information.</p>
<p>Experimental set-up</p>
<p>In this section, we describe the methodology that we applied in this work.As a first step, we wanted to perform a descriptive linguistic analysis of our datasets, trying to provide a response to Hypothesis 5a), i.e., whether the linguistic style distinguishing truthful from deceptive statements varies across different contexts.To achieve this result, we employed the DeCLaRatiVE stylometric analysis.As a second step, we proceeded to test the capacity of the FLAN-T5 model to be fine-tuned on a Lie Detection task.To do so, we provided three scenarios to verify the following hypothesis:</p>
<p>• Hypothesis 1a): Fine-tuning an LLM can effectively classify the veracity of short narratives from raw texts, 1b) outperforming classical machine learning and deep learning approaches in verbal lie detection.• Hypothesis 2): Fine-tuning an LLM on deceptive narratives enables the model to also detect new types of deception; • Hypothesis 3): Fine-tuning an LLM on deceptive narratives enables the model to also detect new types of deception; • Hypothesis 4): Model performance depends on model size, with larger models showing higher accuracy;</p>
<p>We expected hypotheses 1a, 1b, 3, and 4 to be verified, while we did not have any a priori expectation for the second hypothesis.The scenarios are described below:</p>
<ol>
<li>Scenario 1: The model was fine-tuned and tested on a single dataset.This procedure was repeated for each dataset with a different copy of the same model each time (i.e., the same parameters before the fine-tuning process) (Fig. 1).This Scenario assesses the model's capacity to learn how to detect lies related to the same context and responds to Hypothesis 1a; 2. Scenario 2: The model was fine-tuned on two out of the three datasets and tested on the remaining unseen dataset.As for the previous Scenario, this procedure was iterated three times, employing separate instances of the same model, each time with a distinct combination of dataset pairings (Fig. 2).This Scenario assesses how the model performs on samples from a new context to which it has never been exposed during the training phase and provides a response for Hypothesis 2; 3. Scenario 3: We first aggregated the three train and test sets from Scenario 1.Then, we fine-tuned the model on the aggregated datasets and tested the model on the aggregated test sets (Fig. 1).This Scenario assesses the capacity of the model to learn and generalize from samples of truthful and deceptive narratives from multiple contexts and provides a response for Hypothesis 3.</li>
</ol>
<p>In Scenarios 1 and 3, each experiment underwent a 10-fold cross-validation.N-fold cross-validation is a statistical method used to estimate the performance of a model by dividing the dataset into n partitions (n = 10 for this study).For each partition i, we created a training set composed of the remaining n−1 partitions using the i partition as a test set (i.e., 90% of the data belongs to the training set, and 10% of the remaining data belongs to the test set).For each iteration, performance metrics are computed on the test set, stored, and then averaged.This procedure ensures an unbiased performance estimation and allows a fair comparison between different models.For our study, we employed identical train-test splits within scenarios 1 and 3 and for both model sizes to guarantee a fair performance comparison.The average test accuracy from each fold and its corresponding standard deviation are presented as performance metrics.Conversely, in Scenario 2, each pairing combination underwent fine-tuning using the entire two paired datasets as a training set, while the model's performance was assessed using the complete unseen dataset as a test set.</p>
<p>Notably, the Opinion dataset was developed to have each participant's truthful and deceptive statements for a total of five opinions.Therefore, we treated each opinion as a separate sample.In order to avoid the model exhibiting inflated performance on the test set as a result of learning the participants' linguistic style, we adopted the following precautionary measure.Specifically, we ensured an exclusive division of participants between the Together, Scenarios 2 and 3 provide evidence about the generalized capabilities of the fine-tuned FLAN-T5 model in a lie-detection task when tested on unseen data and on a multi-domain dataset.Furthermore, we tested whether model performance may depend on model sizes.Therefore, we first fine-tuned the small-sized version of FLAN-T5 in every scenario, and then we repeated the same experiments in every scenario with the base-sized version, providing a response for Hypothesis 4.</p>
<p>To test Hypothesis 1b, i.e., to test the advantage of our approach when compared to classical machine learning models, we decided to compare the results with two benchmarks:</p>
<p>1.A basic approach consisting of a bag-of-words (BoW) encoder plus a logistic regression classifier 62 (following the experimental procedure of Scenario 1);  www.nature.com/scientificreports/ 2. A literature baseline based on previous studies providing accuracy metrics on the same datasets using a machine learning or a deep learning approach [49][50][51] .For the Opinion dataset -characterized by opinions on five different topics per subject-we compared our results to the performance obtained in 51 with respect to their "within-topic" experiments because our approach is equivalent to theirs, with the only difference that we addressed all the topics in one model.</p>
<p>As a final step, we conducted an explainability analysis to investigate the differences in linguistic style between the truthful and deceptive statements that were correctly classified and misclassified by the model.This procedure aimed to provide a response to Hypothesis 5b, i.e., whether the model takes into account the linguistic style of statements for its final predictions.To achieve this result, we employed the DeCLaRatiVE stylometric analysis.</p>
<p>In Fig. 3, we provided a flow chart of the whole experimental set-up.</p>
<p>Fine-tuning strategy</p>
<p>Fine-tuning of LLMs consists of adapting a pre-trained language model to a specific task by further training the model on task-specific data, thereby enhancing its ability to generate contextually relevant and coherent text in line with the desired task objectives 57 .We fine-tuned FLAN-T5 in its small and base size using the three datasets and following the experimental set-up described above.We approached the lie-detection task as a binary classification problem, given that the three datasets comprised raw texts associated with a binary label, specifically instances classified as truthful or deceptive.</p>
<p>To the best of our knowledge, no fine-tuning strategy is available in the literature for this novel downstream NLP task.Therefore, our strategy followed an adaptation of the Hugginface's guidelines on fine-tuning an LLM for translation.Specifically, we chose the same optimization strategy used to pre-train the original model and the same loss function.</p>
<p>Notably, the classification task between deceptive and truthful statements has never been performed during the FLAN-T5 pre-training phase, nor is it included in any of the tasks the model has been pre-trained on.Therefore, we performed the same experiments, described in the Experimental set-up section, multiple times with different learning rate values (i.e., 1e−3, 1e−4, 1e−5), and we finally chose the configuration shown in Table 4, which yielded the best performance in terms of accuracy.All experiments and runs of the three scenarios were conducted on Google Colaboratory Pro + using their NVIDIA A100 Tensor Core GPU.</p>
<p>Statistical procedure for descriptive linguistic analysis</p>
<p>After applying the DeCLaRatiVE stylometry technique, we obtained a stylistic vector of 26 linguistic features for each text of the three datasets.</p>
<p>In order to assess the significance of the observed differences between the groups, a permutation t-test was employed 63 .This non-parametric method involves pooling all observations and then randomly redistributing them into two groups, preserving the original group sizes.The test statistic of interest (i.e., the difference in means) is then computed for these permuted groups.By repeating this process thousands of times (i.e., n = 10,000), we generated a test statistic distribution under the null hypothesis of no difference between the groups.The observed test statistic from the actual data was then compared to this distribution to compute a p-value, indicating the likelihood of observing such a difference if the null hypothesis was true.The advantage of using a permutation t-test is that no assumption about the distribution of data is needed.This analysis was conducted in Python using SciPy and Pingouin library.For the Memory and Intention dataset, we computed a permutation t-test (n = 10,000) for independent samples for the 26 linguistic features to outline significant differences among the truthful and deceptive texts.</p>
<p>For the Opinion dataset, our analysis proceeded as follows.Firstly, we computed the DeCLaRatiVE stylometry technique for all the subjects' opinions.This resulted in a 2500 (opinions) × 26 (linguistic features) matrix.Then, since each subject provided five opinions (half truthful and half deceptive), we averaged the stylistic vector separately for the truthful and deceptive sets of opinions.This procedure allowed us to obtain two different averaged stylistic vectors for the same subject, one for the truthful opinions and one for the deceptive opinions.Importantly, this averaging process enabled us to obtain results that are independent of the topic (e.g., abortion or cannabis legalization) and the stance taken by the subject (e.g., in favor or against that particular topic).Finally, we validated the statistical significance of these differences by conducting a paired sample permutation test (n = 10,000).Results for each dataset were corrected for multiple comparisons with Holm-Bonferroni correction.</p>
<p>The effect size was expressed by Common Language Effect Size (CLES) with a confidence interval of 95% (95% CI), which is a measure of effect size that is meant to be more intuitive in its understanding by providing the probability that a specific linguistic feature, in a picked-at-random truthful statement, will have a higher score than in a picked-at-random deceptive one 64 .The null value for the CLES is the chance level at 0.5 (in a probability range from 0 to 1) and indicates that, when sampled, one group will be greater than the other, with equal chance.Cohen's d effect size with 95% CI was also computed to add interpretation.</p>
<p>Statistical procedure for explainability analysis</p>
<p>To examine whether the linguistic style of the input statements exerted an influence on the resulting output of the model and to provide explanations for the wrong classification outputs, we applied a DeCLaRatiVE stylometric analysis of statements correctly classified and misclassified by the top-performing model identified in Scenario 3 (FLAN-T5 base).</p>
<p>To this aim, during each iteration from cross-validation, we paired the sentences belonging to the test set and their actual labels with the labels predicted by the model.After the cross-validation ended, for each of the ten folds and for each of the 26 linguistic features of the sentences that composed the test set for that fold, we performed a non-parametric permutation t-test for independent samples (n = 10,000) for the following comparison of interest: To compute the effect size, we computed the average of the CLES and Cohen's d effect size scores with their respective 95% CI obtained from each fold.</p>
<p>Results</p>
<p>Descriptive linguistic analysis</p>
<p>This section outlines the results of the descriptive linguistic analysis in terms of DeCLaRatiVE stylometric analysis to compare the three datasets on linguistic features.Table 4. FLAN-T5 hyperparameters configuration for the small-and base-sized version.The initial learning rate for every scenario was 5e−4 for the small model and 5e−5 for the base model.This choice was motivated by preliminary experiment results, with the smaller model, but not the base model, generally performing better with higher learning rates.The weight decay coefficient was set to 0.01 in all models and Scenarios.The batch size was set to 2 for computational reasons, specifically to avoid running out of available memory, even though it is known that a larger batch size usually leads to better performance.Finally, the number of epochs was set to 3 after preliminary experiments showing the maximum test accuracy after the third epoch without overfitting.www.nature.com/scientificreports/For the three datasets, Figs. 4, 5, and 6 show the differences in the number, the type, the magnitude of the CLES effect size, and the direction of the effect for the linguistic features that survived post-hoc corrections.</p>
<p>Model</p>
<p>To make an example of these differences, the concreteness score of words ('concr_score') presented the largest CLES within the Intention dataset towards the truthful statements (Fig. 6), while in the Opinion dataset, it showed the largest CLES towards the deceptive statements (Fig. 4).Overall, the Intentions dataset displayed fewer significant differences in linguistic features among truthful and deceptive statements than the Opinion and Memory datasets.In Table S5 (Supplementary Information), we reported, for all the linguistic features and the three datasets, all the statistics, the corrected p-values, the effect-size scores expressed by CLES and Cohen's D with 95% CI, and the direction of the effect.</p>
<p>Performance on the lie-detection classification task</p>
<p>This section presents the performance, in terms of averaged accuracy (and standard deviation) of the 10-folds, on the test sets after the last epoch of the small and base model in all the Scenarios.</p>
<p>Scenario 1</p>
<p>In Table 6 are depicted the test accuracies for the FLAN-T5 model, categorized by dataset and model size in Scenario 1.In each case, the base model, on average, outperformed the small model, with the Memory dataset showing the largest improvement of 4% and the Intention dataset showing just a 0.06% increase in average accuracy.These results indicate that the larger model size generally leads to improved performance across the three datasets, with higher accuracy observed in the base version.</p>
<p>Scenario 2</p>
<p>This scenario aimed to investigate our fine-tuned LLM's generalization capability across different deception domains.As presented in Table 5, the test accuracy for the three experiments in this scenario significantly dropped to the chance level, showing that the model, in any case, was able to learn a general rule to detect lies coming from different contexts.</p>
<p>Scenario 3</p>
<p>In Scenario 3, we tested the accuracy of the FLAN-T5 small and base version on the aggregated Opinion, Memory, and Intention datasets.The small-sized FLAN-T5 achieved an average test accuracy of 75.45% (st.dev.± 1.6), while the base-sized FLAN-T5 exhibited a higher average test accuracy of 79.31% (st.dev.± 1.3).In other words, the base-sized model outperformed the small model by approximately four percentage points.</p>
<p>Results in Table 6 show the disaggregated performance on individual datasets between the small and base FLAN-T5 models in Scenario 3, with a comparison to their counterparts in Scenario 1.These comparisons show that FLAN-T5-small in Scenario 3 exhibited worse performance than in Scenario 1. Instead, in Scenario 3, the We identified the top-performing model as the FLAN-T5 base in Scenario 3 because of its higher accuracy in the overall performance.The averaged confusion matrix of the 10 folds for this model is depicted in Fig. 7.</p>
<p>Notably, in any case, we were able to outperform both the bag of word + logistic regression classifier baseline and the performance achieved on the same datasets in previous studies [49][50][51] .</p>
<p>Explainability analysis</p>
<p>This section aims to gain a deeper understanding of the top-performing model identified in Scenario 3 (FLAN-T5 base) through a DeCLaRatiVE stylometric analysis of statements correctly classified and misclassified by the model.The purpose of this analysis was to examine whether the linguistic style of the input statements exerted an influence on the resulting output of the model and to provide explanations for the wrong classification outputs.For this analysis, we compared: The statistically significant features reported survived post-hoc correction for multiple comparisons in each fold.Overall, for comparison a), b), and c), we observed no statistically significant differences (p &lt; 0.05) in any linguistic features for most of the splits with the only exception of:   Conversely, for the d) comparison, several significant features emerged in all the folds and survived corrections for multiple comparisons.Figure 8 depicts the CLES effect size scores of linguistic features, sorted according to the number of times they were found to be significant among the ten folds.The top six features in Fig. 8 represented a cluster of linguistic features related to the Cognitive Load framework.</p>
<p>Discussion</p>
<p>In the present research, we investigated the efficacy of a Large Language Model, specifically FLAN-T5 in its small and base version, in learning and generalizing the intrinsic linguistic representation of deception across different contexts.To accomplish this, we employed three datasets encompassing genuine or fabricated statements regarding personal opinions, autobiographical experiences, and future intentions.</p>
<p>Descriptive linguistic analysis</p>
<p>Descriptive linguistic analysis was performed to compare the three datasets on linguistic features by exploring the differences in the DeCLaRatiVE style, i.e., analyzing 26 linguistic features extracted from the psychological frameworks of Distancing, Cognitive Load, Reality monitoring, and VErifiability approach.This analysis aimed to test Hypothesis 5a, which postulates a variation in the linguistic style that differentiates truthful from deceptive statements across varying contexts (i.e., personal opinions vs. autobiographical memories vs. future intentions).The results from this analysis confirmed our hypothesis, showing that the linguistic features exhibiting statistically significant differences between truthful and deceptive statements indeed varied across datasets.This variation was observed in terms of the total number and type of features, the magnitude of the effect size (from very small to medium), and the direction of the effect.In the following paragraphs, the interpretation of the significant linguistic features of each dataset will be discussed.</p>
<p>Opinions</p>
<p>After analyzing truthful and deceptive opinions using the DeCLaRatiVE stylometry, different linguistic featuresrelated to the theoretical frameworks of CL, RM, and Distancing-were found to be significant.In line with the CL framework, we observed that truthful opinions were characterized by greater complexity, verbosity, and more authenticity in linguistic style 14,31 .</p>
<p>For features related to the RM framework, truthful opinions were characterized by a lesser number of concrete words and a greater number of cognitive words, as also previously shown 55 ; in contrast, deceptive opinions showed higher scores in the concreteness of words, contextual details, and reality monitoring.These differences may reflect on one side the reasoning processes that truth-tellers engage in evaluating the pros and cons of abstract and controversial concepts (e.g., abortion), while for deceivers, it may be indicative of difficulty in abstraction, resulting in faked opinions that sound more grounded in reality.</p>
<p>Finally, in line with previous literature on distancing framework 29,65 and deceptive opinions 20,55 , deceivers utilized more other-related word classes ('Other-reference') and fewer self-related words ('Self-reference'), confirming that individuals may tend to avoid personal involvement when expressing deceptive statements.</p>
<p>Memories</p>
<p>Following the analysis of truthful and deceptive narratives of autobiographical memories through DeCLaRatiVE stylometry, various linguistic features associated with the theoretical frameworks of CL, RM, VA, and Distancing were found to be significant.</p>
<p>As for opinions, according to the CL framework, truthful narratives of autobiographical memories exhibited higher levels of complexity and verbosity and appeared to be more analytical in style 14,31 .</p>
<p>In accordance with the RM framework [32][33][34][35][36][37] , posing that truthful memory accounts tend to reflect the perceptual processes involved while experiencing the event while fabricated accounts are constructed through cognitive operations, we found genuine memories exhibiting higher scores in memory-related words and the number of words associated with spatial and temporal information ('Contextual Embedding'), as well as an overall higher RM score.Conversely, we found deceptive memories showing higher scores in words related to cognitive processes (e.g., reasoning, insight, causation).Furthermore, in line with Kleinberg's truthful concreteness hypothesis 39 , truthful memories were overall characterized by words with higher scores of concreteness.</p>
<p>Along with the VA, truthful memories contained more verifiable details, as indicated by the greater number of named entities about times and locations 23,48 .Notably, we found this effect although participants lied in a low-stake scenario.However, deceptive memories were unexpectedly characterized by a higher number of self-references and named entities of 'People' .This result is in contrast with previous literature on distancing framework 14,29 .One possible explanation of this significant but small effect is that liars may try to increase their credibility by fostering a sense of social connection.</p>
<p>Intentions</p>
<p>Upon examining truthful and deceptive statements of future intentions through DeCLaRatiVE stylometry, several linguistic features were found to be significant.Our findings are consistent with previous research claiming that genuine intentions contain more 'how-utterances' , i.e., indicators of careful planning and concrete descriptions of activities.In contrast, false intentions are characterized by 'why-utterances' , i.e., explanations and reasons base in Scenario 3. The bar plot shows the averaged Common Language Effect Size among the ten folds of linguistic features that survived post-hoc corrections.Linguistic features are sorted in descending order according to the number of times they were found to be significant among the 10 folds (displayed at the side of each bar).Linguistic features higher on average in truthful texts are shown in sky blue, while those higher on average in deceptive texts are shown in salmon.</p>
<p>for why someone planned an activity or for doing something in a certain way 48 .Indeed, we found true intentions were more likely to provide concrete and distinct information about the intended action, grounding their statements in real-world experiences and providing temporal and spatial references.Additionally, true intentions were characterized by a more analytical style and a greater presence of numerical entities.In contrast, false intentions exhibited a higher number of cognitive words and expressions and were temporally oriented toward the present and past.</p>
<p>Furthermore, we found evidence in line with the claim that liars may over-prepare their statements 48 , as indicated by higher verbosity.Finally, in contrast with the distancing framework 14,29 , we found a significantly higher proportion of self-references and mentions of people in deceptive statements.However, the effect size for this finding was small.As for deceptive memories, one possible interpretation is that liars may attempt to appear more credible by creating a sense of social connection.</p>
<p>Lie detection task</p>
<p>In order to test the capacity of the FLAN-T5 model to be fine-tuned on a Lie Detection task, we developed three scenarios.</p>
<p>In Scenario 1, we tested whether fine-tuning LLMs can effectively classify the veracity of short statements based on raw texts with performance highly above the chance level (Hypothesis 1a).To this aim, we fine-tuned FLAN-T5 in its small version to perform lie detection as a classification task.We repeated this procedure for the three datasets (i.e., opinions vs. memories vs. intentions).This fine-tuning process yielded promising results confirming our hypothesis, with an average accuracy of 80.64% (st.dev.± 2.03%) for the Opinion dataset, 76.87% (st.dev.± 2.06%) for the Memory dataset, and 71.46% (st.dev.± 3.65%) for the Intention dataset.</p>
<p>In Scenario 2, we tested whether fine-tuning an LLM on deceptive narratives enables the model to detect new types of deception (Hypothesis 2).To verify this hypothesis, we fine-tuned FLAN-T5 (small version) on two datasets and tested on the third one (e.g., train: opinion + memory; test: intention).Our findings show that the model performed at chance level in all three combinations of this Scenario, suggesting that there are no universal rules the model can learn to distinguish truthful from deceptive statements, enabling a generalization of the task across different contexts.Indeed, as shown in the Descriptive Linguistic Analysis section, the three datasets differed significantly in terms of the content and the linguistic style by which truthful and deceptive narratives are delivered.Therefore, the model struggled to identify a specific pattern of linguistic deception and appeared to engage a domain-specific learning, tailoring its classification capabilities to that specific domain of deception.</p>
<p>In Scenario 3, we tested whether fine-tuning an LLM on a multiple-context dataset enables the model to obtain successful predictions on a multi-context test set (Hypothesis 3).At this aim, we fine-tuned and tested FLAN-T5 (small version) with the three aggregated datasets (i.e., opinion + memory + intention).The small-sized FLAN-T5 achieved an average accuracy of 75.45% (st.dev.± 1.6).Additionally, the disaggregated performance on individual datasets compared to their counterpart in Scenario 1 exhibited solely a small decrease in accuracy (around 1%).These findings confirmed our hypothesis, providing evidence of LLMs' ability to generalize when fine-tuned and texted on a multi-context dataset, in contrast to previous empirical evidence showing a decline in performance in machine learning models on the same scenarios [53][54][55] .</p>
<p>To test whether the model performance increases when employing larger models (Hypothesis 4), we repeated the same experiments in Scenarios 1, 2, and 3 with the base version of FLAN-T5.</p>
<p>In Scenario 1, we found that the base version of FLAN-T5 provided higher accuracy than the small version.In Scenario 3, the base version of the model achieved an average accuracy of 79.31% (st.dev.± 1.3), outperforming the small model by approximately four percentage points.Additionally, this increase in the general accuracy did not compromise the performance on any individual dataset when compared to what achieved by the smaller model or by the FLAN-T5 base in Scenario 1.In contrast, the base version of FLAN-T5 in Scenario 2 still obtained performance around the chance level.</p>
<p>On one hand, the findings obtained from the base model in Scenarios 1 and 3 confirmed the hypothesis that the model size does influence the performance, likely because a bigger model is able to learn a better representation of linguistic patterns of genuine and deceptive narratives.Specifically, in Scenario 3, the FLAN-T5 base, with its larger size, possessed the capability to comprehend and integrate the features of the three distinct datasets altogether, thereby maintaining consistent performance across all individual datasets.In contrast, the smaller FLAN-T5 in Scenario 3 seemed to relinquish certain specialized abilities that are beneficial for specific datasets to classify deception across different contexts.</p>
<p>On the other hand, findings from Scenarios 2 and 3 (with small and base FLAN-T5) showed that LLMs, despite having acquired a comprehensive understanding of language patterns, still require exposure to prior examples to accurately classify deceptive texts within different domains.</p>
<p>Finally, to test whether our approach outperforms classical machine learning and deep learning approaches in verbal lie detection (Hypothesis 1b), we compared the results obtained from FLAN-T5 in its small and base versions with the performance of a simpler baseline of a logistic regressor based on BoW embedding 62 and of Transformer models previously employed in the literature on the Opinion 51 and Intention datasets 49,50 .</p>
<p>Specifically, when comparing the Memory dataset to the logistic regression baseline, there was a 32% increase in performance.This improvement might be attributed to the longer and more complex nature of the stories in the Memory dataset, which challenges the effectiveness of more straightforward methods like logistic regression based on BoW in a lie detection task.In contrast, LLMs already possess a robust language representation; thus, fine-tuning LLMs leverages this representation, tailoring their NLP proficiency specifically for a lie detection task, yielding higher accuracy.</p>
<p>The performance gained by fine-tuning LLMs was less pronounced for the Opinion and Intention datasets.For the Opinion dataset, this could be due to the relative ease of classification in these datasets, where simpler www.nature.com/scientificreports/models can already achieve good performance, leaving a smaller margin for improvement.Nonetheless, the difference between our approach and the baselines is not negligible.In the Opinion dataset, we outperformed the literature baseline of a Transformer model trained from scratch by 17% accuracy and surpassed our logistic regression baseline by six percentage points.For the Intention dataset, our approach showed a 5-percentage point improvement over the logistic regression baseline and around 1-2% improvement over the best literature baseline.Notably, the best literature baseline for the Intention dataset (averaged accuracy: 70.61 ± 2.58%) used a similar approach to ours in terms of the type of model used, involving a Transformer-based model (BERT + Coattention), which may explain the narrower performance gap.Besides the differences in performance, the main advantage of our approach is its simplicity and flexibility compared to those used in previous studies [49][50][51] .Fine-tuning an LLM leverages an existing encoding of language that effortlessly handles any type of statement, unlike logistic regression based on BoW or training a new Transformer-based model from scratch.Taking all these aspects together, fine-tuning LLMs resulted in being more advantageous in terms of feasibility, flexibility, and performance accuracy.</p>
<p>Explainability analysis</p>
<p>To improve the explainability of the performance collected, we investigated whether the linguistic style that characterizes truthful and deceptive narratives could have a role in the model's final predictions (Hypothesis 5b).For this aim, we applied a DeCLaRatiVE stylometric analysis on statements that were correctly classified and misclassified by the top-performing model identified in Scenario 3 (i.e., FLAN-T5 base).</p>
<p>In the misclassified sample, truthful and deceptive statements did not differ significantly for any linguistic feature extracted with the DeCLaRatiVE stylometry technique.The only exception was fold 1, which showed significant differences in the text's readability score, and fold 6, which showed significant differences in 'Reality Monitoring' scores.No significant differences were detected in each fold in linguistic features between deceptive statements that were correctly classified as deceptive (True Negatives) and truthful statements that were misclassified as deceptive (False Negatives), with the exception of 'Reality Monitoring' in folds 6 and 7 and 'Contextual Embedding' score in fold 7. Finally, truthful statements that were correctly classified as truthful (True Positives) and deceptive statements that were misclassified as truthful (False Positives) exhibited no significant differences, except for the number of syllables and number of words in the fold 9. We argue that the observation of significant differences in selected linguistic features across specific folds is more indicative that these findings may not be generalizable and are likely influenced by the particular fold under analysis.When taken together, most of the analyzed folds showed a substantial overlap in linguistic style.Consequently, the model might have exhibited poor classification performance for those statements because, while deceptive, they showed a linguistic style resembling truthful statements and vice-versa.</p>
<p>In contrast, correctly classified statements displayed several significant differences between truthful and deceptive statements.Notably, the top six linguistic features in Fig. 8 resulted in statistical significance in at least 6 out of 10 folds.The fact that we found a consistent pattern of linguistic features in correctly classified statements but not in misclassified statements provides evidence for our hypothesis, suggesting that the linguistic style of statements does have a role in the model's final predictions.More in detail, the top-six linguistic features depicted in Fig. 8 represent a cluster of linguistic cues associated with the CL framework 31 , specifically low-level features related to the length, complexity, and analytical style of the texts that may have enabled the distinction between truthful and deceptive statements.The fact that linguistic cues of CL survived among the several features available -in a mixed dataset of utterances reflecting opinions, memories, and intentions-raises the question of whether CL cues may be more generalizable than other cues that are, in contrast, more specific to a particular type of deception.</p>
<p>Conclusion, limitations, and further work</p>
<p>At the time of writing and to the best of our knowledge, this is the first study involving the use of an LLM for a lie-detection task.</p>
<p>LLMs are Transformer-based models trained on large corpora of text that have proven to generate coherent text in human natural language and have extreme flexibility in a wide range of NLP tasks 28 .In addition, these models can be further fine-tuned on specific tasks using smaller task-specific datasets, achieving state-of-the-art results 28 .In this study, we tested the ability of a fine-tuned LLM (FLAN-T5) on lie-detection tasks.</p>
<p>First, given the extreme flexibility of LLM, we tested whether fine-tuning a LLM is a valid procedure to detect deception from raw texts above chance level and outperform the classical machine and deep learning approaches.We found that fine-tuning FLAN-T5 on a single dataset is a valid procedure to obtain a state-of-the-art accuracy, as proved by the fact that this procedure outperformed the baseline model (BoW + logistic regression) and previous works that applied machine and deep learning techniques on the same datasets [49][50][51]62 .</p>
<p>Second, we wanted to investigate whether fine-tuning an LLM on deceptive narratives enables the model to also detect new types of deceptive narratives.Findings from Scenario 2 disconfirms this hypothesis, suggesting that the model requires previous examples of different deceptive narratives to provide adequate accuracy in this classification task.</p>
<p>Third, we investigated whether it is possible to successfully fine-tune an LLM on a multiple-context dataset.Results from Scenario 3 confirm that fine-tuned LLM may provide adequate accuracy in detecting deception from different contexts.We also found that fine-tuning on multiple datasets can increase the performance with respect to when fine-tuned on a single dataset.</p>
<p>Furthermore, we hypothesized that the model performance may depend on the model size, given that the larger the model, the better the model forms its inner representation of language.Results from Scenario 1 and 3 confirmed that the base-sized model of FLAN-T5 provides higher accuracy than the small-sized version.www.nature.com/scientificreports/Finally, with our experiments, we introduced the DeCLaRatiVE stylometry technique, a new theory-based stylometric approach to investigate deception in texts from four psychological frameworks (Distancing, Cognitive Load, Reality Monitoring, and Verifiability approach).We employed the DeCLaRatiVE stylometry technique to compare the three datasets on linguistic features and we found that fabricated statements from different contexts exhibit different linguistic cues of deception.We also employed the DeCLaRatiVE stylometry technique to conduct an explainability analysis and investigate whether the linguistic style by which truthful or deceptive narratives are delivered is a feature that the model takes into account for its final prediction.At this aim, we compared correctly classified and misclassified statements by the top-performing model (FLAN-T5 base in Scenario 3), finding that correctly classified statements share linguistic features related to the cognitive load theory.In contrast, truthful and deceptive misclassified statements do not present significant differences in linguistic style.</p>
<p>Given the results achieved, we highlight the importance of a diversified dataset to achieve a generalized good performance.We also considered crucial the balance between the diversity of the dataset and the size of the LLM, suggesting that the more diverse the dataset is, the bigger the model required to achieve higher-level accuracy.The main advantage of our approach consists of its applicability to raw text without the need for extensive training or handcrafted features.</p>
<p>Despite the demonstrated success of our model, three significant limitations impact the ecological validity of our findings and their practical application in real-life scenarios.</p>
<p>The first notable limitation pertains to the narrow focus of our study, which concentrated solely on lie detection within three specific contexts: personal opinions, autobiographical memories, and future intentions.This restricted scope limits the possibility of accurately classify deceptive texts within different domains.A second limitation is that we exclusively considered datasets developed in experimental set-ups designed to collect genuine and completely fabricated narratives.However, individuals frequently employ embedded lies in real-life scenarios, in which substantial portions of their narratives are true, rather than fabricating an entirely fictitious story.Finally, the datasets employed in this study were collected in experimental low-stake scenarios where participants had low incentives to lie and appear credible.Because of all the above issues, the application of our model in real-life contexts may be limited, and caution is advised when interpreting the results in such situations.</p>
<p>The limitations addressed in this study underscore the need for future research to expand the applicability and generalizability of lie-detection models for real-life settings.Future works may explore the inclusion of new datasets, trying different LLMs (e.g., the most recent GPT-4), different sizes (e.g., FLAN-T5 XXL version), and different fine-tuning strategies to investigate the variance in performance within a lie-detection task.Furthermore, our fine-tuning approach completely erased the previous capabilities possessed by the model; therefore, future works should also focus on new fine-tuning strategies that do not compromise the model's original capabilities.</p>
<p>13:22849 | https://doi.org/10.1038/s41598-023-50214-0www.nature.com/scientificreports/training and test sets, such that any individual who had their opinions assigned to the training set did not have their opinions assigned to the test set, and vice versa.</p>
<p>Figure 1 .
1
Figure 1.Visual illustration of the Scenarios 1 and 3.</p>
<p>Figure 2 .
2
Figure 2. Visual illustration of the Scenario 2.</p>
<p>Figure 3 .
3
Figure 3. Visual illustration of the whole experimental set-up.The Opinion, Memory, and Intention dataset underwent Descriptive Linguistic Analysis using DeCLaRatiVE stylometry.A baseline model consisting of Bag of Words (BoW) and Logistic Regression (Scenario 1) was also established for the three datasets.Then, the FLAN-T5 model in small and base versions was fine-tuned across Scenarios 1, 2, and 3. Finally, an Explainability Analysis was conducted on the top-performing model using DeCLaRatiVE stylometry to interpret the results.</p>
<p>a. Truthful statements misclassified as deceptive (False Negatives), with deceptive statements misclassified as truthful (False Positives); b.Statements correctly classified as deceptive (True Negatives) vs. truthful statements misclassified as deceptive (False Negatives); c. Statements correctly classified as truthful (True Positives) vs. deceptive statements misclassified as truthful (False Positives).d.Truthful versus deceptive statements correctly classified by the model (True Positives vs. True Negatives).</p>
<p>Figure 4 .
4
Figure 4. Horizontal stacked bar chart presenting the Common Language Effect Size (CLES) estimates for the significant linguistic features that survived post-hoc corrections in the Opinion dataset.The CLES estimates represent the probability (ranging from 0 to 1) of finding a specific linguistic feature in truthful opinions (sky blue) than in deceptive ones (salmon).The CLES for truthful opinions are sorted in descending order, while the CLES for deceptive opinions are sorted in ascending order.</p>
<p>Figure 5 .
5
Figure5.Horizontal stacked bar chart presenting the Common Language Effect Size (CLES) estimates for the significant linguistic features that survived post-hoc corrections in the Memory dataset.The CLES estimates represent the probability (ranging from 0 to 1) of finding a specific linguistic feature in truthful memories (sky blue) than in deceptive ones (salmon).The CLES for truthful memories are sorted in descending order, while the CLES for deceptive memories are sorted in ascending order.</p>
<p>Figure 6 .
6
Figure 6.Horizontal stacked bar chart presenting the Common Language Effect Size (CLES) estimates for the significant linguistic features that survived post-hoc corrections in the Intention dataset.The CLES estimates represent the probability (ranging from 0 to 1) of finding a specific linguistic feature in truthful intentions (sky blue) than in deceptive ones (salmon).The CLES for truthful intentions are sorted in descending order, while the CLES for deceptive intentions are sorted in ascending order.</p>
<p>a. Truthful statements misclassified as deceptive (False Negatives), with deceptive statements misclassified as truthful (False Positives); b.Statements correctly classified as deceptive (True Negatives) vs. truthful statements misclassified as deceptive (False Negatives); c. Statements correctly classified as truthful (True Positives) vs. deceptive statements misclassified as truthful (False Positives).d.Truthful vs. deceptive statements correctly classified by the model (True Positives vs. True Negatives).</p>
<p>1) 'fk_read' in fold 1 (t = 5.30; p = 0.04, CLES = 0.63 [0.55, 0.71], d = 0.46 [0.18, 0.75]) and 'Reality Monitoring'in fold 6 (t = 4.74; p = 0.047, CLES = 0.62 [0.54, 0.70], d = 0.46 [0.17, 0.75]) for the a) comparison; 2) 'Reality Monitoring'in fold 6 (t = −3.39,p = 0.04, CLES = 0.40 [0.34, 0.46], d = −0.34[−0.55, −0.13]) and 'Reality Monitoring' (t = −3.16p = 0.04, CLES = 0.41 [0.34, 0.47], d = −0.34[−0.56, −0.12]) and 'Contextual Embedding' (t = −2.11;p = 0.01, CLES = 0.39 [0.33, 0.45], d = −0.42[−0.63, −0.2]) in fold 7 the b) comparison;</p>
<p>www.nature.com/scientificreports/ 3) 'num_syllables'(t = 76.87,p = 0.01, CLES = 0.64 [0.57, 0.7], d = 0.46 [0.27, 0.7]) and 'word_counts'(t = 59.63, p = 0.01, CLES = 0.64 [0.57, 0.71], d = 0.46 [0.21, 0.7]) in fold 9 for the c) comparison.</p>
<p>Figure 7 .
7
Figure 7. Averaged confusion matrix of the top-performing model identified as FLAN-T5 base in Scenario 3. In each square, the results obtained represent the average (and standard deviation) from the test set of each iteration of the 10-fold cross-validation.</p>
<p>Figure 8 .
8
Figure 8. Linguistic features in Truthful and Deceptive statements that were accurately classified by FLAN-T5base in Scenario 3. The bar plot shows the averaged Common Language Effect Size among the ten folds of linguistic features that survived post-hoc corrections.Linguistic features are sorted in descending order according to the number of times they were found to be significant among the 10 folds (displayed at the side of each bar).Linguistic features higher on average in truthful texts are shown in sky blue, while those higher on average in deceptive texts are shown in salmon.</p>
<p>https://doi.org/10.1038/s41598-023-50214-0</p>
<p>https://doi.org/10.1038/s41598-023-50214-0</p>
<p>Table 2 .
2
Summary statistics of the number of words for each dataset and truthful and deceptive set of statements.Jaccard Similarity Index and its qualitative interpretation in brackets refers to the similarity between truthful and deceptive vocabulary sets for each dataset.
Dataset</p>
<p>(total number) Min-Max number of words Average number of words (SD) Jaccard similarity Index (qualitative interpretation)
All opinions (2500)6-33859.05 (30.66)Truthful opinions (1250)7-33866.74 (31.95)0.35 (low similarity)Deceptive opinions (1250)6-23251.36 (27.24)All intentions (1640)15-25150.44 (30.11)Truthful intentions (783)15-20647.04 (28.36)0.34 (low similarity)Deceptive intentions (857)15-25153.55 (31.31)All memories (5506)22-625255.24 (92.36)Truthful memories (2770)22-625269.78 (94.14)0.34 (low similarity)Deceptive memories (2736)22-609240.51 (88.12)Scientific Reports|(2023) 13:22849 |https://doi.org/10.1038/s41598-023-50214-0Vol.:(0123456789)</p>
<p>Table 3 .
3
List and short description of the 26 linguistic features pertaining to the DeCLaRatiVE Stylometry technique.
LabelDescriptionnum_sentencesTotal number of sentencesnum_wordsTotal number of wordsnum_syllablesTotal number of syllablesavg_syllabes_per_wordAverage number of syllables per wordfk_gradeIndex of the grade level required to understand the textfk_readIndex of the readability of the textAnalyticLIWC summary statistic analyzing the style of the text in term of analytical thinking (0-100)AuthenticLIWC summary statistic analyzing the style of the text in term of authenticity (0-100)ToneStandardized difference (0-100) of 'tone_pos'-'tone_neg'tone_posPercentage of words related to a positive sentiment (LIWC dictionary)tone_negPercentage of words related to a negative sentiment (LIWC dictionary)CognitionPercentage of words related to semantic domains of cognitive processes (LIWC dictionary)memoryPercentage of words related to semantic domains of memory/forgetting (LIWC dictionary)focuspastPercentage of verbs and adverbs related to the past (LIWC dictionary)focuspresentPercentage of verbs and adverbs related to the present (LIWC dictionary)focusfuturePercentage of verbs and adverbs related to the future (LIWC dictionary)Self-referenceSum of LIWC categories 'i' + 'we'Other-referenceSum of LIWC categories 'shehe' + 'they' + 'you'Perceptual detailsSum of LIWC categories 'attention' + 'visual' + 'auditory' + 'feeling'Contextual EmbeddingSum of LIWC categories 'space' + 'motion' + 'time'Reality MonitoringSum of Perceptual details + Contextual Embedding + Affect-CognitionConcreteness scoreMean of concreteness score of words
PeopleUnique named-entities related to people: e.g., 'Mary' , 'Paul' , ' Adam'Temporal detailsUnique named-entities related to time: e.g., 'Monday' , '2:30 PM' , 'Christmas'Spatial detailsUnique named-entities related to space: e.g., 'airport' , 'Tokyo' , 'Central park'Quantity detailsUnique named-entities related to quantities: e.g., '20%' , '5 $' , 'first' , 'ten' , '100 m'</p>
<p>Table 5 .
5
Test accuracy of FLAN-5 models in scenario 2 (three combination of train sets).The performance comparison is among the small and base version of the FLAN-T5 model in the three combination of train set: opinion + memory, opinion + intention, memory + intention.
Train setTest setModel sizeTest accuracyOpinion + MemoryIntentionFLAN-T5 small FLAN-T5 base55.37 55.67Opinion + IntentionMemoryFLAN-T5 small FLAN-T5 base55.37 54.23Memory + IntentionOpinionFLAN-T5 small FLAN-T5 base53.12 49.40</p>
<p>Table 6 .
6
50st acccuracy of the FLAN-T5 models in Scenarios 1 and 3 for the three datasets.Reported values are means ± standard deviation of the 10 folds.Best results per evaluation metric are in bold.The literature baseline for the Opinion dataset refers to the average accuracy and standard deviation from all within-topic accuracies from FastText Embedding + Transformer51.The literature baseline for the Intention dataset refers to the accuracy from Vanilla Random Forest using LIWC features (confidence interval in square brackets)49, the averaged accuracy and standard deviation from RoBERTa + Transformers + Co-Attention model and BERT + co-attention model50respectively.
ModelOpinionMemoryIntentionBag-of-words baseline76.16 ± 2.9%57.57 ± 7.66%67.07 ± 3.18%69.00 [63; 74] %Literature baseline65.16 ± 5.7%-69.86 ± 2.34%70.61 ± 2.58%FLAN-T5 small-Scenario 180.64 ± 2.03%76.87 ± 2.06%71.46 ± 3.65%FLAN-T5 base-Scenario 182.60 ± 3.01%80.61 ± 1.41%71.52 ± 2.21%FLAN-T5 small-Scenario 379 ± 2.11%75.67 ± 1.90%69.32 ± 3.75%FLAN-T5 base-Scenario 382.72 ± 2.39%79.87 ± 1.60%72.25 ± 2.86%
Vol.:(0123456789) Scientific Reports | (2023) 13:22849 | https://doi.org/10.1038/s41598-023-50214-0</p>
<p>Scientific Reports | (2023) 13:22849 | https://doi.org/10.1038/s41598-023-50214-0
AcknowledgementsWe would like to thank Bruno Verschuere and Bennett Kleinberg for sharing the full version of their Intention dataset with us.Data availabilityFor the Opinion dataset, we obtained full access after contacting the corresponding author.The Memory dataset is downloadable at the link: https:// msrop endata.com/ datas ets/ 0a83f 6f-a759-4a17-aaa2-fac8 45773 18.The intention dataset is publicly available at the link: https:// osf.io/ 45z7e/.Code availabitityAll the Colab Notebooks to perform linguistic analysis on the three datasets, fine-tune the model in the three Scenarios, and conduct explainability analysis is available at https:// github.com/ robec oder/ Verba lLieD etect ionWi thLLM.git.Author contributionsG.S. conceptualized the research.R.L., R.R., P.C., and G.S. designed the research.P.C. shared the updated version of the Deceptive Opinion Dataset.R.L. performed the descriptive linguistic analysis and explainability analysis.R.R. developed and implemented the fine-tuning strategy.R.L. and R.R. wrote the paper.P.P. and G.S. supervised all aspects of whole the research and provided critical revisions.Competing interestsThe authors declare no competing interests.
10.1038/s41598-023-50214-0(1234567890) Scientific Reports | (2023) 13:22849 |. </p>
<p>. Scientific Reports, | , 10.1038/s41598-023-50214-020231322849</p>
<p>A social-cognitive framework for understanding serious lies: Activationdecision-construction-action theory. References 1 Walczyk, J J Harris, L L Duck, T K Mulay, D , 10.1016/j.newideapsych.2014.03.001New Ideas Psychol. 342014</p>
<p>Undeutsch hypothesis and criteria based content analysis: A meta-analytic review. B G Amado, R Arce, F Fariña, 10.1016/j.ejpal.2014.11.002Eur J Psychol Appl Legal Context. 72015</p>
<p>Verbal lie detection: Its past, present and future. A Vrij, 10.3390/brainsci12121644Brain Sciences. 121644. 2022</p>
<p>Which lie detection tools are ready for use in the criminal justice system?. A Vrij, R P Fisher, 10.1016/j.jarmac.2016.06.014J. Appl. Res. Mem. Cognit. 52016</p>
<p>Cues to deception. B M Depaulo, 10.1037/0033-2909.129.1.74Psychol. Bull. 1292003</p>
<p>Accuracy of deception judgments. C F BondJr, B M Depaulo, 10.1207/s15327957pspr1003_2Personal. Soc. Psychol. Rev. 102006</p>
<p>Accuracy in detecting truths and lies: Documenting the "veracity effect. T R Levine, H S Park, S A Mccornack, 10.1080/03637759909376468Commun. Monogr. 66681999</p>
<p>Truth-default theory (TDT). T R Levine, 10.1177/0261927x14535916J. Lang. Soc. Psychol. 332014</p>
<p>The source of the truth bias: Heuristic processing?. C N H Street, J Masip, 10.1111/sjop.12204Scand. J. Psychol. 562015</p>
<p>The use-the-best heuristic facilitates deception detection. B Verschuere, 10.1038/s41562-023-01556-2Nat. Hum. Behav. 72023</p>
<p>Authorship similarity detection from email messages. X Chen, P Hao, R Chandramouli, K P Subbalakshmi, 10.1007/978-3-642-23199-5_28International Workshop On Machine Learning and Data Mining. Pattern Recognition, P Editor, Perner, New York, NYSpringer2011</p>
<p>Dark web: Exploring and mining the dark side of the web. H Chen, European Intelligence and Security Informatics Conference. 2011. 2011IEEE</p>
<p>Explanation in computational stylometry. W Daelemans, 10.1007/978-3-642-37256-8_37Computational Linguistics and Intelligent Text Processing. BerlinSpringer2013</p>
<p>10.1038/s41598-023-50214-0(1234567890) Scientific Reports | (2023) 13:22849 |. </p>
<p>Are computers effective lie detectors? A meta-analysis of linguistic cues to deception. V Hauch, I Blandón-Gitlin, J Masip, S L Sporer, 10.1177/1088868314556539Personal. Soc. Psychol. Rev. 195565392015</p>
<p>Computational measures of deceptive language. F Tomas, O Dodier, S Demarchi, 10.3389/fcomm.2022.792378Prospects and issues. Front. Commun. 72022</p>
<p>Automatic deception detection: Methods for finding fake news. N K Conroy, V L Rubin, Y Chen, 10.1002/pra2.2015.145052010082Proc. Assoc. Inf. Sci. Technol. 528220100. 2015</p>
<p>Automatic detection of fake news. V Pérez-Rosas, B Kleinberg, A Lefevre, R Mihalcea, arXiv:1708.071042017arXiv preprint</p>
<p>Automatic deception detection in Italian court cases. T Fornaciari, M Poesio, 10.1007/s10506-013-9140-4Artif. Intell. Law. 212013</p>
<p>Automatic detection of deception in child-produced speech using syntactic complexity features. M Yancheva, F Rudzicz, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. the 51st Annual Meeting of the Association for Computational Linguistics2013</p>
<p>Experiments in open domain deception detection. V Pérez-Rosas, R Mihalcea, 10.18653/v1/d15-1133Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Finding deceptive opinion spam by any stretch of the imagination. M Ott, Y Choi, C Cardie, J T Hancock, arXiv:1107.45572011arXiv preprint</p>
<p>Identifying fake Amazon reviews as learning from crowds. T Fornaciari, M Poesio, 10.3115/v1/e14-1030nProceedings of the 14th Conference of the European Chapter. the 14th Conference of the European Chapterthe Association for Computational Linguistics2014</p>
<p>Using named entities for computer-automated verbal deception detection. B Kleinberg, M Mozes, A Arntz, B Verschuere, 10.1111/1556-4029.13645Journal of forensic sciences. 632017</p>
<p>Hybrid text-based deception models for native and Non-Native English cybercriminal networks. A V Mbaziira, J H Jones, 10.1145/3093241.3093280Proceedings of the International Conference on Compute and Data Analysis. the International Conference on Compute and Data Analysis20173093280</p>
<p>Linguistic cues to deception and perceived deception in interview dialogues. S I Levitan, A Maredia, J Hirschberg, 10.18653/v1/n18-1176Proceedings of the 2018 Conference of the North American Chapter. the 2018 Conference of the North American Chapterthe Association for Computational Linguistics20181</p>
<p>An investigation on the detectability of deceptive intent about flying through verbal deception detection. B Kleinberg, G Nahari, A Arntz, B Verschuere, 10.1525/collabra.80Collabra: Psychol. 2017</p>
<p>Deception detection with machine learning: A systematic review and statistical analysis. A S Constâncio, D F Tsunoda, H Silva, F N De, J M Silveira, Da, D R Carvalho, 10.1371/journal.pone.0281323PLOS ONE. 18232023</p>
<p>A survey of large language models. W X Zhao, arXiv:2303.182232023arXiv preprint</p>
<p>Lying words: Predicting deception from linguistic styles. M L Newman, J W Pennebaker, D S Berry, J M Richards, 10.1177/0146167203029005010Personal. Soc. Psychol. Bull. 2950102003</p>
<p>Covert lie detection using keyboard dynamics. M Monaro, 10.1038/s41598-018-20462-6Sci Rep. 81976. 2018</p>
<p>A cognitive approach to lie detection: A meta-analysis. A Vrij, R P Fisher, H Blank, 10.1111/lcrp.12088Legal Criminol. Psychol. 2212015</p>
<p>Reality monitoring. M K Johnson, C L Raye, 10.1037/0033-295x.88.1.67Psychol. Rev. 881981</p>
<p>The less travelled road to truth: Verbal cues in deception detection in accounts of fabricated and self-experienced events. S L Sporer, 10.1002/(SICI)1099-0720(199710)11:5%3c373::AID-ACP461%3e3.0.CO;2-0AID-ACP461% 3e3.0. CO;2-0Appl. Cognit. Psychol. 115199710. 1997</p>
<p>Reality monitoring and detection of deception in The Detection of Deception in Forensic Contexts. S L Sporer, 10.1017/cbo9780511490071.0042004Cambridge University Press</p>
<p>The detection of deception with the reality monitoring approach: A review of the empirical evidence. J Masip, S L Sporer, E Garrido, C Herrero, 10.1080/10683160410001726356Psychol. Crime Law. 11110683 16041 00017 26356. 2005</p>
<p>Criteria-Based Content Analysis (CBCA) reality criteria in adults: A meta-analytic review. B G Amado, R Arce, F Fariña, M Vilariño, 10.1016/j.ijchp.2016.01.002Int. J. Clin. Health Psychol. 1622016</p>
<p>Reality monitoring: A meta-analytical review for forensic practice. Y Gancedo, F Fariña, D Seijo, M Vilariño, R Arce, 10.5093/ejpalc2021a10Eur. J. Psychol. Appl. Legal Context. 1322021</p>
<p>Verbal lie detection: its past, present and future. A Vrij, 10.3390/brainsci12121644Brain Sci. 121216442022</p>
<p>Detecting deceptive communication through linguistic concreteness. B Kleinberg, I Van Der Vegt, A Arntz, 10.31234/osf.io/p3qjh2019</p>
<p>Exploiting liars' verbal strategies by examining the verifiability of details. G Nahari, A Vrij, R P Fisher, 10.1111/j.2044-8333.2012.02069.xLegal Criminol. Psychol. 192012</p>
<p>The verifiability approach. A Vrij, G Nahari, 10.4324/9781315160276-7Evidence-Based Investigative Interviewing. 2019</p>
<p>Linguistic inquiry and word count: LIWC. J W Pennebaker, M E Francis, R J Booth, 2001. 2001Lawrence Erlbaum Associates712001</p>
<p>The development and psychometric properties of LIWC-22. R L Boyd, A Ashokkumar, S Seraj, J W Pennebaker, 2022Austin, TXUniversity of Texas at Austin</p>
<p>Language of lies in prison: Linguistic classification of prisoners' truthful and deceptive natural language. G D Bond, A Y Lee, 10.1002/acp.1087Appl. Cognit. Psychol. 1932005</p>
<p>Lyin' Ted' , 'crooked hillary' , and 'Deceptive Donald': Language of lies in the 2016 US presidential debates. G D Bond, 10.1002/acp.3376Appl. Cognit. Psychol. 3162017</p>
<p>king of whoppers': Reality monitoring and verbal deception in the 2020 U.S. presidential election debates. G D Bond, L F Speller, L L Cockrell, K G Webb, J L Sievers, Joe ' Sleepy, ' Donald, 10.1177/00332941221105212Psychol. Rep. 94122122022</p>
<p>Man versus Machine: Comparing manual with LIWC coding of perceptual and contextual details for verbal lie detection. M Schutte, G Bogaard, E Mac Giolla, L Warmelink, B Kleinberg, B Verschuere, 10.31234/osf.io/cth582021Center for Open Science</p>
<p>Automated verbal credibility assessment of intentions: The model statement technique and predictive modeling. B Kleinberg, Y Van Der Toolen, A Vrij, A Arntz, B Verschuere, 10.1002/acp.3407Appl. Cognit. Psychol. 3234072018</p>
<p>How humans impair automated deception detection performance. B Kleinberg, B Verschuere, 10.1016/j.actpsy.2020.103250Acta Psychol. 2132021</p>
<p>Explainable verbal deception detection using transformers. L Ilias, F Soldner, B Kleinberg, arXiv:2210.030802022arXiv preprint</p>
<p>DecOp: A multilingual and multi-domain corpus for detecting deception in typed text. P Capuozzo, I Lauriola, C Strapparava, F Aiolli, G Sartori, Proceedings of the 12th Language Resources and Evaluation Conference. the 12th Language Resources and Evaluation Conference2020</p>
<p>Quantifying the narrative flow of imagined versus autobiographical stories. M Sap, 10.1073/pnas.2211715119Proc. Natl. Acad. Sci. 119(45), e2211715119. Natl. Acad. Sci. 119(45), e22117151192022</p>
<p>. Scientific Reports, | , 10.1038/s41598-023-50214-020231322849</p>
<p>Cross-domain deception detection using support vector networks. Á Hernández-Castañeda, H Calvo, A Gelbukh, J J G Flores, 10.1007/s00500-016-2409-2Soft Comput. 212016</p>
<p>Cross-cultural deception detection. V Pérez-Rosas, R Mihalcea, 10.3115/v1/p14-2072Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. the 52nd Annual Meeting of the Association for Computational Linguistics2014</p>
<p>The lie detector: Explorations in the automatic recognition of deceptive language. R Mihalcea, C Strapparava, 10.3115/1667583.1667679Proceedings of the ACL-IJCNLP 2009 conference short papers 309-312. the ACL-IJCNLP 2009 conference short papers 309-312200979</p>
<p>Beyond modelling: Understanding mental disorders in online social media. E A Ríssola, M Aliannejadi, F Crestani, Advances in Information Retrieval: 42nd European Conference on IR Research. Lisbon, PortugalSpringerApril 14-17, 2020. 20202020Proceedings, Part I 42</p>
<p>Scaling instruction-finetuned language models. H W Chung, arXiv:2210.114162022arXiv preprint</p>
<p>Automating linguistics-based cues for detecting deception in text-based asynchronous computer-mediated communications. L Zhou, J K Burgoon, J F Nunamaker, D Twitchell, 10.1023/b:grup.0000011944.62889.6fGroup Decis. Negot. 132004</p>
<p>Analysing deception in witness memory through linguistic styles in spontaneous language. S Solà-Sales, C Alzetta, C Moret-Tatay, F Dell'orletta, 10.3390/brainsci13020317Brain Sci. 133172023</p>
<p>Truth or lie: Exploring the language of deception. J Sarzynska-Wawer, A Pawlak, J Szymanowska, K Hanusz, A Wawer, 10.1371/journal.pone.0281179PLOS ONE. 18792023</p>
<p>M Brysbaert, A B Warriner, V Kuperman, 10.3758/s13428-013-0403-5Concreteness ratings for 40 thousand generally known English word lemmas. 201446</p>
<p>Y C Lin, S A Chen, J J Liu, C J Lin, arXiv:2306.07111Linear Classifier: An Often-Forgotten Baseline for Text Classification. 2023arXiv preprint</p>
<p>Bootstrapping, permutation testing and the method of surrogate data. J H Moore, Phys. Med. Biol. 446L111999</p>
<p>A common language effect size statistic. K O Mcgraw, S P Wong, 10.1037/0033-2909.111.2.361Psychol. Bull. 1113611992</p>
<p>On lying and being lied to: A linguistic analysis of deception in computermediated communication. J T Hancock, L E Curry, S Goorha, M Woodworth, 10.1080/01638530701739181Discourse Process. 45812007</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8513 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8513</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8513</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-265050948</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.04254v3.pdf" target="_blank">Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in Large Language Models (LLMs) have revolutionized decision-making by breaking down complex problems into more manageable language sequences referred to as"thoughts". An effective thought design should consider three key perspectives: performance, efficiency, and flexibility. However, existing thought can at most exhibit two of these attributes. To address these limitations, we introduce a novel thought prompting approach called"Everything of Thoughts"(XoT) to defy the law of"Penrose triangle of existing thought paradigms. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, thereby enhancing LLMs' capabilities and enabling them to generalize to unseen problems efficiently. Through the utilization of the MCTS-LLM collaborative thought revision framework, this approach autonomously produces high-quality comprehensive cognitive mappings with minimal LLM interactions. Additionally, XoT empowers LLMs to engage in unconstrained thinking, allowing for flexible cognitive mappings for problems with multiple solutions. We evaluate XoT on several challenging multi-solution problem-solving tasks, including Game of 24, 8-Puzzle, and Pocket Cube. Our results demonstrate that XoT significantly outperforms existing approaches. Notably, XoT can yield multiple solutions with just one LLM call, showcasing its remarkable proficiency in addressing complex problems across diverse domains.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8513.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8513.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XOT-GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Everything of Thoughts (XOT) with GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>XOT is a framework that pretrains lightweight policy/value networks and uses MCTS to generate thought trajectories which are then revised by an LLM; here evaluated with GPT-4, it aims to combine planning (external search) with LLM revision to achieve high performance, efficiency, and flexible (graph-like) thought structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (with XOT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 large language model (proprietary OpenAI model) used as the LLM solver and reviser in the XOT pipeline; invoked with deterministic decoding (temperature/top-p = 0).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['MCTS-guided thought search (external planner)', 'Policy/value network guided search (pretrained RL-like model)', 'LLM-based thought revision (chain-of-thought style review & correction)', 'Graph-like multi-trajectory reasoning (multi-solution sampling)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>XOT runs MCTS guided by a small pretrained policy/value network f_θ to produce one-step thoughts (state-action pairs) and full thought trajectories; trajectories are converted to textual prompts and fed to GPT-4 which (1) solves using the provided thoughts and (2) detects/corrects errors; if errors are found, MCTS is re-run from the erroneous parent state for additional simulations and the LLM is called again to verify — repeated up to multiple revisions. Multi-solution reasoning samples multiple trajectories (based on visitation counts) producing graph-like thought structures.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared linear chain prompting (CoT) and self-consistency against tree/graph methods (ToT, GoT) and XOT (MCTS + LLM revision). Multi-solution scenarios explicitly evaluated (sampled multiple trajectories from MCTS). Ablations: number of LLM revision iterations (0-3) and incomplete-thought removal.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Custom planning and reasoning benchmarks: Game of 24 (arithmetic composition from 4 numbers), 8-Puzzle (3x3 sliding puzzle), Pocket Cube (2x2 Rubik's cube). Multi-solution variants tested.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Game of 24: 74.45% accuracy with 1 revision; 85.40% with 3 revisions (GPT-4). 8-Puzzle: 93.28% (1 rev), 95.80% (3 rev). Pocket Cube: 77.60% (1 rev), 83.61% (3 rev). Multi-solution MultiAcc: e.g., Game of 24 MultiAcc ≈ 76.25% (table 4). LLM invocations remain low (~1.4–2 calls per problem depending on revision count); f_θ (policy/value) called many times but is small (~1e6 params).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>XOT produces graph-like, intertwining thought structures for multi-solution tasks and supports self-reflection (LLM identifying erroneous steps). Revisions substantially increase accuracy; incomplete thoughts (missing last step) greatly degrade performance on harder spatial tasks (Pocket Cube particularly sensitive). XOT outperforms pure MCTS and all prompting baselines in both single- and multi-solution settings while requiring far fewer LLM calls than ToT/GoT.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Integrating pretrained MCTS (policy/value networks) with LLM revision enables simultaneous gains in performance, efficiency (few LLM calls), and flexibility (graph-like multi-solution reasoning), outperforming CoT, CoT-SC, ToT, GoT, and pure MCTS on the evaluated tasks; iterative LLM revision further improves reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8513.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8513.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XOT-GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Everything of Thoughts (XOT) with GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same XOT pipeline evaluated with GPT-3.5 as the LLM solver/reviser; shows similar patterns of gains though GPT-3.5 sometimes outperforms GPT-4 on specific tasks after revision (notably Game of 24).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (with XOT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 (OpenAI) used as the LLM solver and reviser; deterministic decoding used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['MCTS-guided thought search (external planner)', 'Policy/value network guided search', 'LLM-based thought revision']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Identical XOT flow: f_θ-guided MCTS yields trajectories; GPT-3.5 reviews and refines; revisions iterate. Multi-solution sampling from visitation counts enables multiple candidate solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Same experimental contrasts as for GPT-4: CoT vs CoT-SC vs ToT vs GoT vs MCTS-only vs XOT; ablations on number of revisions and incomplete thoughts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Game of 24, 8-Puzzle, Pocket Cube (same datasets as GPT-4 runs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Game of 24: 79.56% (1 rev), 90.51% (3 rev) — GPT-3.5 notably reaches 90.51% after 3 revisions. 8-Puzzle: 59.66% (1 rev), 63.03% (3 rev). Pocket Cube: 74.32% (1 rev), 84.70% (3 rev). Multi-solution MultiAcc examples: Game of 24 MultiAcc ≈ 62.90% (table 4). LLM calls small (~1.4–2 per example depending on revisions).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Revision effectiveness differs by LLM: GPT-3.5 benefits strongly on Game of 24 after multiple revisions; GPT-4 performs better on spatial tasks. Incomplete thoughts degrade accuracy more for GPT-3.5 than GPT-4. Overall, XOT + GPT-3.5 shows that a smaller LLM can match or exceed GPT-4 on some tasks when combined with external planning + revision.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>XOT's MCTS + iterative LLM revision leads to large gains even with smaller LLMs; revision iterations are an effective and efficient mechanism to improve final accuracy, and the pipeline can sometimes compensate for LLM modeling differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8513.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8513.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCTS-only</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo Tree Search (policy/value networks) without LLM revision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of MCTS with pretrained lightweight policy/value networks to search for solutions, evaluated as an ablation baseline (no LLM revision or final LLM inference).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MCTS (with small policy/value networks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MCTS guided by a two-layer MLP policy/value network (≈1e6 parameters) trained by self-play; used to produce action trajectories and final answers without LLM involvement.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['MCTS planning/search', 'Policy/value guided rollout (learned prior and value)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Offline-trained f_θ produces priors and value estimates used by PUCT in MCTS to select actions and backpropagate values; final solution selected by highest visitation count trajectory without LLM verification.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (search-only)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared directly to XOT (which combines MCTS + LLM revision) to isolate the effect of LLM revision; included in all three task evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Game of 24, 8-Puzzle, Pocket Cube (same splits).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Game of 24: 62.77% accuracy (table 3). 8-Puzzle: 51.26% (table 5). Pocket Cube: 46.44% (table 7). These are consistently lower than XOT (MCTS + LLM), showing benefit of LLM revision.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>MCTS alone can find many valid trajectories but lacks final grounding/verification that LLM revision provides; performance is substantially improved when combined with LLM corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>MCTS provides strong external planning but benefits significantly from LLM-based review/revision; MCTS-only is a useful ablation showing the complementary nature of search and LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8513.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8513.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT (Tree-of-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thought (ToT) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting paradigm that generates multiple one-step thought candidates from an LLM and performs search (breadth/depth) over a tree of thoughts, but evaluates intermediate nodes using the LLM (expensive).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-3.5 (with ToT prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ToT uses the same LLM to enumerate next-step thought candidates and to evaluate/select branches; in experiments both GPT-3.5 and GPT-4 are used as the underlying LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['LLM-driven tree search (enumeration + evaluation)', 'Branching multi-step exploration (tree topology)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>At each step the LLM is prompted to produce candidate next thoughts; the LLM is again used to score/select which branches to expand. ToT keeps b branches per step (b=1 or b=3 studied).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (tree branching)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared ToT variants (b=1, b=3) to CoT, GoT, and XOT across tasks; reported both accuracy and large LLM invocation counts to quantify cost of diverse LLM-driven branching.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Game of 24, 8-Puzzle, Pocket Cube (same datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Game of 24: best ToT (b=3) with GPT-4 achieved 60.58% accuracy (table 3) but required many LLM calls (~39.83 per problem). 8-Puzzle: ToT (b=3) GPT-4 ~13.45% (table 5). Pocket Cube: ToT (b=3) GPT-4 19.57% (table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ToT can explore diverse thought topologies but is costly because the LLM must be invoked repeatedly to evaluate intermediate nodes; performance is substantially lower than XOT in experiments (despite high LLM usage).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Tree-of-Thoughts achieves flexible, diverse reasoning topologies but at the expense of efficiency; XOT offloads evaluation to cheaper policy/value models and retains flexibility while reducing LLM calls and improving accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8513.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8513.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GoT (Graph-of-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-of-Thought (GoT) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that allows aggregation and refinement of thoughts into graph-like structures using the LLM to generate and evaluate candidates, enabling flexible multi-path solutions but requiring many LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-3.5 (with GoT prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GoT uses LLM calls to generate and merge thought candidates into graph structures, relying on LLM for intermediate evaluation and merging decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['LLM-driven graph reasoning (aggregation / merging)', 'Multi-path thought refinement']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>LLM generates one-step candidates and is asked to merge/select across candidates to form a graph-like thought map; evaluation is LLM-based and performed many times during search.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Benchmarked vs CoT, ToT, XOT; multi-solution performance assessed; measured LLM invocations to demonstrate cost.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Game of 24, 8-Puzzle, Pocket Cube</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Game of 24: GoT (k=1) with GPT-4 ~10.95% (table 3; low compared to ToT and XOT). Multi-solution scenarios show GoT producing more answers but with low MultiAcc compared to XOT. GoT requires many LLM calls (e.g., ~7 calls in some setups reported).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GoT supports flexible graph structures but in practice performs worse than XOT on these planning tasks because of the heavy reliance on repeated LLM evaluation without external planning priors.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Graph-of-Thought affords flexible topologies but is inefficient and underperforms compared to XOT when expensive LLM evaluation is the primary search/evaluation engine.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8513.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8513.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT & CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) and Self-Consistency CoT-SC baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CoT elicits step-by-step linear reasoning chains from an LLM; CoT-SC collects multiple CoT samples and aggregates via majority/self-consistency to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-3.5 (CoT / CoT-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CoT prompts request explicit intermediate reasoning steps; CoT-SC draws N samples of CoT and selects the majority answer (self-consistency). Experiments used deterministic decoding for single CoT and multiple samples for CoT-SC aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Linear chain-of-thought prompting (CoT)', 'Self-consistency (majority voting over multiple CoT samples)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT: single-step-by-step chain appended to prompt. CoT-SC: sample many CoT chains (e.g., 10), then use majority voting to pick final answer. Both rely solely on the LLM's internal reasoning without external planners.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared CoT and CoT-SC to ToT/GoT/XOT to measure effects of linear repeated reasoning vs tree/graph diversity and external search. CoT-SC experiments used 10 samples for majority.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Game of 24, 8-Puzzle, Pocket Cube</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Game of 24: CoT very low (≈2.19% for GPT-3.5, small for GPT-4); CoT-SC slightly better but still poor (table 3). 8-Puzzle and Pocket Cube: near 0% for CoT/CoT-SC on GPT-3.5/GPT-4 for these spatial/planning tasks (table 5,7).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Linear CoT is often insufficient for long-horizon spatial/planning tasks; self-consistency helps slightly in some tasks but cannot match external search plus revision. CoT methods are efficient in terms of LLM calls but low performance on complex planning.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>CoT (even with self-consistency) cannot match the performance of approaches that incorporate external planning/search; diversity in topology (trees/graphs) helps but must be paired with efficient evaluation (XOT's approach).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8513.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8513.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IO baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct Input-Output Prompting (IO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Single-step input-to-output prompting (no intermediate explicit thoughts); used as a baseline for simple direct-answer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-3.5 (IO prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard few-shot prompting with example input-output pairs and no explicit intermediate reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Direct mapping (argument to answer) without explicit reasoning steps']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompt presents examples of final answers only (no chain-of-thought). The LLM is asked to directly output the solution.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Included as a lower-bound baseline across tasks to contrast with structured thought methods (CoT, ToT, GoT, XOT).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Game of 24, 8-Puzzle, Pocket Cube</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Game of 24: ~6.57% (GPT-3.5) and ~10.22% (GPT-4). 8-Puzzle: 0% (GPT-3.5) and ~1.68% (GPT-4). Pocket Cube: ~1.09% for both models (tables 3,5,7).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>IO fails on multi-step planning/spatial tasks; provides a low baseline showing the need for intermediate thoughts or external planning for complex problems.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Direct IO prompting is insufficient for the evaluated long-horizon planning tasks; structured thoughts and/or external search lead to much higher success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8513.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8513.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-13B-ft</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned LLaMA-2-13B (on ground-truth labels)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B-parameter LLaMA-2 model fine-tuned on (question, answer) data from simulations/ground-truth to test whether a finetuned smaller model can replace an LLM in planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-13B (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-2-13B model fine-tuned for 5 epochs on the tasks' ground-truth labels using supervised training (training details in Appendix A).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Supervised finetuning to map inputs to final answers (no explicit external search during inference)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Model is trained to predict final answers directly from inputs, converting search/planning problems into supervised prediction; no MCTS or explicit chain-of-thought used at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared to XOT and prompting baselines to evaluate whether distilling simulation knowledge into a smaller LLM is viable for planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Game of 24, 8-Puzzle, Pocket Cube</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Very poor performance: Game of 24: 2.19% (table 3). 8-Puzzle: 0% (table 5). Pocket Cube: 0% (table 7). Fine-tuned LLaMA-2-13B failed due to hallucination/poor planning generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Supervised finetuning on ground-truth labels failed to capture the planning/long-horizon reasoning necessary; hallucination and inability to perform search/planning surfaced, demonstrating limits of simple finetuning for these structured tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Distilling search/planning into a finetuned LLM without explicit planning/search modules is ineffective on these tasks; explicit MCTS + LLM revision (XOT) outperforms straightforward supervised finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Graph of thoughts: Solving elaborate problems with large language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Reasoning with language model is planning with world model <em>(Rating: 1)</em></li>
                <li>AlphaZero-like tree-search can guide large language model decoding and training <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8513",
    "paper_id": "paper-265050948",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "XOT-GPT-4",
            "name_full": "Everything of Thoughts (XOT) with GPT-4",
            "brief_description": "XOT is a framework that pretrains lightweight policy/value networks and uses MCTS to generate thought trajectories which are then revised by an LLM; here evaluated with GPT-4, it aims to combine planning (external search) with LLM revision to achieve high performance, efficiency, and flexible (graph-like) thought structures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (with XOT)",
            "model_description": "GPT-4 large language model (proprietary OpenAI model) used as the LLM solver and reviser in the XOT pipeline; invoked with deterministic decoding (temperature/top-p = 0).",
            "reasoning_methods": [
                "MCTS-guided thought search (external planner)",
                "Policy/value network guided search (pretrained RL-like model)",
                "LLM-based thought revision (chain-of-thought style review & correction)",
                "Graph-like multi-trajectory reasoning (multi-solution sampling)"
            ],
            "reasoning_methods_description": "XOT runs MCTS guided by a small pretrained policy/value network f_θ to produce one-step thoughts (state-action pairs) and full thought trajectories; trajectories are converted to textual prompts and fed to GPT-4 which (1) solves using the provided thoughts and (2) detects/corrects errors; if errors are found, MCTS is re-run from the erroneous parent state for additional simulations and the LLM is called again to verify — repeated up to multiple revisions. Multi-solution reasoning samples multiple trajectories (based on visitation counts) producing graph-like thought structures.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compared linear chain prompting (CoT) and self-consistency against tree/graph methods (ToT, GoT) and XOT (MCTS + LLM revision). Multi-solution scenarios explicitly evaluated (sampled multiple trajectories from MCTS). Ablations: number of LLM revision iterations (0-3) and incomplete-thought removal.",
            "task_or_benchmark": "Custom planning and reasoning benchmarks: Game of 24 (arithmetic composition from 4 numbers), 8-Puzzle (3x3 sliding puzzle), Pocket Cube (2x2 Rubik's cube). Multi-solution variants tested.",
            "performance_results": "Game of 24: 74.45% accuracy with 1 revision; 85.40% with 3 revisions (GPT-4). 8-Puzzle: 93.28% (1 rev), 95.80% (3 rev). Pocket Cube: 77.60% (1 rev), 83.61% (3 rev). Multi-solution MultiAcc: e.g., Game of 24 MultiAcc ≈ 76.25% (table 4). LLM invocations remain low (~1.4–2 calls per problem depending on revision count); f_θ (policy/value) called many times but is small (~1e6 params).",
            "qualitative_findings": "XOT produces graph-like, intertwining thought structures for multi-solution tasks and supports self-reflection (LLM identifying erroneous steps). Revisions substantially increase accuracy; incomplete thoughts (missing last step) greatly degrade performance on harder spatial tasks (Pocket Cube particularly sensitive). XOT outperforms pure MCTS and all prompting baselines in both single- and multi-solution settings while requiring far fewer LLM calls than ToT/GoT.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Integrating pretrained MCTS (policy/value networks) with LLM revision enables simultaneous gains in performance, efficiency (few LLM calls), and flexibility (graph-like multi-solution reasoning), outperforming CoT, CoT-SC, ToT, GoT, and pure MCTS on the evaluated tasks; iterative LLM revision further improves reliability.",
            "uuid": "e8513.0",
            "source_info": {
                "paper_title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "XOT-GPT-3.5",
            "name_full": "Everything of Thoughts (XOT) with GPT-3.5",
            "brief_description": "Same XOT pipeline evaluated with GPT-3.5 as the LLM solver/reviser; shows similar patterns of gains though GPT-3.5 sometimes outperforms GPT-4 on specific tasks after revision (notably Game of 24).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (with XOT)",
            "model_description": "GPT-3.5 (OpenAI) used as the LLM solver and reviser; deterministic decoding used in experiments.",
            "reasoning_methods": [
                "MCTS-guided thought search (external planner)",
                "Policy/value network guided search",
                "LLM-based thought revision"
            ],
            "reasoning_methods_description": "Identical XOT flow: f_θ-guided MCTS yields trajectories; GPT-3.5 reviews and refines; revisions iterate. Multi-solution sampling from visitation counts enables multiple candidate solutions.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Same experimental contrasts as for GPT-4: CoT vs CoT-SC vs ToT vs GoT vs MCTS-only vs XOT; ablations on number of revisions and incomplete thoughts.",
            "task_or_benchmark": "Game of 24, 8-Puzzle, Pocket Cube (same datasets as GPT-4 runs).",
            "performance_results": "Game of 24: 79.56% (1 rev), 90.51% (3 rev) — GPT-3.5 notably reaches 90.51% after 3 revisions. 8-Puzzle: 59.66% (1 rev), 63.03% (3 rev). Pocket Cube: 74.32% (1 rev), 84.70% (3 rev). Multi-solution MultiAcc examples: Game of 24 MultiAcc ≈ 62.90% (table 4). LLM calls small (~1.4–2 per example depending on revisions).",
            "qualitative_findings": "Revision effectiveness differs by LLM: GPT-3.5 benefits strongly on Game of 24 after multiple revisions; GPT-4 performs better on spatial tasks. Incomplete thoughts degrade accuracy more for GPT-3.5 than GPT-4. Overall, XOT + GPT-3.5 shows that a smaller LLM can match or exceed GPT-4 on some tasks when combined with external planning + revision.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "XOT's MCTS + iterative LLM revision leads to large gains even with smaller LLMs; revision iterations are an effective and efficient mechanism to improve final accuracy, and the pipeline can sometimes compensate for LLM modeling differences.",
            "uuid": "e8513.1",
            "source_info": {
                "paper_title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "MCTS-only",
            "name_full": "Monte Carlo Tree Search (policy/value networks) without LLM revision",
            "brief_description": "Use of MCTS with pretrained lightweight policy/value networks to search for solutions, evaluated as an ablation baseline (no LLM revision or final LLM inference).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MCTS (with small policy/value networks)",
            "model_description": "MCTS guided by a two-layer MLP policy/value network (≈1e6 parameters) trained by self-play; used to produce action trajectories and final answers without LLM involvement.",
            "reasoning_methods": [
                "MCTS planning/search",
                "Policy/value guided rollout (learned prior and value)"
            ],
            "reasoning_methods_description": "Offline-trained f_θ produces priors and value estimates used by PUCT in MCTS to select actions and backpropagate values; final solution selected by highest visitation count trajectory without LLM verification.",
            "reasoning_diversity": "similar (search-only)",
            "reasoning_diversity_experimental_setup": "Compared directly to XOT (which combines MCTS + LLM revision) to isolate the effect of LLM revision; included in all three task evaluations.",
            "task_or_benchmark": "Game of 24, 8-Puzzle, Pocket Cube (same splits).",
            "performance_results": "Game of 24: 62.77% accuracy (table 3). 8-Puzzle: 51.26% (table 5). Pocket Cube: 46.44% (table 7). These are consistently lower than XOT (MCTS + LLM), showing benefit of LLM revision.",
            "qualitative_findings": "MCTS alone can find many valid trajectories but lacks final grounding/verification that LLM revision provides; performance is substantially improved when combined with LLM corrections.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "MCTS provides strong external planning but benefits significantly from LLM-based review/revision; MCTS-only is a useful ablation showing the complementary nature of search and LLM reasoning.",
            "uuid": "e8513.2",
            "source_info": {
                "paper_title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ToT (Tree-of-Thought)",
            "name_full": "Tree-of-Thought (ToT) baseline",
            "brief_description": "A prompting paradigm that generates multiple one-step thought candidates from an LLM and performs search (breadth/depth) over a tree of thoughts, but evaluates intermediate nodes using the LLM (expensive).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 / GPT-3.5 (with ToT prompting)",
            "model_description": "ToT uses the same LLM to enumerate next-step thought candidates and to evaluate/select branches; in experiments both GPT-3.5 and GPT-4 are used as the underlying LLM.",
            "reasoning_methods": [
                "LLM-driven tree search (enumeration + evaluation)",
                "Branching multi-step exploration (tree topology)"
            ],
            "reasoning_methods_description": "At each step the LLM is prompted to produce candidate next thoughts; the LLM is again used to score/select which branches to expand. ToT keeps b branches per step (b=1 or b=3 studied).",
            "reasoning_diversity": "diverse (tree branching)",
            "reasoning_diversity_experimental_setup": "Compared ToT variants (b=1, b=3) to CoT, GoT, and XOT across tasks; reported both accuracy and large LLM invocation counts to quantify cost of diverse LLM-driven branching.",
            "task_or_benchmark": "Game of 24, 8-Puzzle, Pocket Cube (same datasets)",
            "performance_results": "Game of 24: best ToT (b=3) with GPT-4 achieved 60.58% accuracy (table 3) but required many LLM calls (~39.83 per problem). 8-Puzzle: ToT (b=3) GPT-4 ~13.45% (table 5). Pocket Cube: ToT (b=3) GPT-4 19.57% (table 7).",
            "qualitative_findings": "ToT can explore diverse thought topologies but is costly because the LLM must be invoked repeatedly to evaluate intermediate nodes; performance is substantially lower than XOT in experiments (despite high LLM usage).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Tree-of-Thoughts achieves flexible, diverse reasoning topologies but at the expense of efficiency; XOT offloads evaluation to cheaper policy/value models and retains flexibility while reducing LLM calls and improving accuracy.",
            "uuid": "e8513.3",
            "source_info": {
                "paper_title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GoT (Graph-of-Thought)",
            "name_full": "Graph-of-Thought (GoT) baseline",
            "brief_description": "An approach that allows aggregation and refinement of thoughts into graph-like structures using the LLM to generate and evaluate candidates, enabling flexible multi-path solutions but requiring many LLM calls.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 / GPT-3.5 (with GoT prompting)",
            "model_description": "GoT uses LLM calls to generate and merge thought candidates into graph structures, relying on LLM for intermediate evaluation and merging decisions.",
            "reasoning_methods": [
                "LLM-driven graph reasoning (aggregation / merging)",
                "Multi-path thought refinement"
            ],
            "reasoning_methods_description": "LLM generates one-step candidates and is asked to merge/select across candidates to form a graph-like thought map; evaluation is LLM-based and performed many times during search.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Benchmarked vs CoT, ToT, XOT; multi-solution performance assessed; measured LLM invocations to demonstrate cost.",
            "task_or_benchmark": "Game of 24, 8-Puzzle, Pocket Cube",
            "performance_results": "Game of 24: GoT (k=1) with GPT-4 ~10.95% (table 3; low compared to ToT and XOT). Multi-solution scenarios show GoT producing more answers but with low MultiAcc compared to XOT. GoT requires many LLM calls (e.g., ~7 calls in some setups reported).",
            "qualitative_findings": "GoT supports flexible graph structures but in practice performs worse than XOT on these planning tasks because of the heavy reliance on repeated LLM evaluation without external planning priors.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Graph-of-Thought affords flexible topologies but is inefficient and underperforms compared to XOT when expensive LLM evaluation is the primary search/evaluation engine.",
            "uuid": "e8513.4",
            "source_info": {
                "paper_title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "CoT & CoT-SC",
            "name_full": "Chain-of-Thought (CoT) and Self-Consistency CoT-SC baselines",
            "brief_description": "CoT elicits step-by-step linear reasoning chains from an LLM; CoT-SC collects multiple CoT samples and aggregates via majority/self-consistency to improve robustness.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 / GPT-3.5 (CoT / CoT-SC)",
            "model_description": "CoT prompts request explicit intermediate reasoning steps; CoT-SC draws N samples of CoT and selects the majority answer (self-consistency). Experiments used deterministic decoding for single CoT and multiple samples for CoT-SC aggregation.",
            "reasoning_methods": [
                "Linear chain-of-thought prompting (CoT)",
                "Self-consistency (majority voting over multiple CoT samples)"
            ],
            "reasoning_methods_description": "CoT: single-step-by-step chain appended to prompt. CoT-SC: sample many CoT chains (e.g., 10), then use majority voting to pick final answer. Both rely solely on the LLM's internal reasoning without external planners.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Compared CoT and CoT-SC to ToT/GoT/XOT to measure effects of linear repeated reasoning vs tree/graph diversity and external search. CoT-SC experiments used 10 samples for majority.",
            "task_or_benchmark": "Game of 24, 8-Puzzle, Pocket Cube",
            "performance_results": "Game of 24: CoT very low (≈2.19% for GPT-3.5, small for GPT-4); CoT-SC slightly better but still poor (table 3). 8-Puzzle and Pocket Cube: near 0% for CoT/CoT-SC on GPT-3.5/GPT-4 for these spatial/planning tasks (table 5,7).",
            "qualitative_findings": "Linear CoT is often insufficient for long-horizon spatial/planning tasks; self-consistency helps slightly in some tasks but cannot match external search plus revision. CoT methods are efficient in terms of LLM calls but low performance on complex planning.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "CoT (even with self-consistency) cannot match the performance of approaches that incorporate external planning/search; diversity in topology (trees/graphs) helps but must be paired with efficient evaluation (XOT's approach).",
            "uuid": "e8513.5",
            "source_info": {
                "paper_title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "IO baseline",
            "name_full": "Direct Input-Output Prompting (IO)",
            "brief_description": "Single-step input-to-output prompting (no intermediate explicit thoughts); used as a baseline for simple direct-answer tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 / GPT-3.5 (IO prompting)",
            "model_description": "Standard few-shot prompting with example input-output pairs and no explicit intermediate reasoning steps.",
            "reasoning_methods": [
                "Direct mapping (argument to answer) without explicit reasoning steps"
            ],
            "reasoning_methods_description": "Prompt presents examples of final answers only (no chain-of-thought). The LLM is asked to directly output the solution.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Included as a lower-bound baseline across tasks to contrast with structured thought methods (CoT, ToT, GoT, XOT).",
            "task_or_benchmark": "Game of 24, 8-Puzzle, Pocket Cube",
            "performance_results": "Game of 24: ~6.57% (GPT-3.5) and ~10.22% (GPT-4). 8-Puzzle: 0% (GPT-3.5) and ~1.68% (GPT-4). Pocket Cube: ~1.09% for both models (tables 3,5,7).",
            "qualitative_findings": "IO fails on multi-step planning/spatial tasks; provides a low baseline showing the need for intermediate thoughts or external planning for complex problems.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Direct IO prompting is insufficient for the evaluated long-horizon planning tasks; structured thoughts and/or external search lead to much higher success rates.",
            "uuid": "e8513.6",
            "source_info": {
                "paper_title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLaMA-2-13B-ft",
            "name_full": "Fine-tuned LLaMA-2-13B (on ground-truth labels)",
            "brief_description": "A 13B-parameter LLaMA-2 model fine-tuned on (question, answer) data from simulations/ground-truth to test whether a finetuned smaller model can replace an LLM in planning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-13B (finetuned)",
            "model_description": "LLaMA-2-13B model fine-tuned for 5 epochs on the tasks' ground-truth labels using supervised training (training details in Appendix A).",
            "reasoning_methods": [
                "Supervised finetuning to map inputs to final answers (no explicit external search during inference)"
            ],
            "reasoning_methods_description": "Model is trained to predict final answers directly from inputs, converting search/planning problems into supervised prediction; no MCTS or explicit chain-of-thought used at inference.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Compared to XOT and prompting baselines to evaluate whether distilling simulation knowledge into a smaller LLM is viable for planning tasks.",
            "task_or_benchmark": "Game of 24, 8-Puzzle, Pocket Cube",
            "performance_results": "Very poor performance: Game of 24: 2.19% (table 3). 8-Puzzle: 0% (table 5). Pocket Cube: 0% (table 7). Fine-tuned LLaMA-2-13B failed due to hallucination/poor planning generalization.",
            "qualitative_findings": "Supervised finetuning on ground-truth labels failed to capture the planning/long-horizon reasoning necessary; hallucination and inability to perform search/planning surfaced, demonstrating limits of simple finetuning for these structured tasks.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Distilling search/planning into a finetuned LLM without explicit planning/search modules is ineffective on these tasks; explicit MCTS + LLM revision (XOT) outperforms straightforward supervised finetuning.",
            "uuid": "e8513.7",
            "source_info": {
                "paper_title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2
        },
        {
            "paper_title": "Graph of thoughts: Solving elaborate problems with large language models",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Reasoning with language model is planning with world model",
            "rating": 1
        },
        {
            "paper_title": "AlphaZero-like tree-search can guide large language model decoding and training",
            "rating": 1
        }
    ],
    "cost": 0.019784749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EVERYTHING OF THOUGHTS : DEFYING THE LAW OF PENROSE TRIANGLE FOR THOUGHT GENERATION
23 Feb 2024</p>
<p>Ruomeng Ding 
Georgia Institute of Technology</p>
<p>Chaoyun Zhang 
Lu Wang 
Yong Xu 
Minghua Ma 
Wei Zhang 
East China Normal University</p>
<p>Si Qin 
Saravan Rajmohan 
Qingwei Lin 
Dongmei Zhang </p>
<p>Microsoft Research Asia</p>
<p>EVERYTHING OF THOUGHTS : DEFYING THE LAW OF PENROSE TRIANGLE FOR THOUGHT GENERATION
23 Feb 2024D91FB764708F32D2E60458EA3DBA5967arXiv:2311.04254v3[cs.AI]
Recent advancements in Large Language Models (LLMs) have revolutionized decision-making by breaking down complex problems into more manageable language sequences referred to as "thoughts".An effective thought design should consider three key perspectives: performance, efficiency, and flexibility.However, existing thought can at most exhibit two of these attributes.To address these limitations, we introduce a novel thought prompting approach called "Everything of Thoughts" (XOT) to defy the law of "Penrose triangle " of existing thought paradigms.XOT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge and planning capability into thoughts, thereby enhancing LLMs' capabilities and enabling them to generalize to unseen problems efficiently.Through the utilization of the MCTS-LLM collaborative thought revision framework, this approach autonomously produces high-quality comprehensive cognitive mappings with minimal LLM interactions.Additionally, XOT empowers LLMs to engage in unconstrained thinking, allowing for flexible cognitive mappings for problems with multiple solutions.We evaluate XOT on several challenging problem-solving tasks, including Game of 24, 8-Puzzle, and Pocket Cube.Our results demonstrate that XOT significantly outperforms existing approaches in various dimensions, showcasing its remarkable proficiency in addressing complex problems across diverse domains.The code and dataset to reproduce the results in the paper are available at https: //github.com/microsoft/Everything-of-Thoughts-XoT-.</p>
<p>INTRODUCTION</p>
<p>Recent advancements in Large Language Models (LLMs) have greatly advanced problem solving in diverse domains such as mathematical reasoning Frieder et al. (2023), knowledge reasoning Omar et al. (2023), root cause analysis Chen et al. (2023) and causal inference Kıcıman et al. (2023), etc..This progress can be largely attributed to the technique of decomposing intricate problems into smaller language sequences referred to as "thoughts".Through a step-by-step inference process involving the use of prompts, each thought functions as an intermediate stage, contributing to the simplification of tackling complex problems to fulfill the problem's ultimate objective.</p>
<p>Effective design of thought steps toward complex problem-solving and reasoning, whether for humans or LLMs, should prioritize three crucial aspects, namely:</p>
<p>• Performance.Performance is the accuracy of the solution to a problem, including the precision of each thought at intermediate stages.This metric holds paramount importance for problem-solving.</p>
<p>• Efficiency.Efficiency relates to the number of LLM inference calls required to solve a single problem.Minimizing this aspect is crucial due to the high computational cost associated with LLM inference, thereby reducing the overall number of cost.</p>
<p>• Flexibility.Flexibility in thought topology refers to the diverse structures that can be employed by LLMs when organizing thoughts for problem-solving.These structures may include chains, trees, or even graphs, mirroring human thought processes.Enabling more flexible thought structures enhances the capacity of LLMs for divergent and creative thinking, which is particularly advantageous in addressing complex problems, especially those with multiple potential solutions.</p>
<p>There exist several thought generation paradigms, such as Chain-of-Thought (CoT) Wei et al. (2022), Tree-of-Thought (ToT) Yao et al. (2023), and Graph-of-Thought (GoT) Besta et al. (2023), etc..However, these paradigms each have their limitations and cannot simultaneously achieve all the three desired attributes, as illustrated in Table 1.Specifically, direct Input-Output (IO) prompting is suitable primarily for simple problem-solving scenarios with single-step processes, lacking both in performance and flexibility.CoT and self-consistency CoT (CoT-SC) enable step-by-step problem solving, resulting in modest performance improvements, but they are confined to linear thought structures, limiting their flexibility.In contrast, ToT and GoT permit more versatile thought topologies, accommodating tree-like or graph-like structures.However, these paradigms require the evaluation of intermediate thought steps through LLM itself, incurring significant computational costs and inefficiencies due to multiple LLM calls.These paradigms are constrained by a law analogous to the "Penrose triangle ", wherein they can achieve a maximum of two out of the three attributes, and none of them can simultaneously attain all three.</p>
<p>We propose a novel solution called "Everything of Thoughts" (XOT) to address the limitations of conventional thought frameworks, enhancing essential attributes of thought generation, including performance, efficiency, and flexibility for LLM inference.1 XOT leverages reinforcement learning (RL) Li (2017) and Monte Carlo Tree Search (MCTS) Silver et al. (2017), in conjunction with lightweight policy and value networks, to pretrain on specific tasks for thought searching and subsequently generalize to new problems.This pretraining effectively integrates external domain knowledge and planning capability into the "thoughts" provided to LLMs, expanding their problemsolving capabilities, and thereby significantly improving Performance.Once trained, XOT efficiently performs thought searching using MCTS with cost-effective policy and value networks for exploration and autonomously generates complete cognitive mappings for LLMs.It then employs a MCTS-LLM collaborative thought revision process to further improve the thought quality while minimizing LLM interactions.This eliminates the need for LLMs to explore and evaluate thoughts themselves, as required by ToT and GoT, enhancing XOT's Efficiency.Furthermore, MCTS demonstrates remarkable Flexibility as it can explore various thought topologies, including graph structures akin to those employed in human mind mapping processes Faste &amp; Lin (2012); Jamieson (2012).This enables diverse and creative thinking for LLMs, making it particularly valuable when dealing with complex thought structures or tasks featuring multiple potential solutions.By concurrently achieving superior performance, efficiency, and flexibility, XOT challenges the constraints posed by the "Penrose triangle " law, significantly surpassing the capabilities of other thought generation paradigms.</p>
<p>We comprehensively evaluate XOT across a diverse range of challenging problem-solving tasks, namely Game of 24, 8-Puzzle, and Pocket Cube.Our experimental results consistently showcase XOT's superior performance, and its capacity to provide multiple solutions to problems efficiently with just a few LLM calls.These findings establish XOT as an effective thought generation approach, paving the way for new avenues in LLMs' problem-solving capabilities.</p>
<p>BACKGROUND</p>
<p>Thought for LLMs.Addressing complex problems often entails breaking down the overarching objective into multiple intermediary steps.The outcomes or cognitive processes associated with each step are thoughts, which can be expressed as linguistic prompt sequences for LLMs to facilitate .ToT organizes thoughts in a tree-like structure and utilizes search algorithms (e.g., Breadth-First Search, Depth-First Search) to expand the tree in pursuit of an optimal solution.However, thought evaluation in ToT relies on LLMs themselves, necessitating multiple costly and inefficient LLM inference calls.</p>
<p>Graph-of-thought (GoT) Besta et al. (2023) (Fig. 1 (e)).GoT extends the ToT approach by enabling the generation of graph-like thought structures through thought aggregation and refinement during intermediate search phases.Although this method permits more flexible thought structures, it still demands multiple LLM inference calls for evaluation, incurring significant computational costs.</p>
<p>XOT: EVERYTHING OF THOUGHTS</p>
<p>XOT serves as an LLM-MCTS collaborative framework designed to enhance the thought generation process, thereby assisting LLMs in resolving complex problems.It leverages MCTS for proficient and efficient thought exploration while harnessing the capabilities of LLMs to refine and amend the thoughts derived from MCTS.This synergistic interaction creates a mutually beneficial arrangement, ultimately enabling the successful resolution of intricate problems characterized by high levels of performance, efficiency, and flexibility.</p>
<p>XOT IN A NUTSHELL</p>
<p>We present an overview of the architecture of XOT in Fig. 1 (f).XOT comprises two key components: (i) a MCTS module guided by policy/value networks; and (ii) an LLM solver for thought revision and inference.The MCTS and policy/value networks need to be trained and then generalize to the inference process.</p>
<p>During the training phase, MCTS is harnessed to explore potential thought structures for a specific task through simulated scenarios.This process entails the recording of states, values, and the visitation frequencies of thought nodes in each simulation.These recorded data are subsequently employed to iteratively train the policy and value estimation model, enabling it to assimilate domain knowledge and comprehend the world model.</p>
<p>Once trained, the estimated policy and value are utilized to guide the MCTS to systematically search for a thought trajectory provided to aid LLMs in problem-solving.Note that thoughts extracted only play a supporting role, assisting LLMs in gathering knowledge from external sources and improving its planning capability .These thoughts do not provide LLMs with definitive or error-free answers, as they may contain inaccuracies or suboptimal solutions.LLMs are responsible for review and refining these thoughts when they seem erroneous or require adjustments.They continue MCTS the search process if needed and eventually formulate the final answers by integrating these external thoughts with their internal knowledge.</p>
<p>THOUGHT SEARCHING FORMULATION</p>
<p>The fundamental objective of employing the thought generation paradigm for LLMs is to identify the optimal decomposition of a complex problem into several manageable sub-steps.Each sub-step aims to alter the current status of the problem, eventually culminating in the successful resolution of the overarching problem.This approach, as seen in ToT and GoT, hinges on well-defined state transitions and clear final objectives.Consequently, it is natural to conceptualize the thought-searching process as a Markov Decision Process (MDP) Puterman (1990), in which:</p>
<p>• State s t : Represents the current status of the problem.The initial state s 0 corresponds to the original problem, while intermediate states are characterized by either decomposed sub-problems or the results stemming from their resolution.• Action a t : Signifies the one-step solution or action associated with tackling a problem, leading to a transition to a new state, by incorporating their outcomes.• Reward r: Reflects the comprehensive evaluation of the solution to the original problem, assessing whether it has been effectively resolved through the process of problem decomposition.• Thought τ : A one-step thought is a combination of one-step state and action, i.e., τ = {s, a}.This formulation naturally encapsulates the process of decomposing a complex problem into multiple sub-tasks, each accompanied by their respective outcomes.</p>
<p>The detailed definitions of state, action, reward and thought for each task are shown in Table 1.The generation of complete thoughts T = {τ 1 , • • • , τ N }, can be construed as the endeavor to discover a thought trajectory to maximize the accumulated reward to address the overall problem.</p>
<p>THOUGHTS SEARCHING WITH MCTS</p>
<p>The formulation above naturally aligns the thought within LLM as a state-action pair.This approach facilitates the effective exploration of its optimal trajectory using a combination of MCTS and RL.This adheres to an iterative simulation cycle that encompasses three key phases: selection, expansion &amp; evaluation, and backpropagation.It heavily depends on the utilization of neural networks f θ , which simultaneously estimate the value and action probability for a given state s t .The aim is to reduce the number of rollouts and accelerate the search process, similar to the approach employed in AlphaGo Zero Silver et al. (2017).We provide a visual representation of an iteration of the MCTS in Fig. 2 (a)-(c) by taking Pocket Cube as an example and detail each process below.</p>
<p>Selection.In the selection phase, the algorithm initiates at the root node and proceeds to choose an action a * from the available set A(s) for single-step thought generation in the current state s.This process continues until a leaf node within the current tree is reached.The selection is guided by the PUCT algorithm Rosin (2011), aiming to maximize the Upper Confidence Bound (UCB) Garivier &amp; Moulines (2011), as follows:
a * = arg max a∈A(s) Q(s, a) + w • P θ (s, a) N (s) 1 + N (s, a)
.</p>
<p>(1)</p>
<p>Here, Q(s, a) denotes the Q-value of a state-action pair (s, a), which estimates the quality of a particular action in a given state.The higher the Q-value, the better the action is considered to be.P θ (s, a) denotes the predicted prior probability of selecting action a given the state s obtained from a neural network f θ , and N (s, a) represents the count of times action a has been chosen in state s.</p>
<p>The parameter w controls the trade-off between exploration and exploitation.The selection process will continue until an unexplored node is encountered.</p>
<p>Evaluation and Expansion.Upon reaching a previously unselected leaf node, we expand to the state s for the next step for new thought exploration.This expansion involves the evaluation of its value and action probability on the state, which are modeled by neural networks parameterized by θ, i.e., (P θ (s), v θ (s)) = f θ (s).Here P θ (s) is the prior probabilities for all actions on s, and v θ (s) denotes its predicted state value.These two values are retained and stored for backup purposes, and state s is masked as "visited".</p>
<p>Backpropagation.Following the expansion of a leaf node in the above phases, which could be either an unexplored or terminal state, the algorithm proceeds to update all the Q(s, a) values via backpropagation.For unexplored nodes, this update involves computing the mean of its estimated value v θ , while for terminated nodes, it's based on the true reward r.These updates occur as information is backpropagated along the trajectory to subsequent nodes.Additionally, the visit count for each state-action pair is also incremented as follows: N (s, a) = N (s, a) + 1.</p>
<p>A simulation is completed after a sequence of selection, evaluation, expansion, and backpropagation steps.After conducting multiple simulations, we proceed to the next step by selecting an action at state s using a probability distribution defined as ε a ∝ N (s, a) 1/γ , where γ is a temperature constant that regulates the level of exploration.</p>
<p>Policy and Value Networks Training.The simulations described above allow us to compile a dataset for each sample state s containing (s, ε(s), v(s)), where ε(s) = {ε a | a ∈ A(s)}, and v(s) represents the ground truth value obtained by accumulating rewards along the trajectory starting from state s.Subsequently, we can train a combined policy and value network f θ to minimize the discrepancy between the predicted value v θ (s) and the actual value v(s), while also maximizing the alignment between the action probabilities produced by the neural network P θ (s) and the search probabilities ε(s).This can be achieved by minimizing the following loss function:
L = (v(s) − v θ (s)) 2 + ε(s) T log P θ (s)).(2)
This training iterates alongside the simulation process to continually enhance the performance of f θ , resulting in progressive improvements in thought searching capabilities.</p>
<p>THOUGHT INFERENCE WITH MCTS</p>
<p>Once trained, we utilize the f θ to guide the MCTS in generating a thought for a new problem, which assists the LLM in solving it.Specifically, MCTS is utilized to perform K simulations aimed at thought searching and problem-solving, as illustrated in Fig. 2 (d).In each simulation, f θ is employed to guide the MCTS in its search for a thought trajectory.Throughout the training process, f θ incorporates external information related to the state and action quality.This information helps LLMs understand the world model, enhancing their long-term reasoning and planning abilities, which are areas they may not excel in Stechly et al. (2023);Valmeekam et al. (2023), thereby ensuring the performance of thought generation.Once the simulation concludes, we record the visiting count N (s, a) and the thought trajectory is obtained based on the number of solutions required:</p>
<p>• Single solution.starting from each state s, the action with the highest visiting count N (s, a) is selected.• Multiple solution.we sample M thought trajectories following the probability distribution ε a ∝ N (s, a) and remove duplicates.This results in one or multiple thought trajectories T * that consist of a sequence of state-action pairs for problem-solving.The trajectories for multi-solution problems may intertwine and converge at the same goal state, resulting in a graph-like thought structure.This demonstrates that XOT is capable of generating thought structures with flexibility.These trajectories are then transformed into text sequences that are concatenated to form a prompt sequence provided to LLMs.Note that the thought trajectory is concatenated into a single prompt, even in the case of problems with multiple solutions.Therefore, we only require a single LLM inference call at this stage.Given that the f θ network is relatively lightweight, this ensures the efficiency of XOT.</p>
<p>Thought-to-Prompt Parsing.Once the thought trajectories T * are extracted from MCTS, we convert them into a textual format necessary for LLM inference.In this conversion process, we transform both the state and action at each step of the thought, i.e., τ = {s, a} in T * , into text.This conversion aims to provide a comprehensive state transition, facilitating LLMs in better understanding the task step by step.In the case of multi-solution scenarios, multiple trajectories are concatenated.This format remains consistent across all baselines, and the resulting prompt text is then fed to LLMs for inference or thought revision.</p>
<p>Thought Revision.It is important to acknowledge that that MCTS may not always provide the globally optimal thought trajectory to directly solve the problem flawlessly.Therefore, the thoughts extracted from MCTS serve as a reference thinking process for the problem, aiding LLMs in a supportive capacity.The LLMs will leverage their internal knowledge to review the extracted thought, identify errors in the thought trajectory, and then ground its knowledge in collaboration with the MCTS to revise and refine the thought.In this context, LLM plays a role akin to a participant in the collaborative framework, guiding MCTS to enhance its performance.</p>
<p>The revision process is iterative in nature, as shown in Fig. 3. Initially, upon obtaining the extracted thought, we instruct the LLM to detect any errors in the thought generated by MCTS using its internal knowledge.If the LLM identifies an error, it results in an error state denoted as s e within the thought.If no error is found, the thought remains unchanged.Starting from the parent state of s e , MCTS conducts an additional set of L simulations, ultimately yielding a revised thought for the LLM.In scenarios involving multiple solutions, each solution undergoes this process individually.</p>
<p>Upon the completion of the revision, we supply the LLMs with the revised thoughts for problemsolving.The revision process can be repeated several times to enhance the reliability of the answer.This collaborative MCTS-LLM framework nurtures a mutually beneficial process for both components, ultimately contributing to the overall performance of problem-solving.Since LLMs are solely utilized for identifying errors during the revision process with only one call, the efficiency of XOT is effectively maintained.</p>
<p>The collaborative revision framework harnesses the strengths of both MCTS and LLMs.MCTS efficiently and flexibly generates candidate thoughts for LLMs through simulations, while LLMs use their internal knowledge to revise and ground these thoughts within the MCTS framework, effectively turning MCTS into a world model for LLMs.This process ensures the generation of high-quality thoughts for problem-solving.</p>
<p>EXPERIMENT</p>
<p>We conduct an extensive evaluation of our XOT approach in comparison to several baseline methods across three challenging tasks: the Game of 24, the 8-Puzzle (with a 3 × 3 grid), and the 2 × 2</p>
<p>Objective</p>
<p>Use four numbers on playing cards to make the number 24 through +, −, ×, or ÷.</p>
<p>Rearrange the tiles in the 3 × 3 puzzle from an scrambled state to a goal state -1 2  3 4 5   6 7 8 .</p>
<p>Rotating the faces of a 2 × 2 pocket cube until each face of the cube is a uniform color .</p>
<p>Input 4 numbers ranging from 1 to 13, e.g., (4,6,10,10).</p>
<p>A scrambled 3 × 3 digital puzzle, e.g., Thought 3 intermediate equations.</p>
<p>The step-by-step sliding, and the puzzle state after the move.</p>
<p>The step-by-step rotation, and the cube state after the move.</p>
<p>State</p>
<p>The remaining 1-4 numbers.</p>
<p>The current number layout of the puzzle.</p>
<p>Colors of each face of the pocket cube.</p>
<p>Action</p>
<p>Picking two number and a operation to compose an equation.</p>
<p>The one-step moving action of the "-" tile.</p>
<p>The one-step rotation action of cube.</p>
<p>Reward 1 if the number of the final number is equal to 24 otherwise -1.</p>
<p>The negative minimum step on solving the current puzzle state toward the goal state.</p>
<p>The negative minimum moving step on solving current cube state toward the goal state.</p>
<p>Pocket Cube.An overview of these tasks is provided in Table 2.These tasks are characterized by their complexity, requiring multiple steps for completion and potentially having multiple solutions.To assess the effectiveness of XOT, we compare it against IO, CoT, CoT-SC, ToT, GoT, and single MCTS without LLMs for inference and revision.We also finetune LLaMA-2-13B Touvron et al. (2023) for comparison, using the same training data and ground truth labels.The setup of LLaMA-2-13B can be found in Appendix A. We employ both GPT-3.5 Ouyang et al. (2022) and GPT-4 OpenAI (2023) for these evaluations.Note that temperature and top p are set to 0.0 for all LLM invoked.We further conduct ablation study to assess the impact of thought revisions, the revision success rate, and the sensitivity to the completeness of the provided thoughts, presented in Section 4.4.We conduct case study in Multi-Solution Scenarios in Section 4.5 to illustrate the thought structures.The computational training costs of MCTS are discussed in Appendix B. The discussion on generalizing XOT to other NLP tasks, such as Document Merging Besta et al. (2023), can be found in Appendix C.</p>
<p>Policy/Value Networks Configurations.The policy and value networks in our model utilize a shared multi-layer perceptron (MLP) architecture with two layers and hidden units arranged as (128,256).Two heads connected to the MLP are responsible for predicting v θ (s) and P θ (s) separately.The total number of parameters in the Policy/Value Network for all three tasks is approximately 10 6 .This design results in a considerably smaller model compared to LLM, making it much more efficient.We train this model through three iterations, with each iteration comprising 10 self-play episodes for MCTS.</p>
<p>Evaluation Metric.For each task, we assess the accuracy of each approach on the test set.Additionally, we track the number of LLM invocations required for all approaches to solve a problem, as well as the number of times f θ is invoked in the case of XOT.It's important to note that f θ is a considerably smaller model compared to LLMs.In the context of multi-solution scenarios, accuracy is computed as the percentage of problems for which any of the answers provided by each approach is correct.Multi-solution Accuracy (MultiAcc) is calculated as the average percentage of correctness across all solutions offered.Furthermore, we capture the total count of distinct solutions provided by each approach, regardless of their correctness, represented as #Sol.Note that we set the maximum solution number to 3 for all problems in multi-solution scenarios.In Table 3 to Table 8, the number of thought revision is denoted by r.</p>
<p>GAME OF 24</p>
<p>The Game of 24 presents a arithmetic challenge wherein the goal is to employ four numbers within the range of 1 to 13, in conjunction with basic arithmetic operations, (i.e., +, −, ×, ÷), to attain a final result of 24.This game may possess multiple valid solutions.</p>
<p>TASK SETUP</p>
<p>We collect a dataset from 4nu, comprising 1,362 games ranked by human solving time, spanning a range of difficulty levels from easy to hard.For our testing phase, we randomly selected 137 games, ensuring coverage of various difficulty intervals.The remaining 1,225 problems were used to train the policy/value networks with MCTS.In the context of this task, as outlined in Table 1, the thoughts refer to the three intermediate equations, while the state encompasses the available numbers (ranging from 1 to 4) for creating the equations.Actions involve the selection of two numbers and an operator to form an equation, and the reward is set to 1 if the final equation is both valid and results in the number 24, utilizing each of the input numbers exactly once, otherwise it is set to -1.Performance is measured by calculating the success rate across the 137 test games.</p>
<p>BASELINES &amp; XOT SETUP</p>
<p>The IO prompt is supported by five in-context examples.In the case of CoT, we augment each input-output pair by including three intermediate equations.As for ToT, we solicit one-step thought candidates from the LLM at each step, subsequently instructing the LLM to categorize each thought candidate for intermediate selection.For experimental comparison, we conduct experiments on both the top-1 candidate (with b=1) and the top-3 candidates (with b=3) being retained, where b indicates the branches retained for exploration at each step.For GoT, we employ LLM to generate one-step thought candidates in the same manner as ToT, then we direct the LLM to select the top-1 thought from all candidates for merging the thoughts.We also examine a CoT-SC baseline, which derives the majority output from 10 CoT samples.For XOT, we perform 200 simulations for each action taken, and this count is increased to 500 during the thought revision process.</p>
<p>In the multi-solution scenario, the IO, CoT, and CoT-SC prompts each include 5 examples, with each problem having 1 to 3 different solutions.For ToT, the top-3 candidates (with b=3) at the final step are considered as different solutions.Rather than keeping only the top-1 thought, GoT is instructed to select between 1 to 3 thoughts from all candidates at each step to generate a wider range of solutions.As for XOT, after performing simulations on MCTS, we sample 500 thought trajectories as for exploration and remove deplicates.The top-3 thoughts with the highest counts are preserved.</p>
<p>RESULTS</p>
<p>Table 3 displays the overall performance of all methods on this task.Notably, XOT consistently outperforms other baselines on both GPT-3.5 and GPT-4, achieving an accuracy of 79.56% and 74.45% respectively, with 1-time revision.However, after 3-time revision process, XOT's accuracy substantially improves to 90.51% and 85.40% for GPT-3.5 and GPT-4 respectively.This underscores the impressive performance of XOT, and demonstrates that the revision process significantly enhances performance, with only a limited increase in the utilization of LLM and f θ .Interestingly, the revision process in XOT mitigates the performance gap attributable to the modeling ability in this task.</p>
<p>As we observe that XOT with GPT-3.5 achieves higher accuracy after revision compared to GPT-4.</p>
<p>Moreover, XOT consistently outperforms the use of MCTS solely.The performance advantages exhibit growth with the number of revision iterations, underscoring the complementary roles of LLM and MCTS, emphasizing their joint necessity in achieving superior results.The fine-tuned LLaMA-2-13B is only successful on 2.19% of the test data.This performance is lower than the IO method, indicating that the finetuning method is not be suitable for planning tasks like the Game of 24.The best-performing prompting baseline, ToT (b=3) on GPT-4, attains an accuracy of 60.58%.However, it demands a substantial number of LLM invocations (39.83), which results in inefficiency.</p>
<p>In contrast, XOT only requires less than 1.8 calls with revision.Although XOT requires some inference calls for f θ , the model is significantly less complex than LLM, making it a much more efficient approach.</p>
<p>Table 4 presents the performance of different methods in the multi-solution scenario.Overall, XOT remains the best-performing approach in terms of MultiAcc, significantly outperforming other baselines.Although XOT does not generate the most number of answers compared to other baselines, it generates more accurate answers, as its MultiAcc significantly outperforms other approaches.Notably, generating multiple solutions does not significantly increase XOT's complexity, as it only requires 2.31 LLM calls with GPT-4 and around 100 calls for a smaller f θ , making it remain efficient.Overall, the remarkable performance of XOT in the multi-solution scenario demonstrates its ability to generate complex thoughts.</p>
<p>8-PUZZLE</p>
<p>The 8-Puzzle is a classic sliding puzzle game that consists of a 3 × 3 grid with eight numbered tiles and one empty space denoted as "-".Its objective is to rearrange the tiles from a given initial configuration into a target configuration.The maximum number of steps necessary for the optimal solution of the 8-Puzzle is 31.This problem falls within the category of NP-complete problems Ratner &amp; Warmuth (1986) and may have multiple solutions.</p>
<p>TASK SETUP</p>
<p>We randomly generated 419 solvable 8-puzzle problems, with 300 instances allocated for training and 119 instances for testing.All generated problems are solvable within 9 steps.The action space encompasses four directions: [Up, Down, Left, Right].Note that the legal action space for each problem state may vary due to the dynamic position of the empty space.As shown in Table 1, the thoughts refer to the step-by-step move, and the puzzle state after the move.</p>
<p>BASELINES &amp; XOT SETUP</p>
<p>The IO prompt is extended with three in-context examples.In the CoT approach, each input-output pair is enriched by incorporating intermediate legal action sets, the current action, and the current state.In ToT, at each stage, a set of one-step thought candidates are derived from the LLM, from the current set of legal actions.We impose a maximum step limit of 9 since all generated problems can be solved within this range.The 8-puzzle's rules are conveyed through a system message, including detailed explanations of each action's execution.Similarly, we perform 20 simulations for each action taken with XOT, and increase this number to 50 for thought revision processes.</p>
<p>In the multi-solution scenario, all of the IO, CoT, and CoT-SC prompts consist of four examples.Each problem is presented with one to three distinct solutions.For ToT (b=3) and GoT (k=3), the maximum number of steps is increased to 12, as correct solutions may not always be optimal and could exceed 9 steps.In the case of XOT, after conducting simulations with MCTS, we sample 50 thought trajectories for exploration and select the top-3 thoughts with the highest counts.</p>
<p>RESULTS</p>
<p>The inherent spatial complexity of the 8-Puzzle, the need for long-term planning, and the presence of invalid actions create a significant challenge for LLMs, which rely solely on textual data as input.This challenge is starkly evident in the poor performance of the baselines on both GPT-3.5,where its IO prompting achieve a mere 0% success rate.XOT successfully addresses this issue by supplying thoughts acquired from MCTS, thereby infusing external knowledge into the problemsolving process.This augmentation empowers LLMs to tackle problems that were previously insurmountable.In summary, when using GPT-4, XOT achieves an accuracy of 93.28% with 1 revision and 95.80% with 3 revisions in the 8-Puzzle task, outperforming the best prompting baseline, ToT (b=3), which only achieves 13.45% accuracy.Additionally, XOT demonstrates efficiency, as it only requires approximately 1.6 LLM calls for 3-time revision setting.The poor performance of finetuned LLaMA-2-13B (0%) revealed a significant issue with hallucination.This underscores the inefficiency and ineffectiveness of finetuning approaches for tasks necessitating long-term planning, while also bringing to light the heightened costs associated with its use.</p>
<p>The multi-solution performance presented in Table 6 confirms that the XOT method continues to outperform other baselines for both GPT-3.5 and GPT-4 models in terms of MultiAcc, whether or not revision is applied.The revision process of XOT is particularly beneficial for GPT-4, as it improves the MultiAcc from 51.26% to 76.33%, compared to single MCTS.These results again demonstrate that XOT can effectively generate complex thought structures for multi-solutions with high performance and efficiency, making it particularly suitable for this task.</p>
<p>POCKET CUBE</p>
<p>The 2 × 2 Pocket Cube is a simplified variant of the classic Rubik's Cube puzzle.Its primary objective is to restore all of its faces to a uniform color by executing various face rotations.The maximum number of steps required to optimally solve the cube is 11, and it is also a NP-complete problem Demaine et al. ( 2017) and may possess multiple solutions.This task is known to be challenging to LLMs cub.</p>
<p>TASK SETUP</p>
<p>We initially set all faces of the cube to a uniform color and then randomly apply 5 actions sequentially selected from the 27 legal actions of the Rubik's Cube.This process resulted in the creation of 1,000 training samples and 183 testing samples.All generated problems can be solved within 4 steps.To simplify the action space, we reduced the 27 legal operations to 9 actions, namely: {U, U', U2, R, R', R2, F, F', F2}, which are used in our experiments with both baselines and XOT.As shown in Table 1, the thoughts pertain to the step-by-step rotation, and the cube state after the move.</p>
<p>BASELINES &amp; XOT SETUP</p>
<p>The IO prompt is augmented with a single in-context example.In CoT, we enrich each input-output pair by including intermediate actions and states.In ToT, we retrieve one-step thought candidates from the LLM at each stage and instruct the LLM to classify each candidate for intermediate selection.A maximum step limit of 4 is imposed, as all generated problems can be resolved within this range.The cube's rules are conveyed through a system message, which includes the definition of the action space and illustrations of the execution of each action.For XOT, we conduct 20 simulations for each action taken and increase it to 500 for revision.</p>
<p>In the multi-solution setup, the IO, CoT, and CoT-SC prompts each include 3 examples, and each problem within these prompts offers 3 unique solutions.As for ToT (b=3) and GoT (k=3), the maximum number of steps allowed is extended to 7. In the case of XOT, after conducting MCTS simulations, we gather 50 thought trajectories, and we keep the top 3 thoughts with the highest counts.</p>
<p>RESULTS</p>
<p>The Pocket Cube task, similar to the 8-Puzzle, poses a challenge that demands spatial imagination skills, making it difficult for LLMs to excel.As expected, most of the baselines show very poor performance in this task, with some baselines achieving 0% accuracy.The best prompting baseline, ToT (b=3) with GPT-4, only attains a success rate of 19.57%.In contrast, XOT can achieve over 77.60% accuracy with 1-time revision and over 80% accuracy with 3-time revision, establishing itself as an expert in solving this task.This is attributed to the injection of external knowledge from MCTS, enabling LLMs to solve problems that they would struggle with on their own.On the other hand, XOT improves accuracy by 30% compared to a single MCTS with one-time revision.This demonstrates the effectiveness of integrating MCTS and LLMs.Notably, XOT maintains high efficiency in this task, requiring only approximately 2 LLM inference calls for both GPT-3.5 and GPT-4.Again, the finetuned LLaMA-2-13B struggles with the Pocket Cube task (0%), due to significant hallucination issues.This comparison further validates the potential of XOT in contexts demanding extensive planning and decision-making accuracy.</p>
<p>In the case of the multi-solution scenario, the performance of the XOT method remains remarkable, achieving over 77% MultiAcc with GPT-4.The revision process continues to play an important role, significantly improving the performance of XOT with both GPT models.The closest competitor in this setting is GoT (k=3) with GPT-4, which achieves a MultiAcc of 16.85%, but it requires a significantly higher number of LLM invocations compared to XOT (13.36 vs. 4.08) and much lower MultiAcc.Overall, XOT retains its position as the best solution for the Pocket Cube.</p>
<p>ABLATION STUDY</p>
<p>In our ablation study, we consider two aspects: the impact of the number of revisions on the performance and efficiency of XOT and the sensitivity of performance to the completeness of the provided thoughts.These angles allow us to gain insights into how XOT's performance can be improved and understand the importance of providing complete thoughts in complex problem-solving tasks.</p>
<p>NUMBER OF REVISIONS</p>
<p>It's important to highlight that the performance of each task can be further improved through multiple revisions of the thought using the MCTS-LLM collaborative framework.In Fig. 4, we compare the performance of GPT-3.5 and GPT-4 models using the XOT method with varying numbers of revisions, ranging from 0 to 3, across all three tasks.</p>
<p>In the Game of 24 task, as the number of revisions increases, both models exhibit improved performance.Notably, GPT-3.5 consistently outperforms GPT-4 in terms of accuracy.After three revisions, GPT-3.5 achieves an accuracy of 90.51%, while GPT-4 reaches 85.40%.This improved performance comes at the cost of increased inference times and model calls, primarily driven by the need for more interactions to generate revised thoughts.For the 8-Puzzle task, the trend of increasing accuracy with more revisions remains valid.However, in this task, GPT-4 significantly outperforms GPT-3.5.After one revision, GPT-4 achieves an accuracy of 93.28%, which increases to 95.80% after the third revision.In contrast, GPT-3.5 only attains an accuracy of 63.03% after the third revision.In the Pocket Cube task, the performance trend is similar.The accuracy of both models improves with an increase in the number of revisions.GPT-3.5 starts at an accuracy of 45.36% without revision and improves to 84.70% after three revisions.GPT-4 begins with an accuracy of 45.90% and reaches 83.61% after three revisions.Inference times and model calls are comparable between the two models, with GPT-4 showing a substantial increase in model calls after the third revision.</p>
<p>Note that the number of LLM invocations does not increase dramatically with additional revisions, even though f θ is called more times to guide simulations.Considering the significant disparity in inference costs between LLM and f θ , increasing the number of revisions to achieve better performance appears to be a favorable trade-off.</p>
<p>We also focus on the efficacy of the revision process within the XOT framework across three distinct tasks.The Revision Success Rate is calculated as the ratio of successfully detected errors to the number of failed cases without revision, thereby providing insight into the effectiveness of revisions.</p>
<p>The results for both GPT-3.5 and GPT-4 are presented in Table 9 and Table 10.Our observations    reveal a high revision success rate in the XoT framework, which increases with the number of revisions.This underscores the effectiveness of LLMs in the revision process, positioning it as a highly efficient approach to thoughts revision.</p>
<p>INCOMPLETE THOUGHT</p>
<p>In this ablation study, we explore the performance of LLMs when provided with incomplete thoughts, specifically omitting the last step of the thought trajectory.This simulates scenarios where MCTS might supply inaccurate or incomplete thoughts.The aim is to test whether LLMs can independently solve problems or rely on their own reasoning, rather than solely relying on the thought from MCTS as answers.We present the performance comparison for all three tasks in Table 11.Note that we only compare ToT and GoT since other baselines do not support this comparison by their nature.</p>
<p>The results clearly show that incomplete thoughts lead to a significant performance drop in all three tasks.GPT-3.5 is more affected than GPT-4, with GPT-3.5 achieving 0% accuracy on several baselines.In contrast, XOT with GPT-4 attains satisfactory performance on the Game of 24 and 8-Puzzle, achieving over 40% accuracy.However, the performance of XOT is dramatically affected in the Pocket Cube task, with accuracy dropping to 6%.This demonstrates that for very complex tasks, LLMs are highly sensitive to the completeness of the thoughts provided.Missing steps in the thought can lead to a substantial drop in performance, highlighting the importance of providing complete thoughts for such tasks.</p>
<p>CASE STUDY</p>
<p>Finally, in Fig. 5, we provide examples of thought structures generated by XOT for all three tasks in the multi-solution scenario.It is noteworthy that, owing to the multiple solutions required, the generated thoughts intertwine during intermediate steps and converge towards the final goal state.This results in a naturally woven thought structure resembling a graph, showcasing the remarkable flexibility achieved by XOT.Upon closer examination of each example, in the case of the Game of 24, there are multiple solutions to reach the goal of 24 from the initial state.XOT effectively predicts these trajectories, indicating its ability to grasp complex thought structures.In the 8-Puzzle example, we observe instances of reflection in the thought structure, with back-and-forth recurrent state transitions.This demonstrates XOT's capacity for self-reflection, a crucial attribute for LLMs, as discussed in previous work Shinn et al. (2023).In the case of the Pocket Cube, XOT identifies four distinct pathways to reach the goal state, leading to successful problem-solving across multiple solutions.</p>
<p>Overall, these cases highlight how XOT encapsulates the flexibility required in thought generation, fostering diverse and creative thinking for LLMs.This enables them to produce multiple highquality answers to a single problem effectively.</p>
<p>EXPERIMENT SUMMARY</p>
<p>In summary, our approach XOT significantly improves the performance of LLMs by introducing a streamlined thought trajectory revision process.This represents a fundamental shift from traditional problem-solving approaches, resulting in substantial performance enhancements across a range of tasks.Notably, XOT excels in solving the Game of 24 and demonstrates its ability to overcome challenges requiring spatial reasoning, such as the 8-Puzzle and Pocket Cube, which were previously challenging for LLMs.The remarkable synergy of improved performance, efficiency, and flexibility exhibited by XOT positions it as an exemplary and superior method for eliciting optimal responses from LLMs.2023) employs LLMs to translate natural language into planning goals, demonstrating their capacity to harness commonsense knowledge and reasoning to provide missing details for under-specified goals.These studies underscore the growing potential of LLMs in the field of planning, with research efforts expanding rapidly.</p>
<p>RELATED WORK</p>
<p>Augmenting LLMs with RL.Enhancing the capabilities of LLMs through the incorporation of external models constitutes an effective strategy for improving their overall quality.The foundational work of ChatGPT Ouyang et al. (2022) leverages RL from human feedback to enable LLMs to adhere to human guidance, resulting in a substantial enhancement of their truthfulness and a reduction in toxic output.Similarly, GLAM Carta et al. (2023) employs online RL to establish alignment between LLMs' knowledge and the broader environment, thus enhancing their ability to generalize to new objects or tasks and ultimately improving their performance.Additionally, an interesting study in Yuan et al. ( 2023) utilizes RL to acquire basic skills in the context of Minecraft Cipollone et al. (2014), with subsequent high-level planning carried out by LLMs.This approach demonstrates promising performance across various Minecraft tasks.Furthermore, the ESPER framework Yu et al. ( 2023) harnesses RL to achieve alignment between multimodal inputs and language model generations, all without the need for direct supervision.This empowers LLMs to effectively tackle multimodal tasks and provides robust visual alignment and rapid inference speeds while preserving the textual domain.Collectively, these research endeavors underscore the considerable potential in augmenting LLMs with reinforcement learning techniques.</p>
<p>MCTS is also integrated with LLMs to enhance both training and inference processes.Feng et al. (2023).These studies underscore the significant potential of integrating MCTS with LLMs to improve their overall capabilities.</p>
<p>DISCUSSION</p>
<p>Generalization While XOT is presently utilized for reasoning and search problems, its applicability can be extended to a broader spectrum of problem domains characterized by decomposable tasks with well-defined objectives.The MCTS utilized in XOT is particularly suitable for such tasks and can therefore generalize to more complex problems.We also note that MCTS is functioning in a supportive role and can be substituted with alternative supervised or RL models for thought exploration and generation, which can serve as a copilot to inject domain knowledge of the realworld model to LLMs.This opens up a promising avenue for future research, enabling LLMs to engage in more effective planning and problem solving processes.</p>
<p>Limitation We also note that the implementation of XOT necessitates the training of additional policy and value models to expedite the inference process.This training process requires the acquisition of datasets from real-world environments, introducing supplementary costs and efforts.However, note that these policy and value models are considerably smaller and more computationally efficient than the underlying LLMs.Consequently, the incurred costs are deemed low, particularly in the context of tasks featured in this study, where the thought steps and objectives are well-defined.In future research endeavors, we intend to explore methods to enhance the efficiency of the training process for XOT in scenarios where the objectives are less straightforward, such as multi-agent planning and code generation tasks Talebirad &amp; Nadiri (2023); Vaithilingam et al. (2022).This endeavor will expand the applicability of the proposed XOT framework to a broader range of applications.</p>
<p>In terms of potential risks, XOT is susceptible to the MCTS module providing incorrect intermediate thoughts, which may result in an inaccurate final answer or hallucination.Changes in the environment could lead to inaccuracies in MCTS and subsequently in the thoughts provided to LLMs.However, LLMs have proven effective in revising thoughts by leveraging their internal knowledge, mitigating the risk associated with inaccuracies in the initial thought generation.Additionally, LLMs may make mistakes and sometimes deviate from the thoughts generated by the MCTS module, leading to errors.This aspect should be taken into consideration when employing the approach.</p>
<p>Conclusion</p>
<p>The XOT framework presented in this paper signifies a significant progression in thought generation for LLMs aimed at solving complex tasks.It challenges the constraints of the "Penrose Triangle " by concurrently achieving performance, efficiency, and flexibility, a feat unattainable by existing prompting paradigms.This accomplishment is achieved through the integration of MCTS with pretrained low-cost policy and value networks, by injecting domain knowledge and planning capability into LLMs, offloading thought searching, and facilitating unconstrained free-style thought exploration.The collaborative thought revision framework involving MCTS and LLM further enhances the quality of thought generation.Experimental evaluations conducted across three intricate real-world problems, namely the Game of 24, 8-Puzzle, and Pocket Cube, provide empirical evidence that our XOT framework significantly outperforms existing prompting paradigms, particularly in scenarios involving multi-solution problems.A LLAMA-2-13B SETUP</p>
<p>LLaMA-2-13B (finetuned).To evaluate the potential of directly distilling knowledge from simulations into a smaller model to possibly avoid using a large model like GPT-4 during testing, we fine-tuned the LLaMA-2-13B model.Our experiments were carried out on eight V100 GPUs, each with 80GB of memory, and lasted approximately 5 hours.The training setup involved 5 epochs, a train batch size of 32, an evaluation batch size of 1, and a single step for gradient accumulation.</p>
<p>The evaluation and save strategies were set to "no" and "steps" respectively, with saving occurring every 20 steps and a limit of one saved model.The learning rate was 2e-5, with no warmup steps and logging every 2 steps.We employed a cosine learning rate scheduler.By using ground truth labels-considered more accurate than labels from MCTS simulations-we aimed to convert an optimization or search problem into a more straightforward prediction or supervised learning challenge, using a training dataset of (question, answer) pairs.</p>
<p>B COMPUTATIONAL TRAINING COSTS OF MCTS</p>
<p>The number of training and testing policy/value model calls for XoT are listed in Table 12.We train this model through three iterations, each comprising 10 self-play episodes for MCTS.Offline pretraining serves as a one-time solution that reduces the computational burden of testing by integrating external knowledge.Methods like ToT and GoT, which rely solely on the LLMs' internal knowledge, do not require pretraining but necessitate frequent calls to LLM during testing.For example, the average number of LLM invocations for three tasks in ToT are 39.83, 54.13, and 56.58, averaging 50.18 times per test problem.The computational cost of these recurring calls during testing exceeds the pretraining cost of the policy/value model in XoT.</p>
<p>Futhermore, it's worth highlighting that GPT-3.5 boasts 175 billion parameters, and GPT-4 is estimated to have an astonishing over 1 trillion parameters.In contrast, the total number of parameters in the Policy/Value Network for all three tasks is approximately 1e6.This deliberate design choice results in a model significantly smaller than LLMs, ensuring efficiency even with additional calls during training.</p>
<p>C EXPERIMENT RESULTS ON OTHER NLP TASKS</p>
<p>In addition to the tasks employed in this paper, many other NLP tasks can be formulated as MCTS searching problems, using LLMs to get rewards and rendering XoT applicable to a broader range of scenarios.For example, in ToT Yao et al. (2023), the task of Creative Writing uses LLMs to evaluate the quality of generated paragraphs.In a similar vein, GoT Besta et al. (2023) utilizes LLMs to rate the outcomes of Document Merging tasks.This strategy of employing LLMs for reward design is gaining traction and is currently a subject of active research Kwon et al. (2023).</p>
<p>To illustrate, we present preliminary results for GPT-3.5 on the Document Merging task in Table 13, where the scores are indicative of a weighted combination of duplication and information intact in the merged document (the higher the better).The objective of this task is to create a new  2023) paper.We utilized the same dataset provided in their repository.</p>
<p>Remarkably, XoT emerges as the most effective approach, achieving the highest score of 8.168.Notably, XoT maintains a balance in resource efficiency, with an average token cost of 15270.80,surpassing both ToT and GoT.These outcomes underscore XoT's advanced capabilities in handling general textual tasks, extending beyond gaming problems.</p>
<p>D PROMPT EXAMPLE</p>
<p>Prompts 1-3 display example CoT prompts utilized for Game of 24, 8-Puzzle, and Pocket Cube.These templates are applicable to CoT, ToT, GoT, and our XOT in the final inference process.Each thought step includes the action taken and the resulting new state.</p>
<p>Instruction: Game of 24</p>
<p>Use numbers and basic arithmetic operations (+ -* /) to obtain 24.</p>
<p>Prompt: Game of 24</p>
<p>Input: 2 9 10 12 Steps: 12 * 2 = 24 (left: 9 10 24) Expression: 9, 10, (12) * (2) 10 -9 = 1 (left: 24 1) Expression: (12) * ( 2), ( 10) -( 9) 1 * 24 = 24 (left: 24) Expression: ((10) -( 9)) * (( 12</p>
<p>Figure 1 :
1
Figure 1: Comparison of XOT versus other prompting paradigms.</p>
<p>Figure 2 :
2
Figure 2: An illustration of iterative phases in MCTS for thought searching ((a)-(c)) and thought inference in problem resolution (d).</p>
<p>Figure 3 :
3
Figure 3: An illustration of thought revision process in XOT.</p>
<p>reach 24, e.g., 4 × 6 + 10 − 10 = 24.The slide sequence of the "-" tile, e.g., (Up, Down, Left, Right • • • ).The rotation move sequence of the cube, e.g., (F, R2, U' • • • ).</p>
<p>Figure 4 :
4
Figure 4: Accuracy, LLM and f θ invoked comparison on XOT w.r.t. the number of revisions.</p>
<p>Figure 5 :
5
Figure 5: Examples of thought structures generated by XOT for all three tasks in the multi-solution scenario.</p>
<p>Decision Making &amp; Planning with LLMs.The utilization of LLMs for decision-making and planning has become a prominent area of research.Similar to human problem-solving, the process involves breaking down complex problems into sub-tasks.Various frameworks, such as CoTWei et al. (2022),ToT Yao et al. (2023), and GoT Besta et al. (2023), have been designed to facilitate problem decomposition in different structural forms, leading to enhanced solutions derived from LLMs.Extensions of these frameworks have also been explored across different domains and modalitiesZhang et al. (2022;2023);Ning et al. (2023);Turpin et al. (2023);Long (2023).Our approach XOT distinguishes itself from the aforementioned work by concurrently achieving superior performance, efficiency, and flexibility, embodying the concept of comprehensive thought generation.Furthermore, the "Describe, Explain, Plan, and Select" framework introduced inWang et al. (2023b) presents an interactive planning approach for LLMs, significantly enhancing planning performance for multi-task agents.Research conducted inSingh et al. (2023) leverages LLMs to suggest next actions or sequences during task planning for robotics, leading to improved task performance across various metrics.Additionally, work presented inXie et al. (</p>
<p>Table 1 :
1
Comparisons of different prompting paradigms.
Paradigm Performance Efficiency FlexibilityIOCoTCoT-SCToTGoTXOT</p>
<p>Table 2 :
2
An overview of tasks employed in this study.
Game of 248-PuzzlePocket Cube</p>
<p>Table 3 :
3
Performance comparison on Game of 24.
ModelAcc. [%]GPT-3.5 LLMfθAcc. [%]GPT-4 LLMfθinvokedinvokedinvokedinvokedIO6.571.00-10.221.00-CoT2.191.00-4.381.00-CoT-SC2.1910.00-4.3810.00-ToT (b=1)5.8422.11-34.3123.50-ToT (b=3)10.2243.96-60.5839.83-GoT (k=1)2.927.00-10.957.00-LLaMA-2-13B2.19--2.19--MCTS62.77--62.77--XoT (w/ 1 r)79.561.3992.1574.451.3888.20XoT (w/ 2 r)88.321.5893.8783.941.5789.63XoT (w/ 3 r)90.511.7295.9485.401.7892.48</p>
<p>Table 4 :
4
Performance comparison on Game of 24 in the multi-solution scenario.
ModelMulti Acc.#SolGPT-3.5 LLM invokedf θ invokedMulti Acc.#SolGPT-4 LLM invokedf θ invokedIO4.872.881.00-8.272.991.00-CoT1.222.771.00-7.792.941.00-CoT-SC1.702.7610.00-8.032.9910.00-ToT (b=3)3.412.9943.96-39.90 2.7839.83-GoT (k=3)8.031.937.00-10.46 1.397.00-XoT (w/ 1 r) 62.90 2.293.51116.3476.25 2.362.31109.64</p>
<p>Table 5 :
5
Performance comparison on 8-Puzzle.
ModelAcc. [%]GPT-3.5 LLMfθAcc. [%]GPT-4 LLMfθinvokedinvokedinvokedinvokedIO0.001.00-1.681.00-CoT0.001.00-7.561.00-CoT-SC0.8410.00-8.4010.00-ToT (b=1)5.8831.76-3.3627.49-ToT (b=3)6.7255.86-13.4554.13-GoT (k=1)3.3619.00-3.3619.00-LLaMA-2-13B0.00--0.00--MCTS51.26--51.26--XoT (w/ 1 r)59.661.5041.0993.281.4855.66XoT (w/ 2 r)59.661.9242.1894.961.5558.91XoT (w/ 3 r)63.032.2942.6095.801.6162.22</p>
<p>Table 6 :
6
Performance comparison on 8-Puzzle in the multi-solution scenario.
ModelMulti Acc.#SolGPT-3.5 LLM invokedf θ invokedMulti Acc.#SolGPT-4 LLM invokedf θ invokedIO0.002.471.00-0.842.971.00-CoT1.432.051.00-7.841.211.00-CoT-SC1.541.9010.00-6.582.0810.00-ToT (b=3)2.522.9855.86-5.602.9754.13-GoT (k=3)3.362.9624.18-16.61 2.7022.76-XoT (w/ 1 r) 27.45 2.854.1952.0676.33 1.524.3066.66</p>
<p>Table 7 :
7
Performance comparison on Pocket Cube.
ModelAcc. [%]GPT-3.5 LLMfθAcc. [%]GPT-4 LLMfθinvokedinvokedinvokedinvokedIO1.091.00-1.091.00-CoT0.001.00-1.091.00-CoT-SC0.0010.00-1.0910.00-ToT (b=1)7.6516.50-11.4816.39-ToT (b=3)17.4958.72-19.5756.58-GoT (k=1)1.648.93-18.038.55-LLaMA-2-13B0.00--0.00--MCTS46.44--46.44--XoT (w/ 1 r)74.321.5564.6377.601.5475.51XoT (w/ 2 r)80.331.8196.4679.321.79146.52XoT (w/ 3 r)84.702.01103.2283.612.0084.63</p>
<p>Table 8 :
8
Performance comparison on Pocket Cube in the multi-solution scenario.
ModelMulti Acc.#SolGPT-3.5 LLM invokedf θ invokedMulti Acc.#SolGPT-4 LLM invokedf θ invokedIO0.272.001.00-1.091.981.00-CoT0.551.051.00-0.821.911.00-CoT-SC0.182.9010.00-0.822.921.00-ToT (b=3)5.832.9958.72-6.522.9956.58-GoT (k=3)1.092.9914.76-16.85 2.7713.36-XoT (w/ 1 r) 48.72 2.204.13115.7377.41 1.724.08122.54</p>
<p>Table 9 :
9
Revision Success Rate for GPT-3.5.
RevisionsGame of 24 8-Puzzle Pocket CubeXoT (w/ 1 r)47.17%20.00%53.00%XoT (w/ 2 r)69.81%21.31%63.64%XoT (w/ 3 r)75.93%26.67%72.00%</p>
<p>Table 10 :
10
Revision Success Rate for GPT-4.
RevisionsGame of 24 8-Puzzle Pocket CubeXoT (w/ 1 r)32.69%85.96%58.59%XoT (w/ 2 r)55.10%89.47%60.00%XoT (w/ 3 r)60.00%91.38%70.00%</p>
<p>Table 11 :
11
Performance comparison on three tasks with incomplete thoughts.Acc.[%] LLM invoked fθ invoked Acc.[%] LLM invoked fθ invoked
TaskModelGPT-3.5GPT-4ToT (b=1)3.6517.15-40.8818.55-Game of 24GoT (k=1)2.195.00-9.495.00-XoT (w/o revise)17.521.0068.7343.071.0068.70ToT (b=1)0.0032.60-6.7226.98-8-PuzzleGoT (k=1)0.0018.63-3.3619.00-XoT (w/o revise)2.521.0036.6640.341.0036.24ToT (b=1)0.5516.48-2.1916.39-Pocket CubeGoT (k=1)0.008.96-1.648.68-XoT (w/o revise)5.461.0018.856.011.0018.89Initial StateU2U'UR2R'R'R2</p>
<p>enhancing the preferability of generated text by LLMs Liu et al. (2023).Additionally, Feng et al., employ MCTS to augment LLMs' decoding and, consequently, their reasoning and planning capabilities</p>
<p>Hao et al. (2023)e "Reasoning via Planning", utilizing LLMs as a world model and reasoning agent, while combining MCTS as a strategic explorer to enhance LLMs' reasoning and planning abilitiesHao et al. (2023).Liu et al., incorporate MCTS and PPO Schulman et al. (2017)to devise a valueguided decoding algorithm, thereby</p>
<p>Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, Jae Sung Park, Ximing Lu, Rowan Zellers, Prithviraj Ammanabrolu, Ronan Le Bras, Gunhee Kim, et al.Fusing pre-trained language models with multimodal prompts through reinforcement learning.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.10845-10856, 2023.Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola.Multimodal chain-of-thought reasoning in language models.arXiv preprint arXiv:2302.00923,2023.
Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and ZongqingLu. Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. arXivpreprint arXiv:2303.16563, 2023.Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting inlarge language models. arXiv preprint arXiv:2210.03493, 2022.Zhuosheng Zhang,</p>
<p>Table 12 :
12
Number of policy/value model calls in training and testing per iteration for different tasks.Disclosure Agreement (NDA) document by combining several input documents that partially overlap in content.The aim is to minimize duplication while maximizing information retention.The experimental setting is aligned with in the GoTBesta et al. (
Game of 24 8-Puzzle Pocket CubeTraining1044.70834.70787.00Testing88.2055.6675.51Table 13: Performance comparison on Document Merging.Method Score (0-10) Cost (Avg num of tokens)IO6.3902292.60CoT6.5243152.90ToT7.71551486.00GoT7.55927685.28XoT8.16815270.80Non-</p>
<p>Process] is not correct since it does not reach the goal state in the end.If the final answer does not reach the goal state, then the corresponding [Process] is considered[wrong].Please help me identify the exact wrong step based on its left number, among [Step 1, Step 2, Step 3, ...].If you are uncertain about which step is wrong, please begin your analysis with[Step 1] for better understanding.Process] is not correct because number 3, 4, 0, 5 are not their goal positions in the end.The puzzle has failed on reaching its goal state.Now please help me identify the exact step number that is wrong.You must provide one wrong step.If you can not provide an exact step number, please consider that it could be "all steps are wrong".[Step4]iswrong,withMove:Right.Instruction: Pocket CubeYou are a virtual expert in solving a 2x2 Pocket Cube.Your task is to restore a scrambled 2x2 Rubik's Cube to its original state.All the given problems can be solved in 1 to 4 moves.You cannot exceed more than 11 moves.Provide the sequence of moves required for the restoration.Please follow the instructions and rules below to complete the solving: 1.A 2x2 Pocket Cube has six faces, namely: [Upper, Front, Bottom, Left, Right, Back] Each consisting of a 2x2 grid of squares, with each square having its own color.2.Colors in the Cube are represented in numbers: [0, 1, 2, 3, 4, 5] 3. The Cube's state is represented into a facelets expanding graph, for instance: Pocket Cube is to move squares in each face to have same numbers.You must make move to the Cube to achieve a Restored State, not limited to the above one.Note that we just need each face to have same numbers, no matter which face has which color.5.You are only allowed to use following moves [U, U', U2, R, R', R2, F, F', F2].["U": Turn the Upper face of the cube 90 degrees clockwise.For instance, after taking move Turn the Right face of the cube 90 degrees clockwise.For instance, after taking move Process] is not correct since it does not reach the goal state in the end.If the final answer does not reach the goal state, then the corresponding[Process]is considered [wrong].Please help me identify the exact wrong step based on its left number, among [Step 1, Step 2, Step 3, ...].If you are uncertain about which step is wrong, please begin your analysis with [Step 1] for better understanding.Please help me identify the exact step number that is wrong.You must provide one wrong step.After finishing all the moves: The Upper face still has 2 differnet colors.The Down face still has 2 differnet colors.The Left face still has 2 differnet colors.The Right face still has 2 differnet colors.The given [Process] is not correct because not every face has the same numbers in the end.The cube has failed on restoring to its original state.Now please help me identify the exact step number that is wrong.You must provide one wrong step.If you can not provide an exact step number, please consider that it could be "all steps are wrong".
7 8 5 Step 2: Choose one valid move from: [Left, Right, Up] 4 0 5 5 2 2 4 4 4 4 3 3 1 1 4 4 4 3After move 'Down': Move: Left 6 7 8 5 Right: 4 4 Left: 3 3 Back: 5 51 2 3 Current State: Finished. Or 5 5 Right: 4 4 Left: 5 5 [Step 2]) * (2)) 3. The next move must be chosen from the valid move set depending on the position of '0'. Answer: (12 * 2) * (10 -9) = 24 4 8 6 7 0 5 Before move: 1 2 3 4 0 6 7 8 5 After move 'Right': 1 2 3 4 6 0 7 8 5 Before move: 1 2 3 4 0 6 7 8 5 After move 'Up': 1 0 3 4 2 6 7 8 5 For example: p1 p2 p3 p4 p5 p6 p7 p8 p9 (1) If '0' is located at position 'p1', the valid move set is ['Right', 'Down']. (2) If '0' is located at position 'p2', the valid move set is ['Left', 'Right', 'Down']. (3) If '0' is located at position 'p3', the valid move set is ['Left', 'Down']. (4) If '0' is located at position 'p4', the valid move set is ['Right', 'Up', 'Down']. (5) If '0' is located at position 'p5', the valid move set is ['Left', 'Right', 'Up', 'Down']. (6) If '0' is located at position 'p6', the valid move set is ['Left', 'Up', 'Down']. (7) If '0' is located at position 'p7', the valid move set is ['Right', 'Up']. (8) If '0' is located at position 'p8', the valid move set is ['Left, 'Right', 'Up']. (9) If '0' is located at position 'p9', the valid move set is ['Left', 'Up']. 4. Diagonal moves are not allowed. 5. The objective is to return the moves which can reach the goal state. Prompt: 8-Puzzle All given problems can be solved within 1 to 9 steps. The next move must be chosen from the valid move set. The maximum step number you can take is 9. Try to reach the goal state using the least number of steps (≤9). <strong>DO NOT exceed 9 steps.</strong> [Initial State]: 3 1 2 6 4 5 7 8 0 [Process]: 3 1 2 6 4 5 7 8 0 Step 1: Choose one valid move from: [Left, Up] Move: Left Current State: 3 1 2 6 4 5 7 0 8 3 1 2 6 4 5 0 7 8 Step 3: Choose one valid move from: [Right, Up] Move: Up Current State: 3 1 2 0 4 5 6 7 8 Step 4: Choose one valid move from: [Right, Up] Move: Up Current State: 0 1 2 3 4 5 6 7 8 Finished. [Moves]: Left, Left, Up, Up Revision: 8-Puzzle The given [Please help me identify the exact step number that is wrong. You must provide one wrong step. [Initial State]: 3 1 2 6 4 5 7 8 0 [Process] 3 1 2 6 4 5 7 8 0 Step 1: Choose one valid move from: [Left, Up] Left 3 1 2 6 4 5 7 0 8 Step 2: Choose one valid move from: [Left, Right, Up] Left 3 1 2 6 4 5 0 7 8 Step 3: Choose one valid move from: [Right, Up] Up 3 1 2 0 4 5 6 7 8 Step 4: Choose one valid move from: [Right, Up] Right 3 1 2 [Restored State] Upper: 2 2 Front: 0 0 Down: 1 1 Back: 4 4 5 5 "U'": Turn the Upper face of the cube 90 degrees counterclockwise (or anti-clockwise). For instance, after taking move U': Upper: 0 0 0 0 1 1 1 1 Back: 5 5 5 5 will become Up: 0 0 4 4 Right: 1 1 1 1 Back: 5 5 5 5 will become 4 3 4 3 Right: 0 1 0 1 Back: 5 5 5 5 5 5 Finished. Now strictly follow the above process to form Restoration Moves. [Restoration Moves]: R U' F' Revision: Pocket Cube [Move] U' [Current Cube State] Upper: 0 0 4 4 Front: 0 1 0 1 The given [Upper: 0 0 0 0 Front: 5 5 2 2 Down: 3 3 3 3 Left: 1 1 4 4 Right: 4 4 1 1 Back: 2 2 5 5 4. A restoration of a Some example Restored States are: [Restored State] Upper: 0 0 0 0 Front: 2 2 2 2 Down: 3 3 3 3 Left: 4 4 4 4 Right: 1 1 1 1 Back: 3 3 Left: 1 1 Right: 4 4 Back: 5 5 U: Upper: 0 0 Front: 2 2 Down: 3 3 Left: 4 4 Right: 1 1 Back: 5 5 will become Up: 0 0 Front: 1 2 Down: 3 3 Left: Front: 2 2 2 2 Down: 3 3 3 3 Left: 4 4 4 4 Right: 1 1 1 1 Back: 5 5 5 5 will become Upper: 0 0 0 0 Front: 4 4 2 2 Down: 3 3 3 3 Left: 5 5 4 4 Right: 2 2 1 1 Back: 1 1 5 5 "U2": Turn the Upper face of the cube 180 degrees (a half turn). For instance, after taking move U2: Upper: 0 0 0 0 Front: 2 2 2 2 Down: 3 3 3 3 Left: 0 0 Front: 5 5 2 2 Down: 3 3 3 3 Left: 1 1 4 4 Right: 4 4 1 1 Back: 2 2 5 5 "R": R: Upper: 0 0 0 0 Front: 2 2 2 2 Down: 3 3 3 3 Left: 4 4 4 4 Right: 1 1 1 1 Back: 5 5 5 5 will become Upper: 0 2 0 2 Front: 2 3 2 3 Down: 3 5 3 5 Up: 0 3 0 3 Front: 2 5 2 5 Down: 3 0 3 0 Left: 4 4 4 4 Right: 1 1 1 1 Back: 2 5 2 5 "F": Turn the Front face of the cube 90 degrees clockwise. For instance, after taking move F: Upper: 0 0 0 0 Front: 2 2 2 2 Down: 3 3 3 3 Left: 4 4 4 4 Right: 1 1 1 1 Back: 5 5 5 5 will become Up: 0 0 4 4 Front: 2 2 2 2 Down: "F'": Turn the Front face of the cube 90 degrees counterclockwise. For instance, after taking move F': Upper: 0 0 0 0 Front: 2 2 2 2 Down: 3 3 3 3 Left: 4 4 4 4 Right: 1 1 1 1 Back: 5 5 5 5 will become Upper: 0 0 1 1 Front: 2 2 2 2 Down: 4 4 3 3 Left: 4 0 4 0 Right: 3 1 Back: Down: 4 3 2 2 2 2 2 2 Right: Front: 3 3 0 0 1 1 0 0 Left: Upper: 2 2 "F2": Turn the Front face of the cube 180 degrees. For instance, after taking move F2: 0 1 5 5 Down: 5 5 0 1 Back: 5 5 3 1 Down: 2 2 2 2 Left: 1 1 3 3 Right: 4 3 The given [[Initial Cube State]: 4 3 Upper: Back: 4 5 5 5 4 4 5 5 Front: [Step 3] 5 1 [Move] F2 5 0 [Current Cube State] Down: Upper: 0 0 0 0 2 0 1 1 Left: Front: 1 1 2 2 3 2 2 2 Right: Down: 2 2 4 4 4 3 3 3 Back: Left: 3 3 4 0 1 5 4 0 [Process]: Right: [Step 1] 3 1 [Move] R 3 1 [Current Cube State] Back: Upper: 5 5 4 0 5 5 4 0 Front: Finished.
[Step 3] is wrong, with Move: F2.</p>
<p>We named it "Everything of Thoughts" to signify its three comprehensive thought generation capabilities.
Revision: Game of 24Using the given[input]numbers and basic arithmetic operations (+, -, <em>, /), follow the steps strictly to achieve a result of 24.All the[input]numbers can reach 24 by basic arithmetic operations (+, -, </em>, /).If the final answer is not exactly 24, then the corresponding[Steps]The 8-puzzle consists of a 3x3 grid containing 8 numbered tiles (from 1 to 8) and one empty space (denoted by 0).Only 0 can be moved horizontally or vertically, and the objective is to reach the goal state from a given initial state.The goal state is typically the numbers ordered sequentially, with the 0 in the first position: [The goal state] 0 1 2 3 4 5 6 7 8 [Rules] 1.Only 0 can be moved horizontally or vertically.2. Each move is chosen from the following set of options: -'Left': move 0 to the left -'Down': move 0 downward -'Right': move 0 to the right -'Up': move 0 upward For example: Before move: 1 2 3 4 0 6 7 8 5 After move 'Left': 1 2 3 0 4 6 7 8 5 Before move:
I Calculated ChatGPT's IQ. </p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, arXiv:2308.096872023arXiv preprint</p>
<p>Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer, arXiv:2302.02662Grounding large language models in interactive environments with online reinforcement learning. 2023arXiv preprint</p>
<p>Empowering practical root cause analysis by large language models for cloud incidents. Yinfang Chen, Huaibing Xie, Minghua Ma, Yu Kang, Xin Gao, Liu Shi, Yunjie Cao, Xuedong Gao, Ming Hao Fan, Wen, arXiv:2305.157782023arXiv preprint</p>
<p>Minecraft as a creative tool: A case study. Maria Cipollone, Catherine C Schifter, Rick A Moffat, International Journal of Game-Based Learning (IJGBL). 422014</p>
<p>Solving the rubik's cube optimally is npcomplete. Sarah Erik D Demaine, Mikhail Eisenstat, Rudoy, arXiv:1706.067082017arXiv preprint</p>
<p>The untapped promise of digital mind maps. Haakon Faste, Honray Lin, Proceedings of the SIGCHI conference on human factors in computing systems. the SIGCHI conference on human factors in computing systems2012</p>
<p>Alphazerolike tree-search can guide large language model decoding and training. Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, Jun Wang, arXiv:2309.171792023arXiv preprint</p>
<p>Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, Julius Berner, arXiv:2301.13867Mathematical capabilities of chatgpt. 2023arXiv preprint</p>
<p>On upper-confidence bound policies for switching bandit problems. Aurélien Garivier, Eric Moulines, International Conference on Algorithmic Learning Theory. Springer2011</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Using modern graph analysis techniques on mind maps to help quantify learning. Peter Jamieson, Education Conference Proceedings. IEEE2012. 2012</p>
<p>Emre Kıcıman, Robert Ness, Amit Sharma, Chenhao Tan, arXiv:2305.00050Causal reasoning and large language models: Opening a new frontier for causality. 2023arXiv preprint</p>
<p>Reward design with language models. Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Yuxi Li, arXiv:1701.07274Deep reinforcement learning: An overview. 2017arXiv preprint</p>
<p>Making ppo even better: Value-guided monte-carlo tree search decoding. Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, Asli Celikyilmaz, arXiv:2309.150282023arXiv preprint</p>
<p>Large language model guided tree-of-thought. Jieyi Long, arXiv:2305.082912023arXiv preprint</p>
<p>Skeleton-of-thought: Large language models can do parallel decoding. Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, Yu Wang, arXiv:2307.153372023arXiv preprint</p>
<p>Chatgpt versus traditional question answering for knowledge graphs: Current status and future directions towards knowledge graph chatbots. Reham Omar, Omij Mangukiya, Panos Kalnis, Essam Mansour, arXiv:2302.064662023arXiv preprint</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Markov decision processes. Martin L Puterman, Handbooks in operations research and management science. 19902</p>
<p>Finding a shortest solution for the n x n extension of the 15-puzzle is intractable. Daniel Ratner, Manfred Warmuth, Proceedings of the Fifth AAAI National Conference on Artificial Intelligence. the Fifth AAAI National Conference on Artificial Intelligence1986</p>
<p>Multi-armed bandits with episode context. Christopher D Rosin, Annals of Mathematics and Artificial Intelligence. 6132011</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Mastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, nature. 55076762017</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. Kaya Stechly, Matthew Marquez, Subbarao Kambhampati, arXiv:2310.123972023arXiv preprint</p>
<p>Multi-agent collaboration: Harnessing the power of intelligent llm agents. Yashar Talebirad, Amirhossein Nadiri, arXiv:2306.033142023arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Open foundation and fine-tuned chat models. 20232arXiv preprint</p>
<p>Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel R Bowman, arXiv:2305.043882023arXiv preprint</p>
<p>Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. Priyan Vaithilingam, Tianyi Zhang, Elena L Glassman, Chi conference on human factors in computing systems extended abstracts. 2022</p>
<p>Can large language models really improve by self-critiquing their own plans?. Karthik Valmeekam, Matthew Marquez, Subbarao Kambhampati, arXiv:2310.081182023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023a</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang, arXiv:2302.015602023barXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, Harold Soh, arXiv:2302.05128Translating natural language to planning goals with large-language models. 2023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
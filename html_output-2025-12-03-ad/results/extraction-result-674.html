<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-674 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-674</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-674</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-0c5598424cc96d8fb500eb553cb7969f86a0ede0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0c5598424cc96d8fb500eb553cb7969f86a0ede0" target="_blank">Evaluating the Factual Consistency of Abstractive Text Summarization</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking.</p>
                <p><strong>Paper Abstract:</strong> Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e674.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e674.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>repro_data_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reproducibility / reusable-data gap for published experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors report being unable to replicate a prior summary re-ranking experiment because the original experimental setup depends on data that is not reusable, indicating a mismatch between the paper's experimental description and the availability of the code/data needed to reproduce it.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>summary ranking / re-ranking experiment</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An experimental setup (from Falke et al., 2019) that ranks candidate summaries by factual correctness; the authors attempted to reproduce that experiment to compare methods.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol / paper methods section</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>external experiment scripts and dataset (not provided or not reusable)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing reusable data / incomplete experimental specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The paper's described re-ranking experiment depends on data or artifacts that are not published in a reusable form (the 'experimental test setup does not rely on data that can be reused'), preventing replication of the experiment despite the description in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>experimental artifacts / dataset availability (evaluation setup)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>attempted replication by authors (manual attempt to re-run published experiment and compare results)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>qualitative failure to reproduce; authors explicitly state they were unable to replicate the experiment (no numeric metric available for replication attempt)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Blocked direct replication and side-by-side comparison; authors could not run the summary re-ranking experiment for direct comparison, limiting reproducibility and cross-method benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>not specified in paper (single noted instance regarding Falke et al., 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>experimental description lacks published, reusable data/artifacts or the original authors did not make required evaluation data reusable</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Release of authors' own code, training-data generation code, and trained model weights (the paper's authors released their artifacts at the cited GitHub) to enable reproducibility for their method; encourage publishing reusable evaluation setups.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially mitigates reproducibility for the authors' method (they released code and models), but does not restore the unavailable external experiment; no quantitative measure provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / summarization</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Factual Consistency of Abstractive Text Summarization', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e674.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e674.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>domain_mismatch_synth_vs_abstractive</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain and abstraction mismatch between synthetic (in-domain) training data and highly abstractive target summaries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synthetic weakly-supervised training data generated by the authors' transformation pipeline (single-sentence transformations) does not fully capture the level of abstraction present in some target datasets (e.g., XSum), causing degraded transfer performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FactCC training and evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A BERT-based factual consistency verifier trained on synthetically generated (document, claim) pairs using in-domain transformations (paraphrase, entity/number/pronoun swaps, negation, noise injection) and evaluated on CNN/DailyMail and XSum summarization outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper methods section (data-generation description)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>data generation scripts and model training code (Python, Transformers/PyTorch)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>dataset / transformation mismatch (incomplete specification of paraphrasing breadth)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The synthetic generation pipeline applies single-sentence transformations and backtranslation but lacks multi-sentence, highly abstractive paraphrasing transformations; as a result the trained verifier generalizes well to extractive/less-abstractive summaries (CNN/DM) but underperforms on highly abstractive summaries (XSum).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training data generation / augmentation (transformation set)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical evaluation across domains: compare model trained on synthetic in-domain data vs models trained on strong supervision from adjacent domains (MNLI, FEVER) on CNN/DM and XSum test sets</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Class-balanced (weighted) accuracy and F1 on manually annotated test sets (Table 3). Example numeric evidence: on XSum BERT+MNLI accuracy = 59.92 vs FactCC accuracy = 54.11 (≈5.8 percentage points higher for MNLI on XSum), while on CNN/DailyMail FactCC outperforms MNLI (72.65 vs 51.39).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Reduced accuracy on more abstractive summaries (XSum): the weakly-supervised in-domain model (FactCC) shows lower accuracy than MNLI on XSum, indicating that the synthetic training regime does not capture necessary abstraction patterns and thus limits cross-domain generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across the two evaluated datasets (CNN/DailyMail vs XSum) in the paper's experiments; specific to high-abstractive evaluation domains like XSum.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>limited set of synthetic transformations (single-sentence only), lacking multi-sentence paraphrasing and abstractive phenomena present in the target domain.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Extend the data generation pipeline with more abstractive, multi-sentence paraphrasing transformations and release data-generation code so others can extend transformations for target domains.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated quantitatively in this paper; authors state this as future work and hypothesize such extensions would improve transfer to abstractive domains.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / summarization / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Factual Consistency of Abstractive Text Summarization', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e674.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e674.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>transformation_coverage_gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coverage gap in synthetic transformation suite (commonsense, temporal, coreference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The set of rule-based transformations used to synthesize training data omits several error types found in model-generated summaries (commonsense, temporal inconsistencies, long-range coreference), so the verifier fails on these errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>synthetic training-data generator (GENERATE_DATA pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A programmatic pipeline that selects sentences from source documents and applies semantically invariant (T+) and semantically variant (T-) transformations (paraphrasing, entity/number/pronoun swaps, negation, noise) to create labeled training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>algorithm specification / methods description (transformation list and generation function)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>data generation scripts (Python, translation API calls, SpaCy NER)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / limited transformation coverage</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although the pipeline covers common lexical/syntactic perturbations (entity/number/pronoun swaps, negation, noise, backtranslation), it does not model commonsense errors, temporal inconsistencies, or incorrect coreference patterns; manual inspection shows models commonly err on these uncovered categories.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data synthesis (transformation design) and resulting training distribution</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual qualitative error analysis and inspection of misclassified examples (Section 5.2 qualitative study)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative counts and statements: authors report 'majority of errors were related to commonsense mistakes' and note systematic failures on temporality and coreference; no precise numeric breakdown provided.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Model misclassifications concentrate on error types not synthesized in training data, reducing verifier effectiveness on real model-generated summaries that contain these phenomena; limits practical utility for fully automatic factual verification.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Authors state 'majority of errors' were commonsense-related in their manual inspection (qualitative indication that these gaps are frequent among failures).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Design choice to synthesize single-sentence, rule-based transformations which did not attempt to model complex discourse-level, commonsense, or temporal reasoning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Augment synthetic transformation set to include temporal, multi-sentence paraphrasing, coreference perturbations, and adversarial commonsense/noise injections; provide data-generation code for community extension.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated within this paper; authors propose these as directions for future work and provide data-generation code to enable improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / data augmentation / weak supervision</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Factual Consistency of Abstractive Text Summarization', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e674.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e674.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>annotation_protocol_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Annotation protocol vs crowdsourcing quality mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The task of factual consistency verification is hard to crowdsource reliably with standard annotation setups; natural-language labeling instructions and typical crowdsourcing protocols produced low inter-annotator agreement unless many annotations per example are collected.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>human annotation and crowdsourcing pipeline for factual consistency labels</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Manual annotation of (document, sentence) pairs to produce validation and test sets and evaluation of span highlights; experiments with and without model-provided highlight assistance measured annotation speed and agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>annotation instructions / dataset documentation</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>crowdsourcing annotation task / human evaluation protocol (platform-dependent scripts and instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>task specification / instruction ambiguity and underestimation of required redundancy</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Authors attempted larger-scale crowdsourcing but observed low annotation quality and low inter-annotator agreement; prior work cited (Falke et al.) indicates κ reaches 0.75 only when 12 annotations per example are collected, making crowdsourcing costly and the simple annotation protocol insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data labeling stage (annotation protocol and quality control)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical measurement of annotation quality and inter-annotator agreement, both from authors' attempts and by referencing prior work</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Fleiss' κ and annotation timing reported in paper: without model highlights average time 224.89s and κ = 0.1571; with model highlights average time 178.34s and κ = 0.2526. Prior work (Falke et al.) reported κ = 0.75 only with 12 annotators per example.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Low-quality annotations and low agreement increase labeling costs and reduce reliability of validation/test sets; this scarcity motivates the authors' reliance on synthetic weak supervision rather than large manually labeled datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>High for this task: the paper reports widespread difficulty crowd-sourcing factual consistency labels unless annotation redundancy is very large; specific numbers: κ=0.1571 (no highlights), κ=0.2526 (with highlights) on sampled tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Task difficulty and ambiguity in natural language instructions, coupled with insufficient redundancy and quality-control mechanisms in crowd platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide model-generated span highlights to annotators (FactCCX) to guide judgments, apply annotation filters (Golden Aligned, Majority Aligned), and recommend higher redundancy; authors released an explainable model to assist annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Measured: highlights made annotators 21% faster (224.89s -> 178.34s) and increased Fleiss' κ by ~38% (from 0.1571 to 0.2526). Overlap metrics between model highlights and unbiased human highlights: article highlight containment accuracy ≈ 65.33% (raw data) and claim highlight F1 ≈ 0.6650, indicating moderate usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / human annotation / crowdsourcing</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating the Factual Consistency of Abstractive Text Summarization', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ranking generated summaries by correctness: An interesting but challenging application for natural language inference <em>(Rating: 2)</em></li>
                <li>On faithfulness and factuality in abstractive summarization <em>(Rating: 2)</em></li>
                <li>Assessing the factual accuracy of generated text <em>(Rating: 2)</em></li>
                <li>Neural text summarization: A critical evaluation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-674",
    "paper_id": "paper-0c5598424cc96d8fb500eb553cb7969f86a0ede0",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "repro_data_gap",
            "name_full": "Reproducibility / reusable-data gap for published experiments",
            "brief_description": "The authors report being unable to replicate a prior summary re-ranking experiment because the original experimental setup depends on data that is not reusable, indicating a mismatch between the paper's experimental description and the availability of the code/data needed to reproduce it.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "summary ranking / re-ranking experiment",
            "system_description": "An experimental setup (from Falke et al., 2019) that ranks candidate summaries by factual correctness; the authors attempted to reproduce that experiment to compare methods.",
            "nl_description_type": "experimental protocol / paper methods section",
            "code_implementation_type": "external experiment scripts and dataset (not provided or not reusable)",
            "gap_type": "missing reusable data / incomplete experimental specification",
            "gap_description": "The paper's described re-ranking experiment depends on data or artifacts that are not published in a reusable form (the 'experimental test setup does not rely on data that can be reused'), preventing replication of the experiment despite the description in the paper.",
            "gap_location": "experimental artifacts / dataset availability (evaluation setup)",
            "detection_method": "attempted replication by authors (manual attempt to re-run published experiment and compare results)",
            "measurement_method": "qualitative failure to reproduce; authors explicitly state they were unable to replicate the experiment (no numeric metric available for replication attempt)",
            "impact_on_results": "Blocked direct replication and side-by-side comparison; authors could not run the summary re-ranking experiment for direct comparison, limiting reproducibility and cross-method benchmarking.",
            "frequency_or_prevalence": "not specified in paper (single noted instance regarding Falke et al., 2019)",
            "root_cause": "experimental description lacks published, reusable data/artifacts or the original authors did not make required evaluation data reusable",
            "mitigation_approach": "Release of authors' own code, training-data generation code, and trained model weights (the paper's authors released their artifacts at the cited GitHub) to enable reproducibility for their method; encourage publishing reusable evaluation setups.",
            "mitigation_effectiveness": "Partially mitigates reproducibility for the authors' method (they released code and models), but does not restore the unavailable external experiment; no quantitative measure provided.",
            "domain_or_field": "natural language processing / summarization",
            "reproducibility_impact": true,
            "uuid": "e674.0",
            "source_info": {
                "paper_title": "Evaluating the Factual Consistency of Abstractive Text Summarization",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "domain_mismatch_synth_vs_abstractive",
            "name_full": "Domain and abstraction mismatch between synthetic (in-domain) training data and highly abstractive target summaries",
            "brief_description": "Synthetic weakly-supervised training data generated by the authors' transformation pipeline (single-sentence transformations) does not fully capture the level of abstraction present in some target datasets (e.g., XSum), causing degraded transfer performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "FactCC training and evaluation pipeline",
            "system_description": "A BERT-based factual consistency verifier trained on synthetically generated (document, claim) pairs using in-domain transformations (paraphrase, entity/number/pronoun swaps, negation, noise injection) and evaluated on CNN/DailyMail and XSum summarization outputs.",
            "nl_description_type": "paper methods section (data-generation description)",
            "code_implementation_type": "data generation scripts and model training code (Python, Transformers/PyTorch)",
            "gap_type": "dataset / transformation mismatch (incomplete specification of paraphrasing breadth)",
            "gap_description": "The synthetic generation pipeline applies single-sentence transformations and backtranslation but lacks multi-sentence, highly abstractive paraphrasing transformations; as a result the trained verifier generalizes well to extractive/less-abstractive summaries (CNN/DM) but underperforms on highly abstractive summaries (XSum).",
            "gap_location": "training data generation / augmentation (transformation set)",
            "detection_method": "empirical evaluation across domains: compare model trained on synthetic in-domain data vs models trained on strong supervision from adjacent domains (MNLI, FEVER) on CNN/DM and XSum test sets",
            "measurement_method": "Class-balanced (weighted) accuracy and F1 on manually annotated test sets (Table 3). Example numeric evidence: on XSum BERT+MNLI accuracy = 59.92 vs FactCC accuracy = 54.11 (≈5.8 percentage points higher for MNLI on XSum), while on CNN/DailyMail FactCC outperforms MNLI (72.65 vs 51.39).",
            "impact_on_results": "Reduced accuracy on more abstractive summaries (XSum): the weakly-supervised in-domain model (FactCC) shows lower accuracy than MNLI on XSum, indicating that the synthetic training regime does not capture necessary abstraction patterns and thus limits cross-domain generalization.",
            "frequency_or_prevalence": "Observed across the two evaluated datasets (CNN/DailyMail vs XSum) in the paper's experiments; specific to high-abstractive evaluation domains like XSum.",
            "root_cause": "limited set of synthetic transformations (single-sentence only), lacking multi-sentence paraphrasing and abstractive phenomena present in the target domain.",
            "mitigation_approach": "Extend the data generation pipeline with more abstractive, multi-sentence paraphrasing transformations and release data-generation code so others can extend transformations for target domains.",
            "mitigation_effectiveness": "Not evaluated quantitatively in this paper; authors state this as future work and hypothesize such extensions would improve transfer to abstractive domains.",
            "domain_or_field": "natural language processing / summarization / machine learning",
            "reproducibility_impact": true,
            "uuid": "e674.1",
            "source_info": {
                "paper_title": "Evaluating the Factual Consistency of Abstractive Text Summarization",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "transformation_coverage_gap",
            "name_full": "Coverage gap in synthetic transformation suite (commonsense, temporal, coreference)",
            "brief_description": "The set of rule-based transformations used to synthesize training data omits several error types found in model-generated summaries (commonsense, temporal inconsistencies, long-range coreference), so the verifier fails on these errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "synthetic training-data generator (GENERATE_DATA pipeline)",
            "system_description": "A programmatic pipeline that selects sentences from source documents and applies semantically invariant (T+) and semantically variant (T-) transformations (paraphrasing, entity/number/pronoun swaps, negation, noise) to create labeled training examples.",
            "nl_description_type": "algorithm specification / methods description (transformation list and generation function)",
            "code_implementation_type": "data generation scripts (Python, translation API calls, SpaCy NER)",
            "gap_type": "incomplete specification / limited transformation coverage",
            "gap_description": "Although the pipeline covers common lexical/syntactic perturbations (entity/number/pronoun swaps, negation, noise, backtranslation), it does not model commonsense errors, temporal inconsistencies, or incorrect coreference patterns; manual inspection shows models commonly err on these uncovered categories.",
            "gap_location": "data synthesis (transformation design) and resulting training distribution",
            "detection_method": "manual qualitative error analysis and inspection of misclassified examples (Section 5.2 qualitative study)",
            "measurement_method": "Qualitative counts and statements: authors report 'majority of errors were related to commonsense mistakes' and note systematic failures on temporality and coreference; no precise numeric breakdown provided.",
            "impact_on_results": "Model misclassifications concentrate on error types not synthesized in training data, reducing verifier effectiveness on real model-generated summaries that contain these phenomena; limits practical utility for fully automatic factual verification.",
            "frequency_or_prevalence": "Authors state 'majority of errors' were commonsense-related in their manual inspection (qualitative indication that these gaps are frequent among failures).",
            "root_cause": "Design choice to synthesize single-sentence, rule-based transformations which did not attempt to model complex discourse-level, commonsense, or temporal reasoning errors.",
            "mitigation_approach": "Augment synthetic transformation set to include temporal, multi-sentence paraphrasing, coreference perturbations, and adversarial commonsense/noise injections; provide data-generation code for community extension.",
            "mitigation_effectiveness": "Not evaluated within this paper; authors propose these as directions for future work and provide data-generation code to enable improvements.",
            "domain_or_field": "natural language processing / data augmentation / weak supervision",
            "reproducibility_impact": true,
            "uuid": "e674.2",
            "source_info": {
                "paper_title": "Evaluating the Factual Consistency of Abstractive Text Summarization",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "annotation_protocol_mismatch",
            "name_full": "Annotation protocol vs crowdsourcing quality mismatch",
            "brief_description": "The task of factual consistency verification is hard to crowdsource reliably with standard annotation setups; natural-language labeling instructions and typical crowdsourcing protocols produced low inter-annotator agreement unless many annotations per example are collected.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "human annotation and crowdsourcing pipeline for factual consistency labels",
            "system_description": "Manual annotation of (document, sentence) pairs to produce validation and test sets and evaluation of span highlights; experiments with and without model-provided highlight assistance measured annotation speed and agreement.",
            "nl_description_type": "annotation instructions / dataset documentation",
            "code_implementation_type": "crowdsourcing annotation task / human evaluation protocol (platform-dependent scripts and instructions)",
            "gap_type": "task specification / instruction ambiguity and underestimation of required redundancy",
            "gap_description": "Authors attempted larger-scale crowdsourcing but observed low annotation quality and low inter-annotator agreement; prior work cited (Falke et al.) indicates κ reaches 0.75 only when 12 annotations per example are collected, making crowdsourcing costly and the simple annotation protocol insufficient.",
            "gap_location": "data labeling stage (annotation protocol and quality control)",
            "detection_method": "empirical measurement of annotation quality and inter-annotator agreement, both from authors' attempts and by referencing prior work",
            "measurement_method": "Fleiss' κ and annotation timing reported in paper: without model highlights average time 224.89s and κ = 0.1571; with model highlights average time 178.34s and κ = 0.2526. Prior work (Falke et al.) reported κ = 0.75 only with 12 annotators per example.",
            "impact_on_results": "Low-quality annotations and low agreement increase labeling costs and reduce reliability of validation/test sets; this scarcity motivates the authors' reliance on synthetic weak supervision rather than large manually labeled datasets.",
            "frequency_or_prevalence": "High for this task: the paper reports widespread difficulty crowd-sourcing factual consistency labels unless annotation redundancy is very large; specific numbers: κ=0.1571 (no highlights), κ=0.2526 (with highlights) on sampled tasks.",
            "root_cause": "Task difficulty and ambiguity in natural language instructions, coupled with insufficient redundancy and quality-control mechanisms in crowd platforms.",
            "mitigation_approach": "Provide model-generated span highlights to annotators (FactCCX) to guide judgments, apply annotation filters (Golden Aligned, Majority Aligned), and recommend higher redundancy; authors released an explainable model to assist annotators.",
            "mitigation_effectiveness": "Measured: highlights made annotators 21% faster (224.89s -&gt; 178.34s) and increased Fleiss' κ by ~38% (from 0.1571 to 0.2526). Overlap metrics between model highlights and unbiased human highlights: article highlight containment accuracy ≈ 65.33% (raw data) and claim highlight F1 ≈ 0.6650, indicating moderate usefulness.",
            "domain_or_field": "natural language processing / human annotation / crowdsourcing",
            "reproducibility_impact": true,
            "uuid": "e674.3",
            "source_info": {
                "paper_title": "Evaluating the Factual Consistency of Abstractive Text Summarization",
                "publication_date_yy_mm": "2019-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
            "rating": 2
        },
        {
            "paper_title": "On faithfulness and factuality in abstractive summarization",
            "rating": 2
        },
        {
            "paper_title": "Assessing the factual accuracy of generated text",
            "rating": 2
        },
        {
            "paper_title": "Neural text summarization: A critical evaluation",
            "rating": 2
        }
    ],
    "cost": 0.014317,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating the Factual Consistency of Abstractive Text Summarization</h1>
<p>Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher Salesforce Research<br>{kryscinski,bmccann,cxiong,rsocher}@salesforce.com</p>
<h4>Abstract</h4>
<p>The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factcc.</p>
<h2>1 Introduction</h2>
<p>The goal of text summarization is to transduce long documents into a shorter form that retains the most important aspects from the source document. Common approaches to summarization are extractive (Dorr et al., 2003; Nallapati et al., 2017) where models directly copy salient parts of the source document into the summary, abstractive (Rush et al., 2015; Paulus et al., 2017) where
the important parts are paraphrased to form novel sentences, and hybrid (Gehrmann et al., 2018), combining the two methods by employing specialized extractive and abstractive components.</p>
<p>Advancements in neural architectures (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), transfer learning (McCann et al., 2017; Devlin et al., 2018), and availability of large-scale supervised datasets (Nallapati et al., 2016; Grusky et al., 2018) allowed deep learning-based approaches to dominate the field. State-of-the-art solutions utilize self-attentive Transformer blocks (Liu, 2019; Liu and Lapata, 2019), attention and copying mechanisms (See et al., 2017; Cohan et al., 2018), and multi-objective training strategies (Guo et al., 2018; Pasunuru and Bansal, 2018), including reinforcement learning techniques (Kryściński et al., 2018; Dong et al., 2018; Wu and Hu, 2018).</p>
<p>Despite significant efforts made by the research community, there are still many challenges limiting progress in summarization: insufficient evaluation protocols that omit important dimensions, such as factual consistency, noisy datasets that leave the task underconstrained, and strong, domain-specific layout biases in the data that dominate training signal (Kryściński et al., 2019).</p>
<p>We address the problem of verifying factual consistency between source documents and generated summaries: a factually consistent summary contains only statements that are entailed by the source document. Recent studies show that up to $30 \%$ of summaries generated by abstractive models contain factual inconsistencies (Cao et al., 2018; Goodrich et al., 2019; Falke et al., 2019; Kryściński et al., 2019). Such high levels of factual inconsistency render automatically generated summaries virtually useless in practice.</p>
<p>The problem of factual consistency is closely related to natural language inference (NLI) and fact</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Source article fragments</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(CNN) The mother of a quadriplegic man who police say was left in the woods for days cannot be extradited to face charges in Philadelphia until she completes an unspecified "treatment," Maryland police said Monday. The Montgomery County (Maryland) Department of Police took Nyia Parler, 41, into custody Sunday (...)</td>
<td style="text-align: center;">(CNN) The classic video game "Space Invaders" was developed in Japan back in the late 1970's - and now their real-life counterparts are the topic of an earnest political discussion in Japan's corridors of power. Luckily, Japanese can sleep soundly in their beds tonight as the government's top military official earnestly revealed that (...)</td>
</tr>
<tr>
<td style="text-align: center;">Model generated claims</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Quadriplegic man Nyia Parler, 41, left in woods for days can not be extradited.</td>
<td style="text-align: center;">Video game "Space Invaders" was developed in Japan back in 1970.</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples of factually incorrect claims output by summarization models. Green text highlights the support in the source documents for the generated claims, red text highlights the errors made by summarization models.
checking. Current NLI datasets (Bowman et al., 2015; Williams et al., 2018) focus on classifying logical entailment between short, single sentence pairs, but verifying factual consistency requires the entire source document. Fact checking focuses on verifying facts against the whole of available knowledge, whereas factual consistency checking focuses on adherence of facts to information provided by a source document without guarantee that the information is true.</p>
<p>We propose a novel, weakly-supervised BERTbased (Devlin et al., 2018) model for verifying factual consistency, and we add specialized modules that explain which portions of both the source document and generated summary are pertinent to the model's decision. Training data is generated from source documents by applying a series of rule-based transformations that were inspired by error-analysis of neural summarization model outputs. Through human evaluation we show that the explanatory modules that augment our factual consistency model provide useful assistance to humans as they verify the factual consistency between a source document and generated summaries. Together with this manuscript we release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC.</p>
<h2>2 Related Work</h2>
<p>This work builds on prior research in factual consistency in text summarization and natural language generation. Goodrich et al. (2019) proposed an automatic, model-dependent metric for evaluating the factual accuracy of generated text. Facts are represented as subject-relation-object triplets and factual accuracy is defined as the pre-
cision between facts extracted from the summary and source document. Despite positive results, the authors highlighted remaining challenges, such as its inability to adapt to negated relations or relation names expressed by synonyms.</p>
<p>A parallel line of research focused on improving factual consistency of summarization models by exploring different architectural choices and strategies for both training and inference. In Falke et al. (2019), the authors proposed re-ranking potential summaries based on factual correctness during beam search. The solution used textual entailment (NLI) models to score summaries by means of the entailment probability between all source document-summary sentence pairs. The summary with the highest aggregate entailment score was used as the final output of the summarization model. The authors concluded that out-of-the-box NLI models do not transfer well to the task of factual correctness. In Cao et al. (2018), the authors proposed a novel, dual-encoder architecture that in parallel encodes the source documents and all the facts contained in them. During generation, the decoder attends to both the encoded source and facts which, according to the authors, forces the output to be conditioned on the both inputs. Human evaluation showed that the proposed technique substantially lowered the number of errors in generated single-sentence summaries.</p>
<p>The synthetic data generation process proposed as part of our approach is based on prior work done in the domains of data augmentation and weakly-supervised learning. Wei and Zou (2019) proposed an augmentation framework aimed at boosting performance of text classification models. The authors used 4 text transformations to synthesize data: synonym replacement, random insertion, random swap, random deletion,</p>
<p>and showed increased performance of classifiers on 5 downstream tasks, both for convolutional and recurrent neural models. In (Sennrich et al., 2015; Edunov et al., 2018) the authors introduced and analyzed the effects of using backtranslation based data augmentation on the performance of machine translation models, while Iyyer et al. (2018) used the mentioned transformation to synthesize training data for a paraphrase generation network. Meng et al. (2018) investigated a two step approach for training text classification models on weakly-supervised data, which includes pretraining models on fully synthetic data.</p>
<h2>3 Methods</h2>
<p>A careful study of the outputs of state-of-the-art summarization models provided us with valuable insights about the specifics of factual errors made during generation and possible means of detecting them. Primarily, checking factual consistency on a sentence-sentence level, where each sentence of the summary is verified against each sentence from the source document, is insufficient. Some cases might require a longer, multi-sentence context from the source document due to ambiguities present in either of the compared sentences. Summary sentences might paraphrase multiple fragments of the source document, while source document sentences might use certain linguistic constructs, such as coreference, which bind different parts of the document together. In addition, errors made by summarization models are most often related to the use of incorrect entity names, numbers, and pronouns. Other errors such as negations and common sense error occur less often. Taking these insights into account, we propose and test a document-sentence approach for factual consistency checking, where each sentence of the summary is verified against the entire body of the source document.</p>
<h3>3.1 Training data</h3>
<p>Currently, there are no training datasets for factual consistency checking. Creating a large-scale, highquality dataset with supervision collected from human annotators is expensive and time consuming. We consider an alternative approach to acquiring training data that is highly scalable.</p>
<p>Considering the state of summarization, in which the level of abstraction of generated summaries is low and models mostly para-
phrase single sentences and short spans from the source (Kryściński et al., 2018; Zhang et al., 2018), we propose using a synthetic, weaklysupervised dataset for the task at hand. Our data creation method requires an unannotated collection of source documents in the same domain as the summarization models that are to be checked. Examples are created by first sampling single sentences, later referred to as claims, from the source documents. Claims then pass through a set of textual transformations that output novel sentences with both positive and negative labels. Though transformations are applied to single sentences, we found that, in keeping with our aforementioned observations of model-generated summaries, the process of verifying their consistency often requires referring to the entire document. A detailed description of the data generation function is presented in Figure 1. The benefit of using a synthetic dataset is that it allows for creation of large volumes of data at a marginal cost. The data generation process also allows to collect additional metadata that can be used in the training process. In our case, the metadata contains information about the original location of the extracted claim in the source document and the locations in the claim where text transformations were applied.</p>
<p>Our data generation process draws inspiration from data augmentation and adversarial example generation techniques in NLP (Iyyer et al., 2018; Wu et al., 2019; Zhang et al., 2019; Wei and Zou, 2019). The proposed process incorporates both semantically invariant $\left(\mathcal{T}^{+}\right)$, and variant $\left(\mathcal{T}^{-}\right)$ text transformations to generate novel claims with CORRECT and INCORRECT labels accordingly. This work uses the following transformations:</p>
<p>Paraphrasing A paraphrasing transformation covers cases where source document sentences are rephrased by the summarization model. Paraphrases were produced by backtranslation using Neural Machine Translation systems (Iyyer et al., 2018). The claim sentence was translated to an intermediate language and translated back to English yielding a semantically-equivalent sentence with minor syntactic and lexical changes. French, German, Chinese, Spanish, and Russian were used as intermediate languages. These languages were chosen based on the performance of recent NMT systems with the expectation that well-performing languages could ensure better translation quality.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Transformation</th>
<th style="text-align: left;">Original sentence</th>
<th style="text-align: left;">Transformed sentence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Paraphrasing</td>
<td style="text-align: left;">Sheriff Lee Baca has now decided to recall some <br> 200 badges his department has handed out to local <br> politicians just two weeks after the picture was re- <br> leased by the U.S. attorney's office in support of <br> bribery charges against three city officials.</td>
<td style="text-align: left;">Two weeks after the US Attorney's Office issued <br> photos to support bribery allegations against three <br> municipal officials, Lee Baca has now decided to <br> recall about 200 badges issued by his department <br> to local politicians.</td>
</tr>
<tr>
<td style="text-align: left;">Sentence negation</td>
<td style="text-align: left;">Snow was predicted later in the weekend for At- <br> lanta and areas even further south.</td>
<td style="text-align: left;">Snow wasn't predicted later in the weekend for At- <br> lanta and areas even further south.</td>
</tr>
<tr>
<td style="text-align: left;">Pronoun swap</td>
<td style="text-align: left;">It comes after his estranged wife Mona Dotcom <br> filed a $\$ 20$ million legal claim for cash and assets.</td>
<td style="text-align: left;">It comes after your estranged wife Mona Dotcom <br> filed a $\$ 20$ million legal claim for cash and assets.</td>
</tr>
<tr>
<td style="text-align: left;">Entity swap</td>
<td style="text-align: left;">Charlton coach Guy Luzon had said on Monday: <br> 'Alou Diarra is training with us.'</td>
<td style="text-align: left;">Charlton coach Bordeaux had said on Monday: <br> 'Alou Diarra is training with us.'</td>
</tr>
<tr>
<td style="text-align: left;">Number swap</td>
<td style="text-align: left;">He says he wants to pay off the $\$ 12.6$ million lien <br> so he can sell the house and be done with it.</td>
<td style="text-align: left;">He says he wants to pay off the $\$ 3.45$ million lien <br> so he can sell the house and be done done with it.</td>
</tr>
<tr>
<td style="text-align: left;">Noise injection</td>
<td style="text-align: left;">Snow was predicted later in the weekend for At- <br> lanta and areas even further south.</td>
<td style="text-align: left;">Snow was was predicted later in the weekend for <br> Atlanta and areas even further south.</td>
</tr>
</tbody>
</table>
<p>Table 2: Examples of text transformations used to generate training data. Green and red text highlight the changes made by the transformation. Paraphrasing is a semantically invariant transformation, Sentence negation, entity, pronoun, and number swaps are semantically variant transformation.</p>
<p>We used the Google Cloud Translation API ${ }^{1}$ for translations.</p>
<p>Entity and Number swapping To learn how to identify examples where the summarization model used incorrect numbers or entities during generation we used the Entity and Number swapping transformation. An NER system was applied to both the claim and source document to extract all entities. To generate a novel, semantically changed claim, an entity in the claim sentence was replaced with an entity from the document. Both of the swapped entities were chosen at random while ensuring that they were unique. Extracted entities were divided into two groups, named entities, covering person, location and institution names, and numbers, such as dates and all other numeric values. Entities were swapped within their respective groups. We used the SpaCy NER tagger (Honnibal and Montani, 2017).</p>
<p>Pronoun swapping To learn how to find incorrect pronoun use in claims we used a pronoun swapping transformation. First, all gender-specific pronouns were first extracted from the claim. Next, a randomly chosen pronoun was swapped with a different one from the same pronoun group to ensure syntactic correctness, i.e. a possessive pronoun could only be replaced with another possessive pronoun. New sentences were considered semantically variant.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Sentence negation To give the consistency checking model the ability to handle negated sentences we used a sentence negation transformation. First, a claim was scanned in search of auxiliary verbs. To switch the meaning, a randomly chosen auxiliary verb was replaced with its negation. Positive sentences would be negated by adding not or $n$ 'r after the verb, negative sentences would be switched by removing the negation.</p>
<p>Noise injection Given that verified summaries are generated by neural networks, they should be expected to contain certain types of noise. In order to make the factual consistency model robust to such generation errors, training examples were injected with noise. For each token in a claim the decision was made whether noise should be added at the given position with a preset probability. If noise should be injected, the token was randomly duplicated or removed from the sequence. Examples of all transformations are shown in Table 2.</p>
<h3>3.2 Development and test data</h3>
<p>Apart from the synthetic training set, separate, manually annotated, validation and test sets were created. Both of the annotated sets used summaries output by state-of-the-art summarization models. Each summary was split into sentences and all (document, sentence) pairs were annotated by the authors of this work. Since the focus was to collect data for verifying the factual consistency of summarization models, any unreadable sentences</p>
<h1>Require:</h1>
<p>S - set of source documents
$\mathcal{T}^{+}$- set of semantically invariant transformations
$\mathcal{T}^{-}$- set of semantically variant transformations
function GENERATE_DATA $\left(\mathcal{S}, \mathcal{T}^{+}, \mathcal{T}^{-}\right)$
$\mathcal{D} \leftarrow \emptyset \quad \triangleright$ set of generated data points
for $d o c$ in $\mathcal{S}$ do
doc_sents $\leftarrow$ sentence_tokenizer (doc)
sent $\leftarrow$ choose_random(doc_sents)
$\mathcal{D} \leftarrow \mathcal{D} \cup{(d o c, s e n t,+})}$
for $f$ n in $\mathcal{T}^{+}$do
new_sent $\leftarrow \mathrm{fn}($ doc, sent $)$
$\mathcal{D} \leftarrow \mathcal{D} \cup{($ doc, new_sent, +$)}$
end for
end for
for example in $\mathcal{D}$ do
doc, sent,.. $\leftarrow$ example
for $f$ n in $\mathcal{T}^{-}$do
new_sent $\leftarrow \mathrm{fn}($ doc, sent $)$
$\mathcal{D} \leftarrow \mathcal{D} \cup{($ doc, new_sent, -$})$
end for
end for
return $\mathcal{D}$
end function
Figure 1: Procedure to generate synthetic training data. $\mathcal{S}$ is a set of source documents, $\mathcal{T}^{+}$is a set of semantically invariant text transformations, $\mathcal{T}^{-}$is a set of semantically variant text transformations, + is a positive label, - is a negative label.
caused by poor generation were not labeled. The validation set consists of 931 examples, the test set contains 503 examples. The model outputs used for annotation were provided by the authors of papers: Hsu et al. (2018); Gehrmann et al. (2018); Jiang and Bansal (2018); Chen and Bansal (2018); See et al. (2017); Kryściński et al. (2018); Li et al. (2018); Pasunuru and Bansal (2018); Zhang et al. (2018); Guo et al. (2018).</p>
<p>Effort was made to collect a larger set of annotations through crowdsourcing platforms, however the inter-annotator agreement and general quality of annotations was too low to be considered reliable. This aligns with the conclusions of (Falke et al., 2019), where the authors showed that for the task of factual consistency the interannotator agreement coefficient $\kappa$ reached 0.75 only when 12 annotations were collected for each example. This in turn yields high annotations costs that our approach aims to circumvent.</p>
<h3>3.3 Models</h3>
<p>Considering the significant improvements in natural language understanding (NLU) tasks (including NLI) coming from using pre-trained</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">CNN/DailyMail</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XSum</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Accuracy <br> (weighted)</td>
<td style="text-align: center;">F1-score</td>
<td style="text-align: center;">Accuracy <br> (weighted)</td>
<td style="text-align: center;">F1-score</td>
</tr>
<tr>
<td style="text-align: center;">BERT+MNLI</td>
<td style="text-align: center;">51.39</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">59.92</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: center;">BERT+FEVER</td>
<td style="text-align: center;">52.07</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">55.23</td>
<td style="text-align: center;">0.26</td>
</tr>
<tr>
<td style="text-align: center;">FactCC (ours)</td>
<td style="text-align: center;">72.65</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">54.11</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: center;">FactCCX (ours)</td>
<td style="text-align: center;">72.88</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">53.05</td>
<td style="text-align: center;">0.60</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance of models evaluated by means of weighted (class-balanced) accuracy and F1 score on the manually annotated test set of CNN/DailyMail (this work) and XSum (Maynez et al., 2020).</p>
<p>Transformer-based models ${ }^{2}$, we decided to use BERT (Devlin et al., 2018) as the base model for our work. An uncased, base (110M params) BERT architecture was used as the starting checkpoint and fine-tuned on the generated training data. The source document and claim sentence were fed as input to the model and two-way classification (CONSISTENT/INCONSISTENT) was done using a single-layer classifier based on the [CLS] token. We refer to this model as the factual consistency checking model (FactCC).</p>
<p>We also trained a version of FactCC with additional span selection heads using supervision of start and end indices for selection and transformation spans in the source and claim. Span selection heads allow the model not only to classify the consistency of the claim, but also highlight spans in the source document that contain the support for the claim and spans in the claim where a possible mistake was made. We refer to this model as the factual consistency checking model with explanations (FactCCX).</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental Setup</h3>
<p>Training data was generated as described in Section 3.1 using news articles from the CNN/DailyMail (Nallapati et al., 2016) dataset as source documents. 1,003,355 training examples were created, out of which $50.2 \%$ were labeled as negative (INCONSISTENT) and the remaining $49.8 \%$ were labeled as positive (CONSISTENT). Models were evaluated in two settings: 1) with summaries from models trained on the CNN/DailyMail (Nallapati et al., 2016) dataset, which contains longer and more extractive reference summaries, and 2) with summaries from models trained on the XSum (Narayan et al.,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Article</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">(CNN) Blues legend B.B. King was hospitalized for dehydration, though the ailment didn't keep him out for long. King's</td>
</tr>
<tr>
<td style="text-align: left;">dehydration was caused by his Type II diabetes, but he "is much better," his daughter, Claudette King, told the Los Angeles</td>
</tr>
<tr>
<td style="text-align: left;">Times. The legendary guitarist and vocalist released a statement thanking those who have expressed their concerns. 'T'm</td>
</tr>
<tr>
<td style="text-align: left;">feeling much better and am leaving the hospital today," King said in a message Tuesday. Angela Moore, a publicist for</td>
</tr>
<tr>
<td style="text-align: left;">Claudette King, said later in the day that he was back home resting and enjoying time with his grandchildren. "He was</td>
</tr>
<tr>
<td style="text-align: left;">struggling before, and he is a trouper," Moore said. "He wasn't going to let his fans down." (...)</td>
</tr>
<tr>
<td style="text-align: left;">Claim</td>
</tr>
<tr>
<td style="text-align: left;">Angela Moore was back home resting and enjoying time with his grandchildren.</td>
</tr>
</tbody>
</table>
<p>Table 4: Example of a test pair correctly classified as incorrect and highlighted by our explainable model. Orange text indicates the span of the source documents that should contain support for the claim. Red text indicates the span of the claim that was selected as incorrect.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Sentence Pair Ranking</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Incorrect</td>
<td style="text-align: center;">$\Delta$</td>
</tr>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">$50.0 \%$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DA (Falke et al., 2019)</td>
<td style="text-align: center;">$42.6 \%$</td>
<td style="text-align: center;">-7.4</td>
</tr>
<tr>
<td style="text-align: left;">InferSent (Falke et al., 2019)</td>
<td style="text-align: center;">$41.3 \%$</td>
<td style="text-align: center;">-8.7</td>
</tr>
<tr>
<td style="text-align: left;">SSE (Falke et al., 2019)</td>
<td style="text-align: center;">$37.3 \%$</td>
<td style="text-align: center;">-12.7</td>
</tr>
<tr>
<td style="text-align: left;">ESIM (Falke et al., 2019)</td>
<td style="text-align: center;">$32.4 \%$</td>
<td style="text-align: center;">-17.6</td>
</tr>
<tr>
<td style="text-align: left;">BERT (Falke et al., 2019)</td>
<td style="text-align: center;">$35.9 \%$</td>
<td style="text-align: center;">-14.1</td>
</tr>
<tr>
<td style="text-align: left;">BERT+FEVER (ours)</td>
<td style="text-align: center;">$34.3 \%$</td>
<td style="text-align: center;">-15.7</td>
</tr>
<tr>
<td style="text-align: left;">BERT+MNLI (ours)</td>
<td style="text-align: center;">$32.1 \%$</td>
<td style="text-align: center;">-17.9</td>
</tr>
<tr>
<td style="text-align: left;">FactCC (ours)</td>
<td style="text-align: center;">$\mathbf{3 0 . 0 \%}$</td>
<td style="text-align: center;">$\mathbf{- 2 0 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 5: Percentage of incorrectly ordered sentence pairs using different consistency prediction models and crowdsourced human performance on the dataset.
2018) dataset, which contains single-sentence, highly abstractive reference summaries. The CNN/DailyMail validation and test sets were manually annotated, as described in Section 3.2, while the XSum test data was collected by Maynez et al. (2020)</p>
<p>Models were implemented using the Transformers library (Wolf et al., 2019) written in PyTorch (Paszke et al., 2017). Models were trained for 10 epochs using batch size of 12 examples and learning rate of $2 \in-5$. Experiments were conducted on 8 Nvidia V100 GPUs, training took 23 hours on average. Best model checkpoints were chosen based on the performance on the validation set, final model performance was evaluated on the test set.</p>
<h3>4.2 Results</h3>
<p>To understand whether datasets for related tasks transfer to the task of verifying factual consistency of summarization models, we trained factual consistency checking models on the MNLI entailment data (Williams et al., 2018) and FEVER
fact-checking data (Thorne et al., 2018). For fair comparison, before training, we removed examples assigned to the neutral class from both of the datasets. Table 3 shows the performance of trained models evaluated by means of class-balanced accuracy and F1 score. Both FactCC and FactCCX models substantially outperform classifiers trained on the MNLI and FEVER datasets when evaluated on the CNN/DailyMail test set. However, on the more abstractive XSum data, results show a reverse trend with the MNLI model achieving highest performance. Considering that both MNLI and FEVER datasets contain abstractive, human-written claims, while our data generation pipeline lacks multi-sentence paraphrasing transformations, such outcome was expected. In the case of more extractive outputs, the results suggests priority of weak-supervision in-domain over strong-supervision in adjacent domains for factual consistency checking.</p>
<p>To compare our models with other NLI models for factual consistency checking, we conducted the sentence ranking experiment described by Falke et al. (2019) using the test data provided by the authors. In this experiment an article sentence is paired with two claim sentences, positive and negative. The goal is to see how often a model assigns a higher probability of being correct to the positive rather than the negative claim. Results are presented in Table 5. Despite being trained in a (document, sentence) setting, our model transfers well to the (sentence-sentence setting and outperforms all other NLI models, including BERT finetuned on the MNLI dataset. We were unable to replicate the summary re-ranking experiment because the experimental test setup does not rely on data that can be reused.</p>
<p>Considering that strongly supervised data is</p>
<p>not available for factual consistency verification, proxy datasets must be used to train automatic verification models. Empirical results presented in this section suggest that when verifying less abstractive domains, it is more beneficial to train on weakly-supervised, but in-domain data, rather than to rely on the models ability to transfer knowledge from strongly supervised datasets in related domains. The experiments also highlight the necessity of extending the data generation pipeline with more abstractive, multi-sentence paraphrasing transformations as part of future work.</p>
<p>In addition to improved performance, using synthetic data allows to train models with explainable components, such as FactCCX. Examples of span selections generated by the model are show in Table 4. The test set consists of model-generated summaries that do not have annotations for quantifying the quality of spans returned by FactCCX. Instead, span quality is measured through human evaluation and discussed in Section 5.</p>
<h2>5 Analysis</h2>
<p>To further understand performance of our proposed models, we conducted human-based experiments and manually inspected model outputs.</p>
<h3>5.1 Human Studies</h3>
<p>Experiments using human annotators demonstrated that the span highlights returned by FactCCX are useful tools for researchers and crowdsource workers manually assessing the factual consistency of summaries. For each experiment, examples were annotated by 3 human judges selected from English-speaking countries. These experiments used 100 examples sampled from the manually annotated CNN/DM test set. Data points were sampled to ensure an equal split between CONSISTENT and INCONSISTENT examples.</p>
<p>To establish whether model-generated highlighted spans in the article and claim are helpful for the task of factual consistency checking, we hired human annotators to complete the mentioned task. Each of the verified document-sentence pairs was augmented with the highlighted spans output by FactCCX. Judges were asked to evaluate the correctness of the claim and instructed to use the provided segment highlights as suggestions. After the annotation task, judges were asked whether the highlighted spans were helpful for solving the
task. The helpfulness of article and claim highlights were evaluated separately. The left part of Table 6 presents the results of the survey. A combined number of $91.75 \%$ annotators found the article highlights at least somewhat helpful, $81.33 \%$ of annotators found the claim highlights at least somewhat helpful. To ensure that low-quality judges do not bias the scores, we applied different data filters to the annotations: Raw Data considered all submitted annotations, Golden Aligned only considered annotations where the annotatorassigned label aligned with the author-assigned label for the example, Majority Aligned only considered examples where the annotator-assigned aligned with the majority-vote label assigned for the example by all judges. As shown in Table 6, filtering the annotations does not yield substantial changes in the helpfulness assessment.</p>
<p>Despite instructing the annotators to consider the provided highlights only as a suggestion when solving the underlying task, the annotators perception of the task could have been biased by the model-highlighted spans. To check how well the model-generated span highlights align with an unbiased human judgement, we repeated the previous experiment with only one change: modelgenerated highlights were not displayed to the annotators. The annotators were asked to solve the underlying task and highlight the spans of the source and claim that they found adequate. Using the annotations provided by the judges, we computed the overlap between the model-generated spans and unbiased human spans. Results are shown in the right part of Table 6. The overlap between spans was evaluated using two metrics. $A c$ curacy is based on a binary score of whether the entire model-generated span was contained within the human-selected span. F1 score is computed between the tokens of the model-generated span and the tokens of the human-selected span. Results show $65.33 \%$ and $65.66 \%$ accuracy and 0.6207 and 0.6650 F1 for the article and claim highlights, respectively. We again applied different data filters to understand how the quality of annotations affects the score. We found that in this case, $a c$ curacy and F1 score were higher in the Majority Aligned than in the case of using Raw Data, and performance increases dramatically in the Majority Aligned. This suggests that when modelgenerated highlights are not provided, the task is less constrained and requires more careful preci-</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model Highlight Helpfulness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model-Annotator <br> Highlight Overlap</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Annotation subset</td>
<td style="text-align: center;">Helpful</td>
<td style="text-align: center;">Somewhat Helpful</td>
<td style="text-align: center;">Not Helpful</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">F1 score</td>
</tr>
<tr>
<td style="text-align: center;">Article Highlights</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Raw Data</td>
<td style="text-align: center;">$79.21 \%$</td>
<td style="text-align: center;">$12.54 \%$</td>
<td style="text-align: center;">$8.25 \%$</td>
<td style="text-align: center;">$65.33 \%$</td>
<td style="text-align: center;">0.6207</td>
</tr>
<tr>
<td style="text-align: center;">Golden Aligned</td>
<td style="text-align: center;">$77.73 \%$</td>
<td style="text-align: center;">$12.66 \%$</td>
<td style="text-align: center;">$9.61 \%$</td>
<td style="text-align: center;">$74.87 \%$</td>
<td style="text-align: center;">0.7161</td>
</tr>
<tr>
<td style="text-align: center;">Majority Aligned</td>
<td style="text-align: center;">$81.11 \%$</td>
<td style="text-align: center;">$11.48 \%$</td>
<td style="text-align: center;">$7.41 \%$</td>
<td style="text-align: center;">$69.88 \%$</td>
<td style="text-align: center;">0.6679</td>
</tr>
<tr>
<td style="text-align: center;">Claim Highlights</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Raw Data</td>
<td style="text-align: center;">$64.44 \%$</td>
<td style="text-align: center;">$16.89 \%$</td>
<td style="text-align: center;">$18.67 \%$</td>
<td style="text-align: center;">$65.66 \%$</td>
<td style="text-align: center;">0.6650</td>
</tr>
<tr>
<td style="text-align: center;">Golden Aligned</td>
<td style="text-align: center;">$67.28 \%$</td>
<td style="text-align: center;">$16.05 \%$</td>
<td style="text-align: center;">$16.67 \%$</td>
<td style="text-align: center;">$80.54 \%$</td>
<td style="text-align: center;">0.8190</td>
</tr>
<tr>
<td style="text-align: center;">Majority Aligned</td>
<td style="text-align: center;">$67.17 \%$</td>
<td style="text-align: center;">$16.67 \%$</td>
<td style="text-align: center;">$16.16 \%$</td>
<td style="text-align: center;">$69.48 \%$</td>
<td style="text-align: center;">0.6992</td>
</tr>
</tbody>
</table>
<p>Table 6: Quality of spans highlighted in the article and claim by the FactCCX model evaluated by human annotators. The left side shows whether the highlights were considered helpful for the task of factual consistency annotations. The right side shows the overlap between model generated and human annotated highlights. Different rows show how the scores change depending on how the collected annotations are filtered.
sion on the part of judges.
To further understand the affects of providing model-generated highlights to annotators, we ran two factual consistency annotation tasks designed to test annotation efficiency. In the first, highlights were provided to the annotators. In the second, annotators did not receive highlights. In both, we measured the average time spent by an annotator on the task and the inter-annotator agreement of annotations. Results are shown in Table 7. When completing the task with highlights, annotators were $21 \%$ faster, and the inter-annotator agreement, measured with Fleiss' $\kappa$, increased by $38 \%$.</p>
<p>Crowdsourcing experiments support the hypothesis that model-generated highlights from FactCCX can play a valuable role in supporting human-based factual consistency checking.</p>
<h3>5.2 Qualitative Study</h3>
<p>To better understand the limitations of our proposed approach, we manually inspected examples that were misclassified by our models. The majority of errors were related to commonsense mistakes made by summarization models. Humans can easily spot such errors, but they are difficult to capture with transformations necessary to generate data for weak supervision.</p>
<p>Our analysis also showed that the proposed models fail to correctly classify examples where the verified claim is highly abstractive. This is especially true when the claim finds support in multiple spans distant from each other in the source document, as mostly found in the XSum dataset. Additionally, the current set of transformations do not adequately capture temporal inconsistencies or</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Task without <br> model highlights</th>
<th style="text-align: center;">Task with <br> model highlights</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Average work <br> time (sec)</td>
<td style="text-align: center;">224.89</td>
<td style="text-align: center;">178.34</td>
</tr>
<tr>
<td style="text-align: left;">Inter-annotator <br> agreement $(\kappa)$</td>
<td style="text-align: center;">0.1571</td>
<td style="text-align: center;">0.2526</td>
</tr>
</tbody>
</table>
<p>Table 7: Annotation speed and inter-annotator agreement measured for factual consistency checking with and without assisting, model generated highlights.
incorrect coreference. Nonetheless, the current transformations yield models already useful to humans by their own judgment; this analysis points toward key areas for future work. Correct and incorrect model predictions are presented in Appendix A.</p>
<h2>6 Conclusions</h2>
<p>We introduced a novel approach for factual consistency checking of summaries generated by abstractive neural models. In our approach, models are trained to perform factual consistency checking on the document-sentence level, which allows them to handle a broader range of errors in comparison to previously proposed sentencesentence approaches. Models are trained using artificially generated, weakly-supervised data created based on insights coming from the analysis of errors made by state-of-the-art summarization models. Quantitative studies showed that on less abstractive domains, such as CNN/DailyMail news articles, our proposed approach outperforms other models trained on existing textual entailment and fact-checking data, motivating our use</p>
<p>of weak-supervision over transfer learning from related domains. Experiments with human annotators showed that our proposed approach, including an explainable factual consistency checking model, can be a valuable tool for assisting humans in factual consistency checking. Shortcomings of our approach explained in Section 5.2 can serve as guidelines for future work. We hope that this work will encourage continued research into factual consistency checking of abstractive summarization models.</p>
<h2>Acknowledgements</h2>
<p>We thank Nitish Shirish Keskar, Dragomir Radev, Ben Krause, and Wenpeng Yin for reviewing this manuscript and providing valuable feedback, and Shashi Narayan for help with experiments on the XSum dataset.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.</p>
<p>Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. In Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 4784-4791.</p>
<p>Yen-Chun Chen and Mohit Bansal. 2018. Fast abstractive summarization with reinforce-selected sentence rewriting. In ACL (1), pages 675-686. Association for Computational Linguistics.</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 17241734 .</p>
<p>Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli</p>
<p>Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 16, 2018, Volume 2 (Short Papers).</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805.</p>
<p>Yue Dong, Yikang Shen, Eric Crawford, Herke van Hoof, and Jackie Chi Kit Cheung. 2018. Banditsum: Extractive summarization as a contextual bandit. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018.</p>
<p>Bonnie Dorr, David Zajic, and Richard Schwartz. 2003. Hedge trimmer: A parse-and-trim approach to headline generation. In HLT-NAACL.</p>
<p>Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding back-translation at scale. CoRR, abs/1808.09381.</p>
<p>Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 2214-2220.</p>
<p>Sebastian Gehrmann, Yuntian Deng, and Alexander M. Rush. 2018. Bottom-up abstractive summarization. In EMNLP, pages 4098-4109. Association for Computational Linguistics.</p>
<p>Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. 2019. Assessing the factual accuracy of generated text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019., pages 166-175.</p>
<p>Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers).</p>
<p>Han Guo, Ramakanth Pasunuru, and Mohit Bansal. 2018. Soft layer-specific multi-task summarization with entailment and question generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers.</p>
<p>Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. http://spacy.io.</p>
<p>Wan Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui Min, Jing Tang, and Min Sun. 2018. A unified model for extractive and abstractive summarization using inconsistency loss. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers.</p>
<p>Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adversarial example generation with syntactically controlled paraphrase networks. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 1875-1885.</p>
<p>Yichen Jiang and Mohit Bansal. 2018. Closed-book training to improve summarization encoder memory. In EMNLP, pages 4067-4077. Association for Computational Linguistics.</p>
<p>Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural text summarization: A critical evaluation. CoRR, abs/1908.08960.</p>
<p>Wojciech Kryściński, Romain Paulus, Caiming Xiong, and Richard Socher. 2018. Improving abstraction in text summarization. In EMNLP, pages 1808-1817. Association for Computational Linguistics.</p>
<p>Wei Li, Xinyan Xiao, Yajuan Lyu, and Yuanzhuo Wang. 2018. Improving neural abstractive document summarization with structural regularization. In EMNLP, pages 4078-4087. Association for Computational Linguistics.</p>
<p>Yang Liu. 2019. Fine-tune BERT for extractive summarization. CoRR, abs/1903.10318.</p>
<p>Yang Liu and Mirella Lapata. 2019. Text summarization with pretrained encoders. CoRR, abs/1908.08345.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan T. McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 1906-1919. Association for Computational Linguistics.</p>
<p>Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In NIPS, pages 62976308 .</p>
<p>Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han. 2018. Weakly-supervised neural text classification. CoRR, abs/1809.01478.</p>
<p>Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. In $A A A I$.</p>
<p>Ramesh Nallapati, Bowen Zhou, Çağlar Gülçehre, Bing Xiang, et al. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. Proceedings of SIGNLL Conference on Computational Natural Language Learning.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium.</p>
<p>Ramakanth Pasunuru and Mohit Bansal. 2018. Multireward reinforced summarization with saliency and entailment. CoRR, abs/1804.06451.</p>
<p>Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop.</p>
<p>Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive summarization. In $I C L R$.</p>
<p>Alexander M Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. Proceedings of EMNLP.</p>
<p>Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In $A C L$.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Improving neural machine translation models with monolingual data. CoRR, abs/1511.06709.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and verification. CoRR, abs/1803.05355.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 6000-6010.</p>
<p>Jason W. Wei and Kai Zou. 2019. EDA: easy data augmentation techniques for boosting performance on text classification tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 6381-6387. Association for Computational Linguistics.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational Linguistics.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Transformers: State-of-the-art natural language processing.</p>
<p>Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, and Songlin Hu. 2019. Conditional BERT contextual augmentation. In Computational Science ICCS 2019 - 19th International Conference, Faro, Portugal, June 12-14, 2019, Proceedings, Part IV, pages $84-95$.</p>
<p>Yuxiang Wu and Baotian Hu. 2018. Learning to extract coherent summary via deep reinforcement learning. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018.</p>
<p>Fangfang Zhang, Jin-ge Yao, and Rui Yan. 2018. On the abstractiveness of neural document summarization. In EMNLP, pages 785-790. Association for Computational Linguistics.</p>
<p>Yuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 1298-1308.</p>
<h1>A Model predictions</h1>
<h2>Example predictions made by the FactCC model.</h2>
<h2>Example 1</h2>
<p>Label: CONSISTENT
Prediction: INCONSISTENT</p>
<h2>Article (CNN/DM)</h2>
<p>(cnn) james best, best known for his portrayal of bumbling sheriff rosco p. coltrane on tv's "the dukes of hazzard," died monday after a brief illness. he was 88 . best died in hospice in hickory, north carolina, of complications from pneumonia, said steve latshaw, a longtime friend and hollywood colleague. although he'd been a busy actor for decades in theater and in hollywood, best didn't become famous until 1979, when "the dukes of hazzard's" cornpone charms began beaming into millions of american homes almost every friday night. for seven seasons, best's rosco p. coltrane chased the moonshine-running duke boys back and forth across the back roads of fictitious hazzard county, georgia, although his "hot pursuit" usually ended with him crashing his patrol car. although rosco was slow-witted and corrupt, best gave him a childlike enthusiasm that got laughs and made him endearing. his character became known for his distinctive "kew-kew-kew" chuckle and for goofy catchphrases such as "cuff 'em and stuff 'em!" upon making an arrest. among the most popular shows on tv in the early ' 90 s , "the dukes of hazzard" ran until 1985 and spawned tv movies, an animated series and video games. several of best's "hazzard" co-stars paid tribute to the late actor on social media. (...)</p>
<h2>Claim</h2>
<p>"hazzard" ran from 1979 to 1985 and was among the most popular shows on tv.</p>
<h2>Example 2</h2>
<p>Label: CONSISTENT
Prediction: INCONSISTENT</p>
<h2>Article (CNN/DM)</h2>
<p>(cnn) the attorney for a suburban new york cardiologist charged in what authorities say was a failed scheme to have another physician hurt or killed is calling the allegations against his client "completely unsubstantiated." appearing Saturday morning on cnn's "new day," randy zelin defended his client, dr. anthony moschetto, who faces criminal solicitation, conspiracy, burglary, arson, criminal prescription sale and weapons charges in connection to what prosecutors called a plot to take out a rival doctor on long island. "none of anything in this case has any evidentiary value," zelin told cnn's christi paul. "it doesn't matter what anyone says, he is presumed to be innocent." moschetto, 54, pleaded not guilty to all charges wednesday. he was released after posting $\$ 2$ million bond and surrendering his passport. zelin said that his next move is to get dr. moshetto back to work. "he's got patients to see. This man, while he was in a detention cell, the only thing that he cared about were his patients. and amazingly, his patients were flooding the office with calls, making sure that he was ok," zelin said. (...)</p>
<h2>Claim</h2>
<p>a lawyer for dr. anthony moschetto says the charges against him are baseless.</p>
<h2>Example 3</h2>
<p>Label: INCONSISTENT
Prediction: CONSISTENT</p>
<h2>Article (CNN/DM)</h2>
<p>(cnn) north korea accused mexico of illegally holding one of its cargo ships wednesday and demanded the release of the vessel and crew. the ship, the mu du bong, was detained after it ran aground off the coast of mexico in july. mexico defended the move wednesday, saying it followed proper protocol because the company that owns the ship, north korea's ocean maritime management company, has skirted united nations sanctions. "because the company has avoided the sanctions imposed by the u.n. security council, the mexican government is acting on the basis of its international obligations as a responsible u.n. member state," the permanent mission of mexico to the united nations said. the security council blacklisted ocean maritime management in july, saying it "played a key role in arranging the shipment of concealed arms and related materiel" on another ship, the chong chon gang, which was detained by panama in 2013. but an myong hun, north korea's deputy ambassador to the united nations, said there was no reason to hold the mu du bong and accused mexico of violating the crew members' human rights by keeping them from their families.</p>
<h2>Claim</h2>
<p>north korea accused mexico of using one of its cargo ships.</p>
<h1>Example 4</h1>
<p>Label: CONSISTENT
Prediction: CONSISTENT</p>
<h2>Article (CNN/DM)</h2>
<p>(cnn) spoiler alert! it's not just women getting cloned. that was the big twist at the end of "orphan black's" second season. the kickoff to the new season leads the list of six things to watch in the week ahead. 1. "orphan black," 9 p.m. et, saturday, april 18, bbc america. the cloning cult sci-fi series remains one of the most critically acclaimed shows on tv, thanks in large part to the performance of tatiana maslany, who has taken on at least six roles on the show so far, including a newly introduced transgender clone. maslany told reporters this week that we can expect even more impressive scenes with multiple clones. "we like to push the boundaries of what we're able to do and the limits of those clone scenes," she said. (...)</p>
<h2>Claim</h2>
<p>critically acclaimed series "orphan black" returns.</p>
<h2>Example 5</h2>
<p>Label: INCONSISTENT
Prediction: INCONSISTENT</p>
<h2>Article (CNN/DM)</h2>
<p>boston (cnn) dzhokhar tsarnaev's bombs tore through their bodies: singeing flesh, shattering bones, shredding muscles and severing limbs. but on tuesday, jurors also began to hear about the holes his bombs left in the hearts of the survivors and the families of the dead. now that he has been found guilty on every count, the jury must decide whether boston marathon bomber tsarnaev, 21, should live or die for what he has done. this is the victim impact part of the case, and the testimony was heartbreaking. four young people are gone, and grief fills the spaces they once occupied. a father with a shock of white hair cried for the daughter he called "princess." "krystle was the light of my life. she was extremely smart, hardworking, beautiful, every father's dream. i miss her a lot," said william a. campbell sc., dabbing at his eyes as he described his daughter, a 29-yearold restaurant manager who was killed in the first blast at the 2013 boston marathon. she was the one who could round up the family and put on big celebrations, he said. (...)</p>
<h2>Claim</h2>
<p>dzhokhar tsarnaev, 21, was killed in the first blast at the 2013 boston marathon.</p>
<h2>Example 6</h2>
<p>Label: INCONSISTENT
Prediction: INCONSISTENT</p>
<h2>Article (CNN/DM)</h2>
<p>(the hollywood reporter) the author of a 2006 novel has accused the "avengers" director and "cabin" director drew goddard of stealing his idea. with just weeks until his box-office victory lap for "avengers: age of ultron," joss whedon is now facing a lawsuit accusing him of stealing the idea for the 2012 meta-horror movie the cabin in the woods. whedon produced and co-wrote the script for cabin with director drew goddard, a writer on whedon's "buffy the vampire slayer" and a fanboy favorite in his own right, with credits that include netflix's "daredevil" (and reportedly may soon include sony's upcoming spider-man projects). whedon and goddard are named as defendants, along with lionsgate and whedon's mutant enemy production company, in the complaint filed monday in california federal court. joss whedon slams 'jurassic world 'clip as "'70s - era sexist" in the complaint, peter gallagher (no, not that peter gallagher) claims whedon and goddard took the idea for "the cabin in the woods' from his 2006 novel "the little white trip: a night in the pines." he's suing for copyright infringement and wants $\$ 10$ million in damages. gallagher is basing his claim on the works' similar premises: both feature a group of young people terrorized by monsters while staying at a cabin in what is revealed to be (spoiler alert) a horror-film scenario designed by mysterious operators. (...)</p>
<h2>Claim</h2>
<p>joss whedon claims whedon and goddard took the idea for "the cabin in the woods".</p>
<h1>Example 7</h1>
<p>Label: INCONSISTENT
Prediction: INCONSISTENT</p>
<h2>Article (XSUM)</h2>
<p>More than 5,300 bottles of alcohol were seized by the investigators in the southern city of Liuzhou. They also found packets of a white powder called Sildenafil, better known as the anti-impotence drug Viagra. Police in the Guangxi region are now investigating the two distillers. The Liuzhou Food and Drug Administration said (in Chinese) that the powder was added to three different types of 'baijiu' - a strong, clear spirit that is the most popular drink in China. They said the haul was worth up to 700,000 yuan. Doctors recommend that adults requiring prescription should take only one dose of Viagra a day, with a lower dose for those over the age of 65 . China continues to face widespread food safety problems. In June, police in cities across China seized more than 100,000 tonnes of smuggled meat, some of which was more than 40 years old. The 2008 tainted milk scandal outraged the nation. Some 300,000 people were affected and at least six babies died after consuming milk adulterated with melamine.</p>
<h2>Claim</h2>
<p>police in southern china have seized more than 1,000 alcohol bottles and seized more than 1,200 bottles of contaminated milk, local media report.</p>
<h2>Example 8</h2>
<p>Label: CONSISTENT
Prediction: CONSISTENT</p>
<h2>Article (XSUM)</h2>
<p>The victim was queuing for food at the branch in St George's Street, Canterbury at about 02:15 GMT on Friday when the assault occurred. Investigating officers said three men entered the restaurant and began being noisy and bumping into people. It is believed one of the group then set light to the woman's hair. Officers have released CCTV images of three men they are keen to speak to regarding the attack. Det Sgt Barry Carr said: "Fortunately the fire was put out quickly and the victim was not seriously hurt, but things could clearly have turned out much worse. This was a nasty and extremely dangerous thing to do, and I urge anyone who recognises the men in the CCTV images to contact me as soon as possible."</p>
<h2>Claim</h2>
<p>a woman was assaulted and assaulted in a mcdonald's restaurant in kent, police have said.</p>
<h2>Example 9</h2>
<p>Label: INCONSISTENT
Prediction: INCONSISTENT</p>
<h2>Article (XSUM)</h2>
<p>Jung won aboard Sam, who was a late replacement when Fischertakinou contracted an infection in July. France's Astier Nicolas took silver and American Phillip Dutton won bronze as GB's William Fox-Pitt finished 12th. Fox-Pitt, 47, was competing just 10 months after being placed in an induced coma following a fall. The three-time Olympic medallist, aboard Chilli Morning, produced a faultless performance in Tuesday's final show-jumping phase. But the former world number one's medal bid had already been ruined by a disappointing performance in the cross-country phase on Monday. He led after the dressage phase, but dropped to 21 st after incurring several time penalties in the cross country. Ireland's Jonty Evans finished ninth on Cooley Rorkes Drift. Why not come along, meet and ride Henry the mechanical horse at some of the Official Team GB fan parks during the Rio Olympics? Find out how to get into equestrian with our special guide. Subscribe to the BBC Sport newsletter to get our pick of news, features and video sent to your inbox.</p>
<h2>Claim</h2>
<p>great britain's eventers missed out on a bronze medal at the rio olympics after losing in the dressage.</p>
<h1>Example 10</h1>
<p>Label: INCONSISTENT
Prediction: CONSISTENT</p>
<h2>Article (XSUM)</h2>
<p>A review for the Commission on Local Tax Reform said there was no "magic bullet" to cure defects in the system. It said the council tax had built-in problems "from day one" but a failure to modify it had stored up more difficulties for policy makers. The commission, set up by the Scottish government and council body Cosla, will report back later this year. Prof Kenneth Gibb, from the University of Glasgow, was asked to review different systems of local taxation across the world. He found that a tax on property was used by almost all OECD countries and was seen by academics as a "good tax" because it was stable, difficult to avoid and could have a desirable impact on housing markets. But it also generated confusion with taxpayers unclear whether it was a tax on wealth or a charge for services such as refuse collection. Some felt it was unfair because it was not linked to current income. Prof Gibb noted that a local income tax, used by many countries, was generally perceived as fairer. But he found such a system created difficulties for local authorities because it meant their income fluctuated. There was also little opportunity to vary tax rates to reflect local priorities. He said: "It is clear there is no magic bullet. "Past experience from the UK and across the world shows that reform is always going to be difficult and will inevitably be bound up with the previous experiences and traumas of past reform. "So whilst the current council tax has many deficiencies, change and reform is a major undertaking." The commission now intends to hold a public consultation across Scotland before publishing its report in the autumn. A Scottish government spokesman said ministers consider the current council tax system "as a whole to be unfair". He added: "That is why, along with our local government partners, we have established the cross-party Commission on Local Tax Reform to examine fairer alternatives. "The Scottish government awaits the commission's report, which is due in the Autumn.</p>
<h2>Claim</h2>
<p>the scottish government has been accused of "unfairly unfair" by a watchdog after a report found that a council tax system was not stored</p>
<h2>Example 11</h2>
<p>Label: INCONSISTENT
Prediction: CONSISTENT</p>
<h2>Article (XSUM)</h2>
<p>The crash happened at about 14:15 BST on the B1191 at Thornton, near Woodhall Spa. Lincolnshire Police said the motorcyclist killed in the collision lived locally, but has not released any further details. The tractor driver was not injured. The force has appealed for witnesses to the collision to come forward. The B1191 was closed in both directions between the B1192 Tattershall Road junction in Woodhall Spa and the A158 Jubilee Way junction in Horncastle</p>
<h2>Claim</h2>
<p>a motorcyclist killed in a crash with a tractor and a tractor in lincolnshire has been named.</p>
<h2>Example 12</h2>
<p>Label: INCONSISTENT
Prediction: CONSISTENT</p>
<h2>Article (XSUM)</h2>
<p>The Australian, 21, beat world number 29 Querrey 6-4 6-4 in 53 minutes to progress to the second round. Kyrgios, ranked a career-high 12th in the world, won the Japan Open on Sunday and is closing in on the top 10. "I was just a bit bored at times," said Kyrgios, when asked why he was not his usual vocal self against Querrey. "I was feeling very tired. It was just tough. I'm just tired so maybe I just wanted to get the job done." Kyrgios said his success in Japan, and the travelling involved in playing at the Qi Zhong Stadium, an hour from Shanghai city centre, had taken its toll. "I didn't have the greatest sleep last night and obviously got in late the day before," he said. "The ride to the courts isn't great either." It was at the Shanghai Masters last year that Kyrgios was fined $\$ 1,500$ for a foul-mouthed outburst, describing the tournament a "circus".</p>
<h2>Claim</h2>
<p>australia's nick kyrgios said he was "not afraid to sleep" after reaching the second round of the shanghai masters.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ http://gluebenchmark.com/leaderboard&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
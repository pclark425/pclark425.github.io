<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8865 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8865</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8865</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-a113053b624b599b204fbd6599284b726c17f916</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a113053b624b599b204fbd6599284b726c17f916" target="_blank">ConjNLI: Natural Language Inference over Conjunctive Sentences</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Con ConjNLI, a challenge stress-test for natural language inference over conjunctive sentences, where the premise differs from the hypothesis by conjuncts removed, added, or replaced, is introduced.</p>
                <p><strong>Paper Abstract:</strong> Reasoning about conjuncts in conjunctive sentences is important for a deeper understanding of conjunctions in English and also how their usages and semantics differ from conjunctive and disjunctive boolean logic. Existing NLI stress tests do not consider non-boolean usages of conjunctions and use templates for testing such model knowledge. Hence, we introduce ConjNLI, a challenge stress-test for natural language inference over conjunctive sentences, where the premise differs from the hypothesis by conjuncts removed, added, or replaced. These sentences contain single and multiple instances of coordinating conjunctions ("and", "or", "but", "nor") with quantifiers, negations, and requiring diverse boolean and non-boolean inferences over conjuncts. We find that large-scale pre-trained language models like RoBERTa do not understand conjunctive semantics well and resort to shallow heuristics to make inferences over such sentences. As some initial solutions, we first present an iterative adversarial fine-tuning method that uses synthetically created training data based on boolean and non-boolean heuristics. We also propose a direct model advancement by making RoBERTa aware of predicate semantic roles. While we observe some performance gains, ConjNLI is still challenging for current methods, thus encouraging interesting future work for better understanding of conjunctions. Our data and code are publicly available at: this https URL</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8865.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8865.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa (A Robustly Optimized BERT Pretraining Approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-style large pre-trained transformer (used in this paper in its base variant) fine-tuned for NLI tasks (MNLI/SNLI) and evaluated on the CONJNLI conjunctive-sentence NLI stress-test. The paper uses RoBERTa as the primary baseline and the backbone for proposed methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoBERTa: A robustly optimized BERT pretraining approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT-style transformer pre-trained at scale; in this work RoBERTa-base is fine-tuned on MNLI/SNLI and then evaluated on CONJNLI and used as backbone for further methods (IAFT, predicate-aware augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>base (RoBERTa-base; exact parameter count not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CONJNLI (Conjunctive NLI stress-test)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A challenge NLI dataset of 1,623 expert-annotated (premise,hypothesis,label) examples where premise and hypothesis differ by conjuncts removed/added/replaced, covering coordinating conjunctions (and/or/but/nor), quantifiers, negations, and requiring boolean and non-boolean inferences over conjuncts.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning on MNLI (baseline); evaluated as-is and with additional methods: simple adversarial fine-tuning (AFT) with heuristically-created adversarial training data, Iterative Adversarial Fine-Tuning (IAFT) mixing adversarial data with MNLI each epoch, and predicate-aware augmentation (late fusion of SRL-derived embeddings from a BERT-SRL model).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>RoBERTa fine-tuned on MNLI (baseline) achieved MNLI Dev: 87.56/87.51 (matched/mismatched); CONJNLI Dev: 64.68% accuracy; CONJNLI Test: 65.50% accuracy. With IAFT (iterative adversarial fine-tuning) the model attained CONJ Dev: 69.18% and CONJ Test: 67.90% while retaining MNLI Dev ≈ 86.93/86.81.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to BERT baselines and older models (ESIM), RoBERTa outperforms them on MNLI and on CONJNLI. IAFT improves over plain RoBERTa by ~2.5-4 points on CONJNLI dev/test while maintaining near-original MNLI performance; simple AFT (two-step fine-tuning) improved CONJNLI less and caused a large drop in MNLI performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>RoBERTa relies on shallow heuristics like lexical overlap (predicting entailment when hypothesis is subset of premise) and struggles with non-boolean conjunction usages (e.g., collective entities, named entities, exclusive 'or', equivalence 'or'), with 'or' being especially challenging; still fails many non-boolean examples even after IAFT and predicate-aware augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>RoBERTa learns many boolean conjunctive heuristics with modest amounts of adversarial data (saturation around ~12k synthetic examples) but remains limited on non-boolean real-world conjunctive phenomena; combining semantic SRL signals gives marginal gains, suggesting need for richer semantics or better adversarial data generation to capture non-boolean cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConjNLI: Natural Language Inference over Conjunctive Sentences', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8865.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8865.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (Bidirectional Encoder Representations from Transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely-used pre-trained bidirectional transformer model; used here in base variants as a baseline for NLI and as the SRL model (BERT-SRL) to produce predicate-aware embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (base, uncased) / BERT-SRL (SRL fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT-base uncased is fine-tuned on SNLI/MNLI as an NLI baseline; a BERT model fine-tuned for Semantic Role Labeling (BERT-SRL) is trained on CoNLL-2005 and its [CLS] embeddings are used (frozen) to provide predicate-aware features to RoBERTa.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>base (BERT-base; exact parameter count not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CONJNLI (Conjunctive NLI stress-test)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as above: an expert-annotated NLI benchmark focusing on boolean and non-boolean inferences over conjunctive sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>BERT is used as a baseline NLI model (fine-tuned on MNLI/SNLI) and as the SRL feature extractor (BERT-SRL) to augment RoBERTa with predicate-aware representations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>BERT (MNLI fine-tuned) achieved MNLI Dev: 84.10/83.90 (matched/mismatched) and CONJNLI Dev: ≈58.10% accuracy; CONJNLI Test: ≈61.40% (as reported in paper tables). The BERT-SRL model achieved a near state-of-the-art SRL F1 of 86.23% on CoNLL-2005 dev (used as frozen feature provider).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>BERT is outperformed by RoBERTa on both MNLI and CONJNLI in this study; using BERT as SRL feature provider helps RoBERTa slightly when predicate-aware embeddings are fused.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>BERT also exhibits lexical overlap heuristic failures on conjunctive inference and performs substantially worse than RoBERTa on CONJNLI; SRL-derived signals provide limited improvements when frozen and fused.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Standard pre-trained models like BERT capture some conjunctive inference patterns but lack robust handling of non-boolean conjunction semantics; explicit semantic features (SRL) can marginally help but are insufficient alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConjNLI: Natural Language Inference over Conjunctive Sentences', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8865.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8865.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ESIM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ESIM (Enhanced Sequential Inference Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior neural NLI architecture (BiLSTM-based) used here as an older baseline showing substantially lower performance on CONJNLI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enhanced LSTM for natural language inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ESIM (LSTM-based NLI model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LSTM-based NLI model (attention and inference composition over sequences) serving as a non-transformer baseline for CONJNLI evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CONJNLI (Conjunctive NLI stress-test)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above: NLI pairs focused on conjunctive sentence phenomena requiring boolean and non-boolean reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Trained on SNLI/MNLI (as a baseline) and evaluated on CONJNLI to contrast older architectures with modern pre-trained transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ESIM achieved ≈53.10% accuracy on CONJNLI (much lower than transformer baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperforms compared to BERT and RoBERTa baselines by a wide margin on CONJNLI, highlighting the difficulty of conjunctive non-boolean inferences for older LSTM-based models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performs poorly on conjunctive NLI; lacks pretraining and the representational power of transformer-based models, therefore falls behind on both boolean and non-boolean inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Older sequence models like ESIM are insufficient for the nuanced conjunctive reasoning captured by CONJNLI; modern pre-trained transformers perform better but still have notable limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConjNLI: Natural Language Inference over Conjunctive Sentences', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8865.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8865.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IAFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Adversarial Fine-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training procedure introduced in the paper that iteratively fine-tunes a pre-trained RoBERTa model by mixing a fixed heuristically-created adversarial conjunctive training set with random sampled MNLI examples every epoch to improve robustness to conjunctive phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-base with Iterative Adversarial Fine-Tuning (IAFT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>IAFT begins with RoBERTa fine-tuned on MNLI; at each epoch it samples an MNLI subset equal in size to the adversarial set and trains on the mixture of MNLI subset + constant adversarial set (15k synthetic conjunctive examples created with boolean/non-boolean heuristics).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>base (RoBERTa-base; exact parameter count not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CONJNLI (Conjunctive NLI stress-test)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Expert-annotated dataset testing boolean and non-boolean conjunctive semantics in NLI (premise/hypothesis differ by conjuncts added/removed/replaced).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Adversarial data creation (15k synthetic examples using boolean heuristics and limited non-boolean heuristics) + iterative fine-tuning mixing MNLI examples each epoch (Algorithm 1 in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>IAFT achieved CONJNLI Dev: 69.18% and CONJNLI Test: 67.90%, improving over RoBERTa baseline (CONJ Dev 64.68%, Test 65.50%) while retaining MNLI Dev performance (~86.93/86.81).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>IAFT outperforms simple Adversarial Fine-Tuning (AFT): AFT had CONJ Dev 67.57% but caused MNLI Dev to drop to ~76.6; IAFT retains MNLI performance while improving CONJNLI. IAFT corrects mostly boolean examples (≈65%) more than non-boolean (≈35%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Adversarial examples were created with shallow heuristics and contain errors (~70% accuracy on a manual 100-example check); IAFT mainly fixes boolean cases and remains weak on non-boolean conjunctions (named entities, collective readings, exclusive/equivalence 'or'); gains saturate after ~12k synthetic examples.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Mixing adversarial synthetic conjunctive examples with original MNLI data each epoch helps teach RoBERTa boolean conjunctive heuristics without catastrophic forgetting, but rule-based adversarial data is insufficient to cover diverse non-boolean real-world cases, indicating need for richer adversarial generation or deeper semantic models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConjNLI: Natural Language Inference over Conjunctive Sentences', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8865.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8865.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PA-RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predicate-Aware RoBERTa (RoBERTa + SRL embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model augmentation that fuses predicate-aware semantic role embeddings (obtained from a BERT-SRL model) with RoBERTa's [CLS] representations to help distinguish cases where predicate roles determine the inference label.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-base + predicate-aware SRL features</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RoBERTa's [CLS] embeddings for premise and hypothesis are concatenated/combined with transformed [CLS] embeddings from a BERT model fine-tuned for SRL (BERT-SRL weights frozen); fused representation is used for NLI classification (predicate-aware late fusion).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>base (RoBERTa-base + BERT-SRL; exact parameter counts not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CONJNLI (Conjunctive NLI stress-test)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above: expert-annotated NLI pairs focusing on conjunction boolean/non-boolean semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Late fusion of SRL-derived predicate-aware embeddings (BERT-SRL trained on CoNLL-2005) into the RoBERTa NLI classifier to make model aware of predicate arguments/roles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Predicate-Aware (PA) RoBERTa showed marginal gains over RoBERTa: overall CONJNLI accuracy reported as ≈66.30% (breakdown by conjunction type: And 66.29%, Or 60.93%, But 81.48%, Multiple 66.81%). This is a modest improvement over plain RoBERTa (All ≈65.60% in some tables / baseline ~64.68 in others).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>PA yields small gains over plain RoBERTa, especially on some categories (multiple conjunctions), but combined PA+IAFT did not lead to further improvement beyond IAFT alone in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Gains are marginal; PA does not fully address non-boolean conjunction issues. Using frozen SRL features may limit benefit; more integrated or more accurate semantic parsing may be needed.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Adding explicit predicate-role information helps some conjunctive inferences where predicate arguments determine labels, but alone is insufficient to solve hard non-boolean cases—encourages future work on richer semantic representations or joint SRL-NLI modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConjNLI: Natural Language Inference over Conjunctive Sentences', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8865.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8865.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial Fine-Tuning (simple two-step)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simpler adversarial training baseline where a model fine-tuned on MNLI is further fine-tuned (two-step) on the adversarial synthetic conjunctive dataset (without iterative mixing), used as a comparison to IAFT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-base with two-step Adversarial Fine-Tuning (AFT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Start with RoBERTa fine-tuned on MNLI, then further fine-tune it solely on the heuristically-created adversarial conjunctive dataset (15k examples) in a two-step process.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>base (RoBERTa-base; exact parameter count not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CONJNLI (Conjunctive NLI stress-test)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above: conjunctive-focused NLI pairs requiring boolean and non-boolean reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Two-step fine-tuning on MNLI then adversarial synthetic conjunctive examples; no per-epoch mixing with MNLI.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>AFT achieved CONJNLI Dev ≈67.57% but caused MNLI Dev scores to drop substantially (to ≈76.61/76.68 matched/mismatched), indicating catastrophic shift toward adversarial heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>AFT improves over the MNLI-only baseline on CONJNLI somewhat, but performs worse than IAFT and harms original MNLI performance significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Biases the model toward the (noisy) adversarial set, causing degradation on original MNLI; adversarial data's heuristic labels are imperfect (~70% accurate on small manual sample) and miss many non-boolean cases.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Direct fine-tuning on synthetically-created adversarial data can help but risks overfitting to heuristic artifacts; iterative mixing with original data (IAFT) mitigates forgetting and yields a better trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ConjNLI: Natural Language Inference over Conjunctive Sentences', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Probing natural language inference models through semantic fragments <em>(Rating: 2)</em></li>
                <li>Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference <em>(Rating: 2)</em></li>
                <li>Adversarial NLI: A new benchmark for natural language understanding <em>(Rating: 2)</em></li>
                <li>Breaking NLI systems with sentences that require simple lexical inferences <em>(Rating: 2)</em></li>
                <li>Do neural models learn systematicity of monotonicity inference in natural language? <em>(Rating: 1)</em></li>
                <li>Inoculation by fine-tuning: A method for analyzing challenge datasets <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8865",
    "paper_id": "paper-a113053b624b599b204fbd6599284b726c17f916",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "RoBERTa",
            "name_full": "RoBERTa (A Robustly Optimized BERT Pretraining Approach)",
            "brief_description": "A BERT-style large pre-trained transformer (used in this paper in its base variant) fine-tuned for NLI tasks (MNLI/SNLI) and evaluated on the CONJNLI conjunctive-sentence NLI stress-test. The paper uses RoBERTa as the primary baseline and the backbone for proposed methods.",
            "citation_title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "mention_or_use": "use",
            "model_name": "RoBERTa (base)",
            "model_description": "BERT-style transformer pre-trained at scale; in this work RoBERTa-base is fine-tuned on MNLI/SNLI and then evaluated on CONJNLI and used as backbone for further methods (IAFT, predicate-aware augmentation).",
            "model_size": "base (RoBERTa-base; exact parameter count not specified in paper)",
            "reasoning_task_name": "CONJNLI (Conjunctive NLI stress-test)",
            "reasoning_task_description": "A challenge NLI dataset of 1,623 expert-annotated (premise,hypothesis,label) examples where premise and hypothesis differ by conjuncts removed/added/replaced, covering coordinating conjunctions (and/or/but/nor), quantifiers, negations, and requiring boolean and non-boolean inferences over conjuncts.",
            "method_or_approach": "Fine-tuning on MNLI (baseline); evaluated as-is and with additional methods: simple adversarial fine-tuning (AFT) with heuristically-created adversarial training data, Iterative Adversarial Fine-Tuning (IAFT) mixing adversarial data with MNLI each epoch, and predicate-aware augmentation (late fusion of SRL-derived embeddings from a BERT-SRL model).",
            "performance": "RoBERTa fine-tuned on MNLI (baseline) achieved MNLI Dev: 87.56/87.51 (matched/mismatched); CONJNLI Dev: 64.68% accuracy; CONJNLI Test: 65.50% accuracy. With IAFT (iterative adversarial fine-tuning) the model attained CONJ Dev: 69.18% and CONJ Test: 67.90% while retaining MNLI Dev ≈ 86.93/86.81.",
            "baseline_comparison": "Compared to BERT baselines and older models (ESIM), RoBERTa outperforms them on MNLI and on CONJNLI. IAFT improves over plain RoBERTa by ~2.5-4 points on CONJNLI dev/test while maintaining near-original MNLI performance; simple AFT (two-step fine-tuning) improved CONJNLI less and caused a large drop in MNLI performance.",
            "limitations_or_failures": "RoBERTa relies on shallow heuristics like lexical overlap (predicting entailment when hypothesis is subset of premise) and struggles with non-boolean conjunction usages (e.g., collective entities, named entities, exclusive 'or', equivalence 'or'), with 'or' being especially challenging; still fails many non-boolean examples even after IAFT and predicate-aware augmentation.",
            "insights_or_conclusions": "RoBERTa learns many boolean conjunctive heuristics with modest amounts of adversarial data (saturation around ~12k synthetic examples) but remains limited on non-boolean real-world conjunctive phenomena; combining semantic SRL signals gives marginal gains, suggesting need for richer semantics or better adversarial data generation to capture non-boolean cases.",
            "uuid": "e8865.0",
            "source_info": {
                "paper_title": "ConjNLI: Natural Language Inference over Conjunctive Sentences",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "BERT",
            "name_full": "BERT (Bidirectional Encoder Representations from Transformers)",
            "brief_description": "A widely-used pre-trained bidirectional transformer model; used here in base variants as a baseline for NLI and as the SRL model (BERT-SRL) to produce predicate-aware embeddings.",
            "citation_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "use",
            "model_name": "BERT (base, uncased) / BERT-SRL (SRL fine-tuned)",
            "model_description": "BERT-base uncased is fine-tuned on SNLI/MNLI as an NLI baseline; a BERT model fine-tuned for Semantic Role Labeling (BERT-SRL) is trained on CoNLL-2005 and its [CLS] embeddings are used (frozen) to provide predicate-aware features to RoBERTa.",
            "model_size": "base (BERT-base; exact parameter count not specified in paper)",
            "reasoning_task_name": "CONJNLI (Conjunctive NLI stress-test)",
            "reasoning_task_description": "Same as above: an expert-annotated NLI benchmark focusing on boolean and non-boolean inferences over conjunctive sentences.",
            "method_or_approach": "BERT is used as a baseline NLI model (fine-tuned on MNLI/SNLI) and as the SRL feature extractor (BERT-SRL) to augment RoBERTa with predicate-aware representations.",
            "performance": "BERT (MNLI fine-tuned) achieved MNLI Dev: 84.10/83.90 (matched/mismatched) and CONJNLI Dev: ≈58.10% accuracy; CONJNLI Test: ≈61.40% (as reported in paper tables). The BERT-SRL model achieved a near state-of-the-art SRL F1 of 86.23% on CoNLL-2005 dev (used as frozen feature provider).",
            "baseline_comparison": "BERT is outperformed by RoBERTa on both MNLI and CONJNLI in this study; using BERT as SRL feature provider helps RoBERTa slightly when predicate-aware embeddings are fused.",
            "limitations_or_failures": "BERT also exhibits lexical overlap heuristic failures on conjunctive inference and performs substantially worse than RoBERTa on CONJNLI; SRL-derived signals provide limited improvements when frozen and fused.",
            "insights_or_conclusions": "Standard pre-trained models like BERT capture some conjunctive inference patterns but lack robust handling of non-boolean conjunction semantics; explicit semantic features (SRL) can marginally help but are insufficient alone.",
            "uuid": "e8865.1",
            "source_info": {
                "paper_title": "ConjNLI: Natural Language Inference over Conjunctive Sentences",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "ESIM",
            "name_full": "ESIM (Enhanced Sequential Inference Model)",
            "brief_description": "A prior neural NLI architecture (BiLSTM-based) used here as an older baseline showing substantially lower performance on CONJNLI.",
            "citation_title": "Enhanced LSTM for natural language inference",
            "mention_or_use": "use",
            "model_name": "ESIM (LSTM-based NLI model)",
            "model_description": "An LSTM-based NLI model (attention and inference composition over sequences) serving as a non-transformer baseline for CONJNLI evaluation.",
            "model_size": "not specified in paper",
            "reasoning_task_name": "CONJNLI (Conjunctive NLI stress-test)",
            "reasoning_task_description": "As above: NLI pairs focused on conjunctive sentence phenomena requiring boolean and non-boolean reasoning.",
            "method_or_approach": "Trained on SNLI/MNLI (as a baseline) and evaluated on CONJNLI to contrast older architectures with modern pre-trained transformers.",
            "performance": "ESIM achieved ≈53.10% accuracy on CONJNLI (much lower than transformer baselines).",
            "baseline_comparison": "Underperforms compared to BERT and RoBERTa baselines by a wide margin on CONJNLI, highlighting the difficulty of conjunctive non-boolean inferences for older LSTM-based models.",
            "limitations_or_failures": "Performs poorly on conjunctive NLI; lacks pretraining and the representational power of transformer-based models, therefore falls behind on both boolean and non-boolean inferences.",
            "insights_or_conclusions": "Older sequence models like ESIM are insufficient for the nuanced conjunctive reasoning captured by CONJNLI; modern pre-trained transformers perform better but still have notable limitations.",
            "uuid": "e8865.2",
            "source_info": {
                "paper_title": "ConjNLI: Natural Language Inference over Conjunctive Sentences",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "IAFT",
            "name_full": "Iterative Adversarial Fine-Tuning",
            "brief_description": "A training procedure introduced in the paper that iteratively fine-tunes a pre-trained RoBERTa model by mixing a fixed heuristically-created adversarial conjunctive training set with random sampled MNLI examples every epoch to improve robustness to conjunctive phenomena.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa-base with Iterative Adversarial Fine-Tuning (IAFT)",
            "model_description": "IAFT begins with RoBERTa fine-tuned on MNLI; at each epoch it samples an MNLI subset equal in size to the adversarial set and trains on the mixture of MNLI subset + constant adversarial set (15k synthetic conjunctive examples created with boolean/non-boolean heuristics).",
            "model_size": "base (RoBERTa-base; exact parameter count not specified in paper)",
            "reasoning_task_name": "CONJNLI (Conjunctive NLI stress-test)",
            "reasoning_task_description": "Expert-annotated dataset testing boolean and non-boolean conjunctive semantics in NLI (premise/hypothesis differ by conjuncts added/removed/replaced).",
            "method_or_approach": "Adversarial data creation (15k synthetic examples using boolean heuristics and limited non-boolean heuristics) + iterative fine-tuning mixing MNLI examples each epoch (Algorithm 1 in paper).",
            "performance": "IAFT achieved CONJNLI Dev: 69.18% and CONJNLI Test: 67.90%, improving over RoBERTa baseline (CONJ Dev 64.68%, Test 65.50%) while retaining MNLI Dev performance (~86.93/86.81).",
            "baseline_comparison": "IAFT outperforms simple Adversarial Fine-Tuning (AFT): AFT had CONJ Dev 67.57% but caused MNLI Dev to drop to ~76.6; IAFT retains MNLI performance while improving CONJNLI. IAFT corrects mostly boolean examples (≈65%) more than non-boolean (≈35%).",
            "limitations_or_failures": "Adversarial examples were created with shallow heuristics and contain errors (~70% accuracy on a manual 100-example check); IAFT mainly fixes boolean cases and remains weak on non-boolean conjunctions (named entities, collective readings, exclusive/equivalence 'or'); gains saturate after ~12k synthetic examples.",
            "insights_or_conclusions": "Mixing adversarial synthetic conjunctive examples with original MNLI data each epoch helps teach RoBERTa boolean conjunctive heuristics without catastrophic forgetting, but rule-based adversarial data is insufficient to cover diverse non-boolean real-world cases, indicating need for richer adversarial generation or deeper semantic models.",
            "uuid": "e8865.3",
            "source_info": {
                "paper_title": "ConjNLI: Natural Language Inference over Conjunctive Sentences",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "PA-RoBERTa",
            "name_full": "Predicate-Aware RoBERTa (RoBERTa + SRL embeddings)",
            "brief_description": "A model augmentation that fuses predicate-aware semantic role embeddings (obtained from a BERT-SRL model) with RoBERTa's [CLS] representations to help distinguish cases where predicate roles determine the inference label.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa-base + predicate-aware SRL features",
            "model_description": "RoBERTa's [CLS] embeddings for premise and hypothesis are concatenated/combined with transformed [CLS] embeddings from a BERT model fine-tuned for SRL (BERT-SRL weights frozen); fused representation is used for NLI classification (predicate-aware late fusion).",
            "model_size": "base (RoBERTa-base + BERT-SRL; exact parameter counts not specified in paper)",
            "reasoning_task_name": "CONJNLI (Conjunctive NLI stress-test)",
            "reasoning_task_description": "As above: expert-annotated NLI pairs focusing on conjunction boolean/non-boolean semantics.",
            "method_or_approach": "Late fusion of SRL-derived predicate-aware embeddings (BERT-SRL trained on CoNLL-2005) into the RoBERTa NLI classifier to make model aware of predicate arguments/roles.",
            "performance": "Predicate-Aware (PA) RoBERTa showed marginal gains over RoBERTa: overall CONJNLI accuracy reported as ≈66.30% (breakdown by conjunction type: And 66.29%, Or 60.93%, But 81.48%, Multiple 66.81%). This is a modest improvement over plain RoBERTa (All ≈65.60% in some tables / baseline ~64.68 in others).",
            "baseline_comparison": "PA yields small gains over plain RoBERTa, especially on some categories (multiple conjunctions), but combined PA+IAFT did not lead to further improvement beyond IAFT alone in this study.",
            "limitations_or_failures": "Gains are marginal; PA does not fully address non-boolean conjunction issues. Using frozen SRL features may limit benefit; more integrated or more accurate semantic parsing may be needed.",
            "insights_or_conclusions": "Adding explicit predicate-role information helps some conjunctive inferences where predicate arguments determine labels, but alone is insufficient to solve hard non-boolean cases—encourages future work on richer semantic representations or joint SRL-NLI modeling.",
            "uuid": "e8865.4",
            "source_info": {
                "paper_title": "ConjNLI: Natural Language Inference over Conjunctive Sentences",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "AFT",
            "name_full": "Adversarial Fine-Tuning (simple two-step)",
            "brief_description": "A simpler adversarial training baseline where a model fine-tuned on MNLI is further fine-tuned (two-step) on the adversarial synthetic conjunctive dataset (without iterative mixing), used as a comparison to IAFT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa-base with two-step Adversarial Fine-Tuning (AFT)",
            "model_description": "Start with RoBERTa fine-tuned on MNLI, then further fine-tune it solely on the heuristically-created adversarial conjunctive dataset (15k examples) in a two-step process.",
            "model_size": "base (RoBERTa-base; exact parameter count not specified in paper)",
            "reasoning_task_name": "CONJNLI (Conjunctive NLI stress-test)",
            "reasoning_task_description": "As above: conjunctive-focused NLI pairs requiring boolean and non-boolean reasoning.",
            "method_or_approach": "Two-step fine-tuning on MNLI then adversarial synthetic conjunctive examples; no per-epoch mixing with MNLI.",
            "performance": "AFT achieved CONJNLI Dev ≈67.57% but caused MNLI Dev scores to drop substantially (to ≈76.61/76.68 matched/mismatched), indicating catastrophic shift toward adversarial heuristics.",
            "baseline_comparison": "AFT improves over the MNLI-only baseline on CONJNLI somewhat, but performs worse than IAFT and harms original MNLI performance significantly.",
            "limitations_or_failures": "Biases the model toward the (noisy) adversarial set, causing degradation on original MNLI; adversarial data's heuristic labels are imperfect (~70% accurate on small manual sample) and miss many non-boolean cases.",
            "insights_or_conclusions": "Direct fine-tuning on synthetically-created adversarial data can help but risks overfitting to heuristic artifacts; iterative mixing with original data (IAFT) mitigates forgetting and yields a better trade-off.",
            "uuid": "e8865.5",
            "source_info": {
                "paper_title": "ConjNLI: Natural Language Inference over Conjunctive Sentences",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Probing natural language inference models through semantic fragments",
            "rating": 2
        },
        {
            "paper_title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "rating": 2
        },
        {
            "paper_title": "Adversarial NLI: A new benchmark for natural language understanding",
            "rating": 2
        },
        {
            "paper_title": "Breaking NLI systems with sentences that require simple lexical inferences",
            "rating": 2
        },
        {
            "paper_title": "Do neural models learn systematicity of monotonicity inference in natural language?",
            "rating": 1
        },
        {
            "paper_title": "Inoculation by fine-tuning: A method for analyzing challenge datasets",
            "rating": 1
        }
    ],
    "cost": 0.0156705,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ConJNLI: Natural Language Inference Over Conjunctive Sentences</h1>
<p>Swarnadeep Saha Yixin Nie Mohit Bansal<br>UNC Chapel Hill<br>{swarna, yixin1, mbansal}@cs.unc.edu</p>
<h4>Abstract</h4>
<p>Reasoning about conjuncts in conjunctive sentences is important for a deeper understanding of conjunctions in English and also how their usages and semantics differ from conjunctive and disjunctive boolean logic. Existing NLI stress tests do not consider non-boolean usages of conjunctions and use templates for testing such model knowledge. Hence, we introduce CONJNLI, a challenge stress-test for natural language inference over conjunctive sentences, where the premise differs from the hypothesis by conjuncts removed, added, or replaced. These sentences contain single and multiple instances of coordinating conjunctions ("and", "or", "but", "nor") with quantifiers, negations, and requiring diverse boolean and non-boolean inferences over conjuncts. We find that large-scale pre-trained language models like RoBERTa do not understand conjunctive semantics well and resort to shallow heuristics to make inferences over such sentences. As some initial solutions, we first present an iterative adversarial fine-tuning method that uses synthetically created training data based on boolean and non-boolean heuristics. We also propose a direct model advancement by making RoBERTa aware of predicate semantic roles. While we observe some performance gains, CONJNLI is still challenging for current methods, thus encouraging interesting future work for better understanding of conjunctions. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Coordinating conjunctions are a common syntactic phenomenon in English: 38.8\% of sentences in the Penn Tree Bank have at least one coordinating word between "and", "or", and "but" (Marcus et al., 1993). Conjunctions add complexity to the sentences, thereby making inferences over such sen-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tences more realistic and challenging. A sentence can have many conjunctions, each conjoining two or more conjuncts of varied syntactic categories such as noun phrases, verb phrases, prepositional phrases, clauses, etc. Besides syntax, conjunctions in English have a lot of semantics associated to them and different conjunctions ("and" vs "or") affect the meaning of a sentence differently.</p>
<p>Recent years have seen significant progress in the task of Natural Language Inference (NLI) through the development of large-scale datasets like SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018). Although large-scale pre-trained language models like BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) have achieved super-human performances on these datasets, there have been concerns raised about these models exploiting idiosyncrasies in the data using tricks like pattern matching (McCoy et al., 2019). Thus, various stress-testing datasets have been proposed that probe NLI models for simple lexical inferences (Glockner et al., 2018), quantifiers (Geiger et al., 2018), numerical reasoning, antonymy and negation (Naik et al., 2018). However, despite the heavy usage of conjunctions in English, there is no specific NLI dataset that tests their understanding in detail. Although SNLI has $30 \%$ of samples with conjunctions, most of these examples do not require inferences over the conjuncts that are connected by the coordinating word. On a random sample of 100 conjunctive examples from SNLI, we find that $72 \%$ of them have the conjuncts unchanged between the premise and the hypothesis (e.g., "Man and woman sitting on the sidewalk" $\rightarrow$ "Man and woman are sitting") and there are almost no examples with non-boolean conjunctions (e.g., "A total of five men and women are sitting." $\rightarrow$ "A total of 5 men are sitting." (contradiction)). As discussed below, inference over conjuncts directly translates to boolean and non-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">#</th>
<th style="text-align: center;">Premise</th>
<th style="text-align: center;">Hypothesis</th>
<th style="text-align: center;">Label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ConJNLI Dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">He is a Worcester resident and a member of the Democratic Party.</td>
<td style="text-align: center;">He is a member of the Democratic Party.</td>
<td style="text-align: center;">entailment</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">He is a member of the Democratic Party.</td>
<td style="text-align: center;">He is a Worcester resident and a member of the Democratic Party.</td>
<td style="text-align: center;">neutral</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">He is a Worcester resident and a member of the Democratic Party.</td>
<td style="text-align: center;">He is a Worcester resident and a member of the Republican Party.</td>
<td style="text-align: center;">contradiction</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">A total of 793880 acre, or $\mathbf{3 6}$ percent of the park was affected by the wildfires.</td>
<td style="text-align: center;">A total of 793880 acre, was affected by the wildfires.</td>
<td style="text-align: center;">entailment</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Its total running time is 9 minutes and 9 seconds, spanning seven tracks.</td>
<td style="text-align: center;">Its total running time is 9 minutes, spanning seven tracks.</td>
<td style="text-align: center;">contradiction ${ }^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">He began recording for the Columbia Phonograph Company, in 1889 or 1890.</td>
<td style="text-align: center;">He began recording for the Columbia Phonograph Company, in 1890.</td>
<td style="text-align: center;">neutral ${ }^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Fowler wrote or co-wrote all but one of the songs on album.</td>
<td style="text-align: center;">Fowler wrote or co-wrote all of the songs on album.</td>
<td style="text-align: center;">contradiction ${ }^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">All devices they tested did not produce gravity or anti-gravity.</td>
<td style="text-align: center;">All devices they tested did not produce gravity.</td>
<td style="text-align: center;">entailment</td>
</tr>
<tr>
<td style="text-align: center;">SNLI Dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">A woman with a green headscarf, blue shirt and a very big grin.</td>
<td style="text-align: center;">The woman is young.</td>
<td style="text-align: center;">neutral</td>
</tr>
<tr>
<td style="text-align: center;">MNLI Dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">You and your friends are not welcome here, said Severn.</td>
<td style="text-align: center;">Severn said the people were not welcome there.</td>
<td style="text-align: center;">entailment</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples from our CONJNLI dataset consisting of single and multiple occurrences of different coordinating conjunctions (and, or, but), boolean or non-boolean in the presence of negations and quantifiers. Typical SNLI and MNLI examples do not require inference over conjuncts. $\dagger=$ Non-boolean usages of different conjunctions.
boolean semantics and thus becomes essential for understanding conjunctions.</p>
<p>In our work, we introduce CONJNLI, a new stress-test for NLI over diverse and challenging conjunctive sentences. Our dataset contains annotated examples where the hypothesis differs from the premise by either a conjunct removed, added or replaced. These sentences contain single and multiple instances of coordinating conjunctions (and, or, but, nor) with quantifiers, negations, and requiring diverse boolean and non-boolean inferences over conjuncts. Table 1 shows many examples from CONJNLI and compares these with typical conjunctive examples from SNLI and MNLI. In the first two examples, the conjunct "a Worcester resident" is removed and added, while in the third example, the other conjunct "a member of the Democratic Party" is replaced by "a member of the Republican Party". Distribution over conjuncts in a conjunctive sentence forms multiple simple sentences. For example, the premise in the first example of Table 1 can be broken into "He is a Worcester resident." and "He is a member of the Democratic Party.". Correspondingly, from
boolean semantics, it requires an inference of the form " $A$ and $B \rightarrow A$ ". Likewise, the third example is of the form " $A$ and $B \rightarrow A$ and $C$ ". While such inferences are rather simple from the standpoint of boolean logic, similar rules do not always translate to English, e.g., in non-boolean cases, i.e., an inference of the form " $A$ and $B \rightarrow A$ " is not always entailment or an inference of the form " $A$ or $B \rightarrow A$ " is not always neutral (Hoeksema, 1988). Consider the three examples marked with a $\dagger$ in Table 1 showing non-boolean usages of "and", "or" and "but" in English. In the fifth example, the total time is a single entity and cannot be separated in an entailed hypothesis. In the sixth example, "or" is used as "exclusive-or" because the person began recording in either 1889 or 1890.</p>
<p>We observe that state-of-the-art models such as BERT and RoBERTa, trained on existing datasets like SNLI and MNLI, often fail to make these inferences for our dataset. For example, BERT predicts entailment for the non-boolean "and" example #5 in Table 1 as well. This relates to the lexical overlap issue in these models (McCoy et al., 2019), since all the words in the hypothesis are also part of the</p>
<p>premise for the example. Conjunctions are also challenging in the presence of negations. For example, a sentence of the form "not $A$ or $B$ " translates to "not $A$ and not $B$ ", as shown in example #8 of Table 1. Finally, a sentence may contain multiple conjunctions (with quantifiers), further adding to the complexity of the task (example #7 in Table 1). Thus, our CONJNLI dataset presents a new and interesting real-world challenge task for the community to work on and allow development of deeper NLI models.</p>
<p>We also present some initial model advancements that attempt to alleviate some of these challenges in our new dataset. First, we create synthetic training data using boolean and non-boolean heuristics. We use this data to adversarially train RoBERTa-style models by an iterative adversarial fine-tuning method. Second, we make RoBERTa aware of predicate semantic roles by augmenting the NLI model with the predicate-aware embeddings of the premise and the hypothesis. Predicate arguments in sentences can help distinguish between two syntactically similar inference pairs with different target labels (Table 5 shows an example). Overall, our contributions are:</p>
<ul>
<li>We introduce CONJNLI, a new stress-test for NLI in conjunctive sentences, consisting of boolean and non-boolean examples with single and multiple coordinating conjunctions ("and", "or", "but", "nor"), negations, quantifiers and requiring diverse inferences over conjuncts (with high inter-annotator agreement between experts).</li>
<li>We show that BERT and RoBERTa do not understand conjunctions well enough and use shallow heuristics for inferences over such sentences.</li>
<li>We propose initial improvements for our task by adversarially fine-tuning RoBERTa using an iterative adversarial fine-tuning algorithm and also augmenting RoBERTa with predicate-aware embeddings. We obtain initial gains but with still large room for improvement, which will hopefully encourage future work on better understanding of conjunctions.</li>
</ul>
<h2>2 Related Work</h2>
<p>Our work is positioned at the intersection of understanding the semantics of conjunctions in English and its association to NLI.</p>
<p>Conjunctions in English. There is a long history of analyzing the nuances of coordinating conjunctions in English and how these compare to
boolean and non-boolean semantics (Gleitman, 1965; Keenan and Faltz, 2012). Linguistic studies have shown that noun phrase conjuncts in "and" do not always behave in a boolean manner (Massey, 1976; Hoeksema, 1988; Krifka, 1990). In the NLP community, studies on conjunctions have mostly been limited to treating it as a syntactic phenomenon. One of the popular tasks is that of conjunct boundary identification (Agarwal and Boggess, 1992). Ficler and Goldberg (2016a) show that state-of-the-art parsers often make mistakes in identifying conjuncts correctly and develop neural models to accomplish this (Ficler and Goldberg, 2016b; Teranishi et al., 2019). Saha and Mausam (2018) also identify conjuncts to break conjunctive sentences into simple ones for better downstream Open IE (Banko et al., 2007). However, we study the semantics of conjunctions through our challenging dataset for NLI.</p>
<p>Analyzing NLI Models. Our research follows a body of work trying to understand the weaknesses of neural models in NLI. Poliak et al. (2018b); Gururangan et al. (2018) first point out that hypothesisonly models also achieve high accuracy in NLI, thereby revealing weaknesses in existing datasets. Various stress-testing datasets have been proposed since, focusing on lexical inferences (Glockner et al., 2018), quantifiers (Geiger et al., 2018), biases on specific words (Sanchez et al., 2018), verb verdicality (Ross and Pavlick, 2019), numerical reasoning (Ravichander et al., 2019), negation, antonymy (Naik et al., 2018), pragmatic inference (Jeretic et al., 2020) and systematicity of monotonicity (Yanaka et al., 2020). Besides syntax, other linguistic information have also been investigated (Poliak et al., 2018a; White et al., 2017) but none of these focus on conjunctions. The closest work on conjunctions is by Richardson et al. (2020) where they probe NLI models through semantic fragments. However, their focus is only on boolean "and", allowing them to assign labels automatically through simple templates. Also, their goal is to get BERT to master semantic fragments, which, as they mention, is achieved with a few minutes of additional finetuning on their templated data. CONJNLI, however, is more diverse and challenging for BERTstyle models, includes all common coordinating conjunctions, and captures non-boolean usages.</p>
<p>Adversarial Methods in NLP. Adversarial training for robustifying neural models has been</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Flow diagram of CONJNLI dataset creation.
proposed in many NLP tasks, most notably in QA (Jia and Liang, 2017; Wang and Bansal, 2018) and NLI (Nie et al., 2019). Nie et al. (2020) improve existing NLI stress tests using adversarially collected NLI data (ANLI) and Kaushik et al. (2020) use counter-factually augmented data for making models robust to spurious patterns. Following Jia and Liang (2017), we also create adversarial training data by performing all data creation steps except for the expensive human annotation. Our iterative adversarial fine-tuning method adapts adversarial training in a fine-tuning setup for BERT-style models and improves results on CONJNLI while maintaining performance on existing datasets.</p>
<h2>3 Data Creation</h2>
<p>Creation of CONJNLI involves four stages, as shown in Figure 1. The (premise, hypothesis) pairs are created automatically, followed by manual verification and expert annotation.</p>
<h3>3.1 Conjunctive Sentence Selection</h3>
<p>We start by choosing conjunctive sentences from Wikipedia containing all common coordinating conjunctions ("and", "or", "but", "nor"). Figure 1 shows an example. We choose Wikipedia because it contains complex sentences with single and multiple conjunctions, and similar choices have also been made in prior work on information extraction from conjunctive sentences (Saha and Mausam, 2018). In order to capture a diverse set of conjunctive phenomena, we gather sentences with multiple conjunctions, negations, quantifiers and various syntactic constructs of conjuncts.</p>
<h3>3.2 Conjuncts Identification</h3>
<p>For conjunct identification, we process the conjunctive sentence using a state-of-the-art constituency parser implemented in AllenNLP ${ }^{2}$ and then choose the two phrases in the resulting constituency parse on either side of the conjunction as conjuncts. A conjunction can conjoin more than two conjuncts, in which case we identify the two surrounding the conjunction and ignore the rest. Figure 1 shows</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>an example where the two conjuncts "a Worcester resident" and "a member of the Democratic Party" are identified with the conjunction "and".</p>
<h3>3.3 NLI Pair Creation</h3>
<p>Once the conjuncts are identified, we perform three operations by removing, adding or replacing one of the two conjuncts to obtain another sentence such that the original sentence and the modified sentence form a plausible NLI pair. Figure 1 shows a pair created by the removal of one conjunct. We create the effect of adding a conjunct by swapping the premise and hypothesis from the previous example. We replace a conjunct by finding a conjunct word that can be replaced by its antonym or co-hyponym. Wikipedia sentences frequently contain numbers or names of persons in the conjuncts which are replaced by adding one to the number and randomly sampling any other name from the dataset respectively. We apply the three conjunct operations on all collected conjunctive sentences.</p>
<h3>3.4 Manual Validation \&amp; Expert Annotation</h3>
<p>Since incorrect conjunct identification can lead to the generation of a grammatically incorrect sentence, the pairs are first manually verified for grammaticality. The grammatical ones are next annotated by two English-speaking experts (with prior experience in NLI and NLP) into entailment, neutral and contradiction labels. We refrain from using Amazon Mechanical Turk for the label assignment because our NLI pairs' labeling requires deeper understanding and identification of the challenging conjunctive boolean versus non-boolean semantics (see examples #1 and #5 in Table 1 where the same conjunct removal operation leads to two different labels). Expert annotation has been performed in previous NLI stress-tests as well (Ravichander et al., 2019; McCoy et al., 2019) so as to ensure a high-quality dataset.
Annotator Instructions and Agreement: Each annotator is initially trained by showing 10 examples, of which some have boolean usages and others non-boolean. The examples are further accompanied with clear explanations for the choice of labels.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Ent</th>
<th style="text-align: center;">Neu</th>
<th style="text-align: center;">Contra</th>
<th style="text-align: right;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Conj Dev</td>
<td style="text-align: center;">204</td>
<td style="text-align: center;">281</td>
<td style="text-align: center;">138</td>
<td style="text-align: right;">623</td>
</tr>
<tr>
<td style="text-align: left;">Conj Test</td>
<td style="text-align: center;">332</td>
<td style="text-align: center;">467</td>
<td style="text-align: center;">201</td>
<td style="text-align: right;">1000</td>
</tr>
<tr>
<td style="text-align: left;">Conj All</td>
<td style="text-align: center;">536</td>
<td style="text-align: center;">748</td>
<td style="text-align: center;">339</td>
<td style="text-align: right;">1623</td>
</tr>
</tbody>
</table>
<p>Table 2: Dataset splits of CONJNLI.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">and</th>
<th style="text-align: center;">or</th>
<th style="text-align: center;">but</th>
<th style="text-align: center;">multiple</th>
<th style="text-align: center;">quant</th>
<th style="text-align: center;">neg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Conj Dev</td>
<td style="text-align: center;">320</td>
<td style="text-align: center;">293</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">152</td>
<td style="text-align: center;">131</td>
<td style="text-align: center;">70</td>
</tr>
<tr>
<td style="text-align: left;">Conj Test</td>
<td style="text-align: center;">537</td>
<td style="text-align: center;">471</td>
<td style="text-align: center;">135</td>
<td style="text-align: center;">229</td>
<td style="text-align: center;">175</td>
<td style="text-align: center;">101</td>
</tr>
<tr>
<td style="text-align: left;">Conj All</td>
<td style="text-align: center;">857</td>
<td style="text-align: center;">764</td>
<td style="text-align: center;">234</td>
<td style="text-align: center;">381</td>
<td style="text-align: center;">306</td>
<td style="text-align: center;">171</td>
</tr>
</tbody>
</table>
<p>Table 3: Data analysis by conjunction types, presence of quantifiers and negations.</p>
<p>The appendix contains a subset of these examples. The annotations are done in two rounds - in the first, each annotator annotated the examples independently and in the second, the disagreements are discussed to resolve final labels. The interannotator agreement between the annotators has a high Cohen's Kappa ( $\kappa$ ) of 0.83 and we keep only those pairs that both agree on.</p>
<h2>4 Data Analysis</h2>
<p>Post-annotation, we arrive at a consolidated set of 1623 examples, which is a reasonably large size compared to previous NLI stress-tests with expert annotations. We randomly split these into 623 validation and 1000 test examples, as shown in Table 2. CONJNLI also replicates the approximate distribution of each conjunction in English (Table 3). Thus, "and" is maximally represented in our dataset, followed by "or"3 and "but". Sentences with multiple conjunctions make up a sizeable $23 \%$ of CONJNLI to reflect real-world challenging scenarios. As we discussed earlier, conjunctions are further challenging in the presence of quantifiers and negations, due to their association with boolean logic. These contribute to $18 \%$ and $10 \%$ of the dataset, resp.</p>
<p>We note that conjunctive sentences can contain conjuncts of different syntactic categories, ranging from words of different part of speech tags to various phrasal constructs to even sentences. Table 4 shows a small subset of the diverse syntactic constructs of conjuncts in CONJNLI. The conjuncts within a sentence may belong to different categories - the first example conjoins a noun phrase with an adjective. Each conjunct can be a clause, as shown in the fifth example.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4: CONJNLI sentences consist of varied syntactic conjunct categories (bolded). $\mathrm{CT}=$ Conjunct Types, $\mathrm{NP}=$ Noun Phrase, $\mathrm{VP}=$ Verb Phrase, $\mathrm{AdvP}=$ Adverbial Phrase.</p>
<h2>5 Methods</h2>
<p>In this section, we first describe our iterative adversarial fine-tuning method (including the creation of adversarial training data), followed by some initial predicate-aware models to try to tackle CONJNLI.</p>
<h3>5.1 Iterative Adversarial Fine-Tuning</h3>
<p>Automated Adversarial Training Data Creation. Creation of large-scale conjunctive-NLI training data, where each example is manually labeled, is prohibitive because of the amount of human effort involved in the process and the diverse types of exceptions involved in the conjunction inference labeling process. Hence, in this section, we first try to automatically create some training data to train models for our challenging CONJNLI stress-test and show the limits of such rule-based adversarial training methods. For this automated training data creation, we follow the same process as Section 3 but replace the expert humanannotation phase with automated boolean rules and some initial heuristics for non-boolean ${ }^{4}$ semantics so as to assign labels to these pairs automatically. For "boolean and", if "A and B" is true, we assume that A and B are individually true, and hence when-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Iterative Adversarial Fine-Tuning
    model \(=\) finetune(RoBERTa, MNLI \(_{\text {train }}\)
    adv_train \(=\) get_adv_data()
    \(k=\) len(adv, train)
    for \(e=1\) to num_epochs do
        MNLI_small = sample_data \(\left(\right.\) MNLI \(\left._{\text {train }}, k\right)\)
        all_data \(=\) MNLI_small \(\bigcup\) adv_train
        Shuffle all_data
        model \(=\) finetune(model, all_data)
    end for
</code></pre></div>

<p>ever we remove a conjunct, we assign the label entailment and whenever we add a conjunct, we assign the label neutral. Examples with conjunct replaced are assigned the label contradiction. As already shown in Table 1, there are of course exceptions to these rules, typically arising from the "nonboolean" usages. Hoeksema (1988); Krifka (1990) show that conjunctions of proper names or named entities, definite descriptions, and existential quantifiers often do not behave according to general boolean principles. Hence, we use these suggestions to develop some initial non-boolean heuristics for our automated training data creation. First, whenever we remove a conjunct from a named entity ("Franklin and Marshall College" $\rightarrow$ "Franklin College"), we assign the label neutral because it typically refers to a different named entity. Second, "non-boolean and" is prevalent in sentences where the conjunct entities together map onto a collective entity and often in the presence of certain trigger words like "total", "group", "combined", etc. (but note that this is not always true). For example, removing the conjunct "flooding" in the sentence "In total, the flooding and landslides killed 3,185 people in China." should lead to contradiction. We look for such trigger words in the sentence and heuristically assign contradiction label to the pair. Like "and", the usage of "or" in English often differs from boolean "or". The appendix contains details of the various interpretations of English "or", and our adversarial data creation heuristics.</p>
<p>We create a total of 15 k adversarial training examples using the aforementioned shallow heuristics, with an equal number of examples for "and", "or" and "but". A random sample of 100 examples consisting of an equal number of "and", "or" and "but" examples are chosen for manual validation by one of the annotators, yielding an accuracy of $70 \%$. We find that most of the errors either have challenging non-boolean scenarios which cannot be handled by our heuristics or have ungrammatical hypotheses, originating from parsing errors.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Architecture diagram of predicate-aware RoBERTa model for CONJNLI. * = BERT-SRL weights are frozen while fine-tuning on the NLI task.</p>
<p>Algorithm for Iterative Adversarial FineTuning. Our adversarial training method is outlined in Algorithm 1. We look to improve results on CONJNLI through adversarial training while maintaining state-of-the-art results on existing datasets like MNLI. Thus, we first fine-tune RoBERTa on the entire MNLI training data. Next, at each epoch, we randomly sample an equal amount of original MNLI training examples with conjunctions as the amount of adversarial training data. We use the combined data to further fine-tune the MNLI finetuned model. At each epoch, we iterate over the original MNLI training examples by choosing a different random set every time, while keeping the adversarial data constant. The results section discusses the efficacy of our algorithm.</p>
<p>As shown later, adversarial training leads to limited improvements on CONJNLI due to the rulebased training data creation. Since real-world conjunctions are much more diverse and tricky, our dataset encourages future work by the community and also motivates a need for direct model development like our initial predicate-aware RoBERTa.</p>
<h3>5.2 Initial Predicate-Aware (SRL) RoBERTa</h3>
<p>We find that CONJNLI contains examples where the inference label depends on the predicate and the predicate roles in the sentence. Consider the two examples in Table 5. The two premises are syntactically similar and both undergo the conjunct replacement operation for creating the hypothesis. However, their respective predicates "premiered" and "played" have different arguments, notably one referring to a premier date while the other describing playing in a location. Motivated by the need to better understand predicates and predicate roles in</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Premise</th>
<th style="text-align: left;">Hypothesis</th>
<th style="text-align: left;">Label</th>
<th style="text-align: left;">SRL Tags</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">It premiered on 27 June 2016 and <br> airs Mon-Fri 10-11pm IST.</td>
<td style="text-align: left;">It premiered on 28 June 2016 and <br> airs Mon-Fri 10-11pm IST.</td>
<td style="text-align: left;">contra</td>
<td style="text-align: left;">ARG1:"It", Verb:"premiered", <br> Temporal:"on 27 June 2016"</td>
</tr>
<tr>
<td style="text-align: left;">He also played in the East-West <br> Shrine Game and was named MVP <br> of the Senior Bowl.</td>
<td style="text-align: left;">He also played in the North-South <br> Shrine Game and was named MVP <br> of the Senior Bowl.</td>
<td style="text-align: left;">neutral</td>
<td style="text-align: left;">ARG1: "He", Discource:"also", <br> Verb:"played", Location:"in the <br> East-West Shrine Game".</td>
</tr>
</tbody>
</table>
<p>Table 5: Two examples from CONJNLI where SRL tags can help the model predict the correct label.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">MD</th>
<th style="text-align: right;">SD</th>
<th style="text-align: right;">CD</th>
<th style="text-align: right;">CT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT-S</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">90.85</td>
<td style="text-align: right;">60.03</td>
<td style="text-align: right;">59.40</td>
</tr>
<tr>
<td style="text-align: left;">BERT-M</td>
<td style="text-align: right;">$84.10 / 83.90$</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">58.10</td>
<td style="text-align: right;">61.40</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-S</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">$\mathbf{9 1 . 8 7}$</td>
<td style="text-align: right;">60.99</td>
<td style="text-align: right;">63.50</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-M</td>
<td style="text-align: right;">$\mathbf{8 7 . 5 6 / 8 7 . 5 1}$</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">$\mathbf{6 4 . 6 8}$</td>
<td style="text-align: right;">$\mathbf{6 5 . 5 0}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison of BERT and RoBERTa trained on SNLI and MNLI and tested on respective dev sets and CONJNLI. MNLI Dev (MD) results are in match/mismatched format. $\mathrm{SD}=\mathrm{SNLI} \mathrm{Dev}, \mathrm{CD}=\mathrm{Conj}$ Dev, $\mathrm{CT}=$ Conj Test.</p>
<p>NLI pairs, we propose a predicate-aware RoBERTa model, built on top of a standard RoBERTa model for NLI. Figure 2 shows the architecture diagram. We make the model aware of predicate roles by using representations of both the premise and the hypothesis from a fine-tuned BERT model on the task of Semantic Role Labeling (SRL). ${ }^{5}$ Details of the BERT-SRL model can be found in the appendix. Let the RoBERTa embedding of the $[C L S]$ token be denoted by $C_{N L I}$. The premise and hypothesis are also passed through the BERT-SRL model to obtain predicate-aware representations for each. These are similarly represented by the corresponding $[C L S]$ token embeddings. We learn a linear transformation on top of these embeddings to obtain $C_{P}$ and $C_{H}$. Following Pang et al. (2019), where they use late fusion of syntactic information for NLI, we perform the same with the predicateaware SRL representations. A final classification head gives the predictions.</p>
<h3>5.3 Predicate-Aware RoBERTa with Adversarial Fine-Tuning</h3>
<p>In the last two subsections, we proposed enhancements both on the data side and the model side to tackle CONJNLI. Our final joint model now combines predicate-aware RoBERTa with iterative adversarial fine-tuning. We conduct experiments to analyze the effect of each of these enhancements as well as their combination.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>6 Experiments and Results</h2>
<p>We perform experiments on three datasets - (1) CONJNLI, (2) SNLI (Bowman et al., 2015) and (3) MNLI (Williams et al., 2018). The appendix contains details about our experimental setup.</p>
<h3>6.1 Baselines</h3>
<p>We first train BERT and RoBERTa on the SNLI (BERT-S, RoBERTa-S) and MNLI (BERT-M, RoBERTa-M) training sets and evaluate their performance on the respective dev sets and CONJNLI, as shown in Table 6. We observe a similar trend for both MNLI and CONJNLI, with MNLI-trained RoBERTa being the best performing model. This is perhaps unsurprising as MNLI contains more complex inference examples compared to SNLI. The results on CONJNLI are however significantly worse than MNLI, suggesting a need for better understanding of conjunctions. We also experimented with older models like ESIM (Chen et al., 2017) and the accuracy on CONJNLI was much worse at $53.10 \%$. All our successive experiments are conducted using RoBERTa with MNLI as the base training data, owing to its superior performance.</p>
<p>In order to gain a deeper understanding of these models' poor performance, we randomly choose 100 examples with "and" and only replace the "and" with "either-or" (exclusive-or) along with the appropriate change in label. For example, "He received bachelor's degree in 1967 and PhD in 1973." $\rightarrow$ "He received bachelor's degree in 1967." (entailment) is changed to "He either received bachelor's degree in 1967 or PhD in 1973." $\rightarrow$ "He received bachelor's degree in 1967." (neutral). We find that while RoBERTa gets most of the "and" examples correct, the "or" examples are mostly incorrect because the change in conjunction does not lead to a change in the predicted label for any of the examples. This points to the lexical overlap heuristic (McCoy et al., 2019) learned by the model that if the hypothesis is a subset of the premise, the label is mostly entailment, while ignoring the type of conjunction.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Conj Dev</th>
<th style="text-align: center;">MNLI Dev</th>
<th style="text-align: right;">Conj Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">58.10</td>
<td style="text-align: center;">$84.10 / 83.90$</td>
<td style="text-align: right;">61.40</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">64.68</td>
<td style="text-align: center;">$\mathbf{8 7 . 5 6 / 8 7 . 5 1}$</td>
<td style="text-align: right;">65.50</td>
</tr>
<tr>
<td style="text-align: left;">AFT</td>
<td style="text-align: center;">67.57</td>
<td style="text-align: center;">$76.61 / 76.68$</td>
<td style="text-align: right;">66.40</td>
</tr>
<tr>
<td style="text-align: left;">IAFT</td>
<td style="text-align: center;">$\mathbf{6 9 . 1 8}$</td>
<td style="text-align: center;">$86.93 / 86.81$</td>
<td style="text-align: right;">$\mathbf{6 7 . 9 0}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Table showing the effectiveness of IAFT over AFT and other baseline models.</p>
<h3>6.2 Iterative Adversarial Fine-Tuning</h3>
<p>We compare our proposed Iterative Adversarial Fine-Tuning (IAFT) approach with simple Adversarial Fine-Tuning (AFT) wherein we start with the MNLI fine-tuned RoBERTa model and fine-tune it further with the adversarial data, in a two-step process. ${ }^{6}$ Table 7 shows that IAFT obtains the best average results between CONJNLI and MNLI with $2 \%$ improvement on the former and retaining state-of-the-art results on the latter. In the simple AFT setup, the model gets biased towards the adversarial data, resulting in a significant drop in the original MNLI results. The slight drop in CONJNLI results also indicates that the MNLI training data is useful for the model to learn about basic paraphrasing skills required in NLI. IAFT achieves that, by mixing the adversarial training data with an equal amount of MNLI examples in every epoch. We also analyze a subset of examples fixed by IAFT and find that unsurprisingly (based on the heuristics used to automatically create the adversarial training data in Sec. 5.1), it corrects more boolean examples than non-boolean ( $65 \%$ vs $35 \%$ ) and the non-boolean examples either have named entities or collective conjuncts.</p>
<p>We note that IAFT is a generic approach and can be used to improve other stress-tests in an adversarial fine-tuning setup. As an example, we apply it on the boolean subset of the dataset by Richardson et al. (2020) containing samples with "boolean and" and find that our model achieves a near perfect accuracy on their test set. Specifically, RoBERTa, trained on only MNLI, achieves a low accuracy of $41.5 \%$ on the test set, but on applying IAFT with an equal mix of MNLI and their training data in every epoch, the test accuracy improves to $99.8 \%$, while also retaining MNLI matched/mismatched results at $86.45 / 86.46 \%$.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 8: Comparison of all our final models on CONJNLI and MNLI.</p>
<h3>6.3 Predicate-Aware RoBERTa with Adversarial Fine-Tuning</h3>
<p>Table 8 consolidates our final results on both the datasets. We compare the baselines BERT and RoBERTa with (1) Predicate-aware RoBERTa (PA), (2) RoBERTa with Iterative Adversarial FineTuning (IAFT), and (3) Predicate-aware RoBERTa with Iterative Adversarial Fine-tuning (PA-IAFT). Our first observation is that PA marginally improves results on both the datasets. This is encouraging, as it shows that NLI in general, can benefit from more semantic information. However, we obtain a larger gain on CONJNLI with adversarial training. This, however, is unsurprising as the adversarial training data is specifically curated for the task, whereas PA is only exposed to the original MNLI training data. On combining both, our results do not improve further, thus promoting future work by the community on better understanding of conjunctions. Finally, all our models encouragingly maintain state-of-the-art results on MNLI.</p>
<h3>6.4 Amount of Adversarial Training Data</h3>
<p>We investigate the amount of training data needed for RoBERTa-style models to learn the heuristics used to create the adversarial data. We experiment with the IAFT model on CONJNLI dev and linearly increase the data size from 6 k to 18 k , comprising of an equal amount of "and", "or" and "but" examples. Figure 3 shows the accuracy curve. We obtain maximum improvements with the first 12 k examples ( 4 points), marginal improvement with the next 3 k and a slight drop in performance with the next 3 k . Early saturation shows that RoBERTa learns the rules using a small number of examples only and also exposes the hardness of CONJNLI.</p>
<h3>6.5 Instability Analysis</h3>
<p>Zhou et al. (2020) perform an in-depth analysis of the various NLI stress tests like HANS (McCoy et al., 2019), BREAK-NLI (Glockner et al., 2018), etc and find that different random initializa-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Effect of amount of adversarial training data.
tion seeds can lead to significantly different results on these datasets. They show that this instability largely arises from high inter-example similarity, as these datasets typically focus on a particular linguistic phenomenon by leveraging only a handful of patterns. Thus, following their suggestion, we conduct an instability analysis of CONJNLI by training RoBERTa on MNLI with 10 different seeds (1 to 10) and find that the results on CONJNLI are quite robust to such variations. The mean accuracy on CONJNLI dev is 64.48 , with a total standard deviation of 0.59 , independent standard deviation of 0.49 and a small inter-data covariance of 0.22 . CONJNLI's stable results compared to most previous stress-tests indicate the diverse nature of conjunctive inferences captured in the dataset.</p>
<h3>6.6 Analysis by Conjunction Type</h3>
<p>In Table 9, we analyze the performance of the models on the subset of examples containing "and", "or", "but" and multiple conjunctions. We find that "or" is the most challenging for pre-trained language models, particularly because of its multiple interpretations in English. We also note that all models perform significantly better on sentences with "but", owing to the lack of non-boolean usages in such sentences. Our initial predicate-aware model encouragingly obtains small improvements on all conjunction types (except "but"), indicating that perhaps these models can benefit from more linguistic knowledge. Although single conjunction examples benefit from adversarial training, multiple conjunctions prove to be challenging mainly due to the difficulty in automatically parsing and creating perfect training examples with such sentences (Ficler and Goldberg, 2016b).</p>
<h3>6.7 Analysis of Boolean versus Non-Boolean Conjunctions</h3>
<p>One of the expert annotators manually annotated the CONJNLI dev set for boolean and non-boolean examples. We find that non-boolean examples con-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">And</th>
<th style="text-align: left;">Or</th>
<th style="text-align: left;">But</th>
<th style="text-align: left;">Multiple</th>
<th style="text-align: left;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: left;">65.36</td>
<td style="text-align: left;">59.87</td>
<td style="text-align: left;">$\mathbf{8 1 . 4 8}$</td>
<td style="text-align: left;">65.93</td>
<td style="text-align: left;">65.60</td>
</tr>
<tr>
<td style="text-align: left;">PA</td>
<td style="text-align: left;">66.29</td>
<td style="text-align: left;">60.93</td>
<td style="text-align: left;">$\mathbf{8 1 . 4 8}$</td>
<td style="text-align: left;">$\mathbf{6 6 . 8 1}$</td>
<td style="text-align: left;">66.30</td>
</tr>
<tr>
<td style="text-align: left;">IAFT</td>
<td style="text-align: left;">$\mathbf{6 7 . 5 9}$</td>
<td style="text-align: left;">$\mathbf{6 2 . 2 0}$</td>
<td style="text-align: left;">80.00</td>
<td style="text-align: left;">62.88</td>
<td style="text-align: left;">$\mathbf{6 7 . 9 0}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison of all models on the subset of each conjunction type of CONJNLI.
tribute to roughly $34 \%$ of the dataset. Unsurprisingly, all models perform significantly better on the boolean subset compared to the non-boolean one. Specifically, the accuracies for RoBERTa, IAFT and PA on the boolean subset are $68 \%, 72 \%$ and $69 \%$ respectively, while on the non-boolean subset, these are $58 \%, 61 \%$ and $58 \%$ respectively. Based on these results, we make some key observations: (1) Non-boolean accuracy for all models are about $10 \%$ less than the boolean counterpart, revealing the hardness of the dataset, (2) IAFT improves both boolean and non-boolean subsets because of the non-boolean heuristics used in creating its adversarial training data, (3) PA only marginally improves the boolean subset, suggesting the need for better semantic models in future work. In fact, CONJNLI also provides a test bed for designing good semantic parsers that can automatically distinguish between boolean and non-boolean conjunctions.</p>
<h2>7 Conclusion</h2>
<p>We presented CONJNLI, a new stress-test dataset for NLI in conjunctive sentences ("and", "or", "but", "nor") in the presence of negations and quantifiers and requiring diverse "boolean" and "nonboolean" inferences over conjuncts. Large-scale pre-trained LMs like RoBERTa are not able to optimally understand the conjunctive semantics in our dataset. We presented some initial solutions via adversarial training and a predicate-aware RoBERTa model, and achieved some reasonable performance gains on CONJNLI. However, we also show limitations of our proposed methods, thereby encouraging future work on CONJNLI for better understanding of conjunctive semantics.</p>
<h2>Acknowledgements</h2>
<p>We thank the reviewers for their helpful suggestions and the experts for data annotation. This work was supported by DARPA MCS Grant N66001-19-2-4031, NSF-CAREER Award 1846185, DARPA KAIROS Grant FA8750-19-2-1004, and Munroe \&amp; Rebecca Cobey Fellowship. The views are those of the authors and not the funding agency.</p>
<h2>References</h2>
<p>Rajeev Agarwal and Lois Boggess. 1992. A simple but useful approach to conjunct identification. In Proceedings of the 30th annual meeting on Association for Computational Linguistics, pages 15-21.</p>
<p>Michele Banko, Michael J Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In $I J$ $C A I$, volume 7, pages 2670-2676.</p>
<p>Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages $632-642$.</p>
<p>Xavier Carreras and Lluís Màrquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of the Ninth Conference on Computational Natural Language Learning, CONLL '05.</p>
<p>Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced LSTM for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages $1657-1668$.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Jessica Ficler and Yoav Goldberg. 2016a. Coordination annotation extension in the Penn Tree Bank. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 834-842.</p>
<p>Jessica Ficler and Yoav Goldberg. 2016b. A neural network for coordination boundary prediction. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23-32.</p>
<p>Atticus Geiger, Ignacio Cases, Lauri Karttunen, and Christopher Potts. 2018. Stress-testing neural models of natural language inference with multiply-quantified sentences. arXiv preprint arXiv:1810.13033.</p>
<p>Lila R Gleitman. 1965. Coordinating conjunctions in English. Language, 41(2):260-293.</p>
<p>Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018. Breaking NLI systems with sentences that require simple lexical inferences. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages $650-655$.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112.</p>
<p>Jack Hoeksema. 1988. The semantics of non-boolean "and". Journal of Semantics, 6(1):19-40.</p>
<p>Paloma Jeretic, Alex Warstadt, Suvrat Bhooshan, and Adina Williams. 2020. Are natural language inference models imppressive? learning implicature and presupposition. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Robin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021-2031.</p>
<p>Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. 2020. Learning the difference that makes a difference with counterfactually-augmented data. In International Conference on Learning Representations.</p>
<p>Edward L Keenan and Leonard M Faltz. 2012. Boolean semantics for natural language, volume 23. Springer Science \&amp; Business Media.</p>
<p>Manfred Krifka. 1990. Boolean and non-boolean 'and'. In Papers from the second symposium on Logic and Language, pages 161-188.</p>
<p>Nelson F Liu, Roy Schwartz, and Noah A Smith. 2019a. Inoculation by fine-tuning: A method for analyzing challenge datasets. In NAACL.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics.</p>
<p>Gerald J Massey. 1976. Tom, Dick, and Harry, and all the king's men. American Philosophical Quarterly, 13(2):89-107.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448.</p>
<p>Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. 2018. Stress test evaluation for natural language inference.</p>
<p>In Proceedings of the 27th International Conference on Computational Linguistics, pages 2340-2353.</p>
<p>Yixin Nie, Yicheng Wang, and Mohit Bansal. 2019. Analyzing compositionality-sensitivity of NLI models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6867-6874.</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Hiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto. 2018. A span selection model for semantic role labeling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Deric Pang, Lucy H Lin, and Noah A Smith. 2019. Improving natural language inference with a pretrained parser. arXiv preprint arXiv:1909.08217.</p>
<p>Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. 2018a. Collecting diverse natural language inference problems for sentence representation evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 67-81.</p>
<p>Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018b. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180-191.</p>
<p>Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose, and Eduard Hovy. 2019. EQUATE: A benchmark evaluation framework for quantitative reasoning in natural language inference. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 349-361.</p>
<p>Kyle Richardson, Hai Hu, Lawrence S Moss, and Ashish Sabharwal. 2020. Probing natural language inference models through semantic fragments. In Proceedings of the AAAI Conference on Artificial Intelligence.</p>
<p>Alexis Ross and Ellie Pavlick. 2019. How well do NLI models capture verb veridicality? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).</p>
<p>Swarnadeep Saha and Mausam. 2018. Open information extraction from conjunctive sentences. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2288-2299.</p>
<p>Ivan Sanchez, Jeff Mitchell, and Sebastian Riedel. 2018. Behavior analysis of NLI models: Uncovering the influence of three factors on robustness. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1975-1985, New Orleans, Louisiana.</p>
<p>Peng Shi and Jimmy Lin. 2019. Simple bert models for relation extraction and semantic role labeling. arXiv preprint arXiv:1904.05255.</p>
<p>Hiroki Teranishi, Hiroyuki Shindo, and Yuji Matsumoto. 2019. Decomposed local models for coordinate structure parsing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3394-3403.</p>
<p>Yicheng Wang and Mohit Bansal. 2018. Robust machine comprehension models via adversarial training. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 575-581.</p>
<p>Aaron Steven White, Pushpendre Rastogi, Kevin Duh, and Benjamin Van Durme. 2017. Inference is everything: Recasting semantic resources into a unified evaluation framework. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 996-1005.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.</p>
<p>Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, and Kentaro Inui. 2020. Do neural models learn systematicity of monotonicity inference in natural language? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Xiang Zhou, Yixin Nie, Hao Tan, and Mohit Bansal. 2020. The curse of performance instability in analysis datasets: Consequences, source, and suggestions. In EMNLP.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Premise</th>
<th style="text-align: center;">Hypothesis</th>
<th style="text-align: center;">Label</th>
<th style="text-align: center;">Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">In 870 or 871 he led The Great Summer Army to England.</td>
<td style="text-align: center;">In 871 he led The Great Summer Army to England.</td>
<td style="text-align: center;">neutral</td>
<td style="text-align: center;">The year can be either 870 or 871 , hence we definitely cannot say if it was 871 .</td>
</tr>
<tr>
<td style="text-align: center;">Upon completion, it will rise 64 stories or 711 ft .</td>
<td style="text-align: center;">Upon completion, it will rise 711 ft .</td>
<td style="text-align: center;">entailment</td>
<td style="text-align: center;">64 stories and 711 feet mean the same. "or" is used to establish equivalence between two same things.</td>
</tr>
<tr>
<td style="text-align: center;">During the shootout, Willis Brooks was killed while a fourth man was seriously wounded.</td>
<td style="text-align: center;">During the shootout, Willis Brooks and two others were killed while a fourth man was seriously wounded.</td>
<td style="text-align: center;">neutral</td>
<td style="text-align: center;">Whether two others were killed is unknown.</td>
</tr>
<tr>
<td style="text-align: center;">Gilbert was the freshman football coach of Franklin and Marshall College in 1938.</td>
<td style="text-align: center;">Gilbert was the freshman football coach of Franklin College in 1938.</td>
<td style="text-align: center;">neutral</td>
<td style="text-align: center;">Gilbert can be the coach of two colleges with slightly different names "Franklin and Marshall College" and "Franklin College".</td>
</tr>
<tr>
<td style="text-align: center;">It premiered on 27 June 2016 and airs Mon-Fri 10-11pm IST.</td>
<td style="text-align: center;">It premiered on 28 June 2016 and airs Mon-Fri 1011 pm IST.</td>
<td style="text-align: center;">contradiction</td>
<td style="text-align: center;">If it premiered on 27 June, it cannot premier on 28 June.</td>
</tr>
</tbody>
</table>
<p>Table 10: Some examples from CONJNLI with gold labels and explanations, used for training the annotators.</p>
<h2>A Appendix</h2>
<h2>A. 1 Annotation Examples</h2>
<p>In Table 10, we list a subset of examples from CONJNLI, shown to the annotators with the purpose of training them. In the first example, the "or" means "boolean exclusive-or" while in the second, it is used to establish equivalence between two phrases. The fourth example is a non-boolean usage of "and" as it appears as part of a named entity.</p>
<h2>A. 2 Automated "Or" Adversarial Data Creation</h2>
<p>In this section, we explain the heuristics used to create the "or" part of the automated adversarial training data for the IAFT model in Sec. 5.1. We observe that "or" in English is used in sentences in multiple different contexts - (1) establishing exclusivity between options, translating to "exclusive-or" in boolean semantics ("He was born in 1970 or 1971."), (2) establishing equivalence between two words or phrases ("Upon completion, it will rise 64 stories or 711 ft ."), and (3) "or" interpreted as "boolean or" ("He can play as a striker or a midfielder."). Note that the inference label varies between cases (1) and (2) for a particular conjunct operation. For example, in case (1), removal of a conjunct is neutral while for case (2), removal of a conjunct is entailment. Differentiating between these is again challenging due to the lack of any particular trigger in such sentences. We observe that the latter two cases are more frequent in our
dataset and thus we heuristically label a pair as entailment when we remove a conjunct and neutral when we add a conjunct. Finally, whenever "or" is present in a named entity, we heuristically label the example as neutral.</p>
<h2>A. 3 BERT-SRL Model</h2>
<p>SRL is the task of predicting the semantic roles for each predicate in the sentence. We follow the standard BIO encoding to denote the span of each argument and model it as a token classification problem. The input to BERT is the tokenized sentence with a $[C L S]$ token at the beginning and a $[S E P]$ token at the end.</p>
<p>$$
[C L S] \text { Sent }[S E P][S E P] \text { Pred }[S E P]
$$</p>
<p>Since BERT converts each word into word pieces, we first propagate the gold SRL tags, which are for each word, to word pieces. Thus, for a word with multiple word pieces, we assign the tag BARG to the first word piece, and the tag I-ARG to the subsequent word pieces. For tags of the form I-ARG and O, we assign the original word tag to all the word pieces. The $[C L S]$ and the $[S E P]$ tokens of the input are assigned the tag O. Predicate information is fed to BERT through the segment ids, by assigning 1 to the predicate token and 0 for others. Finally, we add a classification layer at the top and the model is fine-tuned to predict the tag for each token using cross-entropy loss. Unlike Shi and Lin (2019), our model refrains from using additional LSTM layers at the top, thus making it</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Premise</th>
<th style="text-align: center;">Hypothesis</th>
<th style="text-align: center;">RoBERTa</th>
<th style="text-align: center;">PA</th>
<th style="text-align: center;">IAFT</th>
<th style="text-align: center;">PA-IAFT</th>
<th style="text-align: center;">Gold</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">India measures 3214 km from north to south and 2933 km from east to west.</td>
<td style="text-align: center;">India measures 3214 km from north to south and 2934 km from east to west.</td>
<td style="text-align: center;">ent</td>
<td style="text-align: center;">contra</td>
<td style="text-align: center;">contra</td>
<td style="text-align: center;">contra</td>
<td style="text-align: center;">contra</td>
</tr>
<tr>
<td style="text-align: center;">He appeared alongside Anthony Hopkins in the 1972 Television series War and Peace.</td>
<td style="text-align: center;">He appeared alongside Anthony Hopkins in the 1972 Television series War.</td>
<td style="text-align: center;">contra</td>
<td style="text-align: center;">contra</td>
<td style="text-align: center;">neu</td>
<td style="text-align: center;">neu</td>
<td style="text-align: center;">neu</td>
</tr>
<tr>
<td style="text-align: center;">It was released in January 2000 as the lead single from their album "Here and Now".</td>
<td style="text-align: center;">It was released in January 2000 as the lead single from their album "Now".</td>
<td style="text-align: center;">ent</td>
<td style="text-align: center;">ent</td>
<td style="text-align: center;">neu</td>
<td style="text-align: center;">contra</td>
<td style="text-align: center;">contra</td>
</tr>
<tr>
<td style="text-align: center;">3,000 people died in the days following the earthquakes due to injuries and disease.</td>
<td style="text-align: center;">3,000 people died in the days following the earthquakes due to disease.</td>
<td style="text-align: center;">ent</td>
<td style="text-align: center;">ent</td>
<td style="text-align: center;">ent</td>
<td style="text-align: center;">ent</td>
<td style="text-align: center;">contra</td>
</tr>
</tbody>
</table>
<p>Table 11: Examples from CONJNLI showing where each model is good at and what is still challenging for all.
consistent with other downstream fine-tuning tasks. At inference time, we predict the most probable tag sequence by applying Viterbi Algorithm with two constraints - (1) a sequence cannot start with an IARG tag and (2) an I-ARG tag has to be preceded by a B-ARG tag. The tag for each word is taken to be the predicted tag of the first word piece of that word. We train the SRL model on the CoNLL 2005 dataset (Carreras and Màrquez, 2005) for 5 epochs with a learning rate of $5 * 10^{-} 5$ and obtain a near state-of-the-art F1 of $86.23 \%$ on the dev set. ${ }^{7}$ The BERT-SRL model's weights are frozen for the NLI task.</p>
<h2>A. 4 Experimental Setup</h2>
<p>Our implementation builds on top of the Pytorch implementations (Wolf et al., 2019) of BERT-base uncased (Devlin et al., 2019) and RoBERTa-base (Liu et al., 2019b). We train the models for a maximum of 3 epochs using an initial learning rate of $2 * 10^{-5}$, with linear decay and a weight decay of 0.1 . The dropout probability is chosen to be 0.1 . The size of the predicate-aware representations of the premise and the hypothesis is set to 40 . The maximum sequence length is 128 for both the NLI models and the SRL model. The random seed used in all the experiments is 42 . Each epoch takes 45 minutes (base models) to 1.5 hours (predicateaware models) to run on four V100 GPUs. The total number of parameters in our models is similar to that of BERT-base or RoBERTa-base, depending on the choice of the model. All hyperparameters, the amount of adversarial training and the adversarial training algorithm are chosen based on the best average accuracy of CONJNLI and MNLI dev</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>sets. Batch size and learning rate are manually tuned in the range ${16,32}$ and $\left{10^{-5}, 2 * 10^{-5}\right}$ respectively. Following previous works, we report accuracy for all our models.</p>
<h2>A. 5 Success and Error Analysis</h2>
<p>In Table 11, we present four examples from CONJNLI and the predictions from each of the models and the target label. In the first example, predicateaware RoBERTa understands the roles of the predicate "measures" and outputs the correct label contradiction, which RoBERTa cannot. The second example shows the effect of adversarial training the model learns from adversarial examples that "War and Peace" is a named entity and removing a conjunct from it is neutral. ${ }^{8}$ The third example is however, an exception to the previous rule and only adversarial training fails to make the distinction. On combining predicate-aware RoBERTa with adversarial training, our model learns to associate the predicate "released" to its roles and predicts the correct label. Finally, for the fourth example, we find that our model still fails to make the difficult inferences over "non-boolean and" cases. Note that there is no obvious trigger like "total" in the sentence, thus causing all the models to fail. This also shows that we need a deeper understanding of conjunctive sentential semantics to correctly predict these tricky real-world cases where boolean semantics do not hold.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ The state-of-the-art F1 score for SRL on this dataset is $87.4 \%$ (Ouchi et al., 2018)&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{8}$ Note that the gold label is neutral because a person can appear in two Television series with slightly different names.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6528 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6528</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6528</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-275458725</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.05752v2.pdf" target="_blank">Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer from computational inefficiency and redundancy. First, they overlook the diversity of task difficulties, leading to unnecessarily extensive searches even for easy tasks. Second, they neglect the semantics of reasoning paths, resulting in redundant exploration of semantically identical paths. To address these limitations, we propose Semantic Exploration with Adaptive Gating (SEAG), a computationally efficient method. SEAG employs an adaptive gating mechanism that dynamically decides whether to conduct a tree search, based on the confidence level of answers from a preceding simple reasoning method. Furthermore, its tree-based exploration consolidates semantically identical reasoning steps, reducing redundant explorations while maintaining or even improving accuracy. Our extensive experiments demonstrate that SEAG significantly improves accuracy by 4.3% on average while requiring only 31% of computational costs compared to existing tree search-based methods on complex reasoning benchmarks including GSM8K and ARC with diverse language models such as Llama2, Llama3, and Mistral. Our code is available at https://github.com/ml-postech/SEAG-semantic-exploration-with-adaptive-gating .</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6528.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6528.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SEAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Exploration with Adaptive Gating</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hybrid reasoning framework that first uses an entropy-based adaptive gate to decide whether to accept majority-vote single-path reasoning or perform a tree-based semantic exploration that clusters semantically equivalent actions and uses weighted aggregation with early stopping.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct, Llama2-13B-Chat, Mistral-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B, 13B, 7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Adaptive Gating + Semantic Exploration (SEAG)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>hybrid (adaptive gating + tree-search)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and ARC</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: multi-step math word problems (grade-school arithmetic); ARC: multiple-choice science questions (grade-school science reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>86.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>RAP (tree-search baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>4.3</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>SEAG adaptively avoids expensive tree search for low-uncertainty (low-entropy) problems and invokes tree-based semantic exploration for high-uncertainty problems; semantic clustering reduces exploration of semantically redundant paths and weighted aggregation prioritizes frequently supported semantic clusters, yielding higher accuracy with much lower compute. The paper reports SEAG achieves a mean +4.3% accuracy vs RAP while using ≈31% of RAP's computational cost (average across evaluated settings).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6528.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6528.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SE (Semantic Exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Exploration (tree-based with semantic clustering and semantic PUCT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tree-search reasoning that groups semantically equivalent candidate actions into clusters, defines cluster priors by aggregated LM probability, and applies a PUCT-like selection at the cluster level to avoid redundant expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Semantic Exploration (SE)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (explores multiple semantic clusters but consolidates semantically equivalent variants)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school arithmetic word problems requiring multi-step numerical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>85.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>RAP (tree-search baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>2.5</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>By clustering semantically-equivalent actions, SE reduces the number of unique expanded nodes (25-45% reduction at depth 1 to 50-65% at depth 4) and achieves higher accuracy than other tree-search methods while lowering inference counts; semantic PUCT biases exploration toward clusters with higher aggregate LM probability (self-consistency principle).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6528.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6528.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive Gating (AG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Gating (entropy-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Entropy-based gating mechanism that computes the entropy over k CoT-SC sampled answers and skips tree search when entropy is below a threshold τ, otherwise triggers semantic exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Entropy-based Adaptive Gating (AG)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>decision/gating (meta-controller)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed (decides between single-path and multi-path exploration based on uncertainty)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and ARC</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step reasoning; decides whether to accept CoT-SC majority vote or run tree search</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>percent of instances gated to single-path CoT-SC (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>67.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>no gating (always run tree search)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>AG reduces computation by terminating early for low-entropy (high-confidence) problems — the paper reports SEAG terminates after CoT-SC in ~67% of cases, contributing to much lower average latency and inference counts. The paper shows CoT-SC works well on low-entropy problems while tree-based methods help high-entropy problems.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6528.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6528.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic PUCT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic PUCT (PUCT at semantic-cluster level)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PUCT-style selection algorithm applied to semantic clusters rather than raw actions; cluster prior π(C|s) is the sum of action probabilities in the cluster, and selection encourages exploration of semantically probable clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Semantic PUCT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search selection heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed (encourages exploration across semantically distinct clusters)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step math reasoning with tree-search</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>85.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>UCT / PUCT without semantic clustering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Ablation shows semantic PUCT outperforms UCT and vanilla PUCT in accuracy while keeping comparable efficiency by aggregating priors at the cluster level and focusing exploration on semantically-supported clusters.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6528.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6528.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Weighted aggregation (early stopping)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Early stopping with weighted aggregation of rewards</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregates rewards for terminal answers by weighting node rewards by the size of their semantic clusters and applies an early-stopping threshold α to terminate search when aggregated reward for an answer exceeds α.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Weighted aggregation + early stopping</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search stopping/aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed (weights reflect frequency/support of semantically similar paths)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step math reasoning (deciding when to stop search)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>86.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>equal-weight aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Weighted aggregation (weight |C(n)|) gives more influence to nodes in larger semantic clusters; ablation shows weighted aggregation yields higher accuracy and lower LLM compute than equal weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6528.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6528.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Single-path prompting that elicits step-by-step intermediate reasoning tokens from an LM to improve multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>76.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT-SC / tree-search methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CoT is effective for many problems but is a single greedy path; authors note CoT-SC (self-consistency) improves over single-path CoT by sampling multiple paths and majority voting, and CoT-SC works especially well for low-entropy problems.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6528.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6528.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ensemble approach that samples multiple CoT reasoning paths and picks the most frequent final answer (majority voting) to increase robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>CoT-SC (self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble (sampling + majority vote)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (multiple sampled reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step math reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>84.5</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT, tree-search methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CoT-SC yields large gains on low-entropy instances; the paper uses CoT-SC as the default 'simple' method feeding the AG gate (k=10 samples). Entropy over CoT-SC outputs is the gating signal.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6528.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6528.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tree-structured extension of CoT where intermediate reasoning states are nodes in a tree and search strategies (BFS/DFS) explore multiple reasoning trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step arithmetic reasoning with tree exploration</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>78.5</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT-SC and other tree-search methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>ToT is a baseline tree-search method; compared against SE which adds semantic clustering and semantic PUCT to reduce redundancy and improve accuracy/efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6528.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6528.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reasoning-via-Planning (RAP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MCTS-based reasoning approach that frames LM reasoning as planning with LM acting as both agent and world model; used as a strong tree-search baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>RAP (MCTS-based planning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search (MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step numerical reasoning (MCTS-driven exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>82.5</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>SE / SEAG</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>RAP is the closest baseline; authors report SEAG improves accuracy vs RAP while dramatically reducing inference costs (SEAG uses ~31% of RAP's inferences on average). RAP and other tree-search methods help on high-entropy problems but are computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6528.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6528.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregate SEAG vs RAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reported aggregate comparison of SEAG and existing tree-search baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper-reported aggregate improvement and compute reduction: SEAG improves average accuracy by 4.3% while requiring only 31% of the computational costs compared to existing tree-search based methods (RAP/ToT) across tested benchmarks and models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B-Instruct, Llama2-13B-Chat, Mistral-7B-Instruct (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B, 13B, 7B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>SEAG (aggregate claim)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>hybrid (adaptive gating + tree-search)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and ARC (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step reasoning across arithmetic (GSM8K) and science multiple-choice (ARC)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy improvement (%) / relative compute (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>4.3</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>RAP (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>4.3</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors provide an aggregate statement: SEAG yields +4.3% accuracy on average while using only 31% of the inference cost of tree-search baselines; significance tests are not reported. The improvement is attributed to adaptive gating (avoids unnecessary search), semantic clustering (removes redundant expansions), semantic PUCT (prioritizes informative clusters), and early stopping with weighted aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Reasoning with language model is planning with world model <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation <em>(Rating: 2)</em></li>
                <li>Detecting hallucinations in large language models using semantic entropy <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6528",
    "paper_id": "paper-275458725",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "SEAG",
            "name_full": "Semantic Exploration with Adaptive Gating",
            "brief_description": "Hybrid reasoning framework that first uses an entropy-based adaptive gate to decide whether to accept majority-vote single-path reasoning or perform a tree-based semantic exploration that clusters semantically equivalent actions and uses weighted aggregation with early stopping.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct, Llama2-13B-Chat, Mistral-7B-Instruct",
            "model_size": "8B, 13B, 7B",
            "reasoning_method_name": "Adaptive Gating + Semantic Exploration (SEAG)",
            "reasoning_method_type": "hybrid (adaptive gating + tree-search)",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "GSM8K and ARC",
            "task_description": "GSM8K: multi-step math word problems (grade-school arithmetic); ARC: multiple-choice science questions (grade-school science reasoning).",
            "performance_metric": "accuracy (%)",
            "performance_value": 86.0,
            "comparison_target_method": "RAP (tree-search baseline)",
            "performance_difference": 4.3,
            "statistical_significance": null,
            "analysis_notes": "SEAG adaptively avoids expensive tree search for low-uncertainty (low-entropy) problems and invokes tree-based semantic exploration for high-uncertainty problems; semantic clustering reduces exploration of semantically redundant paths and weighted aggregation prioritizes frequently supported semantic clusters, yielding higher accuracy with much lower compute. The paper reports SEAG achieves a mean +4.3% accuracy vs RAP while using ≈31% of RAP's computational cost (average across evaluated settings).",
            "ablation_study_present": true,
            "uuid": "e6528.0",
            "source_info": {
                "paper_title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "SE (Semantic Exploration)",
            "name_full": "Semantic Exploration (tree-based with semantic clustering and semantic PUCT)",
            "brief_description": "Tree-search reasoning that groups semantically equivalent candidate actions into clusters, defines cluster priors by aggregated LM probability, and applies a PUCT-like selection at the cluster level to avoid redundant expansion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct",
            "model_size": "8B",
            "reasoning_method_name": "Semantic Exploration (SE)",
            "reasoning_method_type": "tree-search",
            "reasoning_style_diversity": "diverse (explores multiple semantic clusters but consolidates semantically equivalent variants)",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school arithmetic word problems requiring multi-step numerical reasoning",
            "performance_metric": "accuracy (%)",
            "performance_value": 85.0,
            "comparison_target_method": "RAP (tree-search baseline)",
            "performance_difference": 2.5,
            "statistical_significance": null,
            "analysis_notes": "By clustering semantically-equivalent actions, SE reduces the number of unique expanded nodes (25-45% reduction at depth 1 to 50-65% at depth 4) and achieves higher accuracy than other tree-search methods while lowering inference counts; semantic PUCT biases exploration toward clusters with higher aggregate LM probability (self-consistency principle).",
            "ablation_study_present": true,
            "uuid": "e6528.1",
            "source_info": {
                "paper_title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Adaptive Gating (AG)",
            "name_full": "Adaptive Gating (entropy-based)",
            "brief_description": "Entropy-based gating mechanism that computes the entropy over k CoT-SC sampled answers and skips tree search when entropy is below a threshold τ, otherwise triggers semantic exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct",
            "model_size": "8B",
            "reasoning_method_name": "Entropy-based Adaptive Gating (AG)",
            "reasoning_method_type": "decision/gating (meta-controller)",
            "reasoning_style_diversity": "mixed (decides between single-path and multi-path exploration based on uncertainty)",
            "benchmark_name": "GSM8K and ARC",
            "task_description": "multi-step reasoning; decides whether to accept CoT-SC majority vote or run tree search",
            "performance_metric": "percent of instances gated to single-path CoT-SC (%)",
            "performance_value": 67.0,
            "comparison_target_method": "no gating (always run tree search)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "AG reduces computation by terminating early for low-entropy (high-confidence) problems — the paper reports SEAG terminates after CoT-SC in ~67% of cases, contributing to much lower average latency and inference counts. The paper shows CoT-SC works well on low-entropy problems while tree-based methods help high-entropy problems.",
            "ablation_study_present": true,
            "uuid": "e6528.2",
            "source_info": {
                "paper_title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Semantic PUCT",
            "name_full": "Semantic PUCT (PUCT at semantic-cluster level)",
            "brief_description": "PUCT-style selection algorithm applied to semantic clusters rather than raw actions; cluster prior π(C|s) is the sum of action probabilities in the cluster, and selection encourages exploration of semantically probable clusters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct",
            "model_size": "8B",
            "reasoning_method_name": "Semantic PUCT",
            "reasoning_method_type": "tree-search selection heuristic",
            "reasoning_style_diversity": "mixed (encourages exploration across semantically distinct clusters)",
            "benchmark_name": "GSM8K",
            "task_description": "multi-step math reasoning with tree-search",
            "performance_metric": "accuracy (%)",
            "performance_value": 85.0,
            "comparison_target_method": "UCT / PUCT without semantic clustering",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Ablation shows semantic PUCT outperforms UCT and vanilla PUCT in accuracy while keeping comparable efficiency by aggregating priors at the cluster level and focusing exploration on semantically-supported clusters.",
            "ablation_study_present": true,
            "uuid": "e6528.3",
            "source_info": {
                "paper_title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Weighted aggregation (early stopping)",
            "name_full": "Early stopping with weighted aggregation of rewards",
            "brief_description": "Aggregates rewards for terminal answers by weighting node rewards by the size of their semantic clusters and applies an early-stopping threshold α to terminate search when aggregated reward for an answer exceeds α.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct",
            "model_size": "8B",
            "reasoning_method_name": "Weighted aggregation + early stopping",
            "reasoning_method_type": "tree-search stopping/aggregation",
            "reasoning_style_diversity": "mixed (weights reflect frequency/support of semantically similar paths)",
            "benchmark_name": "GSM8K",
            "task_description": "multi-step math reasoning (deciding when to stop search)",
            "performance_metric": "accuracy (%)",
            "performance_value": 86.0,
            "comparison_target_method": "equal-weight aggregation",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Weighted aggregation (weight |C(n)|) gives more influence to nodes in larger semantic clusters; ablation shows weighted aggregation yields higher accuracy and lower LLM compute than equal weighting.",
            "ablation_study_present": true,
            "uuid": "e6528.4",
            "source_info": {
                "paper_title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "Single-path prompting that elicits step-by-step intermediate reasoning tokens from an LM to improve multi-step problem solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct",
            "model_size": "8B",
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school arithmetic word problems",
            "performance_metric": "accuracy (%)",
            "performance_value": 76.2,
            "comparison_target_method": "CoT-SC / tree-search methods",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "CoT is effective for many problems but is a single greedy path; authors note CoT-SC (self-consistency) improves over single-path CoT by sampling multiple paths and majority voting, and CoT-SC works especially well for low-entropy problems.",
            "ablation_study_present": false,
            "uuid": "e6528.5",
            "source_info": {
                "paper_title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "CoT-SC",
            "name_full": "Chain-of-Thought Self-Consistency",
            "brief_description": "Ensemble approach that samples multiple CoT reasoning paths and picks the most frequent final answer (majority voting) to increase robustness.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct",
            "model_size": "8B",
            "reasoning_method_name": "CoT-SC (self-consistency)",
            "reasoning_method_type": "ensemble (sampling + majority vote)",
            "reasoning_style_diversity": "diverse (multiple sampled reasoning paths)",
            "benchmark_name": "GSM8K",
            "task_description": "multi-step math reasoning",
            "performance_metric": "accuracy (%)",
            "performance_value": 84.5,
            "comparison_target_method": "CoT, tree-search methods",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "CoT-SC yields large gains on low-entropy instances; the paper uses CoT-SC as the default 'simple' method feeding the AG gate (k=10 samples). Entropy over CoT-SC outputs is the gating signal.",
            "ablation_study_present": false,
            "uuid": "e6528.6",
            "source_info": {
                "paper_title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree-of-Thoughts (ToT)",
            "brief_description": "Tree-structured extension of CoT where intermediate reasoning states are nodes in a tree and search strategies (BFS/DFS) explore multiple reasoning trajectories.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct",
            "model_size": "8B",
            "reasoning_method_name": "Tree-of-Thoughts (ToT)",
            "reasoning_method_type": "tree-search",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "GSM8K",
            "task_description": "multi-step arithmetic reasoning with tree exploration",
            "performance_metric": "accuracy (%)",
            "performance_value": 78.5,
            "comparison_target_method": "CoT-SC and other tree-search methods",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "ToT is a baseline tree-search method; compared against SE which adds semantic clustering and semantic PUCT to reduce redundancy and improve accuracy/efficiency.",
            "ablation_study_present": false,
            "uuid": "e6528.7",
            "source_info": {
                "paper_title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "RAP",
            "name_full": "Reasoning-via-Planning (RAP)",
            "brief_description": "MCTS-based reasoning approach that frames LM reasoning as planning with LM acting as both agent and world model; used as a strong tree-search baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct",
            "model_size": "8B",
            "reasoning_method_name": "RAP (MCTS-based planning)",
            "reasoning_method_type": "tree-search (MCTS)",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "GSM8K",
            "task_description": "multi-step numerical reasoning (MCTS-driven exploration)",
            "performance_metric": "accuracy (%)",
            "performance_value": 82.5,
            "comparison_target_method": "SE / SEAG",
            "performance_difference": -3.5,
            "statistical_significance": null,
            "analysis_notes": "RAP is the closest baseline; authors report SEAG improves accuracy vs RAP while dramatically reducing inference costs (SEAG uses ~31% of RAP's inferences on average). RAP and other tree-search methods help on high-entropy problems but are computationally expensive.",
            "ablation_study_present": false,
            "uuid": "e6528.8",
            "source_info": {
                "paper_title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Aggregate SEAG vs RAP",
            "name_full": "Reported aggregate comparison of SEAG and existing tree-search baselines",
            "brief_description": "Paper-reported aggregate improvement and compute reduction: SEAG improves average accuracy by 4.3% while requiring only 31% of the computational costs compared to existing tree-search based methods (RAP/ToT) across tested benchmarks and models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B-Instruct, Llama2-13B-Chat, Mistral-7B-Instruct (aggregate)",
            "model_size": "8B, 13B, 7B",
            "reasoning_method_name": "SEAG (aggregate claim)",
            "reasoning_method_type": "hybrid (adaptive gating + tree-search)",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "GSM8K and ARC (aggregate)",
            "task_description": "multi-step reasoning across arithmetic (GSM8K) and science multiple-choice (ARC)",
            "performance_metric": "accuracy improvement (%) / relative compute (%)",
            "performance_value": 4.3,
            "comparison_target_method": "RAP (aggregate)",
            "performance_difference": 4.3,
            "statistical_significance": null,
            "analysis_notes": "Authors provide an aggregate statement: SEAG yields +4.3% accuracy on average while using only 31% of the inference cost of tree-search baselines; significance tests are not reported. The improvement is attributed to adaptive gating (avoids unnecessary search), semantic clustering (removes redundant expansions), semantic PUCT (prioritizes informative clusters), and early stopping with weighted aggregation.",
            "ablation_study_present": true,
            "uuid": "e6528.9",
            "source_info": {
                "paper_title": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Reasoning with language model is planning with world model",
            "rating": 2,
            "sanitized_title": "reasoning_with_language_model_is_planning_with_world_model"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation",
            "rating": 2,
            "sanitized_title": "semantic_uncertainty_linguistic_invariances_for_uncertainty_estimation_in_natural_language_generation"
        },
        {
            "paper_title": "Detecting hallucinations in large language models using semantic entropy",
            "rating": 1,
            "sanitized_title": "detecting_hallucinations_in_large_language_models_using_semantic_entropy"
        }
    ],
    "cost": 0.016127,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models
9 Jun 2025</p>
<p>Sungjae Lee sungjaelee25@postech.ac.kr 
Department of Computer Science and Engineering
POSTECHSouth Korea</p>
<p>Hyejin Park 
Graduate School of Artificial Intelligence
POSTECHSouth Korea</p>
<p>Jaechang Kim jaechang@postech.ac.kr 
Graduate School of Artificial Intelligence
POSTECHSouth Korea</p>
<p>Jungseul Ok jungseul@postech.ac.kr 
Department of Computer Science and Engineering
POSTECHSouth Korea</p>
<p>Graduate School of Artificial Intelligence
POSTECHSouth Korea</p>
<p>Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models
9 Jun 2025F6D7ED787EC23881FA7852C0874036D6arXiv:2501.05752v2[cs.AI]
Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths.However, existing methods often suffer from computational inefficiency and redundancy.First, they overlook the diversity of task difficulties, leading to unnecessarily extensive searches even for easy tasks.Second, they neglect the semantics of reasoning paths, resulting in redundant exploration of semantically identical paths.To address these limitations, we propose Semantic Exploration with Adaptive Gating (SEAG), a computationally efficient method.SEAG employs an adaptive gating mechanism that dynamically decides whether to conduct a tree search, based on the confidence level of answers from a preceding simple reasoning method.Furthermore, its tree-based exploration consolidates semantically identical reasoning steps, reducing redundant explorations while maintaining or even improving accuracy.Our extensive experiments demonstrate that SEAG significantly improves accuracy by 4.3% on average while requiring only 31% of computational costs compared to existing tree search-based methods on complex reasoning benchmarks including GSM8K and ARC with diverse language models such as Llama2, Llama3, and Mistral.Our code is available at https://github.com/ml-postech/SEAGsemantic-exploration-with-adaptive-gating.</p>
<p>Introduction</p>
<p>Recent advances in Large Language Models (LLMs) (Brown et al., 2020;Chowdhery et al., 2023;Team et al., 2023;Touvron et al., 2023;Achiam et al., 2023) have demonstrated remarkable potentials in a wide range of complex reasoning tasks including mathematical problemsolving (Lewkowycz et al., 2022;Wu et al., 2022; * Equal Contribution † Corresponding Author Mishra et al., 2022), knowledge application (Zhang et al., 2018;Yavuz et al., 2022), and commonsense reasoning (Patel et al., 2021;Sanh et al., 2022;Madaan et al., 2022).LLMs have made notable strides in complex reasoning tasks (Nye et al., 2021;Gao et al., 2023), yet they face significant challenges in multi-step reasoning.</p>
<p>Building on a single-path prompting, such as Chain-of-Thought (CoT) and its variants (Wei et al., 2022;Wang et al., 2023b;Kojima et al., 2022), recent research has investigated tree search-based methods (Yao et al., 2023;Long, 2023;Hao et al., 2023) for complex tasks that demand exploration in reasoning.However, despite their effectiveness, tree search-based methods often face significant computational inefficiencies caused by two primary factors.First, tree search-based methods are often used for tasks that do not require such complex exploration.However, a key challenge lies in determining when to use the single-path method versus tree-based exploration.Second, the search process repeatedly expands and explores semantically redundant reasoning paths.Addressing the redundancy can be challenging, as it involves identifying when different natural language expressions convey the same meaning (Kuhn et al., 2023;Farquhar et al., 2024), especially in open-ended reasoning.</p>
<p>To address these limitations, we propose Semantic Exploration with Adaptive Gating (SEAG), a novel framework to significantly enhance computational efficiency in complex reasoning tasks.As illustrated in Figure 1, SEAG operates in three key phases: adaptive gating (AG), semantic exploration (SE), and early stopping.The first phase, AG (Section 4.1) evaluates the confidence of answers produced by simpler reasoning methods, such as CoT-SC.Based on this confidence, AG determines whether further tree-based exploration is needed, ensuring efficient use of computational resources.Building on this, the second phase, SE (Section 4.2) groups similar reasoning steps us-  4) of the generated answers obtained through multiple single-path reasoning.If the uncertainty is high, semantic exploration employs tree search to explore multiple reasoning paths, where actions are grouped into semantic clusters (further detailed in Figure 3).Lastly, early stopping terminates the search when the highest aggregated reward surpasses a predefined threshold.</p>
<p>ing semantic clustering (Kuhn et al., 2023;Farquhar et al., 2024), avoiding repeated exploration of semantically equivalent paths.This approach effectively handles instances where different expressions convey the same meaning, as illustrated in Figure 3. Additionally, SE prioritizes exploration of semantic clusters containing more instances to derive solutions with higher potentials.In the final phase, early stopping (Section 4.3) terminates the tree search once a highly confident solution is found, avoiding unnecessary computations.SEAG aggregates results by assigning higher importance to reasoning paths that are more semantically relevant or frequently supported.</p>
<p>Our main contributions are as follows: • We propose semantic exploration, leveraging semantic clustering to reduce redundant computations and prioritize exploration of semantically informative reasoning paths (Section 4.2).</p>
<p>• We introduce a unified approach combining adaptive gating (Section 4.1) and early stopping (Section 4.3) to adaptively determine when to initiate and terminate tree-based exploration, efficiently utilizing computational resources based on confidence measures specific to each process.</p>
<p>• Through extensive experiments on multi-step reasoning benchmarks, SEAG consistently demonstrates superior performance in reasoning accuracy while achieving computational efficiency comparable to or greater than baselines (Section 5.2), as shown in Figure 2.</p>
<p>Related Work</p>
<p>Reasoning with LLMs Eliciting the reasoning capabilities of LLMs has been a key focus of recent research, leading to various reasoning methods to improve their performance on complex, multi-step tasks.One notable approach is Chain-of-Thought (CoT) prompting (Wei et al., 2022), which encourages LLMs to generate intermediate reasoning steps.Self-consistency of CoT (CoT-SC) (Wang et al., 2023b) extends CoT by sampling multiple reasoning paths and selecting the most frequently answers, instead of relying on a single greedy decoding pass.</p>
<p>Tree search-based approaches, such as Tree-of-Thoughts (ToT) (Yao et al., 2023;Long, 2023) and Reasoning-via-Planning (RAP) (Hao et al., 2023), enhance reasoning by structuring the exploration of reasoning paths on trees using search algorithms such as breadth/depth-first search (BFS/DFS) and Monte Carlo Tree Search (MCTS).In addition, recent studies on MCTS-based reasoning have extended the settings involving additional training costs (Wan et al., 2024;Zhang et al., 2024a) or external feedback (Zhou et al., 2024).However, these tree search-based methods incur considerably higher computation costs compared to simpler methods such as CoT-SC.</p>
<p>Linguistic semantics Measuring semantic relation is essential for reducing semantic redundancy in LLM reasoning.Traditional approaches have relied on lexical feature-based (Fernando and Stevenson, 2008;Socher et al., 2011) or embedding-based similarity metrics (Yu et al., 2014;Issa et al., 2018).However, these methods often lack robustness in capturing deeper liguistic semantics.With the rise of Transformer-based models and LLMs, semantic evaluation has achieved significant improvements (He et al., 2020b;Tay et al., 2021).Re-  cently, Kuhn et al. (2023) has introduced a textual entailment-based method for assessing semantic equivalence, which informs our approach to semantic clustering.Similar to our work, Jang et al. (2021) has utilized semantic similarity within a MCTS framework for text-based games.However, their method relied on a predefined set consisting of valid actions provided by the game environment.In contrast, we address open domain problems, where actions, such as task-specific sub-questions, are dynamically generated by prompting LLMs.</p>
<p>Uncertainty estimation Uncertainty estimation of LLMs has emerged as a critical area of study for evaluating and improving their reliability.Existing uncertainty estimation methods often rely on the consistency of sampling results (Wang et al., 2023a;Cole et al., 2023) or responses to semantically equivalent questions (Zhang et al., 2024b).These approaches share an assumption that uncertainty can be quantified through entropy of prediction, and the assumption is commonly used in traditional machine learning (Malinin and Gales, 2018).We integrate uncertainty estimation directly into the reasoning pipeline, whereas other methods typically treat it as a separate step.</p>
<p>Preliminaries</p>
<p>In this section, we first define our problem setting as Markov Decision Process (MDP) reasoning in Section 3.1.Section 3.2 explains Monte Carlo Tree Search (MCTS) for multi-step reasoning.</p>
<p>Markov Decision Process Reasoning</p>
<p>Let p θ denote a pre-trained language model (LM) parameterized by θ, and an input sequence x = (x 1 , . . ., x lx ), where l x represents the token lengths of the input.The model's probability of generating x is expressed by p θ (x) = lx i=1 (x i |x &lt;i ), where x &lt;i represents the sequence of tokens preceding x i .An output sequence y = (y 1 , . . ., y ly ) is generated by auto-regressively, where l y denotes the token lengths of the output.The previously generated tokens are used to predict the next token as p θ (y|x) = ly i=1 p θ (y i |x, y &lt;i ).</p>
<p>Rather than directly mapping the input x to the output y, several reasoning methods have been developed to enhance reasoning by breaking down complex tasks into step-by-step thoughts, with each generated token y i representing an intermediate reasoning step.Detailed explanations of these methods are provided in Appendix A. Following the approach in RAP (Hao et al., 2023), we define our problem as an MDP to effectively model the reasoning task.We leverages LMs in two complementary roles: (i) a reasoning agent and (ii) a world model.This approach frames the reasoning process as an MDP, enabling iterative reasoning through planning and simulation.At each time step t, the state s t represents the current context, including both the input sequence and the reasoning history.In the first role, the LM acts as the reasoning agent, generating actions a t ∼ p θ (a|s t , m) based on the current state s t and a task-specific instruction m.Subsequently, the LM functions as a world model, predicting the next state s t+1 based on the current state and the chosen action, i.e., p θ (s t+1 |s t , a t , m ′ ), where m ′ is an additional prompt guiding the transition process.Each reasoning step is evaluated using a reward function r t = r(s t , a t ) ∈ R, based on the LM's self-evaluation of the generated action and the log probability of the action with the history of states.Further details of the reward design are explained in Appendix B.</p>
<p>Monte Carlo Tree Search (MCTS)</p>
<p>To enhance strategic exploration, we incorporate planning with MCTS (Coulom, 2006), which is particularly effective for decision-making in highdimensional spaces.MCTS builds a search tree over k iterations to explore decision space through four main operators: selection, expansion, simulation, and back-propagation.At each node, which represents a state s, the standard MCTS uses UCT (Kocsis and Szepesvári, 2006;Auer et al., 2002) algorithm to select the optimal action a * based on Q-value as follows:
a * = arg max a∈A(s) Q(s, a) + w ln N (s) N (s, a) ,(1)
where N (•) denotes the total number of visits in previous iterations, A(s) is the set of possible actions at state s, and w is the constant of balancing exploration and exploitation.PUCT (Silver et al., 2016(Silver et al., , 2017) ) provides a viable alternative to UCT by incorporating π(a|s), a predictor of the prior action distribution, into its formulation as follows:
a * = arg max a∈A(s) Q(s, a)+w•π(a|s) N (s) N (s, a) + 1 .
(2)</p>
<p>The predictor π(a|s) can be defined as the sequence probability p θ (a|s, m) in the context of LLMs.A more detailed description of the operators used in MCTS can be found in Appendix C.</p>
<p>Method</p>
<p>Our method consists of adaptive gating (Section 4.1), semantic exploration (Section 4.2), and early stopping with weighted aggregation (Section 4.3), as shown in Figure 1.A detailed description of the algorithm is provided in Appendix D.</p>
<p>Adaptive Gating (AG)</p>
<p>AG adaptively determines when to expand the search tree based on the confidence of generated answers.Specifically, k answers are sampled using a single-path method, such as CoT, and the confidence of these answers is used to determine whether to initiate tree search.The key insight is that not all problems require the same level of complexity in reasoning.First, we generate k reasoning paths using CoT, each producing an output y i ∼ p θ (y|x, z i ≤l ), ∀i ∈</p>
<p>[k], i.e., corresponding to CoT-SC.Let Y denote the set of possible candidate outputs across k reasoning paths.We define q(y) as the estimated probability of output y ∈ Y, computed as:
q(y) = 1 k k i=1 I(y = y i ) , (3)
where I is the indicator function.Notice that semantical clustering in Section 4.2 can be used when calculating q(y), while the indicator function handles cases where the final answer is numeric or discrete.We use entropy H(y) as a gating function across the k reasoning paths, calculated as:
H(y) = − y∈Y q(y) log q(y) ,(4)
which reflects the uncertainty in the model's reasoning.Based on this, if H(y) is smaller than a predefined threshold τ indicating high confidence, the final answer y * is determined through majority voting as follows:
y * = arg max y∈Y k i=1 I(y = y i ), if H(y) ≤ τ .(5)
Otherwise, if H(y) &gt; τ , indicating lower confidence, we proceed by constructing a search tree and employing our proposed method, Semantic Exploration, as described in the following section.An analysis of entropy distribution is shown in Figure 4 in Section 5.3.</p>
<p>Semantic Exploration (SE)</p>
<p>For tree based search, we propose SE, which leverages linguistic semantics to avoid exploring semantically similar nodes.For example, as illustrated in Figure 3, node 1 ("How many pages did Julie read yesterday?")and node 3 ("What is the number of pages Julie finished reading yesterday?")convey the identical meaning.The key components of SE are semantic clustering and semantic PUCT.</p>
<p>Semantic clustering Semantic clustering groups generated actions based on their semantic equivalence.At the current node s, the tree generates d candidate actions, A(s), using a language model.For each pair of actions (a, a ′ ) in A(s), the equivalence relation E(a, a ′ ) is established by checking for bi-directional entailment between the actions (Kuhn et al., 2023;Farquhar et (Yao et al., 2023;Hao et al., 2023) and SE (Ours).Nodes with similar color tones (e.g., blue and green) represent semantically similar node, and the thickness of each line indicates the magnitude of the exploration weight.2024).Specifically, each entailment decision is determined by applying an arg max function to the outputs of the DeBERTa-large model (He et al., 2020a), a relatively lightweight model compared to larger LLMs, which is used for single-sentence inputs.Actions with equivalent meanings are grouped into semantic equivalence clusters C = {C 1 , C 2 , . . ., C d ′ }, where each cluster C i ⊆ A(s) contains semantically identical actions and d ′ represents the total number of clusters.As illustrated in Figure 3, semantic clustering prevents the redundant expansion of subsequent sub-trees from nodes containing actions with equivalent meanings.Our analysis of semantically unique actions is presented in Section 5.3.</p>
<p>Semantic PUCT To achieve efficient exploration with semantic clusters C, we propose semantic PUCT.Previous research in LLMs has shown that more frequently generated samples indicate higher significance (Wang et al., 2023b).Leveraging this self-consistency principle, we define the predictor of each semantic cluster C i as the total probability assigned to the cluster:
π(C i |s) = a∈C i p θ (a|s, m) .(6)
We extend the PUCT algorithm (Equation ( 2)) to operate at the level of semantic clusters as follows:
C * = arg max C∈C Q(s, C)+w•π(C|s) N (s) N (s, C) + 1 .(7)
Once a semantic cluster C i is selected, the highestprobability action within the cluster, i.e., a * = arg max a∈C i p θ (a | s, m), is used to construct the prompt for expansion.We present the effect of semantic PUCT in an ablation study in Section 5.4.</p>
<p>Early Stopping</p>
<p>We introduce an early stopping phase in MCTS to decide whether to terminate iterations early based on the confidence of the most probable answer.To measure this confidence, we use weighted aggregation rewards, which account for the semantic importance of nodes in the search tree.Let T denote the set of terminal nodes, and P (n j ) be the set of nodes along the path from the root node to terminal node n j ∈ T .The reward of a terminal node n j is computed by weighting the size of the semantic cluster |C(n)|, where C(n) is the cluster including the node n:
R(n j ) = n∈P (n j ) |C(n)| • r(n), ∀n j ∈ T . (8)
We define Y (n j ) as the extracted answer of a terminal node n j , and Y ′ as the set of all extracted answers.The aggregated reward R agg (y) for each answer y ∈ Y ′ is then computed by summing the rewards of all terminal nodes that produce the same answer y: R agg (y) =
n j ∈T ,Y (n j )=y R(n j ) .(9)
This weighted aggregation ensures that nodes in larger semantic clusters, i.e., more significant nodes, have a greater influence on the reward aggregation.</p>
<p>At each iteration, when a terminal node is reached, the algorithm terminates early if the highest aggregated reward satisfies sufficient confidence threshold α:
y * = arg max y∈Y ′ R agg (y), if max y∈Y ′ R agg (y) ≥ α . (10)
An ablation study of α is presented in Figure 5.</p>
<p>Experiments</p>
<p>In this section, we present the experimental evaluation of SEAG.Section 5.1 describes the experimental setup, including datasets and models.The main evaluation results are presented in Section 5.2.Detailed analyses follows in Section 5.3, along with ablation studies in Section 5.4.Additional experimental details are provided in Appendix E.</p>
<p>Experimental Setup</p>
<p>Baselines We consider four reasoning methods as baselines across sequential and tree-based reasoning approaches.CoT (Wei et al., 2022) and CoT-SC (Wang et al., 2023b) are sequential reasoning approaches, where CoT generates step-by-step reasoning paths, and CoT-SC extends this by incorporating self-consistency.ToT (Yao et al., 2023) and RAP (Hao et al., 2023) use tree search algorithms to explore multiple reasoning paths, utilizing algorithms such as beam search and MCTS, respectively.For SEAG, SE, and RAP, the number of MCTS iterations is 10 and the number of actions is 4.For AG, we set k = 10 across all experiments.To ensure a fair comparison, we use identical prompts and the same number of in-context examples across all methods.The primary difference lies in the specific search mechanisms employed by each method.We present the detailed prompts for each baseline in Appendix I.</p>
<p>Datasets For evaluating the reasoning ability, we use two standard reasoning tasks: GSM8K (Cobbe et al., 2021), a dataset of 8.5k math word problems requiring multi-step numerical reasoning, and AI2 Reasoning Challenge (ARC) (Clark et al., 2018), a dataset containing 7.8k multiple-choice science questions sourced from grade-level science exams.</p>
<p>For evaluation, we randomly sample 400 instances from the test sets of GSM8K and ARC.</p>
<p>Models</p>
<p>We conduct experiments with different LLMs using Llama3-8B-Instruct (Dubey et al., 2024), Llama2-13B-Chat (Touvron et al., 2023), and Mistral-7B- Instruct-v0.3 (Jiang et al., 2023) to demonstrate the robustness of SEAG.</p>
<p>Evaluation metrics We use accuracy and computational costs, which are measured in terms of the number of inferences and tokens.The number of inferences refer to the total LLM calls required during the reasoning process per instance.Input tokens and output tokens denote the total number of tokens processed during reasoning and generated by the model as output, respectively, for each instance.</p>
<p>Main Results</p>
<p>Table 1 compares the accuracy and inference efficiency of six reasoning methods evaluated on the GSM8K and ARC using three different LLMs.The results for the base LLM are presented in in Appendix G. SEAG consistently outperforms all baseline methods in accuracy across all datasets and models, demonstrating its robustness and effectiveness.Notably, compared to RAP, which is our closest baseline, SEAG achieves a 4.3% increase in accuracy while requiring only 31% as many inferences as RAP, on average.This highlights SEAG's ability to deliver superior reasoning performance with improved computational efficiency.Further discussions on inference cost in terms of tokens are provided in Appendix G.When evaluating SE independently, SE also achieves higher accuracy and fewer inference costs compared to tree search-based methods, RAP and ToT, as illustrated in Figure 2. By incorporating AG, SEAG further improves performance by adaptively determining reasoning paths.This allows SEAG to capitalize on CoT-SC's strength in internal reasoning, achieving higher accuracy while also enhancing computational efficiency.Additional plots using token cost as an alternative cost metric are presented in Appendix F.</p>
<p>Analyses</p>
<p>Necessity of AG Using the results of 10 independent CoT-SC samplings, we calculate confidence in terms of entropy values as defined in Equation (4).Problems are categorized into several groups based on their entropy values.Figure 4 presents the results of applying CoT, RAP, and SE to problems within each group.Notice that low entropy indicates that the model generates the same answer with confidence, while high entropy suggests that the model provides different answers across different samplings.We observe that CoT-SC achieves high accuracy for problems with low entropy.In contrast, RAP and SE, which incorpo-</p>
<p>Effect of the number of tokens</p>
<p>We analyze the effect of increasing average token cost on the accuracy improvement of the methods in Figure 5.</p>
<p>Token cost is calculated based on the pricing policy of commercial LLM APIs, where output tokens are assigned a cost approximately four times higher than input tokens.For RAP, token cost is controlled by varying the number of MCTS iterations, set to 4, 6, 8, and 10.For SE, we adjusts the early stopping threshold α, set to 5, 7, 9, and 11, while fixing the number of MCTS iterations at 10.As shown in Figure 5, SE demonstrates a steeper performance improvement compared to RAP.Furthermore, by incorporating AG into SE, SEAG shows the highest accuracy (when α = 11), and achieves further improvements in both accuracy and token cost.</p>
<p>Latency analysis</p>
<p>We assess the end-to-end latency of each reasoning method on 100 randomly selected ARC instances using the Llama3-8B-Instruct on a single NVIDIA RTX A5000 GPU.</p>
<p>As shown in full SE reasoning process in the remaining 33% of cases with an average latency of ≈90.11 seconds.</p>
<p>The ordering and trend of latency across methods are consistent with those observed in LLM inference counts and token usage.</p>
<p>Ablation Studies</p>
<p>Improvement by semantic PUCT To evaluate the effectiveness of the proposed action selection algorithm, semantic PUCT in equation ( 7), we conduct an ablation study with two existing algorithms: UCT, which relies solely on a count-based uncertainty term in equation (2), and PUCT, which incorporates an uncertainty term involving π(a|s) and a count-based term in equation ( 2).In Table 5, we demonstrate that Semantic PUCT outperforms the existing algorithms while maintaining comparable efficiency by encouraging exploration of semantically probable clusters with π(C|s).</p>
<p>Improvement by weighted aggregation of rewards To evaluate the effectiveness of the aggregation strategy weighted by |C(n)|, we conducted a comparison with an aggregation strategy where all actions were weighted equally (i.e., manually set |C(n)| to 1 in equation 8), regardless of clustering.The experiments were conducted using the SEAG method.To ensure a fair comparison of LLM computation costs, α was set to 11 for the weighted aggregation and 8 for the equal weighting strategy.As shown in Table 5, the weighted aggregation consistently achieved higher accuracy across both benchmarks while incurring lower LLM computation costs.</p>
<p>Conclusion</p>
<p>In this paper, we propose Semantic Exploration with Adaptive Gating (SEAG), a framework that enhances the efficiency and accuracy of multi-step reasoning tasks in LLMs.SEAG addresses two key issues in tree-based reasoning methods: (i) the unnecessary use of complex reasoning techniques for tasks solvable with simpler approaches, and (ii) the generation of semantically redundant nodes during exploration.By combining adaptive gating to evaluate task complexity, semantic exploration to minimize redundancy, and early stopping to reduce computational overhead, SEAG achieves significant performance improvements.</p>
<p>Limitations</p>
<p>Our method focuses on leveraging only internal knowledge to enhance the efficiency of tree searchbased reasoning methods.This focus ensures a fair evaluation of the method's inherent efficiency without reliance on external tools or feedback.Expanding the approach to incorporate such external resources could further improve the reasoning process, presenting a promising direction for future work.Furthermore, the experiments are primarily conducted on benchmarks where final answers are provided in a discrete form, rather than freeform natural language generation.Evaluating the method on tasks open-ended responses is left as future work.</p>
<p>Appendix A Further Details of Reasoning Frameworks in Language Models</p>
<p>Chain-of-Thought (CoT) prompting (Wei et al., 2022) sequentially generates thoughts z 1 , . . ., z l , where z i ∼ p θ (z i |x, z &lt;i ), and then generates the output with these thoughts, i.e., y ∼ p θ (y|x, z ≤l ).</p>
<p>Self-Consistency with CoT (CoT-SC) (Wang et al., 2023b).CoT-SC is an ensemble-based method that leverages k independently sampled reasoning paths, each generated using CoT prompting.Specifically, for each i ∈ [k], a reasoning path generates an output y i ∼ p θ (y|x, z i ≤l ).The final output is determined by majority voting across k paths: arg max y∈Y k i=1 I(y i = y), where Y is the set of all possible candidate outputs and I(•) is the indicator function.</p>
<p>Tree-of-Thought (ToT) prompting (Yao et al., 2023).For exploring multiple reasoning paths, ToT prompting extends CoT by structuring the reasoning process as a tree structure, where each node represents a reasoning state s = [x, z 1 , ..., z i ] with input x and the sequence of intermediate thoughts.</p>
<p>Intermediate thoughts are generated sequentially as z i ∼ p θ (z i |x, z &lt;i ).The exploration relies on search algorithms, such as Breath-First Search (BFS) or Depth-First Search (DFS), to evaluate and prioritize paths using heuristics based on LM evaluations of each state.</p>
<p>B Reward Design in MDP</p>
<p>We adapt the reward design proposed in RAP (Hao et al., 2023), focuses on evaluating the feasibility and desirability of reasoning steps.This approach incorporates a reward function r t = r(s t , a t ) ∈ R to assess the impact of an action a t to a state s t .Our reward design integrates two key components: self-evaluation of the action's helpfulness and confidence of the resulting state.</p>
<p>Self-evaluation by the LLM The model can selfassess the helpfulness of reasoning by answering questions, such as "Is this reasoning step useful?".The reward is derived from the probability of the token "Yes", which provides an estimate of the LLM's confidence in the correctness of its reasoning.This self-evaluation mechanism can adapt to task-specific nuances.</p>
<p>Confidence of the state The confidence of a state is determined by sampling multiple answers from the model and calculating the proportion of the most frequent answer.A higher confidence score indicates stronger alignment with the LLM's internal knowledge, promoting more reliable reasoning steps.For ARC dataset, which requires answers in natural language, we also incorporate the concept of semantic exploration to merge similar states, enabling more accurate confidence estimation.In SE, we construct the LLM prompt by uniformly sampling actions from a semantic cluster when generating multiple answers.</p>
<p>C Monte Carlo Tree Search (MCTS)</p>
<p>Selection In the selection phase, algorithm chooses a move at a node or an action in the tree.The choice is based on pre-defined strategies, the typically the Upper Confidence Bound applied to Trees (UCT) (Kocsis and Szepesvári, 2006), which is based on the upper confidence bound (UCB) (Auer et al., 2002).At node s, we select the action with balance exploration trying less visited actions and exploitation choosing actions with higher stateaction value function Q : S × A → R, which estimated the expected future reward with (s, a).
a * = arg max a∈A(s) Q(s, a) + w ln N (s) N (s, a) ,(11)
where N (•) denotes the total number of visits in previous iterations, A(s) is the set of possible actions at state s, and w is the constant of balancing exploration and exploitation.nodes along the path are updated.After completing a k of MCTS iterations, the algorithm terminates, selecting the final answer.terminate algorithm and aggregation of lots of reasoning traces.</p>
<p>D SEAG Algorithm</p>
<p>Algorithm 1 presents the detailed procedure of SEAG, illustrating its key steps: adaptive gating, semantic exploration, and early stopping.</p>
<p>E Experimental Settings</p>
<p>We describe the detailed experimental settings to ensure reproducibility.Table A1 presents the default hyperparameters for each method.All experiments were conducted using a single GPU and for Llama3-8B and Mistral-7B models, and two GPUs for Llama2-13B model.The total computational resource is required to produce the results in Table 1 is approximately 832 GPU hours.We utilize RTX 3090, A5000, and A6000 GPUs for all experiments.Due to the high computational cost, we report results based on a single run.</p>
<p>F Efficiency with Token Cost</p>
<p>We provide additional scatter plots in Figure A1 where token cost is used as the cost metric instead of the number of inferences as shown in Figure 2. The token cost metric represents the cumulative number of tokens processed by the LLM during inference.The token cost is measured based on a commonly used pricing policy for LLMs, where the total cost is calculated as (Token cost) = (Input tokens) + 4 × (Output tokens).Figure A1 shows similar patterns to those observed in the main paper.This consistency reinforces the robustness of our methods in achieving superior accuracy while minimizing computational expense across different cost perspectives.</p>
<p>G Extended Experimental Results</p>
<p>In this section, we provide additional experimental results comparing the accuracy and computational costs of various methods on the GSM8K and ARC benchmarks.Specifically, we present results using both Llama3-8B-Base in Table A2 and Llama3-8B-Instruct models in Table A3.Both tables highlight the improvements in accuracy and efficiency achieved by our proposed methods, SE (Ours) and SEAG (Ours), across benchmarks.These improvements are evident not only in accuracy across all baselines but also in reductions in the number of inferences, input tokens, and output tokens compared to tree search-based methods.</p>
<p>H Prompt Design Modification for Diverse Action Sampling</p>
<p>To encourage the generation of semantically unique actions, one can consider sampling actions in a</p>
<p>Algorithm 1: Semantic Exploration with Adaptive Gating (SEAG)</p>
<p>1: Input: Input x, the number of reasoning paths k, k ′ , thresholds τ (entropy), α (reward confidence), a semantic equivalence relation E(•, •) 2: Output: Final answer y * 3: Generate k reasoning paths {y i } k i=1 using CoT:</p>
<p>Step 1: Adaptive Gating
y i ∼ p θ (y|x, z i ≤l ), ∀i ∈ [k]
4: Compute the estimated probability for each y ∈ Y, where Y is the set of candidate outputs across k paths:
q(y) = 1 k k i=1 I(y = y i ) 5: Compute entropy H(y) = − y∈Y q(y) log(q(y)) 6: if H(y) ≤ τ then 7:
Select final answer using majority voting: y * = arg max y∈Y k i=1 I(y = y i ) 8:</p>
<p>Return: Final answer y * 9: else 10:</p>
<p>for each iteration from 1 to k ′ do 11: Set current node s.</p>
<p>12: repeat 13: Generate candidate actions A(s).</p>
<p>Step 2: Semantic Exploration 14:</p>
<p>Group actions into semantic clusters
C = {C 1 , C 2 , . . . , C d ′ } using E(a, a ′ ).
15:</p>
<p>Compute π(C i |s) = a∈C i p θ (a|s, m) for each semantic cluster C i .</p>
<p>16:</p>
<p>Select a semantic cluster C via semantic PUCT:
C * = arg max C∈C Q(s, C)+w•π(C|s) N (s) N (s, C) + 1 . 17:
Select action a * = arg max a∈C * p θ (a | s, m) and expand node s.</p>
<p>18:</p>
<p>until A terminal node is reached.</p>
<p>19:</p>
<p>For terminal nodes n j ∈ T , compute path rewards:</p>
<p>Step 3-1: Weighted Aggregation
R(n j ) = n∈P (n j ) |C(n)| • r(n) 20:
Define Y (n j ) as the extracted answer of a terminal node n j and Y ′ as the set of answers.</p>
<p>21:</p>
<p>Aggregate rewards for each answer y ∈ Y ′ : R agg (y) =
n j ∈T ,Y (n j )=y R(n j ) 22:
if max y∈Y R agg (y) ≥ α then</p>
<p>Step 3-2: Early Stopping</p>
<p>23:</p>
<p>Select final answer: y * = arg max y∈Y R agg (y)</p>
<p>24:</p>
<p>Return: of unique semantic clusters expanded with Llama3-8B-Instruct.Interestingly, the sequential manner designed to encourage the generation of semantically distinct actions results in fewer unique semantic clusters compared to the standard i.i.d.sampling as shown in A4.Additionally, we observe a performance drop in accuracy with the sequential manner compared to i.i.d.sampling, from 0.850 to 0.812 on GSM8K and from 0.830 to 0.823 on ARC.Thus, while generating non-redundant actions through prompt-level modifications can be an interesting direction, it seems to be non-trivial.Moreover, conditioning action generation on previously sampled actions may shift the sampling distribution, potentially limiting the virtue of semantic consistency inherent in i.i.d.sampling, which is a key advantage used in SE.</p>
<p>I Prompt</p>
<p>Figure A2, A3, and A4 present examples of prompts.We use a 1-shot example for all inferences, including prompts for answer, action, and reward generation in tree search-based reasoning and CoT prompting.</p>
<p>An answer generation prompt for GSM8K</p>
<p>Given a question, please decompose it into sub-questions.For each sub-question, please answer it in a complete sentence, ending with "The answer is".When the original question is answerable, please start the subquestion with "Now we can answer the question: ".Question 1: Albert is wondering how much pizza he can eat in one day.He buys 2 large pizzas and 2 small pizzas.A large pizza has 16 slices and a small pizza has 8 slices.If he eats it all, how many pieces does he eat that day?Question 1.1: How many slices are in one large pizza?Answer 1.1:One large pizza has 16 slices.The answer is 16.Question 1.2: How many slices are there in total from the large pizzas?Answer 1.2:He buys 2 large pizzas, so 2 * 16 = 32 slices.The answer is 32.Question 1.3: How many slices are in one small pizza?Answer 1.3:One small pizza has 8 slices.The answer is 8. Question 1.4: How many slices are there in total from the small pizzas?Answer 1.4:He buys 2 small pizzas, so 2 * 8 = 16 slices.The answer is 16.Question 1.5: Now we can answer the question: How many pieces does he eat that day?Answer 1.5:There are 32 slices from the large pizzas and 16 slices from the small pizzas, so he eats 32 + 16 = 48 pieces that day.The answer is 48.</p>
<p>Question 2: Josh decides to try flipping a house.He buys a house for $80,000 and then puts in $50,000 in repairs.This increased the value of the house by 150%.How much profit did he make?Question 2.1: How much did Josh spend on the house and repairs in total?Answer 2.1:</p>
<p>An action generation prompt for GSM8K</p>
<p>Given a question, please decompose it into sub-questions.For each sub-question, please answer it in a complete sentence, ending with "The answer is".When the original question is answerable, please start the subquestion with "Now we can answer the question: ".Question 1: Albert is wondering how much pizza he can eat in one day.He buys 2 large pizzas and 2 small pizzas.A large pizza has 16 slices and a small pizza has 8 slices.If he eats it all, how many pieces does he eat that day?Question 1.1: How many slices are in one large pizza?Answer 1.1:One large pizza has 16 slices.The answer is 16.Question 1.2: How many slices are there in total from the large pizzas?Answer 1.2:He buys 2 large pizzas, so 2 * 16 = 32 slices.The answer is 32.Question 1.3: How many slices are in one small pizza?Answer 1.3:One small pizza has 8 slices.The answer is 8. Question 1.4: How many slices are there in total from the small pizzas?Answer 1.4:He buys 2 small pizzas, so 2 * 8 = 16 slices.The answer is 16.Question 1.5: Now we can answer the question: How many pieces does he eat that day?Answer 1.5:There are 32 slices from the large pizzas and 16 slices from the small pizzas, so he eats 32 + 16 = 48 pieces that day.The answer is 48.</p>
<p>Question 2: Josh decides to try flipping a house.He buys a house for $80,000 and then puts in $50,000 in repairs.This increased the value of the house by 150%.How much profit did he make?Question 2.1:</p>
<p>A reward generation prompt for GSM8K</p>
<p>Given a question and some sub-questions, determine whether the last sub-question is useful to answer the question.Output 'Yes' or 'No', and a reason.Question 1: Four years ago, Kody was only half as old as Mohamed.If Mohamed is currently twice as 30 years old, how old is Kody?Question 1.1: How old is Mohamed?Question 1.2: How old was Mohamed four years ago?New question 1.3: How old was Kody four years ago?Is the new question useful?Yes.We need the answer to calculate how old is Kody now.Question 2: Josh decides to try flipping a house.He buys a house for $80,000 and then puts in $50,000 in repairs.This increased the value of the house by 150%.How much profit did he make?New question 2.1: How much did Josh spend on the house?Is the new question useful?</p>
<p>An answer generation prompt for ARC</p>
<p>Given a question, please decompose it into sub-questions.For each sub-question, please answer it in a complete sentence, ending with "The answer is".When the original question is answerable, please start the subquestion with "Now we can answer the question with an option from A to D: ".An action generation prompt for ARC Given a question, please decompose it into sub-questions.For each sub-question, please answer it in a complete sentence, ending with "The answer is".When the original question is answerable, please start the subquestion with "Now we can answer the question with an option from A to D: ".</p>
<p>Figure 1 :
1
Figure1: An overview of SEAG.The framework consists of three main components: adaptive gating, semantic exploration and early stopping, detailed in Section 4. Given an input, adaptive gating determines whether to expand the search space based on the uncertainty H(•) in equation (4) of the generated answers obtained through multiple single-path reasoning.If the uncertainty is high, semantic exploration employs tree search to explore multiple reasoning paths, where actions are grouped into semantic clusters (further detailed in Figure3).Lastly, early stopping terminates the search when the highest aggregated reward surpasses a predefined threshold.</p>
<p>Figure 3 :
3
Figure3: Illustration of tree search-based reasoning methods(Yao et al., 2023;Hao et al., 2023) and SE (Ours).Nodes with similar color tones (e.g., blue and green) represent semantically similar node, and the thickness of each line indicates the magnitude of the exploration weight.</p>
<p>Figure 5 :
5
Figure 5: Comparisons of reasoning methods in average accuracy with increasing token cost.The token cost varies along with the different hyperparameters such as the number of MCTS iterations k and early stopping thresholds α.</p>
<p>Figure A3 :
A3
Figure A3: Example prompts for GSM8K.Italic texts denote 1-shot examples.</p>
<p>Question 1 :
1
Juan and LaKeisha roll a few objects down a ramp.They want to see which object rolls the farthest.What should they do so they can repeat their investigation?Options: A) Put the objects in groups, B) Change the height of the ramp, C) Choose different objects to roll, D) Record the details of the investigation.Question 1.1: What is necessary to ensure that experimental results can be repeated?Answer 1.1: To ensure repeatability, experimental details must be accurately recorded.The answer is to record details. Question 1.2: What kind of information should Juan and LaKeisha record for repeatability?Answer 1.2:They should record details like the objects used, ramp height, and surface conditions.The answer is experimental conditions.Question 1.3: How would recording experimental details help in the investigation?Answer 1.3: Recording details allows them to recreate the exact same conditions for reliable comparison.The answer is that it enables consistent replication.Question 1.4: Now we can answer the question with an option from A to D: What should they do to repeat their investigation?Answer 1.4: Record the details of the investigation.The answer is D. Question 2: Which method is the safest way to watch an eclipse of the Sun? Options: A) Turn away after two or three minutes.B) Look at the Sun through a long telescope.C) Cast an image through a pinhole onto a screen.D) Blink often until your eyes get used to the light.. Question 2.1: Why should you not look directly at the Sun during an eclipse?Answer 2.1:</p>
<p>Question 1 :
1
Figure A4: Example prompts for ARC.Italic texts denote 1-shot examples.</p>
<p>Scatter plots of accuracy and the number of LLM inferences for baselines and our methods, SE and SEAG, with GSM8K (left) and ARC (right) using Llama3-8B-Instruct model.SE achieves superior accuracy while reducing inference cost compared to existing tree search-based methods.SEAG further improves performance, achieving higher accuracy with efficiency comparable to other reasoning methods.
Accuracy0.77 0.78 0.79 0.80 0.81 0.82 0.83 0.84 0.85 0.86CoT CoT-SCToT SE (Ours) ARC SEAG (Ours)RAPFigure 2:</p>
<p>al.,
Tree Search-basedSemantic ExplorationInputInput12311, 3245678910111244, 56788, 9……Action1: "How many pages did Julie read yesterday?"Action 2: "What is the number of pages Julie finished reading today?"Action 3: "What is the number of pages Julie finished reading yesterday?"Action1 and 3: "How many pages did Julie read yesterday?", "What is the number of pages Julie finished reading yesterday?"Action 2: "What is the number of pages Julie finished reading today?"</p>
<p>Table 1 :
1
Comparison of reasoning methods effectiveness in accuracy and efficiency in the number of inferences for both GSM8K and ARC datasets.Bold texts indicate the best accuracies in each setting.
Llama3-8B-InstructLlama2-13B-ChatMistral-7B-InstructBenchmarkMethodAccuracy ↑ # of inferences ↓ Accuracy ↑ # of inferences ↓ Accuracy ↑ # of inferences ↓CoT0.76210.33010.5021CoT-SC0.845100.417100.66510GSM8KToT RAP0.785 0.825104.80 128.400.378 0.372102.06 121.690.613 0.667122.16 161.86SE (Ours)0.85082.630.40369.470.67598.79SEAG (Ours)0.86041.690.43553.090.68584.14CoT0.81810.59810.6881CoT-SC0.823100.637100.70810ARCToT RAP0.797 0.812149.59 196.960.590 0.637209.05 247.220.615 0.705221.36 313.20SE (Ours)0.83096.320.632146.200.713155.13SEAG (Ours)0.84846.150.63826.620.725110.950.6 0.8 1.00.32250.210.215 GSM8K0.1425SE (Ours) RAP CoT-SC0.63750.15750.130.06750.00750.40.090.20.00~0.60.6~1.2 1.2~1.8 1.8~2.4 2.4~3.0 Entropy0~0.20.2~0.5 0.5~1.2 1.2~1.6 1.6~2.0 Entropy
ARC AccuracyFigure 4: Accuracy comparison of CoT-SC, RAP, and SE (Ours) methods across different entropy ranges for GSM8K and ARC datasets.The numbers at the top of each entropy bin represent the proportion of problems within the corresponding entropy range.</p>
<p>Efficiency improvement by SE To evaluate the impact of SE, we measure the number of unique semantic clusters d ′ (i.e., the number of nodes expanded after applying semantic clustering) and the reduction rate at each depth when four actions, d = 4, are generated in each state.Table2presents that semantic clustering effectively reduces the re-
GSM8KARCDepth# of semantic clustersReduction rate (%)# of semantic clustersReduction rate (%)12.3142.363.0025.0022.1546.333.2219.3832.0748.342.6035.0241.8154.701.5561.25Table 2: The average number of semantic clusters(counts) and reduction rate (%) at each depth whenfour actions are generated at every step, d = 4 for bothGSM8K and ARC datasets.rate tree search, demonstrate superior accuracy forhigh-entropy problems.dundant search space from 25-45% at depth 1 to50-65% at depth 4. In overall, semantic clusteringeliminates approximately 20-60% of semanticallyredundant paths, significantly reducing the searchspace across different depths and datasets. Theseresults indicate that SE avoids repeatedly expand-ing and exploring semantically redundant pathsby incorporating semantic equivalence. Addition-ally, in Appendix H, we provide a brief analysis onprompt design modification to encourage diverseaction sampling as an alternative approach to SE.</p>
<p>Table 3
3, SE achieves a reduced aver-age latency of 76.44 seconds, including only 2.54seconds for semantic clustering, compared to ToT(120.63 s) and RAP (151.19 s). Our final methodSEAG further improves computational efficiencyby adaptively invoking SE only when necessary.Specifically, SEAG terminates early after CoT-SCin approximately 67% of cases, resulting in a lowaverage latency of ≈13.67 seconds, and utilizes the</p>
<p>Table A1 :
A1
Expansion Expansion corresponds to generate new child nodes in a decision tree.Given the state at the leaf node, the LM samples d possible actions, forming the action set A(s) which requires d LLM inferences.It then predicts the corresponding next states (the d child nodes).Default hyperparameters for SE, RAP, ToT, and CoT-SC.Parameters are grouped into LLM-related, general, and method-specific settings.
Simulation Simulation corresponds to estimate the Q-value at a given node by simulating fu-ture states from the node. The candidate ac-tions are evaluated using reward function r, se-lecting the action with the highest local reward Parameter Value temperature 0.8 top-k 50 top-p 0.95 batch size 1 the number of examples (prompts) 1 depth limit 5 the number of iterations 10 the number of actions 4 reward alpha 0.5 the number of confidences 8 a default value of reward confidence 0.8 beam size 3 depth limit 5 a Setting LLM-related setting General setting MCTS setting ToT setting CoT-SC setting the number of self-consistency 10
′ = arg max a∈A(s) r(s, a).Back-propagation When a terminal state is reached, marking the completion of a single reasoning path within one iteration, the Q values of all</p>
<p>Table A2 :
A2
Final answer y * Comparison of reasoning methods effectiveness in accuracy and efficiency in the number of inferences, input tokens and output tokens for both GSM8K and ARC datasets using Llama3-8B-Base.
0 i.i.d. 0.74 0.76 0.78 0.80 0.82 0.84 0.86 0.88 CoT CoT-SC 15000 30000 45000 60000 75000 Token cost RAP ToT SE (Ours) SEAG (Ours) GSM8K GSM8K ARC Sequential i.i.d. Sequential sampling generation sampling generation 2.31 1.37 3.00 1.72 2.15 1.53 3.22 2.00 2.07 1.59 2.60 2.05 1.81 1.53 1.55 1.17 Table A4: The average number of semantic clusters Accuracy Depth 1 2 3 4 (counts) for i.i.d. sampling and sequential generation at each depth when four actions are generated at everyAccuracy0.81 0.82 0.83 0.84 0.86 0.85 0.80 0.79 0.78 0.770ARC SE (Ours) 20000 40000 60000 80000 100000 CoT-SC CoT RAP SEAG (Ours) ToT Token CostFigure A1: Scatter plots of accuracy and token cost for baselines and our methods, SE and SEAG, with GSM8K step, d = 4 for both GSM8K and ARC datasets.(left) and ARC (right) using Llama3-8B-Instruct model.BenchmarkMethodAccuracy ↑ # of inferences ↓ Input tokens ↓ Output tokens ↓CoT0.4451218.3785.26CoT-SC0.598102183.76906.97GSM8KToT RAP0.598 0.665121.93 146.1857187.68 67526.582856.04 3113.34SE (Ours)0.67592.8942832.031969.56SEAG (Ours)0.68364.8425853.651263.95CoT0.7131215.8029.56CoT-SC0.787102157.93362.51ARCToT RAP0.748 0.767169.82 208.8275622.06 91586.372886.81 3224.82SE (Ours)0.775103.5544404.501511.65SEAG (Ours)0.78812.333155.79397.37BenchmarkMethodAccuracy ↑ # of inferences ↓ Input tokens ↓ Output tokens ↓CoT0.7621218.3785.86CoT-SC0.845102183.73943.66GSM8KToT RAP0.785 0.825104.80 128.4049003.05 59800.592355.06 2587.73SE (Ours)0.85082.6338743.451766.21SEAG (Ours)0.86041.6917501.681759.50CoT0.8181215.7948.315CoT-SC0.823102157.93505.79ARCToT RAP0.797 0.812149.59 196.9667534.70 88670.523033.07 3636.17SE (Ours)0.83096.3242507.851692.92SEAG (Ours)0.84846.1515993.29655.90Table A3: Comparison of reasoning methods effectiveness in accuracy and efficiency in the number of inferences,input tokens and output tokens for both GSM8K and ARC datasets using Llama3-8B-Instruct.25:else26: 27: sequential manner rather than i.i.d. sampling us-// Back-propagation end if 28: ing the same prompt. Specifically, we modify the Return: Final answer y  *  29: sentence in the original prompt, "Given a ques-end for tion, please decompose it into sub-questions." into 30: end if "Given a question, please decompose it into sub-questions with a distinct meaning from the follow-ing sub-questions: 'How many pages did Julie readyesterday?', 'What is the number of pages Julie fin-ished reading today?'." when sampling the third action to steer the LLM toward generating actions with different meanings. Following the procedure in Table 2, we imple-ment SE using both the i.i.d. and sequential man-ners of action generation and compare the number
AcknowledgementsThis work was supported by Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grants funded by the Korea government (MSIT) (No.RS-2019-II191906, Artificial Intelligence Graduate School Program (POSTECH); No.RS-2024-00457882, AI Research Hub Project; No.RS-2024-00509258, Global AI Frontier Lab) and the Korea Institute for Advancement of Technology (KAIT), funded by the Ministry of Trade, Industry and Energy (MOTIE), Republic of Korea (No.RS-2025-00564342).A CoT prompt for GSM8KPlease solve the following question step by step and conclude by saying "The answer is".Q: Albert is wondering how much pizza he can eat in one day.He buys 2 large pizzas and 2 small pizzas.A large pizza has 16 slices and a small pizza has 8 slices.If he eats it all, how many pieces does he eat that day?A: He buys 2 large pizzas, so he has 2 * 16 = 32 slices.He buys 2 small pizzas, so he has 2 * 8 = 16 slices.There are 32 slices from the large pizzas and 16 slices from the small pizzas, so he eats 32 + 16 = 48 pieces that day.The answer is 48.Q: Elizaś rate per hour for the first 40 hours she works each week is $10.She also receives an overtime pay of 1.2 times her regular hourly rate.If Eliza worked for 45 hours this week, how much are her earnings for this week?A:A CoT prompt for ARC Please solve the following question step by step and conclude by saying "The answer is".Q: Juan and LaKeisha roll a few objects down a ramp.They want to see which object rolls the farthest.What should they do so they can repeat their investigation?Options: A) Put the objects in groups, B) Change the height of the ramp, C) Choose different objects to roll, D) Record the details of the investigation.A: To ensure their investigation can be repeated, they need to record detailed information about the setup, such as the objects used, the height of the ramp, and surface conditions.Recording these details allows them to recreate the same conditions for reliable comparisons.The answer is D. Q: Which method is the safest way to watch an eclipse of the Sun? Options: A) Turn away after two or three minutes.B) Look at the Sun through a long telescope.C) Cast an image through a pinhole onto a screen.D) Blink often until your eyes get used to the light.. A:
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Finite-time analysis of the multiarmed bandit problem. Peter Auer, Nicolò Cesa-Bianchi, Paul Fischer, 10.1023/A:1013689704352Machine Learning. 200247</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems. 202033</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Michael Jq Jeremy R Cole, Daniel Zhang, Julian Gillick, Bhuwan Martin Eisenschlos, Jacob Dhingra, Eisenstein, arXiv:2305.14613Selectively answering ambiguous questions. 2023arXiv preprint</p>
<p>Efficient selectivity and backup operators in monte-carlo tree search. International conference on computers and games. Springer2006</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Detecting hallucinations in large language models using semantic entropy. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, Yarin Gal, Nature. 63080172024</p>
<p>A semantic similarity approach to paraphrase detection. Samuel Fernando, Mark Stevenson, Proceedings of the 11th annual research colloquium of the UK special interest group for computational linguistics. the 11th annual research colloquium of the UK special interest group for computational linguisticsCiteseer2008</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Reasoning with language model is planning with world model. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, Zhiting Hu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Deberta: Decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, arXiv:2006.036542020aarXiv preprint</p>
<p>Realformer: Transformer likes residual attention. Ruining He, Anirudh Ravula, Bhargav Kanagal, Joshua Ainslie, arXiv:2012.117472020barXiv preprint</p>
<p>Abstract meaning representation for paraphrase detection. Fuad Issa, Marco Damonte, Xiaohui Shay B Cohen, Yi Yan, Chang, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLong Papers20181</p>
<p>Monte-carlo planning and learning with language action value estimates. Youngsoo Jang, Seokin Seo, Jongmin Lee, Kee-Eung Kim, International Conference on Learning Representations. 2021</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>Bandit based monte-carlo planning. Levente Kocsis, Csaba Szepesvári, European conference on machine learning. Springer2006</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Advances in Neural Information Processing Systems. 202235</p>
<p>Large language model guided tree-ofthought. Jieyi Long, arXiv:2305.082912023arXiv preprint</p>
<p>Language models of code are few-shot commonsense learners. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Predictive uncertainty estimation via prior networks. Andrey Malinin, Mark Gales, arXiv:1802.105012018Preprint</p>
<p>Lila: A unified benchmark for mathematical reasoning. Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. 2021arXiv preprint</p>
<p>Are nlp models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, 2022International Conference on Learning Representations</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 52975872016</p>
<p>Mastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, nature. 55076762017</p>
<p>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. Richard Socher, Eric Huang, Jeffrey Pennin, Christopher D Manning, Andrew Ng, Advances in neural information processing systems. 201124</p>
<p>Charformer: Fast character transformers via gradient-based subword tokenization. Yi Tay, Sebastian Vinh Q Tran, Jai Ruder, Hyung Won Gupta, Dara Chung, Zhen Bahri, Simon Qin, Cong Baumgartner, Donald Yu, Metzler, arXiv:2106.126722021arXiv preprint</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Alphazero-like tree-search can guide large language model decoding and training. Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus Mcaleer, Ying Wen, Weinan Zhang, Jun Wang, Forty-first International Conference on Machine Learning. 2024</p>
<p>Uncertainty-aware parameter-efficient selftraining for semi-supervised language understanding. Jianing Wang, Qiushi Sun, Nuo Chen, Chengyu Wang, Jun Huang, Ming Gao, Xiang Li, arXiv:2310.130222023aPreprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Autoformalization with large language models. Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, Christian Szegedy, Advances in Neural Information Processing Systems. 202235</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202336</p>
<p>Modeling multi-hop question answering as single sequence prediction. Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221Nitish Shirish Keskar, and Caiming Xiong</p>
<p>Deep learning for answer sentence selection. Lei Yu, Karl Moritz Hermann, Phil Blunsom, Stephen Pulman, arXiv:1412.16322014arXiv preprint</p>
<p>Sac3: Reliable hallucination detection in black-box language models via semantic-aware cross-check consistency. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang ; Jiaxin, Zhuohang Zhang, Kamalika Li, Bradley A Das, Sricharan Malin, Kumar, arXiv:2406.03816arXiv:2311.017402024a. 2024bPreprintRest-mcts*: Llm self-training via process reward guided tree search</p>
<p>Variational reasoning for question answering with knowledge graph. Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander Smola, Le Song, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201832</p>
<p>Language agent tree search unifies reasoning, acting, and planning in language models. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang, Forty-first International Conference on Machine Learning. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7493 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7493</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7493</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-09ff7cdc583b28ec63dfc85c7142f2cb0e5e4eef</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/09ff7cdc583b28ec63dfc85c7142f2cb0e5e4eef" target="_blank">BLINK: Multimodal Large Language Models Can See but Not Perceive</a></p>
                <p><strong>Paper Venue:</strong> European Conference on Computer Vision</p>
                <p><strong>Paper TL;DR:</strong> Blink, a new benchmark for multimodal language models (LLMs) that focuses on core visual perception abilities not found in other evaluations, is introduced and will stimulate the community to help multimodal LLMs catch up with human-level visual perception.</p>
                <p><strong>Paper Abstract:</strong> We introduce Blink, a new benchmark for multimodal language models (LLMs) that focuses on core visual perception abilities not found in other evaluations. Most of the Blink tasks can be solved by humans"within a blink"(e.g., relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning). However, we find these perception-demanding tasks cast significant challenges for current multimodal LLMs because they resist mediation through natural language. Blink reformats 14 classic computer vision tasks into 3,807 multiple-choice questions, paired with single or multiple images and visual prompting. While humans get 95.70% accuracy on average, Blink is surprisingly challenging for existing multimodal LLMs: even the best-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only 13.17% and 7.63% higher than random guessing, indicating that such perception abilities have not"emerged"yet in recent multimodal LLMs. Our analysis also highlights that specialist CV models could solve these problems much better, suggesting potential pathways for future improvements. We believe Blink will stimulate the community to help multimodal LLMs catch up with human-level visual perception.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7493.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7493.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IQ test (graphical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graphical reasoning task in Blink where models select the image that continues a pattern or is spatially consistent with examples; used as a proxy for visual abstract reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Human (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Human subjects answering the Blink IQ test items (authors collected/curated the items and recorded human responses).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / fluid intelligence (graphical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Given visual example(s) that establish a pattern, choose which candidate image continues the pattern (multiple-choice). Designed to probe graphical/pattern reasoning without domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>human subjects (standard experimental presentation); authors annotated human answers</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors: manually collected test samples and human annotations from various public, license-friendly online sources (as reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper notes the human score for IQ test was annotated by the authors and may not reflect typical human performance; authors report overall human average across Blink tasks of 95.70% but IQ-specific human baseline is 80.00%.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7493.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiniGPT-4-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MiniGPT-4-v2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal LLM that couples a visual encoder with an LLM to answer visual questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MiniGPT-4-v2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal LLM (vision+language) evaluated on Blink multiple-choice tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Choose image that continues pattern from visual examples (multiple-choice).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 19.33%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice evaluation with image inputs; temperature 0, retry 10 (standard Blink eval protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Performance substantially below human baseline and near-random; reported in Table 1 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7493.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenFlamingo-v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenFlamingo-v2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source autoregressive vision-language model for few-shot multimodal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenFlamingo-v2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal LLM evaluated on Blink.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice graphical reasoning (pattern completion).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 23.33%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice with images; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7493.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructBLIP-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructBLIP (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned vision-language model (7B) for general multimodal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructBLIP-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B instruction-tuned multimodal LLM evaluated on Blink multiple-choice tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Pattern continuation multiple-choice items.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 23.33%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice with images; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1; performance far below human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7493.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructBLIP-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructBLIP (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned vision-language model (13B) for multimodal instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructBLIP-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B instruction-tuned multimodal LLM evaluated on Blink.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice graphical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 26.00%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7493.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-internLM2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-internLM2 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of LLaVA multimodal models using the internLM family (7B).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-internLM2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B LLaVA variant evaluated on Blink.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Visual pattern completion multiple-choice.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 14.67%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice with images; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>One of the lowest reported IQ accuracies in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7493.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yi-VL-6B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yi-VL (6B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Yi-VL family multimodal model (6B), an open-source vision-language model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi-VL-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>6B multimodal LLM evaluated on Blink.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice graphical reasoning items.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 23.33%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7493.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yi-VL-34B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yi-VL (34B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large variant of Yi-VL multimodal models (34B).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi-VL-34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>34B multimodal LLM evaluated on Blink.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Image-pattern multiple-choice task.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 22.67%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7493.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-v1.5-7B-xtuner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA v1.5 (7B, xtuner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B LLaVA v1.5 model with xtuner fine-tuning toolkit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-v1.5-7B-xtuner</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B LLaVA variant with xtuner evaluated on Blink.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Visual IQ multiple-choice items.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 21.33%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7493.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-v1.5-13B-xtuner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA v1.5 (13B, xtuner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B LLaVA v1.5 model with xtuner fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-v1.5-13B-xtuner</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B LLaVA variant evaluated on Blink.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Pattern completion visual IQ items.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 18.67%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7493.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CogVLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CogVLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained visual expert model for multimodal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CogVLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal model evaluated on Blink benchmark items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice graphical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 26.67%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice with images; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7493.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-v1.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA v1.5 (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>7B version of LLaVA v1.5 vision-language model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-v1.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B LLaVA multimodal model evaluated on Blink.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Graphical IQ multiple-choice evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 24.00%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7493.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-v1.5-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA v1.5 (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>13B version of LLaVA v1.5 vision-language model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-v1.5-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B LLaVA multimodal model evaluated on Blink benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Pattern continuation multiple-choice visual items.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 28.00%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7493.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-v1.6-34B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA v1.6 (34B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>34B LLaVA v1.6 large multimodal model with improved reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-v1.6-34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large LLaVA model (34B) evaluated on Blink tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Visual IQ multiple-choice evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 26.00%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7493.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-VL-Max</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-VL-Max</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A versatile vision-language model capable of understanding and localization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-VL-Max</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-based multimodal model evaluated on Blink.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Graphical pattern multiple-choice items.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 22.00%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e7493.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's multimodal family model (Gemini) with a Pro variant for vision+language tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-based large multimodal model evaluated on Blink.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice graphical reasoning items.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 27.33%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice with images; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1; Gemini Pro performs above several open-source baselines but well below humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e7493.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3 OPUS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3 OPUS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's Claude 3 OPUS multimodal-capable model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3 OPUS</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-based multimodal model evaluated on Blink.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Graphical IQ multiple-choice task.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 21.33%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e7493.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V(ision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 model with vision capabilities, enabling image understanding and multimodal question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V(ision)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 model augmented with vision input (multimodal), evaluated on Blink benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice graphical reasoning (pattern completion) with image inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 24.67%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice with images; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Paper reports GPT-4V's IQ accuracy in Table 1; overall GPT-4V performs around 51.26% average across Blink tasks but only 24.67% on the IQ subtask.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.18">
                <h3 class="extraction-instance">Extracted Data Instance 18 (e7493.18)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A faster variant of OpenAI's GPT-4 model (text-first), evaluated in the paper in a text-only or multimodal setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI model variant evaluated on Blink (reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Visual pattern multiple-choice items (paper reports results).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 30.67%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1; one of the higher-performing GPT variants on the IQ subtask but still well below human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7493.19">
                <h3 class="extraction-instance">Extracted Data Instance 19 (e7493.19)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Another GPT-4 family variant reported by OpenAI; evaluated in the paper on Blink subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 family model variant evaluated on Blink (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>IQ test</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>Abstract reasoning / graphical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice graphical reasoning items.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td>accuracy 80.00%</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>accuracy 30.00%</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot multiple-choice; temperature 0, retry 10</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Authors (manual collection; see paper)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Reported in Table 1; performance still far below human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BLINK: Multimodal Large Language Models Can See but Not Perceive', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mmbench: Is your multi-modal model an all-around player? <em>(Rating: 2)</em></li>
                <li>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi <em>(Rating: 2)</em></li>
                <li>Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts <em>(Rating: 1)</em></li>
                <li>Visual spatial reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7493",
    "paper_id": "paper-09ff7cdc583b28ec63dfc85c7142f2cb0e5e4eef",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "IQ test",
            "name_full": "IQ test (graphical reasoning)",
            "brief_description": "A graphical reasoning task in Blink where models select the image that continues a pattern or is spatially consistent with examples; used as a proxy for visual abstract reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Human (baseline)",
            "model_description": "Human subjects answering the Blink IQ test items (authors collected/curated the items and recorded human responses).",
            "model_size": null,
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / fluid intelligence (graphical reasoning)",
            "test_description": "Given visual example(s) that establish a pattern, choose which candidate image continues the pattern (multiple-choice). Designed to probe graphical/pattern reasoning without domain knowledge.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "",
            "prompting_method": "human subjects (standard experimental presentation); authors annotated human answers",
            "fine_tuned": null,
            "human_data_source": "Authors: manually collected test samples and human annotations from various public, license-friendly online sources (as reported in the paper).",
            "statistical_significance": "",
            "notes": "Paper notes the human score for IQ test was annotated by the authors and may not reflect typical human performance; authors report overall human average across Blink tasks of 95.70% but IQ-specific human baseline is 80.00%.",
            "uuid": "e7493.0",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "MiniGPT-4-v2",
            "name_full": "MiniGPT-4-v2",
            "brief_description": "An open-source multimodal LLM that couples a visual encoder with an LLM to answer visual questions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MiniGPT-4-v2",
            "model_description": "Open-source multimodal LLM (vision+language) evaluated on Blink multiple-choice tasks.",
            "model_size": null,
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Choose image that continues pattern from visual examples (multiple-choice).",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 19.33%",
            "prompting_method": "zero-shot multiple-choice evaluation with image inputs; temperature 0, retry 10 (standard Blink eval protocol)",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Performance substantially below human baseline and near-random; reported in Table 1 of the paper.",
            "uuid": "e7493.1",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "OpenFlamingo-v2",
            "name_full": "OpenFlamingo-v2",
            "brief_description": "An open-source autoregressive vision-language model for few-shot multimodal tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenFlamingo-v2",
            "model_description": "Open-source multimodal LLM evaluated on Blink.",
            "model_size": null,
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Multiple-choice graphical reasoning (pattern completion).",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 23.33%",
            "prompting_method": "zero-shot multiple-choice with images; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1.",
            "uuid": "e7493.2",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "InstructBLIP-7B",
            "name_full": "InstructBLIP (7B)",
            "brief_description": "Instruction-tuned vision-language model (7B) for general multimodal tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InstructBLIP-7B",
            "model_description": "7B instruction-tuned multimodal LLM evaluated on Blink multiple-choice tasks.",
            "model_size": "7B",
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Pattern continuation multiple-choice items.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 23.33%",
            "prompting_method": "zero-shot multiple-choice with images; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1; performance far below human baseline.",
            "uuid": "e7493.3",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "InstructBLIP-13B",
            "name_full": "InstructBLIP (13B)",
            "brief_description": "Instruction-tuned vision-language model (13B) for multimodal instruction following.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InstructBLIP-13B",
            "model_description": "13B instruction-tuned multimodal LLM evaluated on Blink.",
            "model_size": "13B",
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Multiple-choice graphical reasoning.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 26.00%",
            "prompting_method": "zero-shot multiple-choice; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1.",
            "uuid": "e7493.4",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLaVA-internLM2-7B",
            "name_full": "LLaVA-internLM2 (7B)",
            "brief_description": "A variant of LLaVA multimodal models using the internLM family (7B).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-internLM2-7B",
            "model_description": "7B LLaVA variant evaluated on Blink.",
            "model_size": "7B",
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Visual pattern completion multiple-choice.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 14.67%",
            "prompting_method": "zero-shot multiple-choice with images; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "One of the lowest reported IQ accuracies in Table 1.",
            "uuid": "e7493.5",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Yi-VL-6B",
            "name_full": "Yi-VL (6B)",
            "brief_description": "Yi-VL family multimodal model (6B), an open-source vision-language model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Yi-VL-6B",
            "model_description": "6B multimodal LLM evaluated on Blink.",
            "model_size": "6B",
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Multiple-choice graphical reasoning items.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 23.33%",
            "prompting_method": "zero-shot multiple-choice; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1.",
            "uuid": "e7493.6",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Yi-VL-34B",
            "name_full": "Yi-VL (34B)",
            "brief_description": "Large variant of Yi-VL multimodal models (34B).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Yi-VL-34B",
            "model_description": "34B multimodal LLM evaluated on Blink.",
            "model_size": "34B",
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Image-pattern multiple-choice task.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 22.67%",
            "prompting_method": "zero-shot multiple-choice; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1.",
            "uuid": "e7493.7",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLaVA-v1.5-7B-xtuner",
            "name_full": "LLaVA v1.5 (7B, xtuner)",
            "brief_description": "A 7B LLaVA v1.5 model with xtuner fine-tuning toolkit.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-v1.5-7B-xtuner",
            "model_description": "7B LLaVA variant with xtuner evaluated on Blink.",
            "model_size": "7B",
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Visual IQ multiple-choice items.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 21.33%",
            "prompting_method": "zero-shot multiple-choice; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1.",
            "uuid": "e7493.8",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLaVA-v1.5-13B-xtuner",
            "name_full": "LLaVA v1.5 (13B, xtuner)",
            "brief_description": "A 13B LLaVA v1.5 model with xtuner fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-v1.5-13B-xtuner",
            "model_description": "13B LLaVA variant evaluated on Blink.",
            "model_size": "13B",
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Pattern completion visual IQ items.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 18.67%",
            "prompting_method": "zero-shot multiple-choice; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1.",
            "uuid": "e7493.9",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "CogVLM",
            "name_full": "CogVLM",
            "brief_description": "A pretrained visual expert model for multimodal tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CogVLM",
            "model_description": "Multimodal model evaluated on Blink benchmark items.",
            "model_size": null,
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Multiple-choice graphical reasoning tasks.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 26.67%",
            "prompting_method": "zero-shot multiple-choice with images; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1.",
            "uuid": "e7493.10",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLaVA-v1.5-7B",
            "name_full": "LLaVA v1.5 (7B)",
            "brief_description": "7B version of LLaVA v1.5 vision-language model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-v1.5-7B",
            "model_description": "7B LLaVA multimodal model evaluated on Blink.",
            "model_size": "7B",
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Graphical IQ multiple-choice evaluation.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 24.00%",
            "prompting_method": "zero-shot multiple-choice; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1.",
            "uuid": "e7493.11",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLaVA-v1.5-13B",
            "name_full": "LLaVA v1.5 (13B)",
            "brief_description": "13B version of LLaVA v1.5 vision-language model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-v1.5-13B",
            "model_description": "13B LLaVA multimodal model evaluated on Blink benchmark.",
            "model_size": "13B",
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Pattern continuation multiple-choice visual items.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 28.00%",
            "prompting_method": "zero-shot multiple-choice; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1.",
            "uuid": "e7493.12",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLaVA-v1.6-34B",
            "name_full": "LLaVA v1.6 (34B)",
            "brief_description": "34B LLaVA v1.6 large multimodal model with improved reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-v1.6-34B",
            "model_description": "Large LLaVA model (34B) evaluated on Blink tasks.",
            "model_size": "34B",
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Visual IQ multiple-choice evaluation.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 26.00%",
            "prompting_method": "zero-shot multiple-choice; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1.",
            "uuid": "e7493.13",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Qwen-VL-Max",
            "name_full": "Qwen-VL-Max",
            "brief_description": "A versatile vision-language model capable of understanding and localization tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-VL-Max",
            "model_description": "API-based multimodal model evaluated on Blink.",
            "model_size": null,
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Graphical pattern multiple-choice items.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 22.00%",
            "prompting_method": "zero-shot multiple-choice; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1.",
            "uuid": "e7493.14",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Gemini Pro",
            "name_full": "Gemini Pro",
            "brief_description": "Google's multimodal family model (Gemini) with a Pro variant for vision+language tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini Pro",
            "model_description": "API-based large multimodal model evaluated on Blink.",
            "model_size": null,
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Multiple-choice graphical reasoning items.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 27.33%",
            "prompting_method": "zero-shot multiple-choice with images; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1; Gemini Pro performs above several open-source baselines but well below humans.",
            "uuid": "e7493.15",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Claude 3 OPUS",
            "name_full": "Claude 3 OPUS",
            "brief_description": "Anthropic's Claude 3 OPUS multimodal-capable model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude 3 OPUS",
            "model_description": "API-based multimodal model evaluated on Blink.",
            "model_size": null,
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Graphical IQ multiple-choice task.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 21.33%",
            "prompting_method": "zero-shot multiple-choice; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1.",
            "uuid": "e7493.16",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V(ision)",
            "brief_description": "OpenAI's GPT-4 model with vision capabilities, enabling image understanding and multimodal question answering.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4V(ision)",
            "model_description": "GPT-4 model augmented with vision input (multimodal), evaluated on Blink benchmark.",
            "model_size": null,
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Multiple-choice graphical reasoning (pattern completion) with image inputs.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 24.67%",
            "prompting_method": "zero-shot multiple-choice with images; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Paper reports GPT-4V's IQ accuracy in Table 1; overall GPT-4V performs around 51.26% average across Blink tasks but only 24.67% on the IQ subtask.",
            "uuid": "e7493.17",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4 Turbo",
            "name_full": "GPT-4 Turbo",
            "brief_description": "A faster variant of OpenAI's GPT-4 model (text-first), evaluated in the paper in a text-only or multimodal setting.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4 Turbo",
            "model_description": "OpenAI model variant evaluated on Blink (reported in Table 1).",
            "model_size": null,
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Visual pattern multiple-choice items (paper reports results).",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 30.67%",
            "prompting_method": "zero-shot multiple-choice; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1; one of the higher-performing GPT variants on the IQ subtask but still well below human baseline.",
            "uuid": "e7493.18",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "Another GPT-4 family variant reported by OpenAI; evaluated in the paper on Blink subtasks.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "GPT-4 family model variant evaluated on Blink (Table 1).",
            "model_size": null,
            "test_name": "IQ test",
            "test_category": "Abstract reasoning / graphical reasoning",
            "test_description": "Multiple-choice graphical reasoning items.",
            "evaluation_metric": "accuracy",
            "human_performance": "accuracy 80.00%",
            "llm_performance": "accuracy 30.00%",
            "prompting_method": "zero-shot multiple-choice; temperature 0, retry 10",
            "fine_tuned": false,
            "human_data_source": "Authors (manual collection; see paper)",
            "statistical_significance": null,
            "notes": "Reported in Table 1; performance still far below human baseline.",
            "uuid": "e7493.19",
            "source_info": {
                "paper_title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mmbench: Is your multi-modal model an all-around player?",
            "rating": 2
        },
        {
            "paper_title": "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi",
            "rating": 2
        },
        {
            "paper_title": "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts",
            "rating": 1
        },
        {
            "paper_title": "Visual spatial reasoning",
            "rating": 1
        }
    ],
    "cost": 0.02049525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>BLINK(1): Multimodal Large Language Models Can See but Not Perceive</h1>
<p>Xingyu Fu ${ }^{1 \star}$, Yushi $\mathrm{Hu}^{2,3 *}$, Bangzheng $\mathrm{Li}^{4}$, Yu Feng ${ }^{1}$, Haoyu Wang ${ }^{1}$, Xudong Lin $^{5}$, Dan Roth ${ }^{1}$, Noah A. Smith ${ }^{2,3}$, Wei-Chiu Ma ${ }^{3 \dagger}$, Ranjay Krishna ${ }^{2,3 \dagger}$<br>${ }^{1}$ University of Pennsylvania, ${ }^{2}$ University of Washington, ${ }^{3}$ Allen Institute for AI, ${ }^{4}$ University of California, Davis, ${ }^{5}$ Columbia University https://zeyofu.github.io/blink/<br>Figure 1: The Blink Benchmark. Blink contains 14 visual perception tasks that can be solved by humans "within a blink", but pose significant challenges for current multimodal LLMs. These tasks are inspired by classical computer vision problems and recast into multiple-choice questions for multimodal LLMs to answer. Notice that the visual prompts and questions in this figure are different from the actual ones used in the benchmark for illustrative purposes, and answers of the samples are provided. ${ }^{\dagger}$</p>
<h4>Abstract</h4>
<p>We introduce Blink, a new benchmark for multimodal language models (LLMs) that focuses on core visual perception abilities not found in other evaluations. Most of the Blink tasks can be solved by humans "within a blink" (e.g., relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning). However, we find these perception-demanding tasks cast significant challenges for current multimodal LLMs because they resist mediation through natural language. Blink reformats 14 classic computer vision tasks into 3,807 multiple-choice questions, paired with single or multiple images and visual prompting. While humans get $95.70 \%$ accuracy on average,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Blink is surprisingly challenging for existing multimodal LLMs: even the best-performing GPT-4V and Gemini achieve accuracies of $51.26 \%$ and $45.72 \%$, only $13.17 \%$ and $7.63 \%$ higher than random guessing, indicating that such perception abilities have not "emerged" yet in recent multimodal LLMs. Our analysis also highlights that specialist CV models could solve these problems much better, suggesting potential pathways for future improvements. We believe Blink will stimulate the community to help multimodal LLMs catch up with human-level visual perception.</p>
<h1>1 Introduction</h1>
<p>Compared to today, computer vision was originally attempting to interpret images as projections of 3D scenes, not just processing 2D arrays of flat "patterns" [25, $58,61]$. In this pursuit, early research developed a series of intermediate tasks: they focused on understanding optical properties like reflectance [12, 76], 3D primitives through multi-view reasoning [37, 59], geometric reasoning through depth estimation [74], instance recognition through visual correspondence [55], affordance through keypoint grounding [36], and forensics through intrinsic images [9]. Yet in the modern era of large language models (LLMs), we, as a community, have focused less on such perceptual tasks, and instead have developed new tasks, mostly expressed in natural language, emphasizing the vision-language connection learned by multimodal LLMs [3, 6, 18, 20, 24, 26, 50, 51, 56, 62, 71, 77]. This might be because many traditional computer vision tasks resist mediation through natural language, due to the inherent imprecision of language (e.g., it is challenging to precisely pinpoint a spatial keypoint through language).</p>
<p>This paper aims to highlight crucial aspects of visual perception that have been overlooked when evaluating multimodal LLMs. To appropriately position our paper, let us revisit how we currently evaluate perception through using multimodal LLMs [43, 44, 47, 52, 53, 57, 87]. While many of these benchmarks have been popularized as the de facto evaluation measures for influential models like GPT-4V and Gemini-Pro, they conflate perception with language knowledge and reasoning. At the risk of singling out one benchmark, let us consider two questions highlighted in the popular MMBench [52]: "<image 1> Why is this hummingbird called ruby-throated?" and "<image 1> What will happen next? A: the person is gonna laugh B: the person is gonna cry." For the first question, the vision subpart is to recognize the hummingbird. For the second, it only needs a coarse description of the image. Everything else is left to the language model to solve. Such a conflation has also been reported for other benchmarks by previous work $[11,38,84]$. Our experiments show that this conflation reductively evaluates perception as a dense captioning task. In other words, by replacing the image with a task-agnostic dense caption, our experiments show that a "blind" GPT-4 performs well on these "multimodal tasks".</p>
<p>In response, we propose Blink. Blink reimagines traditional computer vision problems through a format that allows us to evaluate multimodal LLMs.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Comparison between Blink and previous benchmarks. Blink has several novel features: (1) Blink incorporates diverse visual prompts, like circles, boxes, and image masks, while previous benchmarks only have text questions and answers. (2) Blink evaluates a more comprehensive range of visual perception abilities, like multi-view reasoning, depth estimation, and reflectance estimation. Prior benchmarks are generally more focused on recognition-based VQA. (3) Blink contains "visual" commonsense problems that humans can answer within seconds, while prior benchmarks like [87] require domain knowledge. The samples of previous benchmarks are from [44, 52, 87]. Part of our samples are curated from $[10,19,29,31,35,42,88]$.</p>
<p>As partially demonstrated in Figure 1, ${ }^{1}$ Blink consists of 14 classic computer vision tasks, ranging from low-level pattern matching (e.g., visual correspondences estimation) to mid-level reasoning (e.g., relative depth estimation), and extending to high-level visual understanding (e.g., visual similarity). The image tasks are meticulously selected such that they are difficult to solve by reducing the evaluation using dense captioning; instead, the models must perceive the contents of the image(s) to answer. We recast each traditional task into a modern question-answering format, where answer choices are either images or text. Blink contains 3.8 K questions across 7.3 K images, where questions may contain multiple images that are curated from a wide range of datasets $[8,10,19,29,31,35,41,46]$, encompassing indoor household scenes as well as outdoor urban or natural environments. The questions and choices are either derived from the datasets, or</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>manually written by humans. On average, each question can be solved by a human subject within a Blink of an eye, except the IQ test.</p>
<p>We carefully evaluate 17 multimodal LLMs with various sizes (i.e., 7B, $13 \mathrm{~B}, 34 \mathrm{~B})$ on Blink. We observe the paradox that while these problems are easy for humans ( $95.70 \%$ average accuracy), they are extremely hard for existing machinery - even GPT-4V model can only achieve $51.26 \%$ accuracy on average, which is $44.44 \%$ worse than humans, and $13.17 \%$ better than random guessing. We also experiment with specialist vision models and find that they perform much better than multimodal LLMs. For example, the specialist outperforms GPT-4V by $62.8 \%$ on visual correspondence estimation, $38.7 \%$ on relative depth estimation, and $34.6 \%$ on multi-view reasoning, in terms of absolute accuracy. Our findings indicate that the perceptual abilities of multimodal LLMs have been previously overestimated. Furthermore, these models may benefit from integrating insights from specialized models that excel in these areas. We believe Blink can serve as an effective testbed for bridging the gap between traditional notions of perception and the modern generative capabilities of multimodal LLMs.</p>
<h1>2 Related Work</h1>
<h3>2.1 Multimodal Models</h3>
<p>Inspired by the impressive success in recent large language models (LLMs) [13, $21,62,75,90]$, a sequence of studies explore multimodal LMMs that can jointly understand vision and language information and generate textual answers through adding a modality adaption structure between a frozen visual encoder [27,64,69] and a frozen LLM [75, 90]. Flamingo [3] and BLIP-2 [45] are two of the earliest works to explore these transformer-based multi-modality conjunction structures. They first pre-train on image-text matching datasets [15, 41, 46, 65] and then fine-tune on task-specific datasets such as visual question answer (VQA) [4,33]. Starting from LLaVA [48,50], people use LLM synthesized instruction-following chat data (which are in VQA format) for instruction tuning and achieve much better results [7, 16, 24, 71]. There have been extended studies that explore further capabilities of multimodal LLMs, especially on VQA reasoning [30-32, 38, 39, 66, 81, 89]. However, they mainly focus on the textual reasoning abilities [80] within the multimodal LLMs and do not emphasize visual perceptions.</p>
<h3>2.2 Multimodal Benchmarks</h3>
<p>Traditional vision-language datasets are designed to assess single-task capabilities, such as optical character recognition (OCR) [54], image captioning [46], and visual question answering [4,33]. However, these datasets are often not comprehensive enough to holistically assess multimodal LMMs on general perception and reasoning abilities. Many recent papers have built more comprehensive benchmarks. MME [28] is one of the earliest holistic benchmarks containing multi-modal Yes/No questions on the defined visual perception and language</p>
<p>reasoning tasks. MM-Vet [86] includes six sub-features from the previous datasets including recognition-focused questions, OCR, and math, providing a diverse while discrete evaluation set. MMBench [52] covers more subjects and provides a more robust circular evaluation setting. Seed-Bench [43,44] benchmark has a more diverse source of inputs, including multiple-image inputs and video, and includes more tasks. However, the visual perception questions in MME, MMBench, MM-Vet, and Seed-Bench are mainly extracted from existing VQA datasets or generated by GPT [62] from image descriptions such as COCO-Caption [46], and are recognition focused, covering topics such as object (attribute)recognition, and OCR. In contrast, we focus on multiple distinct nuanced perception abilities and recognition-level perception is only one of our focus. Some other multimodal benchmarks have distinct focuses. MMMU [87] aims at achieving expert-level artificial general intelligence by collecting domain-knowledge-required questions. HallusionBench [34] mainly tests the language hallucination and visual illusion phenomena. MathVista [57] presents exclusively mathematical domain visual questions based on images such as charts, tables, and diagrams. These benchmarks do not require human-level perception abilities as in Blink and therefore cannot measure model visual perceptions holistically.</p>
<h1>3 The Blink Benchmark</h1>
<p>Our goal is to faithfully evaluate the visual perception capabilities of existing Multimodal LLMs. We seek to study the visual perception gap between humans and machineries, and offer deeper insights into potential pathways towards achieving more generalized machine perception. Based on the observation that existing benchmarks predominantly focus on evaluating visual recognition abilities, we introduce a novel benchmark, Blink, designed to enable both quantitative and qualitative evaluation of the nuanced perception capabilities of multimodal LLM across various dimensions. We unfold this section by illustrating the overall design of Blink (3.1) and discussing its unique features comparing with previous benchmarks. Then we describe each task in detail, providing an in-depth explanation of the data curation process (3.2).</p>
<h3>3.1 Overview of Blink</h3>
<p>To ensure that one can effectively measure what Multimodal LLMs can or cannot perceive, we carefully select 14 tasks (see $\S 3.2$ for the full list) that are difficult to solve by reducing the evaluation into text-only questions using dense captioning. The tasks are drawn from either classic computer vision problems or recent applications of Multimodal LLMs, each of which requires a nuanced understanding of the visual data. They range from low-level pattern matching (e.g., visual correspondence) to mid-level spatial reasoning (e.g., relative depth), and up to high-level visual understanding (e.g., visual similarity). This variety allows for a systematic exploration of Multimodal LLMs' capabilities across different perceptual complexity layers. Furthermore, these visual tasks vary in</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Statistics of Blink. The benchmark includes 14 tasks, ranging from pixellevel to image-level perception, and lowlevel pattern matching (e.g., visual correspondences estimation) to mid-level reasoning (e.g., relative depth estimation), and extending to high-level visual understanding (e.g., visual similarity).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Accuracies of multimodal LLMs on Blink test set. Please refer to Table 1 and $\S 4.2$ for more results and discussions.
granularity, ranging from pixels (e.g., relative reflectance) to patches (e.g., jigsaw) and extending to the full image (e.g., forensic detection), enabling us to evaluate models' proficiency in observing at various scales.</p>
<p>To facilitate the evaluation of multimodal LLMs, we recast all tasks as multiplechoice question-answering problems. The options for answers may include images or texts, while the questions themselves can feature either single or multiple images. Prompts are designed to be both textual and visual in nature. We re-purposed several existing vision datasets as well as collected new data. In total, we contribute 3.9 K multiple-choice questions and 7.3 K images, with an even distribution between the validation and test sets. Numbers of each task are reported in Figure 3, and more detailed statistics can be found in Appendix A.5.</p>
<p>Key features of Blink: Comparing with previous benchmarks, Blink has the following novel features:</p>
<ul>
<li>Visual prompting: Unlike existing benchmarks that support only text prompting, Blink features a variety of visual prompts. This enables one to highlight specific areas within images, facilitating the evaluation of Multimodal LLMs' detailed understanding of these regions. It also offers an interface for researchers to investigate the impact of visual prompting techniques.</li>
<li>
<p>Perception beyond recognition: Besides visual recognition, Blink considers a diverse set of visual perception abilities, such as 3D reasoning, geometric understanding, affordance reasoning, etc. The breadth allows one to evaluate Multimodal LLMs from an unique array of perspectives.</p>
</li>
<li>
<p>"Visual commonsense" that does not require domain knowledge: The questions in Blink are intentionally designed to be straightforward, requiring neither domain-specific knowledge nor expertise to answer. They are crafted in such a way that humans can solve them almost instantaneously, typically within a few seconds. This allows us to explore the fundamental gap in visual perception gap between humans and Multimodal LLMs, highlighting the paradox that problems easily solved by humans often pose significant challenges for machines.</p>
</li>
<li>Interleaved image-text formats: Blink features a heterogeneous questionanswering format, wherein both questions and choices can be presented as text or images. This diversity compels Multimodal LLMs to genuinely understand the questions, pushing the boundaries of their interpretative capabilities.</li>
<li>Diverse image sources: Blink comprises a wide range of in-the-wild images sourced from various origins, covering everything from indoor and outdoor scenes to object-centric views and landscapes. This collection spans abstract diagrams, synthesized images, and authentic photographs, ensuring a comprehensive examination of visual perception</li>
</ul>
<p>The design principles of Blink are also illustrated in Figure 2. We will now describe each task in detail.</p>
<h1>3.2 Dataset Collection Process</h1>
<p>Blink comprises 14 tasks, all of which have been repurposed into a multiple-choice question-answering format. These tasks utilize a diverse collection of images from various sources, and we ensure that each test sample across all tasks features unique images.
Visual correspondence: This task aims to evaluate the ability of Multimodal LLMs to understand and identify the same scene point across various viewpoints, lighting conditions, or time. We exploit HPatches [8] for this task. HPatches contains a number of image sequences, each of which are composed of images taken under different illuminations and/or viewpoints of a scene. For each question, we randomly sample two images and an interest point within them. Then we exploit the ground-truth homography to compute its correspondence. Finally, we randomly select three more interest points to serves as other choices.
Relative reflectance: This task aims to compare the reflectance (albedo) of two pixels. It allows us to evaluate Multimodal LLMs' understanding of material properties and their interaction with light, which is crucial for applications requiring high-fidelity visual interpretations. We curate our samples using human annotations from the Intrinsic Images in the Wild (IIW) dataset [10]. Each question is based on an image and two specified points, with the objective being to identify which point is darker, or whether the two points have similar reflectance.
Relative depth: Humans are good at judging relative depth [19]. This task can thus serve as a proxy to validate if the geometric understanding capabilities</p>
<p>of existing multimodal LLMs are close to human. We curate our samples using human annotations from the Depth in the Wild [19] dataset. Each question contains an image and two specified points. The task is to determine which point is closer.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Qualitative results on Blink. For each task, we show the choice of LLaVA-v1.6-34B [50], Qwen-VL-Max [7], Gemini Pro [71], GPT-4V [62], and humans. Red choice indicates the ground truth. Notice that the markers are intentionally enlarged for visualization purposes, and we make some images inset images to save space. For IQ test, the third image is constructed by overlaying the first and second images.</p>
<p>Spatial relation: Understanding spatial relationships between objects in a scene is essential for interpreting complex visual environments. However, modern Multimodal LLMs often struggles with spatial concepts such as "left" and "right" [85]. This task help us evaluate whether the models finally possess this vital skill. We curate our samples from the Visual Spatial Reasoning [47] dataset. Each sample contains an image and a claim. The task is to determine if the claim is true or false. We reformat the claims into binary questions via GPT-3.5 [13].
Multi-view reasoning: This task is centered on evaluating the multi-view reasoning capabilities of Multimodal LLMs. The objective is to deduce the relative camera motion based on two images of an object captured from different viewpoints. Our data is sourced from the Wild6D dataset [88], which features videos of various objects recorded in diverse settings. We select two random frames from each video to calculate the relative camera motion. Recognizing that even humans might struggle to precisely articulate 3D motion details, we simplify the task by classifying motions into two broad categories: moving towards the left or moving towards the right. Despite the simplicity of these questions, as we will later demonstrate, they pose significant challenges for current models.</p>
<p>Jigsaw: This task assesses the ability of Multimodal LLMs to recognize and group patterns, as well as to align patches based on continuity in shape, color, and texture. We utilize images from the TARA dataset [31] and segment each of them into a $3 \times 3$ grid. We retain the three segments from the upper left corner as the reference image, and treat the central segment along with a randomly chosen segment as options. The objective is to identify the correct patch (i.e., the central patch).
Art style: This task evaluates Multimodal LLMs capability to analyze and discern both local and global similarities in art styles among multiple images. Although there have been prior efforts to incorporate art-related questions into evaluation [87], such attempts primarily focused on questions requiring expertlevel knowledge, including deducing an artist's name and understanding historical contexts, rather than on direct image comparison. For this task, we collect paintings and their stylistic information from WikiArt. Given one reference painting image and two other paintings as options, the model is tasked with identifying the one that most closely shares the art style of the reference painting.
Object localization: The ability to accurately detect and localize objects is critical for scene understanding. While previous benchmarks [52] have explored this task, their focus was primarily on coarse localization. For instance, they might only ask the model if an object is located at the "top" or "right" side of an image. Blink, in contrast, aims for a more fine-grained evaluation. We exploit images from LVIS [35], randomly sampling one object per image along with its ground-truth bounding box. Then we add Gaussian noise to the ground-truth box to create a confounding box. The goal is to select the correct one.
Counting: This task evaluate Multimodal LLMs' abilities in detection, recognition, and compositional reasoning, particularly in complex scenes where objects may overlap, be occluded, or vary in size and appearance. We select our questions</p>
<p>from the TallyQA dataset [2], known for its challenging human-written counting questions. Each sample comprises an image, a question, and a numerical answer. In addition to the correct answer, we randomly select three numbers to serve as confounding options.</p>
<p>Forensic detection: Recent advances in generative AI have raised concerns about malicious uses and have prompted calls for the automatic detection of fake content. To evaluate whether Multimodal LLMs can fulfill such a role, we construct sets of real and synthesized images that describe similar scenes and ask the models to identify the real ones. Specifically, we first generate synthetic images using Stable Diffusion XL [63], employing COCO captions [46] as prompts. Then, we manually search online using these captions as descriptions and select high-quality photographs as the real images.</p>
<p>IQ test: This task evaluates the ability of Multimodal LLMs to engage in graphical reasoning, without requiring any domain-specific knowledge. We manually collect test samples, along with human explanations, from various public, license-friendly online sources. Given visual examples and a selection of images, the objective is to identify the image that either continues the pattern established by the examples or is spatially consistent with them.</p>
<p>Visual similarity: This task aims to verify whether Multimodal LLMs possess a nuanced understanding of visual features, patterns, and aesthetics at a level comparable to humans. We select our samples from the DreamSim dataset [29]. Given a reference image alongside two alternative images, the objective is to identify the image that most closely resembles the reference image in terms of visual similarity.</p>
<p>Semantic correspondence: This task focuses on identifying and matching semantically similar yet visually distinct elements across images, thereby evaluating the ability of Multimodal LLMs to understand the underlying semantics of object parts. Our samples are sourced from the SPair-71k dataset [60], which features pairs of images with multiple corresponding semantic points. For each task, we randomly select one semantic point in an image as a reference, and provide the matching point alongside three random semantic points in the paired image as options. The objective is to accurately identify the correct matches.</p>
<p>Functional correspondence: The task aims to identify points that are functionally similar across objects. It challenges Multimodal LLMs to extend their understanding beyond mere semantics, enabling them to infer the diverse functions an object can perform in various contexts. Such capability is crucial for applications in robotics. We derive our samples from the FunKPoint dataset [42], which features paired images annotated for functional correspondences. Following a method analogous to semantic correspondence, we present an action alongside two object images. One image includes a reference point, while the other offers four potential points. The objective is to select the point that best matches the reference in terms of functional affordances.</p>
<p>Data quality control: To guarantee the quality of Blink, we manually go through all collected data and filter out data that are ambiguous.</p>
<h1>4 Experiments</h1>
<p>In this section, we first describe the experimental setup and the baselines (4.1). Then we present a comprehensive evaluation of 16 recent multimodal LLMs (4.2). We demonstrate that while humans can answer the questions with high accuracy, Blink is challenging for existing models. Finally, we provide detailed analyses on multiple experimental settings, including the effect of reducing images to captions, sensitivity to different visual prompts, and error analysis (4.3).</p>
<h3>4.1 Experimental Setup</h3>
<p>Multimodal LLMs: We evaluate Blink on 16 recent multimodal LLMs, including MiniGPT-4-v2 [16], OpenFlamingo-v2 [5], InstructBLIP (7B and 13B) [24], CogVLM [77], LLaVA(v1, v1.5, v1.6, internLM, and xtuner versions, model size 7B, 13B, and 34B) [23, 26, 49-51], Yi-VL (6B and 34B) ${ }^{2}$, Qwen-VL-MAX [7], Gemini Pro [71], Claude 3 Opus [1] and GPT-4V(vision) [62]. See Appendix B for more details.</p>
<p>Evaluation setup: We follow standard setups as in the VLMEvalKit [22], where the temperature is set to 0 and retry is set to 10 . However, we do not resize the images during any experiment. For the models that do not support multiple images as input, we concatenate the images as input. We extract the choice from the models' output with a set of pre-defined rules and GPT-3.5-turbo [13]. We refer the readers to Appendix A for more details on visual prompting, how we generate the answers in Blink, and the human evaluation protocol.</p>
<h3>4.2 Main Results</h3>
<p>Overall performance: As shown in Table 1, the mean accuracy of 7B and 13B open-source Multimodal LLMs hover around $35-42 \%$, which is similar to random guess ( $38.09 \%$ ). The most proficient open-source model, LLaVA-v1.6-34B, achieves an accuracy of $45.05 \%$. Even the most advanced models, GPT-4V and Gemini Pro and Claude 3 OPUS, achieve accuracies of only $51.26 \%, 45.72 \%$, and $44.11 \%$ respectively. Their performance are merely $13.17 \%, 7.63 \%$ and $6.02 \%$ better than random guessing and lag behind human performance by $44.44 \%, 49.98 \%$ and $51.59 \%$. Notably, for certain tasks such as jigsaw, semantic correspondence, multiview reasoning, object localization, and relative reflectance, some multimodal LLMs even underperform compared to random guessing. Some qualitative results are shown in Figure 5.</p>
<p>In which tasks do multimodal LLMs show relative strengths and weaknesses? Figure 4 shows the accuracies of the best-performing models on Blink: LLaVA-v1.6-34B [50], Gemini Pro [71], and GPT-4V [62]. We observe that multimodal LLMs perform relatively better on spatial reasoning, art style, and counting tasks, in which they are much better than random guessing. The</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Validation <br> $(1,901)$</th>
<th style="text-align: center;">Test <br> $(1,906)$</th>
<th style="text-align: center;">Similarity <br> $(136)$</th>
<th style="text-align: center;">Counting <br> $(120)$</th>
<th style="text-align: center;">Depth <br> $(124)$</th>
<th style="text-align: center;">Jigsaw <br> $(150)$</th>
<th style="text-align: center;">Art <br> $(117)$</th>
<th style="text-align: center;">Fun.Corr. <br> $(130)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random Choice</td>
<td style="text-align: center;">38.09</td>
<td style="text-align: center;">38.09</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: center;">95.67</td>
<td style="text-align: center;">95.70</td>
<td style="text-align: center;">96.70</td>
<td style="text-align: center;">93.75</td>
<td style="text-align: center;">99.19</td>
<td style="text-align: center;">99.00</td>
<td style="text-align: center;">95.30</td>
<td style="text-align: center;">80.77</td>
</tr>
<tr>
<td style="text-align: left;">Open-source multimodal LLMs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">MiniGPT-4-v2 [16]</td>
<td style="text-align: center;">34.23</td>
<td style="text-align: center;">34.57</td>
<td style="text-align: center;">52.94</td>
<td style="text-align: center;">10.83</td>
<td style="text-align: center;">49.19</td>
<td style="text-align: center;">26.00</td>
<td style="text-align: center;">47.86</td>
<td style="text-align: center;">18.46</td>
</tr>
<tr>
<td style="text-align: left;">OpenFlamingo-v2 [5]</td>
<td style="text-align: center;">39.18</td>
<td style="text-align: center;">38.32</td>
<td style="text-align: center;">55.15</td>
<td style="text-align: center;">21.67</td>
<td style="text-align: center;">54.03</td>
<td style="text-align: center;">46.00</td>
<td style="text-align: center;">52.14</td>
<td style="text-align: center;">36.15</td>
</tr>
<tr>
<td style="text-align: left;">InstructBLIP-7B [24]</td>
<td style="text-align: center;">39.72</td>
<td style="text-align: center;">38.65</td>
<td style="text-align: center;">46.32</td>
<td style="text-align: center;">29.17</td>
<td style="text-align: center;">50.81</td>
<td style="text-align: center;">54.00</td>
<td style="text-align: center;">47.86</td>
<td style="text-align: center;">23.85</td>
</tr>
<tr>
<td style="text-align: left;">InstructBLIP-13B [24]</td>
<td style="text-align: center;">42.24</td>
<td style="text-align: center;">39.58</td>
<td style="text-align: center;">46.32</td>
<td style="text-align: center;">30.83</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">54.00</td>
<td style="text-align: center;">50.43</td>
<td style="text-align: center;">22.31</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-internLM2-7B [72]</td>
<td style="text-align: center;">37.71</td>
<td style="text-align: center;">36.06</td>
<td style="text-align: center;">52.94</td>
<td style="text-align: center;">52.50</td>
<td style="text-align: center;">52.42</td>
<td style="text-align: center;">34.67</td>
<td style="text-align: center;">30.77</td>
<td style="text-align: center;">23.08</td>
</tr>
<tr>
<td style="text-align: left;">Yi-VL-6B ${ }^{2}$</td>
<td style="text-align: center;">38.72</td>
<td style="text-align: center;">41.24</td>
<td style="text-align: center;">46.32</td>
<td style="text-align: center;">46.67</td>
<td style="text-align: center;">56.45</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">53.85</td>
<td style="text-align: center;">23.85</td>
</tr>
<tr>
<td style="text-align: left;">Yi-VL-34B ${ }^{2}$</td>
<td style="text-align: center;">41.68</td>
<td style="text-align: center;">42.78</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">58.33</td>
<td style="text-align: center;">53.23</td>
<td style="text-align: center;">54.00</td>
<td style="text-align: center;">46.15</td>
<td style="text-align: center;">39.23</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-v1.5-7B-xtuner [23]</td>
<td style="text-align: center;">39.36</td>
<td style="text-align: center;">40.81</td>
<td style="text-align: center;">46.32</td>
<td style="text-align: center;">53.33</td>
<td style="text-align: center;">50.81</td>
<td style="text-align: center;">54.00</td>
<td style="text-align: center;">47.86</td>
<td style="text-align: center;">23.85</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-v1.5-13B-xtuner [23]</td>
<td style="text-align: center;">42.00</td>
<td style="text-align: center;">41.31</td>
<td style="text-align: center;">46.32</td>
<td style="text-align: center;">45.00</td>
<td style="text-align: center;">54.03</td>
<td style="text-align: center;">53.33</td>
<td style="text-align: center;">47.86</td>
<td style="text-align: center;">26.15</td>
</tr>
<tr>
<td style="text-align: left;">CogVLM [77]</td>
<td style="text-align: center;">41.54</td>
<td style="text-align: center;">39.38</td>
<td style="text-align: center;">46.32</td>
<td style="text-align: center;">38.33</td>
<td style="text-align: center;">50.81</td>
<td style="text-align: center;">52.67</td>
<td style="text-align: center;">49.57</td>
<td style="text-align: center;">23.85</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-v1.5-7B [48]</td>
<td style="text-align: center;">37.13</td>
<td style="text-align: center;">38.01</td>
<td style="text-align: center;">46.32</td>
<td style="text-align: center;">43.33</td>
<td style="text-align: center;">51.61</td>
<td style="text-align: center;">11.33</td>
<td style="text-align: center;">47.86</td>
<td style="text-align: center;">21.54</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-v1.5-13B [48]</td>
<td style="text-align: center;">42.66</td>
<td style="text-align: center;">40.55</td>
<td style="text-align: center;">46.32</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">47.58</td>
<td style="text-align: center;">54.00</td>
<td style="text-align: center;">47.86</td>
<td style="text-align: center;">20.77</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-v1.6-34B [50]</td>
<td style="text-align: center;">46.80</td>
<td style="text-align: center;">45.05</td>
<td style="text-align: center;">46.32</td>
<td style="text-align: center;">68.33</td>
<td style="text-align: center;">64.52</td>
<td style="text-align: center;">56.67</td>
<td style="text-align: center;">47.01</td>
<td style="text-align: center;">30.77</td>
</tr>
<tr>
<td style="text-align: left;">API-based models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Qwen-VL-Max [7]</td>
<td style="text-align: center;">40.28</td>
<td style="text-align: center;">41.94</td>
<td style="text-align: center;">51.47</td>
<td style="text-align: center;">55.83</td>
<td style="text-align: center;">58.87</td>
<td style="text-align: center;">3.33</td>
<td style="text-align: center;">37.61</td>
<td style="text-align: center;">28.46</td>
</tr>
<tr>
<td style="text-align: left;">Gemini Pro [71]</td>
<td style="text-align: center;">45.16</td>
<td style="text-align: center;">45.72</td>
<td style="text-align: center;">55.88</td>
<td style="text-align: center;">65.00</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">54.00</td>
<td style="text-align: center;">49.57</td>
<td style="text-align: center;">32.31</td>
</tr>
<tr>
<td style="text-align: left;">Claude 3 OPUS [1]</td>
<td style="text-align: center;">44.05</td>
<td style="text-align: center;">44.11</td>
<td style="text-align: center;">70.59</td>
<td style="text-align: center;">49.17</td>
<td style="text-align: center;">57.26</td>
<td style="text-align: center;">32.67</td>
<td style="text-align: center;">60.68</td>
<td style="text-align: center;">22.31</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4V(ision) [62]</td>
<td style="text-align: center;">51.14</td>
<td style="text-align: center;">51.26</td>
<td style="text-align: center;">83.09</td>
<td style="text-align: center;">60.83</td>
<td style="text-align: center;">58.87</td>
<td style="text-align: center;">62.67</td>
<td style="text-align: center;">78.63</td>
<td style="text-align: center;">31.54</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 Turbo [62]</td>
<td style="text-align: center;">54.61</td>
<td style="text-align: center;">53.89</td>
<td style="text-align: center;">83.09</td>
<td style="text-align: center;">60.83</td>
<td style="text-align: center;">66.94</td>
<td style="text-align: center;">66.00</td>
<td style="text-align: center;">81.20</td>
<td style="text-align: center;">31.54</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o [62]</td>
<td style="text-align: center;">60.04</td>
<td style="text-align: center;">59.03</td>
<td style="text-align: center;">65.44</td>
<td style="text-align: center;">51.67</td>
<td style="text-align: center;">64.52</td>
<td style="text-align: center;">58.00</td>
<td style="text-align: center;">82.91</td>
<td style="text-align: center;">39.23</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Sem.Corr.</td>
<td style="text-align: center;">Spatial</td>
<td style="text-align: center;">Local.</td>
<td style="text-align: center;">Vis.Corr.</td>
<td style="text-align: center;">Multi-view</td>
<td style="text-align: center;">Reflect.</td>
<td style="text-align: center;">Forensic</td>
<td style="text-align: center;">IQ</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">(140)</td>
<td style="text-align: center;">(143)</td>
<td style="text-align: center;">(125)</td>
<td style="text-align: center;">(172)</td>
<td style="text-align: center;">(133)</td>
<td style="text-align: center;">(134)</td>
<td style="text-align: center;">(132)</td>
<td style="text-align: center;">(150)</td>
</tr>
<tr>
<td style="text-align: left;">Random Choice</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: center;">96.07</td>
<td style="text-align: center;">98.25</td>
<td style="text-align: center;">98.00</td>
<td style="text-align: center;">99.42</td>
<td style="text-align: center;">92.48</td>
<td style="text-align: center;">95.14</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">80.00</td>
</tr>
<tr>
<td style="text-align: left;">Open-source multimodal LLMs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">MiniGPT-4-v2 [16]</td>
<td style="text-align: center;">26.43</td>
<td style="text-align: center;">51.75</td>
<td style="text-align: center;">56.00</td>
<td style="text-align: center;">23.84</td>
<td style="text-align: center;">52.63</td>
<td style="text-align: center;">31.34</td>
<td style="text-align: center;">17.42</td>
<td style="text-align: center;">19.33</td>
</tr>
<tr>
<td style="text-align: left;">OpenFlamingo-v2 [5]</td>
<td style="text-align: center;">23.57</td>
<td style="text-align: center;">46.85</td>
<td style="text-align: center;">52.00</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">41.35</td>
<td style="text-align: center;">43.28</td>
<td style="text-align: center;">15.91</td>
<td style="text-align: center;">23.33</td>
</tr>
<tr>
<td style="text-align: left;">InstructBLIP-7B [24]</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">55.24</td>
<td style="text-align: center;">44.80</td>
<td style="text-align: center;">22.67</td>
<td style="text-align: center;">58.65</td>
<td style="text-align: center;">29.85</td>
<td style="text-align: center;">29.55</td>
<td style="text-align: center;">23.33</td>
</tr>
<tr>
<td style="text-align: left;">InstructBLIP-13B [24]</td>
<td style="text-align: center;">22.86</td>
<td style="text-align: center;">64.34</td>
<td style="text-align: center;">52.00</td>
<td style="text-align: center;">20.93</td>
<td style="text-align: center;">54.14</td>
<td style="text-align: center;">46.27</td>
<td style="text-align: center;">13.64</td>
<td style="text-align: center;">26.00</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-internLM2-7B [72]</td>
<td style="text-align: center;">22.14</td>
<td style="text-align: center;">74.13</td>
<td style="text-align: center;">48.00</td>
<td style="text-align: center;">21.51</td>
<td style="text-align: center;">41.35</td>
<td style="text-align: center;">32.84</td>
<td style="text-align: center;">3.79</td>
<td style="text-align: center;">14.67</td>
</tr>
<tr>
<td style="text-align: left;">Yi-VL-6B ${ }^{2}$</td>
<td style="text-align: center;">26.43</td>
<td style="text-align: center;">72.73</td>
<td style="text-align: center;">49.60</td>
<td style="text-align: center;">29.65</td>
<td style="text-align: center;">48.12</td>
<td style="text-align: center;">29.85</td>
<td style="text-align: center;">20.45</td>
<td style="text-align: center;">23.33</td>
</tr>
<tr>
<td style="text-align: left;">Yi-VL-34B ${ }^{2}$</td>
<td style="text-align: center;">21.43</td>
<td style="text-align: center;">70.63</td>
<td style="text-align: center;">54.40</td>
<td style="text-align: center;">23.84</td>
<td style="text-align: center;">41.35</td>
<td style="text-align: center;">46.27</td>
<td style="text-align: center;">17.42</td>
<td style="text-align: center;">22.67</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-v1.5-7B-xtuner [23]</td>
<td style="text-align: center;">24.29</td>
<td style="text-align: center;">74.83</td>
<td style="text-align: center;">45.60</td>
<td style="text-align: center;">23.84</td>
<td style="text-align: center;">42.11</td>
<td style="text-align: center;">26.87</td>
<td style="text-align: center;">36.36</td>
<td style="text-align: center;">21.33</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-v1.5-13B-xtuner [23]</td>
<td style="text-align: center;">22.14</td>
<td style="text-align: center;">77.62</td>
<td style="text-align: center;">48.00</td>
<td style="text-align: center;">22.09</td>
<td style="text-align: center;">41.35</td>
<td style="text-align: center;">46.27</td>
<td style="text-align: center;">29.55</td>
<td style="text-align: center;">18.67</td>
</tr>
<tr>
<td style="text-align: left;">CogVLM [77]</td>
<td style="text-align: center;">23.57</td>
<td style="text-align: center;">67.13</td>
<td style="text-align: center;">43.20</td>
<td style="text-align: center;">20.93</td>
<td style="text-align: center;">57.14</td>
<td style="text-align: center;">26.87</td>
<td style="text-align: center;">24.24</td>
<td style="text-align: center;">26.67</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-v1.5-7B [48]</td>
<td style="text-align: center;">32.14</td>
<td style="text-align: center;">70.63</td>
<td style="text-align: center;">48.80</td>
<td style="text-align: center;">20.35</td>
<td style="text-align: center;">49.62</td>
<td style="text-align: center;">36.57</td>
<td style="text-align: center;">28.03</td>
<td style="text-align: center;">24.00</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-v1.5-13B [48]</td>
<td style="text-align: center;">23.57</td>
<td style="text-align: center;">67.83</td>
<td style="text-align: center;">47.20</td>
<td style="text-align: center;">20.35</td>
<td style="text-align: center;">41.35</td>
<td style="text-align: center;">45.52</td>
<td style="text-align: center;">27.27</td>
<td style="text-align: center;">28.00</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-v1.6-34B [50]</td>
<td style="text-align: center;">27.86</td>
<td style="text-align: center;">76.22</td>
<td style="text-align: center;">41.60</td>
<td style="text-align: center;">27.33</td>
<td style="text-align: center;">46.62</td>
<td style="text-align: center;">29.85</td>
<td style="text-align: center;">41.67</td>
<td style="text-align: center;">26.00</td>
</tr>
<tr>
<td style="text-align: left;">API-based models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Qwen-VL-Max [7]</td>
<td style="text-align: center;">29.29</td>
<td style="text-align: center;">77.62</td>
<td style="text-align: center;">49.60</td>
<td style="text-align: center;">22.67</td>
<td style="text-align: center;">53.38</td>
<td style="text-align: center;">49.25</td>
<td style="text-align: center;">47.73</td>
<td style="text-align: center;">22.00</td>
</tr>
<tr>
<td style="text-align: left;">Gemini Pro [71]</td>
<td style="text-align: center;">22.14</td>
<td style="text-align: center;">67.13</td>
<td style="text-align: center;">46.40</td>
<td style="text-align: center;">37.21</td>
<td style="text-align: center;">41.35</td>
<td style="text-align: center;">46.27</td>
<td style="text-align: center;">45.45</td>
<td style="text-align: center;">27.33</td>
</tr>
<tr>
<td style="text-align: left;">Claude 3 OPUS [1]</td>
<td style="text-align: center;">20.71</td>
<td style="text-align: center;">57.34</td>
<td style="text-align: center;">46.40</td>
<td style="text-align: center;">31.40</td>
<td style="text-align: center;">57.89</td>
<td style="text-align: center;">27.61</td>
<td style="text-align: center;">62.12</td>
<td style="text-align: center;">21.33</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4V(ision) [62]</td>
<td style="text-align: center;">30.00</td>
<td style="text-align: center;">72.03</td>
<td style="text-align: center;">50.40</td>
<td style="text-align: center;">37.21</td>
<td style="text-align: center;">58.65</td>
<td style="text-align: center;">38.81</td>
<td style="text-align: center;">30.30</td>
<td style="text-align: center;">24.67</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 Turbo [62]</td>
<td style="text-align: center;">32.86</td>
<td style="text-align: center;">67.13</td>
<td style="text-align: center;">48.80</td>
<td style="text-align: center;">42.44</td>
<td style="text-align: center;">57.14</td>
<td style="text-align: center;">34.33</td>
<td style="text-align: center;">51.52</td>
<td style="text-align: center;">30.67</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o [62]</td>
<td style="text-align: center;">45.71</td>
<td style="text-align: center;">76.92</td>
<td style="text-align: center;">56.00</td>
<td style="text-align: center;">71.51</td>
<td style="text-align: center;">60.15</td>
<td style="text-align: center;">38.81</td>
<td style="text-align: center;">85.61</td>
<td style="text-align: center;">30.00</td>
</tr>
</tbody>
</table>
<p>Table 1: Results of different models on the Blink test set. The first row shows task names and number of test data. The best performance in each task is in-bold. For the sake of completion, we also show the average score on the validation set. Detailed scores on the validation set are in Appendix C.
models also demonstrate some capability in relative depth and forensics detection. Overall, they are doing relatively well on mid-level perception tasks. In terms</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Performance of using image caption + text-only GPT-4 es. GPT-4 Vision on MMBench [52], MMMU [87], and Blink (4.3).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Accuracy of GPT-4V with different visual prompts (e.g., different circle sizes, colors) on relative depth, relative reflectance, and visual correspondence tasks. More discussions in $\S 4.3$.
of granularity, the models in general perform better on image-level tasks and struggle on pixel-level and crop-level tasks.
GPT-4V behaves differently: Figure 4 and Table 1 show an interesting phenomenon: GPT-4V's performance pattern is different from other models. Compared with its counterparts, GPT-4V is much better in visual similarity, art style, jigsaw, and multi-view reasoning. Specifically, its performance on visual similarity is $29 \%$ better than Gemini Pro, demonstrating that GPT-4V possesses a nuanced understanding of visual patterns and aesthetics that is similar to humans. In contrast, Gemini Pro and LLaVA have similar performance patterns.
Human performance: Human evaluators achieve over $95 \%$ accuracy across most tasks, with an average accuracy of $95.70 \%$. ${ }^{3}$ This performance disparity between humans and multimodal LLMs highlights the significant visual perception gap that exists between current machine learning models and humans in perceiving, processing, and understanding complex visual and textual context.</p>
<h1>4.3 Analysis</h1>
<p>Is dense captioning all you need for a multimodal LLM benchmark? To answer the question, we reduce multimodal benchmarks to a text-only problem. Specifically, we convert images into task-agnostic dense image captions with GPT-4V. The dense caption describes detailed information about the image and the visual prompts (e.g., where each circle is), using language. For each multimodal question, we prompt the text-only GPT-4-0125-preview model with image captions and the textual question and evaluate if the "blind" GPT-4 can answer the question. We call this Caption + LLM. This experiment is predicated on the hypothesis that captioning involves predominantly recognition-centric perception. If using captions along with text-only LLMs yields performance comparable to or surpassing that achieved through the integration of images with multimodal LLMs, then the perception demands of that benchmark are primarily confined to recognition only.</p>
<p>We experiment with Blink, MMBench [52] and MMMU [87], as illustrated in Figure 6. Surprisingly, we find that the Caption + LLM setting achieves better</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: center;">Vis.Corr.</th>
<th style="text-align: center;">Depth</th>
<th style="text-align: center;">Multi-view</th>
<th style="text-align: center;">Sem.Corr.</th>
<th style="text-align: center;">Forensic</th>
<th style="text-align: center;">Reflect.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">33.33</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: center;">99.56</td>
<td style="text-align: center;">99.59</td>
<td style="text-align: center;">92.10</td>
<td style="text-align: center;">94.60</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">99.63</td>
</tr>
<tr>
<td style="text-align: left;">Gemini Pro</td>
<td style="text-align: center;">42.44</td>
<td style="text-align: center;">40.32</td>
<td style="text-align: center;">44.36</td>
<td style="text-align: center;">26.62</td>
<td style="text-align: center;">50.76</td>
<td style="text-align: center;">45.52</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4V</td>
<td style="text-align: center;">33.72</td>
<td style="text-align: center;">59.68</td>
<td style="text-align: center;">55.64</td>
<td style="text-align: center;">28.78</td>
<td style="text-align: center;">34.09</td>
<td style="text-align: center;">38.81</td>
</tr>
<tr>
<td style="text-align: left;">Specialist</td>
<td style="text-align: center;">DIFT [70]</td>
<td style="text-align: center;">DepthAnything [83]</td>
<td style="text-align: center;">LoFTR [68]</td>
<td style="text-align: center;">DIFT [70]</td>
<td style="text-align: center;">DIRE [79]</td>
<td style="text-align: center;">Ordinal Shading [14]</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">96.51</td>
<td style="text-align: center;">97.58</td>
<td style="text-align: center;">90.22</td>
<td style="text-align: center;">71.22</td>
<td style="text-align: center;">68.94</td>
<td style="text-align: center;">77.61</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison between multimodal LLMs, specialists, and human performance on the Blink dev set. The specialists perform much better than multimodal LLMs.
results on MMBench than GPT-4V (with $5.7 \%$ increase in accuracy). On MMMU, Caption + LLM achieves $47.2 \%$ accuracy, which is $9.6 \%$ lower than GPT-4V performance, but is still much better than random guessing. On Blink, Caption + LLM fails, achieving random guessing performance. These results indicate that dense captions cover the visual information needed for MMBench. For MMMU, image captions carry a large portion of visual information needed to answer the domain-knowledge-specific questions. Meanwhile, the performance decrease observed in Blink suggests the necessity for advanced perceptual abilities beyond what is currently attainable with general captions. This variance highlights the limitations of existing multimodal LLM benchmarks in addressing the full spectrum of visual perception.</p>
<p>Effect of visual prompting on Blink: Several Blink tasks involve visual prompting. Prior work [67] shows that factors like shape, size, and color may affect task performance, and circles give the best overall performance. Following [67], we adopt circles in Blink and analyze the effect of circle sizes and colors on multiple tasks in Figure 7. We experiment with relative depth, relative reflectance, and visual correspondence, with 100 validation set samples per task. The images are all reshaped to 1024 px height. We experiment with circles with $5 \mathrm{px}, 10 \mathrm{px}$, and 15 px radius, and with red or gray color. We find that red is better than gray for all tasks. Also, the optimal circle size is task-dependent. On average 10 px circles work the best, and we use it for all evaluations in this paper. The experiments suggest that visual prompting can have a big impact on multimodal LLM performance, and improving visual prompts or improving model robustness to prompt variation is a promising direction for future research [82].</p>
<p>Can specialist models solve Blink tasks? Specialists can serve as a proxy upper bound of how good multimodal LLMs could be. We download the trained checkpoints for six specialist models and evaluate them on Blink. As shown in Table 2, the specialists perform much better than GPT-4V and Gemini Pro, outperforming the best multimodal LLM by $18 \%$ to $57 \%$ on these tasks. Specifically, DepthAnything [83] and DIFT [70] achieve human-level performance on depth estimation and visual correspondence, whereas multimodal LLMs fail miserably. This sheds light on the possibility that multimodal LLMs may progress on these tasks given the correct data and training strategy. For instance, one possible way is to distill existing specialist models into multimodal LLMs [40].</p>
<p>Error analysis of GPT-4V: We randomly sampled 140 error instances made by GPT-4V on Blink, 10 per task, and meticulously examined them. The most common types of errors are: Hallucinate fine-grained patterns and attributes ( $24.2 \%$ ): the model hallucinates the nuanced details of objects. This error is most common for relative reflectance, forensics detection, and jigsaw tasks. Hallucinate visual prompt locations ( $20.0 \%$ ): the circle location described by the model is wrong. This is common for visual correspondence and relative depth tasks. Other errors include Failures on capturing overall setting or style ( $8.6 \%$ ), and Failures on grounding an object ( $5.7 \%$ ). More details are in Appendix C.3.</p>
<h1>5 Conclusion</h1>
<p>We introduced Blink, a new multimodal LLM benchmark that evaluates core visual perception abilities not found in existing evaluations. While these tasks seem trivial for humans to solve "within a blink", we find they pose significant challenges for current multimodal LLMs. Even the powerful GPT-4V and Gemini models only achieve around $50 \%$ accuracy on Blink, far below the $95.7 \%$ human performance. We conduct extensive analysis, measuring the effect of converting images to dense captions, visual prompting, self-consistency, analyzing the capabilities of specialist models, and conducting error analysis. We highlight that specialist computer vision models are performing much better than GPT-4V and Gemini on Blink, shedding light on the possibility that multimodal LLMs may have big progress on these tasks. Ultimately, Blink provides a simple yet effective testbed for multimodal LLMs to catch up with human-level visual perception.</p>
<h2>References</h2>
<ol>
<li>Introducing the next generation of claude. https://www.anthropic.com/news/ claude-3-family (March 2024) 11, 12, 23, 24</li>
<li>Acharya, M., Kafle, K., Kanan, C.: Tallyqa: Answering complex counting questions. In: AAAI (2019) 10</li>
<li>Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems 35, $23716-23736$ (2022) $2,4,22$</li>
<li>Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.: Vqa: Visual question answering. In: Proceedings of the IEEE international conference on computer vision. pp. 2425-2433 (2015) 4</li>
<li>Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa, S., Jitsev, J., Kornblith, S., Koh, P.W., Ilharco, G., Wortsman, M., Schmidt, L.: Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390 (2023) $11,12,22,24$</li>
<li>Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond (2023) 2</li>
<li>
<p>Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966 (2023) 4, 8, 11, 12, 23, 24</p>
</li>
<li>
<p>Balntas, V., Lenc, K., Vedaldi, A., Mikolajczyk, K.: Hpatches: A benchmark and evaluation of handcrafted and learned local descriptors. In: CVPR (2017) 3, 7</p>
</li>
<li>Barrow, H., Tenenbaum, J., Hanson, A., Riseman, E.: Recovering intrinsic scene characteristics. Comput. vis. syst 2(3-26), 2 (1978) 2</li>
<li>Bell, S., Bala, K., Snavely, N.: Intrinsic images in the wild. ACM Trans. on Graphics (SIGGRAPH) 33(4) (2014) 3, 7</li>
<li>Berrios, W., Mittal, G., Thrush, T., Kiela, D., Singh, A.: Towards language models that can see: Computer vision through the lens of natural language. arXiv preprint arXiv:2306.16410 (2023) 2</li>
<li>Black, M.J., Anandan, P.: A framework for the robust estimation of optical flow. In: 1993 (4th) International Conference on Computer Vision. pp. 231-236. IEEE (1993) 2</li>
<li>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems 33, 1877-1901 (2020) $4,9,11,21,22$</li>
<li>Careaga, C., Aksoy, Y.: Intrinsic image decomposition via ordinal shading. ACM Trans. Graph. (2023) 14</li>
<li>Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12M: Pushing webscale image-text pre-training to recognize long-tail visual concepts. In: CVPR (2021) 4</li>
<li>Chen, J., Zhu, D., Shen, X., Li, X., Liu, Z., Zhang, P., Krishnamoorthi, R., Chandra, V., Xiong, Y., Elhoseiny, M.: Minigpt-v2: large language model as a unified interface for vision-language multi-task learning (2023) 4, 11, 12, 24</li>
<li>Chen, J., Zhu, D., Shen, X., Li, X., Liu, Z., Zhang, P., Krishnamoorthi, R., Chandra, V., Xiong, Y., Elhoseiny, M.: Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478 (2023) 22</li>
<li>Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.: Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793 (2023) 2</li>
<li>Chen, W., Fu, Z., Yang, D., Deng, J.: Single-image depth perception in the wild. Advances in neural information processing systems 29 (2016) 3, 7, 8</li>
<li>Chen, X., Djolonga, J., Padlewski, P., Mustafa, B., Changpinyo, S., Wu, J., Ruiz, C.R., Goodman, S., Wang, X., Tay, Y., Shakeri, S., Dehghani, M., Salz, D., Lucic, M., Tschannen, M., Nagrani, A., Hu, H., Joshi, M., Pang, B., Montgomery, C., Pietrzyk, P., Ritter, M., Piergiovanni, A., Minderer, M., Pavetic, F., Waters, A., Li, G., Alabdulmohsin, I., Beyer, L., Amelot, J., Lee, K., Steiner, A.P., Li, Y., Keysers, D., Arnab, A., Xu, Y., Rong, K., Kolesnikov, A., Seyedhosseini, M., Angelova, A., Zhai, X., Houlsby, N., Soricut, R.: Pali-x: On scaling up a multilingual vision and language model (2023) 2</li>
<li>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., Fiedel, N.: Palm: Scaling language modeling with pathways (2022) 4</p>
</li>
<li>
<p>Contributors, O.: Opencompass: A universal evaluation platform for foundation models. https://github.com/open-compass/opencompass (2023) 11</p>
</li>
<li>Contributors, X.: Xtuner: A toolkit for efficiently fine-tuning llm. https://github. com/InternLM/xtuner (2023) 11, 12, 23, 24</li>
<li>Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: Instructblip: Towards general-purpose vision-language models with instruction tuning (2023) $2,4,11,12,23,24$</li>
<li>DO CT OR OF, P.E.: MACHINE PERCEPTION OF THREE-DIMENSIONAL, SO LIDS. Ph.D. thesis, MASSACHUSETTS INSTITUTE OF TECHNOLOGY (1961) 2</li>
<li>Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., Zhang, W., Li, Y., Yan, H., Gao, Y., Zhang, X., Li, W., Li, J., Chen, K., He, C., Zhang, X., Qiao, Y., Lin, D., Wang, J.: Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420 (2024) 2, 11</li>
<li>Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., Cao, Y.: Eva: Exploring the limits of masked visual representation learning at scale. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 19358-19369 (2023) 4, 22</li>
<li>Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 (2023) 4</li>
<li>Fu, S., Tamir, N., Sundaram, S., Chai, L., Zhang, R., Dekel, T., Isola, P.: Dreamsim: Learning new dimensions of human visual similarity using synthetic data (2023) 3, 10</li>
<li>Fu, X., Zhang, S., Kwon, G., Perera, P., Zhu, H., Zhang, Y., Li, A.H., Wang, W.Y., Wang, Z., Castelli, V., Ng, P., Roth, D., Xiang, B.: Generate then select: Open-ended visual question answering guided by world knowledge. In: Rogers, A., Boyd-Graber, J., Okazaki, N. (eds.) Findings of the Association for Computational Linguistics: ACL 2023. pp. 2333-2346. Association for Computational Linguistics, Toronto, Canada (Jul 2023). https://doi.org/10.18653/v1/2023.findings-acl.147, https://aclanthology.org/2023.findings-acl. 1474</li>
<li>Fu, X., Zhou, B., Chandratreya, I., Vondrick, C., Roth, D.: There's a time and place for reasoning beyond the image. In: Muresan, S., Nakov, P., Villavicencio, A. (eds.) Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 1138-1149. Association for Computational Linguistics, Dublin, Ireland (May 2022). https://doi.org/10.18653/v1/2022. acl-long.81, https://aclanthology.org/2022.acl-long.81 3, 4, 9</li>
<li>Fu, X., Zhou, B., Chen, S., Yatskar, M., Roth, D.: Interpretable by design visual question answering. arXiv preprint arXiv:2305.14882 (2023) 4</li>
<li>Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2017) 4</li>
<li>Guan, T., Liu, F., Wu, X., Xian, R., Li, Z., Liu, X., Wang, X., Chen, L., Huang, F., Yacoob, Y., Manocha, D., Zhou, T.: Hallusionbench: An advanced diagnostic suite for entangled language hallucination \&amp; visual illusion in large vision-language models (2023) 5</li>
<li>
<p>Gupta, A., Dollar, P., Girshick, R.: Lvis: A dataset for large vocabulary instance segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 5356-5364 (2019) 3, 9</p>
</li>
<li>
<p>Harris, C., Stephens, M., et al.: A combined corner and edge detector. In: Alvey vision conference. vol. 15, pp. 10-5244. Citeseer (1988) 2</p>
</li>
<li>Hartley, R., Zisserman, A.: Multiple view geometry in computer vision. Cambridge university press (2003) 2</li>
<li>Hu, Y., Hua, H., Yang, Z., Shi, W., Smith, N.A., Luo, J.: Promptcap: Prompt-guided task-aware image captioning. arXiv preprint arXiv:2211.09699 (2022) 2, 4</li>
<li>Hu, Y., Liu, B., Kasai, J., Wang, Y., Ostendorf, M., Krishna, R., Smith, N.A.: Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. arXiv preprint arXiv:2303.11897 (2023) 4</li>
<li>Hu, Y., Stretcu, O., Lu, C.T., Viswanathan, K., Hata, K., Luo, E., Krishna, R., Fuxman, A.: Visual program distillation: Distilling tools and programmatic reasoning into vision-language models. arXiv preprint arXiv:2312.03052 (2023) 14</li>
<li>Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision 123, 32-73 (2017) 3, 4</li>
<li>Lai, Z., Purushwalkam, S., Gupta, A.: The functional correspondence problem. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. $15772-15781(2021) 3,10$</li>
<li>Li, B., Ge, Y., Ge, Y., Wang, G., Wang, R., Zhang, R., Shan, Y.: Seed-bench-2: Benchmarking multimodal large language models. arXiv preprint arXiv:2311.17092 (2023) $2,5$</li>
<li>Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., Shan, Y.: Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125 (2023) $2,3,5$</li>
<li>Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023) 4, 23</li>
<li>Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollr, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. pp. 740-755. Springer (2014) 3, 4, 5, 10</li>
<li>Liu, F., Emerson, G., Collier, N.: Visual spatial reasoning. Transactions of the Association for Computational Linguistics 11, 635-651 (2023) 2, 9, 21</li>
<li>Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning (2023) $4,12,23,24$</li>
<li>Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning (2023) 11</li>
<li>Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Lee, Y.J.: Llava-next: Improved reasoning, ocr, and world knowledge (January 2024), https://llava-v1.github. io/blog/2024-01-30-llava-next/ 2, 4, 8, 11, 12, 23, 24</li>
<li>Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. Advances in neural information processing systems 36 (2024) 2, 11</li>
<li>Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., Chen, K., Lin, D.: Mmbench: Is your multi-modal model an all-around player? (2023) $2,3,5,9,13,21$</li>
<li>Liu, Y., Li, Z., Li, H., Yu, W., Huang, M., Peng, D., Liu, M., Chen, M., Li, C., Jin, L., et al.: On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895 (2023) 2</li>
<li>
<p>Liu, Y., Li, Z., Yang, B., Li, C., Yin, X., lin Liu, C., Jin, L., Bai, X.: On the hidden mystery of ocr in large multimodal models (2024) 4</p>
</li>
<li>
<p>Lowe, D.G.: Object recognition from local scale-invariant features. In: Proceedings of the seventh IEEE international conference on computer vision. vol. 2, pp. 1150-1157. Ieee (1999) 2</p>
</li>
<li>Lu, J., Clark, C., Lee, S., Zhang, Z., Khosla, S., Marten, R., Hoiem, D., Kembhavi, A.: Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. arXiv preprint arXiv:2312.17172 (2023) 2</li>
<li>Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.W., Galley, M., Gao, J.: Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255 (2023) 2, 5</li>
<li>Marr, D.: Vision: A computational investigation into the human representation and processing of visual information. MIT press (2010) 2</li>
<li>Marr, D., Poggio, T.: Cooperative computation of stereo disparity: A cooperative algorithm is derived for extracting disparity information from stereo image pairs. Science 194(4262), 283-287 (1976) 2</li>
<li>Min, J., Lee, J., Ponce, J., Cho, M.: Spair-71k: A large-scale benchmark for semantic correspondence. arXiv prepreint arXiv:1908.10543 (2019) 10</li>
<li>Minsky, M., Papert, S.: An introduction to computational geometry. Cambridge tiass., HIT 479(480), 104 (1969) 2</li>
<li>OpenAI: Gpt-4 technical report (2023) $2,4,5,8,11,12,23,24,25$</li>
<li>Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Mller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023) 10</li>
<li>Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021) 4, 22, 23</li>
<li>Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., Komatsuzaki, A.: Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114 (2021) 4</li>
<li>Schwenk, D., Khandelwal, A., Clark, C., Marino, K., Mottaghi, R.: A-okvqa: A benchmark for visual question answering using world knowledge. In: European Conference on Computer Vision. pp. 146-162. Springer (2022) 4</li>
<li>Shtedritski, A., Rupprecht, C., Vedaldi, A.: What does clip know about a red circle? visual prompt engineering for vlms. arXiv preprint arXiv:2304.06712 (2023) 14</li>
<li>Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. CVPR (2021) 14</li>
<li>Sun, Q., Fang, Y., Wu, L., Wang, X., Cao, Y.: Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389 (2023) 4, 23</li>
<li>Tang, L., Jia, M., Wang, Q., Phoo, C.P., Hariharan, B.: Emergent correspondence from image diffusion. arXiv preprint arXiv:2306.03881 (2023) 14</li>
<li>Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023) 2, 4, 8, 11, 12, 23, 24, 25</li>
<li>Team, I.: Internlm: A multilingual language model with progressively enhanced capabilities. https://github.com/InternLM/InternLM (2023) 12, 23, 24</li>
<li>Team, M.N.: Introducing mpt-7b: A new standard for open-source, commercially usable llms (2023), www.mosaicml.com/blog/mpt-7b, accessed: 2023-05-05 22</li>
<li>
<p>Torralba, A., Oliva, A.: Depth estimation from image structure. IEEE Transactions on pattern analysis and machine intelligence 24(9), 1226-1238 (2002) 2</p>
</li>
<li>
<p>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023) 4, 22</p>
</li>
<li>Wang, J.Y., Adelson, E.H.: Layered representation for motion analysis. In: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. pp. 361-366. IEEE (1993) 2</li>
<li>Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., Xu, J., Xu, B., Li, J., Dong, Y., Ding, M., Tang, J.: Cogvlm: Visual expert for pretrained language models (2023) 2, 11, 12, 23, 24</li>
<li>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., Zhou, D.: Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022) 26</li>
<li>Wang, Z., Bao, J., Zhou, W., Wang, W., Hu, H., Chen, H., Li, H.: Dire for diffusiongenerated image detection. arXiv preprint arXiv:2303.09295 (2023) 14</li>
<li>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models (2022) 4</li>
<li>Yan, A., Yang, Z., Wu, J., Zhu, W., Yang, J., Li, L., Lin, K., Wang, J., McAuley, J., Gao, J., et al.: List items one by one: A new data source and learning paradigm for multimodal llms. arXiv preprint arXiv:2404.16375 (2024) 4</li>
<li>Yang, J., Zhang, H., Li, F., Zou, X., Li, C., Gao, J.: Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441 (2023) 14</li>
<li>Yang, L., Kang, B., Huang, Z., Xu, X., Feng, J., Zhao, H.: Depth anything: Unleashing the power of large-scale unlabeled data. In: CVPR (2024) 14</li>
<li>Yang, Z., Gan, Z., Wang, J., Hu, X., Lu, Y., Liu, Z., Wang, L.: An empirical study of gpt-3 for few-shot knowledge-based vqa. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp. 3081-3089 (2022) 2</li>
<li>Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.C., Liu, Z., Wang, L.: The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421 9 (2023) 9</li>
<li>Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., Wang, L.: Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 (2023) 5</li>
<li>Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., Wei, C., Yu, B., Yuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu, Y., Huang, W., Sun, H., Su, Y., Chen, W.: Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502 (2023) $2,3,5,9,13$</li>
<li>Ze, Y., Wang, X.: Category-level 6d object pose estimation in the wild: A semisupervised learning approach and a new dataset. Advances in Neural Information Processing Systems 35, 27469-27483 (2022) 3, 9</li>
<li>Zellers, R., Bisk, Y., Farhadi, A., Choi, Y.: From recognition to cognition: Visual commonsense reasoning. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019) 4</li>
<li>Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al.: Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems 36 (2024) 4, 23</li>
</ol>
<p>In the supplemental materials, Appendix A contains additional details on Blink dataset collection and model inference, Appendix B provides more details of the baseline models, Appendix C includes experimental analyses on Blink, and Appendix D discusses limitations.</p>
<h1>A Blink Details</h1>
<h2>A. 1 Visual Prompts Details</h2>
<p>There are three types of visual prompts in Blink: circles, boxes, and masks as shown in Figure 5. As for visual correspondence, functional correspondence, semantic correspondence, the red circles have radius 10px on images resized to 1024px height. For relative reflectance, we draw white circles to avoid color confusions. For object localization, the boxes are in red and green. For jigsaw, the masks are kept black. Since the examples in Figure 5 are different from the actual ones for illustrative purposes, we show some actual-sized example data as in Figures 9 to 19, with GPT-4V predictions attached.</p>
<div class="codehilite"><pre><span></span><code><span class="n">gpt_query_template</span> = {
    <span class="s">&quot;You are an AI assistant who will help me to match an answer with several options of a single-choice question.&quot;</span>
    <span class="s">&quot;You are provided with a question, several options, and an answer, and you need to find which option is most similar to the answer.&quot;</span>
    <span class="s">&quot;If the answer says things like refuse to answer, I&#39;m sorry cannot help, etc., output (Z)&quot;</span>
    <span class="s">&quot;If the meaning of all options are significantly different from the answer, or the answer does not select any option, output (Z)&quot;</span>\}
    <span class="s">&quot;Your should output one of the choices, (A),(B),(C),(D),(E) (if they are valid options), or (Z)in&quot;</span>
    <span class="s">&quot;Example 1: &#39;in&#39;</span>
<span class="s">    &quot;</span><span class="n">Question:</span> <span class="n">Which</span> <span class="n">point</span> <span class="k">is</span> <span class="n">closer</span> <span class="nb">to</span> <span class="n">the</span> <span class="n">camera</span><span class="o">?</span><span class="n">InSelect</span> <span class="nb">from</span> <span class="n">the</span> <span class="n">following</span> <span class="n">choices</span>.<span class="n">inOptions:</span> (<span class="n">A</span>) <span class="n">Point</span> <span class="n">A&#39;in</span>(<span class="n">B</span>) <span class="n">Point</span> <span class="n">B&#39;in</span>(<span class="o">Z</span>) <span class="n">Failed</span> <span class="s">&quot;</span>
<span class="s">    &quot;</span><span class="n">Answer:</span> <span class="n">Point</span> <span class="n">B</span>, <span class="n">where</span> <span class="n">the</span> <span class="nb">child</span> <span class="k">is</span> <span class="n">sitting</span>, <span class="k">is</span> <span class="n">closer</span> <span class="nb">to</span> <span class="n">the</span> <span class="n">camera</span>.<span class="s">&#39;inYour output: (B)In&quot;</span>
<span class="s">    &quot;Example 2: &#39;</span><span class="n">in&#39;</span>
    <span class="s">&quot;Question: Which point is closer to the camera?InSelect from the following choices.&#39;inOptions: (A) Point A&#39;in(B) Point B&#39;in(Z) Failed &quot;</span>
    <span class="s">&quot;Answer: I&#39;m sorry, but I can&#39;t assist with that request.&#39;inYour output: (Z)In&quot;</span>
    <span class="s">&quot;Example 3: &#39;in&#39;</span>
<span class="s">    &quot;</span><span class="n">Question:</span> <span class="n">Which</span> <span class="n">point</span> <span class="k">is</span> <span class="n">corresponding</span> <span class="nb">to</span> <span class="n">the</span> <span class="n">reference</span> <span class="n">point</span><span class="o">?</span><span class="n">InSelect</span> <span class="nb">from</span> <span class="n">the</span> <span class="n">following</span> <span class="n">choices</span>.<span class="s">&#39;inOptions: (A) Point A&#39;</span><span class="n">in</span>(<span class="n">B</span>) <span class="n">Point</span> <span class="n">B&#39;in</span>(<span class="o">Z</span>) <span class="n">Failed</span> <span class="s">&quot;</span>
<span class="s">    &quot;</span><span class="n">Answer:</span> <span class="n">The</span> <span class="n">reference</span> <span class="n">point</span> (<span class="n">REF</span>) <span class="n">on</span> <span class="n">the</span> <span class="nb">first</span> <span class="n">image</span> <span class="k">is</span> <span class="nb">at</span> <span class="n">the</span> <span class="n">tip</span> <span class="nb">of</span> <span class="n">the</span> <span class="n">pot</span>, <span class="n">which</span> <span class="k">is</span> <span class="n">the</span> <span class="n">part</span> <span class="n">used</span> <span class="nb">to</span> <span class="n">Poke</span> <span class="k">if</span> <span class="n">the</span> <span class="n">pots</span> <span class="n">were</span> <span class="n">used</span> <span class="k">for</span> <span class="n">that</span> <span class="nb">action</span>.
        <span class="n">Looking</span> <span class="nb">at</span> <span class="n">the</span> <span class="nb">second</span> <span class="n">image</span>, <span class="n">we</span> <span class="k">need</span> <span class="nb">to</span> <span class="n">find</span> <span class="n">the</span> <span class="n">part</span> <span class="nb">of</span> <span class="n">the</span> <span class="n">object</span> <span class="n">that</span> <span class="n">would</span> <span class="n">correspond</span> <span class="nb">to</span> <span class="n">poking</span>.<span class="s">&#39;in(A) Point A is at the tip of the spoon&#39;</span><span class="o">s</span>
    <span class="nb">handle</span>, <span class="n">which</span> <span class="k">is</span> <span class="nb">not</span> <span class="n">used</span> <span class="k">for</span> <span class="n">poking</span>.<span class="s">&#39;in(B) Point B is at the bottom of the spoon, which is not used for poking.&#39;</span><span class="n">in</span>(<span class="n">C</span>) <span class="n">Point</span> <span class="n">C</span> <span class="k">is</span> <span class="n">on</span> <span class="n">the</span> <span class="n">side</span> <span class="nb">of</span> <span class="n">the</span>
    <span class="n">popoonot</span>, <span class="n">which</span> <span class="k">is</span> <span class="nb">not</span> <span class="n">used</span> <span class="k">for</span> <span class="n">poking</span>.<span class="s">&#39;in(D) Point D is at the tip of the spoon, which is not used for poking.&#39;</span><span class="n">in&#39;nTherefore</span>, <span class="n">there</span> <span class="k">is</span> <span class="n">no</span> <span class="n">correct</span>
    <span class="n">answer</span> <span class="nb">in</span> <span class="n">the</span> <span class="n">choices&#39;inYour</span> <span class="n">output:</span> (<span class="o">Z</span>)<span class="n">In</span><span class="s">&quot;</span>
<span class="s">    &quot;</span><span class="n">Example</span> <span class="mi">4</span>: <span class="s">&#39;in&#39;</span>
    <span class="s">&quot;Question: (question)?&#39;inOptions: (options)In(Z) Failed&#39;inAnswer: (prediction)InYour output: &quot;</span>)
</code></pre></div>

<p>Figure 8: The evaluation prompts used for option label extraction.</p>
<h2>A. 2 Spatial Relation Curation Process</h2>
<p>We curate our samples from the Visual Spatial Reasoning [47] dataset. Each original sample contains an image and a claim, which is either true or false. One example being "Caption: The cow is ahead of the person. Label: False." We reformat the claims into binary questions via GPT-3.5 [13], e.g. "Question: Is the cow ahead of the person? Choices: (A) Yes (B) No Label: (B)"</p>
<h2>A. 3 Evaluation Prompts</h2>
<p>Following MMBench [52], given model outputs, we first try to extract choices with exact matching (e.g., for ' C ', we try to match " C " and " $(\mathrm{C})^{\prime \prime}$, etc). If failed,</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Note that the human score for IQ test is annotated by authors. It may not reflect typical human performance, which is also expected to vary.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
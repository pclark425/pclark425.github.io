<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9794 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9794</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9794</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-6a3ed569d47b4ea08aca4f69ec7da5e8d87734b0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6a3ed569d47b4ea08aca4f69ec7da5e8d87734b0" target="_blank">Understanding Telecom Language Through Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Global Communications Conference</p>
                <p><strong>Paper TL;DR:</strong> The developed framework shows a stepping stone towards realizing intent-driven and self-evolving wireless networks from Telecom languages, and paves the way for the implementation of generative AI in the Telecom domain.</p>
                <p><strong>Paper Abstract:</strong> The recent progress of artificial intelligence (AI) opens up new frontiers in the possibility of automating many tasks involved in Telecom networks design, implementation, and deployment. This has been further pushed forward with the evolution of generative artificial intelligence (AI), including the emergence of large language models (LLMs), which is believed to be the cornerstone toward realizing self-governed, interactive AI agents. Motivated by this, in this paper, we aim to adapt the paradigm of LLMs to the Telecom domain. In particular, we fine-tune several LLMs including BERT, distilled BERT, RoBERTa and GPT-2, to the Telecom domain languages, and demonstrate a use case for identifying the 3rd Generation Partnership Project (3GPP) standard working groups. We consider training the selected models on 3GPP technical documents (Tdoc) pertinent to years 2009-2019 and predict the Tdoc categories in years 2020-2023. The results demonstrate that fine-tuning BERT and RoBERTa model achieves 84.6% accuracy, while GPT-2 model achieves 83% in identifying 3GPP working groups. The distilled BERT model with around 50% less parameters achieves similar performance as others. This corroborates that fine-tuning pretrained LLM can effectively identify the categories of Telecom language. The developed framework shows a stepping stone towards realizing intent-driven and self-evolving wireless networks from Telecom languages, and paves the way for the implementation of generative AI in the Telecom domain.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9794",
    "paper_id": "paper-6a3ed569d47b4ea08aca4f69ec7da5e8d87734b0",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00256075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Understanding Telecom Language Through Large Language Models</h1>
<p>Lina Bariah, Hang Zou, Qiyang Zhao, Belkacem Mouhouche, Faouzi Bader, and Merouane Debbah<br>Technology Innovation Institute, 9639 Masdar City, Abu Dhabi, UAE<br>Email: firstname.lastname@tii.ae</p>
<h4>Abstract</h4>
<p>The recent progress of artificial intelligence (AI) opens up new frontiers in the possibility of automating many tasks involved in Telecom networks design, implementation, and deployment. This has been further pushed forward with the evolution of generative artificial intelligence (AI), including the emergence of large language models (LLMs), which is believed to be the cornerstone toward realizing self-governed, interactive AI agents. Motivated by this, in this paper, we aim to adapt the paradigm of LLMs to the Telecom domain. In particular, we fine-tune several LLMs including BERT, distilled BERT, RoBERTa and GPT-2, to the Telecom domain languages, and demonstrate a use case for identifying the 3rd Generation Partnership Project (3GPP) standard working groups. We consider training the selected models on 3GPP technical documents (Tdoc) pertinent to years 2009-2019 and predict the Tdoc categories in years 2020-2023. The results demonstrate that fine-tuning BERT and RoBERTa model achieves $\mathbf{8 4 . 6 \%}$ accuracy, while GPT2 model achieves $\mathbf{8 3 \%}$ in identifying 3GPP working groups. The distilled BERT model with around $\mathbf{5 0 \%}$ less parameters achieves similar performance as others. This corroborates that fine-tuning pretrained LLM can effectively identify the categories of Telecom language. The developed framework shows a stepping stone towards realizing intent-driven and self-evolving wireless networks from Telecom languages, and paves the way for the implementation of generative AI in the Telecom domain.</p>
<p>Index Terms-Generative AI, Large Language Models, Pretrained Transformer, Telecom Language, 3GPP</p>
<h2>I. INTRODUCTION</h2>
<p>In the last couple of decades, considerable efforts have been devoted to push the frontiers of wireless technologies in order to achieve key performance indicators (KPIs) pertinent to latency, reliability, spectral and energy efficiencies, to name a few, through the exploitation of artificial intelligence (AI) as a network orchestrator. Recently, parallel initiatives have been focused on advancing the paradigm of self-evolving networks (under several names including autonomous networks, zerotouch networks, self-optimizing/configuring/healing networks, etc.), through the evolution of native intelligent network architecture [13]. However, recent developments are revolving around realizing adaptivity, in which wireless networks functionalities can be autonomously adjusted to fit within a particular scenario. The ultimate vision of self-evolving networks goes way beyond adaptivity and automation. In particular, it expands toward realizing perpetual sustainability of network performance and the flexibility to accommodate highly complex, and sometimes unfamiliar, network scenarios, and hence, this necessitates generalized, inclusive, and
multi-functional schemes that are capable of handling diverse network conditions. Accordingly, conventional AI algorithms are highly probable to fall behind in fulfilling the required performance, and therefore, a radical departure to more innovative AI-driven approaches is anticipated to shape the future of next generation wireless networks.</p>
<p>Foundation models (FMs) was coined by Stanford Center for Research on Foundation Models (CRFM) in 2021 and have attracted a considerable attention as generalized models that are capable of handling a wide range of downstream tasks [14]. In particular, FMs are extremely large neural networks that are trained over massive unlabeled datasets, in a selfsupervised fashion, allowing several opportunities to be reaped with reduced time and cost (that would be unbearable in case of human labeling). Rapidly after being developed, FMs have found their applications in several domains, including text classification and summarizing, sentiment analysis, information extraction, and image captioning. While FMs were not aimed to follow a particular model or application, language-related models, i.e., large language models (LLMs), are currently one of the most common subfield of FMs, which rely on the principle of pretraining large models over a large-scale corpus. Such pretrained large models, e.g., bidirectional encoder representations from transformers (BERT) [15] and generative pre-trained transformers (GPT) [16], can be further fine-tuned in various downstream tasks, and hence, avoid the cost of retraining large models from scratch in the new domains.</p>
<h2>A. Related Work</h2>
<p>Focusing on text generation-related tasks, language models trained on large corpus can successfully understand the natural language, and create human-like language responses according to the specific tasks. Several domain-specific variations of well-known pretrained language models were presented in the literature to demonstrate the opportunities that can be obtained from domain-specific fine-tuning and retraining. In [1], the authors proposed SCIBERT, a BERT-based language model that is fine-tuned to the scientific domain, where it was trained over corpus from scientific publications. The authors in [2] have considered fine-tuning BERT model using Google Patents Public datasets to perform patents classification. Furthermore, generative language model, based on multiple choice question answering, is fine-tuned using social commenting platforms in [3], in order to realize zero-shot text classification. From a</p>
<p>TABLE I: BERT fine-tuning: Domains and Data</p>
<table>
<thead>
<tr>
<th>Ref.</th>
<th>Domain</th>
<th>Data</th>
</tr>
</thead>
<tbody>
<tr>
<td>[1]</td>
<td>General Science</td>
<td>Scientific publication</td>
</tr>
<tr>
<td>[2]</td>
<td>Multi-domain Patents</td>
<td>Google Patents Public dataset</td>
</tr>
<tr>
<td>[3]</td>
<td>General domain Q&amp;A</td>
<td>Social commenting platforms</td>
</tr>
<tr>
<td>[4]</td>
<td>General domain</td>
<td>Wikipedia articles</td>
</tr>
<tr>
<td>[5]</td>
<td>Cross-domain sentiment analysis</td>
<td>Amazon reviews dataset</td>
</tr>
<tr>
<td>[6]</td>
<td>Customers delivery and pickup services (open-domain)</td>
<td>AG's News Corpus &amp; TREC dataset</td>
</tr>
<tr>
<td>[7]</td>
<td>Authorship attribute</td>
<td>Public datasets, e.g., IMDb</td>
</tr>
<tr>
<td>[8]</td>
<td>Chinese language</td>
<td>Chinese web news</td>
</tr>
<tr>
<td>[9]</td>
<td>Russian language</td>
<td>OpenCorpora (Russian online media and Russian Wikipedia)</td>
</tr>
<tr>
<td>[10]</td>
<td>Arabic language</td>
<td>Arabic media &amp; news (Modern Standard Arabic (MSA) &amp; Dialectal Arabic (DA))</td>
</tr>
<tr>
<td>[11]</td>
<td>Healthcare</td>
<td>Wikipedia articles</td>
</tr>
<tr>
<td>[12]</td>
<td>Telecom Q&amp;A</td>
<td>TeleQuAD</td>
</tr>
<tr>
<td>Our work</td>
<td>Telecom Language Understanding</td>
<td>3GPP technical documents and specifications</td>
</tr>
</tbody>
</table>
<p>different perspective, the authors in [4] proposed a Universal Language Model Fine-tuning (ULMFiT) approach for finetuning generative large models for enhanced text classification. The proposed scheme in [4] demonstrated reduced error by 18-24% up to six-class text classification, when tested over general Wikipedia articles. Cross-domain sentiment analysis through fine-tuning BERT and XLNet models is proposed in [5], in which the fine-tuned model showed promising results with less amount of data. The authors in [6] explored several active learning strategies to adapt BERT model into customers transactions application to classify transactions to different market-related categories, for improved market demands understanding. Targeting different domain, the work in [7] presents BertAA, a framework for BERT fine-tuning for authorship classification purposes, in which public datasets, e.g., IMDb, are utilized to refine the BERT model and enable it to extract the characteristics of authors' identities from the provided text. The proposed work in [7] showed 5.3% improved in the authorship attribute task. From a language perspective, multi-lingual and single-lingual frameworks were presented in the literature to fine-tune/retrain a pre-trained BERT model in order to allow the model to deal with different languages, e.g. Chinese [8], Russian [9], Arabic [10]. The presented results in [8]- [10] demonstrated the robustness of BERT as a large model for different languages. For healthcare applications, the authors in [11] provided a framework for disease name recognition, where a BERT model, fine-tuned using data pertinent to disease knowledge, demonstrated an enhanced performance compared to the literature.</p>
<h3>V-B Contributions</h3>
<p>While the field of domain-specific fine-tuning of large generative models is very active and several contributions were presented for different domains, the telecom domain is still almost untouched. We strongly believe that adapting various large generative models to the telecom domain is a key building block in the development of self-evolving networks, where such models can play an essential role through the different stages of designing, building, and operating wireless networks. The advantages of large Telecom language models are envisioned to be particularly important with the rise of generative agents paradigm [17], in which LLM implemented at Telecom networks will require a comprehensive understanding of the Telecom terminologies, and their relationship with different network operational and configuration functions, in order to enable them to communicate meaningfully and to perform Telecom-specific downstream tasks when implemented in future wireless networks.</p>
<p>Within this context, in [12], the authors have focused on adapting BERT-like model to the telecom domain, where the considered model is pretrained/fine-tuned in order to perform a question answering downstream task within the telecom domain. Note that the work in [12] is constrained by the small dataset used (few hundreds of technical documents and web articles), which was prepared in a manual manner as follows. The data were acquired from technology specification files of the 3GPP, and it was collected from 347 telecom-related documents, resulting in 2,021 question-answer pairs only. It is worthy to note that the dataset used in [12] is not publicly available. For enabling a holistic understanding of Telecom language, a comprehensive dataset comprising a wide-range of technical discussion pertinent to different network operational, configuration, and design parameters need to be generated and used in the pretraining/fine-tuning process. Motivated by this, in this paper, we develop a framework for adapting pretrained generative models, including BERT, DistilBERT, RoBERTa, and GPT-2 models, to the Telecom domain, through exploiting a huge number of technical documents that consist of technical specification from 3GPP standard. Among different language</p>
<p>models, the selection of considered models are motivated by the fact that it generates a contextual representation for each word, while considering previous and following words, rendering it a well-suited model for technical text classification.</p>
<p>The main contributions of our work are summarized as follows:</p>
<ol>
<li>Create an annotated large Telecom datasets from 3GPP technical specification of various working group (WG), including technical pertinent to radio frequency (RF) spectrum usage, network architecture, radio interface protocols, signaling procedures, and mobility management, network architecture, system interfaces, security, quality of service (QoS), network management, routing, switching, and control functions.</li>
<li>Adapt the pre-trained BERT, DistilBERT, RoBERTa, and GPT-2 model into the Telecom domain, through finetuning the models for 3GPP Tdocs text classification. The fine-tuned models allow to identify a particular technical text, related to 3GPP cellular architecture category, i.e., radio access network (RAN), system architecture (SA), or core network and terminals (CT), with characterizing the WG corresponding to each category.</li>
</ol>
<p>The remaining of the paper is organized as follows. In Sec. II we detail the developed approach to adapt the pretrained models to Telecom domain, including solutions on data collection, data pre-processing, and model fine-tuning. Experimental results with performance analysis are discussed in Sec. III. Finally, the paper is concluded in Sec. IV.</p>
<h2>II. Method</h2>
<h3>A. LLMs for Telecom Language Classification</h3>
<p>In this work, we use BERT, DistilBERT, RoBERTa, GPT-2 language models, which are trained on large amounts of unlabeled textual data using self-supervised or contrastive learning [15]. These models can be adapted to various downstream tasks via fine-tuning. Specifically, the architecture of BERT and its variants allows it to understand the context and meaning of words in a sentence by taking into account the surrounding words on both sides of the target word. This bidirectional approach helps the pre-trained model to capture more complex relationships between words and their contextual meaning, making it a powerful tool for text classification.</p>
<p>The following models are implemented in our work: 1) Pretrained BERT-Base (uncased): contains 12 layers, 768 hidden units, 12 self-attention heads, and 110M parameters; 2) DistilBERT: a lighter version of BERT-Base (uncased) with 40% less parameters, which is particularly useful for wireless network with constrained resources; 3) RoBERTa: contains the same architecture as BERT, with byte-level byte pair encoding (BPE) as a tokenizer is used, which operates at the byte level instead of the traditional character or subword levels; 4) GPT-2: the smallest version with 6 layers, 36 hidden units, 48 self-attention heads, and 124M parameters. A linear classification layer with SoftMax function is added to the pre-trained models to produce the WGs.</p>
<p>In order to adapt the selected models into the desired Telecom domain for the downstream task of text classification, we consider the single-task single-label fine-tuning approach [18] for 3GPP Tdoc classification. A cross entropy loss function is used to update the pre-trained model weights. For efficient fine-tuning, we employed a batch size of 32, ensuring a balance between computational efficiency and memory requirements. The learning rate was set to 2e-5, enabling gradual convergence to an optimal solution. To prevent overfitting, we applied L2 regularization with a rate of 0.01. Also, F1 score is considered to evaluate the performance of the tuned models.</p>
<h3>B. 3GPP Technical Document Dataset</h3>
<p>3GPP is the main Standard Developing Organization (SDO) in the area of Telecommunication. The universal standards for 3G, 4G and 5G have been developed by 3GPP since 1999. 3GPP works with Tdoc contributed by companies during the development phase and produces technical specifications as a final output. The specification work is carried out in Technical Specification Groups (TSGs). There are three Technical Specifications Groups: RAN, SA, and CT. Each TSG consists of multiple WG focused on specific areas, ranging from radio access network specifications, core network specifications, service requirements and specifications, and architecture and protocols for mobile communication systems, to QoS and performance requirements, security and privacy in mobile communication systems, interoperability and compatibility requirements, network management and operation, and testing and certification procedures. These topics are further divided into specific subtopics, and each Tdoc file may focus on one or more of these areas. The content of Tdoc files is typically technical and detailed, intended for experts and engineers involved in the development and implementation of mobile communication systems. Thus, The ability to classify a text into one of the WG requires a deep understanding of the functions and scope of each group.</p>
<p>In this paper, the technical documents are acquired from the 3GPP website. The collected files belong to years 2009-2023 and include technical specifications put by different WGs, including, RAN1, RAN2, RAN3, RAN4, RAN5, SA1, SA2, SA3, SA4, SA5, SA6, CT1, CT3, CT4, CT6. The Tdoc files are available as ZIP files, and accordingly Apache Tika application [19] is used to unzip and extract the information from the files. Table II demonstrates the size of the dataset acquired from 3GPP WGs, where documents belonging to years 2009-2019 are used for training, while documents related to years 2020-2023 are exploited for testing.</p>
<h3>C. Data Pre-Processing</h3>
<p>We pre-process the 3GPP Tdoc files via following steps:</p>
<ol>
<li>Parse the HTML tags in the text and return the text content without any HTML tags using BeautifulSoup.</li>
<li>Remove any URLs (web links) from the text: identify the regex pattern that matches URLs starting with either "http" or "https" and may include alphanumeric characters, special characters, and encoded characters.</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Pipeline for Adapting LLM to 3GPP Technical Language</p>
<p>TABLE II: 3GPP Tdoc files in different WGs and years</p>
<table>
<thead>
<tr>
<th>WG</th>
<th>Train ('09-'19)</th>
<th>Train ('15-'19)</th>
<th>Test ('20-'23)</th>
</tr>
</thead>
<tbody>
<tr>
<td>RAN1</td>
<td>83905</td>
<td>56969</td>
<td>39230</td>
</tr>
<tr>
<td>RAN2</td>
<td>82853</td>
<td>55189</td>
<td>38744</td>
</tr>
<tr>
<td>RAN3</td>
<td>36651</td>
<td>22981</td>
<td>20219</td>
</tr>
<tr>
<td>RAN4</td>
<td>43845</td>
<td>28953</td>
<td>43482</td>
</tr>
<tr>
<td>RAN5</td>
<td>50812</td>
<td>31706</td>
<td>32456</td>
</tr>
<tr>
<td>SA1</td>
<td>19497</td>
<td>11282</td>
<td>10086</td>
</tr>
<tr>
<td>SA2</td>
<td>64065</td>
<td>42931</td>
<td>43860</td>
</tr>
<tr>
<td>SA3</td>
<td>19546</td>
<td>13903</td>
<td>13815</td>
</tr>
<tr>
<td>SA4</td>
<td>6583</td>
<td>1776</td>
<td>5128</td>
</tr>
<tr>
<td>SA5</td>
<td>28044</td>
<td>15031</td>
<td>13040</td>
</tr>
<tr>
<td>SA6</td>
<td>9010</td>
<td>9010</td>
<td>10360</td>
</tr>
<tr>
<td>CT1</td>
<td>30990</td>
<td>20840</td>
<td>18910</td>
</tr>
<tr>
<td>CT3</td>
<td>22269</td>
<td>12734</td>
<td>12584</td>
</tr>
<tr>
<td>CT4</td>
<td>28245</td>
<td>14731</td>
<td>13109</td>
</tr>
<tr>
<td>CT6</td>
<td>4446</td>
<td>2213</td>
<td>1571</td>
</tr>
<tr>
<td>Total</td>
<td>520761</td>
<td>340244</td>
<td>316594</td>
</tr>
</tbody>
</table>
<ol>
<li>Remove tables from the parsed HTML document using <em>BeautifulSoup</em>.</li>
<li>Divide each document into multiple text segments with different number of words extracted from natural language toolkit (NLTK). This allows us to evaluate the model's capability of understanding technical descriptions in different lengths.</li>
<li>Remove headers, footers, captions, and pseudo codes, while ensuring each paragraph doesn't exceed a particular maximum length. Also, we eliminate the references section and all the text afterward.</li>
<li>Remove change requests (CRs), drafts, templates due to their limited technical information.</li>
</ol>
<h3>III. EXPERIMENT RESULTS AND DISCUSSIONS</h3>
<p>In this section, we present experimental results to demonstrate the accuracy of the fine-tuned LLMs in understanding and classifying technical text within the Telecom domain. We split 3GPP Tdoc into training, validation, and test datasets.</p>
<p>Specifically, the test set contains textual segments of Tdocs from 2020 to 2023 (April). The training and validation sets contain: 1) Tdocs from 2010 to 2019 ('10-'19); and 2) Tdocs from 2015 to 2019 ('15-'19). The proportion of these two datasets is 80% and 20%. The number of words within a textual segment in training, validation, and test set is 200 in what follows without further mentioning.</p>
<p>We start by comparing the performance of different LLMs fine-tuned with 3GPP files from 2015 to 2019 in terms of classification accuracy as illustrated in Fig. 2. The selected models have the following sizes: BERT (117M), RoBERTa (125M), GPT-2 (124M), and DistilBERT (66M). Considering 100% of the files, while it can be observed that all models experience relatively close accuracy, GPT-2 model encounters the weakest performance. This is motivated by the fact that for text classification, it is important for the large model to have concise and interpretable predictions features rather than generative capabilities, where the latter is the key element of GPT-2. Meanwhile, RoBERTa, the optimized version of BERT, demonstrates the strongest performance. It can be noticed further that the performance gap increases as the number of Tdoc files decreases.</p>
<p>In Fig. 3, we evaluate the accuracy of prediction and the receiver operating characteristic - area under the curve (ROCAUC) as a function of the portion of textual segments used for fine-tuning a BERT model. We can observe that a BERT model fine-tuned to 3GPP files from 2015 to 2019 can achieve an accuracy performance around 80% even if only 20% of the text segments are used. Furthermore, although fine-tuning to Telecom language is essential, it can be demonstrated that Tdoc files from recent years are sufficient to provide the needed accuracy. On the other hand, when the number of files is relatively small (below 10%), data 2010-2015 produce better understanding of the Telecom technical language.</p>
<p>The role of 3GPP WGs vary from one to another. Therefore, the structure of files and especially the number of files available differs distinctly. For examples, RAN1 and RAN2 contain much more files than other WGs given that they are main categories in RAN (specifying PHY, MAC, RLC, PDCP layers), and hence, more activities pertinent to these layers are conducted within these two groups. To show the impact</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Classification accuracy of different LLMs vs. portion of 3GPP files used for fine-tuning.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Classificaton accuracy and ROC-AUC vs. portion of 3GPP files used for fine-tuning BERT ('10-'19 and '15-'19).</p>
<p>of different WGs on the performance of the classification, the accuracy of a BERT model fine-tuned by 3GPP files is illustrated in Table III for different combinations of WGs. It can be noticed that the fine-tuned model can achieve better classification accuracy for textual segment among RAN1, SA1 and CT1 than the combination of RAN1, RAN2 and RAN3. This is stemmed from the fact that Tdoc files belonging to different category number but fall within the same TSG are highly correlated, and therefore, the probability of error is higher. In contrast, technical files within different TSGs</p>
<p>TABLE III: Classification accuracy on different WG combinations from BERT</p>
<table>
<thead>
<tr>
<th>RAN</th>
<th>SA</th>
<th>CT</th>
<th>Accuracy (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>98.05</td>
</tr>
<tr>
<td>1,2,3</td>
<td>None</td>
<td>None</td>
<td>88.90</td>
</tr>
<tr>
<td>1,2,3</td>
<td>1</td>
<td>1</td>
<td>88.26</td>
</tr>
<tr>
<td>1,2,3,4</td>
<td>2,5</td>
<td>None</td>
<td>87.42</td>
</tr>
<tr>
<td>1,2,3</td>
<td>1,2,3</td>
<td>1,3,4</td>
<td>86.61</td>
</tr>
<tr>
<td>1,2,3,4</td>
<td>1,2,3,4</td>
<td>1,3,4,6</td>
<td>85.57</td>
</tr>
<tr>
<td>1,2,3,4,5</td>
<td>1,2,3,4,5,6</td>
<td>1,3,4,6</td>
<td>84.35</td>
</tr>
</tbody>
</table>
<p>comprises relatively uncorrelated topics, and therefore, the model has a higher capability to distinguish between these different TSGs. The presented results in Table III reveal that the test accuracy is determined mainly by the documents of RAN1, RAN2 and RAN3.</p>
<p>In Fig. 4 we evaluate the impact of the length of technical text segments to the accuracy of classification. This is a critical aspect to be studied, as it is important to know the minimum amount of text required by the tuned LLMs to realize accurate identification of the technical groups. We set the maximum number of words for training and validation to 200 and we vary the number of words during the testing process. We can observe that the accuracy increases as the number of words grows. However, performance enhancement difference starts to decrease as well as the number of words increases, indicating the significant role of selecting the optimum size for a LLM, in order to strike a balance between performance and computing complexity.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Classification accuracy vs. maximum text segment length in words produced from fine-tuned BERT</p>
<h2>IV. CONCLUSION</h2>
<p>Motivated by the promising potentials of LLMs, in this paper, we proposed a framework for 3GPP technical documents identification, where we leveraged pre-trained language models, fine-tuned using 3GPP data, in order to allow the model to identify the 3GPP specification categories with the corresponding working group. In more details, we have considered BERT, DistilBERT, RoBERTa, and GPT-2 models, in which they are fine-tuned using 3GPP Tdocs belonging to TSGs, namely RAN, SA, and CT. The obtained results demonstrate the applicability of adapting a pre-trained language model into the Telecom domain, where all fine-tuned models showed accurate classification performance under different scenarios. It is important to emphasize the significance of developing LLMs that are capable of understanding the Telecom language, as a cornerstone to enable autonomous networks driven by intelligent generative agents.</p>
<h2>REFERENCES</h2>
<p>[1] I. Beltagy, K. Lo, and A. Cohan, "SciBERT: A pretrained language model for scientific text," arXiv preprint arXiv:1903.10676, 2019.
[2] J.-S. Lee and J. Hsiang, "Patent classification by fine-tuning BERT language model," World Patent Information, vol. 61, p. 101965, 2020.
[3] R. Puri and B. Catanzaro, "Zero-shot text classification with generative language models," arXiv preprint arXiv:1912.10165, 2019.
[4] J. Howard and S. Ruder, "Universal language model fine-tuning for text classification," arXiv preprint arXiv:1801.06146, 2018.
[5] B. Myagmar, J. Li, and S. Kimura, "Cross-domain sentiment classification with bidirectional contextualized transformer language models," IEEE Access, vol. 7, pp. 163219-163230, 2019.
[6] S. Prabhu, M. Mohamed, and H. Misra, "Multi-class text classification using bert-based active learning," arXiv preprint arXiv:2104.14289, 2021.
[7] M. Fabien, E. Villatoro-Tello, P. Motlicek, and S. Parida, "BertAA: BERT fine-tuning for authorship attribution," in Proceedings of the 17th International Conference on Natural Language Processing (ICON), 2020, pp. 127-137.
[8] X. Chen, P. Cong, and S. Lv, "A long-text classification method of Chinese news based on BERT and CNN," IEEE Access, vol. 10, pp. $34046-34057,2022$.
[9] K. Lagutina, "Topical text classification of Russian news: a comparison of BERT and standard models," in Conference of Open Innovations Association (FRUCT). IEEE, 2022, pp. 160-166.
[10] W. Antoun, F. Baly, and H. Hajj, "Arabert: Transformer-based model for Arabic language understanding," arXiv preprint arXiv:2003.00104, 2020.
[11] Y. He, Z. Zhu, Y. Zhang, Q. Chen, and J. Caverlee, "Infusing disease knowledge into BERT for health question answering, medical inference and disease name recognition," arXiv preprint arXiv:2010.03746, 2020.
[12] H. Holm, "Bidirectional encoder representations from transformers (BERT) for question answering in the telecom domain.: Adapting a BERT-like language model to the telecom domain using the ELECTRA pre-training approach," 2021.
[13] C.-X. Wang, X. You, X. Gao, X. Zhu, Z. Li, C. Zhang, H. Wang, Y. Huang, Y. Chen, H. Haas et al., "On the road to 6G: Visions, requirements, key technologies and testbeds," IEEE Commun. Surveys Tuts., 2023.
[14] R. Bommasani et al., "On the opportunities and risks of foundation models," arXiv preprint arXiv:2108.07258, 2021.
[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[16] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., "Language models are unsupervised multitask learners," OpenAI blog, vol. 1, no. 8, p. 9, 2019.
[17] J. S. Park, J. C. Oâ€™Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein, "Generative agents: Interactive simulacra of human behavior," arXiv preprint arXiv:2304.03442, 2023.
[18] C. Sun, X. Qiu, Y. Xu, and X. Huang, "How to fine-tune bert for text classification?" in Chinese Computational Linguistics: 18th China National Conference, CCL 2019, Kunming, China, October 18-20, 2019, Proceedings 18. Springer, 2019, pp. 194-206.
[19] "Apache tika - a content analysis toolkit," https://tika.apache.org, accessed: 2023-05-04.</p>            </div>
        </div>

    </div>
</body>
</html>
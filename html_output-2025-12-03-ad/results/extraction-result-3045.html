<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3045 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3045</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3045</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-267500377</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.03610v1.pdf" target="_blank">RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents</a></p>
                <p><strong>Paper Abstract:</strong> Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3045.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3045.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that augments LLM/VLM agents with an episodic, retrieval-based memory of past successful task executions; it retrieves contextually relevant trajectory snippets and uses them as in-context examples to guide planning and action generation in both text-only and multimodal (embodied) tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAP (Retrieval-Augmented Planning)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Modular agent that uses a Reasoner (LLM) to produce plans and retrieval keys, a Retriever that computes similarity between current context and stored episodic logs, an Executor (LLM/VLM) that uses retrieved trajectory windows via in-context learning to generate next actions, and an external Memory database of logged successful episodes; supports text agents (LLMs) and multimodal embodied agents (VLMs + policy network).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented episodic memory (external database of successful episode logs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memory stores episodic logs L_i = {T_i, p_i, τ_Li} (task, overall plan, trajectory of plans/actions/observations). The Reasoner generates a retrieval key (k_0) from the current action plan. Retriever scores logs by a weighted average: score = w_t·sim(T0, T_Li) + w_p·sim(p0, p_Li) + w_k·sim(k0, τ_Li); similarities use cosine on embeddings (sentence-transformers for text, CLIP-based ViT for images). The Retriever returns a windowed segment centered on the most similar action/observation to reduce noise. Executor uses the retrieved trajectory window as in-context examples to produce the next action. Memory is populated from successful episodes and can be expanded across recursive trials; component weights and retrieval mode (action vs observation) are adapted per environment.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, WebShop, Franka Kitchen, Meta-World</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step text-based planning tasks in ALFWorld and WebShop (complex multi-step reasoning and web interaction) and multimodal embodied robotic manipulation tasks in Franka Kitchen and Meta-World (visual perception + sequential physical actions).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / multi-step sequential decision-making (textual and multimodal embodied tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ALFWorld (GPT-3.5, RAP): overall success 85.8% (RAP_train: 91.0%); WebShop (GPT-3.5, RAP): success rate 48.0%, overall reward score 76.1%; Franka Kitchen (LLaVA with RAP): 61.6% success; Meta-World (LLaVA with RAP): 79.2% success; CogVLM with RAP: Franka Kitchen 56.9%, Meta-World 68.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baselines without RAP: ALFWorld ReAct (GPT-3.5) overall 52.2%; WebShop ReAct (GPT-3.5) success 35.0%, score 61.8%; Franka Kitchen LLaVA baseline 43.4%; Meta-World LLaVA baseline 65.4%; CogVLM baseline Franka Kitchen 44.2%, Meta-World 61.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAP substantially improves planning and task success by retrieving and analogizing from past successful trajectories: the paper reports gains of 33.6% (ALFWorld), 13.0% (WebShop), 18.2% (Franka Kitchen), and 12.7% (Meta-World) over ReAct in the reported settings. RAP is effective across LLMs (GPT-3.5, GPT-4, Llama2-13b) and for multimodal VLM agents (LLaVA, CogVLM). Memory constructed with one model (GPT-3.5) can improve performance of another model (Llama2-13b), demonstrating cross-model transfer of episodic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Paper notes that passing full trajectories can introduce noise, so RAP uses windowed trajectory segments; retrieval weights and modes must be adapted per environment (task-space constraints); performance depends on quality and relevance of stored successful episodes and on accurate similarity scoring (risk of retrieval mismatches). No claims about memory scaling limits or catastrophic forgetting are empirically detailed beyond these practical considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3045.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3045.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM agent paradigm that interleaves chain-of-thought style reasoning ('Thoughts') and environment-interacting actions, enabling dynamic action-state synergy; used in this paper as a baseline agent without an external episodic memory database.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-driven agent that alternates between reasoning and acting (thought-action interleaving), producing action plans and executing them in the environment; it relies on the LLM context and generated intermediate reasoning rather than an external episodic memory store.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, WebShop (evaluated as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-step text planning and web interaction benchmarks used in the paper (ALFWorld and WebShop).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / tool-use (textual)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ALFWorld (GPT-3.5, ReAct) overall success 52.2%; WebShop (GPT-3.5, ReAct) success 35.0% (score 61.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ReAct serves as a strong baseline for LLM agents but is outperformed by RAP when RAP leverages an external episodic memory of successful trajectories; lacking an external retrieval-augmented memory limits ReAct on tasks where reuse of past episodic patterns is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not include an external episodic memory of prior successful task executions, which the paper identifies as a disadvantage for certain multi-step and multimodal tasks where prior experiences provide strong cues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3045.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3045.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that requires LLM agents to perform self-reflection on unsuccessful episodes to improve future performance; cited and evaluated as a baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Method that collects failed episodes and prompts the LLM to 'reflect' (generate insights) about failures to guide future attempts; this creates a textual reflective memory used to improve future behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>reflective/episodic memory (textual self-reflection on failures)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Stores traces of unsuccessful episodes and elicits LLM self-reflection to derive corrective guidance; reflexive outputs are then used to influence future attempts (as described in the literature and used as a baseline in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, WebShop (evaluated as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step text-based planning and web interaction tasks where learning from previous failures can help improve subsequent trials.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / iterative improvement (textual)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ALFWorld (GPT-3.5, Reflexion) overall success reported 74.6%; WebShop (GPT-3.5, Reflexion) success 35.0% (score 61.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reflexion improves over ReAct in some settings by leveraging textual self-reflection on prior failures, but in this paper RAP (retrieval of successful episodic trajectories) outperforms Reflexion on the evaluated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reflexion operates primarily on textual reflections derived from failures; authors argue that directly retrieving diverse successful experiences (as RAP does) can be more effective for generalization across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3045.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3045.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADaPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ADaPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that adaptively decomposes tasks into sub-tasks and re-executes as needed; cited and evaluated as a baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ADaPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Technique that decomposes complex tasks into sub-tasks and selectively re-executes or refines sub-policies to complete tasks; referenced as prior work improving LLM-based task solving.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>task-decomposition / short-horizon re-execution memory (uses subtask traces)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>ADaPT performs decomposition and re-execution, which implies use of previous subtask traces for corrective re-runs; in the paper it is treated as a baseline rather than a primary focus.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, WebShop (evaluated as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning and interaction tasks where decomposition into subtasks can improve execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / decomposition (textual)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ALFWorld (GPT-3.5, ADaPT) overall success ≈ 71.6% (reported in comparisons); WebShop (GPT-3.5, ADaPT) success 43.0% (score 64.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ADaPT improves over simple ReAct in some textual benchmarks by decomposing problems, but RAP (episodic retrieval of successful trajectories) achieves higher success rates in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Paper notes ADaPT is limited to reflecting on trajectories within the same task instance rather than generalizing across diverse successful examples, which RAP targets explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3045.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3045.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generation approach that augments generative models with retrieved external documents as context; cited in related work as foundational for retrieval-augmented methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG (conceptual reference)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieval + generation paradigm where retrieved passages are passed into a generative model to produce context-grounded outputs; often used for knowledge-intensive NLP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external knowledge store</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Retrieve relevant documents by similarity and feed them into the generator as extra context; paper cites this as background motivating retrieval-augmented memory for agents.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>N/A (cited as methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used in knowledge-intensive generation tasks in literature; here cited as prior art motivating RAP.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>knowledge-augmented generation (general)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAP builds on the retrieval-augmented idea of RAG but applies it to episodic trajectory retrieval for sequential decision-making and multimodal agents, rather than document retrieval for QA.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3045.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3045.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExpeL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ExpeL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that aggregates generated experiences and feeds them back into LLMs for reflection/learning; cited in related work as an alternative way of using experience.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ExpeL (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Approach that collects experiences generated during task attempts and has the LLM reflect on them (textually) to improve future performance; discussed as related work contrasting with RAP's implicit retrieval-from-diverse experiences approach.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>experience aggregation / reflective memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Passes collected experiences into the LLM to perform textual reflection to derive improvements; cited to contrast explicit reflection versus RAP's implicit retrieval-analogy mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>N/A (cited as methodology)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prior work used for iterative agent improvement; mentioned to position RAP's different design choice.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>iterative improvement / learning from experience</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper contrasts ExpeL's explicit reflective step with RAP's approach of implicitly drawing analogies from a diverse memory of successful episodes without an extra re-tasking step.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Adapt: As-needed decomposition and planning with language models <em>(Rating: 2)</em></li>
                <li>ExpeL <em>(Rating: 1)</em></li>
                <li>LLaVA: Large Language and Vision Assistant <em>(Rating: 1)</em></li>
                <li>CogVLM: Visual Expert for Pretrained Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3045",
    "paper_id": "paper-267500377",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "RAP",
            "name_full": "Retrieval-Augmented Planning",
            "brief_description": "A framework that augments LLM/VLM agents with an episodic, retrieval-based memory of past successful task executions; it retrieves contextually relevant trajectory snippets and uses them as in-context examples to guide planning and action generation in both text-only and multimodal (embodied) tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAP (Retrieval-Augmented Planning)",
            "agent_description": "Modular agent that uses a Reasoner (LLM) to produce plans and retrieval keys, a Retriever that computes similarity between current context and stored episodic logs, an Executor (LLM/VLM) that uses retrieved trajectory windows via in-context learning to generate next actions, and an external Memory database of logged successful episodes; supports text agents (LLMs) and multimodal embodied agents (VLMs + policy network).",
            "memory_used": true,
            "memory_type": "retrieval-augmented episodic memory (external database of successful episode logs)",
            "memory_mechanism_description": "Memory stores episodic logs L_i = {T_i, p_i, τ_Li} (task, overall plan, trajectory of plans/actions/observations). The Reasoner generates a retrieval key (k_0) from the current action plan. Retriever scores logs by a weighted average: score = w_t·sim(T0, T_Li) + w_p·sim(p0, p_Li) + w_k·sim(k0, τ_Li); similarities use cosine on embeddings (sentence-transformers for text, CLIP-based ViT for images). The Retriever returns a windowed segment centered on the most similar action/observation to reduce noise. Executor uses the retrieved trajectory window as in-context examples to produce the next action. Memory is populated from successful episodes and can be expanded across recursive trials; component weights and retrieval mode (action vs observation) are adapted per environment.",
            "task_name": "ALFWorld, WebShop, Franka Kitchen, Meta-World",
            "task_description": "Multi-step text-based planning tasks in ALFWorld and WebShop (complex multi-step reasoning and web interaction) and multimodal embodied robotic manipulation tasks in Franka Kitchen and Meta-World (visual perception + sequential physical actions).",
            "task_type": "planning / multi-step sequential decision-making (textual and multimodal embodied tasks)",
            "performance_with_memory": "ALFWorld (GPT-3.5, RAP): overall success 85.8% (RAP_train: 91.0%); WebShop (GPT-3.5, RAP): success rate 48.0%, overall reward score 76.1%; Franka Kitchen (LLaVA with RAP): 61.6% success; Meta-World (LLaVA with RAP): 79.2% success; CogVLM with RAP: Franka Kitchen 56.9%, Meta-World 68.8%.",
            "performance_without_memory": "Baselines without RAP: ALFWorld ReAct (GPT-3.5) overall 52.2%; WebShop ReAct (GPT-3.5) success 35.0%, score 61.8%; Franka Kitchen LLaVA baseline 43.4%; Meta-World LLaVA baseline 65.4%; CogVLM baseline Franka Kitchen 44.2%, Meta-World 61.5%.",
            "has_performance_comparison": true,
            "key_findings": "RAP substantially improves planning and task success by retrieving and analogizing from past successful trajectories: the paper reports gains of 33.6% (ALFWorld), 13.0% (WebShop), 18.2% (Franka Kitchen), and 12.7% (Meta-World) over ReAct in the reported settings. RAP is effective across LLMs (GPT-3.5, GPT-4, Llama2-13b) and for multimodal VLM agents (LLaVA, CogVLM). Memory constructed with one model (GPT-3.5) can improve performance of another model (Llama2-13b), demonstrating cross-model transfer of episodic memory.",
            "limitations_or_challenges": "Paper notes that passing full trajectories can introduce noise, so RAP uses windowed trajectory segments; retrieval weights and modes must be adapted per environment (task-space constraints); performance depends on quality and relevance of stored successful episodes and on accurate similarity scoring (risk of retrieval mismatches). No claims about memory scaling limits or catastrophic forgetting are empirically detailed beyond these practical considerations.",
            "uuid": "e3045.0",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct",
            "brief_description": "An LLM agent paradigm that interleaves chain-of-thought style reasoning ('Thoughts') and environment-interacting actions, enabling dynamic action-state synergy; used in this paper as a baseline agent without an external episodic memory database.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "ReAct",
            "agent_description": "LLM-driven agent that alternates between reasoning and acting (thought-action interleaving), producing action plans and executing them in the environment; it relies on the LLM context and generated intermediate reasoning rather than an external episodic memory store.",
            "memory_used": false,
            "memory_type": null,
            "memory_mechanism_description": null,
            "task_name": "ALFWorld, WebShop (evaluated as baseline)",
            "task_description": "Same multi-step text planning and web interaction benchmarks used in the paper (ALFWorld and WebShop).",
            "task_type": "planning / tool-use (textual)",
            "performance_with_memory": null,
            "performance_without_memory": "ALFWorld (GPT-3.5, ReAct) overall success 52.2%; WebShop (GPT-3.5, ReAct) success 35.0% (score 61.8%).",
            "has_performance_comparison": true,
            "key_findings": "ReAct serves as a strong baseline for LLM agents but is outperformed by RAP when RAP leverages an external episodic memory of successful trajectories; lacking an external retrieval-augmented memory limits ReAct on tasks where reuse of past episodic patterns is beneficial.",
            "limitations_or_challenges": "Does not include an external episodic memory of prior successful task executions, which the paper identifies as a disadvantage for certain multi-step and multimodal tasks where prior experiences provide strong cues.",
            "uuid": "e3045.1",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion",
            "brief_description": "A prior approach that requires LLM agents to perform self-reflection on unsuccessful episodes to improve future performance; cited and evaluated as a baseline in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Reflexion",
            "agent_description": "Method that collects failed episodes and prompts the LLM to 'reflect' (generate insights) about failures to guide future attempts; this creates a textual reflective memory used to improve future behavior.",
            "memory_used": true,
            "memory_type": "reflective/episodic memory (textual self-reflection on failures)",
            "memory_mechanism_description": "Stores traces of unsuccessful episodes and elicits LLM self-reflection to derive corrective guidance; reflexive outputs are then used to influence future attempts (as described in the literature and used as a baseline in experiments).",
            "task_name": "ALFWorld, WebShop (evaluated as baseline)",
            "task_description": "Multi-step text-based planning and web interaction tasks where learning from previous failures can help improve subsequent trials.",
            "task_type": "planning / iterative improvement (textual)",
            "performance_with_memory": "ALFWorld (GPT-3.5, Reflexion) overall success reported 74.6%; WebShop (GPT-3.5, Reflexion) success 35.0% (score 61.8%).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Reflexion improves over ReAct in some settings by leveraging textual self-reflection on prior failures, but in this paper RAP (retrieval of successful episodic trajectories) outperforms Reflexion on the evaluated benchmarks.",
            "limitations_or_challenges": "Reflexion operates primarily on textual reflections derived from failures; authors argue that directly retrieving diverse successful experiences (as RAP does) can be more effective for generalization across tasks.",
            "uuid": "e3045.2",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ADaPT",
            "name_full": "ADaPT",
            "brief_description": "An approach that adaptively decomposes tasks into sub-tasks and re-executes as needed; cited and evaluated as a baseline in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "ADaPT",
            "agent_description": "Technique that decomposes complex tasks into sub-tasks and selectively re-executes or refines sub-policies to complete tasks; referenced as prior work improving LLM-based task solving.",
            "memory_used": true,
            "memory_type": "task-decomposition / short-horizon re-execution memory (uses subtask traces)",
            "memory_mechanism_description": "ADaPT performs decomposition and re-execution, which implies use of previous subtask traces for corrective re-runs; in the paper it is treated as a baseline rather than a primary focus.",
            "task_name": "ALFWorld, WebShop (evaluated as baseline)",
            "task_description": "Multi-step reasoning and interaction tasks where decomposition into subtasks can improve execution.",
            "task_type": "planning / decomposition (textual)",
            "performance_with_memory": "ALFWorld (GPT-3.5, ADaPT) overall success ≈ 71.6% (reported in comparisons); WebShop (GPT-3.5, ADaPT) success 43.0% (score 64.0%).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "ADaPT improves over simple ReAct in some textual benchmarks by decomposing problems, but RAP (episodic retrieval of successful trajectories) achieves higher success rates in the reported experiments.",
            "limitations_or_challenges": "Paper notes ADaPT is limited to reflecting on trajectories within the same task instance rather than generalizing across diverse successful examples, which RAP targets explicitly.",
            "uuid": "e3045.3",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A retrieval-augmented generation approach that augments generative models with retrieved external documents as context; cited in related work as foundational for retrieval-augmented methods.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "RAG (conceptual reference)",
            "agent_description": "Retrieval + generation paradigm where retrieved passages are passed into a generative model to produce context-grounded outputs; often used for knowledge-intensive NLP tasks.",
            "memory_used": true,
            "memory_type": "retrieval-augmented external knowledge store",
            "memory_mechanism_description": "Retrieve relevant documents by similarity and feed them into the generator as extra context; paper cites this as background motivating retrieval-augmented memory for agents.",
            "task_name": "N/A (cited as methodology)",
            "task_description": "Used in knowledge-intensive generation tasks in literature; here cited as prior art motivating RAP.",
            "task_type": "knowledge-augmented generation (general)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "RAP builds on the retrieval-augmented idea of RAG but applies it to episodic trajectory retrieval for sequential decision-making and multimodal agents, rather than document retrieval for QA.",
            "limitations_or_challenges": null,
            "uuid": "e3045.4",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ExpeL",
            "name_full": "ExpeL",
            "brief_description": "A prior method that aggregates generated experiences and feeds them back into LLMs for reflection/learning; cited in related work as an alternative way of using experience.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "ExpeL (as cited)",
            "agent_description": "Approach that collects experiences generated during task attempts and has the LLM reflect on them (textually) to improve future performance; discussed as related work contrasting with RAP's implicit retrieval-from-diverse experiences approach.",
            "memory_used": true,
            "memory_type": "experience aggregation / reflective memory",
            "memory_mechanism_description": "Passes collected experiences into the LLM to perform textual reflection to derive improvements; cited to contrast explicit reflection versus RAP's implicit retrieval-analogy mechanism.",
            "task_name": "N/A (cited as methodology)",
            "task_description": "Prior work used for iterative agent improvement; mentioned to position RAP's different design choice.",
            "task_type": "iterative improvement / learning from experience",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Paper contrasts ExpeL's explicit reflective step with RAP's approach of implicitly drawing analogies from a diverse memory of successful episodes without an extra re-tasking step.",
            "limitations_or_challenges": null,
            "uuid": "e3045.5",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Adapt: As-needed decomposition and planning with language models",
            "rating": 2,
            "sanitized_title": "adapt_asneeded_decomposition_and_planning_with_language_models"
        },
        {
            "paper_title": "ExpeL",
            "rating": 1
        },
        {
            "paper_title": "LLaVA: Large Language and Vision Assistant",
            "rating": 1,
            "sanitized_title": "llava_large_language_and_vision_assistant"
        },
        {
            "paper_title": "CogVLM: Visual Expert for Pretrained Language Models",
            "rating": 1,
            "sanitized_title": "cogvlm_visual_expert_for_pretrained_language_models"
        }
    ],
    "cost": 0.016905,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents
6 Feb 2024</p>
<p>Tomoyuki Kagaya <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#107;&#97;&#103;&#97;&#121;&#97;&#46;&#116;&#111;&#109;&#111;&#121;&#117;&#107;&#105;&#64;&#106;&#112;&#46;&#112;&#97;&#110;&#97;&#115;&#111;&#110;&#105;&#99;&#46;&#99;&#111;&#109;">&#107;&#97;&#103;&#97;&#121;&#97;&#46;&#116;&#111;&#109;&#111;&#121;&#117;&#107;&#105;&#64;&#106;&#112;&#46;&#112;&#97;&#110;&#97;&#115;&#111;&#110;&#105;&#99;&#46;&#99;&#111;&#109;</a> 
Equal contribution</p>
<p>Panasonic Connect Co., Ltd
Japan</p>
<p>Jing Thong <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#106;&#105;&#110;&#103;&#121;&#117;&#97;&#110;&#46;&#116;&#104;&#111;&#110;&#103;&#64;&#115;&#103;&#46;&#112;&#97;&#110;&#97;&#115;&#111;&#110;&#105;&#99;&#46;&#99;&#111;&#109;">&#106;&#105;&#110;&#103;&#121;&#117;&#97;&#110;&#46;&#116;&#104;&#111;&#110;&#103;&#64;&#115;&#103;&#46;&#112;&#97;&#110;&#97;&#115;&#111;&#110;&#105;&#99;&#46;&#99;&#111;&#109;</a> 
Yuan 
Equal contribution</p>
<p>Panasonic R&amp;D Center
Singapore</p>
<p>Yuxuan Lou lou@u.nus.edu&gt;. 
Equal contribution</p>
<p>National University of Singapore
Singapore</p>
<p>Jayashree Karlekar 
Panasonic R&amp;D Center
Singapore</p>
<p>Sugiri Pranata 
Panasonic R&amp;D Center
Singapore</p>
<p>Akira Kinose 
Panasonic Connect Co., Ltd
Japan</p>
<p>Koki Oguri 
Panasonic Connect Co., Ltd
Japan</p>
<p>Felix Wick 
Panasonic R&amp;D Center Ger-many
Germany</p>
<p>Yang You 
National University of Singapore
Singapore</p>
<p>Thong Jing Yuan</p>
<p>RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents
6 Feb 202479DDD7B142ED638CFA471A76298A3560arXiv:2402.03610v1[cs.LG]
Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration.However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges.Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities.RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks.Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks.These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.</p>
<p>Introduction</p>
<p>Recent research has revealed the high inferential abilities of Large Language Models (LLMs) as agents (Wang et al., 2023b;Xi et al., 2023), indicating their potential application in various areas like decision-making tasks and robotic control.Previous works such as ReAct (Yao et al., 2023) have shown that LLMs can generate accurate actions as language agents by iteratively performing actions and reasoning.</p>
<p>In this paper, we introduce a novel framework, Retrieval-Augmented Planning (RAP), which embodies a pivotal human ability -to leverage generalized past experiences for current tasks -and apply it to LLM agents.Our approach involves storing past experiences in memory, retrieving them appropriately based on the similarity with present context including multimodal information, and generating subsequent actions via in-context learning, thereby enhancing the decision-making capacity of language agents.Central to this framework is the LLMs' ability to perform analogy-making from various abstracted patterns (Mirchandani et al., 2023).Leveraging this capability, our memory stores both context and action-observation trajectories for each experience.The approach effectively facilitates deriving correct actions from memory examples within task constraints.Furthermore, by storing multimodal information in memory and considering it when retrieving past experiences, our approach flexibly utilizes multimodal information with LLMs and Vision-Language Models (VLMs) separately for language agents.Consequently, our approach proves to be effective for memory utilization by language agents in both decision-making and robotics tasks, in textual and multimodal environments.Specifically, RAP achieves 33.6%, 13.0%, 18.2%, and 12.7% gain over ReAct on the ALFWorld, Webshop, Franka Kitchen, and Meta World benchmarks respectively.</p>
<p>To summarize, our contributions are as follows:</p>
<p>• We propose RAP, a novel framework that enhances LLM agents' planning capacity.It strategically enriches the decision-making process by storing past experiences and intelligently retrieving them based on their similarity to the current situation.• RAP is capable of being applied not only in textual environments but also in multimodal embodied tasks, marking it as a pioneering effort in employing memory retrieval techniques for multimodal agents, a first in this domain to our knowledge.• We validate the effectiveness of RAP empirically across both textual and multimodal benchmarks.RAP shows significant improvements compared to prior SOTA methods in both types of environments.</p>
<p>Related Work</p>
<p>Language Models and Vision-Language Models as Foundations</p>
<p>Large Language Models (LLMs) such as GPT (OpenAI, 2023) and the LLaMA series (GenAI, 2023), leveraging transformer architecture and self-supervised learning objectives like Next Token Prediction, have excelled in generating coherent, human-like text.These models, pre-trained on extensive text corpora, possess vast linguistic knowledge and reasoning abilities.Extending beyond LLMs, multi-modal domains have given rise to Vision-Language Models (VLMs) (Yin et al., 2023) such as LLaVA (Liu et al., 2023) and CogVLM (Wang et al., 2023c), which integrate textual and visual inputs, exemplified by LLaVA's training on image-caption pairs from the CC3M datasets (Sharma et al., 2018).Our work utilizes these foundations to build agents for textual and embodied environments: text-based agents employing LLMs, and embodied agents integrating VLMs for visual perception and action planning.We focus on enhancing these agents' planning capabilities through memory retrieval techniques, enabling them to selectively access and utilize relevant memory for improved sequential decision-making.</p>
<p>Language Models as AI Agents</p>
<p>Recent works have leveraged LLM's anthropomorphic capabilities when building autonomous agents.These agents can be depicted into having 4 key aspects: Profile (agent characteristics), Memory (past information), Planning (future strategies) and Action (execution policies) (Wang et al., 2023a).A notable example is Chain-of-Thought (CoT) (Wei et al., 2022), where agents are encouraged to mirror human cognitive mechanisms by incorporating reasoning into intermediate steps for complex problem-solving tasks.With a dynamic reasoning process, ReAct (Yao et al., 2023) interleaves generated actions and environmental states, improving the reasoning ability through action-state synergy.</p>
<p>Our work seeks to enhance the ReAct framework by allowing agents to identify specific objects within observations and additionally retrieve relevant aspects of past experiences</p>
<p>Retrieval-Augmented Generation with Memory</p>
<p>Among works (Madaan et al., 2022;Liu et al., 2021;Su et al., 2022) that seek to derive better answers from LLMs via memory, RAG (Lewis et al., 2020) is a notable method that combines retrieval-based mechanisms with generative models.Responses from memory are selected based on similarity and passed into LLMs as additional context to deliver outputs that are creative and contextually-grounded.Building on RAG, Reflexion (Shinn et al., 2023) requires LLMs to self-reflect on unsuccessful tasks for self-improvement in solving tasks over time.ADaPT (Prasad et al., 2023) further decomposes into sub-tasks and re-executes where necessary.Yet, these works only reflect on trajectories within the task.Hence, these insights are often restricted to each task.Building on Reflexion, ExpeL (Zhao et al., 2023) passes all generated experiences into the LLM to reflect in a text-based manner.In contrast, our work adopts a different approach by implicitly drawing from a diverse range of experiences from memory without explicitly requiring an additional step of re-tasking the LLM to extract insights.With this approach, our agent can not only efficiently generalize experiences from other successful tasks to solve the current task, but also be flexible enough to extract relevant components from experiences for current task, enhancing the agent's ability to expand its memory from textual to multimodal contexts.</p>
<p>RAP: Retrieval-Augmented Planning</p>
<p>We developed Retrieval-Augmented Planning (RAP), a framework that leverages past experiences to facilitate decision-making according to the current context.Fig. 2 provides an overview of the framework, which consists of four core components: Memory, Reasoner, Retriever, and Executor.The specific details of each module will be discussed in sections 3.2 to 3.5.</p>
<p>Preliminaries</p>
<p>In this work, we consider an agent operating in a particular environment and assigned with completing some task T.</p>
<p>Memory</p>
<p>To enable retrieval-augmented planning, we first construct memory databases.The databases contain logs of prior successful task executions.For each log L i completing a task T i in H i steps, we record the task information T i , the overall plan p i , the trajectory of the agent τ Li including plans, actions, and observations sequences.
L i = {T i , p i , τ Li } (1) τ Li = { ⃗ p ′ Li , ⃗ α Li , ⃗ o Li }(2)
For textual environments, the observations are textual descriptions of the world state.For multimodal environments, the observations are visual representations -images from a fixed viewpoint camera after each agent action.</p>
<p>The logs are collected by having agents attempt the tasks and saving streams of successful episodes.The episodic logs capture the steps needed to complete the tasks.Storing these examples allows the agents to leverage prior experience when planning for new instances of the tasks.</p>
<p>During interactions with the environment, the agents can selectively retrieve relevant memory samples to make more informed action decisions.For text tasks, the textual logs provide crucial context.For embodiment, prior visual observations reveal outcomes of actions in the space.By retrieving prototypical executions, the agents can plan smarter policies while avoiding past failures.The memory augmentation thus equips the models with vital environmental knowledge for sequential decision-making.</p>
<p>Reasoner</p>
<p>The Reasoner generates overall plans, action plans, and retrieval keys based on the agent's current situation and action trajectory, using LLMs.Initially, the Reasoner produces the overall plan from the task information.Based on the task and the overall plan, the Reasoner generates an action plan.Also, in accordance with ReAct, an action or action plan is dynamically generated by LLMs, considering the current task status.If an action plan is generated, a retrieval key is created based on the generated action plan.For instance, in ALFWorld (Shridhar et al., 2021), if an action plan like "I need to find the watch" is generated, the retrieval key would be "search watch".Hence, the Reasoner enables agents to take into consideration the current situation and context.</p>
<p>Retriever</p>
<p>The Retriever is designed to extract the most relevant memory logs to guide the agent's subsequent actions to complete the current task.This process is shown in Fig. 3 (Left).</p>
<p>The similarity score, comparing the current state S 0 with log L i , is calculated as a weighted average of the task similarity, overall plan alignment, and retrieval key congruence.</p>
<p>Let the current agent task be T 0 , the overall plan be p 0 , and the retrieval key generated by the Reasoner based on current action plan p ′ be k 0 .The similarity score between current state S 0 with log L i is calculated as a weighted average of the similarity score for task, overall plan, and retrieval key.
Score(S 0 , L i ) = w t • sim(T 0 , T Li ) + w p • sim(p 0 , p Li ) + w k • sim(k 0 , τ Li )(3)
Each component's similarity score is determined using cosine similarity of their feature representations.For text data, the representations are derived using sentence-transformers (Reimers &amp; Gurevych, 2019).For images, the representa-tions are generated with a CLIP-based Vision Transformer.</p>
<p>The similarity score between the retrieval key and the log trajectory is adaptive based on environment type and retrieval key type.In multimodal environments, the retrieval key corresponds to agents' current visual observation.Thus, the retrieval-key similarity score is the score between current and logged visual trajectory observations as in equation ( 4).
sim(k 0 , τ Li ) = max(cos sim(k visual 0 , o j )), for o j ∈ ⃗ o Li(4)
In textual environments, the retrieval-key similarity score is adaptive based on key type.In scenarios involving the retrieval key for searching or locating objects, the similarity score is calculated between the retrieval key and the logged textual trajectory observations, as in equation ( 5).
sim(k 0 , τ Li ) = max(cos sim(k text 0 , o j )), for o j ∈ ⃗ o Li (5)
For the case retrieval-key is interacting with object action planning, it is an action similarity score, as in equation ( 6).
sim(k 0 , τ Li ) = max(cos sim(k text 0 , α j )), for α j ∈ ⃗ α Li (6)
Furthermore, component weights are adaptively calibrated based on the environment.In environments with a constrained task space, task similarity is assigned a higher weight, For example, in Franka Kitchen environment which has only 5 tasks.We only retrieve logs with same task type.</p>
<p>For each retrieved experience, only a window of trajectory centered around the most similar action is passed to the agent.This allows agents to focus on actions most similar to current task, rather than full trajectories that may instead create additional noise to the agent.</p>
<p>In summary, our meticulously-crafted retrieval method efficiently identifies the most pertinent logs by calculating a weighted similarity score that takes into account various aspects including task information, overall planning, and retrieval key.This process ensures that the most relevant and contextually appropriate logs are selected from a vast repository of memory logs.Once these logs are retrieved, they serve as an invaluable resource for the large language model serving as Executor.</p>
<p>Executor</p>
<p>The Executor receives past experiences from the Retriever and generates the next action by utilizing these experiences through in-context learning.This process is illustrated in Fig. 3 (Right).By presenting the past experience aligned with the current context as a prompt, it enables accurate decisionmaking for the next action, mirroring the process humans leverage past experiences for future actions.Additionally, the length of the current task trajectory is used in the same way as past experiences, utilizing only a constant number of new trajectories.This encourages effective analogy-making from experiences through in-context learning in LLMs.</p>
<p>Experiments</p>
<p>To validate the effectiveness of our framework in various environments, we performed evaluations on four benchmarks.These include the text-based multi-step tasks in ALFWorld (Shridhar et al., 2021) and Webshop (Yao et al., 2022), and robotics tasks, FrankaKitchen (Gupta et al., 2019) and Meta-World (Yu et al., 2021), which are multimodal environments with texts and images.ALFWorld (Shridhar et al., 2021) is a synthetic text-based game that challenges an agent to solve multi-step tasks in a variety of interactive environments based on TextWorld (Côté et al., 2018).Following ReAct, we evaluated an agent in 134 unseen games, including six types of tasks: Pick, Clean, Heat, Cool, Look, and Pick2.In this environment, agents are required to accomplish complex tasks using textbased actions in a simulated household providing textual and image feedback.Following previous works (Yao et al., 2023;Shinn et al., 2023;Prasad et al., 2023), RAP runs recursively until it reaches a depth (trial) of 3. In RAP train , we use 1000 tasks from the provided training set, and run recursively with memory from successful tasks both from the training set and previous trials.Additionally, we use task information including the task type for the Retriever.During action-based similarity calculation, it extracts four experiences and ten actions from both before and after the most similar action.Meanwhile, during observation-based similarity calculation, it uses eight experiences and five actions from both before and after the most similar action.</p>
<p>We conducted evaluations using three models: GPT-3.5 (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), andLlama2-13b (GenAI, 2023).In Table 1, our experiments with GPT-3.5 show that RAP(85.8%) and RAP train (91.0%) achieve a significantly higher success rate compared to previous works such as ReAct(52.2%),Reflexion(74.6%),and ADaPT(71.6%).Further, RAP is also effective even with high-performance models like GPT-4, and locally-operated models such as Llama2-13b, thus illustrating the efficacy of our method across various LLMs.</p>
<p>Figure 4 highlights the progression of improvements to success rate over three trials, where RAP shows higher success rates compared to ReAct, thus indicating the effective utility of successful experiences from other tasks in memory.In addition, RAP with GPT-3.5 achieves performance equivalent * We use the performance reported by (Prasad et al., 2023) to ReAct with GPT-4.Also, RAP with memory built from the training set via GPT-3.5 surpasses ReAct with GPT-4.</p>
<p>WEBSHOP</p>
<p>WebShop (Yao et al., 2022) is a web application that simulates online shopping, where agents are required to select products for purchase based on a given user instruction.</p>
<p>WebShop contains a total of 1.18M real-world products featured on Amazon, and comprises a wide variety of structured and unstructured texts.Following Reflexion (Shinn et al., 2023) and ADaPT (Prasad et al., 2023), we evaluated an agent across 100 instructions.For each instruction, agents are required to reason and select a desired product that is most aligned to the given instruction based on observations returned by the web application, and perform additional precise interactions with the portal to navigate through the web application such as searching or clicking buttons.Such interactions are performed in a text-based manner where the agent issues a textual command into the web application.Following previous studies, we allow the agent to run recursively until it reaches a depth (trial) of 3.</p>
<p>During our evaluation, we ran the initial trial with a Re-Act agent and formulated the memory database based on successful tasks.The memory database would be further expanded for subsequent trials based on successful tasks in the preceding trials.Here, successful tasks are counted as those with a reward of 1.In addition, during retrieval of actions in memory, the Retriever extracts three experiences and five actions from before and after the most similar action.</p>
<p>Moreover, unlike other environments where objects are generalizable across different tasks, WebShop has an additional unique feature where actions in each task are highly dependent on the scenario outlined in that task.As such, apart from the correlation between the current reasoning and trajectories in memory, our agent also considers the relationship between the action of each task in memory that is most similar to the current action and its corresponding scenario for that task.This builds on the concept of "A is to B as C is to D", where the generated action depends not only on similar trajectories in memory, but also how these trajectories relate back to their scenario, and how the current trajectory should be related to the current scenario.By incorporating intra-task relationships, this allows the agent to better reason how the actions in memory are correlated with their own scenarios, and thereafter generate an action that is also aligned to the current scenario at hand.</p>
<p>We performed evaluations using two different models: GPT-3.5 (Ouyang et al., 2022) and Llama2-13b (GenAI, 2023).</p>
<p>In Table 2, experiments with GPT-3.5 demonstrate that our method (48.0%) achieves a higher success rate compared to previous studies such as ReAct (35.0%),Reflexion (35.0%), and ADaPT (43.0%).Furthermore, our method is able to achieve a higher overall reward score (76.1%) as compared to ReAct (61.8%),Reflexion (61.8%) and ADaPT (64.0%).</p>
<p>Multimodal Environments</p>
<p>We evaluated our proposed technique in embodied multimodal agents on two benchmark environments: Franka Kitchen and Meta-World.These simulations offer a diverse set of household and robotic manipulation tasks requiring visual perception and physical interaction.</p>
<p>We constructed embodied agents using two VLM foundations -LLaVA and CogView.For each VLM, we compared task performance of the base model to a RAP-enhanced agent utilizing our memory retrieval system.</p>
<p>The Franka Kitchen benchmark consists of compound tasks like arranging objects and preparing meals.Meta-World provides a suite of 50 distinct robotic skills focused on fine manipulation.For both sets, the agent must plan actions based on visual observations in an interactive 3D environment.We report quantitative results on task success rates with and without RAP augmentation.Our method allows the VLM Agents to selectively reference prior successful executions during planning.This provides vital visual context and demonstrates the benefits of memory-augmented reasoning for embodied agents.</p>
<p>To map the high-level plans of the VLM agents to executable environment actions, we train a policy network on 25 demonstrations for each task.We evaluate on 5 subtasks with 2 different camera views per benchmark.For each (task, view) combination we run 50 trials with different random seeds and report success rates.</p>
<p>Table 3 and Figure 6 shows that RAP can significantly enhance embodied multimodal agents planning on both benchmarks.The results offer insights into how memory can aid these models for sequential decision making and embodied tasks requiring interactive visual perception.</p>
<p>Ablation Study</p>
<p>In this section, we provide additional experimental results and showcase some critical components of experiences that are required to be stored into memory.</p>
<p>Evaluation across various Retrievers</p>
<p>ALFWORLD EVALUATION</p>
<p>We perform evaluation on ALFWorld with GPT-3.5 by varying the Retriever, as shown in Table 4.The results of RAP act and RAP obs illustrate the effectiveness of switching the information used for retrieval depending on the situation.</p>
<p>Furthermore, we utilize visual observation provided by ALF-World instead of textual observation, and perform an evaluation using similarity between textual retrieval key and image observation with CLIP (Radford et al., 2021).As a result, slightly better performance is demonstrated than when using text observation information.This suggests that employing direct image data, rather than information converted into text, could enable more effective retrieval.Here, all evaluations are performed on Llama2-13b.</p>
<p>As shown in Table 5, through RAP obs , the agent is able to retrieve trajectories from memory based on either actions or observations, depending on the current stage of solving the task.With the incorporation of intra-task similarity in RAP intra , the agent is able to align the relationship between task information and the corresponding trajectories of each experience when projecting to the current task.By retrieving based on product category in RAP cat , the agent is able to retrieve experiences that are more related to the current task.</p>
<p>Overall, RAP takes into account these components, resulting in an overall boost of 6.5% and 5.0% for overall reward and success rate respectively on Llama2-13b.With these, RAP also demonstrates a boost of 14.3% and 13.0% for overall reward and success rate respectively on GPT-3.5 in Table 2, showcasing RAP's generalizability across different models.</p>
<p>Transfer Learning via Memory</p>
<p>RAP is capable of utilizing past experiences that are stored in memory.Since the experience of solving tasks is independent of the model, the model used for evaluation does not need to match the one used for memory construction.Here, we illustrate a verification of transfer learning between models by using memory constructed via different models for the evaluation model.From Section 4.1.1,we use 1000 samples from training data, but no recursive trial is conducted (d max = 1) to simply verify the effect of transfer learning.</p>
<p>Table 6 shows results of transfer learning, which indicate memory generated with GPT-3.5 is also effective in Llama2-13b.Thus, RAP allows sharing experiences across models.</p>
<p>Table 6.ALFWorld success rate(%) with Memory and dmax=1.</p>
<p>ModelMemory indicates the language model used to construct memory from the training data.</p>
<p>Model</p>
<p>ModelMemory Success Rate GPT-3.5 -44.0 GPT-3.5 GPT-3.5 63.4 Llama2-13b -20.9 Llama2-13b GPT-3.5 27.6</p>
<p>Conclusion</p>
<p>We propose Retrieval-Augmented Planning (RAP), which stores past experiences, extracts pertinent experiences from multimodal information such as text and images, and guides subsequent actions.Our framework demonstrated superior performance compared to baseline methods in various LLMs and across four diverse agent and robotics benchmarks.Through these results, our framework enables language agents to flexibly utilize past experiences in accordance with current situations, mirroring a human ability, and thereby enhancing decision-making capabilities.</p>
<p>Impact Statements</p>
<p>This paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p>
<p>B.1.2. PROMPT FOR RETRIEVAL KEY IN REASONER</p>
<p>Here are examples.think: First I need to find a spraybottle.A spraybottle is more likely to appear in cabinet (1-4), countertop (1), toilet (1), sinkbasin (1-2), garbagecan (1).I can check one by one, starting with cabinet 1. &gt; search: spraybottle think: Now I put the first creditcard in dresser.Next, I need to find the second creditcard.I can directly go to countertop 1. &gt; search: creditcard think: Now I take a pen (2).Next, I need to find a desklamp.A desklamp is more likely to appear in dresser (1), shelf (1-9), bed (1), garbagecan (1), drawer (1-10  Action: think[For mn4 color foundation for sensitive skin, the item has options '1 ', 'dc1', 'dn3', 'dn4', 'lc1', 'ln3', 'ln4', 'lw2','mc1','mn3','mn4', and'</p>
<p>Figure 1 .
1
Figure 1.Overview of RAP.Our framework stores past experiences and retrieves them based on the current situation.Left: The evaluation process on ALFWorld.ICL stands for in-context learning.Right: The evaluation process on Franka Kitchen.</p>
<p>Figure 2 .
2
Figure 2. RAP Core Components based on the current context.By doing so, our agent can adaptably receive different experiences at different points in time that are most similar to the situation at hand.</p>
<p>Figure 3 .
3
Figure 3. Memory-Retrieval in RAP.Left to Middle: The Retriever, calculating similarities with Memory, dynamically switches between action or observation based on the Retrieval Key.This figure illustrates the process of calculating similarity with observation.Right: Executor receives related experiences from memory and utilizes them in the prompt.</p>
<p>Figure 4 .
4
Figure 4. Success Rate of ALFWorld with trial number 4.1.Textual Environments 4.1.1.ALFWORLD</p>
<p>Figure 5
5
Figure5illustrates a comparison of the improvement in success rate and reward score over three trials for ReAct and RAP, with both methods using GPT-3.5.Here, it indicates a marked improvement in both success rate and reward score for our method, indicating that successful experiences from the other tasks are being effectively utilized.</p>
<p>Figure 5 .
5
Figure 5. Success Rate and Reward Score of WebShop over 3 trials</p>
<p>Figure 6 .
6
Figure6.Evaluation on Franka-Kitchen and Meta-World Benchmark.We evaluate with two vision-language models LLaVA and CogVLM both w/wo our proposed RAP method.The results demonstrate that our method notably enhances the performance of multimodal LLM agents in executing embodied tasks.</p>
<p>Action</p>
<p>[dc1][dn3][dn4][lc1][ln3][ln4][lw2]  [mc1][mn3][mn4][mw2]  size [1 fl oz (pack of 1)][1 ounce (pack of 1)][1 fluid ounce] Physicians Formula The Healthy Foundation with SPF 20, LW2, 1 Fl Oz Price: $7.59 Rating: N.A. [Description] [Features] [Reviews] [Attributes] [Buy Now] Act 11: think[For mn4 color foundation for sensitive skin, the item has options 'lw2' and '1 fluid ounce' and seems good to buy.to Search] Page 1 (Total results: 50) [Next &gt;] [B077PR9TL4] Physicians Formula The Healthy Foundation with SPF 20, LW2, 1 Fl Oz $7.59 [B07VLKYJNH] P ÜR 4-in-1 Love Your Selfie Longwear Foundation &amp; Concealer -Full Coverage Foundation &amp; Concealer Makeup For Dark Spots, Blemishes and Imperfections -Long-Lasting Liquid Makeup &amp; Skincare $36.0 [B09J1X9M1P] Neutrogena Healthy Skin Sensitive Skin Serum Foundation with Pro-Vitamin B5, Color Correcting &amp; Pore Minimizing Liquid Foundation &amp; Face Serum, Buildable Coverage, Medium 02, 1 oz $14.39 Action: think[B077PR9TL4, B07VLKYJNH, and B09J1X9M1P are mn4 color foundation for sensitive skin and price lower than 40.00 dollars.[dc1][dn3][dn4][lc1][ln3][ln4][lw2][mc1][mn3][mn4][mw2] size [1 fl oz (pack of 1)][1 ounce (pack of 1)][1 fluid ounce] Physicians Formula The Healthy Foundation with SPF 20, LW2, 1 Fl Oz Price: $7.59 Rating: N.A. [Description] [Features] [Reviews] [Attributes] [Buy Now]</p>
<p>mw2' and seems good to buy.] Observation: OK.Action: click[mn4] Observation: You have clicked mn4.Action: click[Buy Now] Observation: Your score (min 0.0, max 1.0): 1.0 Success</p>
<p>The agent forms an overall plan p, then interacts with the environment over a finite horizon of H timesteps.At each timestep t ∈ {1, 2, . . .H}, the agent forms an action plan p ′ t , selects an action α t from the action space A and receives an observation o t from the observation space O.
The trajectoryτ = { ⃗ p ′ t , ⃗ α t , ⃗ o t } of the agent up to time t consists of the sequences of plans ⃗ p ′ t = (p ′ 1 , ...p ′ t ), actions ⃗ α t = (α 1 , ...α t )and observations ⃗ o
t = (o 1 , ...o t ).</p>
<p>Table 1 .
1
ALFWorld task-specific success rate(%).
Method(dmax=3)ModelPick CleanHeatCool Look Pick2AllActGPT-3.566.751.673.961.938.917.653.7ReActGPT-3.550.041.973.966.755.623.552.2ReflexionGPT-3.575.077.465.276.283.370.674.6ADaPT *GPT-3.587.580.660.876.261.152.971.6RAP(Ours)GPT-3.595.887.178.390.588.970.685.8RAPtrain(Ours)GPT-3.595.8 100.082.685.7 100.076.591.0ReActGPT-483.371.095.781.0 100.094.185.8RAP(Ours)GPT-495.890.3100.0 95.2 100.088.294.8ReActLlama2-13b 29.241.934.852.438.917.636.6RAP(Ours)Llama2-13b 62.561.356.561.944.417.653.0</p>
<p>Table 2 .
2
WebShop Score (%) and Success Rate(%).
Method(dmax=3)ModelScore Success RateReActGPT-3.561.835.0ReflexionGPT-3.561.835.0ADaPTGPT-3.564.043.0RAP(Ours)GPT-3.576.148.0ReActLlama2-13b64.631.0RAP(Ours)Llama2-13b71.136.0</p>
<p>Table 3 .
3
Average success rates on Franka Kitchen and Meta World of Vision-Languge Model Agents w/wo RAP(%)
MethodFranka Kitchen Meta WorldLLaVA43.465.4LLaVA with RAP61.679.2CogVLM44.261.5CogVLM with RAP56.968.8</p>
<p>Table 4 .
4
ALFWorld success rate(%) with different retrievers.RAPact uses only action information, while RAP obs utilizes only observation information for retrieval.RAP clip refers to evaluations using images, rather than texts, as observations with CLIP.
Method(dmax=3) Success RateReAct52.2RAPact82.1RAP obs84.3RAP clip86.6RAP85.85.1.2. WEBSHOP EVALUATIONIn this subsection, we illustrate the effect of varying thecomponents extracted from each experience by the Retriever.</p>
<p>Table 5 .
5
Webshop overall score (%) and success rate(%) with different retrievers on Llama2-13b.RAP obs uses additional retrieval by observations on top of action-based retrieval in RAPact.Also, RAPintra and RAPcat uses intra-task retrieval and product-category retrieval.RAP indicates combination of RAP obs , RAPintra and RAPcat.
Method(dmax=3) Success Rate Overall ScoreReAct31.064.6RAPact33.068.6RAP obs33.069.0RAPintra34.069.3RAPcat35.069.9RAP36.071.1</p>
<p>Here is the task.Please make an action from the examples.Task: put some watch on safe.Plan: To solve the task, I need to find and take a watch, then put it on the safe.
). I can check one by one, starting with dresser 1. think: Now I find the second saltshaker (2). Next, I need to take it. &gt; search: desklamp think: Now I find a lettuce (1). Next, I need to take it. &gt; action: take think: Now I find a pan (1). Next, I need to take it. &gt; action: take &gt; action: take think: Now I heat an egg (2). Next, I need to put it in/on diningtable 1. &gt; action: put think: Now I take a spraybottle (2). Next, I need to put it in/on toilet 1. &gt; action: put think: Now I take an apple (1). Next, I need to go to a microwave (1) and heat it. &gt; action: heat think: Now I take a bread (1). Next, I need to go to a microwave (1) and heat it. &gt; action: heat think: Now I take a mug (3). Next, I need to go to a fridge (1) and cool it. &gt; action: cool think: Now I take a potato (2). Next, I need to go to a fridge (1) and cool it. &gt; action: cool think: Now I find a desklamp (1). Next, I need to use it. &gt; action: use think: Now I find a desklamp (3). Next, I need to use it. &gt; action: use Here is the task. Please make a plan from the examples. think: First I need to find a watch. A watch is more likely to appear in/on cabinet (1-4), drawer (1-5). I can check one by one, starting with cabinet 1 &gt; B.1.3. PROMPT FOR ACTION PLAN IN REASONER AND FOR ACTION IN EXECUTOR Here is the task information. You are in the middle of a room. Looking quickly around you, you see a armchair 1, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a dresser 1, a garbagecan 1, a safe 1, a shelf 12, a shelf 11, a shelf 10, a shelf 9, a shelf 8, a shelf 7, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, a shelf 1, a sidetable 1, and a sofa 1. Here are examples. {Task: put a vase in safe. Plan: To solve the task, I need to find and take a vase, then put it in the safe. &gt; think: First I need to find a vase. OK. &gt; go to shelf 1 On the shelf 1, you see nothing. ... &gt; go to shelf 6 On the shelf 6, you see a vase 2. &gt; think: Now I find a vase (2). Next, I need to take it OK. &gt; take vase 2 from shelf 6 You pick up the vase 2 from the shelf 6.} examples x n &gt; C. ALFWorld trajectories Task Description You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1. Your task is to: put a mug in desk. ReAct Act 1: think: To solve the task, I need to find and take a mug, then put it in desk Obs 1: OK. Act 2: think: First I need to find a mug. A mug is more likely to appear in drawer (1-6), shelf (1-6), desk (1-2), garbagecan (1), laundryhamper (1), safe (1). I can check one by one, starting with drawer 1 Obs 2: OK. Act 3: go to drawer 1 Obs 3: The drawer 1 is closed. Act 4: open drawer 1 Obs 4: You open the drawer 1. The drawer 1 is open. In it, you see a pen 1. ... Act 23: think: Hmm, it seems like the mug is not in any of the drawers. Let me check the other possible locations Obs 23: OK. ... Fail RAP Plan: To solve the task, I need to find and take a mug, then put it in desk. Act 1: think: First I need to find a mug Obs 1: OK. Retrieval key: search mug Act 2: go to shelf 1 Obs 2: On the shelf 1, you see a mug 2. Act 3: take mug 2 from shelf 1 Obs 3: You pick up the mug 2 from the shelf 1. Act 4: think: Now I find a mug (2). Next, I need to put it in/on desk 1 Obs 4: OK. Retrieval key: action put Act 5: go to desk 1 Obs 5: On the desk 1, you see a bowl 1, a cd 2, a creditcard 2, a desklamp 1, a keychain 1, a laptop 2, a pen 2, and a pencil 2. Act 6: put mug 2 in/on desk 1 Obs 6: You put the mug 2 in/on the desk 1. Success Task Description You are in the middle of a room. Looking quickly around you, you see a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a countertop 1, a garbagecan 1, a handtowelholder 2, a handtowelholder 1, a sinkbasin 2, a sinkbasin 1, a toilet 1, a toiletpaperhanger 1, and a towelholder 1. Your task is to: clean some cloth and put it in cabinet. ReAct Act 1: think: To solve the task, I need to find and take a cloth, then clean it with sinkbasin, then put it in a cabinet Obs 1: OK. Act 2: think: First I need to find a cloth. A cloth is more likely to appear in a cabinet (1-4), coun-tertop (1), sinkbasin (1-2), handtowelholder (1-2), towelholder (1), toiletpaperhanger (1), garbage-can (1). I can check one by one, starting with cabinet 1 Obs 2: OK. ... Act 6: open cabinet 2 Obs 6: You open the cabinet 2. The cabinet 2 is open. In it, you see a cloth 1. Act 7: take cloth 1 from cabinet 2 Obs 7: You pick up the cloth 1 from the cabinet 2. Act 8: think: Now I take a cloth (1). Next, I need to go to sinkbasin (1-2) and clean it Obs 8: OK. Act 9: go to sinkbasin 1 Obs 9: On the sinkbasin 1, you see nothing. Act 10: go to sinkbasin 2 Obs 10: On the sinkbasin 2, you see nothing. Act 11: think: I can't clean the cloth without water. I need to find a source of water Obs 11: OK. Act 12: go to toilet 1 ... Fail RAP Plan: To solve the task, I need to find and take a cloth, then clean it with sinkbasin, then put it in cabinet. Act 1: think: First I need to find a cloth Obs 1: OK. Retrieval key: search cloth Act 2: go to cabinet 1 Obs 2: The cabinet 1 is closed. Act 3: open cabinet 1 Obs 3: You open the cabinet 1. The cabinet 1 is open. In it, you see a toiletpaper 1. Act 4: go to cabinet 2 Obs 4: The cabinet 2 is closed. Act 5: open cabinet 2 Obs 5: You open the cabinet 2. The cabinet 2 is open. In it, you see a cloth 1. Act 6: take cloth 1 from cabinet 2 Obs 6: You pick up the cloth 1 from the cabinet 2. Act 7: think: Now I take a cloth (1). Next, I need to go to sinkbasin (1) and clean it Obs 7: OK. Retrieval key: action clean Act 8: go to sinkbasin 1 Obs 8: On the sinkbasin 1, you see nothing. Act 9: clean cloth 1 with sinkbasin 1 Obs 9: You clean the cloth 1 using the sinkbasin 1. Act 10: think: Now I clean a cloth (1). Next, I need to put it in/on cabinet (1-6) Obs 10: OK. Retrieval key: action put Act 11: go to cabinet 3 Obs 11: The cabinet 3 is closed. Act 12: open cabinet 3 Obs 12: You open the cabinet 3. The cabinet 3 is open. In it, you see a soapbottle 1. Act 13: put cloth 1 in/on cabinet 3 Obs 13: You put the cloth 1 in/on the cabinet 3. Task Description WebShop Instruction: i am looking for mn4 color foundation for my sensitive skin, and price lower than 40.00 dollars [Search] ReAct Action: search[mn4 color foundation sensitive skin] Observation: [Back to Search] Page 1 (Total re-sults: 50) [Next &gt;] [B077PR9TL4] Physicians For-mula The Healthy Foundation with SPF 20, LW2, 1 Fl Oz $7.59 [B07VLKYJNH] P ÜR 4-in-1 Love Your Selfie Longwear Foundation &amp; Concealer -Full Coverage Foundation &amp; Concealer Makeup For Dark Spots, Blemishes and Imperfections -Long-Lasting Liquid Makeup &amp; Skincare $36.0 [B09J1X9M1P] Neutrogena Healthy Skin Sen-sitive Skin Serum Foundation with Pro-Vitamin B5, Color Correcting &amp; Pore Minimizing Liquid Foundation &amp; Face Serum, Buildable Coverage, Medium 02, 1 oz $14.39 Action: think[B077PR9TL4 and B09J1X9M1P are mn4 color foundation for sensitive skin and price lower than 40 dollars.] Success D. Webshop trajectories Observation: OK.
A. ParametersIn Webshop, all experiments were performed with a temperature setting of 0.0, due to the additional requirement of precise commands for interactions with the web application.To evaluate RAP in multimodal environments, we focus on two benchmarks: Franka Kitchen and Meta-World.These benchmarks offer a diverse range of tasks, simulating real-world scenarios in a controlled environment, making them ideal for evaluating the performance of multimodal agents.Franka Kitchen Benchmark simulates a kitchen environment, where the agent interacts with various kitchen appliances and items.We have selected five specific tasks that test the agent's ability to manipulate objects and controls within this environment.These tasks are listed in  The Meta-World benchmark is designed to evaluate an agent's skill in more generalized object manipulation tasks.We have selected five tasks that represent a broad range of actions.Task No. Meta-World Tasks 1. assemble Pick up a nut and place it onto a peg 2.buttion Press a button 3. drawer Open a drawer 4. hammer Hammer a screw on the wall 5. binGrasp the puck from one bin and place it into another binE.2. Model SpecificationIn this subsection, we detail the model specifications for our multimodal agent, which is built upon two SOTA visionlanguage models: LLaVA and CogVLM.These models allows for a more comprehensive understanding and interaction with multimodal environments.LLaVA Model: Our agent utilizes the LLaVA-v1.513B model.This version of LLaVA incorporates the Vicuna-v1.513B as its underlying Large Language Model (LLM).The integration of Vicuna-v1.5 13B, known for its robust linguistic processing capabilities.CogVLM Model: For the CogVLM model, we employ the Cog-VLM 17B version.This model boasts a significant number of parameters -10 billion are dedicated to visual understanding, and 7 billion are focused on language processing.In our experimental setup for the language generation of these models, we set the temperature to 0.0.This setting is chosen to prioritize precision and determinism in the generated outputs, which is crucial for the consistency and reliability of the agent's planning responses in our multimodal tasks.E.3. Policy Network detailsPolicy Network is a crucial component of our framework designed to translate high-level action plans generated by the vision-language model into precise, low-level control actions suitable for the specific action space of the environment.Our approach utilizes a Multi-Layer Perceptron (MLP) neural network for this purpose.In both Franka Kitchen and Meta-World environments, the policy network's learning is facilitated through a few-shot learning approach, leveraging a limited but highly informative set of demonstration data.For each environment, we provide 25 expert demonstrations sourced from the D4RL dataset.These demonstrations consist of trajectories that include both observations and actions, showcasing expert-level performance in the respective tasks.In the Franka Kitchen tasks, each demonstration is composed of 50 state-action pairs, reflecting the sequence and specifics of actions required to complete the task.Meanwhile, for the Meta-World tasks, each demonstration sample comprises 500 state-action pairs.
Textworld: A learning environment for text-based games. M.-A Côté, Ákos Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, R Y Tao, M Hausknecht, L E Asri, M Adada, W Tay, A Trischler, arXivInternational Joint Conference on Artificial Intelligence (IJCAI). 2018. 2023GenAI, M. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. A Gupta, V Kumar, C Lynch, S Levine, K Hausman, Conference on Robot Learning (CoRL). 2019</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W.-T Yih, T Rocktäschel, S Riedel, D Kiela, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20. the 34th International Conference on Neural Information Processing Systems, NIPS'20Red Hook, NY, USACurran Associates Inc2020ISBN 9781713829546</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, ArXiv, volume abs/2304.084852023</p>
<p>. J Liu, D Shen, Y Zhang, B Dolan, L Carin, W Chen, 2021What makes good in-context examples for gpt-3? In arXiv</p>
<p>Memoryassisted prompt editing to improve gpt-3 after deployment. A Madaan, N Tandon, P Clark, Y Yang, S Mirchandani, F Xia, P Florence, B Ichter, D Driess, M G Arenas, K Rao, D Sadigh, A Zeng, Empirical Methods in Natural Language Processing. 20222023Conference on Robot Learning (CoRL)</p>
<p>Gpt-4 technical report. arXiv2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P F Christiano, J Leike, R Lowe, Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Learning transferable visual models from natural language supervision. A Prasad, A Koller, M Hartmann, P Clark, A Sabharwal, M Bansal, T Khot, A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, arXivProceedings of the 38th International Conference on Machine Learning. M Meila, T Zhang, the 38th International Conference on Machine LearningPMLR2023. Jul 2021139Adapt: As-needed decomposition and planning with language models</p>
<p>N Reimers, I Gurevych, - Sentence, Bert, arXivSentence Embeddings using Siamese BERT-Networks. 2019</p>
<p>Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. P Sharma, N Ding, S Goodman, R Soricut, 10.18653/v1/P18-1238Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. I Gurevych, Y Miyao, the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational LinguisticsJuly 20181</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, E Berman, A Gopinath, K Narasimhan, S Yao, Neural Information Processing Systems (NeurIPS). 2023</p>
<p>Aligning text and embodied environments for interactive learning. M Shridhar, X Yuan, M.-A Côté, Y Bisk, A Trischler, M Hausknecht, Alfworld, International Conference on Learning Representations (ICLR). 2021</p>
<p>Selective annotation makes language models better few-shot learners. H Su, J Kasai, C H Wu, W Shi, T Wang, J Xin, R Zhang, M Ostendorf, L Zettlemoyer, N A Smith, T Yu, arXiv2022</p>
<p>L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, W X Zhao, Z Wei, J.-R Wen, arXivA survey on large language model based autonomous agents. 2023a</p>
<p>. L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, W X Zhao, Z Wei, J.-R Wen, A Survey on Large Language Model based Autonomous Agents. In arXiv. 2023b</p>
<p>CogVLM: Visual Expert for Pretrained Language Models. W Wang, Q Lv, W Yu, W Hong, J Qi, Y Wang, J Ji, Z Yang, L Zhao, X Song, J Xu, B Xu, J Li, Y Dong, M Ding, J Tang, ArXiv, volume abs/2311.030792023c</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, 35</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, R Zheng, X Fan, X Wang, L Xiong, Y Zhou, W Wang, C Jiang, Y Zou, X Liu, Z Yin, S Dou, R Weng, W Cheng, Q Zhang, W Qin, Y Zheng, X Qiu, X Huang, Gui , T , arXivThe rise and potential of large language model based agents: A survey. 2023</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. S Yao, H Chen, J Yang, K Narasimhan, arXiv2022</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>A survey on multimodal large language models. S Yin, C Fu, S Zhao, K Li, X Sun, T Xu, E Chen, arXiv2023</p>
<p>T Yu, D Quillen, Z He, R Julian, A Narayan, H Shively, A Bellathur, K Hausman, C Finn, S Levine, Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. 2021</p>
<p>Task Description WebShop Instruction: i want capri sun pacific cooler mixed fruit naturally flavored juice drinks, and price lower than 20.00 dollars [Search] ReAct Action: search[capri sun pacific cooler mixed fruit naturally flavored juice drinks] Observation: [Back to Search] Page 1 (Total results: 50) [Next &gt;] [B000R30X2A] Capri Sun Pacific Cooler Mixed Fruit Naturally Flavored Juice Drink Blend (Pack of 10) (Packaging May Vary) $2.55 [B075CSWGJR] Capri Sun Pacific Cooler Ready-to-Drink Juice, 6 Fl Oz (Pack of 30) $100.0 [B0083GG2SC] Capri Sun Coolers Variety Pack Ready-to-Drink Juice (40 Pouches, 4 Boxes of 10) $100.0 Action: think[B000R30X2A is capri sun pacific cooler mixed fruit naturally flavored juice drinks and price lower than 20. A Zhao, D Huang, Q Xu, M Lin, Y.-J Liu, G Huang, AAAI Conference on Artificial Intelligence (AAAI). 202355Action: think[For capri sun pacific cooler mixed fruit naturally flavored juice drinks, the item has options. fl oz (pack of 10)'] Observation: OK. Action: click[assorted] Observation: You have clicked assorted. Action: click[Buy Now] Observation: Your score (min 0.0, max 1.0</p>
<p>B000R30X2A] Capri Sun Pacific Cooler Mixed Fruit Naturally Flavored Juice Drink Blend (Pack of 10) (Packaging May Vary) $2.55 [B075CSWGJR] Capri Sun Pacific Cooler Ready-to-Drink Juice, 6 Fl Oz (Pack of 30) $100.0 [B0083GG2SC] Capri Sun Coolers Variety Pack Ready-to-Drink Juice (40 Pouches, 4 Boxes of 10) $100.0 Action: think[B000R30X2A is capri sun pacific cooler mixed fruit naturally flavored juice drinks and seems good to buy. Action: think[For capri sun pacific cooler mixed fruit naturally flavored juice drinks, the item has options 'assorted', 'pacific cooler', and '6 fl oz (pack of 10)' and seems good to buy. 6 fl oz (pack of 10. Observation: You have clicked 6 fl oz (pack of 10. Action: click[Buy Now] Observation: Your score (min 0.0, max 1.0): 1.0</p>            </div>
        </div>

    </div>
</body>
</html>
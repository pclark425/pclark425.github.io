<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2262 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2262</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2262</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-218674320</p>
                <p><strong>Paper Title:</strong> A Combined Data-driven and Physics-driven Method for Steady Heat Conduction Prediction using Deep Convolutional Neural Networks</p>
                <p><strong>Paper Abstract:</strong> With several advantages and as an alternative to predict physics ﬁeld, machine learning methods can be classiﬁed into two distinct types: data-driven relying on training data and physics-driven using physics law. Choosing heat conduction problem as an example, we compared the data- and physics-driven learning process with deep Convolutional Neural Networks (CNN). It shows that the convergences of the error to ground truth solution and the residual of heat conduction equation exhibit remarkable diﬀerences. Based on this observation, we propose a combined-driven method for learning acceleration and more accurate solutions. With a weighted loss function, reference data and physical equation are able to simultaneously drive the learning. Several numerical experiments are conducted to investigate the eﬀectiveness of the combined method. For the data-driven based method, the introduction of physical equation not only is able to speed up the convergence, but also produces physically more consistent solutions. For the physics-driven based method, it is observed that the combined method is able to speed up the convergence up to 49.0% by using a not very restrictive coarse reference.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2262.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2262.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Data-driven CNN (DDM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data-driven deep convolutional neural network for heat conduction prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised learning approach that trains a U-net convolutional neural network on labeled numerical solutions (FVM) to directly predict steady temperature fields on a 2D grid.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Combined Data-driven and Physics-driven Method for Steady Heat Conduction Prediction using Deep Convolutional Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Computational physics — steady heat conduction (Laplace equation)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict the steady temperature field (solution of 2D Laplace equation with Dirichlet boundary conditions) for given geometry and boundary conditions, replacing or accelerating traditional numerical solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Training data are available but require numerical simulation: authors use FVM-generated labeled data; experiments include both a single-sample scenario and a multi-case dataset of 4,374 samples (80/20 train/test). Data labeled, interpolated to 128×128 grid. Data generation is costly and can create a dependency loop on numerical solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured 2D spatial fields represented as dense 128×128 grids (image-like matrices); inputs encode geometry and boundary conditions as channels.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate: solves a linear second-order PDE (Laplace) on a 2D spatial grid; solution requires capturing both global structure and local (second-derivative) consistency; input space includes geometric variation (hole shapes/positions) and boundary temperature variations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>High: underlying physics (Laplace equation) and numerical methods (FVM) are well-established; prior work exists on data-driven surrogates for physics fields.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — global prediction accuracy is important but local physics consistency (satisfaction of Laplace operator) is also required; pure black-box predictions can produce locally inconsistent results.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>U-net convolutional neural network (supervised data-driven)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>A U-net CNN with 17 layers and encoding/decoding blocks (batch normalization, activation, convolution, dropout) that takes multi-channel 128×128 input (geometry and BCs) and outputs a single-channel temperature field; trained with Adam optimizer and an L1 error loss between network output and FVM target (L_data = |T_out - T_tar|). Experiments used both single-case (one target) and multiple-case training (4,374 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised deep learning (data-driven surrogate)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for problems with available labeled numerical solutions; directly applicable to predict steady PDE fields on grid domains. Limitations: requires labeled data generation, and without physics constraints can produce locally inconsistent solutions (smooth global structure may appear but Laplace residuals remain large).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Single-case baseline: data-driven learning reached error <0.2% in ~3k iterations (~110 s); for convergence criterion E=0.005 over 5 runs, average iterations = 684 (baseline). Multi-case (4,374 samples) produced small E within 1000 epochs. (All numbers reported by authors.)</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Works well at recovering global temperature structure rapidly; local Laplace residual (LR) remains relatively large for long periods leading to rough contours and isolated artifacts; overall accurate but less physically consistent locally compared to physics-driven or combined methods.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Can provide fast surrogates for steady PDE solutions when simulation data is available; reduces runtime of repetitive queries once trained, but does not eliminate dependency on numerical solvers for training data generation unless combined with physics constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly to physics-driven CNN and combined method: data-driven converges faster to target but yields higher Laplace residuals (implicit error); combined method (data+physics) achieves faster LR convergence and smoother, more physically consistent outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of high-quality labeled FVM data, appropriate U-net architecture (skip connections), and optimizer settings; however, lacking explicit physics loss limits local consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Supervised CNNs quickly learn global field structure from labels but can leave significant local (differential) residuals; adding physics-based loss terms addresses this weakness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2262.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2262.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Physics-driven CNN (PDM) / PINN-style</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-driven convolutional neural network using PDE residual loss (physics-informed)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics-driven training approach that enforces the Laplace equation and boundary conditions by minimizing the PDE residual and boundary-loss terms, enabling training without labeled target fields.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Combined Data-driven and Physics-driven Method for Steady Heat Conduction Prediction using Deep Convolutional Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Computational physics — steady heat conduction (Laplace equation)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Infer the steady temperature field by training a neural network to satisfy the governing PDE (Laplace equation) and Dirichlet boundary conditions, avoiding the need for labeled numerical targets.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Does not require labelled solution data; relies on boundary condition specification and geometry; thus suitable when simulation labels are scarce or expensive. No external labeled data needed in single-case physics-driven experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured 2D spatial fields represented as dense 128×128 grids; second input channel acts as mask for void regions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate to higher training complexity: optimization must reduce PDE residuals (LR) across the domain, which can produce slow convergence and nontrivial error modes (e.g., the observed 'valley' artifact); training landscape can be harder to optimize compared to supervised loss.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>High in terms of physical theory (Laplace PDE known); physics-informed ML approaches (PINNs and neural PDE solvers) are established in literature and referenced by the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — the method encodes mechanistic knowledge directly into the loss, so mechanistic consistency is a primary objective. Interpretability in terms of PDE residuals is intrinsic.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-informed/trained U-net CNN minimizing PDE residual (Laplace residual)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Same U-net architecture as the data-driven approach, but trained with a loss composed of the domain-averaged Laplace residual LR = mean(∂^2 T/∂x^2 + ∂^2 T/∂y^2) and boundary constraint loss terms (L_BC,in). No direct supervision from full-field targets is used; boundary values are enforced as Dirichlet constraints via loss.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Physics-informed machine learning / physics-driven deep learning (unsupervised with physics loss)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable when governing PDEs and boundary conditions are known and labeled solutions are expensive or unavailable. Limitations: slower convergence (authors report ~23k iterations vs ~3k for supervised to reach 0.2% error), and training can produce large-scale artifacts requiring many iterations to correct.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Single-case baseline: physics-driven learning took ~23k iterations (~750 s) to reach errors <0.2%, compared to ~3k iterations for data-driven; when combined with reference targets, convergence speed improved up to 49.0% depending on threshold L_thr and reference quality.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Produces smooth local fields early (low LR) but may produce incorrect global structure initially (the 'valley') and converge more slowly to true solution; robust to lack of labeled data but expensive in optimization steps. Sensitive to initial references in hybrid variants.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Reduces dependence on costly labeled simulations and can produce physically consistent solutions; can be used as a general approach for PDE-constrained learning and surrogate generation when labels are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to data-driven: physics-driven avoids labeled data but is substantially slower and may produce global-structure errors; combined hybrid methods outperform pure physics-driven in convergence speed when reference targets are available.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Explicit inclusion of PDE residual and carefully implemented boundary-condition loss; use of mask channels to exclude void regions; however, optimization landscape challenges and scale differences between loss terms require careful weighting or hybrid strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Enforcing PDE residuals yields locally smooth and physics-consistent solutions without labels, but optimizing only the physics loss can be slow and can produce incorrect global structures unless complemented by data or reference guidance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2262.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2262.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Combined (Hybrid) Data+Physics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Combined data-driven and physics-driven method using a weighted loss (hybrid physics-data CNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid training method that simultaneously minimizes supervised target error and PDE residual via a weighted loss L = L_data + R * L_phy (data-driven based) or a staged hybrid loss with thresholding (physics-driven based) to accelerate convergence and improve physical consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Computational physics — steady heat conduction (Laplace equation)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Improve convergence speed and physical accuracy of CNN-based field prediction by combining explicit supervision from reference targets (where available) with physics constraints (Laplace residual) in a single loss function; also apply staged strategy when starting from physics-driven training with coarse references.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Designed to operate both when labeled data are abundant (data-driven based combined) and when labels are scarce (physics-driven based combined with coarse/reference targets). Authors test both single-sample and multi-sample (4,374) cases and several reference-target types (true, coarse, systematic, random, zero).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Same structured 2D grid data (128×128) used for both supervised and physics terms; reference targets can be full-resolution labels or coarse/approximate profiles (downsampled, systematic bias, or noisy).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Handles the same PDE-based field prediction complexity as the original methods; hybrid approach adds hyperparameter tuning (weight R, threshold L_thr) to balance loss scales and training phases.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Builds on mature PDE theory and existing PINN/data-driven CNN literature; combines established components (U-net, PDE residuals, weighted losses) into a novel applied formulation for steady heat conduction.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High: the method explicitly enforces mechanistic consistency through the physics loss while retaining data-driven accuracy; interpretability is enhanced by satisfying PDE residual metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Weighted-loss hybrid U-net (data + physics) with optional staged thresholding</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Two combined variants: (1) Data-driven based combined: single loss L = L_data + R * L_phy for entire training, where R scales the PDE residual term to match L_data magnitude. (2) Physics-driven based combined: staged loss L = L_data,ref + R * L_phy while L >= L_thr, then switch to only L_phy when below threshold; reference targets can be true, coarse, systematically biased, or noisy. Same U-net architecture and Adam optimizer are used. Weighted loss design and tuning (R, L_thr) are critical to avoid domination by LR term.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Hybrid — physics-informed supervised learning (physics-constrained deep learning / hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable: works when some reference information is available (even coarse/noisy) or when labels exist; improves training speed and physical consistency. Requires tuning of weight R and threshold L_thr; unsuitable choices (e.g., zero-profile reference) can prevent convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Data-driven based combined (single-case): average iterations to reach E=0.005 reduced from 684 (original) to 267 (combined) — a 60.9% acceleration. Physics-driven based combined: acceleration over original physics-driven method reported as 22.7%, 34.4%, and 49.0% for thresholds L_thr = 0.15, 0.10, 0.05 respectively (depending on reference target quality). Single-case time comparisons: baseline data-driven ~3k iterations (110 s) vs physics-driven ~23k (750 s) for <0.2% error; combined methods reduce iterations substantially in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Combined method produces smoother temperature contours, much smaller Laplace residuals, and faster elimination of large error artifacts (the 'valley'); robust to imperfect reference targets (coarse/systematic/random) except for pathological references (zero-profile), and yields better local physics consistency while preserving global accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant: reduces training iterations and improves physical fidelity of learned surrogates, enabling faster deployment of ML-based PDE solvers and more trustworthy surrogate models; generalizable to other PDE-expressed physics laws and potentially to complex flow fields.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Directly compared against pure data-driven and pure physics-driven methods on same architecture and tasks: combined achieves faster LR convergence than data-driven and much faster global convergence than physics-driven when reference targets are available; outperforms both in producing physically consistent outputs with fewer iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key factors: (1) weighted loss to balance magnitude differences between L_data and L_phy (hyperparameter R), (2) choice of reference targets (can be coarse/noisy but not misleading), (3) thresholding strategy (L_thr) for staged training in physics-driven variant, (4) U-net architecture and optimizer settings. Proper tuning prevents skewed optimization dominated by one loss term.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Combining explicit supervision with PDE residuals via a balanced weighted loss yields faster convergence and improved local physical consistency: data provides global guidance while physics loss enforces local differential constraints, and even coarse/noisy references substantially accelerate physics-driven training.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations <em>(Rating: 2)</em></li>
                <li>Data-driven solutions of nonlinear partial differential equations. Physics informed deep learning (part i) <em>(Rating: 2)</em></li>
                <li>Physics-informed deep learning (part ii): Data-driven discovery of nonlinear partial differential equations <em>(Rating: 2)</em></li>
                <li>Weakly-supervised deep learning of heat transport via physics informed loss <em>(Rating: 2)</em></li>
                <li>Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data <em>(Rating: 2)</em></li>
                <li>Deep learning the physics of transport phenomena <em>(Rating: 1)</em></li>
                <li>Deepxde: A deep learning library for solving differential equations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2262",
    "paper_id": "paper-218674320",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "Data-driven CNN (DDM)",
            "name_full": "Data-driven deep convolutional neural network for heat conduction prediction",
            "brief_description": "A supervised learning approach that trains a U-net convolutional neural network on labeled numerical solutions (FVM) to directly predict steady temperature fields on a 2D grid.",
            "citation_title": "A Combined Data-driven and Physics-driven Method for Steady Heat Conduction Prediction using Deep Convolutional Neural Networks",
            "mention_or_use": "use",
            "scientific_problem_domain": "Computational physics — steady heat conduction (Laplace equation)",
            "problem_description": "Predict the steady temperature field (solution of 2D Laplace equation with Dirichlet boundary conditions) for given geometry and boundary conditions, replacing or accelerating traditional numerical solvers.",
            "data_availability": "Training data are available but require numerical simulation: authors use FVM-generated labeled data; experiments include both a single-sample scenario and a multi-case dataset of 4,374 samples (80/20 train/test). Data labeled, interpolated to 128×128 grid. Data generation is costly and can create a dependency loop on numerical solvers.",
            "data_structure": "Structured 2D spatial fields represented as dense 128×128 grids (image-like matrices); inputs encode geometry and boundary conditions as channels.",
            "problem_complexity": "Moderate: solves a linear second-order PDE (Laplace) on a 2D spatial grid; solution requires capturing both global structure and local (second-derivative) consistency; input space includes geometric variation (hole shapes/positions) and boundary temperature variations.",
            "domain_maturity": "High: underlying physics (Laplace equation) and numerical methods (FVM) are well-established; prior work exists on data-driven surrogates for physics fields.",
            "mechanistic_understanding_requirements": "Medium — global prediction accuracy is important but local physics consistency (satisfaction of Laplace operator) is also required; pure black-box predictions can produce locally inconsistent results.",
            "ai_methodology_name": "U-net convolutional neural network (supervised data-driven)",
            "ai_methodology_description": "A U-net CNN with 17 layers and encoding/decoding blocks (batch normalization, activation, convolution, dropout) that takes multi-channel 128×128 input (geometry and BCs) and outputs a single-channel temperature field; trained with Adam optimizer and an L1 error loss between network output and FVM target (L_data = |T_out - T_tar|). Experiments used both single-case (one target) and multiple-case training (4,374 samples).",
            "ai_methodology_category": "Supervised deep learning (data-driven surrogate)",
            "applicability": "Appropriate for problems with available labeled numerical solutions; directly applicable to predict steady PDE fields on grid domains. Limitations: requires labeled data generation, and without physics constraints can produce locally inconsistent solutions (smooth global structure may appear but Laplace residuals remain large).",
            "effectiveness_quantitative": "Single-case baseline: data-driven learning reached error &lt;0.2% in ~3k iterations (~110 s); for convergence criterion E=0.005 over 5 runs, average iterations = 684 (baseline). Multi-case (4,374 samples) produced small E within 1000 epochs. (All numbers reported by authors.)",
            "effectiveness_qualitative": "Works well at recovering global temperature structure rapidly; local Laplace residual (LR) remains relatively large for long periods leading to rough contours and isolated artifacts; overall accurate but less physically consistent locally compared to physics-driven or combined methods.",
            "impact_potential": "Can provide fast surrogates for steady PDE solutions when simulation data is available; reduces runtime of repetitive queries once trained, but does not eliminate dependency on numerical solvers for training data generation unless combined with physics constraints.",
            "comparison_to_alternatives": "Compared directly to physics-driven CNN and combined method: data-driven converges faster to target but yields higher Laplace residuals (implicit error); combined method (data+physics) achieves faster LR convergence and smoother, more physically consistent outputs.",
            "success_factors": "Availability of high-quality labeled FVM data, appropriate U-net architecture (skip connections), and optimizer settings; however, lacking explicit physics loss limits local consistency.",
            "key_insight": "Supervised CNNs quickly learn global field structure from labels but can leave significant local (differential) residuals; adding physics-based loss terms addresses this weakness.",
            "uuid": "e2262.0"
        },
        {
            "name_short": "Physics-driven CNN (PDM) / PINN-style",
            "name_full": "Physics-driven convolutional neural network using PDE residual loss (physics-informed)",
            "brief_description": "A physics-driven training approach that enforces the Laplace equation and boundary conditions by minimizing the PDE residual and boundary-loss terms, enabling training without labeled target fields.",
            "citation_title": "A Combined Data-driven and Physics-driven Method for Steady Heat Conduction Prediction using Deep Convolutional Neural Networks",
            "mention_or_use": "use",
            "scientific_problem_domain": "Computational physics — steady heat conduction (Laplace equation)",
            "problem_description": "Infer the steady temperature field by training a neural network to satisfy the governing PDE (Laplace equation) and Dirichlet boundary conditions, avoiding the need for labeled numerical targets.",
            "data_availability": "Does not require labelled solution data; relies on boundary condition specification and geometry; thus suitable when simulation labels are scarce or expensive. No external labeled data needed in single-case physics-driven experiments.",
            "data_structure": "Structured 2D spatial fields represented as dense 128×128 grids; second input channel acts as mask for void regions.",
            "problem_complexity": "Moderate to higher training complexity: optimization must reduce PDE residuals (LR) across the domain, which can produce slow convergence and nontrivial error modes (e.g., the observed 'valley' artifact); training landscape can be harder to optimize compared to supervised loss.",
            "domain_maturity": "High in terms of physical theory (Laplace PDE known); physics-informed ML approaches (PINNs and neural PDE solvers) are established in literature and referenced by the paper.",
            "mechanistic_understanding_requirements": "High — the method encodes mechanistic knowledge directly into the loss, so mechanistic consistency is a primary objective. Interpretability in terms of PDE residuals is intrinsic.",
            "ai_methodology_name": "Physics-informed/trained U-net CNN minimizing PDE residual (Laplace residual)",
            "ai_methodology_description": "Same U-net architecture as the data-driven approach, but trained with a loss composed of the domain-averaged Laplace residual LR = mean(∂^2 T/∂x^2 + ∂^2 T/∂y^2) and boundary constraint loss terms (L_BC,in). No direct supervision from full-field targets is used; boundary values are enforced as Dirichlet constraints via loss.",
            "ai_methodology_category": "Physics-informed machine learning / physics-driven deep learning (unsupervised with physics loss)",
            "applicability": "Applicable when governing PDEs and boundary conditions are known and labeled solutions are expensive or unavailable. Limitations: slower convergence (authors report ~23k iterations vs ~3k for supervised to reach 0.2% error), and training can produce large-scale artifacts requiring many iterations to correct.",
            "effectiveness_quantitative": "Single-case baseline: physics-driven learning took ~23k iterations (~750 s) to reach errors &lt;0.2%, compared to ~3k iterations for data-driven; when combined with reference targets, convergence speed improved up to 49.0% depending on threshold L_thr and reference quality.",
            "effectiveness_qualitative": "Produces smooth local fields early (low LR) but may produce incorrect global structure initially (the 'valley') and converge more slowly to true solution; robust to lack of labeled data but expensive in optimization steps. Sensitive to initial references in hybrid variants.",
            "impact_potential": "Reduces dependence on costly labeled simulations and can produce physically consistent solutions; can be used as a general approach for PDE-constrained learning and surrogate generation when labels are unavailable.",
            "comparison_to_alternatives": "Compared to data-driven: physics-driven avoids labeled data but is substantially slower and may produce global-structure errors; combined hybrid methods outperform pure physics-driven in convergence speed when reference targets are available.",
            "success_factors": "Explicit inclusion of PDE residual and carefully implemented boundary-condition loss; use of mask channels to exclude void regions; however, optimization landscape challenges and scale differences between loss terms require careful weighting or hybrid strategies.",
            "key_insight": "Enforcing PDE residuals yields locally smooth and physics-consistent solutions without labels, but optimizing only the physics loss can be slow and can produce incorrect global structures unless complemented by data or reference guidance.",
            "uuid": "e2262.1"
        },
        {
            "name_short": "Combined (Hybrid) Data+Physics",
            "name_full": "Combined data-driven and physics-driven method using a weighted loss (hybrid physics-data CNN)",
            "brief_description": "A hybrid training method that simultaneously minimizes supervised target error and PDE residual via a weighted loss L = L_data + R * L_phy (data-driven based) or a staged hybrid loss with thresholding (physics-driven based) to accelerate convergence and improve physical consistency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Computational physics — steady heat conduction (Laplace equation)",
            "problem_description": "Improve convergence speed and physical accuracy of CNN-based field prediction by combining explicit supervision from reference targets (where available) with physics constraints (Laplace residual) in a single loss function; also apply staged strategy when starting from physics-driven training with coarse references.",
            "data_availability": "Designed to operate both when labeled data are abundant (data-driven based combined) and when labels are scarce (physics-driven based combined with coarse/reference targets). Authors test both single-sample and multi-sample (4,374) cases and several reference-target types (true, coarse, systematic, random, zero).",
            "data_structure": "Same structured 2D grid data (128×128) used for both supervised and physics terms; reference targets can be full-resolution labels or coarse/approximate profiles (downsampled, systematic bias, or noisy).",
            "problem_complexity": "Handles the same PDE-based field prediction complexity as the original methods; hybrid approach adds hyperparameter tuning (weight R, threshold L_thr) to balance loss scales and training phases.",
            "domain_maturity": "Builds on mature PDE theory and existing PINN/data-driven CNN literature; combines established components (U-net, PDE residuals, weighted losses) into a novel applied formulation for steady heat conduction.",
            "mechanistic_understanding_requirements": "High: the method explicitly enforces mechanistic consistency through the physics loss while retaining data-driven accuracy; interpretability is enhanced by satisfying PDE residual metrics.",
            "ai_methodology_name": "Weighted-loss hybrid U-net (data + physics) with optional staged thresholding",
            "ai_methodology_description": "Two combined variants: (1) Data-driven based combined: single loss L = L_data + R * L_phy for entire training, where R scales the PDE residual term to match L_data magnitude. (2) Physics-driven based combined: staged loss L = L_data,ref + R * L_phy while L &gt;= L_thr, then switch to only L_phy when below threshold; reference targets can be true, coarse, systematically biased, or noisy. Same U-net architecture and Adam optimizer are used. Weighted loss design and tuning (R, L_thr) are critical to avoid domination by LR term.",
            "ai_methodology_category": "Hybrid — physics-informed supervised learning (physics-constrained deep learning / hybrid)",
            "applicability": "Highly applicable: works when some reference information is available (even coarse/noisy) or when labels exist; improves training speed and physical consistency. Requires tuning of weight R and threshold L_thr; unsuitable choices (e.g., zero-profile reference) can prevent convergence.",
            "effectiveness_quantitative": "Data-driven based combined (single-case): average iterations to reach E=0.005 reduced from 684 (original) to 267 (combined) — a 60.9% acceleration. Physics-driven based combined: acceleration over original physics-driven method reported as 22.7%, 34.4%, and 49.0% for thresholds L_thr = 0.15, 0.10, 0.05 respectively (depending on reference target quality). Single-case time comparisons: baseline data-driven ~3k iterations (110 s) vs physics-driven ~23k (750 s) for &lt;0.2% error; combined methods reduce iterations substantially in many settings.",
            "effectiveness_qualitative": "Combined method produces smoother temperature contours, much smaller Laplace residuals, and faster elimination of large error artifacts (the 'valley'); robust to imperfect reference targets (coarse/systematic/random) except for pathological references (zero-profile), and yields better local physics consistency while preserving global accuracy.",
            "impact_potential": "Significant: reduces training iterations and improves physical fidelity of learned surrogates, enabling faster deployment of ML-based PDE solvers and more trustworthy surrogate models; generalizable to other PDE-expressed physics laws and potentially to complex flow fields.",
            "comparison_to_alternatives": "Directly compared against pure data-driven and pure physics-driven methods on same architecture and tasks: combined achieves faster LR convergence than data-driven and much faster global convergence than physics-driven when reference targets are available; outperforms both in producing physically consistent outputs with fewer iterations.",
            "success_factors": "Key factors: (1) weighted loss to balance magnitude differences between L_data and L_phy (hyperparameter R), (2) choice of reference targets (can be coarse/noisy but not misleading), (3) thresholding strategy (L_thr) for staged training in physics-driven variant, (4) U-net architecture and optimizer settings. Proper tuning prevents skewed optimization dominated by one loss term.",
            "key_insight": "Combining explicit supervision with PDE residuals via a balanced weighted loss yields faster convergence and improved local physical consistency: data provides global guidance while physics loss enforces local differential constraints, and even coarse/noisy references substantially accelerate physics-driven training.",
            "uuid": "e2262.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "rating": 2,
            "sanitized_title": "physicsinformed_neural_networks_a_deep_learning_framework_for_solving_forward_and_inverse_problems_involving_nonlinear_partial_differential_equations"
        },
        {
            "paper_title": "Data-driven solutions of nonlinear partial differential equations. Physics informed deep learning (part i)",
            "rating": 2,
            "sanitized_title": "datadriven_solutions_of_nonlinear_partial_differential_equations_physics_informed_deep_learning_part_i"
        },
        {
            "paper_title": "Physics-informed deep learning (part ii): Data-driven discovery of nonlinear partial differential equations",
            "rating": 2,
            "sanitized_title": "physicsinformed_deep_learning_part_ii_datadriven_discovery_of_nonlinear_partial_differential_equations"
        },
        {
            "paper_title": "Weakly-supervised deep learning of heat transport via physics informed loss",
            "rating": 2,
            "sanitized_title": "weaklysupervised_deep_learning_of_heat_transport_via_physics_informed_loss"
        },
        {
            "paper_title": "Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data",
            "rating": 2,
            "sanitized_title": "surrogate_modeling_for_fluid_flows_based_on_physicsconstrained_deep_learning_without_simulation_data"
        },
        {
            "paper_title": "Deep learning the physics of transport phenomena",
            "rating": 1,
            "sanitized_title": "deep_learning_the_physics_of_transport_phenomena"
        },
        {
            "paper_title": "Deepxde: A deep learning library for solving differential equations",
            "rating": 1,
            "sanitized_title": "deepxde_a_deep_learning_library_for_solving_differential_equations"
        }
    ],
    "cost": 0.012870999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Combined Data-driven and Physics-driven Method for Steady Heat Conduction Prediction using Deep Convolutional Neural Networks
May 19, 2020 16 May 2020</p>
<p>Hao Ma 
Department of Aerospace and Geodesy
Technical University of Munich
85748GarchingGermany</p>
<p>Xiangyu Hu xiangyu.hu@tum.de 
Department of Mechanical Engineering
Technical University of Munich
85748GarchingGermany</p>
<p>Yuxuan Zhang 
Beijing Aerospace Propulsion Institute
100076BeijingChina</p>
<p>Nils Thuerey 
Technical University of Munich
85748GarchingGermany</p>
<p>Oskar J Haidn 
Department of Aerospace and Geodesy
Technical University of Munich
85748GarchingGermany</p>
<p>A Combined Data-driven and Physics-driven Method for Steady Heat Conduction Prediction using Deep Convolutional Neural Networks
May 19, 2020 16 May 2020Preprint submitted to X. X. X(Yuxuan Zhang), nils.thuerey@tum.de (Nils Thuerey), oskar.haidn@tum.de (Oskar J. Haidn) restrictive coarse reference.Machine learningPhysics field predictionData drivenPhysics drivenConvolutional neural networksConvergence speedCombined methodWeighted loss function 2
With several advantages and as an alternative to predict physics field, machine learning methods can be classified into two distinct types: data-driven relying on training data and physics-driven using physics law. Choosing heat conduction problem as an example, we compared the data-and physics-driven learning process with deep Convolutional Neural Networks (CNN). It shows that the convergences of the error to ground truth solution and the residual of heat conduction equation exhibit remarkable differences. Based on this observation, we propose a combined-driven method for learning acceleration and more accurate solutions. With a weighted loss function, reference data and physical equation are able to simultaneously drive the learning. Several numerical experiments are conducted to investigate the effectiveness of the combined method. For the data-driven based method, the introduction of physical equation not only is able to speed up the convergence, but also produces physically more consistent solutions. For the physics-driven based method, it is observed that the combined method is able to speed up the convergence up to 49.0% by using a not very * Corresponding author.</p>
<p>Introduction</p>
<p>With abundant training methods and high-performance computing resources, machine learning has been applied for many scientific research fields, including computational physics for modeling [1,2], optimization [3,4], control [5] and other critical tasks [6,7]. A specific application is to predict physics field for reducing or avoiding the large computational cost of the traditional numerical (finite volume/element/difference) methods, such as Computational Fluid Dynamics (CFD) which solve Partial Differential Equations(PDEs) [8].</p>
<p>In machine learning approaches, data-driven methods were widely used to train the model with a large amount of labeled training data. For physics field prediction, the data-driven methods can be roughly identified as indirect to train closure models [9,10] and direct to obtain the solutions [11,12]. While the indirect method has achieved great successes in recent [13,14], the direct data-driven method has also exhibited the capability to capture physics characteristics and to provide accurate estimates without resorting to expensive numerical computations [15,16,17,18]. However, since the training data sets are still generated from traditional numerical solutions [19,20,21], it leads to an embarrassing loop that it does not truly solve the issue of expansive numerical computation. In addition, in some difficult cases, though a large number of training samples are used, the data-driven method may still not be able to obtain sufficiently accurate solutions. Choosing the work in Ref. [19] as an example, independent of the number of training samples, considerable errors always manifest themselves in the inferred flow field just behind the airfoil. Similar phenomenon also happens in Ref. [12]. It requires another novel approach to eliminate this shortcoming other than simply utilizing an even larger training data set.</p>
<p>In fact, the physics law which is unknown in data-driven methods could be explicitly employed in the learning process [22,23]. Raissi et al. introduced this idea into machine learning algorithms and named it as Physics Informed Neural Networks (PINN) [24]. By introducing PDEs into the loss function, PINN is able to predict the solution that satisfies physics law. The inferred solution is trained to obey the corresponding PDE and boundary conditions. The effectiveness of this physics-driven method has been demonstrated through a collection of physics problems [25,26,27,29]. Compared with data-driven methods, it extricate machine learning from the dependence of training data, remarkably decrease the cost of data set generation [28,30,31]. However, as shown in a single case training later, the cost of physics-driven method is typically more expensive than that of the data-driven method.</p>
<p>In order to remedy the above mentioned shortcomings, we propose an idea that combining data-and physics-drivens together. To our knowledge, this is the first attempt that simultaneously utilizes training data and physics law to drive machine learning for physics field prediction. Choosing the steady solution of heat conduction as an example, we first consider the original data-driven and physics-driven methods based on a deep CNN respectively and compare their learning progresses for a single case training. Then, based on the comparisons, we propose a weighted loss function combining the effects of reference target and physics law (given as Laplace equation) for training the CNN to predict temperature fields. After this, several numerical experiments are conducted and analyzed to study the improvements achieved by the combined method.</p>
<p>Preliminaries</p>
<p>We choose the steady solution of heat conduction whose physics law can be expressed with a second-order PDE, i.e. Laplace equation as an example. Focus on this problem, we first describe the CNN architecture used and the original data-and physics-driven methods, especially their distinct loss functions.</p>
<p>U-net Architecture for CNN</p>
<p>The CNN used here is based on a U-net architecture, which is firstly proposed for machine vision [32]. Including the input and output layer, the U-net architecture consists of 17 layers and corresponding convolutional blocks. Each convolutional block has a similar structure: batch normalization, active function, convolutional calculation, and dropout [33]. Eventually the output only has one channel giving the temperature field. It is noteworthy that there are concatenation operations between corresponding en-coding and decoding blocks as shown in Figure 1. These connections effectively double the amount of feature channels in every decoding block and enable the neural networks to consider the information from the encoding layers.</p>
<p>The CNNs are trained using stochastic gradient descent optimization, which requires a loss function to calculate the model error. Except for different loss functions, the U-nets and other training settings are kept same in the following description on the original data-and physics-driven methods. By means of backpropagation, the weights and other parameters of the entire networks are adjusted by Adam optimizer [34] and eventually the loss is minimized and the CNN are able to reconstruct the solution of heat conduction problems. More details of the U-net architecture and CNN can be found in Ref. [19].</p>
<p>Original Data-and Physics-driven Methods</p>
<p>In the data-driven method, the loss function compares the difference between training target and output result as
L data = |T out − T tar | .(1)
The subscript "data" here denotes data-driven. T out and T tar are output and target temperature distributions respectively.</p>
<p>Based on Fouriers law, when thermal conductivity is considered as constant and there is no inner heat source, the physics law of heat conduction can be described as two-dimensional Laplace equation
∂ 2 T ∂x 2 + ∂ 2 T ∂y 2 = 0,(2)
which is a typical PDE whose solution is important in many branches of physics.</p>
<p>In the physics-driven method, this physics law is used to drive the learning process by the loss function
L phy = ∂ 2 T ∂x 2 + ∂ 2 T ∂y 2 = e 1 .(3)
The boundaries are constrained with Dirichlet boundary conditions by which the temperatures of the outer and inner boundaries are kept as a constant.</p>
<p>The boundary conditions are implemented differently for the outer and inner boundaries. While the temperatures at the outer boundaries are assigned as constants, their values at the inner boundary as well as the inside void region are constrained by a loss function as
L BC,in = T − T BC,in = e 2 .(4)
A schematic of the physics-driven method is shown in Figure 2. Note that, for the physics-driven method, the second inputting channel of the U-net CNN not only describes the geometry but also functions as a mask, by which the Laplace equation is not effective in the void region.</p>
<p>Observations on Learning Processes and Combined Method</p>
<p>In the following, we describe the observations on the learning processes of the original data-and physics-driven methods, which motivates the combined method, for a single case training.</p>
<p>Single Case Training</p>
<p>In general, machine learning methods are able to train multiple cases simultaneously. Here, the single case which is defined by a specific geometry and boundary condition combination is considered and the CNN are trained to output the corresponding field solution. By this way, 4 training tasks as shown in Figure 3 are carried out. The learning algorithms are based on PyTorch framework [33] and the trainings are conducted on a single NVIDIA GeForce RTX 2080 Ti Card. The training samples for data-driven method are obtained by Finite Volume Method (FVM) [35] and every numerical solution is interpolated into a 128 × 128 grid to suit the learning domain with a same resolution.</p>
<p>Taking Task 1 as an example, it takes data-driven learning about 3k iterative steps (110 s) and physics-driven learning about 23k iterative steps (750 s) , respectively, to reach the errors less than 0.2%.</p>
<p>Comparison of Learning Processes</p>
<p>As shown in Figure 4, in the data-driven learning process of Task 1, after a few iterative steps, a brief form of the global temperature profile (global structure) starts to appear, all local values approach those of target solution.</p>
<p>The temperature contours are rough at first and smoothed successively with the global structure itself unchanged. However, in the physics-driven learning process, the contours become smooth after a few iterative steps. Then a large error spot (denoted as a valley) appears, which also happens in the other training tasks. The global structure is very different from that of the true solution.</p>
<p>After the gradual disappearance of the valley, the still existing residuals near the boundaries make the subsequent learning process similar to a typical numerical solution process of an unsteady heat conduction problem with a Dirichlet boundary condition, as shown in the last two sub-figures of Figure 4b.</p>
<p>In order to study the convergence process, we defined the overall error E and the Laplace residual LR. The former is defined as
E = 1 n × n n×n i=1 |T out − T tar | ,(5)
where n × n is the resolution of learning domain. It is an estimation the degree that the outputs satisfy the solution. While the Laplace residual LR is defined as
LR = 1 n × n n×n i=1 ∂ 2 T ∂x 2 + ∂ 2 T ∂y 2 .(6)
It represents the degree that the outputs satisfy Laplace equation, i.e., the local structure of solution. For data-driven learning, the loss function only considers the error between output and target rather than the residual of Laplace equation, so we call E loss term or explicit error and LR non-loss term or implicit error. Similarly in physics-driven learning, LR is explicit error and E implicit error.</p>
<p>As shown in Figure 5, for both data-driven and physics-driven learning,  However, due to the lack of explicit restriction to target solution or the global structure, the implicit error E decreases slowly and the convergence of the solution is much slower than that of the data-driven learning.</p>
<p>Combined Method</p>
<p>Based on the above observations, we propose to improve physics consistency and increase the convergence speed by combining both E and LR being into the loss terms.</p>
<p>It is observed that the scale of LR is significantly larger than E as shown in Figure 5. Utilizing a simple summation of these two errors as the total loss leads a skewed optimization [37] with a dominance of the Laplace residual. In order to remedy this issue, we employ a weighted loss function which has been widely used in object detection [38] and audio detection [39]. The weighted loss function considering both target data and Laplace equation is written as
L = L data + R * L phy ,(7)
where R is a constant hyperparameter which is tuned to adapt the scales. With this weighted loss function, the different loss terms can be easily scaled to an equivalent magnitude. The combined method actually has two types: datadriven based and physics-driven based. For the data-driven based method, the loss function is Equation (7) and employed during the whole learning process.</p>
<p>For the physics-driven based method, the loss function is modified as
L =      L data,ref + R * L phy L ≥ L thr L phy L &lt; L thr ,(8)
where L data,ref is the error term with some reference targets depending on different practical considerations. L thr is the threshold value of the loss indicating that once the loss is less than the threshold, the loss function will only consist of the Laplace term.</p>
<p>Numerical Experiments</p>
<p>Data-driven Based Training</p>
<p>The data-driven based training uses two distinct data sets: one has only a single sample and the another consists of multiple samples.</p>
<p>Single Case Training</p>
<p>As shown in Figure 6, unlike the original method, LR and E of combined method exhibit a similar convergence behavior. After the dramatic drop in the beginning period, they change to the relatively slow decrease and then the steady decline together. LR has obtained a considerable acceleration of convergence.</p>
<p>To check the overall performance, instead of a single run, 5 independent runs are conducted with the original and combined methods. It is observed that the averaged iterative steps when E reaches the criteria of convergence (0.005) are 684 and 267 respectively. This result gives that the combined method remarkably accelerates the learning with a rate of 60.9%. In addition, the combined method also leads to a notable improvement on obtaining the physics-consistent solution, even though the overall errors have a same level (Figure 7). This is due to that the Laplace residuals are much smaller, i.e. the local structure of solution has a better physics consistency.</p>
<p>Multiple Cases Training</p>
<p>For multiple cases training, the data set is obtained with the variation of boundary temperature (T boundary = 0/0.5/1), hole shape (square/round) and position (9 different positions). There are 4,374 samples split randomly into training/test sets with an 80/20 ratio.</p>
<p>As shown in Figure 8, the decrease trend of E are almost same for the original and combined methods. When the training reaches 1000 epochs, both of them are sufficiently small. However, LR of them exhibit different convergence behaviors. The LR of combined method, as loss term, decreases much faster.</p>
<p>For the temperature profile, the enhancement obtained by combined method is similar to that in the single case training. From the zoomed-views (as shown in Figure 9), the temperature contours obtained from combined method are much smoother, which suggests the solution is more consistent with physics law.</p>
<p>Physics-driven Based Training</p>
<p>In this section, single case is considered for physics-driven based training.</p>
<p>According to Equation (8), a reference target is required beforehand. Here, as shown in Figure 10, 5 reference targets are chosen. Among these targets,  Due to the introduction of E as one loss-term, the convergence speeds with the true and coarse targets are improved a lot. As shown in Figure 11, compared with the steady decline in the physics-driven learning, E of combined method drops more dramatically in the beginning. We set the threshold L thr in Equation 8 as 0.1. After L thr reached, only LR is the loss term. So E changes to steady descent immediately while LR still goes on rapid decline. For T tar = T zero , LR and E have the similar trend at the very beginning. However, the overall error E never reaches the threshold as the optimizer cannot find a way to reduce the gradient because of the incorrect reference target. Compared with the original physics-driven method, it is observed that the combined method with all the reference targets, except for zero profiles, are able to obtain a more accurate result by eliminating the large error spot quickly (as shown in Figure 12). These results suggest that the requirement for appropriate reference targets are not very restrictive in practical applications. To study the influence of the threshold L thr , the mean costs of 5 independent trainings with different reference targets are summarized in  </p>
<p>Conclusion</p>
<p>In this paper, we proposed a combined data-driven and physics-driven method to directly predict field solution of physics problems using deep CNN. The combined method simultaneously utilizes training data and physics law to drive the learning process. For the data-driven based method, besides accelerated convergence, the obtained local structure of solution achieves a better physics consistency. For the physics-driven based method, learning process can be accelerated considerably even with not very restrictive choices of reference targets, which is useful for practical application when a accurate reference is not available. It is noteworthy that the related CNN architecture and heat conduction problem are generic and the combined method suits for other physics laws which can be expressed as PDEs. Further research will be carried out for predicting complex flow field with the present method.data </p>
<p>Figure 1 :
1Schematic of U-net architecture. In the input layer, the color red and blue represent value 1 and 0 respectively. Each green box corresponds to a multi-channel feature matrix. Black corner arrows denote the down-sampling or up-sampling operation using convolutional calculation. Orange arrows denote the concatenation of the feature channels between encoding and decoding. As shown in Figure 1, geometry and boundary conditions are input into the architecture as square matrices with a size of 128 × 128. Then the corresponding square filters are utilized to conduct the convolutional calculation layer by layer until the matrices with only one data point are obtained. In this encoding process, the values of input matrices are progressively down-sampled by convolutional calculations. With the amount of feature channels increasing, large-scale information is extracted. Then the decoding process which can be regarded as a series of inverse convolutional operations mirrors the behavior of encoding. The solutions are reconstructed in the up-sampling layers along with the increase of spatial resolution and the decrease of feature channel amounts.</p>
<p>Figure 2 :
2Schematic of the physics-driven method. U-net CNN generates the solution. The backpropagation computes the gradient of the loss function and update the weights of the multilayer CNN to satisfy the Laplace equation and boundary conditions.</p>
<p>Figure 3 :Figure 4 :
34Single case training with different geometries and boundary conditions. The geometry of first two tasks is a simple square plate. Task 1: the temperature of left boundary is 1, the other three are 0. Task 2: the temperatures of left and bottom boundaries are 1, the other two are 0. The geometry of last two tasks is a square plate with a central hole. The boundary conditions of Tasks 3 and 4 are the same with Tasks 1 and 2 respectively, except that the temperature of inner hole is 0. Left to right: the results obtained by finite volume (FVM), data-driven (DDM) and physics-driven (PDM) methods respectively. The learned results are almost identical with numerical simulation references. Comparison of learning process of Task 1. The numbers below the contours are iterative steps. For data-driven learning, contours transform from rough to smooth while the global structure keeps unchanged. For physics-driven learning, the contours become smooth at the early training stage and then remedy "valley" gradually.</p>
<p>E
and LR drop dramatically in the beginning. However, after approximate 10 iterative steps, the convergence behaviors of the two terms, as explicit or implicit error, exhibits significant differences. When the explicit errors have gradually approached an adequately small value, the implicit errors are still large. Finally, after much large number of iteration steps, both errors decrease to sufficiently small values, i.e. both the solution and its local structure are obtained.</p>
<p>Figure 5 :
5Training history of LR and E (steps = 0 ∼ 500). The convergence behaviors of two learning methods have significant differences.For data-driven learning, as shown inFigure 4athe temperature contour keeps rough for a long period. And there are even small isolated islands in the neighboring temperature levels. The reason why this phenomenon happens is that the error of each data point approach to zero locally and separately. There is no corresponding explicit relation between these adjacent data points to restrict the local structures, which is given as Laplace equation in this case. For physics-driven learning, the relation of adjacent data points or local structure is constrained by Laplace equation explicitly and the values varies smoothly.</p>
<p>Figure 6 :
6Enhancements for data-driven method in single case training: LR and E. The convergence speed of LR has a notable improvement.</p>
<p>Figure 7 :Figure 8 :
78Enhancements for data-driven method in single case training: CNN outputs and local Laplace residuals. E of the two methods are both 0.05. Compared with the data-driven method, the contour of combined method is smoother and the local Laplace residuals in data points are much smaller. Enhancements for data-driven method in multiple cases training): LR and E. The histories of E of the two methods are almost same, while the LR of combined method decreases much faster.</p>
<p>Figure 9 :
9Enhancements for data-driven method in multiple cases training: outputs of CNN. There are two typical cases from test set with different geometries. The colors in first column represent T boundary = 0/0.5/1. The last three columns show part of the output which marked with the red dot line in the second column. In contrast to the original data-driven method, the temperature contours obtained by combined method are smoother. the true and zero profiles represent the true or false limits, while the 3 coarse temperature profiles mimic the practical application where the accurate solution is not available.</p>
<p>Figure 10 :
10Reference targets. Ttrue and Tzero are true and zero temperature profile respectively. Tcoarse is down-sampled from the true temperature profile, which could be a result of numerical simulation with a coarse mesh. T systematic is the coarse profile with a different boundary temperature and represents the experimental results with systematic errors. It may also estimate whether a reference target can be applied for training other cases. T random is the coarse temperature profile with random errors which represents an experimental result with measurement uncertainties.</p>
<p>Figure 11 :
11Ttar = Ttrue (c) Ttar = Tzero (d) Ttar = Tcoarse (e) Ttar = T systemaitc (f) Ttar = T random Enhancements for physics-driven method: LR and E (threshold = 0.1). Exceptfor Tzero, all the reference targets are able to accelerate the learning notably.</p>
<p>Figure 12 :
12Ttar = Ttrue (c) Ttar = Tzero (d) Ttar = Tcoarse (e) Ttar = T systemaitc (f) Ttar = T random Enhancements for physics-driven method: outputs of CNN (threshold = 0.1, iterative step = 1000). Tzero is not able to obtain the right solution. The other four reference targets are able to remedy the "valley" faster.</p>
<p>(No. 201703170250) and Yuxuan Zhang (No. 201804980021) are supported by China Scholarship Council when they conduct the work this paper represents. The authors thank the colleagues for the beneficial discussion, especially Chi Zhang and Nikolaos Perakis.</p>
<p>Table 1 .
1It is observed that the combined method improves physics-driven learning considerably. Compared with the true reference target, the other three coarse targets have a only slightly slower convergence speed. With decreasing L thr =0.15, 0.1 and 0.05, the acceleration obtained by using the true coarse target over the original physics-driven method are 22.7%, 34.4% and 49.0% respectively. Similar behaviors have been obtained for the other two coarse references. It is concluded that the smaller thresholds result in the bigger improvements. While in a real application, a too small threshold may result in the non-convergence as suggested by the zero profile reference. So one may choose a moderate threshold for safety.</p>
<p>Table 1 :
1Costs of different methods. The cost is represented as the number of iteration steps when E reaches L thr and the convergence absolute criteria C (0.01).Methods </p>
<p>L thr = 0.15 
L thr = 0.1 
L thr = 0.05 </p>
<p>L thr 
C 
L thr 
C 
L thr 
C </p>
<p>Physics-driven 
1322 
13494 
2542 
13494 
6334 
13494 </p>
<p>T true 
51 
9875 
112 
8904 
138 
6758 </p>
<p>T coarse 
57 
10428 
109 
8853 
145 
6876 </p>
<p>T systematic 
48 
10091 
116 
9025 
140 
6894 </p>
<p>T random 
53 
9917 
105 
8963 
139 
7054 </p>
<p>Data-driven multi-scale multi-physics models to derive process-structure-property relationships for additive manufacturing. W Yan, S Lin, O L Kafka, Y Lian, C Yu, Z Liu, J Yan, S Wolff, H Wu, E Ndip-Agbor, Computational Mechanics. 615W. Yan, S. Lin, O. L. Kafka, Y. Lian, C. Yu, Z. Liu, J. Yan, S. Wolff, H. Wu, E. Ndip-Agbor, et al., Data-driven multi-scale multi-physics models to de- rive process-structure-property relationships for additive manufacturing, Computational Mechanics 61 (5) (2018) 521-541.</p>
<p>Machine learning methods for data-driven turbulence modeling. Z J Zhang, K Duraisamy, 22nd AIAA Computational Fluid Dynamics Conference. 2460Z. J. Zhang, K. Duraisamy, Machine learning methods for data-driven tur- bulence modeling, in: 22nd AIAA Computational Fluid Dynamics Confer- ence, 2015, p. 2460.</p>
<p>Data-driven computational mechanics. T Kirchdoerfer, M Ortiz, Computer Methods in Applied Mechanics and Engineering. 304T. Kirchdoerfer, M. Ortiz, Data-driven computational mechanics, Com- puter Methods in Applied Mechanics and Engineering 304 (2016) 81-101.</p>
<p>Physics-based and data-driven surrogate models for pumping optimization of coastal aquifers. V Christelis, A Mantoglou, European Water. 57V. Christelis, A. Mantoglou, Physics-based and data-driven surrogate mod- els for pumping optimization of coastal aquifers, European Water 57 (2017) 481-488.</p>
<p>Data-driven physics for human soft tissue animation. M Kim, G Pons-Moll, S Pujades, S Bang, J Kim, M J Black, S.-H Lee, ACM Transactions on Graphics (TOG). 36454M. Kim, G. Pons-Moll, S. Pujades, S. Bang, J. Kim, M. J. Black, S.-H. Lee, Data-driven physics for human soft tissue animation, ACM Transactions on Graphics (TOG) 36 (4) (2017) 54.</p>
<p>Dynamic mode decomposition of numerical and experimental data. P J Schmid, Journal of fluid mechanics. 656P. J. Schmid, Dynamic mode decomposition of numerical and experimental data, Journal of fluid mechanics 656 (2010) 5-28.</p>
<p>Machine learning for fluid mechanics. S L Brunton, B R Noack, P Koumoutsakos, Annual Review of Fluid Mechanics. 52S. L. Brunton, B. R. Noack, P. Koumoutsakos, Machine learning for fluid mechanics, Annual Review of Fluid Mechanics 52 (2019).</p>
<p>. J D Anderson, J Wendt, Computational fluid dynamics. 206SpringerJ. D. Anderson, J. Wendt, Computational fluid dynamics, Vol. 206, Springer, 1995.</p>
<p>Physics-informed machine learning approach for reconstructing reynolds stress modeling discrepancies based on dns data. J.-X Wang, J.-L Wu, H Xiao, Physical Review Fluids. 2334603J.-X. Wang, J.-L. Wu, H. Xiao, Physics-informed machine learning ap- proach for reconstructing reynolds stress modeling discrepancies based on dns data, Physical Review Fluids 2 (3) (2017) 034603.</p>
<p>Physics-informed machine learning approach for augmenting turbulence models: A comprehensive framework. J.-L Wu, H Xiao, E Paterson, Physical Review Fluids. 3774602J.-L. Wu, H. Xiao, E. Paterson, Physics-informed machine learning ap- proach for augmenting turbulence models: A comprehensive framework, Physical Review Fluids 3 (7) (2018) 074602.</p>
<p>I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. Generative adversarial netsI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative adversarial nets, in: Ad- vances in neural information processing systems, 2014, pp. 2672-2680.</p>
<p>A B Farimani, J Gomes, V S Pande, arXiv:1709.02432Deep learning the physics of transport phenomena. arXiv preprintA. B. Farimani, J. Gomes, V. S. Pande, Deep learning the physics of trans- port phenomena, arXiv preprint arXiv:1709.02432 (2017).</p>
<p>Turbulence modeling in the age of data. K Duraisamy, G Iaccarino, H Xiao, Annual Review of Fluid Mechanics. 51K. Duraisamy, G. Iaccarino, H. Xiao, Turbulence modeling in the age of data, Annual Review of Fluid Mechanics 51 (2019) 357-377.</p>
<p>Reynolds averaged turbulence modelling using deep neural networks with embedded invariance. J Ling, A Kurzawski, J Templeton, Journal of Fluid Mechanics. 807J. Ling, A. Kurzawski, J. Templeton, Reynolds averaged turbulence mod- elling using deep neural networks with embedded invariance, Journal of Fluid Mechanics 807 (2016) 155-166.</p>
<p>Deep generative markov state models. H Wu, A Mardt, L Pasquali, F Noe, Advances in Neural Information Processing Systems. H. Wu, A. Mardt, L. Pasquali, F. Noe, Deep generative markov state mod- els, in: Advances in Neural Information Processing Systems, 2018, pp. 3975-3984.</p>
<p>S Bhatnagar, Y Afshar, S Pan, K Duraisamy, S Kaushik, Prediction of aerodynamic flow fields using convolutional neural networks. S. Bhatnagar, Y. Afshar, S. Pan, K. Duraisamy, S. Kaushik, Prediction of aerodynamic flow fields using convolutional neural networks, Computa- tional Mechanics (2019) 1-21.</p>
<p>Data-driven fluid simulations using regression forests. S Jeong, B Solenthaler, M Pollefeys, M Gross, ACM Transactions on Graphics (TOG). 346199S. Jeong, B. Solenthaler, M. Pollefeys, M. Gross, et al., Data-driven fluid simulations using regression forests, ACM Transactions on Graphics (TOG) 34 (6) (2015) 199.</p>
<p>Accelerating eulerian fluid simulation with convolutional networks. J Tompson, K Schlachter, P Sprechmann, K Perlin, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70J. Tompson, K. Schlachter, P. Sprechmann, K. Perlin, Accelerating eule- rian fluid simulation with convolutional networks, in: Proceedings of the 34th International Conference on Machine Learning-Volume 70, JMLR. org, 2017, pp. 3424-3433.</p>
<p>N Thuerey, K Weissenow, H Mehrotra, N Mainali, L Prantl, X Hu, arXiv:1810.08217arXiv:1810.08217v2Deep learning methods for reynolds-averaged navier-stokes simulations of airfoil flows. arXiv preprintN. Thuerey, K. Weissenow, H. Mehrotra, N. Mainali, L. Prantl, X. Hu, Deep learning methods for reynolds-averaged navier-stokes simulations of airfoil flows, arXiv preprint arXiv:1810.08217 (2018). arXiv:1810.08217v2.</p>
<p>A machine learning approach for determining the turbulent diffusivity in film cooling flows. P M Milani, J Ling, G Saez-Mischlich, J Bodart, J K Eaton, Journal of Turbomachinery. 140221006P. M. Milani, J. Ling, G. Saez-Mischlich, J. Bodart, J. K. Eaton, A machine learning approach for determining the turbulent diffusivity in film cooling flows, Journal of Turbomachinery 140 (2) (2018) 021006.</p>
<p>Data-driven prediction of unsteady flow fields over a circular cylinder using deep learning. S Lee, D You, arXiv:1804.06076arXiv preprintS. Lee, D. You, Data-driven prediction of unsteady flow fields over a circular cylinder using deep learning, arXiv preprint arXiv:1804.06076 (2018).</p>
<p>Neural algorithm for solving differential equations. H Lee, I S Kang, Journal of Computational Physics. 911H. Lee, I. S. Kang, Neural algorithm for solving differential equations, Journal of Computational Physics 91 (1) (1990) 110-131.</p>
<p>Artificial neural networks for solving ordinary and partial differential equations. I E Lagaris, A Likas, D I Fotiadis, IEEE transactions on neural networks. 95I. E. Lagaris, A. Likas, D. I. Fotiadis, Artificial neural networks for solving ordinary and partial differential equations, IEEE transactions on neural networks 9 (5) (1998) 987-1000.</p>
<p>M Raissi, P Perdikaris, G E Karniadakis, arXiv:1711.10566Data-driven discovery of nonlinear partial differential equations. arXiv preprintPhysics informed deep learningM. Raissi, P. Perdikaris, G. E. Karniadakis, Physics informed deep learning (part ii): Data-driven discovery of nonlinear partial differential equations, arXiv preprint arXiv:1711.10566 (2017).</p>
<p>M Raissi, P Perdikaris, G E Karniadakis, arXiv:1711.10561Data-driven solutions of nonlinear partial differential equations. arXiv preprintPhysics informed deep learningM. Raissi, P. Perdikaris, G. E. Karniadakis, Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations, arXiv preprint arXiv:1711.10561 (2017).</p>
<p>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M Raissi, P Perdikaris, G E Karniadakis, Journal of Computational Physics. 378M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics-informed neural net- works: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations, Journal of Computational Physics 378 (2019) 686-707.</p>
<p>L Lu, X Meng, Z Mao, G E Karniadakis, arXiv:1907.04502Deepxde: A deep learning library for solving differential equations. arXiv preprintL. Lu, X. Meng, Z. Mao, G. E. Karniadakis, Deepxde: A deep learning library for solving differential equations, arXiv preprint arXiv:1907.04502 (2019).</p>
<p>L Sun, H Gao, S Pan, J.-X Wang, arXiv:1906.02382Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data. arXiv preprintL. Sun, H. Gao, S. Pan, J.-X. Wang, Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data, arXiv preprint arXiv:1906.02382 (2019).</p>
<p>R Sharma, A B Farimani, J Gomes, P Eastman, V Pande, arXiv:1807.11374Weaklysupervised deep learning of heat transport via physics informed loss. arXiv preprintR. Sharma, A. B. Farimani, J. Gomes, P. Eastman, V. Pande, Weakly- supervised deep learning of heat transport via physics informed loss, arXiv preprint arXiv:1807.11374 (2018).</p>
<p>Physicsconstrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Y Zhu, N Zabaras, P.-S Koutsourelakis, P Perdikaris, Journal of Computational Physics. 394Y. Zhu, N. Zabaras, P.-S. Koutsourelakis, P. Perdikaris, Physics- constrained deep learning for high-dimensional surrogate modeling and un- certainty quantification without labeled data, Journal of Computational Physics 394 (2019) 56-81.</p>
<p>N Geneva, N Zabaras, arXiv:1906.05747Modeling the dynamics of pde systems with physics-constrained deep auto-regressive networks. arXiv preprintN. Geneva, N. Zabaras, Modeling the dynamics of pde systems with physics-constrained deep auto-regressive networks, arXiv preprint arXiv:1906.05747 (2019).</p>
<p>U-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical image computing and computer-assisted intervention. SpringerO. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for biomedical image segmentation, in: International Conference on Medical image computing and computer-assisted intervention, Springer, 2015, pp. 234-241.</p>
<p>Pytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Advances in Neural Information Processing Systems. A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An impera- tive style, high-performance deep learning library, in: Advances in Neural Information Processing Systems, 2019, pp. 8024-8035.</p>
<p>D P Kingma, J Ba, arXiv:1412.6980Adam: A method for stochastic optimization. arXiv preprintD. P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv preprint arXiv:1412.6980 (2014).</p>
<p>Openfoam: A c++ library for complex physics simulations. H Jasak, A Jemcov, Z Tukovic, International workshop on coupled methods in numerical dynamics. IUC Dubrovnik Croatia1000H. Jasak, A. Jemcov, Z. Tukovic, et al., Openfoam: A c++ library for com- plex physics simulations, in: International workshop on coupled methods in numerical dynamics, Vol. 1000, IUC Dubrovnik Croatia, 2007, pp. 1-20.</p>
<p>An overview of gradient descent optimization algorithms. S Ruder, arXiv:1609.04747arXiv preprintS. Ruder, An overview of gradient descent optimization algorithms, arXiv preprint arXiv:1609.04747 (2016).</p>
<p>Multi-task learning and weighted crossentropy for dnn-based keyword spotting. S Panchapagesan, M Sun, A Khare, S Matsoukas, A Mandal, B Hoffmeister, S Vitaladevuni, Interspeech9S. Panchapagesan, M. Sun, A. Khare, S. Matsoukas, A. Mandal, B. Hoffmeister, S. Vitaladevuni, Multi-task learning and weighted cross- entropy for dnn-based keyword spotting., in: Interspeech, Vol. 9, 2016, pp. 760-764.</p>
<p>You only look once: Unified, real-time object detection. J Redmon, S Divvala, R Girshick, A Farhadi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJ. Redmon, S. Divvala, R. Girshick, A. Farhadi, You only look once: Uni- fied, real-time object detection, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 779-788.</p>
<p>Dnn and cnn with weighted and multi-task loss functions for audio event detection. H Phan, M Krawczyk-Becker, T Gerkmann, A Mertins, arXiv:1708.03211arXiv preprintH. Phan, M. Krawczyk-Becker, T. Gerkmann, A. Mertins, Dnn and cnn with weighted and multi-task loss functions for audio event detection, arXiv preprint arXiv:1708.03211 (2017).</p>            </div>
        </div>

    </div>
</body>
</html>
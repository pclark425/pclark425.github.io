<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9308 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9308</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9308</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-262012566</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.08532v3.pdf" target="_blank">EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9308.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9308.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BBH_CoT_vs_EVOPROMPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BIG-Bench Hard (BBH) Chain-of-Thought baseline versus EVOPROMPT-optimized prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of the standard Chain-of-Thought (CoT) prompt "Let's think step by step." (with 3-shot CoT demos) against prompts optimized by EVOPROMPT (DE and GA) on 22 BBH reasoning tasks, reporting normalized improvements relative to the baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BIG-Bench Hard (BBH) reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A suite of 22 challenging multi-step reasoning tasks from BIG-Bench Hard requiring multi-step chain-of-thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>3-shot Chain-of-Thought demonstrations with the baseline instruction "Let's think step by step."; EVOPROMPT replaces the baseline instruction with optimized discrete instruction prompts (single instruction prompt plus 3-shot CoT examples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Baseline CoT prompt "Let's think step by step." (3-shot) compared to EVOPROMPT (DE) and EVOPROMPT (GA) optimized instruction prompts (used with same 3-shot CoT demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>normalized baseline ("Let's think step by step.") used as 0; EVOPROMPT (DE) achieved average +3.5% normalized score and up to +25% on some tasks; EVOPROMPT (GA) achieved average +2.5% normalized score (per-paper aggregated normalized scores across 22 tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>EVOPROMPT (DE) generally outperformed EVOPROMPT (GA) (DE > GA by >2% on 6 tasks; GA sometimes better on individual tasks but gap small ~1%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+up to 25% normalized score on individual BBH tasks; mean improvement +3.5% for DE vs baseline, +2.5% for GA vs baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Optimizing the discrete instruction prompt (while keeping the same CoT demonstrations) can yield better task-aligned instructions that improve model reasoning; DE's design (mutating only differing parts and incorporating current best prompt) helps escape local optima leading to larger gains on hard reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>EVOPROMPT used GPT-3.5 to implement evolutionary operators and optimized prompts for GPT-3.5 evaluation; 3-shot chain-of-thought demonstrations were used; for each BBH task a subset of the test set was sampled as development set for prompt optimization; normalized scores reported relative to the baseline instruction; same initial population used for all 22 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9308.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9308.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TextClass_FewShot_vs_EVOPROMPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot classification with manual prompts versus EVOPROMPT-optimized prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of discrete instruction presentation formats (manual human-designed instructions, PromptSource/Natural Instructions) versus EVOPROMPT-generated prompts on seven language understanding classification tasks, using one demonstration per class (few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B (and experiments reported for GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B for Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language understanding (SST-2, MR, CR, SST-5, AG's News, TREC, Subj)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary and multi-class sentiment, topic, and subjectivity classification tasks; outputs restricted by prepending a demonstration with one example per class.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Discrete instruction prompts (human-crafted manual instructions, Natural Instructions, PromptSource variants); few-shot demonstrations: prepend one example per class before test case (1-shot per class). EVOPROMPT generates new human-readable discrete instructions via evolutionary search.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Manual Instructions (MI), Natural Instructions (NI), PromptSource, and other automatic prompt methods (APE, APO) compared against EVOPROMPT (DE and GA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example aggregated accuracy: Manual Instructions (MI) avg ≈ 71.07% (Alpaca per Table 1), EVOPROMPT (GA) avg ≈ 76.25%, EVOPROMPT (DE) avg ≈ 77.05% (Alpaca averages reported in paper). Specific per-task numbers reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>EVOPROMPT (DE/GA) outperform MI, NI, PromptSource and previous automated methods (APE/APO) on average; e.g., EVOPROMPT increases average accuracy by ~5–6 percentage points over MI in reported aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+~5–6% absolute accuracy improvement on average (EVOPROMPT vs manual prompts) across the evaluated classification datasets (Alpaca results).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Optimizing the wording and structure of the discrete instruction better aligns the model's behavior with the task; evolutionary search balances exploration and exploitation to find higher-quality human-readable instructions; DE is particularly helpful for escaping local optima when initial prompts are poor (notably for Subj).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Development set size = 200 for classification; results on Alpaca averaged over 3 seeds with standard deviations; for GPT-3.5 only one seed due to cost; evaluation used full test sets as in prior works; demonstrations: one example per class prepended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9308.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9308.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summarization_ROUGE_vs_EVOPROMPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Summarization (SAMSum) ROUGE performance: manual prompt vs EVOPROMPT (GA/DE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of different prompt presentation formats (manual instructions vs EVOPROMPT-optimized discrete prompts) for dialogue summarization measured by ROUGE-1/2/L on SAMSum using Alpaca-7B and GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B and GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B for Alpaca; GPT-3.5 size not specified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SAMSum dialogue summarization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Dialogue abstractive summarization dataset; evaluation with ROUGE-1, ROUGE-2, ROUGE-L.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Text generation prompts: manual instruction templates (Sanh et al. baseline), compared with EVOPROMPT-generated discrete instructions; likely accompanied by task-specific templates described in Appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Manual Instruction (Sanh et al. 2021) and APE (iterative Monte Carlo search) vs EVOPROMPT (GA and DE).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Alpaca ROUGE-1: MI 35.92 vs EVOPROMPT (DE) 39.46; ROUGE-2: MI 11.16 vs DE 13.93; ROUGE-L: MI 31.67 vs DE 35.49. GPT-3.5 ROUGE-1: MI 43.95 vs DE 46.49; ROUGE-2: MI 17.11 vs DE 19.49; ROUGE-L: MI 39.09 vs DE 41.96 (per Table 2 in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>EVOPROMPT (DE) > EVOPROMPT (GA) (DE outperforms GA notably on summarization); both EVOPROMPT variants outperform MI and APE.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Alpaca: +3.54 ROUGE-1 points (DE vs MI); GPT-3.5: +2.54 ROUGE-1 points (DE vs MI). DE improved ~1 ROUGE-1 point over GA in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>DE's focus on modifying differing parts while preserving common useful components helps create instruction prompts that better guide generation tasks; DE's variant is particularly effective for complex language generation tasks like summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>ROUGE scores reported on test sets; Alpaca results averaged over 3 seeds; templates for generation follow prior works; population size and iteration hyper-parameters in Appendix; EVOPROMPT (DE) and (GA) implemented with GPT-3.5 performing evolutionary operators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9308.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9308.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simplification_SARI_vs_EVOPROMPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text simplification (ASSET) SARI performance: manual prompt vs EVOPROMPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of manual instruction prompts against EVOPROMPT-generated discrete prompts for sentence simplification using the ASSET dataset measured by SARI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B and GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B for Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ASSET text simplification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Sentence simplification benchmark with multiple references; measured with SARI score.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Task-specific discrete instruction prompts for simplification (manual baseline templates) versus EVOPROMPT-optimized human-readable discrete prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Manual Instruction (Zhang et al. baseline), APE, EVOPROMPT (GA), EVOPROMPT (DE).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per Table 3: Alpaca SARI: MI 43.03, APE 45.90, EVOPROMPT (GA) 46.43, EVOPROMPT (DE) 46.21. GPT-3.5: MI 43.80 vs EVOPROMPT (DE) 47.40 (approx +3.6 SARI points).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>EVOPROMPT (GA/DE) outperform MI and APE; both GA and DE provide >3 SARI points improvement over MI in reported cases.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+~3 SARI points improvement across Alpaca and GPT-3.5 compared with manual instruction baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Optimized discrete instruction prompts found by evolutionary search better instruct model behavior (what to delete/keep/modify) for editing tasks; EVOPROMPT balances exploration/exploitation more effectively than simple re-sampling or edit-focused methods.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>ASSET evaluation used SARI metric; initial population collected from prior works and augmented to size 10; Alpaca results averaged across 3 seeds; generation templates and decoding hyperparameters described in Appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9308.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9308.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DE_mutation_diff_vs_all</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differential Evolution: mutate only differing parts vs mutate all parts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A design ablation in EVOPROMPT (DE) that compares mutating only the different parts between two prompts ('Diff') versus allowing mutation over the entire prompts ('All'), evaluated on Subj (understanding) and ASSET (generation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B (experiments reported for DE design ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Subj (subjectivity classification) and ASSET (simplification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Subj: classify sentences as subjective vs objective; ASSET: text simplification measured by SARI.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>EVOPROMPT (DE) evolutionary operator variants: (1) mutate only the different parts between two prompts (Diff); (2) randomly mutate entire prompts (All).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Diff (proposed) versus All (ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that the 'Diff' design consistently yields performance gains across both tasks (numerical values in Table 5 but not reproduced in-text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>'Diff' > 'All' for Subj and ASSET (consistent gains reported in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Preserving shared components of prompts (which likely encode beneficial task-specific phrasing) while mutating only differing parts reduces destructive edits and concentrates search on the meaningful variations (the analogue of DE's differential vector), improving optimization efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Ablation implemented by replacing the first two DE steps with instruction to randomly mutate both prompts; evaluation on Subj and ASSET reported in Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9308.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9308.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DE_prompt3_best_vs_random_eliminate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DE design: using current best prompt as Prompt 3 vs random Prompt 3 vs eliminating Prompt 3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation testing the effect of choosing Prompt 3 in the DE instantiation: using the current top-performing prompt (proposed), sampling Prompt 3 randomly, or eliminating the use of Prompt 3 entirely.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B (DE ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Subj and ASSET (ablation targets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Subjectivity classification and text simplification tasks used to measure the impact of Prompt 3 selection in DE.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>DE evolutionary operator where mutated different parts are combined with a third prompt (Prompt 3) before crossover; variants differ in how Prompt 3 is chosen.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Prompt 3 = current best prompt (proposed) vs Prompt 3 = random prompt vs Prompt 3 eliminated (direct crossover with mutated parts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that introducing Prompt 3 is important and choosing the best prompt as Prompt 3 is more effective than random sampling (detailed numbers in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Best-as-Prompt-3 > random-as-Prompt-3 > eliminate-Prompt-3 (qualitative ordering from Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Incorporating the current best prompt anchors the mutation to high-quality formulations, enabling exploitation of proven phrasing while still injecting novel differences; random or no anchoring reduces effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Ablation settings compared on Subj and ASSET with performance shown in Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9308.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9308.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GA_selection_roulette_vs_others</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Algorithm selection strategy: roulette-wheel selection vs tournament vs random</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of parent selection strategies inside EVOPROMPT (GA) showing that roulette-wheel selection (fitness-proportionate) yields higher scores than tournament or random selection on BBH tasks and other evaluated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B / GPT-3.5 (GA experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B for Alpaca; GPT-3.5 unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BBH tasks and other datasets (GA selection ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various tasks where GA selection strategy influences which parental prompts are chosen to produce offspring.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>GA evolutionary process with different parent-selection formats: roulette-wheel (fitness proportionate), tournament selection, and random selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Roulette-wheel vs Tournament vs Random selection strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper states EVOPROMPT (GA) with roulette-wheel selection achieves higher scores than tournament and random selection (Table 4 contains numeric comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Roulette-wheel selection performed best empirically in their GA instantiation; tournament and random underperformed relative to roulette-wheel in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Roulette-wheel selection biases parent choice toward higher-fitness prompts while still allowing lower-fitness ones a chance, providing a balance between exploitation and exploration suited to prompt optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Selection ablation experiments reported in Table 4 (BBH/other datasets); GA used roulette-wheel by default in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9308.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9308.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InitPop_top_random_bottom_variations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of initial population quality and introduced variations on EVOPROMPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study of how the choice of initial prompts (top-performing, randomly selected, bottom-performing) and adding GPT-3.5-generated variations affects EVOPROMPT performance on SST-5 and related datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B (initialization ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-5 (and pilot experiments across others)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Sentiment classification (5-way) used to measure sensitivity to initialization quality.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Initial population selection format: top-n prompts (highest dev performance), random-n prompts, bottom-n prompts; optional augmentation with GPT-3.5 resampled variations (var-n).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>top-n vs random-n vs bottom-n with/without variations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports: (1) Randomly selecting initial prompts achieves similar performance to selecting top-performing ones; (2) adding generated variations to top prompts gives slight improvement; (3) DE outperforms GA when starting from bottom-performing initial prompts (Table 6 provides numeric values).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Top-with-variations slightly > top-no-variations; random ≈ top; DE > GA when initial prompts are poor (bottom).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>EVOPROMPT's evolutionary search can recover strong prompts starting from diverse or even low-quality seeds; diversity in the population matters more than perfect hand-crafted initialization, and DE's mutation strategy helps when seeds are poor.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompts were pre-ranked by dev performance; resampling template (from Zhou et al. 2022) used to generate variations; results summarized in Table 6 on SST-5.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9308.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9308.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PopulationSize_iter_effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effects of population size and number of iterations on EVOPROMPT performance and convergence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis of how population size (4–12) and number of evolutionary iterations affect EVOPROMPT (DE and GA) performance and convergence speed on classification and generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B (population/iteration analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-5, Subj (classification), ASSET (generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Representative classification and generation tasks used to study trade-offs between population diversity and overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>EVOPROMPT evolutionary search with varying population sizes and fixed iteration budgets; report best and average dev scores per iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Population sizes compared (4,6,8,10,12) and iteration trajectories (best vs average over time) for DE and GA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports that for classification tasks performance increases with population size for both DE and GA (DE benefit greater); for ASSET (simplification) a population of 6 performs comparably to 10 with much less overhead. Convergence curves (best/average) gradually improve over iterations (Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Larger populations improve performance for complex tasks (Subj); small populations suffice for simpler generation tasks (ASSET). DE benefits more from larger population diversity than GA.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Larger populations provide more diversity enabling DE in particular to explore differing parts effectively; however, generation tasks with simpler optimization landscapes don't need large populations, so the format (population size) should be tuned based on task complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Population sizes explored from 4 to 12; curves averaged over 3 seeds; figures plotted in Figure 6 and convergence in Figure 7.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9308.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9308.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptDiversity_DE_vs_GA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt diversity: EVOPROMPT (DE) produces longer, higher-variance prompts and more novel words than GA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical observation that DE-generated populations tend to have longer prompts, higher variance, and more new words mutated per iteration compared to GA, indicating DE favors exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-7B (diversity analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate (8 datasets: 7 understanding + 1 simplification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Diversity statistics collected across a set of understanding and simplification datasets to measure properties of prompts evolved by GA vs DE.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Comparison of resulting prompt population statistics over iterations: average length, length variance, number of new words mutated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>EVOPROMPT (DE) vs EVOPROMPT (GA) population statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports DE produces longer prompts with higher variance and mutates more new words than GA across iterations (Figure 9).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>DE's populations are more diverse (length, variance, new words) than GA's, suggesting greater exploration capability.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>DE's design (mutating the differing parts and recombining with a best anchor) yields more substantial and diverse prompt edits, allowing escape from local optima; GA's token-inheritance style yields more conservative edits and less lexical novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Statistics averaged over 8 datasets and 3 random seeds; plotted in Figure 9.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Let's think step by step <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>APE: Automatic prompt engineer (Zhou et al., 2022) <em>(Rating: 2)</em></li>
                <li>Automatic prompt optimization with "gradient descent" and beam search (Pryzant et al., 2023) <em>(Rating: 2)</em></li>
                <li>Natural Instructions (Mishra et al., 2022b) <em>(Rating: 1)</em></li>
                <li>PromptSource (Bach et al., 2022) <em>(Rating: 1)</em></li>
                <li>Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance (Fu et al., 2023) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9308",
    "paper_id": "paper-262012566",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "BBH_CoT_vs_EVOPROMPT",
            "name_full": "BIG-Bench Hard (BBH) Chain-of-Thought baseline versus EVOPROMPT-optimized prompts",
            "brief_description": "Comparison of the standard Chain-of-Thought (CoT) prompt \"Let's think step by step.\" (with 3-shot CoT demos) against prompts optimized by EVOPROMPT (DE and GA) on 22 BBH reasoning tasks, reporting normalized improvements relative to the baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003)",
            "model_size": null,
            "task_name": "BIG-Bench Hard (BBH) reasoning tasks",
            "task_description": "A suite of 22 challenging multi-step reasoning tasks from BIG-Bench Hard requiring multi-step chain-of-thought reasoning.",
            "presentation_format": "3-shot Chain-of-Thought demonstrations with the baseline instruction \"Let's think step by step.\"; EVOPROMPT replaces the baseline instruction with optimized discrete instruction prompts (single instruction prompt plus 3-shot CoT examples).",
            "comparison_format": "Baseline CoT prompt \"Let's think step by step.\" (3-shot) compared to EVOPROMPT (DE) and EVOPROMPT (GA) optimized instruction prompts (used with same 3-shot CoT demonstrations).",
            "performance": "normalized baseline (\"Let's think step by step.\") used as 0; EVOPROMPT (DE) achieved average +3.5% normalized score and up to +25% on some tasks; EVOPROMPT (GA) achieved average +2.5% normalized score (per-paper aggregated normalized scores across 22 tasks).",
            "performance_comparison": "EVOPROMPT (DE) generally outperformed EVOPROMPT (GA) (DE &gt; GA by &gt;2% on 6 tasks; GA sometimes better on individual tasks but gap small ~1%).",
            "format_effect_size": "+up to 25% normalized score on individual BBH tasks; mean improvement +3.5% for DE vs baseline, +2.5% for GA vs baseline.",
            "explanation_or_hypothesis": "Optimizing the discrete instruction prompt (while keeping the same CoT demonstrations) can yield better task-aligned instructions that improve model reasoning; DE's design (mutating only differing parts and incorporating current best prompt) helps escape local optima leading to larger gains on hard reasoning tasks.",
            "null_or_negative_result": false,
            "experimental_details": "EVOPROMPT used GPT-3.5 to implement evolutionary operators and optimized prompts for GPT-3.5 evaluation; 3-shot chain-of-thought demonstrations were used; for each BBH task a subset of the test set was sampled as development set for prompt optimization; normalized scores reported relative to the baseline instruction; same initial population used for all 22 tasks.",
            "uuid": "e9308.0",
            "source_info": {
                "paper_title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "TextClass_FewShot_vs_EVOPROMPT",
            "name_full": "Few-shot classification with manual prompts versus EVOPROMPT-optimized prompts",
            "brief_description": "Evaluation of discrete instruction presentation formats (manual human-designed instructions, PromptSource/Natural Instructions) versus EVOPROMPT-generated prompts on seven language understanding classification tasks, using one demonstration per class (few-shot).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B (and experiments reported for GPT-3.5)",
            "model_size": "7B for Alpaca",
            "task_name": "Language understanding (SST-2, MR, CR, SST-5, AG's News, TREC, Subj)",
            "task_description": "Binary and multi-class sentiment, topic, and subjectivity classification tasks; outputs restricted by prepending a demonstration with one example per class.",
            "presentation_format": "Discrete instruction prompts (human-crafted manual instructions, Natural Instructions, PromptSource variants); few-shot demonstrations: prepend one example per class before test case (1-shot per class). EVOPROMPT generates new human-readable discrete instructions via evolutionary search.",
            "comparison_format": "Manual Instructions (MI), Natural Instructions (NI), PromptSource, and other automatic prompt methods (APE, APO) compared against EVOPROMPT (DE and GA).",
            "performance": "Example aggregated accuracy: Manual Instructions (MI) avg ≈ 71.07% (Alpaca per Table 1), EVOPROMPT (GA) avg ≈ 76.25%, EVOPROMPT (DE) avg ≈ 77.05% (Alpaca averages reported in paper). Specific per-task numbers reported in Table 1.",
            "performance_comparison": "EVOPROMPT (DE/GA) outperform MI, NI, PromptSource and previous automated methods (APE/APO) on average; e.g., EVOPROMPT increases average accuracy by ~5–6 percentage points over MI in reported aggregate.",
            "format_effect_size": "+~5–6% absolute accuracy improvement on average (EVOPROMPT vs manual prompts) across the evaluated classification datasets (Alpaca results).",
            "explanation_or_hypothesis": "Optimizing the wording and structure of the discrete instruction better aligns the model's behavior with the task; evolutionary search balances exploration and exploitation to find higher-quality human-readable instructions; DE is particularly helpful for escaping local optima when initial prompts are poor (notably for Subj).",
            "null_or_negative_result": false,
            "experimental_details": "Development set size = 200 for classification; results on Alpaca averaged over 3 seeds with standard deviations; for GPT-3.5 only one seed due to cost; evaluation used full test sets as in prior works; demonstrations: one example per class prepended.",
            "uuid": "e9308.1",
            "source_info": {
                "paper_title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Summarization_ROUGE_vs_EVOPROMPT",
            "name_full": "Summarization (SAMSum) ROUGE performance: manual prompt vs EVOPROMPT (GA/DE)",
            "brief_description": "Comparison of different prompt presentation formats (manual instructions vs EVOPROMPT-optimized discrete prompts) for dialogue summarization measured by ROUGE-1/2/L on SAMSum using Alpaca-7B and GPT-3.5.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B and GPT-3.5 (text-davinci-003)",
            "model_size": "7B for Alpaca; GPT-3.5 size not specified",
            "task_name": "SAMSum dialogue summarization",
            "task_description": "Dialogue abstractive summarization dataset; evaluation with ROUGE-1, ROUGE-2, ROUGE-L.",
            "presentation_format": "Text generation prompts: manual instruction templates (Sanh et al. baseline), compared with EVOPROMPT-generated discrete instructions; likely accompanied by task-specific templates described in Appendix.",
            "comparison_format": "Manual Instruction (Sanh et al. 2021) and APE (iterative Monte Carlo search) vs EVOPROMPT (GA and DE).",
            "performance": "Alpaca ROUGE-1: MI 35.92 vs EVOPROMPT (DE) 39.46; ROUGE-2: MI 11.16 vs DE 13.93; ROUGE-L: MI 31.67 vs DE 35.49. GPT-3.5 ROUGE-1: MI 43.95 vs DE 46.49; ROUGE-2: MI 17.11 vs DE 19.49; ROUGE-L: MI 39.09 vs DE 41.96 (per Table 2 in paper).",
            "performance_comparison": "EVOPROMPT (DE) &gt; EVOPROMPT (GA) (DE outperforms GA notably on summarization); both EVOPROMPT variants outperform MI and APE.",
            "format_effect_size": "Alpaca: +3.54 ROUGE-1 points (DE vs MI); GPT-3.5: +2.54 ROUGE-1 points (DE vs MI). DE improved ~1 ROUGE-1 point over GA in some settings.",
            "explanation_or_hypothesis": "DE's focus on modifying differing parts while preserving common useful components helps create instruction prompts that better guide generation tasks; DE's variant is particularly effective for complex language generation tasks like summarization.",
            "null_or_negative_result": false,
            "experimental_details": "ROUGE scores reported on test sets; Alpaca results averaged over 3 seeds; templates for generation follow prior works; population size and iteration hyper-parameters in Appendix; EVOPROMPT (DE) and (GA) implemented with GPT-3.5 performing evolutionary operators.",
            "uuid": "e9308.2",
            "source_info": {
                "paper_title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Simplification_SARI_vs_EVOPROMPT",
            "name_full": "Text simplification (ASSET) SARI performance: manual prompt vs EVOPROMPT",
            "brief_description": "Comparison of manual instruction prompts against EVOPROMPT-generated discrete prompts for sentence simplification using the ASSET dataset measured by SARI.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B and GPT-3.5",
            "model_size": "7B for Alpaca",
            "task_name": "ASSET text simplification",
            "task_description": "Sentence simplification benchmark with multiple references; measured with SARI score.",
            "presentation_format": "Task-specific discrete instruction prompts for simplification (manual baseline templates) versus EVOPROMPT-optimized human-readable discrete prompts.",
            "comparison_format": "Manual Instruction (Zhang et al. baseline), APE, EVOPROMPT (GA), EVOPROMPT (DE).",
            "performance": "Per Table 3: Alpaca SARI: MI 43.03, APE 45.90, EVOPROMPT (GA) 46.43, EVOPROMPT (DE) 46.21. GPT-3.5: MI 43.80 vs EVOPROMPT (DE) 47.40 (approx +3.6 SARI points).",
            "performance_comparison": "EVOPROMPT (GA/DE) outperform MI and APE; both GA and DE provide &gt;3 SARI points improvement over MI in reported cases.",
            "format_effect_size": "+~3 SARI points improvement across Alpaca and GPT-3.5 compared with manual instruction baselines.",
            "explanation_or_hypothesis": "Optimized discrete instruction prompts found by evolutionary search better instruct model behavior (what to delete/keep/modify) for editing tasks; EVOPROMPT balances exploration/exploitation more effectively than simple re-sampling or edit-focused methods.",
            "null_or_negative_result": false,
            "experimental_details": "ASSET evaluation used SARI metric; initial population collected from prior works and augmented to size 10; Alpaca results averaged across 3 seeds; generation templates and decoding hyperparameters described in Appendix.",
            "uuid": "e9308.3",
            "source_info": {
                "paper_title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "DE_mutation_diff_vs_all",
            "name_full": "Differential Evolution: mutate only differing parts vs mutate all parts",
            "brief_description": "A design ablation in EVOPROMPT (DE) that compares mutating only the different parts between two prompts ('Diff') versus allowing mutation over the entire prompts ('All'), evaluated on Subj (understanding) and ASSET (generation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B (experiments reported for DE design ablations)",
            "model_size": "7B",
            "task_name": "Subj (subjectivity classification) and ASSET (simplification)",
            "task_description": "Subj: classify sentences as subjective vs objective; ASSET: text simplification measured by SARI.",
            "presentation_format": "EVOPROMPT (DE) evolutionary operator variants: (1) mutate only the different parts between two prompts (Diff); (2) randomly mutate entire prompts (All).",
            "comparison_format": "Diff (proposed) versus All (ablation).",
            "performance": "Paper reports that the 'Diff' design consistently yields performance gains across both tasks (numerical values in Table 5 but not reproduced in-text).",
            "performance_comparison": "'Diff' &gt; 'All' for Subj and ASSET (consistent gains reported in Table 5).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Preserving shared components of prompts (which likely encode beneficial task-specific phrasing) while mutating only differing parts reduces destructive edits and concentrates search on the meaningful variations (the analogue of DE's differential vector), improving optimization efficiency.",
            "null_or_negative_result": false,
            "experimental_details": "Ablation implemented by replacing the first two DE steps with instruction to randomly mutate both prompts; evaluation on Subj and ASSET reported in Table 5.",
            "uuid": "e9308.4",
            "source_info": {
                "paper_title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "DE_prompt3_best_vs_random_eliminate",
            "name_full": "DE design: using current best prompt as Prompt 3 vs random Prompt 3 vs eliminating Prompt 3",
            "brief_description": "Ablation testing the effect of choosing Prompt 3 in the DE instantiation: using the current top-performing prompt (proposed), sampling Prompt 3 randomly, or eliminating the use of Prompt 3 entirely.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B (DE ablations)",
            "model_size": "7B",
            "task_name": "Subj and ASSET (ablation targets)",
            "task_description": "Subjectivity classification and text simplification tasks used to measure the impact of Prompt 3 selection in DE.",
            "presentation_format": "DE evolutionary operator where mutated different parts are combined with a third prompt (Prompt 3) before crossover; variants differ in how Prompt 3 is chosen.",
            "comparison_format": "Prompt 3 = current best prompt (proposed) vs Prompt 3 = random prompt vs Prompt 3 eliminated (direct crossover with mutated parts).",
            "performance": "Paper reports that introducing Prompt 3 is important and choosing the best prompt as Prompt 3 is more effective than random sampling (detailed numbers in Table 5).",
            "performance_comparison": "Best-as-Prompt-3 &gt; random-as-Prompt-3 &gt; eliminate-Prompt-3 (qualitative ordering from Table 5).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Incorporating the current best prompt anchors the mutation to high-quality formulations, enabling exploitation of proven phrasing while still injecting novel differences; random or no anchoring reduces effectiveness.",
            "null_or_negative_result": false,
            "experimental_details": "Ablation settings compared on Subj and ASSET with performance shown in Table 5.",
            "uuid": "e9308.5",
            "source_info": {
                "paper_title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GA_selection_roulette_vs_others",
            "name_full": "Genetic Algorithm selection strategy: roulette-wheel selection vs tournament vs random",
            "brief_description": "Comparison of parent selection strategies inside EVOPROMPT (GA) showing that roulette-wheel selection (fitness-proportionate) yields higher scores than tournament or random selection on BBH tasks and other evaluated datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B / GPT-3.5 (GA experiments)",
            "model_size": "7B for Alpaca; GPT-3.5 unspecified",
            "task_name": "BBH tasks and other datasets (GA selection ablation)",
            "task_description": "Various tasks where GA selection strategy influences which parental prompts are chosen to produce offspring.",
            "presentation_format": "GA evolutionary process with different parent-selection formats: roulette-wheel (fitness proportionate), tournament selection, and random selection.",
            "comparison_format": "Roulette-wheel vs Tournament vs Random selection strategies.",
            "performance": "Paper states EVOPROMPT (GA) with roulette-wheel selection achieves higher scores than tournament and random selection (Table 4 contains numeric comparisons).",
            "performance_comparison": "Roulette-wheel selection performed best empirically in their GA instantiation; tournament and random underperformed relative to roulette-wheel in reported experiments.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Roulette-wheel selection biases parent choice toward higher-fitness prompts while still allowing lower-fitness ones a chance, providing a balance between exploitation and exploration suited to prompt optimization.",
            "null_or_negative_result": false,
            "experimental_details": "Selection ablation experiments reported in Table 4 (BBH/other datasets); GA used roulette-wheel by default in main experiments.",
            "uuid": "e9308.6",
            "source_info": {
                "paper_title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "InitPop_top_random_bottom_variations",
            "name_full": "Effect of initial population quality and introduced variations on EVOPROMPT",
            "brief_description": "Study of how the choice of initial prompts (top-performing, randomly selected, bottom-performing) and adding GPT-3.5-generated variations affects EVOPROMPT performance on SST-5 and related datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B (initialization ablation)",
            "model_size": "7B",
            "task_name": "SST-5 (and pilot experiments across others)",
            "task_description": "Sentiment classification (5-way) used to measure sensitivity to initialization quality.",
            "presentation_format": "Initial population selection format: top-n prompts (highest dev performance), random-n prompts, bottom-n prompts; optional augmentation with GPT-3.5 resampled variations (var-n).",
            "comparison_format": "top-n vs random-n vs bottom-n with/without variations.",
            "performance": "Paper reports: (1) Randomly selecting initial prompts achieves similar performance to selecting top-performing ones; (2) adding generated variations to top prompts gives slight improvement; (3) DE outperforms GA when starting from bottom-performing initial prompts (Table 6 provides numeric values).",
            "performance_comparison": "Top-with-variations slightly &gt; top-no-variations; random ≈ top; DE &gt; GA when initial prompts are poor (bottom).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "EVOPROMPT's evolutionary search can recover strong prompts starting from diverse or even low-quality seeds; diversity in the population matters more than perfect hand-crafted initialization, and DE's mutation strategy helps when seeds are poor.",
            "null_or_negative_result": false,
            "experimental_details": "Prompts were pre-ranked by dev performance; resampling template (from Zhou et al. 2022) used to generate variations; results summarized in Table 6 on SST-5.",
            "uuid": "e9308.7",
            "source_info": {
                "paper_title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "PopulationSize_iter_effects",
            "name_full": "Effects of population size and number of iterations on EVOPROMPT performance and convergence",
            "brief_description": "Analysis of how population size (4–12) and number of evolutionary iterations affect EVOPROMPT (DE and GA) performance and convergence speed on classification and generation tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B (population/iteration analyses)",
            "model_size": "7B",
            "task_name": "SST-5, Subj (classification), ASSET (generation)",
            "task_description": "Representative classification and generation tasks used to study trade-offs between population diversity and overhead.",
            "presentation_format": "EVOPROMPT evolutionary search with varying population sizes and fixed iteration budgets; report best and average dev scores per iteration.",
            "comparison_format": "Population sizes compared (4,6,8,10,12) and iteration trajectories (best vs average over time) for DE and GA.",
            "performance": "Paper reports that for classification tasks performance increases with population size for both DE and GA (DE benefit greater); for ASSET (simplification) a population of 6 performs comparably to 10 with much less overhead. Convergence curves (best/average) gradually improve over iterations (Figure 7).",
            "performance_comparison": "Larger populations improve performance for complex tasks (Subj); small populations suffice for simpler generation tasks (ASSET). DE benefits more from larger population diversity than GA.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Larger populations provide more diversity enabling DE in particular to explore differing parts effectively; however, generation tasks with simpler optimization landscapes don't need large populations, so the format (population size) should be tuned based on task complexity.",
            "null_or_negative_result": false,
            "experimental_details": "Population sizes explored from 4 to 12; curves averaged over 3 seeds; figures plotted in Figure 6 and convergence in Figure 7.",
            "uuid": "e9308.8",
            "source_info": {
                "paper_title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "PromptDiversity_DE_vs_GA",
            "name_full": "Prompt diversity: EVOPROMPT (DE) produces longer, higher-variance prompts and more novel words than GA",
            "brief_description": "Empirical observation that DE-generated populations tend to have longer prompts, higher variance, and more new words mutated per iteration compared to GA, indicating DE favors exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Alpaca-7B (diversity analysis)",
            "model_size": "7B",
            "task_name": "Aggregate (8 datasets: 7 understanding + 1 simplification)",
            "task_description": "Diversity statistics collected across a set of understanding and simplification datasets to measure properties of prompts evolved by GA vs DE.",
            "presentation_format": "Comparison of resulting prompt population statistics over iterations: average length, length variance, number of new words mutated.",
            "comparison_format": "EVOPROMPT (DE) vs EVOPROMPT (GA) population statistics.",
            "performance": "Paper reports DE produces longer prompts with higher variance and mutates more new words than GA across iterations (Figure 9).",
            "performance_comparison": "DE's populations are more diverse (length, variance, new words) than GA's, suggesting greater exploration capability.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "DE's design (mutating the differing parts and recombining with a best anchor) yields more substantial and diverse prompt edits, allowing escape from local optima; GA's token-inheritance style yields more conservative edits and less lexical novelty.",
            "null_or_negative_result": false,
            "experimental_details": "Statistics averaged over 8 datasets and 3 random seeds; plotted in Figure 9.",
            "uuid": "e9308.9",
            "source_info": {
                "paper_title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Let's think step by step",
            "rating": 2,
            "sanitized_title": "lets_think_step_by_step"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "APE: Automatic prompt engineer (Zhou et al., 2022)",
            "rating": 2,
            "sanitized_title": "ape_automatic_prompt_engineer_zhou_et_al_2022"
        },
        {
            "paper_title": "Automatic prompt optimization with \"gradient descent\" and beam search (Pryzant et al., 2023)",
            "rating": 2,
            "sanitized_title": "automatic_prompt_optimization_with_gradient_descent_and_beam_search_pryzant_et_al_2023"
        },
        {
            "paper_title": "Natural Instructions (Mishra et al., 2022b)",
            "rating": 1,
            "sanitized_title": "natural_instructions_mishra_et_al_2022b"
        },
        {
            "paper_title": "PromptSource (Bach et al., 2022)",
            "rating": 1,
            "sanitized_title": "promptsource_bach_et_al_2022"
        },
        {
            "paper_title": "Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance (Fu et al., 2023)",
            "rating": 1,
            "sanitized_title": "chainofthought_hub_a_continuous_effort_to_measure_large_language_models_reasoning_performance_fu_et_al_2023"
        }
    ],
    "cost": 0.018328,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>1 May 2025</p>
<p>Qingyan Guo 
Rui Wang 
Junliang Guo junliangguo@microsoft.com 
Bei Li 
Kaitao Song kaitaosong@microsoft.com 
Xu Tan 
Guoqing Liu guoqingliu@microsoft.com 
Jiang Bian 
Yujiu Yang yang.yujiu@sz.tsinghua.edu.cn 
Tsinghua University 
Microsoft Research </p>
<p>Northeastern University</p>
<p>Microsoft Research Asia</p>
<p>1 May 2025D575916D2E3B686032348E0BB55D1350arXiv:2309.08532v3[cs.CL]
Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort.To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EVOPROMPT, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence.To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs.This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs.Specifically, abstaining from any gradients or parameters, EVOPROMPT starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set.We optimize prompts for both closed-and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks.EVOPROMPT significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH).Furthermore, EVOPROMPT demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.Our code is available at https://github.com/beeevita/EvoPrompt.</p>
<p>INTRODUCTION</p>
<p>Large language models (LLMs) show remarkable performance on multiple natural language processing (NLP) tasks (Touvron et al., 2023;Ouyang et al., 2022).To adapt to downstream tasks, simply adding an instruction to the input text, also called discrete prompt, steers LLMs to carry out the desired task with negligible impact on computational cost (Liu et al., 2023).Such approach also eliminates the need for all the parameters and gradients in LLMs, making it suitable for LLMs with block-box APIs such as GPT-3 and GPT-4 (Brown et al., 2020;OpenAI, 2023).Despite the convenience, the performance of the LLMs towards a certain task is significantly influenced by the prompt (Liu et al., 2023;Zhu et al., 2023).Accordingly, the key challenge of this approach lies in the design of the prompt, which has emerged as a crucial technique known as prompt engineering (Zhou et al., 2022).Given the wide variation in prompts across language models and tasks, the prompt design typically requires substantial human effort and expertise with subjective and relatively limited guidelines (Mishra et al., 2022a;b;Liu et al., 2023;Zamfirescu-Pereira et al., 2023;Wang et al., 2023).</p>
<p>To alleviate human effort on discrete prompt design, previous approaches usually rely on access to the token probabilities from the output layer of LLMs, which may not always be accessible through APIs (Deng et al., 2022;Zhang et al., 2023a).Some recent works consider enumerating diverse prompts and selecting the best ones (Zhou et al., 2022;Jiang et al., 2020), or modifying current prompts to improve them (Guo et al., 2023;Prasad et al., 2022;Pryzant et al., 2023).Such approaches either emphasize exploring diverse prompts, which may lead to indecisiveness and wasted resources, or focus on exploiting upon the current identified good prompts, which may result in stagnation and confine the search to local optima.Several conventional derivative-free algorithms are well-designed and strike a good balance between exploration and exploitation (Conn et al., 2009;Rios &amp; Sahinidis, 2013).Among these, evolutionary algorithms (EAs) stand out as they are simple and efficient, as well as suitable for discrete prompt optimization (Storn &amp; Price, 1997;Brest et al., 2006;Zhang &amp; Sanderson, 2009;Vesterstrom &amp; Thomsen, 2004).Sequences of phrases in prompts can be regarded as gene sequences in typical EAs, making them compatible with the natural evolutionary process.</p>
<p>In this paper, we borrow the idea of EAs and propose a discrete prompt tuning framework, EVO-PROMPT.While evolutionary operators in EAs are typically designed for sequences, they tend to independently alter tokens to generate new candidate solutions.Unfortunately, this approach ignores the connections among tokens, which is crucial for maintaining coherence and readability in prompts.Taking advantage of LLMs' expertise in NLP and the exceptional optimization capabilities of EAs, we connect these two approaches, where LLMs generate new candidate prompts following evolutionary operators, and EAs guide the optimization process to retain the optimal prompts.Specifically, based on several initial prompts, we utilize LLMs to act as evolutionary operators to generate new prompt candidates, and the prompt with better performance on the development set is preserved.The above operations upon the updating population are iteratively applied to improve the quality.By elaborately designing the evolutionary operators and adjusting the update strategy, EVOPROMPT can be instantiated with various types of EAs.We optimize the prompts for two different LLMs (i.e., Alpaca (Taori et al., 2023), and GPT-3.5 (Brown et al., 2020)) on a diverse range of neural language understanding and generation tasks, as well as challenging BIG-Bench tasks, using a total of 31 datasets.EVOPROMPT consistently gets better prompts compared with both manually designed ones and previous automatic prompt generation methods.The main contributions of this paper include:</p>
<p>• We propose a novel framework for automatic discrete prompt optimization connecting LLMs and EAs, called EVOPROMPT, which enjoys the following advantages: 1) It does not require access to any parameters or gradients of LLMs; 2) It strikes a balance between exploration and exploitation leading to better results; 3) The generated prompts are human-readable.• Experiments conducted on 31 datasets demonstrate the effectiveness of EVOPROMPT compared with crafted prompts, as well as existing methods.We release the optimal prompts obtained by EVOPROMPT for these common tasks such as sentiment classification, topic classification, subjectivity classification, simplification, summarization and reasoning.• We demonstrate that LLMs are capable of implementing multiple types of EAs provided with appropriate instructions.We hope that our explorations will inspire further investigations on the combination of LLMs and conventional algorithms, paving the way for new and innovative applications of LLMs.</p>
<p>RELATED WORKS</p>
<p>Prompts in LLMs Prompting is an efficient method for employing LLMs in specialized tasks.However, the performance is heavily influenced by the choice of the prompt.Recently, automatic prompt optimization has obtained wide attention.Continuous prompt-based methods, which only tune parameters of some input tokens (Li &amp; Liang, 2021;Liu et al., 2021b;a;Zhang et al., 2021) attract lots of attention.In spite of their effective performance, two drawbacks of such paradigms can not be ignored: 1) The optimization of continuous prompts requires parameters of LLMs that are inaccessible for black-box APIs.2) Soft prompts often fall short of interpretability (Lester et al., 2021).Discrete prompts, simply adding several discrete tokens, such as "It was" (Schick &amp; Schütze, 2021), or task-specific descriptive instructions, such as "Classify the comment into positive or negative.",to the input text, can offer an interactive interface to humans with better interpretability and show promising performance in various NLP tasks (Liu et al., 2023).</p>
<p>Discrete Prompts Various approaches have been proposed for automatic discrete prompt searching and generation (Shin et al., 2020;Shi et al., 2022;Wallace et al., 2019;Deng et al., 2022;Zhang et al., 2023a), while these methods still rely on the gradients or the token probabilities from the output layer.More recently, considering the high variance of different prompts for downstream tasks, some works focus on exploration by enumerating and selecting the best prompt from a number of candidates, mainly augmented by re-sampling (Zhou et al., 2022;Jiang et al., 2020).Approaches based on prompt edit (Zhang et al., 2023a;Prasad et al., 2022) emphasize exploitation, which may potentially lead to local optima.Another approach collects the incorrectly predicted cases and analyzes the corresponding root cause to improve existing prompts (Pryzant et al., 2023;Guo et al., 2023), which also emphasizes exploitation.Additionally, such approaches are constrained to tasks with standard answers and cannot be directly applied to generation tasks.Our proposed EVOPROMPT empowered with evolutionary algorithms strikes a balance between exploration and exploitation without requiring any parameters or gradients.</p>
<p>LLMs and Optimization Algorithms LLMs demonstrate the potential to serve as black-box optimizers (Zheng et al., 2023); however, this black-box approach lacks explainability.Some works have revealed that LLMs have the capability to imitate specific operations in conventional algorithms.</p>
<p>For instance, LLMs can perform "Gradient Descent" in discrete space by collecting incorrectly predicted samples (Pryzant et al., 2023;Guo et al., 2023).Meanwhile, it has been demonstrated that LLMs can imitate the mutation (Lehman et al., 2022) or crossover (Meyerson et al., 2023) operator in the genetic algorithm (GA).Chen et al. (2023) further integrates LLMs and GA for neural architecture search, while Lanzi &amp; Loiacono (2023) introduce a similar approach to game design.Our work has taken a significant step forward by proposing a general framework that connects LLMs with evolutionary algorithms, which can be instantiated to a diverse range of evolutionary algorithms through customization of evolutionary and selection processes, thereby broadening its applicability and potential influence in the domain.We aspire this work to inspire broader applications of combining LLMs and conventional algorithms.</p>
<p>AUTOMATIC DISCRETE PROMPT OPTIMIZATION
← {s i = f D (p i )|i ∈ [1, N ]} 2: for t = 1 to T do 3:
Selection: select a certain number of prompts from current population as parent prompts p r1 , . . ., p r k ∼ P t−1</p>
<p>4:</p>
<p>Evolution: generate a new prompt based on the selected parent prompts by leveraging LLM to perform evolutionary operators p ′ i ← Evo(p r1 , . . ., p r k )</p>
<p>5:</p>
<p>Evaluation:
s ′ i ← f (p ′ i , D) 6: Update: P t ← {P t−1 , p ′ i } and S t ← {S t−1 , s ′ i }
based on the evaluation scores 7: end for 8: Return the best prompt, p * , among the final population
P T : p * ← argmax p∈P T f (p, D)
Current advanced LLMs are typically interacted via black-box APIs, while the gradients and parameters are inaccessible.Evolutionary algorithms (EAs) are derivative-free algorithms with exceptional accuracy and rapid convergence.Accordingly, we consider introducing EAs into discrete prompt optimization.However, to generate new candidate solutions, evolutionary operators typically edit the elements in current solutions independently, without considering the connections between them.This makes it challenging to apply evolutionary operators on discrete prompts, which require coherence and readability.To address this challenge, we propose a synergistic approach that connects the natural language processing expertise of LLMs with the optimization capabilities of EAs, called EVOPROMPT.Specifically, LLMs generate new candidate prompts based on evolutionary operators, while EAs guide the optimization process to find the optimal prompts.</p>
<p>Genetic Algorithm (GA) Implemented by LLMs</p>
<p>Query:</p>
<p>Please follow the instruction step-by-step to generate a better prompt.1. Cross over the following prompts and generate a new prompt: 2. Mutate the prompt generated in Step 1 and generate a final prompt bracketed with <prompt> and </prompt>.</p>
<p>Response:</p>
<p>Prompt 2: Assign a sentiment label to the given sentence from ['negative', 'positive'] and return only the label without any other text.</p>
<p>Prompt 1: Now you are a categorizer, your mission is to ascertain the sentiment of the provided text, either favorable or unfavourable. 1. Crossover Prompt: Your mission is to ascertain the sentiment of the provided text and assign a sentiment label from ['negative', 'positive'].</p>
<ol>
<li><prompt>Determine the sentiment of the given sentence and assign a label from ['negative', 'positive'].</prompt> Figure 1: GA process implemented by LLMs (Evo(•) in Algorithm 1).In Step 1, LLMs perform crossover on the given two prompts (words in orange and blue are inherited from Prompt 1 and Prompt 2, respectively).In Step 2, LLMs perform mutation on the prompt.</li>
</ol>
<p>In order to implement EVOPROMPT in practice, it is necessary to instantiate it with a specific algorithm of EAs.There are various types of EAs, and in this paper, we consider two widely used algorithms, including Genetic Algorithm (GA) (Holland, 1975) and Differential Evolution (DE) (Storn &amp; Price, 1997).GA is among the most highly regarded evolutionary algorithms (Holland, 1975;1992;Mitchell, 1998;Mirjalili et al., 2020) and DE has emerged as one of the most widely utilized algorithms for complex optimization challenges since its inception (Storn &amp; Price, 1997;Price, 2013;Das &amp; Suganthan, 2010;Pant et al., 2020).In the following, we will first outline the proposed EVOPROMPT, and then instantiate EVOPROMPT with GA and DE respectively.</p>
<p>FRAMEWORK OF EVOPROMPT</p>
<p>EAs typically start with an initial population of N solutions (prompts in our setting), then iteratively generate new solutions using evolutionary operators (e.g., mutation and crossover) on the current population and update it based on a fitness function.Following typical EAs, EVOPROMPT mainly contains three steps:</p>
<p>• Initial population: Contrary to most existing automatic prompt methods that neglect priori human knowledge, we apply available manual prompts as the initial population to leverage the wisdom of humans.Besides, EAs typically start from random solutions, resulting in a diverse population and avoiding being trapped in a local optimum.Accordingly, we also introduce some prompts generated by LLMs (Zhou et al., 2022) into the initial population.</p>
<p>• Evolution: In each iteration, EVOPROMPT uses LLMs as evolutionary operators to generate a new prompt based on several parent prompts selected from the current population.To accomplish this, we design steps of the mutation and crossover operators for each specific type of EAs, along with corresponding instructions to guide the LLMs in generating new prompts based on these steps.</p>
<p>• Update: We evaluate the generated candidate prompts on a development set and retain those with superior performance, similar to the survival of the fittest in nature.The specific updating strategy may vary depending on the type of EAs used.</p>
<p>The algorithm stops when the number of iterations reaches a predefined value.The details of EVOPROMPT are outlined in Algorithm 1.When instantiating EVOPROMPT with a specific algorithm of EAs, the evolutionary processes need to be adjusted, and the key challenge is to design the evolutionary operators on discrete prompts.</p>
<p>"tweet" -&gt; "review" "Categorize"-&gt; "Analyze" "Sentiment analysis" -&gt; "Sentiment identification"</p>
<p>Differential Evolution (DE) Algorithm Implemented by LLMs</p>
<p>− </p>
<p>Query:</p>
<p>Please follow the instruction step-by-step to generate a better prompt.</p>
<ol>
<li>Identify the different parts between the Prompt 1 and Prompt 2:</li>
</ol>
<p>New Prompt: In this task, you are given reviews about products.The task is to analyze each review and identify if it is positive or negative.</p>
<p>Final Prompt: <prompt>Here, you'll be given reviews about products and you'll need to analyze each review and identify if it is positive or negative.</prompt>Prompt 1: Categorize the tweet according to if it has a positive or negative sentiment.</p>
<p>Prompt 2: Carry out sentiment analysis for every sentence to decide if it is positive or negative.</p>
<p>Different parts: "tweet" vs "sentence" ''Categorize'' vs ''Carry out sentiment analysis''</p>
<p>Prompt 3: In this task, you are given sentences from product reviews.The task is to classify a sentence as positive or as negative.
𝐂𝐫𝐨𝐬𝐬𝐨𝐯𝐞𝐫 𝒂 + 𝑭(𝒃 − 𝒄) 𝑭(𝒃 − 𝒄)
2. Randomly mutate the different parts 3. Combine the different parts with Prompt 3, selectively replace it with the different parts in Step 2 and generate a new prompt.</p>
<p>Cross over the prompt in the</p>
<p>Step 3 with the following basic prompt and generate a final prompt bracketed with <prompt> and </prompt>:</p>
<p>Basic Prompt: Here, you'll be given sentences from reviews about products and you'll need to decide if it's a positive or a negative review.</p>
<p>Response:</p>
<p>1.</p>
<p>2.</p>
<p>3.</p>
<p>4.</p>
<p>INSTANTIATION WITH GENETIC ALGORITHM</p>
<p>Selection In GA, parent solutions are conventionally selected using the roulette wheel selection method, guided by their fitness values (Lipowski &amp; Lipowska, 2012).Analogously, we employ the roulette wheel selection to choose two parent prompts from the current population, based on their performance scores obtained on the development sets.Let s i denote the performance score of the i-th prompt within a population containing N prompts.The probability of selecting the i-th prompt as a parent can be expressed as
p i = s i / N j=1 s j .
Evolution Conforming to the GA framework, we generate a new candidate prompt via two steps: 1) Crossover is performed between the parent prompts to produce a new offspring prompt that inherits characteristics from both parents; 2) Mutation is applied to the offspring prompt, introducing random alterations to certain elements.We formalize this two-stage operation into algorithmic instructions for guiding LLMs to implement Evo(•) in Algorithm 1.The entire process is illustrated in Figure 1.</p>
<p>Update We employ a straightforward selection strategy for updating the population: at each iteration, EVOPROMPT produces N new prompts, which are merged with the existing population of N prompts.Subsequently, the top N prompts, based on their scores, are retained to form the updated population.Accordingly, the overall quality of the population undergoes continuous enhancement, culminating in the selection of the best one within the final population as the optimal prompt.</p>
<p>INSTANTIATION WITH DIFFERENTIAL EVOLUTION</p>
<p>Here, we begin with some preliminary knowledge of DE.Unlike GA, the solutions of DE are represented by numerical vectors.Each vector within the population is sequentially selected as a base vector, denoted as x, which subsequently undergoes mutation and crossover.During mutation, a mutated solution y is generated from a randomly selected solution a from the current population.The mutation is achieved by adding a scaled difference between two distinct, randomly selected solutions b and c to a, i.e., y = a + F (b − c), where F is the scaled parameter.</p>
<p>Crossover is to generate a trial solution x ′ = [x ′ 1 , ..., x ′ n ] by choosing each parameter in the vector from either the basic solution x or the mutated solution y.Then, x is replaced with x ′ if x ′ is better than x.Within step-by-step evolution, DE ends with a population of high quality.A modified version of DE uses the current best solution as vector a to exploit information from the best one.</p>
<p>Evolution The evolutionary process of DE can be decoupled into three steps: 1)
F (b − c); 2) y = a + F (b − c);
3) Crossover of x and y.In EVOPROMPT based on DE, we follow the three steps to design the evolutionary process, as well as the corresponding instructions for LLMs to generate a new prompt based on these steps as illustrated in Figure 2: • Inspired by the differential vector in DE, we consider mutating only the different parts of two randomly selected prompts in the current population (Step 1 and Step 2 in Figure 2).The prompts in the current population are considered the current best ones.Accordingly, the shared components of two prompts tend to have a positive impact on the performance, and thus need to be preserved.</p>
<p>• A variant of DE employs the current best vector during the mutation process, where a mutated vector is generated by adding the scale of the differential vector to the current best vector.Building upon this idea, we generate a mutated prompt by selectively replacing parts of the current best one with the mutated different parts for combination.(Step 3 in Figure 2).</p>
<p>• Crossover replaces certain components of a basic prompt (i.e., a candidate of the current population) with segments from the mutated prompt.This operation combines the features of two different prompts, potentially creating a new and improved solution (Step 4 in Figure 2).</p>
<p>Update Following the standard DE, each prompt p i in the current population is chosen as a basic prompt in turn to generate a corresponding new prompt p ′ i using the instruction in Figure 2.Then, the prompt with a higher score, either p i or p ′ i , is retained.Accordingly, the population size remains constant while the overall quality of the population is enhanced.</p>
<p>EXPERIMENTS</p>
<p>IMPLEMENTATION DETAILS AND BASELINES</p>
<p>With GPT-3.5 performing evolutionary operators, we optimize prompts using EVOPROMPT for the open-source Alpaca-7b (Taori et al., 2023) and closed-source GPT-3.5 (text-davinci-003) (Brown et al., 2020).We pick the prompt with the highest score on the development set and report its score on the test set.Results reported on Alpaca are averaged over 3 random seeds and the standard deviation is provided, while for GPT-3.5, we report results of one seed due to budget limitation.In our evaluation, we compare EVOPROMPT against three categories of prompt-based approaches, detailed as follows:</p>
<p>• • PromptSource (Bach et al., 2022) and Natural Instructions (NI) (Mishra et al., 2022b): These repositories aggregate human-composed prompts across a diverse range of datasets.</p>
<p>• APE (Zhou et al., 2022) and APO (Pryzant et al., 2023): APE employs an iterative Monte Carlo Search strategy, emphasizing on exploration.We reproduce it and initialize populations of equivalent sizes to that of EVOPROMPT.APO harnesses incorrectly predicted instances as "pseudo-gradient" to iteratively refine the original prompt, which emphasizes exploitation.We reproduce APO on binary classification tasks with the optimal manual prompt as the initial one.Table 2: Main results on SAMSum dataset (summarization task) for Alpaca-7b and GPT-3.5.</p>
<p>LANGUAGE UNDERSTANDING</p>
<p>Datasets and Settings</p>
<p>We first conduct experiments on language understanding tasks across 7 datasets to validate our methods, including sentiment classification (SST-2 (Socher et al., 2013), MR (PANG, 2005), CR (Hu &amp; Liu, 2004), SST-5 (Socher et al., 2013)), topic classification (AG's News (Zhang et al., 2015), TREC (Voorhees &amp; Tice, 2000)) and subjectivity classification (Subj (Pang &amp; Lee, 2004)).To constrain the output label space, we prepend the demonstration consisting of one example per class before the test case.See Appendix B for more details.</p>
<p>Main Results</p>
<p>Datasets and Settings</p>
<p>For language generation, we evaluate our EVOPROMPT on text summarization and simplification tasks.For summarization, we adopt SAMSum (Gliwa et al., 2019), a challenging and intricate dialogue summarization dataset, and report ROUGE-1/2/L scores on Alpaca-7b and GPT-3.5.For text simplification, which aims to simplify the source text while preserving its original meaning, we employ the ASSET dataset (Alva-Manchego et al., 2020), a benchmark known for its multiple reference translations.We apply SARI score (Xu et al., 2016) as the evaluation metric, an n-gram-based scoring system extensively utilized for text editing tasks.Additional details regarding our experimental setup can be found in Appendix B.</p>
<p>Main Results</p>
<p>The summarization and simplification results are presented in Tables 2 and 3. EVOPROMPT achieves a substantial performance gain over manually designed prompts, exhibiting an improvement of over 3 points in SARI scores across both Alpaca and GPT-3.5 API.Furthermore, EVOPROMPT consistently outperforms the APE approach across the evaluated scenarios, indicating that the generated prompts effectively harness the capabilities of LLMs for superior performance.Moreover, EVOPROMPT (DE) notably outperforms EVOPROMPT (GA) in the summarization task, while demonstrating comparable performance in the text simplification task.This suggests that the DE variant is particularly effective for more complex language generation tasks like summarization.</p>
<p>BIG BENCH HARD (BBH)</p>
<p>Datasets and Settings To validate our methods on diverse tasks, we apply BBH (Suzgun et al., 2022) including a suite of 23 challenging BIG-Bench tasks requiring multi-step reasoning.Since these tasks are challenging, we focus on optimizing the prompts for GPT-3.5.We sample a subset from the test set as the development set and report the normalized scores1 in comparison to the prompt "Let's think step by step."(Kojima et al., 2022) with 3-shot Chain-of-Thought demonstrations (following Fu et al. ( 2023)) on the test set.We use task IDs to simplify the denotation of each task and remove one since the accuracy already reaches 100% with the manual prompt.Please see Appendix C.2 and Table 17 for details, as well as further comparisons with previous works.</p>
<p>Main Results EVOPROMPT obtains better prompts for all 22 tasks (Figure 3).Specifically, EVO-PROMPT (DE) achieves up to a 25% improvement with an average of 3.5%, whereas EVOPROMPT (GA) reaches a peak improvement of 15% with a 2.5% average.Though for some tasks the GA counterpart outperforms the DE version, the performance gap remains relatively small (i.e., around 1%).Meanwhile, EVOPROMPT (DE) surpasses EVOPROMPT (GA) by over 2% on 6 tasks.Accordingly, the DE version is generally a good choice for these challenging tasks.For EVOPROMPT (GA), we apply the roulette wheel selection strategy by default to select parental prompts, contributing to the offspring.To further explore the effect of various selection strategies, we compare our approach with another two popular strategies, i.e., tournament (Wikipedia contributors, 2023) and random selection, as presented in Table 4.We observe that EVOPROMPT (GA) with roulette wheel achieves higher scores, showcasing the effectiveness of this selection method.</p>
<p>ANALYSIS</p>
<p>DESIGNS IN DE</p>
<p>For EVOPROMPT (DE), we delve into two key design considerations in adapting the evolutionary operators of DE to discrete prompts: 1) mutation on different parts, and 2) choosing the current top-performing prompt as "Prompt 3" in Figure 2. We assess the impact of these design choices on two datasets: Subj, an understanding dataset where EVOPROMPT (DE) outperforms EVOPROMPT (GA), and ASSET, a generation dataset where both variants demonstrate similar performance.</p>
<p>Mutation on Different Parts</p>
<p>To illustrate the benefits of mutating only the different parts, we replace the first two steps in Figure 2 with the instruction "Randomly mutate Prompt 1 and Prompt 2" to allow mutation on all contents in Prompts 1 and 2, denoted as "All" in Table 5.Meanwhile, the original design in EVOPROMPT, which mutates only the different parts, is denoted as "Diff".As shown in Table 5, the design of mutation on only the different parts consistently yields performance gains across two tasks.</p>
<p>Selection of Prompt 3 Applying one of the variants of the DE algorithm, in EVOPROMPT (DE), we pick the best prompt in the current population as Prompt 3 in Figure 2. We validate this design via the following settings: 1) Prompt 3 is randomly sampled from the current population, denoted as "random" in Table 5; 2) Eliminate the use of Prompt 3 by letting the Basic Prompt directly cross over with the mutated different parts (i.e., remove Step 3 in Figure 2), denoted as "eliminate" in Tabel 5. Table 5 clearly demonstrates the importance of introducing Prompt 3.Moreover, it is shown that choosing the best prompt as Prompt 3 is more effective than random sampling.Table 6: Ablations of the initial population on SST-5, where top-n, random-n, bottom-n denotes the top-performing, randomly selected, bottom-performing n prompts, and var-n denotes the number of generated n variations.</p>
<p>POPULATION INITIALIZATION</p>
<p>We investigate the effect of initial population quality on EVOPROMPT.We conduct pilot experiments to sort the prompts (designed manually or generated by GPT-3.5) according to their performance on the dev set.We then select bottom, random and top prompts along with their corresponding variations as initial prompts.These variations are generated using the resampling template designed in Zhou et al. (2022), shown in Figure 4 in the Appendix B.2, which is used to introduce randomness to the initialization.</p>
<p>Table 6 demonstrates that: 1) Crafted design of initial prompts is not essential, as randomly selecting prompts can achieve a similar performance to selecting the top-performing ones; 2) When selecting the top-performing prompts, introducing randomness by allowing GPT-3.5 to generate variations can lead to a slight improvement in overall performance; however, when randomly selecting prompts, there is no need to introduce additional randomness for EVOPROMPT (DE); 3) When using top-performing initial prompts, EVOPROMPT (GA) performs slightly better than EVOPROMPT (DE); however, when starting with bottom-performing initial prompts, EVOPROMPT (DE) outperforms EVOPROMPT (GA), which indicates that DE is a better choice when the available manual prompts are not of high quality.</p>
<p>CONCLUSIONS</p>
<p>We introduce EVOPROMPT to optimize discrete prompts, which connects LLMs with evolutionary algorithms.Extensive experiments on 31 datasets demonstrate the superiority of EVOPROMPT, yielding consistent performance gains over both manual instructions and existing methods.Besides, We validate that LLMs can serve as an effective, interpretable interface for implementing evolutionary algorithms like GA and DE.While this study focused on EAs, the extensibility of our approach opens avenues for applying LLMs to other conventional algorithms, such as particle swarm optimization (PSO) (Kennedy &amp; Eberhart, 1995), ant colony optimization (ACO) (Dorigo &amp; Gambardella, 1997) and more recent Quality-Diversity (QD) optimization algorithms.Our findings aim to inspire future research at the intersection of LLMs and traditional algorithms, encouraging innovative applications.</p>
<p>Algorithm 2 Discrete prompt optimization: EVOPROMPT (GA)</p>
<p>Require: Initial prompts P 0 = {p 1 , p 2 , . . ., p N }, size of population N , a dev set D 1: Initial fitness evaluation:
S 0 ← {s i = f (p i , D)|i ∈ [1, N ]} 2: for t = 1 to T do ▷ T : Number of iterations 3: for i = 1 to N do 4:
Selection based on fitness using roulette wheel: p r1 , p r2 ∼ P t−1</p>
<p>5:</p>
<p>Evolution: p ′ i ← GA(p r1 , p r2 ) (Refer to Figure 1) 6:</p>
<p>Evaluation:
s i ← f (p ′ i , D) 7:
end for 8:
S ′ t ← {s i |i ∈ [1, N ]}, P ′ t ← {p ′ i |i ∈ [1, N ]} 9:
Update score:
S t ← Top-N {S t−</p>
<p>A DETAILS OF ALGORITHM IMPLEMENTATION</p>
<p>We instantiate EVOPROMPT two representative evolutionary algorithms, GA and DE.Though both algorithms use consistent general selection processes, creating offspring, and updating, it is worth noting that the selection strategies, ways of mutation and crossover, and the updating strategies in these two algorithms are different.The specific algorithms for each of them are shown in Algorithm 2 and Algorithm 3.</p>
<p>B EXPERIMENTAL SETTINGS B.1 DATASETS</p>
<p>Table 7 shows the statistics of the text classification, simplification and summarization datasets.For Big-Bench Hard, We use serial numbers to denote 22 tasks, the descriptions are reported in Table 17.Note that for the task of "web of lies", the accuracy of the baseline is 100%, so here we have not included this task for prompt optimization.Additionally, both tasks of "logical deduction objects" and "tracking shuffled objects" have three sub-tasks.</p>
<p>B.2 TEMPLATES</p>
<p>Generate a variation of the following instruction while keep the semantic meaning.Input: <prompt> Output:</p>
<p>Template for Variation</p>
<p>Figure 4: Template used for resampling (Zhou et al., 2022).</p>
<p>Templates for Task Implementation For different models, we apply different templates shown in Table 8, 9 and 10, referring to the previous works (Iyer et al., 2022;Taori et al., 2023;Zhang et al., 2023b;Li et al., 2023;Fu et al., 2023).Please follow the instruction step-by-step to generate a better prompt. 1. Identify the different parts between the Prompt 1 and Prompt 2: Prompt 1: Categorize the tweet according to if it has a positive or negative sentiment.Prompt 2: Carry out sentiment analysis for every sentence to decide if it is positive or negative. 2. Randomly mutate the different parts 3. Combine the different parts with Prompt 3, selectively replace it with the different parts in Step 2 and generate a new prompt.Prompt 3: In this task, you are given sentences from product reviews.The task is to classify a sentence as positive or as negative.4. Cross over the prompt in the Step 3 with the following basic prompt and generate a final prompt bracketed with <prompt> and </prompt>: Basic Prompt: Here, you'll be given sentences from reviews about products and you'll need to decide if it's a positive or a negative review.</p>
<p>Differential Evolution (DE) Algorithm Implemented by LLMs</p>
<p>Response:</p>
<ol>
<li>Identifying the different parts between Prompt 1 and Prompt 2: 2. Randomly mutate the different parts: "tweet" -&gt; "review" "Categorize" -&gt; "Analyze" "Sentiment analysis" -&gt; "Sentiment identification'' New Prompt: In this task, you are given reviews about products.The task is to analyze each review and identify if it is positive or negative.</li>
</ol>
<p>Final Prompt: <prompt>Here, you'll be given reviews about products and you'll need to analyze each review and identify if it is positive or negative.</prompt>Basic Prompt: Here, you'll be given sentences from reviews about products and you'll need to decide if it's a positive or a negative review.</p>
<p>Prompt 1: Categorize the tweet according to if it has a positive or negative sentiment.</p>
<p>Prompt 2: Carry out sentiment analysis for every sentence to decide if it is positive or negative.</p>
<p>Different parts: "tweet" vs "sentence" ''Categorize'' vs ''Carry out sentiment analysis'' Prompt 3: In this task, you are given sentences from product reviews.The task is to classify a sentence as positive or as negative.Text Classification The population of prompts is initialized with widely used instructions in the previous works (Mishra et al., 2022b;Zhang et al., 2022).We paraphrase and rewrite them to initialize the population.The size of the development set is 200.We report the results on the full test set (the same as the previous related works (Deng et al., 2022;Zhang et al., 2023a)), as shown in Table 11.</p>
<p>Text Generation For the initial population, we collect instructions for summarization and simplification from Li et al.</p>
<p>C ADDITIONAL RESULTS</p>
<p>C.1 PARAMETERS IN EVOLUTIONARY ALGORITHMS</p>
<p>Effect of Population Size Intuitively, a trade-off exists between the performance and the overhead caused by the population size.We explore the performance of EVOPROMPT (DE) and EVOPROMPT (GA) respectively at varying population sizes from 4 to 12.The results are plotted in Figure 6.</p>
<p>For classification datasets, as the size increases, curves for DE and GA show an ascending trend.Furthermore, the increase in DE attributed to population diversity was greater than that in GA since DE focuses on different parts.Differences among prompts within populations bring about substantial mutations, leading DE to explore potential prompts since keeping common parts balances exploration and exploitation effectively.</p>
<p>For the relatively simple generation task (i.e., ASSET), a population size of 6 demonstrates a comparable performance to a population size of 10, though with a 2.5-fold increase in overhead.This suggests that for relatively simple tasks large populations are unnecessary, while for complex tasks (i.e., Subj), a larger population with diversity brings improvement.</p>
<p>Effect of Number of Iterations</p>
<p>To further explore the process of convergence, for SST-5, Subj and ASSET, we plot the best and average scores on the development set for EVOPROMPT for DE and GA over the whole population after each iterative step (Figure 7).Curves of best and average scores gradually converge with an increasing trend as evolution proceeds, indicating that the population's quality as a whole is steadily increasing as the evolution process.</p>
<p>C.3 COST ANALYSIS</p>
<p>Overhead mainly comes from prompt evaluation and generation.For evaluation, our overhead is N * |D| * T , where N is the size of the population, |D| is the size of the development set, and T is the number of iterations.These parameters differ from the task and can be found in Appendix B.3.For the cost from prompt generation, the cost mainly depends on the number of API results, T * N .So the total number of API requests is N * T * (1 + |D|), the same as APE.Moreover, given that the API of LLMs is typically billed based on the number of tokens used, we also estimate the total number of tokens used in the API requests during the prompt optimization process, as shown in Table 13.All the scores reported are over the test set on one random seed.We analyze the overhead mainly from two aspects: 1) the performance of our methods compared with APE under the same number of iterations; 2) the performance until convergence measured by the average score on the dev set.</p>
<p>We can observe that with the same number of iterations, both GA and DE outperform APE significantly while introducing only a slight overhead in terms of the number of tokens.The convergence rates of APE and GA are similar while DE is slightly slower, but it delivers better performance.This implies the relatively high ceiling of EVOPROMPT.</p>
<p>C.4 ANALYSIS OF PROMPT</p>
<p>Diversity Analysis We further investigate the diversity of prompts generated by GA and DE after each iterative step respectively.We mainly plot the average prompt length, variance and number of new words mutated after each step, as shown in Figure 9.It can be observed that EVOPROMPT (DE) generates longer prompts with higher variances than EVOPROMPT (GA), which implies that DE prefers exploration for diversity.In the latter iterations, DE mutates more new words than GA, and thus shows better potential to escape from the local optimum.</p>
<p>Optimal Prompts We release the optimal prompts generated by EVOPROMPT for understanding (Table 14), text simplification (Table 16), summarization (Table 15) and BBH tasks (Table 17, 18) .</p>
<p>D FUTURE WORKS</p>
<p>There are several promising directions for future investigation:</p>
<p>• Based on our framework, more applications can be explored, including game levels generation, text-to-images generation, non-trivial NP-hard problems (e.g.traveling salesman problem), etc.</p>
<p>• There exist many variants of DE and we give priority to the most canonical and classical ones for current exploration.In future work, it will be interesting to consider more advanced DEvariants (Das et al., 2016;Das &amp; Suganthan, 2010).For example, some recent DE-variants have been investigating adaptive control parameters.The main challenge in applying these variants to</p>
<p>77.53</p>
<p>Table 18: Instructions with the best performance on GPT3.5 generated by EVOPROMPT (either DE or GA) on BBH datasets.Duplicate IDs are due to the tasks with several sub-tasks.</p>
<p>Figure 2 :
2
Figure 2: DE process implemented by LLMs (Evo(•) in Algorithm 1).In Step 1, LLMs find the different parts (words in ■ and ■) between Prompt 1 and Prompt 2 (b − c in typical DE).In Step 2, LLMs perform mutation (words in ■ ) on them (imitation of F(b − c)).Next, LLMs incorporate the current best prompt as Prompt 3 with the mutated results in Step 2, to generate a new prompt (counterpart of a + F(b − c) in DE).Finally, LLMs perform crossover upon the current basic prompt p i and the generated prompt in Step 3. See Figure 5 in Appendix B.2 for the complete response.</p>
<p>Manual Instructions (MI): These serve as task-specific guidelines and are crafted based on established works, specifically referenced from Zhang et al. (2023b) for language understanding, Sanh et al. (2021) for summarization, and Zhang et al. (2023c) for text simplification.</p>
<p>Figure 3 :
3
Figure 3: Normalized scores on BBH tasks for EVOPROMPT (GA) and EVOPROMPT (DE).</p>
<p>over the prompt in Step 3 with the following basic prompt and generate a final prompt bracketed with <prompt> and </prompt>: 3. Combine the different parts with Prompt 3, selectively replace it with the different parts in Step 2 and generate a new prompt: Query:</p>
<p>Figure 5 :
5
Figure 5: DE algorithm implemented by LLMs for discrete prompt optimization with complete response (Evo(•) in Algorithm 1).In Step 1, LLMs find the different parts (words in ■ and ■) between Prompt 1 and Prompt 2 (b − c in typical DE).In Step 2, LLMs perform mutation (words in ■ ) on them (imitation of F(b − c)).Next, LLMs incorporate the current best prompt as Prompt 3 with the mutated results in Step 2, to generate a new prompt (counterpart of a + F(b − c) in DE).Finally, LLMs perform crossover upon the current basic prompt p i and the generated prompt in Step 3.</p>
<p>Figure 6 :
6
Figure 6: Effect of population size on SST-5 (left), Subj (middle), and ASSET (right).All the results are averaged over 3 random seeds.</p>
<p>(2023);Sanh et al. (2021);Zhang et al. (2023c) and augment them to the expected size (10 in our setting), either written manually or generated by GPT-3.5.</p>
<p>Figure 7 :Figure 8 :
78
Figure 7: The best and average scores of each iteration on SST-5 (left), Subj (middle), and ASSET (right) development set on Alpaca-7b.All the results are averaged over 3 random seeds.</p>
<p>Figure 9 :
9
Figure 9: Statistics about the prompt length, including average values over the whole population (a), variance over the prompt length (b), and number of new words evolved after each step (c).Note that all the values are averaged over 8 datasets, including 7 understanding datasets and one simplification dataset, and 3 random seeds.</p>
<p>determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.Progress slowly and carefully through this mathematical reasoning challenge one step at a time.determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.Using a logical, step-by-step approach, work through this task to find the correct answer.determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.Examine this issue logically and in detail, step-bystep, analyzing each part of the problem one at analyze each part of the problem in a step by step way to ensure the right answer is obtained.</p>
<p>Table 1 :
1
Main results on language understanding (accuracy) on Alpaca-7b.
MethodSST-2CRMRSST-5AG's News TRECSubjAvg.MI (Zhang et al., 2023b)93.6891.4088.7542.9070.6350.6049.7571.07NI (Mishra et al., 2022c)92.8690.9089.6048.6448.8955.0052.5568.21PromptSource (Bach et al., 2022) 93.03---45.4336.20--APE (Zhou et al., 2022)93.45(0.14) 91.13(0.45) 89.98(0.29) 46.32(0.49) 71.76(2.81)58.73(1.37) 64.18(0.59) 73.80APO (Pryzant et al., 2023)93.87(0.39) 91.20(0.04) 89.85(0.35) ---70.55(1.02)-EVOPROMPT (GA)95.13(0.21) 91.27(0.06) 90.07(0.25) 49.91(0.61) 72.81(0.61)64.00(0.16) 70.55(2.58) 76.25EVOPROMPT (DE)94.75(0.21) 91.40(0.04) 90.22(0.09) 49.89(1.73) 73.82(0.35)63.73(1.54) 75.55(2.26) 77.05MethodAlpacaGPT-3.5ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-LMI (Sanh et al., 2021)35.9211.1631.6743.9517.1139.09APE (Zhou et al., 2022)35.44(0.79) 10.60(0.38) 31.80(0.50)43.4316.7238.25EVOPROMPT (GA)38.46(1.45) 13.36(0.75) 34.20(1.40)45.2218.5241.06EVOPROMPT (DE)39.46(0.51) 13.93(0.33) 35.49(0.56)46.4919.4941.96</p>
<p>Table 1
14.3 LANGUAGE GENERATIONMethodAlpacaGPT-3.5MI (Zhang et al., 2023c)43.0343.80APE (Zhou et al., 2022)45.90(0.09) 46.71EVOPROMPT (GA)46.43(0.19) 47.36EVOPROMPT (DE)46.21(0.27) 47.40Table 3: Main results (SARI) on simplification(ASSET) for Alpaca-7b and GPT3.5.
, shows that: 1) Compared with previous works on prompt generation and human written instructions, EVOPROMPT based on both GA and DE delivers significantly better results.2) EVOPROMPT (GA) is slightly better than EVOPROMPT (DE) on sentiment classification datasets.When it comes to topic classification datasets, EVOPROMPT (DE) performs better.Notably, on the subjectivity classification task (Subj), EVOPROMPT (DE) exhibits a substantial improvement over its GA counterpart, achieving a 5% accuracy advantage.This may be contributed by the exceptional ability of DE to evade local optima when the initial prompts are not of high quality.</p>
<p>Table 4 :
4
Designs in EVOPROMPT (GA).</p>
<p>Table 5 :
5
Designs in EVOPROMPT (DE).</p>
<p>Return the best prompt, p * , among the final population P T : p * ← argmax p∈P T f (p, D) Algorithm 3 Discrete prompt optimization: EVOPROMPT (DE) Require: Initial prompts P 0 = {p 1 , p 2 , . . ., p N }, size of population N , a dev set D DE(p i , p r1 , p r2 , p best ) where p best is the current best prompt.(Refer to Figure 2)
10:1 , S ′ t } t } using S t−1 , S ′ Update: P t ← Top-N {P t−1 , P ′ t ,11: end for12: 5:Selection: p  *  i = arg max i } p∈{pi,p ′f (p, D)▷ Keep the better one in the population6:end for7:
1: for t = 1 to T do ▷ T : Number of iterations 2:for p i in P t−1 do 3:Sample donors: p r1 , p r2 ∼ P t−1 , r1 ̸ = r2 ̸ = i 4: Evolution: p ′ i ← Update:P t ← {p * i |i ∈ [1, N ]} 8:end for 9: Return the best prompt, p * , among the final population P T : p * ← argmax p∈P T f (p, D)</p>
<p>Table 14 :
14
(Bach et al., 2022)2b)ase perform Sentiment Classification task.Given the sentence, assign a sentiment label from['negative',  'positive'].Return label only without any other text.93.68NaturalInstructionIn this task, you are given sentences from movie reviews.The task is to classify a sentence as "great" if the sentiment of the sentence is positive or as "terrible" if the sentiment of the sentence is negative.Natural Instruction In this task, you are given sentences from movie reviews.The task is to classify a sentence as "great" if the sentiment of the sentence is positive or as "terrible" if the sentiment of the sentence is negative.Natural Instruction In this task, you are given sentences from movie reviews.The task is to classify a sentence as "great" if the sentiment of the sentence is positive or as "terrible" if the sentiment of the sentence is negative.EVOPROMPTHave your friend evaluate the movie they had just seen and provide a summary opinion (e.g.terrible, bad, okay, good, or great) to determine the sentiment of the movie review.Natural Instruction In this task, you are given a news article.Your task is to classify the article to one out of the four topics "World", "Sports", "Business", "Tech" if the article"s main topic is relevant to the world, sports, business, and technology, correspondingly.If you are not sure about the topic, choose the closest option.Assess the entire concept of the news story and choose from the World, Sports, Business or Tech categories to categorize it into the correct category.Natural Instruction In this task, you are given sentences from reviews.The task is to classify a sentence as "subjective" if the opinion of the sentence is subjective or as "objective" if the opinion of the sentence is objective.Manual Instructions (followingZhang et al. (2023b)andZhang et al. (2023c)), Natural Instructions(Mishra et al., 2022b), PromptSource(Bach et al., 2022)as baselines and instructions with best performance on Alpaca-7b generated by EVOPROMPT (either DE or GA) on classification datasets.Carefully examine the text or listen to the conversation to identify the key ideas, comprehend the main idea, and summarize the critical facts and ideas in the concise language without any unnecessary details or duplication.39.86/14.24/36.09GPTReduce the core by reading or listening carefully to identify the main ideas and key points, so readers can comprehend the important concepts and essential information.46.49/19.49/41.96
DatasetMethodContent
TRECManual Instruction Please perform Question Classification task.Given the question, assign a label from ['Description', 'Entity', 'Expression', 'Human', 'Location', 'Number'].Return label only without any other text.50.60NaturalInstruction You are given a question.You need to detect which category better describes the question.Answer with "Description", "Entity", "Expression", "Human", "Location", and "Number".</p>
<p>Table 15 :
15
Sanh et al. (2021)s (followingSanh et al. (2021)as the baseline and instructions with best performance on Alpaca-7b and GPT3.5 generated by EVOPROMPT (either DE or GA) on SAMSum.Let's start by taking a rational and organized approach, breaking it down into smaller parts and thinking it through logically, while being realistic and handling it carefully and methodically to ensure the right solution.Dyck-n word.Let's be realistic and solve this challenge carefully and slowly, taking it slow to complete it correctly, so we can be realistic and cautiously reach the goal.
Task ID TaskDescriptionPromptScore06causal_judgementAnswer questions about causal attri-At first, let's handle things cautiously and resolve65.78bution.this by examining every detail and dealing withone problem at a time.07date_understandingInfer the date from context.Be realistic and practical like a detective, and use85.60evidence to solve the problem in a logical, step-by-step approach.08ruin_namesSelect the humorous edit that 'ru-Break down a math task into smaller sections and69.60ins' the input movie or musical artistsolve each one.name.09word_sortingSort a list of words.Analyze each part of the problem logically to solve56.40it like a detective.10geometric_shapesName geometric shapes from theirWe'll methodically work through this problem to-64.00SVG paths.gether.11movie_recommendationRecommend movies similar to theBefore exploring the answer,86.00given list of movies.12salient_translation_error_detectionDetect the type of error in an En-Break down the problem into individual steps in62.80glish translation of a German sourceorder to solve it.sentence.13formal_fallaciesDistinguish deductively valid argu-Let's be realistic and evaluate the situation system-56.00ments from formal fallacies.atically, tackling it gradually.14penguins_in_a_tableAnswer questions about a table of84.25penguins and their attributes.15dyck_languagesCorrectly close a 44.4016multistep_arithmetic_twoSolve multi-step arithmetic prob-Before we dive into the answer,51.60lems.17navigateGiven a series of navigation instruc-Let's logically work together to systematically94.20tions, determine whether one wouldsolve this math problem one step at a time in uni-end up back at the starting point.son.18reasoning_about_colored_objectsAnswer extremely simple questionsUsing a detective's mindset, break down each ele-88.00about the colors of objects on a sur-ment of this mathematical reasoning challenge oneface.step at a time and reason like a detective to uncoverthe solution.19boolean_expressionsEvaluate the result of a randomLet's gradually unravel this mathematical chal-Boolean expression.lenge by methodically addressing it by examiningeach element and investigating each factor.
The accuracy difference between a given prompt and the baseline prompt "Let's think step by step." A score of 0 corresponds to the normalized score of the baseline prompt.
ACKNOWLEDGEMENTSThis work was partly supported by the National Key Research and Development Program of China (No. 2020YFB1708200), and the Shenzhen Science and Technology Program (JCYJ20220818101001004).Template for Prompt Generation We apply the resampling template, shown in Figure4, to generate variations of manual initial prompts.For our EVOPROMPT, the complete DE algorithm implemented by LLMs is shown in Figure5.For both DE and GA, we prepend a one-shot example of the algorithm execution, guiding LLMs to operate precisely.============================== INSTRUCTIONAL PROMPTS ==============================Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.Below is an instruction that describes a task, paired with an input that provides further context.Write a response that appropriately completes the request.B.3 HYPER PARAMETERSThe parameters for the experiments are shown in Table11.For evolutionary algorithms implemented by GPT-3.5, following previous work(Shi et al., 2024), we use Top-p decoding (temperature=0.5,P = 0.95).For the task implementation, we use greedy decoding and the default temperature for Alpaca.For the generation tasks implemented by GPT-3.5, the temperature is 0.0.APE(Zhou et al., 2022)optimizes the Chain-of-Thought (CoT) prompt for reasoning tasks on InstructGPT.Considering that both InstructGPT and GPT-3.5 belong to the GPT family and we may observe similar trends, we evaluate the CoT prompt proposed by APE, "Let's work this out in a step by step way to be sure we have the right answer.", on reasoning tasks and plot the 3-shot performance in Figure8.For simplicity, we use the same initial population for all the 22 BBH tasks without priori knowledge of each task.In future works, by incorporating task-specific prompts, either manually designed or generated by LLMs, we may further enhance the performance.First, let us ponder and start off by taking our time, going step by step, and using our logic to approach this before we dive into the answer.71.2005 logical_deduction_three_objects A logical deduction task which requires deducing the order of a sequence of objects.Let's approach it cautiously, examining it thoroughly and methodically, and then approach it incrementally towards a resolution.05 logical_deduction_five_objectsA logical deduction task which requires deducing the order of a sequence of objects.Split the problem into steps and thoughtfully progress through them to find the answer after the proof.65.20 05 logical_deduction_seven_objects A logical deduction task which requires deducing the order of a sequence of objects.Let's take a step-by-step approach to systematically dissect this math task.54.40Table17: Instructions with the best performance on GPT3.5 generated by EVOPROMPT (either DE or GA) on BBH datasets.Duplicate IDs are due to the tasks with several sub-tasks.prompt optimization within the discrete language space lies in assessing the capacity of LLMs to adapt to these continuous control parameters.• We hope our study can inspire further exploration of the connection between LLMs and other traditional algorithms, extending beyond EAs.The main challenge is adapting the specific elements of traditional algorithms to work within LLMs.For example, these elements may include direction of motion, velocity in partial swarm optimization (PSO)(Kennedy &amp; Eberhart, 1995), the path in ant colony optimization algorithms (APO)(Dorigo &amp; Gambardella, 1997), and characteristic in MAP-Elites(Mouret &amp; Clune, 2015).
Asset: A dataset for tuning and evaluation of sentence simplification models with multiple rewriting transformations. Fernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton, Benoît Sagot, Lucia Specia, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Promptsource: An integrated development environment and repository for natural language prompts. Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Abheesht Nayak, Taewoon Sharma, Saiful Kim, Thibault Bari, Févry, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations2022</p>
<p>Self-adapting control parameters in differential evolution: A comparative study on numerical benchmark problems. Janez Brest, Sao Greiner, Borko Boskovic, Marjan Mernik, Viljem Zumer, IEEE transactions on evolutionary computation. 1062006</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Evoprompting: Language models for code-level neural architecture search. Angelica Chen, David M Dohan, David R So, arXiv:2302.148382023arXiv preprint</p>
<p>Introduction to derivative-free optimization. Katya Andrew R Conn, Luis N Scheinberg, Vicente, 2009SIAM</p>
<p>Differential evolution: A survey of the state-of-the-art. Swagatam Das, Ponnuthurai Nagaratnam Suganthan, IEEE transactions on evolutionary computation. 1512010</p>
<p>Recent advances in differential evolution-an updated survey. Swarm and evolutionary computation. Swagatam Das, Sankha Subhra Mullick, Ponnuthurai N Suganthan, 201627</p>
<p>Rlprompt: Optimizing discrete text prompts with reinforcement learning. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, Zhiting Hu, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Ant colony system: a cooperative learning approach to the traveling salesman problem. Marco Dorigo, Maria Luca, Gambardella, IEEE Transactions on evolutionary computation. 111997</p>
<p>Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, Tushar Khot, arXiv:2305.173062023arXiv preprint</p>
<p>Samsum corpus: A humanannotated dialogue dataset for abstractive summarization. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, Aleksander Wawer, arXiv:1911.122372019arXiv preprint</p>
<p>Yiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu, Dongyan Zhao, Nan Duan, arXiv:2304.10464Learning to program with natural language. 2023arXiv preprint</p>
<p>Adaptation in Natural and Artificial Systems. John H Holland, 1975. ISBN 0262581116University of Michigan PressAnn Arbor</p>
<p>Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence. H John, Holland, 1992MIT press</p>
<p>Mining and summarizing customer reviews. Minqing Hu, Bing Liu, KDD. 2004</p>
<p>Opt-iml: Scaling language model instruction meta learning through the lens of generalization. Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, arXiv:2212.12017Jiang, Frank F Xu, Jun Araki, and Graham Neubig2022. 2020Transactions of the Association for Computational Linguistics8arXiv preprintHow can we know what language models know?</p>
<p>Particle swarm optimization. James Kennedy, Russell Eberhart, Proceedings of ICNN'95-international conference on neural networks. ICNN'95-international conference on neural networksIEEE19954</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Chatgpt and other large language models as evolutionary engines for online interactive collaborative game design. Pier Luca, Lanzi , Daniele Loiacono, arXiv:2303.021552023arXiv preprint</p>
<p>Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, Kenneth O Stanley, arXiv:2206.08896Evolution through large models. 2022arXiv preprint</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, EMNLP. 2021</p>
<p>Deliberate then generate: Enhanced prompting framework for text generation. Bei Li, Rui Wang, Junliang Guo, Kaitao Song, Xu Tan, Hany Hassan, Arul Menezes, Tong Xiao, Jiang Bian, Jingbo Zhu, arXiv:2305.198352023arXiv preprint</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Roulette-wheel selection via stochastic acceptance. Adam Lipowski, Dorota Lipowska, Physica A: Statistical Mechanics and its Applications. 39162012</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang, arXiv:2110.076022021aarXiv preprint</p>
<p>Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang, arXiv:2103.10385Gpt understands, too. 2021barXiv preprint</p>
<p>Language model crossover: Variation through few-shot prompting. Elliot Meyerson, Mark J Nelson, Herbie Bradley, Arash Moradi, Amy K Hoover, Joel Lehman, arXiv:2302.121702023arXiv preprint</p>
<p>Genetic algorithm: Theory, literature review, and application in image reconstruction. Seyedali Mirjalili, Jin Song Dong, Ali Safa Sadiq, Hossam Faris, Nature-Inspired Optimizers: Theories, Literature Reviews and Applications. 2020</p>
<p>Reframing instructional prompts to gptk's language. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, Hannaneh Hajishirzi, Findings of the Association for Computational Linguistics: ACL 2022. 2022a</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics2022b1</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, ACL. 2022c</p>
<p>An introduction to genetic algorithms. Melanie Mitchell, 1998MIT press</p>
<p>Illuminating search spaces by mapping elites. Jean-Baptiste Mouret, Jeff Clune, arXiv:1504.049092015arXiv preprint</p>
<p>arXiv:2303.08774OpenAI. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. Pang Bo, ACL. 2005</p>
<p>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. Bo Pang, Lillian Lee, Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04). the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)2004</p>
<p>Differential evolution: A review of more than two decades of research. Millie Pant, Hira Zaheer, Laura Garcia-Hernandez, Ajith Abraham, Engineering Applications of Artificial Intelligence. 901034792020</p>
<p>Grips: Gradient-free, edit-based instruction search for prompting large language models. Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal, arXiv:2203.072812022arXiv preprint</p>
<p>Differential evolution. Price Kenneth, Handbook of optimization: From classical to modern approach. Springer2013</p>
<p>Automatic prompt optimization with" gradient descent" and beam search. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng, arXiv:2305.034952023arXiv preprint</p>
<p>Is chatgpt a general-purpose natural language processing task solver?. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang, arXiv:2302.064762023arXiv preprint</p>
<p>Derivative-free optimization: a review of algorithms and comparison of software implementations. Luis Miguel, Rios , Nikolaos V Sahinidis, Journal of Global Optimization. 562013</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, arXiv:2110.082072021arXiv preprint</p>
<p>Exploiting cloze-questions for few-shot text classification and natural language inference. Timo Schick, Hinrich Schütze, Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European ChapterMain Volume2021</p>
<p>A thorough examination of decoding methods in the era of llms. Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, Wai Lam, arXiv:2402.069252024arXiv preprint</p>
<p>Toward human readable prompt tuning: Kubrick's the shining is a good movie. Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, Luke Zettlemoyer, arXiv:2212.105392022arXiv preprint</p>
<p>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing2020</p>
<p>Recursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, Christopher Potts, EMNLP. 2013</p>
<p>Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. Rainer Storn, Kenneth Price, Journal of global optimization. 111997</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Stanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>A comparative study of differential evolution, particle swarm optimization, and evolutionary algorithms on numerical benchmark problems. Jakob Vesterstrom, Rene Thomsen, Proceedings of the 2004 congress on evolutionary computation. the 2004 congress on evolutionary computationIEEE20042IEEE Cat</p>
<p>Building a question answering test collection. M Ellen, Dawn M Voorhees, Tice, Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval. the 23rd annual international ACM SIGIR conference on Research and development in information retrieval2000</p>
<p>Universal adversarial triggers for attacking and analyzing nlp. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Hint-enhanced in-context learning wakes large language models up for knowledge-intensive tasks. Yifan Wang, Qingyan Guo, Xinzhe Ni, Chufan Shi, Lemao Liu, Haiyun Jiang, Yujiu Yang, arXiv:2311.019492023arXiv preprint</p>
<p>Wikipedia contributors. Tournament selection -Wikipedia, the free encyclopedia. 2023. September-202326</p>
<p>Optimizing statistical machine translation for text simplification. Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, Chris Callison-Burch, Transactions of the Association for Computational Linguistics. 42016</p>
<p>Why johnny can't prompt: how non-ai experts try (and fail) to design llm prompts. Richmond Y Jd Zamfirescu-Pereira, Bjoern Wong, Qian Hartmann, Yang, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>Jade: Adaptive differential evolution with optional external archive. Jingqiao Zhang, Arthur C Sanderson, 10.1109/TEVC.2009.2014613IEEE Transactions on Evolutionary Computation. 1352009</p>
<p>Differentiable prompt makes pre-trained language models better few-shot learners. Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, Huajun Chen, International Conference on Learning Representations. 2021</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint</p>
<p>Tempera: Test-time prompt editing via reinforcement learning. Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, Joseph E Gonzalez, The Eleventh International Conference on Learning Representations. 2023a</p>
<p>Sentiment analysis in the era of large language models: A reality check. Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, Lidong Bing, arXiv:2305.150052023barXiv preprint</p>
<p>Character-level convolutional networks for text classification. Xiang Zhang, Junbo Zhao, Yann Lecun, NeurIPS. 282015</p>
<p>Multi-task instruction tuning of llama for specific scenarios: A preliminary study on writing assistance. Yue Zhang, Leyang Cui, Deng Cai, Xinting Huang, Tao Fang, Wei Bi, arXiv:2305.132252023carXiv preprint</p>
<p>Can gpt-4 perform neural architecture search?. Mingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, Samuel Albanie, arXiv:2304.109702023arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, arXiv:2306.04528Towards evaluating the robustness of large language models on adversarial prompts. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
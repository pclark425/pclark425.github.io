<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6996 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6996</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6996</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-263864711</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclweb.org/anthology/2021.naacl-main.278.pdf" target="_blank">Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training</a></p>
                <p><strong>Paper Abstract:</strong> Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6996.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6996.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TEKGEN triple-concat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TEKGEN triple concatenation representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple sequential text encoding of a set of RDF-style triples where triples for a single subject are concatenated into a token sequence of the form: subject relation_1 object_1 , relation_2 object_2 , ... and fed as input to a text-to-text model (T5) to generate natural-language verbalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>triple-concatenation (subject-centric)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes graph information by listing triples as flat text tokens in the order: subject relation_1 object_1 , relation_2 object_2 , ...; relation and object are rendered as canonical KG labels/aliases and separated by punctuation/commas. If the subject alias does not appear in a target training sentence, a pronoun replacement heuristic is applied during alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>edge-list ordering: concatenate triples (subject, relation, object) in a single flat sequence (subject-first) — ordering within the set determined by the entity-subgraph construction (see entity-subgraph entry); no graph-structure markers beyond relation/object tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Wikidata→Wikipedia aligned corpus (KG-Text alignment); WebNLG (used for finetuning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation / KG verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained text-to-text transformer (T5-large) sequentially fine-tuned first on the noisy Wikidata↔Wikipedia aligned corpus (5k steps) to increase coverage and then on the clean WebNLG 2017 dataset (500 steps) to reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human evaluation (semantics and fluency, 1–5 scale); semantic-quality classifier correlation (BERT-based); qualitative comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provided a straightforward input format enabling use of pretrained text-to-text models for KG verbalization; improved coverage of entities and relations when T5 was first fine-tuned on the aligned KG-text corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lossy with respect to explicit graph structure (no edge nesting or explicit graph delimiters beyond token order); ordering of triples is not canonical (entity-subgraph algorithm injects randomness for first triple), which can affect determinism. Models trained only on the noisy aligned corpus hallucinate (e.g., infer missing relations); requires additional clean finetuning to reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared favorably when combined with entity-subgraph grouping and WebNLG finetuning vs. using single triple inputs or finetuning only on WebNLG; sequential finetuning (aligned corpus → WebNLG) reduced hallucination relative to only using the aligned (noisy) data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6996.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6996.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entity-subgraph serialization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Subject-centric entity-subgraph serialization (relation co-occurrence based grouping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to create small subgraphs (max depth 5) of triples all sharing the same subject, selected by relation co-occurrence statistics, and serialize that subgraph as a concatenated triple sequence for generation; intended to produce more coherent multi-triple verbalizations and reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>entity-subgraph serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Constructs a small ordered set of triples (up to 5) sharing the same subject by: selecting a random initial triple for a subject, then greedily adding triples whose relations have the highest co-occurrence counts with the previously chosen relation (counts derived from KG↔text alignment). The final subgraph is serialized by concatenating the triples in the chosen order (subject relation object, ...).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, hierarchical-by-subgraph, token-based, lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>relation co-occurrence-based greedy traversal within a subject's neighborhood (not graph-traversal like DFS/BFS); ordering determined by greedy co-occurrence ranking with a random first choice.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>KELM Corpus generation from Wikidata (entity subgraphs derived from aligned data)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (multi-triple verbalization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large (TEKGEN) used at inference with entity-subgraph inputs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same T5-large model as above; at inference the model receives serialized entity-subgraphs (multi-triple) instead of single triples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human evaluation: fluency and semantics rated 1–5; comparison of mean and standard deviation across systems; qualitative examples</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Using entity-subgraphs at inference produced generated sentences with higher mean semantic and fluency ratings and lower standard deviation compared to single-triple inference when paired with the TEKGEN model; entity-subgraphs reduced hallucination compared to single-triple inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Subgraph construction is heuristic (relation co-occurrence) and subject-centric — it does not capture multi-hop relations across different subjects (no transitive closure or multi-hop inference). The first-triple randomness prevents full determinism. Quality depends on the co-occurrence statistics derived from the (noisy) aligned corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperformed single-triple inference using the same model (T5) in human evaluation; reduced variance in fluency. When entity-subgraphs were not used, T5 tended to hallucinate more (especially when only trained on noisy aligned corpus).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6996.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6996.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KELM sentences</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KELM verbalized sentences (natural-language sentences derived from Wikidata)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The final natural-language sentences produced by TEKGEN from entity-subgraphs (the KELM Corpus) — a large synthetic corpus of verbalized Wikidata facts intended for augmenting LM pretraining/retrieval corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>natural-language verbalization (KELM sentence)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each entity-subgraph is mapped to one generated natural-language sentence (or short sentence sequence) by the T5-based TEKGEN system; the sentences are human-readable, paraphrase KG relation labels into natural expressions, and are grouped by subject to form documents for retrieval LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (natural language), loss-aware (intended readable output), token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>model-based generation from serialized triples (triple-concatenation input) with top-5 sampling (temperature 0.5) and semantic-quality filtering (BERT-based); post-processing groups sentences into subject documents.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>KELM Corpus (∼18M generated sentences from ∼45M Wikidata triples); KELM documents (∼5.7M subject-grouped documents)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KG verbalization for pre-training data generation; retrieval-corpus augmentation for retrieval-augmented LM pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large (generator) + BERT-base (semantic filter); used to augment REALM pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>TEKGEN: a two-step sequentially-finetuned T5-large generator (first on noisy aligned Wikidata↔Wikipedia corpus for 5000 steps, then on WebNLG for 500 steps) with a BERT-base uncased semantic-quality classifier fine-tuned on WebNLG human assessment data to score/filter generations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human evaluation (semantics & fluency 1–5); downstream retrieval-LM metrics: Exact Match (EM) on NQ and WQ; accuracy on LAMA probe (subcorpora: Google-RE, T-REx, SQuAD, ConceptNet).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Human eval: TEKGEN + subgraph inference produced higher mean semantics and fluency ratings and lower stddev vs baselines (no numeric mean reported in text). Downstream (REALM) - AUGMENTED (Wikipedia + KELM Documents) yields NQ EM 41.47% and WQ EM 43.90% (vs ORIGINAL Wikipedia rerun 38.84% and 40.80% respectively). LAMA gains with AUGMENTED KELM sentences: +12.94% (Google-RE), +0.95% (T-REx), +3.61% (SQuAD), +0.47% (ConceptNet) absolute improvements over ORIGINAL REALM.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Augmenting REALM's retrieval corpus with KELM sentences produced consistent improvements on knowledge-intensive tasks (open-domain QA and LAMA probe) with a relatively small token overhead (KELM ~286M words, ≈14% of Wikipedia token count). Generated sentences produced larger improvement than replacing the retrieval corpus with raw triples, indicating that verbalization is more effective for retrieval‑based LM pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Synthetic sentences remain limited to first-order facts (direct relations to a subject) and do not encode multi-hop implications (e.g., grandchild relations); corpus is much smaller (286M tokens) than Wikipedia (≈2B tokens), so linguistic format differences may bias learned representations; generation can still hallucinate if not properly filtered; semantic filtering is a separate post-process and not jointly optimized.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>When used to augment REALM, KELM sentences outperform using raw grouped triples (Triple Documents) as retrieval corpus (AUGMENTED with KELM > AUGMENTED with Triple Documents). Replacing Wikipedia with only KELM (REPLACED) hurt performance on NQ but helped on WQ, indicating format and coverage trade-offs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6996.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6996.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triple Documents (raw)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Triple Documents (subject-grouped raw Wikidata triples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation where raw Wikidata triples are grouped by subject into short document-like text blobs (each document is a list of raw triples) and used directly as retrieval corpus for retrieval-augmented language model pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>subject-grouped raw-triple document</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Group all raw KG triples with a common subject into a text document where triples are listed as structured tokens (e.g., (s, r, o) entries); no natural-language paraphrasing is applied — documents are collections of raw triple strings.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential token-based (KG token serialization), lossless-for-triples (relative to the listed triples) but not natural-language</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>grouping by subject entity — form a 'document' by aggregating all triples (s, r, o) for that subject and listing them (order unspecified or as found in KG).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Wikidata grouped into Triple Documents (used as an alternative retrieval corpus for REALM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>retrieval corpus for retrieval-augmented LM pretraining (REPLACED / AUGMENTED experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>REALM (retrieval-augmented language model) pretraining/evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Triple Documents were used as the retrieval corpus in pretraining of REALM (with the same CC-News pretraining corpus and default hyperparameters as other variants) and evaluated on NQ, WQ, and LAMA without additional finetuning for LAMA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) on NQ and WQ; accuracy on LAMA probe</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>REPLACED (Triple Documents only) REALM: NQ EM 21.14%, WQ EM 42.54%. AUGMENTED (Wikipedia + Triple Documents): NQ EM 40.28%, WQ EM 42.91%. Compared to ORIGINAL (Wikipedia rerun) NQ 38.84% and WQ 40.80%.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Using raw triple documents as the retrieval corpus alone (REPLACED) degrades NQ performance substantially while sometimes helping WQ, indicating limited generalization when the corpus is not in natural-language form; augmenting Wikipedia with Triple Documents recovers most performance but AUGMENTED with verbalized KELM sentences performs better.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not in natural-language form — retrieval models and downstream readers trained on natural text may not use these effectively; format mismatch with natural-language pretraining corpora; lack of sentence structure may hurt answer span extraction and generalization on real-query datasets like NQ.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared directly with KELM verbalized sentences: Triple Documents underperform as retrieval augmentation for REALM, especially on NQ; KELM (verbalized) augmentation yields larger and more consistent gains across LAMA and Q&A tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The WebNLG challenge: Generating text from RDF data <em>(Rating: 2)</em></li>
                <li>T-REx: A large scale alignment of natural language with knowledge base triples <em>(Rating: 2)</em></li>
                <li>Realm: Retrieval-augmented language model pre-training <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 1)</em></li>
                <li>Kgpt: Knowledge-grounded pretraining for data-to-text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6996",
    "paper_id": "paper-263864711",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "TEKGEN triple-concat",
            "name_full": "TEKGEN triple concatenation representation",
            "brief_description": "A simple sequential text encoding of a set of RDF-style triples where triples for a single subject are concatenated into a token sequence of the form: subject relation_1 object_1 , relation_2 object_2 , ... and fed as input to a text-to-text model (T5) to generate natural-language verbalizations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "triple-concatenation (subject-centric)",
            "representation_description": "Encodes graph information by listing triples as flat text tokens in the order: subject relation_1 object_1 , relation_2 object_2 , ...; relation and object are rendered as canonical KG labels/aliases and separated by punctuation/commas. If the subject alias does not appear in a target training sentence, a pronoun replacement heuristic is applied during alignment.",
            "representation_type": "sequential, token-based, lossy",
            "encoding_method": "edge-list ordering: concatenate triples (subject, relation, object) in a single flat sequence (subject-first) — ordering within the set determined by the entity-subgraph construction (see entity-subgraph entry); no graph-structure markers beyond relation/object tokens.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "Wikidata→Wikipedia aligned corpus (KG-Text alignment); WebNLG (used for finetuning)",
            "task_name": "graph-to-text generation / KG verbalization",
            "model_name": "T5-large (finetuned)",
            "model_description": "Pretrained text-to-text transformer (T5-large) sequentially fine-tuned first on the noisy Wikidata↔Wikipedia aligned corpus (5k steps) to increase coverage and then on the clean WebNLG 2017 dataset (500 steps) to reduce hallucinations.",
            "performance_metric": "Human evaluation (semantics and fluency, 1–5 scale); semantic-quality classifier correlation (BERT-based); qualitative comparisons",
            "performance_value": null,
            "impact_on_training": "Provided a straightforward input format enabling use of pretrained text-to-text models for KG verbalization; improved coverage of entities and relations when T5 was first fine-tuned on the aligned KG-text corpus.",
            "limitations": "Lossy with respect to explicit graph structure (no edge nesting or explicit graph delimiters beyond token order); ordering of triples is not canonical (entity-subgraph algorithm injects randomness for first triple), which can affect determinism. Models trained only on the noisy aligned corpus hallucinate (e.g., infer missing relations); requires additional clean finetuning to reduce hallucination.",
            "comparison_with_other": "Compared favorably when combined with entity-subgraph grouping and WebNLG finetuning vs. using single triple inputs or finetuning only on WebNLG; sequential finetuning (aligned corpus → WebNLG) reduced hallucination relative to only using the aligned (noisy) data.",
            "uuid": "e6996.0"
        },
        {
            "name_short": "Entity-subgraph serialization",
            "name_full": "Subject-centric entity-subgraph serialization (relation co-occurrence based grouping)",
            "brief_description": "A method to create small subgraphs (max depth 5) of triples all sharing the same subject, selected by relation co-occurrence statistics, and serialize that subgraph as a concatenated triple sequence for generation; intended to produce more coherent multi-triple verbalizations and reduce hallucination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "entity-subgraph serialization",
            "representation_description": "Constructs a small ordered set of triples (up to 5) sharing the same subject by: selecting a random initial triple for a subject, then greedily adding triples whose relations have the highest co-occurrence counts with the previously chosen relation (counts derived from KG↔text alignment). The final subgraph is serialized by concatenating the triples in the chosen order (subject relation object, ...).",
            "representation_type": "sequential, hierarchical-by-subgraph, token-based, lossy",
            "encoding_method": "relation co-occurrence-based greedy traversal within a subject's neighborhood (not graph-traversal like DFS/BFS); ordering determined by greedy co-occurrence ranking with a random first choice.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "KELM Corpus generation from Wikidata (entity subgraphs derived from aligned data)",
            "task_name": "graph-to-text generation (multi-triple verbalization)",
            "model_name": "T5-large (TEKGEN) used at inference with entity-subgraph inputs",
            "model_description": "Same T5-large model as above; at inference the model receives serialized entity-subgraphs (multi-triple) instead of single triples.",
            "performance_metric": "Human evaluation: fluency and semantics rated 1–5; comparison of mean and standard deviation across systems; qualitative examples",
            "performance_value": null,
            "impact_on_training": "Using entity-subgraphs at inference produced generated sentences with higher mean semantic and fluency ratings and lower standard deviation compared to single-triple inference when paired with the TEKGEN model; entity-subgraphs reduced hallucination compared to single-triple inference.",
            "limitations": "Subgraph construction is heuristic (relation co-occurrence) and subject-centric — it does not capture multi-hop relations across different subjects (no transitive closure or multi-hop inference). The first-triple randomness prevents full determinism. Quality depends on the co-occurrence statistics derived from the (noisy) aligned corpus.",
            "comparison_with_other": "Outperformed single-triple inference using the same model (T5) in human evaluation; reduced variance in fluency. When entity-subgraphs were not used, T5 tended to hallucinate more (especially when only trained on noisy aligned corpus).",
            "uuid": "e6996.1"
        },
        {
            "name_short": "KELM sentences",
            "name_full": "KELM verbalized sentences (natural-language sentences derived from Wikidata)",
            "brief_description": "The final natural-language sentences produced by TEKGEN from entity-subgraphs (the KELM Corpus) — a large synthetic corpus of verbalized Wikidata facts intended for augmenting LM pretraining/retrieval corpora.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "natural-language verbalization (KELM sentence)",
            "representation_description": "Each entity-subgraph is mapped to one generated natural-language sentence (or short sentence sequence) by the T5-based TEKGEN system; the sentences are human-readable, paraphrase KG relation labels into natural expressions, and are grouped by subject to form documents for retrieval LMs.",
            "representation_type": "sequential (natural language), loss-aware (intended readable output), token-based",
            "encoding_method": "model-based generation from serialized triples (triple-concatenation input) with top-5 sampling (temperature 0.5) and semantic-quality filtering (BERT-based); post-processing groups sentences into subject documents.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "KELM Corpus (∼18M generated sentences from ∼45M Wikidata triples); KELM documents (∼5.7M subject-grouped documents)",
            "task_name": "KG verbalization for pre-training data generation; retrieval-corpus augmentation for retrieval-augmented LM pretraining",
            "model_name": "T5-large (generator) + BERT-base (semantic filter); used to augment REALM pretraining",
            "model_description": "TEKGEN: a two-step sequentially-finetuned T5-large generator (first on noisy aligned Wikidata↔Wikipedia corpus for 5000 steps, then on WebNLG for 500 steps) with a BERT-base uncased semantic-quality classifier fine-tuned on WebNLG human assessment data to score/filter generations.",
            "performance_metric": "Human evaluation (semantics & fluency 1–5); downstream retrieval-LM metrics: Exact Match (EM) on NQ and WQ; accuracy on LAMA probe (subcorpora: Google-RE, T-REx, SQuAD, ConceptNet).",
            "performance_value": "Human eval: TEKGEN + subgraph inference produced higher mean semantics and fluency ratings and lower stddev vs baselines (no numeric mean reported in text). Downstream (REALM) - AUGMENTED (Wikipedia + KELM Documents) yields NQ EM 41.47% and WQ EM 43.90% (vs ORIGINAL Wikipedia rerun 38.84% and 40.80% respectively). LAMA gains with AUGMENTED KELM sentences: +12.94% (Google-RE), +0.95% (T-REx), +3.61% (SQuAD), +0.47% (ConceptNet) absolute improvements over ORIGINAL REALM.",
            "impact_on_training": "Augmenting REALM's retrieval corpus with KELM sentences produced consistent improvements on knowledge-intensive tasks (open-domain QA and LAMA probe) with a relatively small token overhead (KELM ~286M words, ≈14% of Wikipedia token count). Generated sentences produced larger improvement than replacing the retrieval corpus with raw triples, indicating that verbalization is more effective for retrieval‑based LM pretraining.",
            "limitations": "Synthetic sentences remain limited to first-order facts (direct relations to a subject) and do not encode multi-hop implications (e.g., grandchild relations); corpus is much smaller (286M tokens) than Wikipedia (≈2B tokens), so linguistic format differences may bias learned representations; generation can still hallucinate if not properly filtered; semantic filtering is a separate post-process and not jointly optimized.",
            "comparison_with_other": "When used to augment REALM, KELM sentences outperform using raw grouped triples (Triple Documents) as retrieval corpus (AUGMENTED with KELM &gt; AUGMENTED with Triple Documents). Replacing Wikipedia with only KELM (REPLACED) hurt performance on NQ but helped on WQ, indicating format and coverage trade-offs.",
            "uuid": "e6996.2"
        },
        {
            "name_short": "Triple Documents (raw)",
            "name_full": "Triple Documents (subject-grouped raw Wikidata triples)",
            "brief_description": "A representation where raw Wikidata triples are grouped by subject into short document-like text blobs (each document is a list of raw triples) and used directly as retrieval corpus for retrieval-augmented language model pretraining.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "subject-grouped raw-triple document",
            "representation_description": "Group all raw KG triples with a common subject into a text document where triples are listed as structured tokens (e.g., (s, r, o) entries); no natural-language paraphrasing is applied — documents are collections of raw triple strings.",
            "representation_type": "sequential token-based (KG token serialization), lossless-for-triples (relative to the listed triples) but not natural-language",
            "encoding_method": "grouping by subject entity — form a 'document' by aggregating all triples (s, r, o) for that subject and listing them (order unspecified or as found in KG).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Wikidata grouped into Triple Documents (used as an alternative retrieval corpus for REALM)",
            "task_name": "retrieval corpus for retrieval-augmented LM pretraining (REPLACED / AUGMENTED experiments)",
            "model_name": "REALM (retrieval-augmented language model) pretraining/evaluation",
            "model_description": "Triple Documents were used as the retrieval corpus in pretraining of REALM (with the same CC-News pretraining corpus and default hyperparameters as other variants) and evaluated on NQ, WQ, and LAMA without additional finetuning for LAMA.",
            "performance_metric": "Exact Match (EM) on NQ and WQ; accuracy on LAMA probe",
            "performance_value": "REPLACED (Triple Documents only) REALM: NQ EM 21.14%, WQ EM 42.54%. AUGMENTED (Wikipedia + Triple Documents): NQ EM 40.28%, WQ EM 42.91%. Compared to ORIGINAL (Wikipedia rerun) NQ 38.84% and WQ 40.80%.",
            "impact_on_training": "Using raw triple documents as the retrieval corpus alone (REPLACED) degrades NQ performance substantially while sometimes helping WQ, indicating limited generalization when the corpus is not in natural-language form; augmenting Wikipedia with Triple Documents recovers most performance but AUGMENTED with verbalized KELM sentences performs better.",
            "limitations": "Not in natural-language form — retrieval models and downstream readers trained on natural text may not use these effectively; format mismatch with natural-language pretraining corpora; lack of sentence structure may hurt answer span extraction and generalization on real-query datasets like NQ.",
            "comparison_with_other": "Compared directly with KELM verbalized sentences: Triple Documents underperform as retrieval augmentation for REALM, especially on NQ; KELM (verbalized) augmentation yields larger and more consistent gains across LAMA and Q&A tasks.",
            "uuid": "e6996.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The WebNLG challenge: Generating text from RDF data",
            "rating": 2,
            "sanitized_title": "the_webnlg_challenge_generating_text_from_rdf_data"
        },
        {
            "paper_title": "T-REx: A large scale alignment of natural language with knowledge base triples",
            "rating": 2,
            "sanitized_title": "trex_a_large_scale_alignment_of_natural_language_with_knowledge_base_triples"
        },
        {
            "paper_title": "Realm: Retrieval-augmented language model pre-training",
            "rating": 2,
            "sanitized_title": "realm_retrievalaugmented_language_model_pretraining"
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 1,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Kgpt: Knowledge-grounded pretraining for data-to-text generation",
            "rating": 1,
            "sanitized_title": "kgpt_knowledgegrounded_pretraining_for_datatotext_generation"
        }
    ],
    "cost": 0.0141845,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training
June 6-11, 2021</p>
<p>Oshin Agarwal oagarwal@seas.upenn.edu 
University of Pennsylvania</p>
<p>Heming Ge hemingge@google.com 
Google Research</p>
<p>Siamak Shakeri siamaks@google.com 
Google Research</p>
<p>Rami Al-Rfou 
Google Research</p>
<p>Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training</p>
<p>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJune 6-11, 20213554
Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domainspecific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, largescale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe.</p>
<p>Introduction</p>
<p>Data-To-Text Generation (Kukich, 1983;Goldberg et al., 1994) involves converting knowledge graph (KG) triples of the form (subject, relation, object) into a natural language sentence(s). There are many standard datasets for this task such as WebNLG (Gardent et al., 2017) and many systems have been developed to improve performance on these datasets. However, to the best of our knowledge, no prior work has attempted to verbalize a full knowledge graph. Verbalizing a full KG has additional challenges over small benchmark datasets, such as entity and relation coverage and the lack of grouped sets of triples that can produce coherent sentences together. In this paper, we convert the English Wikidata KG (Vrandečić and Krötzsch, 2014) into natural language text (Figure 1). The * Work done during internship at Google  Figure 1: An example of generating text from KG. First, the entity subgraphs on the left are created and then converted to the sentence on the right. generated corpus, which we call the KELM Corpus, consists of ∼18M sentences spanning ∼45M triples with ∼1500 distinct relations. For training the verbalization system, we also create an English Wikidata KG-Wikipedia Text aligned corpus consisting of a variety of entities such as dates and numerical quantities.</p>
<p>We evaluate the quality of the generated corpus through human evaluation of a random sample. We further showcase the utility of this corpus in language model pre-training. Text represents a limited coverage of the world knowledge. Therefore, we expect the language models to be restricted to facts that are expressed in natural language. Moreover, facts may not be expressed as explicitly in text as they are in KGs, and the variability in the quality of text can eventually cause biases in the resulting models (Bolukbasi et al., 2016;Sheng et al., 2019;Manzini et al., 2019). Building models that handle structured data and free form text seamlessly has been a long sought-after goal. However, their integration is challenging due to different structural formats. KG verbalization provides a simple way to integrate KGs with natural text. We illustrate this by augmenting the REALM (Guu et al., 2020) </p>
<p>Entity Subgraph Creator</p>
<p>Relation co-occurrence counts ( </p>
<p>TEKGEN</p>
<p>One of the challenges in converting an entire KG to text is the wide variety of entities and relations. Wikidata consists of ∼6M entities and ∼1500 relations. In comparison, the WebNLG dataset has ∼600 entities and ∼20 relations. In this section, we discuss the various components of TEKGEN, also illustrated in Figure 2 -1. Create a large yet noisy training dataset using distant supervision.</p>
<ol>
<li>
<p>Sequentially fine-tune T5, first on the dataset from step 1 for improved coverage, then on a small clean dataset for reduced hallucination.</p>
</li>
<li>
<p>Build a filter for the generated text based on its semantic quality w.r.t. the KG triples.</p>
</li>
</ol>
<p>Training Dataset</p>
<p>We first create training data using distant supervision by aligning Wikidata triples to Wikipedia text (see Figure 3).</p>
<p>KG-Text Alignment</p>
<p>For each entity, we constrain the candidate sentences to the root section of its Wikipedia page  because this section generally describes the relations of the subject entity with other entities. For each sentence in this section, we match all triples that have this entity as the subject. A triple is said to match if any alias of the object entity occurs in the sentence. We do not match relations to text as there are too many ways to express them. Constraining to the subject entity's page and root section generally ensures that the relation is expressed in the sentence if it mentions the object entity. Each triple can align to multiple sentences and each sentence can have multiple triples aligned to it. If any alias of the subject entity occurs in the given sentence, the sentence is selected as is, else the first animate third-person personal or possessive pronoun is replaced by the subject entity's canonical name. The pronoun replacement heuristic also works well because of this constraint. All triples aligned to a given sentences are combined together as a single example.</p>
<p>Alignment statistics are shown in Table 1 and some alignment examples are shown in Table 2. There are a total of ∼45M triples, ∼35% of which were aligned to sentences. This results in ∼8M examples, covering ∼42% of the relations.</p>
<p>Note that each sentence in the aligned corpus is matched to triples with a common subject entity. While this results in some noise, such errors should be small due to the constraint that the text is the root section of the subject entity page. This constraint allows us to maintain the same property of common subject entity as the entity subgraph used in inference ( §3). It also simplified the alignment process, removing the need to match relations to text. In comparison, the T-REx (Elsahar et al., 2018)   errors due to entity linking and incorrect entailment, which are unlikely in our corpus due to this constraint.</p>
<p>Types of Triples</p>
<p>We extract several types of triples, each of which have slightly different matching techniques. Other alignment corpora built using Wikipedia hyperlinks  would miss many of these triples with entities without Wikipedia pages such as quantities, dates and certain occupations, and hence relations such as date of birth, publication year and distance from Earth. The 2012 reelection campaign of Barack Obama, the 44th President of the United States, was formally announced on April 4, 2011. Blue whale (parent taxon, Balaenoptera)</p>
<p>The blue whale (Balaenoptera musculus) is a marine mammal belonging to the baleen whale suborder Mysticeti. While the type of the triples is important in the alignment process, the verbalization model is agnostic to the type and treats all triples the same.</p>
<p>Model</p>
<p>We perform a two-step sequential finetuning of the pre-trained T5-large  model for converting triples to text. Triples are concatenated as subject relation_1 object_1, ....relation_n object_n for input to T5. The model is first fine-tuned on the aligned corpus for 5000 steps to increase the coverage of entities and relations. However, this results in the generation of Wikipedia-like sentences and hallucination if a certain expected input triple is missing. For example, Wikipedia sentences generally mention date of birth, date of death, occupation together. If the occupation is missing in the input, the system hallucinates a random occupation. "Neff Maiava date of birth 01 May 1924, date of death, 21 April 2018." generates "Neff Maiava (1 May 1924 -21 April 2018) was an Albanian actor."; hallucinating a profession. To overcome this, we further fine-tune the model on WebNLG 2017 data for 500 steps. While WebNLG has low coverage, the information in the input triples matches the target sentence exactly. WebNLG also has a different sentence structure than Wikipedia. This reduces conformity to Wikipedia sentence structure and hence reduces hallucination. We use a learning rate of 0.001, a batch size of 1048576 tokens and a maximum decoding length of 256.  </p>
<p>Quality Filtering</p>
<p>We perform a semantic quality based filtering of the sentences generated by the triple-to-text module. This is a separate post-processing module used during inference and is not jointly optimized with the text generation module. A semantic quality score is assigned to each generated sentence w.r.t. the input triples that indicates whether or not the generated text captures the full meaning of the triples and does not hallucinate extra information. The score is generated using a BERT base uncased model with input of the form [CLS] concatenated-triples [SEP] reference-or-generated-sentence. It is fine-tuned for 1000 steps on the WebNLG 2017 human assessment data. The data contains system predictions submitted to the shared task rated on a scale of 1-3 for semantics and fluency. We use the semantics score and scale it to 0-1. We also add gold references with a score of 1. This results in 2706 examples, 90% of which are used for finetuning and the remaining for evaluation. High correlations are obtained between the predicted scores and human scores on the evaluation split (Table 3).</p>
<p>KELM Corpus</p>
<p>In this section, we utilize the TEKGEN model and filtering mechanism to build a synthetic corpus that captures the KG in natural language format.</p>
<p>Entity Subgraph</p>
<p>Datasets such as WebNLG have instances with grouped triples that can be expressed as a fluent sentence. Such groups are not available for a large KG and using one triple at a time for inference would lead to hallucination as training uses multi-  Figure 4: Entity Subgraph Creation Algorithm using relation co-occurrence counts based on relation-sentence alignment in the training data. Each entity subgraph consists of a maximum of five triples, all with the same subject entity. The first triple is chosen at random. The second triple is chosen such that its relation has the highest co-occurrence count with the relation in the first triple and so on. ple triples per example. Therefore, we develop a strategy to create entity subgraphs based on relation co-occurrence counts i.e. frequency of alignment of two relations to the same sentence in the training data. The algorithm is shown in Figure 4. It produces ∼18M entity subgraphs from ∼45M triples so the final corpus will have 18M generated sentences corresponding to each entity subgraph.
all_triple_sets ← {} rel_pairs ← {} depth ← 5 for all ri ∈ KG do P ← {(rj, cij)∀(ri, rj, cij) ∈ train_align_counts} rel_pairs(ri) ← maxheap(P ) end for for all entities s ∈ KG do R ← {(r, o)∀(s, r, o) ∈ KG} while R = ∅ do triple_set ← {} (r1, o1) ← random(R) triple_set.add(s, r1, o1) R.remove(s, r1, o1) KG.remove(s, r1, o1) for i = 2 to depth do ri ← NONE M ← rel_pairs(ri−1) while M = ∅ do (rj, cij) ← M.next if rj ∈ R then ri ← rj (ri, oi) ← R.</p>
<p>Generation</p>
<p>For each entity subgraph, we concatenate all its triples as before. We perform top 5 sampling with a temperature of 0.5. The bottom 1% of the generated sentences are filtered out based on the semantic score assigned using the model in §2.3.  Table 4: Human evaluation of the generated corpora, on a scale of 1-5, for semantics and fluency.</p>
<p>Human Evaluation</p>
<p>Generation quality of the KELM Corpus is evaluated using human ratings on a random sample of 200 entity subgraphs. Automatic metrics such as BLEU (Papineni et al., 2002) or BERTscore (Zhang et al., 2019) cannot be used due to the lack of gold references. Following prior work, the generated text is rated for two aspects-fluency and semantics, on a scale of 1-5, where 1 means not at all fluent/does not capture meaning at all and 5 means completely fluent/fully captures meaning with no hallucination. We have eight annotators total and each example is rated by two of them. All annotators are linguists, NLP researchers or NLP practitioners and volunteered for the evaluation. We do not use any crowd sourcing platform. For each instance, scores of the two annotators are averaged to get the final rating. The Pearson correlation between the two sets of ratings is 0.56 for semantics and 0.43 for fluency. We compare TEKGEN to two baseline systems. For both baselines, we fine-tune a T5-large model only on WebNLG 2017 data but use different inference input. For one system, we use one triple at a time as input, resulting in 524 instances from the 200 entity subgraphs. For the second, we use the entity subgraphs as input, resulting in 200 instances. Scores are shown in Table 4. Entity subgraphs during inference do not improve the mean scores but reduce the standard deviation of the fluency. In comparison, TEKGEN with inference using entity subgraphs improve both the semantics and fluency of the generated text. Both the mean scores are higher and the standard deviations are lower. It paraphrases canonical names of relations in the KG to more natural expressions more often. Some examples of generation using the two systems are shown in Table 5. In the second example, the relation 'inception' is paraphrased to 'started' using WebNLG_finetuning+Triple_Inference and 'founded' using TEKGEN+Subgraph_Inference, the latter being more appropriate for organizations.</p>
<p>For completeness, we evaluate two more base-  line systems in which T5-large model is finetuned only on the KG-Text aligned corpus but use the two different inference inputs-single triple and entity subgraphs. One annotator rated the same sample for semantics. The former had an average score of 2.34 and the latter 2.73. Since these scores were very low, we did not pursue the evaluation of these systems further. The use of just the aligned corpus which is noisy to some extent results in the worst performing system out of all the methods.</p>
<p>Knowledge Enhanced LMs</p>
<p>In this section, we showcase an application of the generated KELM Corpus as a way to integrate KGs into natural text corpora for pre-training language models (LMs), as shown in Figure 5. We choose REALM (Guu et al., 2020) as a representative of the recently introduced family of retrieval language models and therefore we expect our work to be equally applicable to other such language models. We show gains on LAMA knowledge probe and open domain QA with augmentation. We also perform experiments where we integrate raw Wikidata triples instead of KELM corpus to confirm the effectiveness of verbalization.</p>
<p>Retrieval Language Models</p>
<p>REALM is a retrieval-based language model and uses two corpora for pre-training-a retrieval corpus and a pre-training corpus. During pre-training, a sentence is selected at random from the pre-training corpus and a random word or salient span (dates and entities) is masked in this sentence. Then using a joint representation of the masked sentence and each of the documents in the retrieval corpus, the masked word is predicted. In the finetuning stage, the model is provided with a query/question as input in place of masked sentence from the pretraining corpora. It retrieves a small set of documents from the retrieval corpus based on the vector similarity of the query and document representation and then selects a span of text from the retrieved documents as the answer.</p>
<p>KELM Documents</p>
<p>We group sentences in the KELM corpus by subject entities to create 5722974 (5.7M) documents. We call these KELM documents. We then replace/augment the retrieval corpus in REALM with these synthetic documents. KELM Corpus has only ∼286M words (∼14%) in comparison to ∼2B words in English Wikipedia. </p>
<p>KG as natural language corpus</p>
<p>Spork EP is an EP released by indie rock band Flake Music.</p>
<p>Natural Language Corpus</p>
<p>Language Model</p>
<p>Knowledge Graph Figure 5: Knowledge Graph verbalization for integration with natural text corpora for language model pre-training.</p>
<p>Evaluation Datasets</p>
<p>We perform evaluation using two open domain question answering datasets and one knowledge probing dataset.</p>
<p>Question Answering</p>
<p>NaturalQuestions (NQ) (Kwiatkowski et al., 2019): Queries to Google and their answers.</p>
<p>WebQuestions (WQ) (Berant et al., 2010): question-answers from the Google Suggest API. We keep the same settings as REALM for both NQ and WQ i.e. we work on the open domain setting for both datasets where no passage is provided as context for each question. Finetuning is performed on respective training splits.</p>
<p>Knowledge Probe</p>
<p>LAMA (Petroni et al., 2019): Fill-in-the-Blank style factual queries with single token answers from four sources: Google-RE, 3 T-REx (Elsahar et al., 2018), SQuAD (Rajpurkar et al., 2016) and Con-ceptNet (Speer and Havasi, 2012). Google-RE also consists of aliases of each answer.</p>
<p>REALM did not include LAMA as one of its evaluation datasets. So we first evaluate REALM on LAMA using the original retrieval corpus and then using the KELM Corpus. No finetuning is performed and the masked word predictions from the pre-trained models are used as answers.</p>
<p>Results</p>
<p>We evaluate REALM on WQ, NQ and LAMA under three settings by modifying the retrieval corpus. The REPLACED and AUGMENTED models are evaluated using both the raw Wikidata triples and the generated sentences. Wikidata triples are grouped by subject entity to form Triple Documents and KELM Corpus sentences are also grouped by subject entity to form KELM Corpus Documents ( §4.2). The model is pre-trained for 200k steps with the CC-News pre-training corpus in all cases with default hyperparameters.</p>
<p>ORIGINAL For NQ and WQ, we fine-tuned the pre-trained REALM on the respective training splits. While we were able to reproduce the accuracy on WQ, the accuracy on NQ was ∼1.5% absolute less than the reported accuracy (row 1&amp;2 in Table 7). For LAMA probe, we first evaluated the pre-trained REALM, reporting the results on the different sub-corpora in Table 6 (row Wikipedia under REALM). Even the ORIGINAL REALM model shows substantial improvement over prior models. The ability of REALM to access the corpus documents during inference not only make it interpretable but also better on the knowledge intensive tasks. It obtains an accuracy of 67.36% on Google-RE, 68.18% on T-REx and 27.96% on    Table 7). This can be attributed to the nature of the datasets-WQ is a KG-based dataset whereas NQ consists of real queries issued to Google. On LAMA (rows 2&amp;3 under REALM in Table 6), the performance is lower than the ORIGINAL model but much higher than BERT. Both Triple Documents and KELM Corpus Documents have similar performance. When using just the KG, the format doesn't matter. However, a system trained on raw triples may not generalize for tasks where sentence structure is important.</p>
<p>AUGMENTED We observe improvements on all the datasets (last two rows of Tables 6&amp;7) with the AUGMENTED model which uses both the Wikipedia text and the KELM Documents. There is an absolute gain of 2.63% and 3.10% on NQ and WQ respectively over the ORIGINAL model. Similarly, there is an absolute gain of 12.94%, 0.95%, 3.61% and 0.47% on Google-RE, T-REx, SQuAD and ConceptNet in LAMA respectively. Unlike the REPLACED model, the improvement is higher when the generated sentences in KELM Corpus are added instead of the raw Wikidata triples, confirming the effectiveness of verbalization of KG into natural language sentences. Wikipedia is the dominant corpus with 2B words whereas KELM corpus sentences are succinct with a total of 286M words ( §4.2) so it is likely the learned representations favour the Wikipedia format which is natural language sentences. We expect augmenting other retrieval-based models such as DPR (Karpukhin et al., 2020) and RAG (Lewis et al., 2020) with the KELM corpus should also improve their performance, given that their enhancements are orthogonal to our contribution. Moreover, we note that our augmented corpus represents a scalable strategy for future QA systems; by adding only 14% more tokens to the original REALM model we outperform huge and computationally expensive models such as   We inspected the errors of the AUGMENTED model with KELM Documents on LAMA. Apart from real errors where the prediction is actually incorrect, there were some false errors that can be broadly classified into three categories-1. Ambiguous Query: e.g. In "X was born in ____", the answer could be the year or the place of birth but only one of them is acceptable depending on the subcorpus.</p>
<ol>
<li>
<p>Incomplete Answer Set: e.g. In "Konstantin Mereschkowski had a career as ____", the gold target is biologist and the prediction is botanist but both should be correct.</p>
</li>
<li>
<p>Answer granularity: The prediction is correct but more specific. e.g. In "On the CPI scale, Kenya ranks ____", the gold answer is low but the prediction is 101, which is in fact correct.</p>
</li>
</ol>
<p>Related Work</p>
<p>Data-to-Text Generation Data-to-Text Generation has several benchmark datasets with slightly different objectives-WebNLG (Gardent et al., 2017) to convert a group of triples to text, E2ENLG (Dušek et al., 2018) Shimorina and Gardent, 2018) have been developed and evaluated on these datasets, such as graph transformers over structured data (Koncel-Kedziorski et al., 2019), latent templates for interpretability (Wiseman et al., 2018) and text-to-text generation with T5 (Kale, 2020).  (Etzioni et al., 2008;Angeli et al., 2015;Clancy et al., 2019) inherently create such a corpus but these works generally do not release the extracted KG triples.</p>
<p>KG-Text alignment</p>
<p>Incorporating KGs Most prior works on incorporating KG with text often learn KG entity representations and add them to the mention spans linked to the entity Févry et al., 2020) or create subgraphs relevant to the query that are expanded with text in the embedding space Sun et al., 2019;Xiong et al., 2019). Some others incorporate additional modules. Verga et al. (2020) extend Févry et al. (2020) by adding a triple memory with (subject, relation) encoding as the key and the object encoding as the value. Das et al. (2017) use universal schema (Riedel et al., 2013) that embeds text and KGs in a shared space for their integration. K M et al. (2018) learn a single representation for all the triples mentioned in a sentences during pre-training and update it further in task-specific finetuning. In contrast, we convert the KG into text and use it to augment the pre-training data.</p>
<p>Future Work</p>
<p>The KELM corpus sentences covers all facts in the KG but the generated sentences are limited to a given entity and its direct relations to other entities. For example, given the triples (X, child, Y) and (Y, child, Z), it does not the contain "Z is a grandchild of X". More complex sentences could be generated by incorporating multi-hop relations in the KG. Recent work has also shown promising results on generating multilingual text from English triples (Castro Ferreira et al., 2020;Agarwal et al., 2020). Our proposed approach can be applied to generate a multilingual corpus of facts in various languages using English Wikidata.</p>
<p>Conclusion</p>
<p>In this paper, we converted an entire KG (Wikidata) to natural text (KELM Corpus), tackling various challenges over verbalizing domain-specific benchmark datasets. We further showcase that KG verbalization can be used to integrate KGs and natural text corpora by including the verbalized KG as additional pre-training data. We augment a retrieval-based language model with the generated synthetic KELM corpus as a retrieval corpus.</p>
<p>We evaluated the augmented model on open domain QA and a knowledge probe, showing improvements on both. The KELM Corpus is publicly available at https://github. com/google-research-datasets/ KELM-corpus.</p>
<p>Figure 3 :
3KG-Text alignment algorithm.</p>
<p>retrieval corpus with the KELM Corpus. We evaluate Training data Input: To Kill a Mockingbird author Harper Lee, publication date 11 July 1960. Target: To Kill a Mockingbird is a novel by Harper Lee published in 1960.Wikidata 
(KG) </p>
<p>Wikipedia 
(text) </p>
<p>Triple &lt;-&gt; 
Sentence Aligner </p>
<p>Text to Text Generator </p>
<p>T5 </p>
<p>Finetuning 1 </p>
<p>Pipelines for training the TEKGEN model and generating the KELM corpus. In Step 1 , KG triples are aligned with Wikipedia text using distant supervision. In Steps 2 &amp; 3 , T5 is fine-tuned sequentially first on this corpus, followed by a small number of steps on the WebNLG corpus, In Step 4 , BERT is fine-tuned to generate a semantic quality score for generated sentences w.r.t. triples. Steps 2 , 3 &amp; 4 together form TEKGEN. To generate the KELM corpus, in Step 5 , entity subgraphs are created using the relation pair alignment counts from the training corpus generated in step 1 . The subgraph triples are then converted into natural text using TEKGEN. Both the TEKGEN training corpus and the KELM corpus are available at https://github.com/ google-research-datasets/KELM-corpusdate of birth, date of death, 539854) 
(date of birth, occupation, 809626) 
(date of death, occupation, 393490) </p>
<p>Semantic quality 
filter 
(BERT) </p>
<p>KELM Corpus 
(text) </p>
<p>(To Kill a Mockingbird, author, 
Harper Lee) 
(To Kill a Mockingbird, 
publication date, 11 July 1960) </p>
<p>To Kill a Mockingbird is 
a novel by Harper Lee 
published in 1960. </p>
<p>To Kill a Mockingbird award 
received Pulitzer Prize for 
Fiction, Pulitzer Prize for Fiction 
point in time 1961, Pulitzer 
Prize for Fiction winner Harper 
Lee. 
(To Kill a Mockingbird, award 
received, Pulitzer Prize for Fiction) 
(To Kill a Mockingbird, Pulitzer Prize 
for Fiction point in time, 1961) 
(To Kill a Mockingbird, Pulitzer Prize 
for Fiction winner, Harper Lee.) </p>
<p>Harper Lee won 
the 1961 Pulitzer 
Prize for Fiction 
for To Kill a 
Mockingbird. ✅ </p>
<p>Harper Lee won the 
1961 Pulitzer Prize 
for Fiction for To Kill 
a Mockingbird. ✅ </p>
<p>Harper Bazaar won 
the Pulitzer Prize for 
Poetry in 1691. ❌ </p>
<p>① 
② </p>
<p>④ </p>
<p>⑤ </p>
<p>WebNLG </p>
<p>(triples, text) </p>
<p>Finetuning 2 </p>
<p>③ </p>
<p>TEKGEN </p>
<p>Figure 2: the augmented system on the LAMA knowledge 
probe (Petroni et al., 2019) and open domain QA 
and show improvements on both. Through ablation 
experiments where we augment the retrieval corpus 
with the raw triples instead, we further confirm the 
effectiveness of verbalization. Our contributions 
are as follows -</p>
<p>• TEKGEN (Text from KG Generator): A data-
to-text sequence-to-sequence model for ver-
balizing an entire KG </p>
<p>• TEKGEN training corpus: Text-KG aligned 
corpora with a wide variety of relations in-
cluding dates and quantities </p>
<p>• KELM Corpus, 1 (Corpus for Knowledge-
Enhanced Language Model Pre-training): A 
large-scale synthetic corpus of Wikidata KG 
as natural text </p>
<p>• Data-to-text generation as a method to inte-
grate KGs with textual pre-training corpora, 
showing improvements on open domain QA 
and LAMA probe with the augmented model </p>
<p>1 </p>
<p>corpus does not have this noise due the use of typical NLP pipeline with coreference resolution and predicate linking. However, it suffers fromTotal KG triples 
45,578,261 
Triples aligned 
16,090,457 
Total sentences aligned 
7,978,814 
Total KG relations 
1,522 
Relations aligned 
663 </p>
<p>Table 1 :
1KG-Text alignment statistics.</p>
<p>Table 2 :
2Examples of alignment (training data).We retain the main triple as such and 
reformat the subproperty as a triple of the 
form (subject_entity, object_entity </p>
<p>subproperty_name, subproperty_value) </p>
<p>e.g. (Barack, spouse, Michelle) has the 
subproperty (place of marriage, Trinity 
Church). These are converted to (Barack, 
spouse, Michelle) and (Barack, Michelle 
place of marriage, Trinity Church). </p>
<p>Table 3 :
3Semantic Filtering Evaluation.</p>
<p>The 10x10 Photobooks are the result of a non-profit organization. 10x10 Photobooks was started in 00 2012.Edu was born in 1949 and is a member of Tigres UANL. Edu ( footballer , born in 1949 ) Tigres UANL's end time was 01 January 1983. Edu ( footballer , born 1949 ) was at Tigres UANL from 01 January 1978. Edu, who was born in 1949, played for Tigres UANL between 1978 and 1983. To Kill a Mockingbird won the Pulitzer Prize for Fiction. To Kill a Mockingbird was Pulitzer Prize for Fiction, awarded in 00 1961. Harper Lee was the winner of the Pulitzer Prize for Fiction for To Kill a Mockingbird.Input Triples 
WebNLG_Finetuning + Triples_Inference TEKGEN + Subgraph_Inference 
(Michelle Obama, height, +71 inch) 
Michelle Obama's height is +71 inch. 
Michelle Obama is 71 inches tall. 
(10x10 Photobooks, instance of, 
Nonprofit organization), 
(10x10 Photobooks inception, 00 2012) </p>
<p>10x10 Photobooks, founded in 2012 
is a nonprofit organization. </p>
<p>(Edu (footballer, born 1949), member of 
sports team, Tigres UANL) 
(Edu (footballer, born 1949 ), Tigres 
UANL end time, 01 January 1983) 
(Edu (footballer, born 1949 ), Tigres 
UANL start time, 01 January 1978) </p>
<p>(To Kill a Mockingbird, award received, 
Pulitzer Prize for Fiction) 
(To Kill a Mockingbird Pulitzer Prize for 
Fiction point in time 00 1961) 
(To Kill a Mockingbird Pulitzer Prize for 
Fiction winner Harper Lee) </p>
<p>Harper Lee won the 1961 Pulitzer 
Prize for Fiction for To Kill a 
Mockingbird. </p>
<p>(Caucasus Mountains, country, Georgia 
(country)) 
(Caucasus Mountains, instance of, 
Mountain range) 
(Caucasus Mountains, country, Russia) 
(Caucasus Mountains, highest point, 
Mount Elbrus) 
(Caucasus Mountains, country, Armenia) </p>
<p>(Caucasus Mountains, topic's main 
category, Category:Caucasus Mountains) </p>
<p>The Caucasus Mountains are located in 
Georgia. 
The Caucasus Mountains is an example of a 
Mountain range. 
Caucasus Mountains is in Russia. 
The highest point in the Caucasus Mountain 
-s is Mount Elbrus. 
Caucasus Mountains is in the country of 
Armenia. 
The Caucasus Mountains is categorised as a 
Caucasus Mountains. </p>
<p>The Caucasus Mountains are a 
mountain range found in Georgia, 
Armenia and Russia. Mount Elbrus 
is the highest point in the Caucasus 
Mountains. </p>
<p>Table 5 :
5Examples of text generated by the final model in comparison to the model trained only on WebNLG.</p>
<p>Table 6 :
6Accuracy on LAMA probe. Pretaining corpus is CCnews and the retrieval corpus changed for REALM.REALM Retrieval Corpus 
NQ 
WQ 
ORIGINAL 
Wikipedia (reported) 
40.40 
40.70 
Wikipedia (rerun) 
38.84 
40.80 
REPLACED 
Triple Documents 
21.14 
42.54 
KELM Documents 
22.58 
41.19 
AUGMENTED 
Wikipedia + Triple Documents 
40.28 
42.91 
Wikipedia + KELM Documents 
41.47 
43.90 </p>
<p>Table 7 :
7Exact Match (EM) accuracy of REALM on NQ and WQ. Pretraining corpus used is CC-News.SQuAD. In comparison, the reported accuracy for 
BERT (Devlin et al., 2019) is 10.50% on Google-
RE, 32.30% on T-REx and 17.40% on SQuAD. 
BERT performs better on 1-1 T-REx relations with 
74.50% accuracy as compared to REALM with 
55.81% accuracy. However, this group consists of 
only two relations; capital and capital of. BERT 
also has better performance than REALM on the 
ConceptNet subcorpus. On inspection of some of 
the queries in ConceptNet, we found the questions 
to be vague and possibly hard for even humans. For 
example, Raven can ___ and Time is ___. </p>
<p>REPLACED The REPLACED model which uses 
only KELM Corpus Documents, performs better 
than the ORIGINAL model on WQ but the accuracy 
is much lower on NQ (rows 3&amp;4 in </p>
<p>(11B parameters) on NQ (35.20 → 41.47) and WQ (42.80 → 43.90). Wikipedia is the dominant corpus with 2B words whereas KELM corpus sentences are succinct with a total of 286M words ( §4.2) so it is likely the learned representations favour the Wikipedia format which is natural language sentences.</p>
<p>to convert database key-value pairs or pictures to text, WikiBio (Lebret et al., 2016) for biography generation from text, Wiseman et al. (2017) for text describing score statistics tables of basketball games, both ToTTo (Parikh et al., 2020) and DART (Radev et al., 2020) to generate text given a table and relevant highlighted cells. Many systems (van der Lee et al., 2018; Castro Ferreira et al., 2019;</p>
<p>T-REx(Elsahar et al., 2018) is a widely used Text-KG aligned corpus, built using systems such as coreference resolution and predicate linkers (details in §2.1.1). and also created an aligned corpus using Wikipedia hyperlinks and coreference resolution. (details on comparison in §2.1.2). In contrast, we use alias-based heuristics coupled with source text selection constraints to generate a corpus of 16M triples aligned with 8M sentences. Lastly, open information extraction i.e. automatic KG construction from text
https://en.wikipedia.org/wiki/ Wikipedia:Date_formattings
https://code.google.com/archive/p/ relation-extraction-corpus/
AcknowledgmentsWe thank William Woods, Jonni Kanerva, Tania Rojas-Esponda, Jianmo Ni, Aaron Cohen and Itai Rolnick for rating the synthetic corpus sample for human evaluation. We also thank Kelvin Guu for his valuable feedback on the paper.
Machine translation aided bilingual data-to-text generation and semantic parsing. Oshin Agarwal, Mihir Kale, Heming Ge, Siamak Shakeri, Rami Al-Rfou, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+). the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)Dublin, Ireland(Virtual). Association for Computational LinguisticsOshin Agarwal, Mihir Kale, Heming Ge, Siamak Shak- eri, and Rami Al-Rfou. 2020. Machine transla- tion aided bilingual data-to-text generation and se- mantic parsing. In Proceedings of the 3rd Interna- tional Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 125-130, Dublin, Ireland (Virtual). Association for Computa- tional Linguistics.</p>
<p>Leveraging linguistic structure for open domain information extraction. Gabor Angeli, Melvin Jose Johnson Premkumar, Christopher D Manning, 10.3115/v1/P15-1034Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaLong Papers1Association for Computational LinguisticsGabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D. Manning. 2015. Leveraging linguis- tic structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan- guage Processing (Volume 1: Long Papers), pages 344-354, Beijing, China. Association for Computa- tional Linguistics.</p>
<p>Global learning of focused entailment graphs. Jonathan Berant, Ido Dagan, Jacob Goldberger, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. the 48th Annual Meeting of the Association for Computational LinguisticsUppsala, SwedenAssociation for Computational LinguisticsJonathan Berant, Ido Dagan, and Jacob Goldberger. 2010. Global learning of focused entailment graphs. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220-1229, Uppsala, Sweden. Association for Com- putational Linguistics.</p>
<p>Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Tolga Bolukbasi, Kai-Wei Chang, Y James, Venkatesh Zou, Adam T Saligrama, Kalai, Advances in neural information processing systems. Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Ad- vances in neural information processing systems, pages 4349-4357.</p>
<p>The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results. Claire Thiago Castro Ferreira, Nikolai Gardent, Chris Ilinykh, Simon Van Der Lee, Diego Mille, Anastasia Moussallem, Shimorina, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+). the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)DublinIreland (Virtual). Association for Computational LinguisticsThiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh, Chris van der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina. 2020. The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+ 2020). In Proceedings of the 3rd International Work- shop on Natural Language Generation from the Se- mantic Web (WebNLG+), pages 55-76, Dublin, Ire- land (Virtual). Association for Computational Lin- guistics.</p>
<p>Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Chris Thiago Castro Ferreira, Van Der Lee, Emiel Emiel Van Miltenburg, Krahmer, 10.18653/v1/D19-1052Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)HongThiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, and Emiel Krahmer. 2019. Neu- ral data-to-text generation: A comparison between pipeline and end-to-end architectures. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 552-562, Hong</p>
<p>Association for Computational Linguistics. China Kong, Kong, China. Association for Computational Lin- guistics.</p>
<p>Kgpt: Knowledge-grounded pretraining for data-to-text generation. Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang, arXiv:2010.02307arXiv preprintWenhu Chen, Yu Su, Xifeng Yan, and William Yang Wang. 2020. Kgpt: Knowledge-grounded pre- training for data-to-text generation. arXiv preprint arXiv:2010.02307.</p>
<p>Scalable knowledge graph construction from text collections. Ryan Clancy, F Ihab, Jimmy Ilyas, Lin, 10.18653/v1/D19-6607Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER). the Second Workshop on Fact Extraction and VERification (FEVER)Hong Kong, ChinaAssociation for Computational LinguisticsRyan Clancy, Ihab F. Ilyas, and Jimmy Lin. 2019. Scal- able knowledge graph construction from text collec- tions. In Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER), pages 39-46, Hong Kong, China. Association for Compu- tational Linguistics.</p>
<p>Transformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, V Quoc, Ruslan Le, Salakhutdinov, arXiv:1901.02860arXiv preprintZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car- bonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language mod- els beyond a fixed-length context. arXiv preprint arXiv:1901.02860.</p>
<p>Question answering on knowledge bases and text using universal schema and memory networks. Rajarshi Das, Manzil Zaheer, Siva Reddy, Andrew Mccallum, arXiv:1704.08384arXiv preprintRajarshi Das, Manzil Zaheer, Siva Reddy, and Andrew McCallum. 2017. Question answer- ing on knowledge bases and text using universal schema and memory networks. arXiv preprint arXiv:1704.08384.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.</p>
<p>Ondřej Dušek, Jekaterina Novikova, Verena Rieser, arXiv:1810.01170Findings of the e2e nlg challenge. arXiv preprintOndřej Dušek, Jekaterina Novikova, and Verena Rieser. 2018. Findings of the e2e nlg challenge. arXiv preprint arXiv:1810.01170.</p>
<p>T-REx: A large scale alignment of natural language with knowledge base triples. Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, Elena Simperl, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)Miyazaki, JapanEuropean Language Resources Association (ELRAHady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. T-REx: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh Interna- tional Conference on Language Resources and Eval- uation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).</p>
<p>Open information extraction from the web. Oren Etzioni, Michele Banko, Stephen Soderland, Daniel S Weld, Communications of the ACM. 5112Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extrac- tion from the web. Communications of the ACM, 51(12):68-74.</p>
<p>Entities as experts: Sparse memory access with entity supervision. Thibault Févry, Baldini Livio, Nicholas Soares, Eunsol Fitzgerald, Tom Choi, Kwiatkowski, arXiv:2004.07202arXiv preprintThibault Févry, Livio Baldini Soares, Nicholas FitzGer- ald, Eunsol Choi, and Tom Kwiatkowski. 2020. En- tities as experts: Sparse memory access with entity supervision. arXiv preprint arXiv:2004.07202.</p>
<p>The WebNLG challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/W17-3518Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationSantiago de Compostela, SpainAssociation for Computational LinguisticsClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Pro- ceedings of the 10th International Conference on Natural Language Generation, pages 124-133, San- tiago de Compostela, Spain. Association for Compu- tational Linguistics.</p>
<p>Using natural-language processing to produce weather forecasts. E Goldberg, N Driedger, R I Kittredge, IEEE Expert. 92E. Goldberg, N. Driedger, and R. I. Kittredge. 1994. Using natural-language processing to produce weather forecasts. IEEE Expert, 9(2):45-53.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, arXiv:2002.08909Realm: Retrievalaugmented language model pre-training. arXiv preprintKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Ming-Wei Chang. 2020. Realm: Retrieval- augmented language model pre-training. arXiv preprint arXiv:2002.08909.</p>
<p>Learning beyond datasets: Knowledge graph augmented neural networks for natural language processing. K M Annervaz, Somnath Basu Roy Chowdhury, Ambedkar Dukkipati, 10.18653/v1/N18-1029Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaLong PapersAssociation for Computational LinguisticsAnnervaz K M, Somnath Basu Roy Chowdhury, and Ambedkar Dukkipati. 2018. Learning beyond datasets: Knowledge graph augmented neural net- works for natural language processing. In Proceed- ings of the 2018 Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol- ume 1 (Long Papers), pages 313-322, New Orleans, Louisiana. Association for Computational Linguis- tics.</p>
<p>Text-to-text pre-training for data-totext tasks. Mihir Kale, arXiv:2005.10433arXiv preprintMihir Kale. 2020. Text-to-text pre-training for data-to- text tasks. arXiv preprint arXiv:2005.10433.</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, Wentau Yih, arXiv:2004.04906arXiv preprintVladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen- tau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.</p>
<p>Text Generation from Knowledge Graphs with Graph Transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, 10.18653/v1/N19-1238Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text Generation from Knowledge Graphs with Graph Transformers. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2284-2293, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Design of a knowledge-based report generator. Karen Kukich, 10.3115/981311.98134021st Annual Meeting of the Association for Computational Linguistics. Cambridge, Massachusetts, USAAssociation for Computational LinguisticsKaren Kukich. 1983. Design of a knowledge-based re- port generator. In 21st Annual Meeting of the As- sociation for Computational Linguistics, pages 145- 150, Cambridge, Massachusetts, USA. Association for Computational Linguistics.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics. Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav PetrovKenton LeeTom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu- ral questions: a benchmark for question answering research. Transactions of the Association of Compu- tational Linguistics.</p>
<p>Neural text generation from structured data with application to the biography domain. Rémi Lebret, David Grangier, Michael Auli, 10.18653/v1/D16-1128Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsRémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213, Austin, Texas. Association for Computational Lin- guistics.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, arXiv:2005.11401Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprintPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock- täschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401.</p>
<p>Barack's wife hillary: Using knowledge graphs for fact-aware language modeling. Robert Logan, Nelson F Liu, Matthew E Peters, Matt Gardner, Sameer Singh, 10.18653/v1/P19-1598Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsRobert Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, and Sameer Singh. 2019. Barack's wife hillary: Using knowledge graphs for fact-aware lan- guage modeling. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics, pages 5962-5971, Florence, Italy. Associa- tion for Computational Linguistics.</p>
<p>Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings. Thomas Manzini, Yao Lim, Alan W Chong, Yulia Black, Tsvetkov, 10.18653/v1/N19-1062Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsThomas Manzini, Lim Yao Chong, Alan W Black, and Yulia Tsvetkov. 2019. Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings. In Proceed- ings of the 2019 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 615-621, Minneapo- lis, Minnesota. Association for Computational Lin- guistics.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Com- putational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>P Ankur, Xuezhi Parikh, Sebastian Wang, Manaal Gehrmann, Bhuwan Faruqui, Dhingra, arXiv:2004.14373Diyi Yang, and Dipanjan Das. 2020. Totto: A controlled table-to-text generation dataset. arXiv preprintAnkur P Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. Totto: A controlled table-to-text generation dataset. arXiv preprint arXiv:2004.14373.</p>
<p>Deep contextualized word representations. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, 10.18653/v1/N18-1202Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics1Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 2227-2237, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Knowledge enhanced contextual word representations. Matthew E Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, Noah A Smith, 10.18653/v1/D19-1005Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsMatthew E. Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge enhanced contextual word representations. In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 43-54, Hong Kong, China. Associ- ation for Computational Linguistics.</p>
<p>Association for Computational Linguistics. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaLanguage models as knowledge bases?Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowl- edge bases? In Proceedings of the 2019 Confer- ence on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 2463-2473, Hong Kong, China. As- sociation for Computational Linguistics.</p>
<p>Dart: Open-domain structured data record to text generation. Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Nazneen Fatema Rajani, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, arXiv:2007.02871arXiv preprintDragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Nazneen Fatema Ra- jani, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. 2020. Dart: Open-domain structured data record to text generation. arXiv preprint arXiv:2007.02871.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the lim- its of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>SQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Relation extraction with matrix factorization and universal schemas. Sebastian Riedel, Limin Yao, Andrew Mccallum, Benjamin M Marlin, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAtlanta, GeorgiaAssociation for Computational LinguisticsSebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In Pro- ceedings of the 2013 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 74-84, Atlanta, Georgia. Association for Computa- tional Linguistics.</p>
<p>How much knowledge can you pack into the parameters of a language model. Adam Roberts, Colin Raffel, Noam Shazeer, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsAdam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the param- eters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418-5426, Online. Association for Computational Linguistics.</p>
<p>The woman worked as a babysitter: On biases in language generation. Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng, 10.18653/v1/D19-1339Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsEmily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in language generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 3407- 3412, Hong Kong, China. Association for Computa- tional Linguistics.</p>
<p>Handling rare items in data-to-text generation. Anastasia Shimorina, Claire Gardent, 10.18653/v1/W18-6543Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg University, The NetherlandsAssociation for Computational LinguisticsAnastasia Shimorina and Claire Gardent. 2018. Han- dling rare items in data-to-text generation. In Proceedings of the 11th International Conference on Natural Language Generation, pages 360-370, Tilburg University, The Netherlands. Association for Computational Linguistics.</p>
<p>Representing general relational knowledge in Concept-Net 5. Robyn Speer, Catherine Havasi, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12). the Eighth International Conference on Language Resources and Evaluation (LREC'12)Istanbul, TurkeyEuropean Language Resources Association (ELRARobyn Speer and Catherine Havasi. 2012. Repre- senting general relational knowledge in Concept- Net 5. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pages 3679-3686, Istanbul, Turkey. Eu- ropean Language Resources Association (ELRA).</p>
<p>PullNet: Open domain question answering with iterative retrieval on knowledge bases and text. Haitian Sun, Tania Bedrax-Weiss, William Cohen, 10.18653/v1/D19-1242Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsHaitian Sun, Tania Bedrax-Weiss, and William Cohen. 2019. PullNet: Open domain question answering with iterative retrieval on knowledge bases and text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 2380- 2390, Hong Kong, China. Association for Computa- tional Linguistics.</p>
<p>Automated learning of templates for data-to-text generation: comparing rule-based, statistical and neural methods. Chris Van Der Lee, Emiel Krahmer, Sander Wubben, 10.18653/v1/W18-6504Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg University, The Netherlands. Association for Computational LinguisticsChris van der Lee, Emiel Krahmer, and Sander Wubben. 2018. Automated learning of templates for data-to-text generation: comparing rule-based, statistical and neural methods. In Proceedings of the 11th International Conference on Natural Lan- guage Generation, pages 35-45, Tilburg Univer- sity, The Netherlands. Association for Computa- tional Linguistics.</p>
<p>Pat Verga, Haitian Sun, Baldini Livio, William W Soares, Cohen, arXiv:2007.00849Facts as experts: Adaptable and interpretable neural memory over symbolic knowledge. arXiv preprintPat Verga, Haitian Sun, Livio Baldini Soares, and William W Cohen. 2020. Facts as experts: Adapt- able and interpretable neural memory over symbolic knowledge. arXiv preprint arXiv:2007.00849.</p>
<p>Wikidata: A free collaborative knowledge base. Denny Vrandečić, Markus Krötzsch, Communications of the ACM. 57Denny Vrandečić and Markus Krötzsch. 2014. Wiki- data: A free collaborative knowledge base. Commu- nications of the ACM, 57:78-85.</p>
<p>Challenges in data-to-document generation. Sam Wiseman, Stuart Shieber, Alexander Rush, 10.18653/v1/D17-1239Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsSam Wiseman, Stuart Shieber, and Alexander Rush. 2017. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing, pages 2253-2263, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Learning neural templates for text generation. Sam Wiseman, M Stuart, Alexander M Shieber, Rush, arXiv:1808.10122arXiv preprintSam Wiseman, Stuart M Shieber, and Alexander M Rush. 2018. Learning neural templates for text gen- eration. arXiv preprint arXiv:1808.10122.</p>
<p>Improving question answering over incomplete KBs with knowledgeaware reader. Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, William Yang Wang, 10.18653/v1/P19-1417Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsWenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and William Yang Wang. 2019. Improving question answering over incomplete KBs with knowledge- aware reader. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics, pages 4258-4264, Florence, Italy. Associa- tion for Computational Linguistics.</p>
<p>Donghan Yu, Chenguang Zhu, Yiming Yang, Michael Zeng, arXiv:2010.00796Jaket: Joint pre-training of knowledge graph and language understanding. arXiv preprintDonghan Yu, Chenguang Zhu, Yiming Yang, and Michael Zeng. 2020. Jaket: Joint pre-training of knowledge graph and language understanding. arXiv preprint arXiv:2010.00796.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Q Kilian, Yoav Weinberger, Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. arXiv preprintTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Eval- uating text generation with bert. arXiv preprint arXiv:1904.09675.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8313 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8313</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8313</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278789559</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.15993v1.pdf" target="_blank">Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku</a></p>
                <p><strong>Paper Abstract:</strong> The success of Large Language Models (LLMs) in human-AI collaborative decision-making hinges on their ability to provide trustworthy, gradual, and tailored explanations. Solving complex puzzles, such as Sudoku, offers a canonical example of this collaboration, where clear and customized explanations often hold greater importance than the final solution. In this study, we evaluate the performance of five LLMs in solving and explaining \sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solving puzzles, none can explain the solution process in a manner that reflects strategic reasoning or intuitive problem-solving. These findings underscore significant challenges that must be addressed before LLMs can become effective partners in human-AI collaborative decision-making.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8313.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8313.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned, open-source large language model from the Llama-3.1 family evaluated zero-shot on 6×6 Sudoku puzzles; included to test multi-step logical reasoning and explanation capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned transformer model (Llama-3.1 family) used off-the-shelf in zero-shot mode to attempt 6×6 Sudoku solves; accessed via Hugging Face Chat API in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>6×6 Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>grid-based logic puzzle (2×3 boxes) requiring integration of row, column and box constraints — spatial/relational grid reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot textual prompting: input is a 6×6 grid with zeros for blanks (six lines of six integers). Prompt instructs to solve via logical reasoning, no code, and to return completed grid. Temperature=0, top-p=0.5.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No chain-of-thought or symbolic tool integration used; model run in zero-shot instruction-following mode. Paper reports open-source models tended to prioritize row/column constraints over box constraints, indicating heuristic/local pattern exploitation rather than explicit global constraint integration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated on full dataset (2,293 puzzles) together with other open-source models; open-source models collectively achieved 0.04% fully correct solutions (i.e., near-zero complete-solution accuracy). Other metrics (cell/row/column/box accuracies) reported lower performance versus closed model but specific per-model breakdown not given in paper for this model alone.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No direct evidence that the model uses robust spatial reasoning; observed failure mode is inability to integrate all grid constraints simultaneously and a bias toward satisfying row/column constraints before box constraints, implying lack of consistent global spatial constraint reasoning. No probing/ablation to demonstrate true spatial representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against other open-source models (Llama-3.1-8B-Instruct, Gemma-2-9B-Instruct, Mistral-7B-Instruct-v0.3) on the full dataset — collectively these open-source models performed very poorly (0.04% full solutions). Also compared qualitatively to OpenAI o1-preview on a 100-puzzle subset, where o1-preview performed substantially better.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failed to produce complete correct solutions on essentially all puzzles tested (near-zero full-solution accuracy). Reported failure: inability to integrate box constraints reliably; explanations were not produced/analyzed for this model in the paper. Evaluation was zero-shot with no fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8313.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8313.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller instruction-tuned member of the Llama-3.1 family (8B parameters), evaluated zero-shot for solving 6×6 Sudoku puzzles to assess scale-dependent puzzle reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned transformer model from the Llama-3.1 family used without fine-tuning in zero-shot experiments via Hugging Face Chat API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>6×6 Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>grid-based logic puzzle (2×3 boxes) requiring grid/spatial relational reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same zero-shot textual prompt as other models: six-line grid with zeros for empties; instruction to solve logically and return final grid. Temperature=0, top-p=0.5.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot instruction compliance; no symbolic solver integration or chain-of-thought prompting used. Paper notes open-source models showed a tendency to prioritize row/column constraints over box constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Included in open-source evaluation on 2,293 puzzles; open-source models (including this one) achieved 0.04% fully correct solutions overall; individual per-model breakdown not separately enumerated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No positive evidence; performance indicates failures to consistently enforce spatial constraints across the grid. Paper reports general inability of open-source models to integrate all constraints simultaneously.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against larger open-source Llama-3.1-70B-Instruct, Gemma-2-9B-Instruct, Mistral-7B-Instruct-v0.3 (same full-dataset evaluation), and to OpenAI o1-preview on a smaller subset (o1-preview outperformed open-source models).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Near-zero rate of fully correct solutions; model fails on global constraint integration (boxes). Experiment was zero-shot with no fine-tuning; exact per-model error modes not deeply analyzed beyond aggregate open-source failure patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8313.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8313.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-2-9B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma-2-9B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned, open-source 9B-parameter model (Gemma-2 family) evaluated zero-shot on 6×6 Sudoku as part of multi-model comparison of puzzle-solving abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-9B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source, instruction-tuned transformer (Gemma-2 family) used in zero-shot mode to produce Sudoku solutions; run on GroqCloud as described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>6×6 Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>grid-based logic puzzle requiring relational/spatial constraint reasoning across rows, columns, and 2×3 boxes</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot prompt with grid as textual input; requested final completed grid only. Models run with deterministic sampling (temperature=0) and top-p=0.5.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No explicit chain-of-thought or symbolic backend. Reported behavior aligned with other open-source models—local pattern/row/column satisfaction bias rather than consistent global box constraint enforcement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Part of the open-source models evaluated on 2,293 puzzles; open-source group achieved 0.04% fully correct solutions overall. Per-model breakdown not specified beyond group aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Paper provides no evidence that Gemma-2 uses structured spatial reasoning; observed failures point to inability to reconcile global spatial constraints simultaneously.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared in aggregate to other open-source models and to o1-preview (closed model) on a 100-puzzle subset where o1-preview substantially outperformed open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failed to produce reliable complete solutions; prioritized row/column constraints and neglected box constraints, indicating limitations in capturing global spatial relationships. No fine-tuning was applied in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8313.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8313.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B-Instruct-v0.3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-Instruct-v0.3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter instruction-tuned open-source model (Mistral family), evaluated zero-shot to probe whether modest-size LLMs can solve and explain 6×6 Sudoku puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct-v0.3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned transformer model from the Mistral family used in zero-shot experiments via the Hugging Face Chat API; intended to test reasoning abilities at moderate parameter scale.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>6×6 Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>grid-based logic puzzle (2×3 boxes) that requires spatial/relational constraint reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot prompt: textual 6×6 grid with zeros for empty cells; asked to logically solve without using code and return the final grid. Temperature=0, top-p=0.5.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No chain-of-thought or integration with symbolic solvers during evaluation; observed heuristic/local constraint satisfaction (rows/columns) rather than consistent global reasoning over boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Included in the open-source models evaluated on the full dataset (2,293 puzzles); overall open-source fully-correct solution rate was 0.04%. Per-model metrics not individually reported beyond aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No direct evidence of structured spatial reasoning; failures to enforce box constraints indicate lack of robust global spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared in the group of open-source models and contrasted with OpenAI o1-preview on a 100-puzzle subset (o1-preview performed much better).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failed to produce reliable complete solutions; demonstrated inability to integrate all Sudoku constraints. No fine-tuning or chain-of-thought prompting was used, limiting capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8313.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8313.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed/proprietary OpenAI model (referred to as o1-preview) that substantially outperformed evaluated open-source LLMs on 6×6 Sudoku solving accuracy but produced poor, often unfaithful explanations of its solving process.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1-preview (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed/proprietary OpenAI model accessed via OpenAI's user interface; evaluated zero-shot for solving and explaining 6×6 Sudoku puzzles. Paper does not disclose architecture or parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>6×6 Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>grid-based logic puzzle (2×3 boxes) requiring reasoning about placements across rows, columns and boxes (spatial/relational grid reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot prompting on a subset of 100 puzzles for solving accuracy and a further subset of 20 puzzles for explanation evaluation. Input: textual 6×6 grid; instructions: solve logically, no code. Temperature=0, top-p=0.5. Explanations solicited in natural language and evaluated by human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No explicit chain-of-thought or symbolic backend was reported in these experiments; model was run in deterministic setting. The paper analyzes behavioral patterns (e.g., difficulty integrating global constraints) rather than proposing internal mechanisms. Explanations generated were natural-language but often lacked faithful, stepwise deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Solving: On a random 100-puzzle subset, o1-preview achieved 65% full-solution correctness. Per-difficulty: 100% accuracy on Easy (ER 1.0-1.2) and Medium (ER 1.5); performance degrades with difficulty — for Diabolical puzzles (ER 6.2+), cell-wise accuracy dropped to 57.2% and complete-solution accuracy to 40%. Across difficulty levels cell-wise accuracy ranged 57–100%. Explanation quality (20-puzzle subset): Justification rated 'Yes' in 5% of assessments (52.5% 'Maybe', 42.5% 'No'); Clarity 'Yes' 7.5% (32.5% 'Maybe', 60% 'No'); Educational value 'Yes' 2.5% (52.5% 'Maybe', 45% 'No'). Inter-annotator weighted Kappa: 0.6 (educational value), 0.6 (justification), 0.4 (clarity).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Mixed/negative: While o1-preview can often produce correct final grids (especially on easier puzzles), the paper reports no evidence of faithful, human-like spatial reasoning steps. Empirical evidence indicates the model struggles to integrate multiple global constraints (boxes) as difficulty increases, suggesting solutions may be produced without transparent spatial deductive traces. Explanations rarely justified placements or conveyed generalizable solving strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to four open-source models on puzzle-solving (open-source group had near-zero full-solution rates); o1-preview substantially outperformed them on the 100-puzzle subset. Explanations were evaluated against human expert judgment (three evaluators) and found deficient. Paper also contrasts pure LLM approaches with hybrid symbolic methods (e.g., PuzzleLM + Z3) in related work as potential alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Explanation failures: even when solving correctly, o1-preview typically failed to provide faithful, stepwise justifications, misused basic terminology, and lacked systematic progression; educational value of explanations was very low. Performance degrades with puzzle difficulty — especially on puzzles requiring forcing chains or sophisticated integration of global constraints. Evaluation used a limited sample (100 puzzles for solving, 20 for explanations) and models were not fine-tuned for the task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Assessing logical puzzle solving in large language models: Insights from a minesweeper case study <em>(Rating: 2)</em></li>
                <li>PuzzleLM <em>(Rating: 2)</em></li>
                <li>Puzzlebench: Can llms solve challenging first-order combinatorial reasoning problems? <em>(Rating: 2)</em></li>
                <li>Puzzle solving using reasoning of large language models: A survey <em>(Rating: 2)</em></li>
                <li>PuzzleVQA: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8313",
    "paper_id": "paper-278789559",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "Llama-3.1-70B-Instruct",
            "name_full": "Llama-3.1-70B-Instruct",
            "brief_description": "An instruction-tuned, open-source large language model from the Llama-3.1 family evaluated zero-shot on 6×6 Sudoku puzzles; included to test multi-step logical reasoning and explanation capabilities.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-70B-Instruct",
            "model_description": "Open-source instruction-tuned transformer model (Llama-3.1 family) used off-the-shelf in zero-shot mode to attempt 6×6 Sudoku solves; accessed via Hugging Face Chat API in the experiments.",
            "model_size": "70B",
            "puzzle_name": "6×6 Sudoku",
            "puzzle_type": "grid-based logic puzzle (2×3 boxes) requiring integration of row, column and box constraints — spatial/relational grid reasoning",
            "task_setup": "Zero-shot textual prompting: input is a 6×6 grid with zeros for blanks (six lines of six integers). Prompt instructs to solve via logical reasoning, no code, and to return completed grid. Temperature=0, top-p=0.5.",
            "mechanisms_or_strategies": "No chain-of-thought or symbolic tool integration used; model run in zero-shot instruction-following mode. Paper reports open-source models tended to prioritize row/column constraints over box constraints, indicating heuristic/local pattern exploitation rather than explicit global constraint integration.",
            "performance_metrics": "Evaluated on full dataset (2,293 puzzles) together with other open-source models; open-source models collectively achieved 0.04% fully correct solutions (i.e., near-zero complete-solution accuracy). Other metrics (cell/row/column/box accuracies) reported lower performance versus closed model but specific per-model breakdown not given in paper for this model alone.",
            "evidence_of_spatial_reasoning": "No direct evidence that the model uses robust spatial reasoning; observed failure mode is inability to integrate all grid constraints simultaneously and a bias toward satisfying row/column constraints before box constraints, implying lack of consistent global spatial constraint reasoning. No probing/ablation to demonstrate true spatial representations.",
            "comparisons": "Compared against other open-source models (Llama-3.1-8B-Instruct, Gemma-2-9B-Instruct, Mistral-7B-Instruct-v0.3) on the full dataset — collectively these open-source models performed very poorly (0.04% full solutions). Also compared qualitatively to OpenAI o1-preview on a 100-puzzle subset, where o1-preview performed substantially better.",
            "limitations_or_failure_cases": "Failed to produce complete correct solutions on essentially all puzzles tested (near-zero full-solution accuracy). Reported failure: inability to integrate box constraints reliably; explanations were not produced/analyzed for this model in the paper. Evaluation was zero-shot with no fine-tuning.",
            "uuid": "e8313.0",
            "source_info": {
                "paper_title": "Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Llama-3.1-8B-Instruct",
            "name_full": "Llama-3.1-8B-Instruct",
            "brief_description": "A smaller instruction-tuned member of the Llama-3.1 family (8B parameters), evaluated zero-shot for solving 6×6 Sudoku puzzles to assess scale-dependent puzzle reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct",
            "model_description": "Open-source instruction-tuned transformer model from the Llama-3.1 family used without fine-tuning in zero-shot experiments via Hugging Face Chat API.",
            "model_size": "8B",
            "puzzle_name": "6×6 Sudoku",
            "puzzle_type": "grid-based logic puzzle (2×3 boxes) requiring grid/spatial relational reasoning",
            "task_setup": "Same zero-shot textual prompt as other models: six-line grid with zeros for empties; instruction to solve logically and return final grid. Temperature=0, top-p=0.5.",
            "mechanisms_or_strategies": "Zero-shot instruction compliance; no symbolic solver integration or chain-of-thought prompting used. Paper notes open-source models showed a tendency to prioritize row/column constraints over box constraints.",
            "performance_metrics": "Included in open-source evaluation on 2,293 puzzles; open-source models (including this one) achieved 0.04% fully correct solutions overall; individual per-model breakdown not separately enumerated in the paper.",
            "evidence_of_spatial_reasoning": "No positive evidence; performance indicates failures to consistently enforce spatial constraints across the grid. Paper reports general inability of open-source models to integrate all constraints simultaneously.",
            "comparisons": "Compared against larger open-source Llama-3.1-70B-Instruct, Gemma-2-9B-Instruct, Mistral-7B-Instruct-v0.3 (same full-dataset evaluation), and to OpenAI o1-preview on a smaller subset (o1-preview outperformed open-source models).",
            "limitations_or_failure_cases": "Near-zero rate of fully correct solutions; model fails on global constraint integration (boxes). Experiment was zero-shot with no fine-tuning; exact per-model error modes not deeply analyzed beyond aggregate open-source failure patterns.",
            "uuid": "e8313.1",
            "source_info": {
                "paper_title": "Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Gemma-2-9B-Instruct",
            "name_full": "Gemma-2-9B-Instruct",
            "brief_description": "An instruction-tuned, open-source 9B-parameter model (Gemma-2 family) evaluated zero-shot on 6×6 Sudoku as part of multi-model comparison of puzzle-solving abilities.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemma-2-9B-Instruct",
            "model_description": "Open-source, instruction-tuned transformer (Gemma-2 family) used in zero-shot mode to produce Sudoku solutions; run on GroqCloud as described in the paper.",
            "model_size": "9B",
            "puzzle_name": "6×6 Sudoku",
            "puzzle_type": "grid-based logic puzzle requiring relational/spatial constraint reasoning across rows, columns, and 2×3 boxes",
            "task_setup": "Zero-shot prompt with grid as textual input; requested final completed grid only. Models run with deterministic sampling (temperature=0) and top-p=0.5.",
            "mechanisms_or_strategies": "No explicit chain-of-thought or symbolic backend. Reported behavior aligned with other open-source models—local pattern/row/column satisfaction bias rather than consistent global box constraint enforcement.",
            "performance_metrics": "Part of the open-source models evaluated on 2,293 puzzles; open-source group achieved 0.04% fully correct solutions overall. Per-model breakdown not specified beyond group aggregate.",
            "evidence_of_spatial_reasoning": "Paper provides no evidence that Gemma-2 uses structured spatial reasoning; observed failures point to inability to reconcile global spatial constraints simultaneously.",
            "comparisons": "Compared in aggregate to other open-source models and to o1-preview (closed model) on a 100-puzzle subset where o1-preview substantially outperformed open-source models.",
            "limitations_or_failure_cases": "Failed to produce reliable complete solutions; prioritized row/column constraints and neglected box constraints, indicating limitations in capturing global spatial relationships. No fine-tuning was applied in experiments.",
            "uuid": "e8313.2",
            "source_info": {
                "paper_title": "Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Mistral-7B-Instruct-v0.3",
            "name_full": "Mistral-7B-Instruct-v0.3",
            "brief_description": "A 7B-parameter instruction-tuned open-source model (Mistral family), evaluated zero-shot to probe whether modest-size LLMs can solve and explain 6×6 Sudoku puzzles.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct-v0.3",
            "model_description": "Open-source instruction-tuned transformer model from the Mistral family used in zero-shot experiments via the Hugging Face Chat API; intended to test reasoning abilities at moderate parameter scale.",
            "model_size": "7B",
            "puzzle_name": "6×6 Sudoku",
            "puzzle_type": "grid-based logic puzzle (2×3 boxes) that requires spatial/relational constraint reasoning",
            "task_setup": "Zero-shot prompt: textual 6×6 grid with zeros for empty cells; asked to logically solve without using code and return the final grid. Temperature=0, top-p=0.5.",
            "mechanisms_or_strategies": "No chain-of-thought or integration with symbolic solvers during evaluation; observed heuristic/local constraint satisfaction (rows/columns) rather than consistent global reasoning over boxes.",
            "performance_metrics": "Included in the open-source models evaluated on the full dataset (2,293 puzzles); overall open-source fully-correct solution rate was 0.04%. Per-model metrics not individually reported beyond aggregate.",
            "evidence_of_spatial_reasoning": "No direct evidence of structured spatial reasoning; failures to enforce box constraints indicate lack of robust global spatial reasoning.",
            "comparisons": "Compared in the group of open-source models and contrasted with OpenAI o1-preview on a 100-puzzle subset (o1-preview performed much better).",
            "limitations_or_failure_cases": "Failed to produce reliable complete solutions; demonstrated inability to integrate all Sudoku constraints. No fine-tuning or chain-of-thought prompting was used, limiting capabilities.",
            "uuid": "e8313.3",
            "source_info": {
                "paper_title": "Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "o1-preview",
            "name_full": "OpenAI o1-preview",
            "brief_description": "A closed/proprietary OpenAI model (referred to as o1-preview) that substantially outperformed evaluated open-source LLMs on 6×6 Sudoku solving accuracy but produced poor, often unfaithful explanations of its solving process.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "o1-preview (OpenAI)",
            "model_description": "Closed/proprietary OpenAI model accessed via OpenAI's user interface; evaluated zero-shot for solving and explaining 6×6 Sudoku puzzles. Paper does not disclose architecture or parameter count.",
            "model_size": null,
            "puzzle_name": "6×6 Sudoku",
            "puzzle_type": "grid-based logic puzzle (2×3 boxes) requiring reasoning about placements across rows, columns and boxes (spatial/relational grid reasoning)",
            "task_setup": "Zero-shot prompting on a subset of 100 puzzles for solving accuracy and a further subset of 20 puzzles for explanation evaluation. Input: textual 6×6 grid; instructions: solve logically, no code. Temperature=0, top-p=0.5. Explanations solicited in natural language and evaluated by human experts.",
            "mechanisms_or_strategies": "No explicit chain-of-thought or symbolic backend was reported in these experiments; model was run in deterministic setting. The paper analyzes behavioral patterns (e.g., difficulty integrating global constraints) rather than proposing internal mechanisms. Explanations generated were natural-language but often lacked faithful, stepwise deduction.",
            "performance_metrics": "Solving: On a random 100-puzzle subset, o1-preview achieved 65% full-solution correctness. Per-difficulty: 100% accuracy on Easy (ER 1.0-1.2) and Medium (ER 1.5); performance degrades with difficulty — for Diabolical puzzles (ER 6.2+), cell-wise accuracy dropped to 57.2% and complete-solution accuracy to 40%. Across difficulty levels cell-wise accuracy ranged 57–100%. Explanation quality (20-puzzle subset): Justification rated 'Yes' in 5% of assessments (52.5% 'Maybe', 42.5% 'No'); Clarity 'Yes' 7.5% (32.5% 'Maybe', 60% 'No'); Educational value 'Yes' 2.5% (52.5% 'Maybe', 45% 'No'). Inter-annotator weighted Kappa: 0.6 (educational value), 0.6 (justification), 0.4 (clarity).",
            "evidence_of_spatial_reasoning": "Mixed/negative: While o1-preview can often produce correct final grids (especially on easier puzzles), the paper reports no evidence of faithful, human-like spatial reasoning steps. Empirical evidence indicates the model struggles to integrate multiple global constraints (boxes) as difficulty increases, suggesting solutions may be produced without transparent spatial deductive traces. Explanations rarely justified placements or conveyed generalizable solving strategies.",
            "comparisons": "Directly compared to four open-source models on puzzle-solving (open-source group had near-zero full-solution rates); o1-preview substantially outperformed them on the 100-puzzle subset. Explanations were evaluated against human expert judgment (three evaluators) and found deficient. Paper also contrasts pure LLM approaches with hybrid symbolic methods (e.g., PuzzleLM + Z3) in related work as potential alternatives.",
            "limitations_or_failure_cases": "Explanation failures: even when solving correctly, o1-preview typically failed to provide faithful, stepwise justifications, misused basic terminology, and lacked systematic progression; educational value of explanations was very low. Performance degrades with puzzle difficulty — especially on puzzles requiring forcing chains or sophisticated integration of global constraints. Evaluation used a limited sample (100 puzzles for solving, 20 for explanations) and models were not fine-tuned for the task.",
            "uuid": "e8313.4",
            "source_info": {
                "paper_title": "Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Assessing logical puzzle solving in large language models: Insights from a minesweeper case study",
            "rating": 2,
            "sanitized_title": "assessing_logical_puzzle_solving_in_large_language_models_insights_from_a_minesweeper_case_study"
        },
        {
            "paper_title": "PuzzleLM",
            "rating": 2
        },
        {
            "paper_title": "Puzzlebench: Can llms solve challenging first-order combinatorial reasoning problems?",
            "rating": 2,
            "sanitized_title": "puzzlebench_can_llms_solve_challenging_firstorder_combinatorial_reasoning_problems"
        },
        {
            "paper_title": "Puzzle solving using reasoning of large language models: A survey",
            "rating": 2,
            "sanitized_title": "puzzle_solving_using_reasoning_of_large_language_models_a_survey"
        },
        {
            "paper_title": "PuzzleVQA: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns",
            "rating": 1,
            "sanitized_title": "puzzlevqa_diagnosing_multimodal_reasoning_challenges_of_language_models_with_abstract_visual_patterns"
        }
    ],
    "cost": 0.01186375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6×6 Sudoku
21 May 2025</p>
<p>Anirudh Maiya 
University of Colorado Boulder</p>
<p>Razan Alghamdi 
University of Colorado Boulder</p>
<p>Maria Leonor Pacheco 
University of Colorado Boulder</p>
<p>Ashutosh Trivedi 
University of Colorado Boulder</p>
<p>Fabio Somenzi 
University of Colorado Boulder</p>
<p>Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6×6 Sudoku
21 May 20251E662D679630348CA07E90766AA389EDarXiv:2505.15993v1[cs.CL]
The success of Large Language Models (LLMs) in human-AI collaborative decisionmaking hinges on their ability to provide trustworthy, gradual, and tailored explanations.Solving complex puzzles, such as Sudoku, offers a canonical example of this collaboration, where clear and customized explanations often hold greater importance than the final solution.In this study, we evaluate the performance of five LLMs in solving and explaining 6×6 Sudoku puzzles.While one LLM demonstrates limited success in solving puzzles, none can explain the solution process in a manner that reflects strategic reasoning or intuitive problemsolving.These findings underscore significant challenges that must be addressed before LLMs can become effective partners in human-AI collaborative decision-making.</p>
<p>Introduction</p>
<p>The recent success of Large Language Models (LLMs) in automating reasoning tasks using natural language highlights their potential in human-AI collaborative decision-support systems.However, their effectiveness in critical domains-such as safety-critical systems, legally sensitive decisionmaking, medical support systems, and businesscritical processes-depends on their ability to provide step-by-step, consistent, and trustworthy explanations tailored to the prior knowledge and current capabilities of the human partner.Solving complex, multi-step puzzles like Sudoku offers a meaningful example of such decision-making scenarios.These puzzles share several key challengesincluding step-by-step reasoning, the need for tailored explanations, and the importance of maintaining consistency-but without the risks associated with making incorrect critical decisions.Sudoku thus provides a controlled environment to study the capabilities of LLMs in addressing these challenges while isolating them from high-stakes con-sequences.This paper explores whether LLMs can effectively navigate these decision-making challenges by focusing on their performance in solving and explaining Sudoku problems.</p>
<p>The ability of LLMs to reason and solve problems has become a key area of interest in the field of artificial intelligence (Huang and Chang, 2023;Yin et al., 2024).Although LLMs have shown remarkable performance in tasks involving language generation, translation, and comprehension (Brown et al., 2020;Wei et al., 2024), their ability to handle more structured reasoning tasks is less understood (Mirzadeh et al., 2024).In the last year, several studies have attempted to test the abilities of LLMs to solve complex puzzles (Chia et al., 2024;Li et al., 2024;Giadikiaroglou et al., 2024a).While these models are increasingly capable of generating correct answers, the explanations they provide for their reasoning process often fall short (Turpin et al., 2024).We argue that good reasoners should be able to clearly expose their deductive processes.Therefore, reasoning explanations should be faithful, provide enough support for individual decisions, and provide insight into the process they took to arrive at any given solution.</p>
<p>While we can obtain correct answers and faithful reasoning steps for many puzzles with symbolic reasoners (de Moura and Bjørner, 2011), it can be hard for humans to make sense of the explanations provided by these tools.These explanations usually come in the form of fine-grained proofs that require a good grasp of proof terminology and are often overwhelmingly detailed.They often refer to encodings of the problem that are cumbersome to relate to the original presentation of the puzzle.To be useful in practical scenarios, explanations need to be easily understandable to humans (Gilpin et al., 2018).Given that LLMs excel at producing human-like output (Ouyang et al., 2022), we are interested in studying the challenges and opportunities of LLMs for explaining puzzles solutions.</p>
<p>Research Questions and Findings</p>
<p>We investigate the explanation capabilities of LLMs in the context of 6×6 Sudoku puzzles.Sudoku is a widely recognized logic-based puzzle (Rosenhouse and Taalman, 2011), which provides an ideal benchmark to explore these facets of LLMs.Solving Sudoku requires deductive reasoning and rule-based elimination, skills that go beyond simple pattern recognition.We choose 6 × 6 Sudoku because, while not being overly difficult, it often requires a few nontrivial reasoning steps to be solved.With our study, we aim to answer the following two research questions: (1) LLMs solve 6×6 Sudoku puzzles?(2) Can LLMs successfully explain the steps needed to arrive to a given solution?To answer these questions, we construct a dataset of 2,293 6×6 Sudoku puzzles with their solutions, and show that open-source models can correctly solve less than 1% of the puzzles, while the latest Open AI model can solve 65% of them.Then, select a subset of 20 puzzles spanning different difficulty levels to evaluate explanation capabilities using a structured survey, and show that explanations fall short for the best performing models.</p>
<p>Contributions</p>
<p>This study contributes to a deeper understanding of the (in)ability of LLMs to articulate the reasoning steps needed to explain a particular solution to multi-step reasoning tasks.We show that while these tools show promise (as evidenced by the increase performance for newer models), they still have significant challenges in exposing reasoning steps in a way that is both faithful and insightful.Finally, we outline a research agenda for combining LLMs with logic-based reasoning to address some of these challenges 1 .</p>
<p>Related Work</p>
<p>The challenges that LLMs face in solving puzzles that require complex reasoning have been well-documented (Giadikiaroglou et al., 2024b), with most methods struggling to successfully produce consistent results.In response to these challenges, methods that combine symbolic reasoning and LLMs have recently received attention with varying success (Mittal et al., 2024;Jiang et al., 2022;Xin et al., 2024).PuzzleLM (Mittal et al., 2024) first prompts an LLM to generate an instanceagnostic program capable of handling various problem classes, such as Sudoku, N-Queens, graph coloring, etc.The program then converts the problem 1 All data and code will be released.</p>
<p>instance into SMT constraints and passes them to an external solver like Z3.The solver processes these constraints and returns a solution, which the program then converts to the desired output format.Xin et al. (2024) introduces a method for generating large-scale formal proof data from informal math problems.It fine-tunes an LLM, translates informal problems into formal Lean 4 statements, filters low-quality formalizations using a chain-ofthought evaluation, and attempts to prove both the formal statement and its negation.This iterative process refines the model's performance by incorporating newly generated data to enhance subsequent rounds of statement generation.</p>
<p>However, significant less attention has been given to the problem of whether LLMs can produce useful explanations (in form of reasoning steps) for the solutions that they provide.In this work, we study this question in the context of 6x6 sudoku and show that, while LLMs have improved their capacity of producing correct solutions, generating explanations continues to be challenging.</p>
<p>Task Formulation</p>
<p>This study assesses the problem-solving capabilities of LLMs using 6 × 6 Sudoku puzzles.We evaluate two key aspects: first, the ability of LLMs to generate correct solutions for puzzles of varying difficulty, and second, their capability to provide meaningful explanations for how these solutions are derived.These complementary tasks allow us to assess both the computational accuracy and reasoning transparency of LLMs in structured problemsolving scenarios.</p>
<p>Dataset We generated 2,293 6×6 Sudoku puzzles using the Z3 solver.Z3 decides the satisfiability of sentences of many decidable fragments of firstorder logic.It combines the CDCL algorithm for propositional satisfiability with solvers for conjunctions of literals for background theories like linear arithmetic.</p>
<p>Puzzle Generation Process In our Sudoku generation process, each puzzle is modeled as a grid of cells, where every row, column, and 2×3 box must contain the numbers 1 to 6 without repetition.To do this, the generator initially evaluates random seeds, that is, partially filled grids with 11 givens, until Z3 confirms the existence of a solution (not necessarily unique).Once a solved grid is found, givens are removed from it in random order until none can be removed lest the uniqueness of the solution should be lost.Z3 is used to check that the solution remains unique.Namely, if x is the value in Row y Column z that is a candidate for removal, the constraint that asserts rycz = x is replaced by rycz ̸ = x.If the resulting constraints are unsatisfiable, the candidate can be dropped.This results in a dataset whose every puzzle has a unique solution and is minimal, in the sense that removal of any clue introduces additional solutions.</p>
<p>The minimum number of givens for a 6x6 puzzle is 8 (McGuire et al., 2014).While our generation process does not guarantee uniform sampling from all 28,200,960 valid 6x6 grids, we get essentially the same statistics for the number of givens when we start from a uniform random sample of grids.(We do not sample from the uniform distribution of grids in our generator because the approach is computationally infeasible for grids larger than 6x6.)Consequently, approximately 96% of our resulting puzzles have between 9 and 11 givens.To assess the representativeness of our sample, we compare with publicly available benchmarks 2 , which show the following distribution for a sample of 323,231 symmetrical 6x6 puzzles: 0.2% puzzles with 8 givens; approx.88.7% with 10 or 12 givens, 11.1% with 14 or more givens.Symmetrical puzzles have an even number of givens.Moreover, the puzzles in the Menneske data set with more than 8 givens, unlike ours, have redundant givens.Both differences contribute to the higher average number of givens when compared to our sample.</p>
<p>Assessing puzzle difficulty To assess puzzle difficulty, we employ the widely-accepted Explainer Rating (ER) system, which rates puzzles based on the most complex solving technique required for their solution.As shown in Figure 2, the ER scale ranges from 1.0 (basic techniques) to above 6.2 (advanced techniques), with each solving technique assigned a specific rating value.Our puzzles fall into five distinct difficulty categories: Easy (ER 1.0-1.2),Medium (ER 1.5), Hard (ER 1.7-2.5),Fiendish(ER 2.6-6.0) and Diabolical (ER 6.2+).Further details can be found in App. A.</p>
<p>This distribution ensures our dataset spans a wide range of difficulty levels, with 93.5% of puzzles being Easy or Medium difficulty, and 6.5% requiring more advanced solving techniques.This variety allows us to evaluate both basic solving ca- pabilities and more sophisticated reasoning strategies.To assess the reliability of this metric, 20 puzzles with varying numbers of back-jumps were solved and rated by two authors of the paper.The human solvers rated the puzzles on a scale from 1 to 5, with 1 denoting the easiest and 5 being the hardest among the set.Comparing the average rating of the two human solvers against the ER for the same puzzles resulted in a Spearman's Ranking Correlation Coefficient (ρ) of 0.86994.To put it in perspective, the two individual human rankings have a lower (ρ) of 0.8042.This ensured that the ER is reflective of the human perception of a puzzle's difficulty as seen in Fig. 1 4 Experiments</p>
<p>Can LLMs Solve Sudoku Puzzles?We tested the performance of several state-of-the-art LLMs on our 6 × 6 Sudoku puzzles.The open-source models Llama-3.1-70B-Instruct and Llama-3.1-8B-Instruct(Dubey et al., 2024), Gemma-2-9B-Instruct (Rivière et al., 2024) and Mistral-7B-Instruct-v0.3 (Jiang et al., 2023) were evaluated on our full dataset of 2,293 puzzles.Due to the access limitations and rate restrictions of closed proprietary models, we evaluated OpenAI's o1-preview model on a randomly selected subset of 100 puzzles.Each model was tasked with generating solutions and the correctness of their outputs was evaluated at multiple levels: cell-wise, row-wise, column-wise, box-wise, and full accuracy.The prompt used can be found in App.B.</p>
<p>Table 1 shows the results split into two sections: performance on the full dataset (2,293 puzzles) for open-source models, and performance on the 100puzzle subset for all models including o1-preview.Open source models only managed 0.04% fully correct solutions, regardless of their size, showing that they struggle with integrating all constraints across the grid.For these models, column-wise and row-wise constraints were prioritized over box- Performance analysis across difficulty levels reveals that o1-preview achieves 100% accuracy for Easy (ER 1.0-1.2) and Medium (ER 1.5) puzzles, but shows significant degradation as complexity increases.For Diabolical puzzles (ER 6.2+), cellwise accuracy drops to 57.2% and complete solution accuracy to 40%, suggesting the model struggles with integrating multiple constraints as puzzle complexity increases (detailed analysis in App.E) Can LLMs Provide Good Explanations for Solutions?We conducted a detailed evaluation of o1-preview's explanation capabilities on a subset of 20 puzzles (difficulty distribution of these puzzles in App.D, performance analysis in App.E).Three expert evaluators (details on survey participators in App.F) assessed the quality of o1's explanations using a structured survey, with each explanation being evaluated by two assessors.</p>
<p>The evaluation was conducted through surveying the correctness of the final solution, comments on the overall explanation, and a three-point Likert scale (Yes, Maybe, No) assessing three key dimensions: (1) Justification (the explanation provides justification for the solution), ( 2) Clarity (the explanation is easy to follow and detailed), and (3) Educational Value (the explanation promotes understanding of Sudoku solving strategies).For each of the 20 puzzles, two evaluators independently rated o1's explanations across these three dimensions.This resulted in a total of 120 individual assessments (20 puzzles × 3 questions × 2 evaluators).We measured inter-annotator agreement using weighted Kappa and obtained 0.6 for educational value, 0.6 for justification, and 0.4 for clarity, which suggests moderate to good agreement.While o1-preview achieved correct solutions in 65% of cases, the evaluation of its explanations across 20 puzzles revealed significant limitations.Only 5% of the responses indicate the explanation indeed justifies the solution with a "Yes" ratings (52.5% "Maybe", 42.5% "No"); while the model identified correct digit placements, it rarely justified why the placements were were chosen.The clarity dimension scored poorly, with 7.5% "Yes" responses (32.5% "Maybe", 60% "No"), with reported issues regarding logical progression and inconsistent terminology.Educational value proved to be the weakest dimension, with just 2.5% "Yes" responses (52.5% "Maybe", 45% "No"), as explanations failed to convey generalizable solving strategies.The qualitative feedback reinforced these findings -evaluators consistently noted that explanations lacked systematic progression, failed to justify moves, misused basic terminology, and poorly articulated the solution path.These results highlight a stark contrast between o1-preview's ability to solve puzzles and its capacity to explain its reasoning process effectively.</p>
<p>Our analysis of the performance of five LLMs has found that, with the exception of OpenAI o1preview, they are not currently capable of solving 6×6 Sudoku puzzles.OpenAI's LLM fares much better than the other LLMs as a solver, but does not provide explanations that may shed light on what deductions may be used to solve a puzzle.We argue that improving explanation generation for multi-step reasoning processes is a promising area of research, where the ability of LLMs to generate easy-to-understand language has great potential.</p>
<p>The ability to explain multi-step reasoning is of even greater value for decision processes that have unique sets of constraints and are much harder to solve and explain.It is therefore justified to expect that LLMs may need the help of tools based on different forms of reasoning.We are particularly interested in exploring approaches that combine LLMs with logic-based reasoning, as implemented in SMT solvers (e.g., Z3 or CVC5 (Barbosa et al., 2022)) and proof assistants like Lean (de Moura et al., 2015) or Isabelle HOL (Nipkow et al., 2024).Explanations produced by these tools are usually reliable and detailed, but are notoriously hard for humans to parse and digest.We expect LLMs to be able to interpret those explanations in ways accessible to the non-specialist and with levels of detail that match the needs of the human user.</p>
<p>Limitations</p>
<p>Although this study examines the capabilities of LLMs in solving and explaining 6×6 Sudoku puzzles, there are several limitations.1. Dataset Scope: Although we generated 2,293 puzzles using the Z3 solver, the subset used for human and o1-preview comparison was limited to 100 puzzles for solutions and 20 puzzles for explanations.This small experiment was enough to appreciate the inability of o1 to come up with satisfying explanations for its solutions.Given that our subsets contained puzzles of varying difficulty and that results were consistently deficient, we believe that we have enough evidence to assert that this is an area where cutting-edge LLMs struggle.However, a larger-scale study could strengthen these findings and provide a more comprehensive view of the models' overall capabilities and performance under different difficulty levels.</p>
<p>Puzzle Complexity:</p>
<p>We evaluated the models on 6 × 6 Sudoku puzzles, which, while requiring some non-trivial reasoning, are simpler than the more commonly studied 9 × 9 puzzles.The limited complexity of the 6×6 grid may not fully capture the range of reasoning challenges that LLMs might encounter in more difficult puzzles, potentially underestimating the models' limitations in handling more complex tasks.</p>
<ol>
<li>Impact of Fine-Tuning: Our study was focused on evaluating the capacity of LLMs to produce solutions and explanations out of the box, without any fine-tuning.Of particular interest was to study the disconnect between o1's ability to solve puzzles vs. its inability to provide consistent, faithful explanations for its reasoning process.While fine-tuning models was out of scope for this work, it is possible that conclusions could differ if any training were to be performed.Output Format: Output the completed 6×6 grid, replacing the 0s with the correct numbers from 1 to 6, such that all the constraints are satisfied.Problem to Solve: 0 0 1 2 0 0 0 0 0 0 0 0 0 0 2 0 0 4 0 0 0 6 0 5 0 0 0 0 3 0 4 0 0 0 0 6</li>
</ol>
<p>Provide the completed 6×6 grid only</p>
<p>C Experiment Details</p>
<p>The experiments were conducted using the following configurations:</p>
<ol>
<li>
<p>Zero-shot Prompting: For all experiments, we employed zero-shot prompting to evaluate the models' ability to solve 6x6 Sudoku puzzles without additional task-specific training or prior context.</p>
</li>
<li>
<p>Temperature and top-p: All models were run with a fixed temperature of 0 to ensure deterministic outputs across multiple runs.Additionally we use top-p sampling value of 0.5 for all models.</p>
</li>
<li>
<p>Infrastructure: Gemma-2-9B-Instruct was was run on GroqCloud.Llama-3.1-70B-Instruct,Llama-3.1-8B-Instruct, and Mistral-7B-Instruct-v0.3 models were accessed and run via the Hugging Face Chat API.o1preview was used via OpenAI's user interface.</p>
</li>
</ol>
<p>Accuracy Evaluation:</p>
<p>The number of prefilled (givens) was also considered when calculating row, column, box and cell-wise accuracy.</p>
<p>D Difficulty Distribution in 20-Puzzle Subset</p>
<p>For our detailed explanation analysis, we selected 20 puzzles from the larger dataset, ensuring representation across different difficulty levels.?? shows the distribution of these puzzles across the Explainer Rating (ER) categories.This distribution was intentionally designed to be more balanced than the original dataset to ensure sufficient coverage of higher difficulty levels while maintaining representation of common puzzle types.The selection allows us to evaluate o1-preview's explanation capabilities across the full spectrum of puzzle complexity.</p>
<p>E o1-preview Performance Analysis by Difficulty Level and Evaluation Metric</p>
<p>Figure.</p>
<p>3 presents a detailed heatmap analysis of o1preview's solving performance across the surveyed puzzles, revealing three key patterns:</p>
<p>• Perfect performance on simpler puzzles: For Easy (ER 1.0-1.2) and Medium (ER 1.5) difficulty levels, o1-preview achieves 100% accuracy across all metrics.</p>
<p>• Progressive degradation with difficulty: Performance declines notably for harder puzzles, with success rates dropping to 40% for complete solutions in Diabolical puzzles (ER 6.2+).</p>
<p>• Consistent metric hierarchy: Cell-wise accuracy remains highest across all difficulty levels (57-100%), while complete solution rates show the steepest decline (40-100%), suggesting the model struggles more with global constraints than local patterns.</p>
<p>F Survey Participants and Expertise</p>
<p>Our Likert survey involved three participants from our institution: one faculty member and two graduate students, all of whom were connected to the research project.While their experience with Sudoku varied, all participants had a solid understanding of Sudoku rules and demonstrated experience in solving puzzles across different difficulty levels.</p>
<p>Figure 3 :
3
Figure 3: Heatmap visualization of o1-preview's performance across different difficulty levels and evaluation metrics for the 20-puzzle subset.</p>
<p>Table 1 :
1
Sudoku Solving Performance Across Models and Dataset Sizes.Upper section shows results for opensource models evaluated on the full dataset (2,293 puzzles).Lower section shows results for all models, including o1-preview, evaluated on a subset of 100 puzzles
Accuracy (%)</p>
<p>and Denny Zhou.2024.Chain-of-thought prompting elicits reasoning in large language models.In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22, Red Hook, NY, USA.Curran Associates Inc.
Distribution of Sudoku Puzzle Difficulty (N = 2,293)73.4%1500Number of Puzzles500 100020.1%Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu (Benjamin Liu), Chong Ruan, prover: Advancing theorem proving in llms through Wenda Li, and Xiaodan Liang. 2024. Deepseek-0Easy (1.0-1.2) Hidden Singles in blocks onlyMedium (1.5) Hidden Singles in rows/columnsHard (1.7-2.5) Explainer Rating (ER) 1.6% No candidates neededFiendish (2.6-6.0) 3.4% Requires writing down candidatesDiabolical (6.2+) 1.5% Requires Forcing Chainslarge-scale synthetic data. ArXiv, abs/2405.14333.Figure 2: Distribution of Sudoku puzzle difficulty acrossthe dataset (N = 2,293). Each category corresponds toZhangyue Yin, Qiushi Sun, Qipeng Guo, Zhiyuan Zeng, Xiaonan Li, Junqi Dai, Qinyuan Cheng, Xuanjingspecific solving techniques required, from basic hidden singles to complex forcing chains.Huang, and Xipeng Qiu. 2024. Reasoning in flux:Enhancing large language models reasoning through uncertainty-aware adaptive guidance. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),B Prompt Details ' Task Description: You are given an incomplete$pages 2401-2416, Bangkok, Thailand. Association6×6 Sudoku grid. The objective is to fill in thefor Computational Linguistics.empty cells (represented by 0s) with numbersfrom 1 to 6 while following these rules:Row Constraint: Each number from 1 to 6 mustA Puzzle difficulty rubricappear exactly once in every row.Column Constraint: Each number from 1 to 61. Easy (ER 1.0-1.2): Requires only hidden sin-gles in blocks, making up 73.4% of the dataset. These puzzles can be solved using basic scan-ning techniques.must appear exactly once in every column. Box Constraint: Each number from 1 to 6 must appear exactly once in each of the six 2×3 boxes. Important: Do not use any code to solve this puzzle. Solve this puzzle using logical reasoningand only provide the final grid.2. Medium (ER 1.5): Requires hidden singles in rows/columns, comprising 20.1% of puzzles. These puzzles need slightly more advanced pattern recognition.Input Format: A 6×6 grid is represented by six lines of input. Each line contains six space-separated integers. The number 0 indicates an empty cell that you need to fill, while numbers 1 to 6 represent pre-filled cells.3. Hard (ER 1.7-2.5): No candidates needed butrequires more complex deduction, represent-ing 1.6% of the dataset.4. Fiendish (ER 2.6-6.0): Requires writing downcandidates and advanced techniques, makingup 3.4% of puzzles.5. Diabolical (ER 6.2+): Requires forcing chains&amp;%and sophisticated solving strategies, compris-ing 1.5% of the dataset.
AcknowledgmentsAshutosh Trivedi holds the position of Royal Society Wolfson Visiting Fellow and acknowledges the support of the Wolfson Foundation and the Royal Society for this fellowship.
Cesare Tinelli, and Yoni Zohar. 2022. cvc5: A versatile and industrial-strength SMT solver. Haniel Barbosa, Clark W Barrett, Martin Brain, Gereon Kremer, Hanna Lachnitt, Makai Mann, Abdalrhman Mohamed, Mudathir Mohamed, Aina Niemetz, Andres Nötzli, Alex Ozdemir, Mathias Preiner, Andrew Reynolds, Ying Sheng, Tools and Algorithms for the Construction and Analysis of Systems TACAS 2022. 13243</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Advances in Neural Information Processing Systems. Curran Associates, Inc202033Clemens Winter, and 12 others</p>
<p>PuzzleVQA: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. Ken Yew, Vernon Chia, Deepanway Toh, Lidong Ghosal, Soujanya Bing, Poria, 10.18653/v1/2024.findings-acl.962Findings of the Association for Computational Linguistics ACL 2024. Bangkok, Thailand2024Association for Computational Linguistics</p>
<p>Leonardo De, Moura , Nikolaj Bjørner, Satisfiability modulo theories: Introduction and applications. 201154</p>
<p>The Lean theorem prover (system description). Leonardo De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, Jakob Von Raumer, Conference on Automated Deduction (CADE-25). 2015</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, 10.48550/ARXIV.2407.21783CoRR, abs/2407.21783202482</p>
<p>Giorgos Filandrianos, and Giorgos Stamou. 2024a. Puzzle solving using reasoning of large language models: A survey. Panagiotis Giadikiaroglou, Maria Lymperaiou, arXiv:2402.11291Preprint</p>
<p>Puzzle solving using reasoning of large language models: A survey. Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou, 10.18653/v1/2024.emnlp-main.646Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024b</p>
<p>Explaining explanations: An overview of interpretability of machine learning. Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, Lalana Kagal, 10.1109/DSAA.2018.000182018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA). 2018</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, 10.18653/v1/2023.findings-acl.67Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las, Florian Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, 10.48550/ARXIV.2310.06825Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed2023Mistral 7b. CoRR, abs/2310.06825</p>
<p>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothée Lacroix, Yuhuai Wu, Guillaume Lample, ArXiv, abs/2210.122832022</p>
<p>Assessing logical puzzle solving in large language models: Insights from a minesweeper case study. Yinghao Li, Haorui Wang, Chao Zhang, 10.18653/v1/2024.naacl-long.4Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics20241</p>
<p>There is no 16-clue sudoku: Solving the sudoku minimum number of clues problem via hitting set enumeration. Gary Mcguire, Bastian Tugemann, Gilles Civario, 10.1080/10586458.2013.870056Experimental Mathematics. 2322014</p>
<p>Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar, arXiv:2410.052292024Preprint</p>
<p>Puzzlebench: Can llms solve challenging first-order combinatorial reasoning problems?. Chinmay Mittal, Krishna Kartik, arXiv:2402.02611Parag Singla, and 1 others. 2024arXiv preprint</p>
<p>Isabelle HOL: A Proof Assistant for Higher-Order Logic. Tobias Nipkow, Lawrence C Paulson, Markus Wenzel, 2024Springer</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Advances in Neural Information Processing Systems. Curran Associates, IncJan Leike, and Ryan Lowe. 202235</p>
<p>Improving open language models at a practical size. Morgane Rivière, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, 10.48550/ARXIV.2408.00118CoRR, abs/2408.00118Gemma. 22024Anton Tsitsulin, and 80 others</p>
<p>Taking Sudoku Seriously: The Math Behind the World's Most Popular Pencil Puzzle. J Rosenhouse, L Taalman, 2011Oxford University Press</p>
<p>Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel R Bowman, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2024</p>
<p>. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2968 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2968</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2968</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-15b91292ba80adaa87361a0e8894e47899f02f1d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/15b91292ba80adaa87361a0e8894e47899f02f1d" target="_blank">Learning Dynamic Belief Graphs to Generalize on Text-Based Games</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel graph-aided transformer agent (GATA) that infers and updates latent belief graphs during planning to enable effective action selection by capturing the underlying game dynamics.</p>
                <p><strong>Paper Abstract:</strong> Playing text-based games requires skills in processing natural language and sequential decision making. Achieving human-level performance on text-based games remains an open challenge, and prior research has largely relied on hand-crafted structured representations and heuristics. In this work, we investigate how an agent can plan and generalize in text-based games using graph-structured representations learned end-to-end from raw text. We propose a novel graph-aided transformer agent (GATA) that infers and updates latent belief graphs during planning to enable effective action selection by capturing the underlying game dynamics. GATA is trained using a combination of reinforcement and self-supervised learning. Our work demonstrates that the learned graph-based representations help agents converge to better policies than their text-only counterparts and facilitate effective generalization across game configurations. Experiments on 500+ unique games from the TextWorld suite show that our best agent outperforms text-based baselines by an average of 24.2%.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2968.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2968.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-Aided Transformer Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based RL agent that infers and updates a continuous, latent belief graph from text observations and uses it (via an R-GCN encoder and attention-based aggregator) to score candidate text actions in TextWorld games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GATA maintains a continuous adjacency-tensor belief graph G_t (real-valued tensor in [-1,1]^{R x N x N}) updated recurrently via a graph-updater that produces a graph-difference Δg_t; an RNN hidden state h_t carries recurrent memory across time and is decoded by an MLP f_d to form the adjacency tensor. The action selector encodes G_t with an R-GCN (relation embeddings conditioned on relation label text), encodes text observations with a transformer encoder, fuses graph and text with a bidirectional attention aggregator, and scores candidate actions with self-attention + MLP. Graph-updater is pre-trained with two self-supervised objectives (Observation Generation and Contrastive Observation Classification) and then frozen while the action selector is trained with Double DQN + multi-step learning + prioritized replay.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>Transformer encoder (custom transformer block used for text encoding and transformer-based decoders for OG/CG; not a pretrained LLM like GPT)</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>TextWorld (FTWP-generated cooking games)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Choice-based TextWorld games where an agent receives partial text observations and must gather/process ingredients according to a discovered recipe; challenges include partial observability, long-term dependencies, sparse rewards, and combinatorial action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory (continuous latent belief graph) + recurrent hidden state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Belief graph G_t is a continuous adjacency tensor in [-1,1]^{R x N x N} (R=10 relation types, N=99 entity slots). The graph updater computes Δg_t = f_Δ(h_{G_{t-1}}, h_{O_t}, h_{A_{t-1}}) using a representation aggregator; an RNN (GRU-style) consumes Δg_t producing hidden state h_t which is decoded by f_d (2-layer MLP with tanh) to produce G_t. The graph encoder uses an R-GCN (6 layers, hidden size 64) conditioned on learned relation embeddings and averaged relation-name word embeddings. The recurrent hidden state h_t acts as temporal memory across time-steps.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Graph representations are read via an R-GCN that produces node vectors; retrieval for action scoring uses a bi-directional attention aggregator (trilinear similarity) between text token representations and node representations, followed by self-attention and masked pooling to produce state vectors for scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Configured to support up to R=10 relation types and N=99 entity slots (set to match TextWorld game maxima); internal RNN hidden state of unspecified dimensionality (paper uses H=64 for many layers).</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Inferred relations (edge strengths) between entities, entity attributes/conditions (encoded as node features via embeddings), and temporally accumulated state information about what the agent has seen/acted on.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>GATA variants using the learned continuous belief graph achieved large gains over text-only baselines; the paper reports the best GATA variant achieving an average relative improvement of +24.2% over the Tr-DQN baseline across difficulty levels (Table 2). Specific per-level normalized test scores are reported in Table 2 (e.g., many level entries in the 60-75% range depending on variant and training set size).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Text-only baseline Tr-DQN (transformer text encoder, no explicit graph belief) reported as baseline in Table 2; GATA's reported improvements are relative to Tr-DQN (e.g., +24.2% average relative improvement for the best GATA). Exact Tr-DQN normalized scores per level are listed in Table 2 (examples: Tr-DQN normalized test scores: level1 66.2, level2 26.0, level3 16.7, level4 18.2 for 20 training games).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>+24.2% average relative improvement over Tr-DQN (text-only baseline) for the best GATA variant; other GATA variants (different pretraining/objective or with/without direct text observation input) show varying improvements (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Structured graph-structured belief memory provides a useful inductive bias: learned belief graphs enable better policies and generalization across unseen TextWorld games compared to text-only baselines; combining graph beliefs with current text observations via attention often further improves performance; continuous belief graphs with learned update functions are more robust than discrete update schemes pre-trained on ground-truth graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Belief graphs are noisy and denser than ground-truth graphs (less sparse), can accumulate errors over time, and do not reach the performance of agents with full ground-truth graphs (GATA-GTF); learned graph features can encode syntactic/decoding artifacts from self-supervised tasks rather than pure state relations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Compared to implicit RNN memory (Tr-DRQN / LSTM-DRQN) and count-based episodic bonuses (Tr-DRQN+), graph-based memory (GATA) outperforms those text-only memory variants in generalization experiments, especially when training on larger sets of games; ground-truth graph input (GATA-GTF) yields substantially higher upper-bound performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Dynamic Belief Graphs to Generalize on Text-Based Games', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2968.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2968.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATA-GTP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GATA with Ground-Truth Pretraining (discrete graph updater)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant of GATA whose discrete graph updater is pre-trained using ground-truth (discrete) KGs from FTWP; during RL it still infers belief graphs from text (discrete updates are generated as add/delete commands).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GATA-GTP</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same action selector as GATA; uses a discrete graph updater pre-trained on TextWorld ground-truth graphs. The discrete updater models Δg_t as sequences of atomic graph update commands (add/delete node1,node2,relation) produced by a transformer Seq2Seq 'command generator'. During RL the updater is detached and emits token sequences to update a discrete belief KG fed into the R-GCN encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>Transformer Seq2Seq decoder (custom) used for command generation; not a large pretrained LLM</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>TextWorld (FTWP-generated cooking games)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Same TextWorld experimental suite as GATA.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>discrete graph-based memory (symbolic KG, updated via generated add/delete commands)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Belief graph is discrete multi-relational KG; updates are generated as sequences of add(node1,node2,relation) and delete(...) commands produced by a transformer-based Seq2Seq command generator trained with teacher forcing on differences between consecutive ground-truth graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>R-GCN encodes the discrete KG; action selection uses the same attention-based aggregator between text and R-GCN node vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Capacity governed by the ground-truth KG size available in TextWorld (discrete graphs from TextWorld APIs); specifics not restated beyond the dataset-provided sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Symbolic graph triples (edges) representing relations and attributes observed so far (the 'seen' partial KG or the inferred discrete KG).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>GATA-GTP shows improved performance over text-only baselines when text observations are also provided, but overall is substantially poorer than continuous GATA in some settings; Table 2 reports mixed scores (some levels improved, some degraded); e.g., when combined with observations (Δ) GATA-GTP outperforms Tr-DQN by +14.9% in one configuration but overall underperforms continuous GATA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baselines: Tr-DQN scores shown in Table 2; GATA-GTP's performance is compared against Tr-DQN and GATA in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>Varies by level/config; reported relative improvements vs Tr-DQN up to +24.6% for some configurations, but average performance can be worse than continuous GATA.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Using ground-truth graphs for pre-training discrete updaters does not guarantee better RL performance — discrete update operations can be brittle, susceptible to error accumulation and rounding/discrete prediction errors; continuous belief graphs with learned updates can be easier to train and more robust.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Discrete update generation can accumulate errors over steps; discrete rounding and brittle update operations may degrade downstream policy learning compared to continuous representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Compared to continuous GATA, GATA-GTP (discrete) is often outperformed despite having stronger supervision during pretraining; compared to ground-truth-input agent (GATA-GTF) it underperforms (GATA-GTF is an upper-bound).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Dynamic Belief Graphs to Generalize on Text-Based Games', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2968.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2968.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATA-GTF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GATA with Ground-Truth Full Graph input</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An upper-bound variant that feeds the full ground-truth game graph (fully observable discrete KG) directly to the action selector (no graph-updater), removing partial observability and updater error.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GATA-GTF</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Action selector identical to GATA but receives the exact ground-truth full KG G^{full} at each step (encoded by R-GCN). There is no graph-updater module; agent thus operates in a fully-observable MDP using precise structured state.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>TextWorld (FTWP-generated cooking games)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Same TextWorld experimental suite; ground-truth graphs provide exact full state.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>ground-truth graph state (perfect/explicit memory of full environment state)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Uses TextWorld-provided discrete full-state KG at every timestep as input to R-GCN encoder; no updater or learned memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>R-GCN encodes the full KG and attention aggregator (if text also provided) is used by the scorer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Effectively the full game-state KG (unlimited within game-defined graph size).</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Complete, accurate relations and entity attributes of the game state at each timestep (full ground-truth KG).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Significantly higher than all other agents; reported as the empirical upper-bound. Table 2 shows GATA-GTF average relative improvement up to +81.6% over Tr-DQN in one experimental setup (very large gains per Table 2), with many per-level normalized test scores much higher than learned-belief agents (examples: 95.0, 95.0, 70.0 normalized scores on some levels for 100-training-game experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>Large — GATA-GTF outperforms other agents substantially (e.g., average relative improvement reported as +81.6% over Tr-DQN in Table 2 for some configs), indicating the value of perfect structured state information.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Providing perfect structured state (ground-truth KG) converts the game to a fully-observable MDP and yields a strong upper-bound on performance; gap between GATA (learned belief graphs) and GATA-GTF indicates room for improved graph learning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Not applicable as memory is ground-truth; but unrealistic in practical partial-observation settings since it requires privileged access to full state.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Serves as gold-standard comparison showing that accurate structured memory (full KG) yields much higher performance than learned/noisy belief graphs or text-only approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Dynamic Belief Graphs to Generalize on Text-Based Games', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2968.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2968.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tr-DRQN / LSTM-DRQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer/Recurrent Deep Recurrent Q-Network baselines (DRQN family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Text-only baselines that use recurrent networks to provide implicit temporal memory for partially observable text games: original LSTM-DQN/DRQN architectures adapted here with a transformer text encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Tr-DRQN (and LSTM-DQN / LSTM-DRQN variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baselines: Tr-DQN is a transformer-encoder-based DQN (no explicit memory). Tr-DRQN replaces the feedforward Q-network with a recurrent Q-network (DRQN) to provide implicit memory across steps (hidden state preserves history). LSTM-DQN / LSTM-DRQN refer to LSTM-based variants in prior work; in this paper the authors re-implement them with the transformer text encoder for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>Transformer encoder (custom) for text encoding; RNN (LSTM/GRU) used for recurrence — not a large pretrained LLM</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>TextWorld (FTWP-generated cooking games)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Same TextWorld experimental suite.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit recurrent memory (RNN hidden state)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Recurrent hidden state (from LSTM/GRU or DRQN architecture) that summarizes past observations/actions; Tr-DRQN uses transformer encoder for tokens then passes representations into recurrent Q-network.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Internal hidden state of RNN passed forward to next timestep (recency-based implicit recurrence).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not explicitly specified in this paper; architecture details (hidden sizes) are replaced by transformer encoder + recurrent layers but exact memory capacity not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Compressed summary of past observations and actions in the RNN hidden state (implicit belief about unobserved parts of state).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Tr-DRQN outperforms Tr-DQN by a small margin (reported +3.9% relative improvement averaged in the paper). Per Table 2, gains are modest and vary by difficulty level.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Tr-DQN (no recurrent memory) is the baseline; examples in Table 2: Tr-DQN normalized scores for 20 training games: level1 66.2, level2 26.0, level3 16.7, level4 18.2.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>+3.9% relative improvement (Tr-DRQN vs Tr-DQN) reported on average in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Recurrent implicit memory provides modest benefits over feedforward (no-memory) DQN baselines for text-based games; however, explicit structured belief graphs (GATA) provide larger improvements and better generalization in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Implicit RNN memory may be insufficient to capture structured world state and long-term relational information required by TextWorld; DRQN agents struggled to scale on larger training sets compared to graph-based GATA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Compared to graph-based memory (GATA), RNN implicit memory yields smaller gains; count-based episodic bonuses (Tr-DRQN+) can help exploration but do not match the improvements from learned graph-structured belief memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Dynamic Belief Graphs to Generalize on Text-Based Games', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2968.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2968.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tr-DRQN+ (episodic counting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer DRQN with Episodic Counting Bonus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DRQN baseline augmented with an episodic counting bonus for exploration (following Yuan et al. [50]) to encourage visiting novel states during training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Tr-DRQN+</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same as Tr-DRQN but includes an episodic state-counting auxiliary exploration reward (count-based bonus) to encourage exploration in multi-game RL training, implemented on top of recurrent DRQN architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>Transformer encoder (custom) + recurrent DRQN; not a pretrained LLM</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>TextWorld (FTWP-generated cooking games)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Same TextWorld experimental suite.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit recurrent memory + episodic count-based exploration memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>RNN hidden state for temporal memory; additionally an episodic count table (counts of visited states or hashed state signatures) used to compute exploration bonus during training.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Recency through RNN hidden state; counts consulted when computing exploration bonus; retrieval is based on hashed/state identifiers (not fully specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Not specified in the paper (counting mechanism details inherited from prior work [50]).</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Implicit summaries in RNN hidden state and episodic counts of visited states (used to produce exploration bonus).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Tr-DRQN+ shows modest improvements over Tr-DQN in some settings; Table 2 reports average relative improvement ~+3.6% in some configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Tr-DQN baseline as in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>Small (~+3-4% relative improvement over Tr-DQN depending on experimental setup).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Augmenting recurrent agents with episodic counting helps exploration and yields modest gains, but still falls short of the improvements delivered by structured graph-based beliefs in GATA.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Count-based episodic bonuses encourage exploration but do not provide structured relational memory needed for reasoning about entities and their relations; scalable generalization remains limited.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Compared to pure recurrent memory (Tr-DRQN) the count bonus gives slight gains; compared to graph-based GATA the overall performance remains lower.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Dynamic Belief Graphs to Generalize on Text-Based Games', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2968.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2968.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory networks (Urbanek et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory Networks used for adventure/dialog-like tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related prior work (Urbanek et al.) applied memory networks and ranking systems to tackle adventure-themed dialogue / interactive fiction tasks, mentioned in related work as a memory-based approach to language-grounded action.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory networks (as in Urbanek et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory network architectures store and attend over a set of memories (e.g., past text observations or facts) and use a ranking or retrieval mechanism to select responses/actions; cited as prior work on language+action tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Adventure-themed dialog / interactive fiction benchmarks (as in Urbanek et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Dialog/action tasks in a fantasy text-adventure setting; models must both speak and act given text context.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>memory networks (explicit memory slots with attention-based read)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Memory composed of discrete memory slots (textual facts / passages) that are attendable via an attention mechanism; specific architecture not detailed in this paper (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Attention-based retrieval over memory slots.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Past observations, dialog context, possibly extracted entities/facts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Referenced as prior evidence that explicit memory mechanisms (memory networks) can be useful in text-interaction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Dynamic Belief Graphs to Generalize on Text-Based Games', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2968.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2968.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Count-based episodic memory (Yuan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counting-based exploration / memory for text games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that proposes a count-based episodic memory bonus to encourage exploration and generalization across text games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Count-based episodic memory (episodic counting bonus)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Exploration mechanism that maintains counts (or hashed counts) of visited states within episodes and provides intrinsic rewards for visiting novel states; used to improve exploration and generalization in text-based game RL.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>TextWorld / families of text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Text-based games requiring exploration across rooms and interactions to solve cooking/quest-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic count-based memory (exploration bonus)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Counts of states or state-signatures (episodic) are stored and consulted to compute exploration bonuses; details inherited from cited work [50].</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Lookup of counts by state/hashes; recency / novelty-based retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Counts of visited states or hashed state signatures during an episode.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Cited as an effective exploration mechanism; in this paper Tr-DRQN+ (which incorporates this episodic counting bonus) yields modest improvements over Tr-DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Dynamic Belief Graphs to Generalize on Text-Based Games', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Counting to explore and generalize in text-based games <em>(Rating: 2)</em></li>
                <li>Building dynamic knowledge graphs from text-based games <em>(Rating: 2)</em></li>
                <li>Learning to speak and act in a fantasy text adventure game <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2968",
    "paper_id": "paper-15b91292ba80adaa87361a0e8894e47899f02f1d",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "GATA",
            "name_full": "Graph-Aided Transformer Agent",
            "brief_description": "A transformer-based RL agent that infers and updates a continuous, latent belief graph from text observations and uses it (via an R-GCN encoder and attention-based aggregator) to score candidate text actions in TextWorld games.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GATA",
            "agent_description": "GATA maintains a continuous adjacency-tensor belief graph G_t (real-valued tensor in [-1,1]^{R x N x N}) updated recurrently via a graph-updater that produces a graph-difference Δg_t; an RNN hidden state h_t carries recurrent memory across time and is decoded by an MLP f_d to form the adjacency tensor. The action selector encodes G_t with an R-GCN (relation embeddings conditioned on relation label text), encodes text observations with a transformer encoder, fuses graph and text with a bidirectional attention aggregator, and scores candidate actions with self-attention + MLP. Graph-updater is pre-trained with two self-supervised objectives (Observation Generation and Contrastive Observation Classification) and then frozen while the action selector is trained with Double DQN + multi-step learning + prioritized replay.",
            "base_llm_model": "Transformer encoder (custom transformer block used for text encoding and transformer-based decoders for OG/CG; not a pretrained LLM like GPT)",
            "base_llm_size": null,
            "text_game_name": "TextWorld (FTWP-generated cooking games)",
            "text_game_description": "Choice-based TextWorld games where an agent receives partial text observations and must gather/process ingredients according to a discovered recipe; challenges include partial observability, long-term dependencies, sparse rewards, and combinatorial action spaces.",
            "uses_memory": true,
            "memory_type": "graph-based memory (continuous latent belief graph) + recurrent hidden state",
            "memory_architecture": "Belief graph G_t is a continuous adjacency tensor in [-1,1]^{R x N x N} (R=10 relation types, N=99 entity slots). The graph updater computes Δg_t = f_Δ(h_{G_{t-1}}, h_{O_t}, h_{A_{t-1}}) using a representation aggregator; an RNN (GRU-style) consumes Δg_t producing hidden state h_t which is decoded by f_d (2-layer MLP with tanh) to produce G_t. The graph encoder uses an R-GCN (6 layers, hidden size 64) conditioned on learned relation embeddings and averaged relation-name word embeddings. The recurrent hidden state h_t acts as temporal memory across time-steps.",
            "memory_retrieval_mechanism": "Graph representations are read via an R-GCN that produces node vectors; retrieval for action scoring uses a bi-directional attention aggregator (trilinear similarity) between text token representations and node representations, followed by self-attention and masked pooling to produce state vectors for scoring.",
            "memory_capacity": "Configured to support up to R=10 relation types and N=99 entity slots (set to match TextWorld game maxima); internal RNN hidden state of unspecified dimensionality (paper uses H=64 for many layers).",
            "what_is_stored_in_memory": "Inferred relations (edge strengths) between entities, entity attributes/conditions (encoded as node features via embeddings), and temporally accumulated state information about what the agent has seen/acted on.",
            "performance_with_memory": "GATA variants using the learned continuous belief graph achieved large gains over text-only baselines; the paper reports the best GATA variant achieving an average relative improvement of +24.2% over the Tr-DQN baseline across difficulty levels (Table 2). Specific per-level normalized test scores are reported in Table 2 (e.g., many level entries in the 60-75% range depending on variant and training set size).",
            "performance_without_memory": "Text-only baseline Tr-DQN (transformer text encoder, no explicit graph belief) reported as baseline in Table 2; GATA's reported improvements are relative to Tr-DQN (e.g., +24.2% average relative improvement for the best GATA). Exact Tr-DQN normalized scores per level are listed in Table 2 (examples: Tr-DQN normalized test scores: level1 66.2, level2 26.0, level3 16.7, level4 18.2 for 20 training games).",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "+24.2% average relative improvement over Tr-DQN (text-only baseline) for the best GATA variant; other GATA variants (different pretraining/objective or with/without direct text observation input) show varying improvements (see Table 2).",
            "key_findings_about_memory": "Structured graph-structured belief memory provides a useful inductive bias: learned belief graphs enable better policies and generalization across unseen TextWorld games compared to text-only baselines; combining graph beliefs with current text observations via attention often further improves performance; continuous belief graphs with learned update functions are more robust than discrete update schemes pre-trained on ground-truth graphs.",
            "memory_limitations": "Belief graphs are noisy and denser than ground-truth graphs (less sparse), can accumulate errors over time, and do not reach the performance of agents with full ground-truth graphs (GATA-GTF); learned graph features can encode syntactic/decoding artifacts from self-supervised tasks rather than pure state relations.",
            "comparison_with_other_memory_types": "Compared to implicit RNN memory (Tr-DRQN / LSTM-DRQN) and count-based episodic bonuses (Tr-DRQN+), graph-based memory (GATA) outperforms those text-only memory variants in generalization experiments, especially when training on larger sets of games; ground-truth graph input (GATA-GTF) yields substantially higher upper-bound performance.",
            "uuid": "e2968.0",
            "source_info": {
                "paper_title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "GATA-GTP",
            "name_full": "GATA with Ground-Truth Pretraining (discrete graph updater)",
            "brief_description": "Variant of GATA whose discrete graph updater is pre-trained using ground-truth (discrete) KGs from FTWP; during RL it still infers belief graphs from text (discrete updates are generated as add/delete commands).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GATA-GTP",
            "agent_description": "Same action selector as GATA; uses a discrete graph updater pre-trained on TextWorld ground-truth graphs. The discrete updater models Δg_t as sequences of atomic graph update commands (add/delete node1,node2,relation) produced by a transformer Seq2Seq 'command generator'. During RL the updater is detached and emits token sequences to update a discrete belief KG fed into the R-GCN encoder.",
            "base_llm_model": "Transformer Seq2Seq decoder (custom) used for command generation; not a large pretrained LLM",
            "base_llm_size": null,
            "text_game_name": "TextWorld (FTWP-generated cooking games)",
            "text_game_description": "Same TextWorld experimental suite as GATA.",
            "uses_memory": true,
            "memory_type": "discrete graph-based memory (symbolic KG, updated via generated add/delete commands)",
            "memory_architecture": "Belief graph is discrete multi-relational KG; updates are generated as sequences of add(node1,node2,relation) and delete(...) commands produced by a transformer-based Seq2Seq command generator trained with teacher forcing on differences between consecutive ground-truth graphs.",
            "memory_retrieval_mechanism": "R-GCN encodes the discrete KG; action selection uses the same attention-based aggregator between text and R-GCN node vectors.",
            "memory_capacity": "Capacity governed by the ground-truth KG size available in TextWorld (discrete graphs from TextWorld APIs); specifics not restated beyond the dataset-provided sizes.",
            "what_is_stored_in_memory": "Symbolic graph triples (edges) representing relations and attributes observed so far (the 'seen' partial KG or the inferred discrete KG).",
            "performance_with_memory": "GATA-GTP shows improved performance over text-only baselines when text observations are also provided, but overall is substantially poorer than continuous GATA in some settings; Table 2 reports mixed scores (some levels improved, some degraded); e.g., when combined with observations (Δ) GATA-GTP outperforms Tr-DQN by +14.9% in one configuration but overall underperforms continuous GATA.",
            "performance_without_memory": "Baselines: Tr-DQN scores shown in Table 2; GATA-GTP's performance is compared against Tr-DQN and GATA in Table 2.",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "Varies by level/config; reported relative improvements vs Tr-DQN up to +24.6% for some configurations, but average performance can be worse than continuous GATA.",
            "key_findings_about_memory": "Using ground-truth graphs for pre-training discrete updaters does not guarantee better RL performance — discrete update operations can be brittle, susceptible to error accumulation and rounding/discrete prediction errors; continuous belief graphs with learned updates can be easier to train and more robust.",
            "memory_limitations": "Discrete update generation can accumulate errors over steps; discrete rounding and brittle update operations may degrade downstream policy learning compared to continuous representations.",
            "comparison_with_other_memory_types": "Compared to continuous GATA, GATA-GTP (discrete) is often outperformed despite having stronger supervision during pretraining; compared to ground-truth-input agent (GATA-GTF) it underperforms (GATA-GTF is an upper-bound).",
            "uuid": "e2968.1",
            "source_info": {
                "paper_title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "GATA-GTF",
            "name_full": "GATA with Ground-Truth Full Graph input",
            "brief_description": "An upper-bound variant that feeds the full ground-truth game graph (fully observable discrete KG) directly to the action selector (no graph-updater), removing partial observability and updater error.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GATA-GTF",
            "agent_description": "Action selector identical to GATA but receives the exact ground-truth full KG G^{full} at each step (encoded by R-GCN). There is no graph-updater module; agent thus operates in a fully-observable MDP using precise structured state.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "TextWorld (FTWP-generated cooking games)",
            "text_game_description": "Same TextWorld experimental suite; ground-truth graphs provide exact full state.",
            "uses_memory": true,
            "memory_type": "ground-truth graph state (perfect/explicit memory of full environment state)",
            "memory_architecture": "Uses TextWorld-provided discrete full-state KG at every timestep as input to R-GCN encoder; no updater or learned memory.",
            "memory_retrieval_mechanism": "R-GCN encodes the full KG and attention aggregator (if text also provided) is used by the scorer.",
            "memory_capacity": "Effectively the full game-state KG (unlimited within game-defined graph size).",
            "what_is_stored_in_memory": "Complete, accurate relations and entity attributes of the game state at each timestep (full ground-truth KG).",
            "performance_with_memory": "Significantly higher than all other agents; reported as the empirical upper-bound. Table 2 shows GATA-GTF average relative improvement up to +81.6% over Tr-DQN in one experimental setup (very large gains per Table 2), with many per-level normalized test scores much higher than learned-belief agents (examples: 95.0, 95.0, 70.0 normalized scores on some levels for 100-training-game experiments).",
            "performance_without_memory": null,
            "has_ablation_study": true,
            "memory_improvement_magnitude": "Large — GATA-GTF outperforms other agents substantially (e.g., average relative improvement reported as +81.6% over Tr-DQN in Table 2 for some configs), indicating the value of perfect structured state information.",
            "key_findings_about_memory": "Providing perfect structured state (ground-truth KG) converts the game to a fully-observable MDP and yields a strong upper-bound on performance; gap between GATA (learned belief graphs) and GATA-GTF indicates room for improved graph learning.",
            "memory_limitations": "Not applicable as memory is ground-truth; but unrealistic in practical partial-observation settings since it requires privileged access to full state.",
            "comparison_with_other_memory_types": "Serves as gold-standard comparison showing that accurate structured memory (full KG) yields much higher performance than learned/noisy belief graphs or text-only approaches.",
            "uuid": "e2968.2",
            "source_info": {
                "paper_title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Tr-DRQN / LSTM-DRQN",
            "name_full": "Transformer/Recurrent Deep Recurrent Q-Network baselines (DRQN family)",
            "brief_description": "Text-only baselines that use recurrent networks to provide implicit temporal memory for partially observable text games: original LSTM-DQN/DRQN architectures adapted here with a transformer text encoder.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Tr-DRQN (and LSTM-DQN / LSTM-DRQN variants)",
            "agent_description": "Baselines: Tr-DQN is a transformer-encoder-based DQN (no explicit memory). Tr-DRQN replaces the feedforward Q-network with a recurrent Q-network (DRQN) to provide implicit memory across steps (hidden state preserves history). LSTM-DQN / LSTM-DRQN refer to LSTM-based variants in prior work; in this paper the authors re-implement them with the transformer text encoder for fair comparison.",
            "base_llm_model": "Transformer encoder (custom) for text encoding; RNN (LSTM/GRU) used for recurrence — not a large pretrained LLM",
            "base_llm_size": null,
            "text_game_name": "TextWorld (FTWP-generated cooking games)",
            "text_game_description": "Same TextWorld experimental suite.",
            "uses_memory": true,
            "memory_type": "implicit recurrent memory (RNN hidden state)",
            "memory_architecture": "Recurrent hidden state (from LSTM/GRU or DRQN architecture) that summarizes past observations/actions; Tr-DRQN uses transformer encoder for tokens then passes representations into recurrent Q-network.",
            "memory_retrieval_mechanism": "Internal hidden state of RNN passed forward to next timestep (recency-based implicit recurrence).",
            "memory_capacity": "Not explicitly specified in this paper; architecture details (hidden sizes) are replaced by transformer encoder + recurrent layers but exact memory capacity not reported.",
            "what_is_stored_in_memory": "Compressed summary of past observations and actions in the RNN hidden state (implicit belief about unobserved parts of state).",
            "performance_with_memory": "Tr-DRQN outperforms Tr-DQN by a small margin (reported +3.9% relative improvement averaged in the paper). Per Table 2, gains are modest and vary by difficulty level.",
            "performance_without_memory": "Tr-DQN (no recurrent memory) is the baseline; examples in Table 2: Tr-DQN normalized scores for 20 training games: level1 66.2, level2 26.0, level3 16.7, level4 18.2.",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "+3.9% relative improvement (Tr-DRQN vs Tr-DQN) reported on average in experiments.",
            "key_findings_about_memory": "Recurrent implicit memory provides modest benefits over feedforward (no-memory) DQN baselines for text-based games; however, explicit structured belief graphs (GATA) provide larger improvements and better generalization in many settings.",
            "memory_limitations": "Implicit RNN memory may be insufficient to capture structured world state and long-term relational information required by TextWorld; DRQN agents struggled to scale on larger training sets compared to graph-based GATA.",
            "comparison_with_other_memory_types": "Compared to graph-based memory (GATA), RNN implicit memory yields smaller gains; count-based episodic bonuses (Tr-DRQN+) can help exploration but do not match the improvements from learned graph-structured belief memory.",
            "uuid": "e2968.3",
            "source_info": {
                "paper_title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Tr-DRQN+ (episodic counting)",
            "name_full": "Transformer DRQN with Episodic Counting Bonus",
            "brief_description": "A DRQN baseline augmented with an episodic counting bonus for exploration (following Yuan et al. [50]) to encourage visiting novel states during training.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Tr-DRQN+",
            "agent_description": "Same as Tr-DRQN but includes an episodic state-counting auxiliary exploration reward (count-based bonus) to encourage exploration in multi-game RL training, implemented on top of recurrent DRQN architecture.",
            "base_llm_model": "Transformer encoder (custom) + recurrent DRQN; not a pretrained LLM",
            "base_llm_size": null,
            "text_game_name": "TextWorld (FTWP-generated cooking games)",
            "text_game_description": "Same TextWorld experimental suite.",
            "uses_memory": true,
            "memory_type": "implicit recurrent memory + episodic count-based exploration memory",
            "memory_architecture": "RNN hidden state for temporal memory; additionally an episodic count table (counts of visited states or hashed state signatures) used to compute exploration bonus during training.",
            "memory_retrieval_mechanism": "Recency through RNN hidden state; counts consulted when computing exploration bonus; retrieval is based on hashed/state identifiers (not fully specified in this paper).",
            "memory_capacity": "Not specified in the paper (counting mechanism details inherited from prior work [50]).",
            "what_is_stored_in_memory": "Implicit summaries in RNN hidden state and episodic counts of visited states (used to produce exploration bonus).",
            "performance_with_memory": "Tr-DRQN+ shows modest improvements over Tr-DQN in some settings; Table 2 reports average relative improvement ~+3.6% in some configurations.",
            "performance_without_memory": "Tr-DQN baseline as in Table 2.",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "Small (~+3-4% relative improvement over Tr-DQN depending on experimental setup).",
            "key_findings_about_memory": "Augmenting recurrent agents with episodic counting helps exploration and yields modest gains, but still falls short of the improvements delivered by structured graph-based beliefs in GATA.",
            "memory_limitations": "Count-based episodic bonuses encourage exploration but do not provide structured relational memory needed for reasoning about entities and their relations; scalable generalization remains limited.",
            "comparison_with_other_memory_types": "Compared to pure recurrent memory (Tr-DRQN) the count bonus gives slight gains; compared to graph-based GATA the overall performance remains lower.",
            "uuid": "e2968.4",
            "source_info": {
                "paper_title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Memory networks (Urbanek et al.)",
            "name_full": "Memory Networks used for adventure/dialog-like tasks",
            "brief_description": "Related prior work (Urbanek et al.) applied memory networks and ranking systems to tackle adventure-themed dialogue / interactive fiction tasks, mentioned in related work as a memory-based approach to language-grounded action.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Memory networks (as in Urbanek et al.)",
            "agent_description": "Memory network architectures store and attend over a set of memories (e.g., past text observations or facts) and use a ranking or retrieval mechanism to select responses/actions; cited as prior work on language+action tasks.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "Adventure-themed dialog / interactive fiction benchmarks (as in Urbanek et al.)",
            "text_game_description": "Dialog/action tasks in a fantasy text-adventure setting; models must both speak and act given text context.",
            "uses_memory": true,
            "memory_type": "memory networks (explicit memory slots with attention-based read)",
            "memory_architecture": "Memory composed of discrete memory slots (textual facts / passages) that are attendable via an attention mechanism; specific architecture not detailed in this paper (only cited).",
            "memory_retrieval_mechanism": "Attention-based retrieval over memory slots.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Past observations, dialog context, possibly extracted entities/facts.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Referenced as prior evidence that explicit memory mechanisms (memory networks) can be useful in text-interaction tasks.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2968.5",
            "source_info": {
                "paper_title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Count-based episodic memory (Yuan et al.)",
            "name_full": "Counting-based exploration / memory for text games",
            "brief_description": "Referenced prior work that proposes a count-based episodic memory bonus to encourage exploration and generalization across text games.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Count-based episodic memory (episodic counting bonus)",
            "agent_description": "Exploration mechanism that maintains counts (or hashed counts) of visited states within episodes and provides intrinsic rewards for visiting novel states; used to improve exploration and generalization in text-based game RL.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "TextWorld / families of text-based games",
            "text_game_description": "Text-based games requiring exploration across rooms and interactions to solve cooking/quest-style tasks.",
            "uses_memory": true,
            "memory_type": "episodic count-based memory (exploration bonus)",
            "memory_architecture": "Counts of states or state-signatures (episodic) are stored and consulted to compute exploration bonuses; details inherited from cited work [50].",
            "memory_retrieval_mechanism": "Lookup of counts by state/hashes; recency / novelty-based retrieval.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Counts of visited states or hashed state signatures during an episode.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Cited as an effective exploration mechanism; in this paper Tr-DRQN+ (which incorporates this episodic counting bonus) yields modest improvements over Tr-DQN.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2968.6",
            "source_info": {
                "paper_title": "Learning Dynamic Belief Graphs to Generalize on Text-Based Games",
                "publication_date_yy_mm": "2020-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2
        },
        {
            "paper_title": "Counting to explore and generalize in text-based games",
            "rating": 2
        },
        {
            "paper_title": "Building dynamic knowledge graphs from text-based games",
            "rating": 2
        },
        {
            "paper_title": "Learning to speak and act in a fantasy text adventure game",
            "rating": 1
        }
    ],
    "cost": 0.019895249999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning Dynamic Belief Graphs to Generalize on Text-Based Games</h1>
<p>Ashutosh Adhikari ${ }^{\dagger <em>}$ Xingdi Yuan ${ }^{\text { }}$ * Marc-Alexandre Côte ${ }^{\text { } </em>}$ * Mikuláš Zelinka ${ }^{\dagger}$ Marc-Antoine Rondeau ${ }^{\circ}$<br>$\underset{\text { Adam Trischler }{ }^{\circ}}{\text { Romain Laroche }}$ William L. Hamilton ${ }^{\circ}$<br>${ }^{\dagger}$ University of Waterloo ${ }^{\circ}$ Microsoft Research, Montréal ${ }^{\dagger}$ Charles University<br>${ }^{\text {Mila }} \quad{ }^{\circ}$ McGill University ${ }^{\text { }}$ HEC Montréal ${ }^{\ddagger}$ Vector Institute eric.yuan@microsoft.com</p>
<h4>Abstract</h4>
<p>Playing text-based games requires skills in processing natural language and sequential decision making. Achieving human-level performance on text-based games remains an open challenge, and prior research has largely relied on hand-crafted structured representations and heuristics. In this work, we investigate how an agent can plan and generalize in text-based games using graph-structured representations learned end-to-end from raw text. We propose a novel graph-aided transformer agent (GATA) that infers and updates latent belief graphs during planning to enable effective action selection by capturing the underlying game dynamics. GATA is trained using a combination of reinforcement and self-supervised learning. Our work demonstrates that the learned graph-based representations help agents converge to better policies than their text-only counterparts and facilitate effective generalization across game configurations. Experiments on 500+ unique games from the TextWorld suite show that our best agent outperforms text-based baselines by an average of $24.2 \%$.</p>
<h2>1 Introduction</h2>
<p>Text-based games are complex, interactive simulations in which the game state is described with text and players act using simple text commands (e.g., light torch with match). They serve as a proxy for studying how agents can exploit language to comprehend and interact with the environment. Text-based games are a useful challenge in the pursuit of intelligent agents that communicate with humans (e.g., in customer service systems).</p>
<p>Solving text-based games requires a combination of reinforcement learning (RL) and natural language processing (NLP) techniques. However, inherent challenges like partial observability, long-term dependencies, sparse rewards, and combinatorial action spaces make these games very difficult. ${ }^{2}$ For instance, Hausknecht et al. [16] show that a state-of-the-art model achieves a mere $2.56 \%$ of the total possible score on a curated set of text-based games for human players [5]. On the other hand, while text-based games exhibit many of the same difficulties as linguistic tasks like open-ended dialogue, they are more structured and constrained.</p>
<p>To design successful agents for text-based games, previous works have relied largely on heuristics that exploit games' inherent structure. For example, several works have proposed rule-based components</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: GATA playing a text-based game by updating its belief graph. In response to action $A_{t-1}$, the environment returns text observation $O_{t}$. Based on $O_{t}$ and $\mathcal{G}<em t="t">{t-1}$, the agent updates $\mathcal{G}</em>$. In the figure, blue box with squares is the game engine, green box with diamonds is the graph updater, red box with slashes is the action selector.
that prune the action space or shape the rewards according to a priori knowledge of the game dynamics [50, 24, 1, 48]. More recent approaches take advantage of the graph-like structure of textbased games by building knowledge graph (KG) representations of the game state: Ammanabrolu and Riedl [4], Ammanabrolu and Hausknecht [3], for example, use hand-crafted heuristics to populate a KG that feeds into a deep neural agent to inform its policy. Despite progress along this line, we expect more general, effective representations for text-based games to arise in agents that learn and scale more automatically, which replace heuristics with learning [37].
This work investigates how we can learn graph-structured state representations for text-based games in an entirely data-driven manner. We propose the graph aided transformer agent (GATA) ${ }^{3}$ that, in lieu of heuristics, learns to construct and update graph-structured beliefs ${ }^{4}$ and use them to further optimize rewards. We introduce two self-supervised learning strategies—based on text reconstruction and mutual information maximization-which enable our agent to learn latent graph representations without direct supervision or hand-crafted heuristics.
We benchmark GATA on 500+ unique games generated by TextWorld [9], evaluating performance in a setting that requires generalization across different game configurations. We show that GATA outperforms strong baselines, including text-based models with recurrent policies. In addition, we compare GATA to agents with access to ground-truth graph representations of the game state. We show that GATA achieves competitive performance against these baselines even though it receives only partial text observations of the state. Our findings suggest, promisingly, that graph-structured representations provide a useful inductive bias for learning and generalizing in text-based games, and act as a memory enabling agents to optimize rewards in a partially observed setting.}$ and selects a new action $A_{t</p>
<h1>2 Background</h1>
<p>Text-based Games: Text-based games can be formally described as partially observable Markov decision processes (POMDPs) [9]. They are environments in which the player receives text-only observations $O_{t}$ (these describe the observable state, typically only partially) and interacts by issuing short text phrases as actions $A_{t}$ (e.g., in Figure 1, go west moves the player to a new location). Often, the end goal is not clear from the start; the agent must infer the objective by earning sparse rewards for completing subgoals. Text-based games have a variety of difficulty levels determined mainly by the environment's complexity (i.e., how many locations in the game, and how many objects are interactive), the game length (i.e., optimally, how many actions are required to win), and the verbosity (i.e., how much text information is irrelevant to solving the game).</p>
<p>Problem Setting: We use TextWorld [9] to generate unique choice-based games of varying difficulty. All games share the same overarching theme: an agent must gather and process cooking ingredients, placed randomly across multiple locations, according to a recipe it discovers during the game. The agent earns a point for collecting each ingredient and for processing it correctly. The game is won</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>upon completing the recipe. Processing any ingredient incorrectly terminates the game (e.g., slice carrot when the recipe asked for a diced carrot). To process ingredients, an agent must find and use appropriate tools (e.g., a knife to slice, dice, or chop; a stove to fry, an oven to roast).</p>
<p>We divide generated games, all of which have unique recipes and map configurations, into sets for training, validation, and test. Adopting the supervised learning paradigm for evaluating generalization, we tune hyperparameters on the validation set and report performance on a test set of previously unseen games. Testing agents on unseen games (within a difficulty level) is uncommon in prior RL work, where it is standard to train and test on a single game instance. Our approach enables us to measure the robustness of learned policies as they generalize (or fail to) across a "distribution" of related but distinct games. Throughout the paper, we use the term generalization to imply the ability of a single policy to play a distribution of related games (within a particular difficulty level).</p>
<p>Graphs and Text-based Games: We expect graph-based representations to be effective for textbased games because the state in these games adheres to a graph-like structure. The essential content in most observations of the environment corresponds either to entity attributes (e.g., the state of the carrot is sliced) or to relational information about entities in the environment (e.g., the kitchen is north_of the bedroom). This information is naturally represented as a dynamic graph $\mathcal{G}<em t="t">{t}=\left(\mathcal{V}</em>}, \mathcal{E<em t="t">{t}\right)$, where the vertices $\mathcal{V}</em>$ represent relations between entities (e.g., north_of, in, is) that hold at a particular time-step $t$. By design, in fact, the full state of any game generated by TextWorld can be represented explicitly as a graph of this type [53]. The aim of our model, GATA, is to estimate the game state by learning to build graph-structured beliefs from raw text observations. In our experiments, we benchmark GATA against models with direct access to the ground-truth game state rather than GATA's noisy estimate thereof inferred from text.}$ represent entities (including the player, objects, and locations) and their current conditions (e.g., closed, fried, sliced), while the edges $\mathcal{E}_{t</p>
<h1>3 Graph Aided Transformer Agent (GATA)</h1>
<p>In this section, we introduce GATA, a novel transformer-based neural agent that can infer a graphstructured belief state and use that state to guide action selection in text-based games. As shown in Figure 2, the agent consists of two main modules: a graph updater and an action selector. ${ }^{3}$ At game step $t$, the graph updater extracts relevant information from text observation $O_{t}$ and updates its belief graph $\mathcal{G}<em t="t">{t}$ accordingly. The action selector issues action $A</em>$. Figure 1 illustrates the interaction between GATA and a text-based game.}$ conditioned on $O_{t}$ and the belief graph $\mathcal{G}_{t</p>
<h3>3.1 Belief Graph</h3>
<p>We denote by $\mathcal{G}$ a belief graph representing the agent's belief about the true game state according to what it has observed so far. We instantiate $\mathcal{G} \in[-1,1]^{\mathcal{R} \times \mathcal{N} \times \mathcal{N}}$ as a real-valued adjacency tensor, where $\mathcal{R}$ and $\mathcal{N}$ indicate the number of relation types and entities. Each entry ${r, i, j}$ in $\mathcal{G}$ indicates the strength of an inferred relationship $r$ from entity $i$ to entity $j$. We select $\mathcal{R}=10$ and $\mathcal{N}=99$ to match the maximum number of relations and entities in our TextWorld-generated games. In other words, we assume that GATA has access to the vocabularies of possible relations and entities but it must learn the structure among these objects, and their semantics, from scratch.</p>
<h3>3.2 Graph Updater</h3>
<p>The graph updater constructs and updates the dynamic belief graph $\mathcal{G}$ from text observations $O_{t}$. Rather than generating the entire belief graph at each step $t$, we generate a graph update, $\Delta g_{t}$, that represents the change of the agent's belief after receiving a new observation. This is motivated by the fact that observations $O_{t}$ typically communicate only incremental information about the state's change from time step $t-1$ to $t$. The relation between $\Delta g_{t}$ and $\mathcal{G}$ is given by</p>
<p>$$
\mathcal{G}<em t-1="t-1">{t}=\mathcal{G}</em>
$$} \oplus \Delta g_{t</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: GATA in detail. The coloring scheme is same as in Figure 1. The graph updater first generates $\Delta g_{t}$ using $\mathcal{G}<em t="t">{t-1}$ and $O</em>}$. Afterwards the action selector uses $O_{t}$ and the updated graph $\mathcal{G<em t="t">{t}$ to select $A</em>$. Purple dotted line indicates a detached connection (i.e., no back-propagation through such connection).}$ from the list of action candidates $C_{t</p>
<p>where $\oplus$ is a graph operation function that produces the new belief graph $\mathcal{G}<em t-1="t-1">{t}$ given $\mathcal{G}</em>$. We formulate the graph operation function $\oplus$ using a recurrent neural network (e.g., a GRU [8]) as:}$ and $\Delta g_{t</p>
<p>$$
\begin{aligned}
\Delta g_{t} &amp;= f_{\Delta}(h_{\mathcal{G}<em O__t="O_{t">{t-1}}, h</em>); \
h_{t} &amp;= R N N(\Delta g_{t}, h_{t-1}); \
\mathcal{G}}}, h_{A_{t-1}<em d="d">{t} &amp;= f</em>).
\end{aligned}
$$}(h_{t</p>
<p>The function $f_{\Delta}$ aggregates the information in $\mathcal{G}<em t-1="t-1">{t-1}$, $A</em>}$, and $O_{t}$ to generate the graph update $\Delta g_{t}$. $h_{\mathcal{G<em t-1="t-1">{t-1}}$ denotes the representation of $\mathcal{G}</em>}$ from the graph encoder. $h_{O_{t}}$ and $h_{A_{t-1}}$ are outputs of the text encoder (refer to Figure 2, left part). The vector $h_{t}$ is a recurrent hidden state from which we decode the adjacency tensor $\mathcal{G<em t="t">{t}$; $h</em>$). We elaborate on each of the sub-modules in Appendix A.}$ acts as a memory that carries information across game steps—a crucial function for solving POMDPs [15]. The function $f_{d}$ is a multi-layer perceptron (MLP) that decodes the recurrent state $h_{t}$ into a real-valued adjacency tensor (i.e., the belief graph $\mathcal{G}_{t</p>
<p><strong>Training the Graph Updater:</strong> We pre-train the graph updater using two self-supervised training regimes to learn structured game dynamics. After pre-training, the graph updater is fixed during GATA's interaction with games; at this time it provides belief graphs $\mathcal{G}$ to the action selector. We train the action selector subsequently via RL. Both pre-training tasks share the same goal: to ensure that $\mathcal{G}_{t}$ encodes sufficient information about the environment state at game step $t$. For training data, we gather a collection of transitions by following walkthroughs in <em>FTWP</em> games. 6 To ensure variety in the training data, we also randomly sample trajectories off the optimal path. Next we describe our pre-training approaches for the graph updater.</p>
<ul>
<li><strong>Observation Generation (OG):</strong> Our first approach to pre-train the graph updater involves training a decoder model to reconstruct text observations from the belief graph. Conditioned on the belief graph, $\mathcal{G}<em t-1="t-1">{t}$, and the action performed at the previous game step, $A</em>}$, the observation generation task aims to reconstruct $O_{t} = {O_{t}^{1}, \ldots, O_{t}^{L_{O_{t}}}}$ token by token, where $L_{O_{t}}$ is the length of $O_{t}$. We formulate this task as a sequence-to-sequence (Seq2Seq) problem and use a transformer-based model [43] to generate the output sequence. Specifically, conditioned on $\mathcal{G<em t-1="t-1">{t}$ and $A</em>}$. We train the Seq2Seq model using teacher-forcing to optimize the negative log-likelihood loss:}$, the transformer decoder predicts the next token $O_{t}^{i}$ given ${O_{t}^{1}, \ldots, O_{t}^{i-1</li>
</ul>
<p>$$
\mathcal{L}<em i="1">{\text{OG}} = -\sum</em>}^{L_{O_{t}}} \log p_{\text{OG}}(O_{t}^{i} \mid O_{t}^{1}, \ldots, O_{t}^{i-1}, \mathcal{G<em t-1="t-1">{t}, A</em>),
$$</p>
<p>where $p_{\text{OG}}$ is the conditional distribution parametrized by the observation generation model.</p>
<ul>
<li><strong>Contrastive Observation Classification (COC):</strong> Inspired by the literature on contrastive representation learning [41, 19, 44, 7], we reformulate OG mentioned above as a contrastive prediction task. We use contrastive learning to maximize mutual information between the predicted $\mathcal{G}<em t="t">{t}$ and the text observations $O</em>}$. Specifically, we train the model to differentiate between representations corresponding to true observations $O_{t}$ and "corrupted" observations $\widetilde{O<em t="t">{t}$, conditioned on $\mathcal{G}</em>$. To obtain corrupted observations, we sample randomly from the set of all collected observations across our pre-training data. We use a noise-contrastive objective and minimize the binary cross-entropy (BCE) loss given by}$ and $A_{t-1</li>
</ul>
<p>$$
\mathcal{L}<em t="1">{\text{COC}} = \frac{1}{K} \sum</em>}^{K} \left( \mathbb{E<em O__t="O_{t">{O} \left[ \log \mathcal{D}(h</em>}}, h_{\mathcal{G<em _widetilde_O="\widetilde{O">{t}}) \right] + \mathbb{E}</em>}} \left[ \log (1 - \mathcal{D}(h_{\widetilde{O<em _mathcal_G="\mathcal{G">{t}}, h</em>)) \right] \right).
$$}_{t}</p>
<p><sup>6</sup>This is an independent and unique set of TextWorld games [39]. Details are provided in Appendix F.</p>
<p>Here, $K$ is the length of a trajectory as we sample a positive and negative pair at each step and $\mathcal{D}$ is a discriminator that differentiates between positive and negative samples. The motivation behind contrastive unsupervised training is that one does not require to train complex decoders. Specifically, compared to OG, the COC’s objective relaxes the need for learning syntactical or grammatical features and allows GATA to focus on learning the semantics of the $O_{t}$.</p>
<p>We provide further implementation level details on both these self-supervised objectives in Appendix B.</p>
<h3>3.3 Action Selector</h3>
<p>The graph updater discussed in the previous section defines a key component of GATA that enables the model to maintain a structured belief graph based on text observations. The second key component of GATA is the action selector, which uses the belief graph $\mathcal{G}<em t="t">{t}$ and the text observation $O</em>$ at each time-step to select an action. As shown in Figure 2, the action selector consists of four main components: the text encoder and graph encoder convert text inputs and graph inputs, respectively, into hidden representations; a representation aggregator fuses the two representations using an attention mechanism; and a scorer ranks all candidate actions based on the aggregated representations.</p>
<ul>
<li>Graph Encoder: GATA’s belief graphs, which estimate the true game state, are multi-relational by design. Therefore, we use relational graph convolutional networks (R-GCNs) [32] to encode the belief graphs from the updater into vector representations. We also adapt the R-GCN model to use embeddings of the available relation labels, so that we can capture semantic correspondences among relations (e.g., east_of and west_of are reciprocal relations). We do so by learning a vector representation for each relation in the vocabulary that we condition on the word embeddings of the relation’s name. We concatenate the resulting vector with the standard node embeddings during R-GCN’s message passing phase. Our R-GCN implementation uses basis regularization [32] and highway connections [36] between layers for faster convergence. Details are given in Appendix A.1.</li>
<li>Text Encoder: We adopt a transformer encoder [43] to convert text inputs from $O_{t}$ and $A_{t-1}$ into contextual vector representations. Details are provided in Appendix A.2.</li>
<li>Representation Aggregator: To combine the text and graph representations, GATA uses a bi-directional attention-based aggregator [49, 33]. Attention from text to graph enables the agent to focus more on nodes that are currently observable, which are generally more relevant; attention from nodes to text enables the agent to focus more on tokens that appear in the graph, which are therefore connected with the player in certain relations. Details are provided in Appendix A.3.</li>
<li>Scorer: The scorer consists of a self-attention layer cascaded with an MLP layer. First, the self-attention layer reinforces the dependency of every token-token pair and node-node pair in the aggregated representations. The resulting vectors are concatenated with the representations of action candidates $C_{t}$ (from the text encoder), after which the MLP generates a single scalar for every action candidate as a score. Details are provided in Appendix A.4.</li>
</ul>
<p>Training the Action Selector: We use Q-learning [45] to optimize the action selector on reward signals from the training games. Specifically, we use Double DQN [42] combined with multi-step learning [38] and prioritized experience replay [31]. To enable GATA to scale and generalize to multiple games, we adapt standard deep Q-Learning by sampling a new game from the set of training games to collect an episode. Consequently, the replay buffer contains transitions from episodes of different games. We provide further details on this training procedure in Appendix E.2.</p>
<h3>3.4 Variants Using Ground-Truth Graphs</h3>
<p>In GATA, the belief graph is learned entirely from text observations. However, the TextWorld API also provides access to the underlying graph states for games, in the format of discrete KGs. Thus, for comparison, we also consider two models that learn from or encode ground-truth graphs directly.</p>
<p>GATA-GTP: Pre-training a discrete graph updater using ground-truth graphs. We first consider a model that uses ground-truth graphs to pre-train the graph updater, in lieu of self-supervised methods. GATA-GTP uses ground-truth graphs from FTWP during pre-training, but infers belief graphs from the raw text during RL training of the action selector to compare fairly against GATA. Here, the belief graph $\mathcal{G}_{t}$ is a discrete multi-relational graph. To pre-train a discrete graph updater, we adapt the</p>
<p>command generation approach proposed by Zelinka et al. [53]. We provide details of this approach in Appendix C.</p>
<p>GATA-GTF: Training the action selector using ground-truth graphs. To get a sense of the upper bound on performance we might obtain using a belief graph, we also train an agent that uses the full ground-truth graph $\mathcal{G}^{\text {full }}$ during action selection. This agent requires no graph updater module; we simply feed the ground-truth graphs into the action selector (via the graph encoder). The use of ground-truth graphs allows GATA-GTF to escape the error cascades that may result from inferred belief graphs. Note also that the ground-truth graphs contain full state information, relaxing partial observability of the games. Consequently, we expect more effective reward optimization for GATAGTF compared to other graph-based agents. GATA-GTF's comparison with text-based agents is a sanity check for our hypothesis-that structured representations help learning general policies.</p>
<h1>4 Experiments and Analysis</h1>
<p>We conduct experiments on generated text-based games (Section 2) to answer two key questions: Q1: Does the belief-graph approach aid GATA in achieving high rewards on unseen games after training? In particular, does GATA improve performance compared to SOTA text-based models?
Q2: How does GATA compare to models that have access to ground-truth graph representations?</p>
<h3>4.1 Experimental Setup and Baselines</h3>
<p>We divide the games into four subsets with one difficulty level per subset. Each subset contains 100 training, 20 validation, and 20 test games, which are sampled from a distribution determined by their difficulty level. To elaborate on the diversity of games: for easier games, the recipe might only require a single ingredient and the world is limited to a single location, whereas harder games might require an agent to navigate a map of 6 locations to collect and appropriately process up to three ingredients. We also test GATA's transferability across difficulty levels by mixing the four difficulty levels to build level 5 . We sample 25 games from each of the four difficulty levels to build a training set. We use all validation and test games from levels 1 to 4 for level 5 validation and test. In all experiments, we select the top-performing agent on validation sets and report its test scores; all validation and test games are unseen in the training set. Statistics of the games are shown in Table 1.</p>
<p>Table 1: Games statistics (averaged across all games within a difficulty level).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Level</th>
<th style="text-align: center;">Recipe Size</th>
<th style="text-align: center;">#Locations</th>
<th style="text-align: center;">Max Score</th>
<th style="text-align: center;">Need Cut</th>
<th style="text-align: center;">Need Cook</th>
<th style="text-align: center;">#Action Candidates</th>
<th style="text-align: center;">#Objects</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">17.1</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">17.5</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">34.1</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">33.4</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Mixture of levels $[1,2,3,4]$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>As baselines, we use our implementation of LSTM-DQN [29] and LSTM-DRQN [50], both of which use only $O_{t}$ as input. Note that LSTM-DRQN uses an RNN to enable an implicit memory (i.e., belief); it also uses an episodic counting bonus to encourage exploration [50]. This draws an interesting comparison with GATA, wherein the belief is extracted and updated dynamically, in the form of a graph. For fair comparison, we replace the LSTM-based text encoders with a transformer-based text encoder as in GATA. We denote those agents as Tr-DQN and Tr-DRQN respectively. We denote a Tr-DRQN equipped with the episodic counting bonus as Tr-DRQN+. These three text-based baselines are representative of the current top-performing neural agents on text-based games.</p>
<p>Additionally, we test the variants of GATA that have access to ground-truth graphs (as described in Section 3.4). Comparing with GATA, the GATA-GTP agent also maintains its belief graphs throughout the game; however, its graph updater is pre-trained on FTWP using ground-truth graphsa stronger supervision signal. GATA-GTF, on the other hand, does not have a graph updater. It directly uses ground-truth graphs as input during game playing.</p>
<p>Table 2: Agents' normalized test scores and averaged relative improvement ( $\%$ †) over Tr-DQN across difficulty levels. An agent m's relative improvement over Tr-DQN is defined as $\left(\mathrm{R}<em _mathrm_Tr="\mathrm{Tr">{\mathrm{m}}-\right.$ $\left.\mathrm{R}</em>}-\mathrm{DQN}}\right) / \mathrm{R<em t="t">{\mathrm{Tr}-\mathrm{DQN}}$ where R is the score. All numbers are percentages. $\diamond$ represents ground-truth full graph; represents discrete $\mathcal{G}</em>$ generated by GATA, when the graph updater is pre-trained with OG and COC tasks, respectively.}$ generated by GATA-GTP; represents $O_{t}$. $\cdot$ and $\infty$ are continuous $G_{t</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">20 Training Games</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">100 Training Games</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Difficulty Level</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$\%$ †</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$\%$ †</td>
<td style="text-align: center;">$\%$ †</td>
</tr>
<tr>
<td style="text-align: center;">Agent</td>
<td style="text-align: center;">Text-based Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Tr-DQN</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Tr-DRQN</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">$+10.3$</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">$-2.6$</td>
<td style="text-align: center;">$+3.9$</td>
</tr>
<tr>
<td style="text-align: center;">Tr-DRQN+</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">$+10.7$</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">$-3.4$</td>
<td style="text-align: center;">$+3.6$</td>
</tr>
<tr>
<td style="text-align: center;">Input</td>
<td style="text-align: center;">GATA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\cdot$</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">$-0.2$</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">$+16.1$</td>
<td style="text-align: center;">$+8.0$</td>
</tr>
<tr>
<td style="text-align: center;">$\cdot \star$</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">$+24.8$</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">$+16.1$</td>
<td style="text-align: center;">$+20.4$</td>
</tr>
<tr>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">$+27.1$</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">$+13.2$</td>
<td style="text-align: center;">$+20.2$</td>
</tr>
<tr>
<td style="text-align: center;">$\infty \star$</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">$+34.9$</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">33.4</td>
<td style="text-align: center;">$+13.6$</td>
<td style="text-align: center;">$+24.2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GATA-GTP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\diamond$</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">$+16.6$</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">$-18.9$</td>
<td style="text-align: center;">$-1.2$</td>
</tr>
<tr>
<td style="text-align: center;">$\diamond \star$</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">12.3</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">$+24.6$</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">$+5.2$</td>
<td style="text-align: center;">$+14.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GATA-GTF</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\cdot$</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">$+64.2$</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">$+99.0$</td>
<td style="text-align: center;">$+81.6$</td>
</tr>
</tbody>
</table>
<h1>Q1: Performance of GATA compared to text-based baselines</h1>
<p>In Table 2, we show the normalized test scores achieved by agents trained on either 20 or 100 games for each difficulty level. Equipped with belief graphs, GATA significantly outperforms all text-based baselines. The graph updater pre-trained on both of the self-supervised tasks (Section 3.2) leads to better performance than the baselines ( $\cdot$ and $\infty$ ). We observe further improvements in GATA's policies when the text observations $(\boldsymbol{\Delta})$ are also available. We believe the text observations guide GATA's action scorer to focus on currently observable objects through the bi-attention mechanism. The attention may further help GATA to counteract accumulated errors from the belief graphs. In addition, we observe that Tr-DRQN and Tr-DRQN+ outperform Tr-DQN, with $3.9 \%$ and $3.6 \%$ relative improvement $(\% \uparrow)$. This suggests the implicit memory of the recurrent components improves performance. We also observe GATA substantially outperforms Tr-DQN when trained on 100 games, whereas the DRQN agents struggle to optimize rewards on the larger training sets.</p>
<h2>Q2: Performance of GATA compared to models with access to the ground-truth graph</h2>
<p>Table 2 also reports test performance for GATA-GTP ( $\boldsymbol{\Delta}$ ) and GATA-GTF ( $\boldsymbol{\Delta}$ ). Consistent with GATA, we find GATA-GTP also performs better when given text observations $(\boldsymbol{\Delta})$ as additional input to the action scorer. Although GATA-GTP outperforms Tr-DQN by $14.9 \%$ when text observations are available, its overall performance is still substantially poorer than GATA. Although the graph updater in GATA-GTP is trained with ground-truth graphs, we believe the discrete belief graphs and the discrete operations for updating them (Appendix C.1) make this approach vulnerable to an accumulation of errors over game steps, as well as errors introduced by the discrete nature of the predictions (e.g., round-off error). In contrast, we suspect that the continuous belief graph and the learned graph operation function (Eqn. 2) are easier to train and recover more gracefully from errors.
Meanwhile, GATA-GTF, which uses ground-truth graphs $\mathcal{G}^{\text {full }}$ during training and testing, obtains significantly higher scores than does GATA and all other baselines. Because $\mathcal{G}^{\text {full }}$ turns the game environment into a fully observable MDP and encodes accurate state information with no error accumulation, GATA-GTF represents the performance upper-bound of all the $\mathcal{G}_{t}$-based baselines. The scores achieved by GATA-GTF reinforce our intuition that belief graphs improve text-based game</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Left: Training curves on 20 level 2 games (averaged over 3 seeds). Right: Density comparison between a ground-truth graph (binary) and a belief graph G generated by the COC pre-training procedure. Both matrices are slices of adjacency tensors corresponding the is relation.</p>
<p>agents. At the same time, the performance gap between GATA and GATA-GTF invites investigation into better ways to learn accurate graph representations of text.</p>
<h3>Additional Results</h3>
<p>We also show the agents' training curves and examples of the belief graphs G generated by GATA. Figure 3 (<strong>Left</strong>) shows an example of all agents' training curves. We observe consistent trends with the testing results of Table 2 — GATA outperforms the text-based baselines and GATA-GTP, but a significant gap exists between GATA and GATA-GTF (which uses ground-truth graphs as input to the action scorer). Figure 3 (<strong>Right</strong>) highlights the sparsity of a ground-truth graph compared to that of a belief graph G. Since generation of G is unsupervised by any ground-truth graphs, we do not expect G to be interpretable nor sparse. Further, since the self-supervised models learn belief graphs directly from text, some of the learned features may correspond to the underlying grammar or other features useful for the self-supervised tasks, rather than only being indicative of relationships between objects. However, we show G encodes useful information for a relation prediction probing task in Appendix D.5.</p>
<p>Given space limitations, we only report a representative selection of our results in this section. Appendix D provides an exhaustive set of results including training curves, training scores, and test scores for all experimental settings introduced in this work. We also provide a detailed qualitative analysis including hi-res visualizations of the belief graphs. We encourage readers to refer to it.</p>
<h1>5 Related Work</h1>
<p><strong>Dynamic graph extraction:</strong> Numerous recent works have focused on constructing graphs to encode structured representations of raw data, for various tasks. Kipf et al. [23] propose contrastive methods to learn latent structured world models (C-SWMs) as state representations for vision-based environments. Their work, however, does not focus on learning policies to play games or to generalize across varying environments. Das et al. [10] leverage a machine reading comprehension mechanism to query for entities and states in short text passages and use a dynamic graph structure to track changing entity states. Fan et al. [12] propose to encode graph representations by linearizing the graph as an input sequence in NLP tasks. Johnson [21] construct graphs from text data using gated graph transformer neural networks. Yang et al. [46] learn transferable latent relational graphs from raw data in a self-supervised manner. Compared to the existing literature, our work aims to infer multi-relational KGs dynamically from partial text observations of the state and subsequently use these graphs to inform general policies. Concurrently, Srinivas et al. [35] propose to learn state representations with contrastive learning methods to facilitate RL training. However, they focus on vision-based environments and they do not investigate generalization.</p>
<p>More generally, we want to note that compared to traditional knowledge base construction (KBC) works, our approach is more related to the direction of neural relational inference [22]. In particular, we seek to generate task-specific graphs, which tend to be dynamic, contextual and relatively small, whereas traditional KBC focus on generating large, static graphs.</p>
<p><strong>Playing Text-based Games:</strong> Recent years have seen a host of work on playing text-based games. Various deep learning agents have been explored [29, 17, 14, 51, 20, 3, 52, 47]. Fulda et al. [13] use pre-trained embeddings to reduce the action space. Zahavy et al. [51], Seurin et al. [34], and</p>
<p>Jain et al. [20] explicitly condition an agent's decisions on game feedback. Most of this literature trains and tests on a single game without considering generalization. Urbanek et al. [40] use memory networks and ranking systems to tackle adventure-themed dialog tasks. Yuan et al. [50] propose a count-based memory to explore and generalize on simple unseen text-based games. Madotto et al. [26] use GoExplore [11] with imitation learning to generalize. Adolphs and Hofmann [1] and Yin and May [48] also investigate the multi-game setting. These methods rely either on reward shaping by heuristics, imitation learning, or rule-based features as inputs. We aim to minimize hand-crafting, so our action selector is optimized only using raw rewards from games while other components of our model are pre-trained on related data. Recently, Ammanabrolu and Riedl [4], Ammanabrolu and Hausknecht [3], Yin and May [48] leverage graph structure by using rule-based, untrained mechanisms to construct KGs to play text-based games.</p>
<h1>6 Conclusion</h1>
<p>In this work, we investigate how an RL agent can play and generalize within a distribution of textbased games using graph-structured representations inferred from text. We introduce GATA, a novel neural agent that infers and updates latent belief graphs as it plays text-based games. We use a combination of RL and self-supervised learning to teach the agent to encode essential dynamics of the environment in its belief graphs. We show that GATA achieves good test performance, outperforming a set of strong baselines including agents pre-trained with ground-truth graphs. This evinces the effectiveness of generating graph-structured representations for text-based games.</p>
<h2>7 Broader Impact</h2>
<p>Our work's immediate aim-improved performance on text-based games-might have limited consequences for society; however, taking a broader view of our work and where we'd like to take it forces us to consider several social and ethical concerns. We use text-based games as a proxy to model and study the interaction of machines with the human world, through language. Any system that interacts with the human world impacts it. As mentioned previously, an example of language-mediated, human-machine interaction is online customer service systems.</p>
<ul>
<li>In these systems, especially in products related to critical needs like healthcare, providing inaccurate information could result in serious harm to users. Likewise, failing to communicate clearly, sensibly, or convincingly might also cause harm. It could waste users' precious time and diminish their trust.</li>
<li>The responses generated by such systems must be inclusive and free of bias. They must not cause harm by the act of communication itself, nor by making decisions that disenfranchise certain user groups. Unfortunately, many data-driven, free-form language generation systems currently exhibit bias and/or produce problematic outputs.</li>
<li>Users' privacy is also a concern in this setting. Mechanisms must be put in place to protect it. Agents that interact with humans almost invariably train on human data; their function requires that they solicit, store, and act upon sensitive user information (especially in the healthcare scenario envisioned above). Therefore, privacy protections must be implemented throughout the agent development cycle, including data collection, training, and deployment.</li>
<li>Tasks that require human interaction through language are currently performed by people. As a result, advances in language-based agents may eventually displace or disrupt human jobs. This is a clear negative impact.</li>
</ul>
<p>Even more broadly, any systems that generate convincing natural language could be used to spread misinformation.</p>
<p>Our work is immediately aimed at improving the performance of RL agents in text-based games, in which agents must understand and act in the world through language. Our hope is that this work, by introducing graph-structured representations, endows language-based agents with greater accuracy and clarity, and the ability to make better decisions. Similarly, we expect that graph-structured representations could be used to constrain agent decisions and outputs, for improved safety. Finally, we believe that structured representations can improve neural agents' interpretability to researchers and users. This is an important future direction that can contribute to accountability and transparency</p>
<p>in AI. As we have outlined, however, this and future work must be undertaken with awareness of its hazards.</p>
<h1>8 Acknowledgements</h1>
<p>We thank Alessandro Sordoni and Devon Hjelm for the helpful discussions about the probing task. We also thank David Krueger, Devendra Singh Sachan, Harm van Seijen, Harshita Sahijwani, Jacob Miller, Koustuv Sinha, Loren Lugosch, Meng Qu, Travis LaCroix, and the anonymous ICML 2020 and NeurIPS 2020 reviewers and ACs for their insightful comments on an earlier draft of this work. The work was funded in part by an academic grant from Microsoft Research, an NSERC Discovery Grant RGPIN-2019-05123, an IVADO Fundamental Research Project Grant PRF-2019-3583139727, as well as Canada CIFAR Chairs in AI, held by Prof. Hamilton, Prof. Poupart and Prof. Tang.</p>
<h2>References</h2>
<p>[1] Adolphs, L. and Hofmann, T. (2019). Ledeepchef: Deep reinforcement learning agent for families of text-based games. CoRR, abs/1909.01646.
[2] Alain, G. and Bengio, Y. (2017). Understanding intermediate layers using linear classifier probes. ArXiv, abs/1610.01644.
[3] Ammanabrolu, P. and Hausknecht, M. (2020). Graph constrained reinforcement learning for natural language action spaces. In International Conference on Learning Representations.
[4] Ammanabrolu, P. and Riedl, M. (2019). Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3557-3565, Minneapolis, Minnesota. Association for Computational Linguistics.
[5] Atkinson, T., Baier, H., Copplestone, T., Devlin, S., and Swan, J. (2018). The text-based adventure ai competition. IEEE Transactions on Games, 11:260-266.
[6] Ba, L. J., Kiros, J. R., and Hinton, G. E. (2016). Layer normalization. CoRR, abs/1607.06450.
[7] Bachman, P., Hjelm, R. D., and Buchwalter, W. (2019). Learning representations by maximizing mutual information across views. In Advances in Neural Information Processing Systems, pages $15509-15519$.
[8] Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724-1734, Doha, Qatar. Association for Computational Linguistics.
[9] Côté, M.-A., Kádár, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Tao, R. Y., Hausknecht, M., Asri, L. E., Adada, M., Tay, W., and Trischler, A. (2018). Textworld: A learning environment for text-based games. CoRR, abs/1806.11532.
[10] Das, R., Munkhdalai, T., Yuan, X., Trischler, A., and McCallum, A. (2019). Building dynamic knowledge graphs from text using machine reading comprehension. In International Conference on Learning Representations.
[11] Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., and Clune, J. (2019). Go-explore: a new approach for hard-exploration problems. ArXiv, abs/1901.10995.
[12] Fan, A., Gardent, C., Braud, C., and Bordes, A. (2019). Using local knowledge graph construction to scale Seq2Seq models to multi-document inputs. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4186-4196, Hong Kong, China. Association for Computational Linguistics.</p>
<p>[13] Fulda, N., Ricks, D., Murdoch, B., and Wingate, D. (2017). What can you do with a rock? affordance extraction via word embeddings. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pages 1039-1045.
[14] Hausknecht, M., Ammanabrolu, P., Marc-Alexandre, C., and Yuan, X. (2020). Interactive fiction games: A colossal adventure. In Thirty-Fourth AAAI Conference on Artificial Intelligence.
[15] Hausknecht, M. and Stone, P. (2015). Deep recurrent q-learning for partially observable mdps. In AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents (AAAI-SDMIA15).
[16] Hausknecht, M. J., Loynd, R., Yang, G., Swaminathan, A., and Williams, J. D. (2019). Nail: A general interactive fiction agent. CoRR, abs/1902.04259.
[17] He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., and Ostendorf, M. (2016). Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630, Berlin, Germany. Association for Computational Linguistics.
[18] Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D. (2018). Rainbow: Combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence.
[19] Hjelm, D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and Bengio, Y. (2019). Learning deep representations by mutual information estimation and maximization. In ICLR 2019. ICLR.
[20] Jain, V., Fedus, W., Larochelle, H., Precup, D., and Bellemare, M. G. (2020). Algorithmic improvements for deep reinforcement learning applied to interactive fiction. In Thirty-Fourth AAAI Conference on Artificial Intelligence.
[21] Johnson, D. D. (2017). Learning graphical state transitions. In International Conference on Learning Representations (ICLR).
[22] Kipf, T., Fetaya, E., Wang, K.-C., Welling, M., and Zemel, R. (2018). Neural relational inference for interacting systems. arXiv preprint arXiv:1802.04687.
[23] Kipf, T., van der Pol, E., and Welling, M. (2020). Contrastive learning of structured world models. In International Conference on Learning Representations.
[24] Lima, P. (2019). First textworld challenge - first place solution.
[25] Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. (2020). On the variance of the adaptive learning rate and beyond. In Proceedings of the Eighth International Conference on Learning Representations (ICLR 2020).
[26] Madotto, A., Namazifar, M., Huizinga, J., Molino, P., Ecoffet, A., Zheng, H., Papangelis, A., Yu, D., Khatri, C., and Tur, G. (2020). Exploration based language learning for text-based games.
[27] Meng, R., Yuan, X., Wang, T., Brusilovsky, P., Trischler, A., and He, D. (2019). Does order matter? an empirical study on generating multiple keyphrases as a sequence. CoRR.
[28] Mikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., and Joulin, A. (2018). Advances in pre-training distributed word representations. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018).
[29] Narasimhan, K., Kulkarni, T., and Barzilay, R. (2015). Language understanding for textbased games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1-11, Lisbon, Portugal. Association for Computational Linguistics.
[30] Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. (2017). Automatic differentiation in pytorch. In NIPS-W.
[31] Schaul, T., Quan, J., Antonoglou, I., and Silver, D. (2016). Prioritized experience replay. In International Conference on Learning Representations, Puerto Rico.</p>
<p>[32] Schlichtkrull, M., Kipf, T. N., Bloem, P., Van Den Berg, R., Titov, I., and Welling, M. (2018). Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593-607. Springer.
[33] Seo, M. J., Kembhavi, A., Farhadi, A., and Hajishirzi, H. (2017). Bidirectional attention flow for machine comprehension. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.
[34] Seurin, M., Preux, P., and Pietquin, O. (2019). "i'm sorry dave, i'm afraid i can't do that" deep q-learning from forbidden action. CoRR, abs/1910.02078.
[35] Srinivas, A., Laskin, M., and Abbeel, P. (2020). Curl: Contrastive unsupervised representations for reinforcement learning. arXiv preprint arXiv:2004.04136.
[36] Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway networks. CoRR, abs/1505.00387.
[37] Sutton, R. (2019). The Bitter Lesson.
[38] Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning, 3(1):9-44.
[39] Trischler, A., Côté, M.-A., and Lima, P. (2019). First TextWorld Problems, the competition: Using text-based games to advance capabilities of AI agents.
[40] Urbanek, J., Fan, A., Karamcheti, S., Jain, S., Humeau, S., Dinan, E., Rocktäschel, T., Kiela, D., Szlam, A., and Weston, J. (2019). Learning to speak and act in a fantasy text adventure game. CoRR, abs/1903.03094.
[41] van den Oord, A., Li, Y., and Vinyals, O. (2018). Representation learning with contrastive predictive coding. CoRR, abs/1807.03748.
[42] van Hasselt, H., Guez, A., and Silver, D. (2015). Deep reinforcement learning with double q-learning. In AAAI.
[43] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. (2017). Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems 30, pages 5998-6008. Curran Associates, Inc.
[44] Veličković, P., Fedus, W., Hamilton, W. L., Liò, P., Bengio, Y., and Hjelm, R. D. (2019). Deep Graph Infomax. In International Conference on Learning Representations.
[45] Watkins, C. J. C. H. and Dayan, P. (1992). Q-learning. Machine Learning, 8(3):279-292.
[46] Yang, Z., Zhao, J., Dhingra, B., He, K., Cohen, W. W., Salakhutdinov, R. R., and LeCun, Y. (2018). Glomo: Unsupervised learning of transferable relational graphs. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information Processing Systems 31, pages 8950-8961. Curran Associates, Inc.
[47] Yin, X. and May, J. (2019a). Comprehensible context-driven text game playing. In 2019 IEEE Conference on Games (CoG), pages 1-8. IEEE.
[48] Yin, X. and May, J. (2019b). Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and bandit feedback to learn families of text-based adventure games. CoRR, abs/1908.04777.
[49] Yu, A. W., Dohan, D., Luong, M., Zhao, R., Chen, K., Norouzi, M., and Le, Q. V. (2018). Qanet: Combining local convolution with global self-attention for reading comprehension. In International Conference on Learning Representations.
[50] Yuan, X., Côté, M.-A., Sordoni, A., Laroche, R., Combes, R. T. d., Hausknecht, M., and Trischler, A. (2018). Counting to explore and generalize in text-based games. arXiv preprint arXiv:1806.11525.</p>
<p>[51] Zahavy, T., Haroush, M., Merlis, N., Mankowitz, D. J., and Mannor, S. (2018). Learn what not to learn: Action elimination with deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 3562-3573.
[52] Zelinka, M. (2018). Baselines for reinforcement learning in text games. 2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI), pages 320-327.
[53] Zelinka, M., Yuan, X., Cote, M.-A., Laroche, R., and Trischler, A. (2019). Building dynamic knowledge graphs from text-based games. arXiv preprint arXiv:1910.09532.</p>
<h1>Contents in Appendices:</h1>
<ul>
<li>In Appendix A, we describe each of the components in GATA in detail.</li>
<li>In Appendix B, we provide detailed information on how we pre-train GATA's graph updater with the two proposed methods (i.e., OG and COC).</li>
<li>In Appendix C, we provide detailed information on GATA-GTP, the discrete version of GATA. Since the action scorer module is the same as in GATA, this appendix elaborates on how a discrete graph updater works and how to pre-train the discrete graph updater.</li>
<li>In Appendix D, we provide additional results and discussions. This includes training curves, training scores, testing scores, and high-res examples of the belief graphs learned by GATA. We provide a set of probing experiments to show that the belief graphs learned by GATA can capture useful information for relation classification tasks. We also provide qualitative analysis on GATA's OG task, which also suggests the belief graphs contain useful information for reconstructing the text observation $O_{t}$.</li>
<li>In Appendix E, we provide implementation details for all our experiments.</li>
<li>In Appendix F, we show examples of graphs in TextWorld games.</li>
</ul>
<h2>A Details of GATA</h2>
<h2>Notations</h2>
<p>In this section, we use $O_{t}$ to denote text observation at game step $t, C_{t}$ to denote a list of action candidate provided by a game, and $\mathcal{G}_{t}$ to denote a belief graph that represents GATA's belief to the state.
We use $L$ to refer to a linear transformation and $L^{f}$ means it is followed by a non-linear activation function $f$. Brackets $[\cdot ; \cdot]$ denote vector concatenation. Overall structure of GATA is shown in Figure 2.</p>
<h2>A. 1 Graph Encoder</h2>
<p>As briefly mentioned in Section 3.3, GATA utilizes a graph encoder which is based on R-GCN [32].
To better leverage information from relation labels, when computing each node's representation, we also condition it on a relation representation $E$ :</p>
<p>$$
\tilde{h}<em _in="\in" _mathcal_R="\mathcal{R" r="r">{i}=\sigma\left(\sum</em>}} \sum_{j \in \mathcal{N<em r="r">{i}^{r}} W</em>\right]\right)
$$}^{l}\left[h_{j}^{l} ; E_{r}\right]+W_{0}^{l}\left[h_{i}^{l} ; E_{r</p>
<p>in which, $l$ denotes the $l$-th layer of the R-GCN, $\mathcal{N}<em r="r">{i}^{r}$ denotes the set of neighbor indices of node $i$ under relation $r \in \mathcal{R}, \mathcal{R}$ indicates the set of different relations, $W</em>}^{l}$ and $W_{0}^{l}$ are trainable parameters. Since we use continuous graphs, $\mathcal{N<em r="r">{i}^{r}$ includes all nodes (including node $i$ itself). To stabilize the model and preventing from the potential explosion introduced by stacking R-GCNs with continuous graphs, we use Sigmoid function as $\sigma$ (in contrast with the commonly used ReLU function).
As the initial input $h^{0}$ to the graph encoder, we concatenate a node embedding vector and the averaged word embeddings of node names. Similarly, for each relation $r, E</em>$ is the concatenation of a relation embedding vector and the averaged word embeddings of $r$ 's label. Both node embedding and relation embedding vectors are randomly initialized and trainable.
To further help our graph encoder to learn with multiple layers of R-GCN, we add highway connections [36] between layers:</p>
<p>$$
\begin{aligned}
g &amp; =L^{\text {sigmoid }}\left(\tilde{h}<em i="i">{i}\right) \
h</em>}^{l+1} &amp; =g \odot \tilde{h<em i="i">{i}+(1-g) \odot h</em>
\end{aligned}
$$}^{l</p>
<p>where $\odot$ indicates element-wise multiplication.
We use a 6-layer graph encoder, with a hidden size $H$ of 64 in each layer. The node embedding size is 100 , relation embedding size is 32 . The number of bases we use is 3 .</p>
<h1>A. 2 Text Encoder</h1>
<p>We use a transformer-based text encoder, which consists of a word embedding layer and a transformer block [43]. Specifically, word embeddings are initialized by the 300-dimensional fastText [28] word vectors trained on Common Crawl ( 600 B tokens) and kept fixed during training in all settings.
The transformer block consists of a stack of 5 convolutional layers, a self-attention layer, and a 2-layer MLP with a ReLU non-linear activation function in between. In the block, each convolutional layer has 64 filters, each kernel's size is 5 . In the self-attention layer, we use a block hidden size $H$ of 64 , as well as a single head attention mechanism. Layernorm [6] is applied after each component inside the block. Following standard transformer training, we add positional encodings into each block's input.
We use the same text encoder to process text observation $O_{t}$ and the action candidate list $C_{t}$. The resulting representations are $h_{O_{t}} \in \mathbb{R}^{L_{O_{t}} \times H}$ and $h_{C_{t}} \in \mathbb{R}^{N_{C_{t}} \times L_{C_{t}} \times H}$, where $L_{O_{t}}$ is the number of tokens in $O_{t}, N_{C_{t}}$ denotes the number of action candidates provided, $L_{C_{t}}$ denotes the maximum number of tokens in $C_{t}$, and $H=64$ is the hidden size.</p>
<h2>A. 3 Representation Aggregator</h2>
<p>The representation aggregator aims to combine the text observation representations and graph representations together. Therefore this module is activated only when both the text observation $O_{t}$ and the graph input $\mathcal{G}<em _mathcal_G="\mathcal{G">{t}$ are provided. In cases where either of them is absent, for instance, when training the agent with only $\mathcal{G}^{\text {belief }}$ as input, the aggregator will be deactivated and the graph representation will be directly fed into the scorer.
For simplicity, we omit the subscript $t$ denoting game step in this subsection. At any game step, the graph encoder processes graph input $\mathcal{G}$, and generates the graph representation $h</em>$ denotes the number of tokens in $O$.
We adopt a standard representation aggregation method from question answering literature [49] to combine the two representations using attention mechanism.
Specifically, the aggregator first uses an MLP to convert both $h_{\mathcal{G}}$ and $h_{O}$ into the same space, the resulting tensors are denoted as $h_{\mathcal{G}}^{\prime} \in \mathbb{R}^{N_{\mathcal{G}} \times H}$ and $h_{O}^{\prime} \in \mathbb{R}^{L_{O} \times H}$. Then, a trilinear similarity function [33] is used to compute the similarities between each token in $h_{O}^{\prime}$ with each node in $h_{\mathcal{G}}^{\prime}$. The similarity between $i$ th token in $h_{O}^{\prime}$ and $j$ th node in $h_{\mathcal{G}}^{\prime}$ is thus computed by:}} \in \mathbb{R}^{N_{\mathcal{G}} \times H}$. The text encoder processes text observation $O$ to generate text representation $h_{O} \in \mathbb{R}^{L_{O} \times H} . N_{\mathcal{G}}$ denotes the number of nodes in the graph $\mathcal{G}, L_{O</p>
<p>$$
\operatorname{Sim}(i, j)=W\left(h_{O_{i}}^{\prime}, h_{\mathcal{G}<em O__i="O_{i">{j}}^{\prime}, h</em>\right)
$$}}^{\prime} \oplus h_{\mathcal{G}_{j}}^{\prime</p>
<p>where $W$ is trainable parameters in the trilinear function. By applying the above computation for each pair of $h_{O}^{\prime}$ and $h_{\mathcal{G}}^{\prime}$, a similarity matrix $S \in \mathbb{R}^{L_{O} \times N_{\mathcal{G}}}$ is resulted.
Softmax of the similarity matrix $S$ along both dimensions (number of nodes $N_{\mathcal{G}}$ and number of tokens $L_{O}$ ) are computed, producing $S_{\mathcal{G}}$ and $S_{O}$. The information contained in the two representations are then aggregated by:</p>
<p>$$
\begin{aligned}
h_{O \mathcal{G}} &amp; =\left[h_{O}^{\prime} ; P ; h_{O}^{\prime} \oplus P ; h_{O}^{\prime} \oplus Q\right] \
P &amp; =S_{\mathcal{G}} h_{\mathcal{G}}^{\prime \top} \
Q &amp; =S_{\mathcal{G}} S_{O}^{\top} h_{O}^{\prime \top}
\end{aligned}
$$</p>
<p>where $h_{O \mathcal{G}} \in \mathbb{R}^{L_{O} \times 4 H}$ is the aggregated observation representation, each token in text is represented by the weighted sum of graph representations. Similarly, the aggregated graph representation $h_{\mathcal{G} O} \in$ $\mathbb{R}^{N_{\mathcal{G}} \times 4 H}$ can also be obtained, where each node in the graph is represented by the weighted sum of text representations. Finally, a linear transformation projects the two aggregated representations to a space with size $H$ of 64 :</p>
<p>$$
\begin{aligned}
&amp; h_{\mathcal{G} O}=L\left(h_{\mathcal{G} O}\right) \
&amp; h_{O \mathcal{G}}=L\left(h_{O \mathcal{G}}\right)
\end{aligned}
$$</p>
<h1>A. 4 Scorer</h1>
<p>The scorer consists of a self-attention layer, a masked mean pooling layer, and a two-layer MLP. As shown in Figure 2 and described above, the input to the scorer is the action candidate representation $h_{C_{t}}$, and one of the following game state representation:</p>
<p>$$
s_{t}= \begin{cases}h_{\mathcal{G}<em O__t="O_{t">{t}} &amp; \text { if only graph input is available, } \ h</em>
$$}} &amp; \text { if only text observation is available, this degrades GATA to a Tr-DQN, } \ h_{\mathcal{G} O_{t}}, h_{O \mathcal{G}_{t}} &amp; \text { if both are available. }\end{cases</p>
<p>First, a self-attention is applied to the game state representation $s_{t}$, producing $\hat{s_{t}}$. If $s_{t}$ includes graph representations, this self-attention mechanism will reinforce the connection between each node and its related nodes. Similarly, if $s_{t}$ includes text representation, the self-attention mechanism strengthens the connection between each token and other related tokens. Further, masked mean pooling is applied to the self-attended state representation $\hat{s_{t}}$ and the action candidate representation $h_{C_{t}}$, this results in a state representation vector and a list of action candidate representation vectors. We then concatenate the resulting vectors and feed them into a 2-layer MLP with a ReLU non-linear activation function in between. The second MLP layer has an output dimension of 1, after squeezing the last dimension, the resulting vector is of size $N_{C_{t}}$, which is the number of action candidates provided at game step $t$. We use this vector as the score of each action candidate.</p>
<h2>A. 5 The $\mathrm{f}_{\Delta}$ Function</h2>
<p>As mentioned in Eqn. 2, $\mathrm{f}<em t-1="t-1">{\Delta}$ is an aggregator that combines information in $\mathcal{G}</em>$.
In specific, $\mathrm{f}_{\Delta}$ uses the same architecture as the representation aggregator described in Appendix A.3. Denoting the aggregator as a function Aggr:}, A_{t-1}$, and $O_{t}$ to generate the graph difference $\Delta g_{t</p>
<p>$$
h_{P Q}, h_{Q P}=\operatorname{Aggr}\left(h_{P}, h_{Q}\right)
$$</p>
<p>$\mathrm{f}<em O__t="O_{t">{\Delta}$ takes text observation representations $h</em>}} \in \mathbb{R}^{L_{O_{t}} \times H}$, belief graph representations $h_{\mathcal{G<em _mathcal_G="\mathcal{G">{t-1}} \in$ $\mathbb{R}^{N</em>$ is the number of nodes in the graph; $H$ is hidden size of the input representations.
We first aggregate $h_{O_{t}}$ with $h_{\mathcal{G}}} \times H}$, and action representations $h_{A_{t-1}} \in \mathbb{R}^{L_{A_{t-1}} \times H}$ as input. $L_{O_{t}}$ and $L_{A_{t-1}}$ are the number of tokens in $O_{t}$ and $A_{t-1}$, respectively; $N_{\mathcal{G}<em A__t-1="A_{t-1">{t-1}}$, then similarly $h</em>$ :}}$ with $h_{\mathcal{G}_{t-1}</p>
<p>$$
\begin{aligned}
&amp; h_{O \mathcal{G}}, h_{\mathcal{G} O}=\operatorname{Aggr}\left(h_{O_{t}}, h_{\mathcal{G}<em A="A" _mathcal_G="\mathcal{G">{t-1}}\right) \
&amp; h</em>\right)
\end{aligned}
$$}}, h_{\mathcal{G} A}=\operatorname{Aggr}\left(h_{A_{t-1}}, h_{\mathcal{G}_{t-1}</p>
<p>The output of $\mathrm{f}_{\Delta}$ is:</p>
<p>$$
\Delta g_{t}=\left[h_{O \mathcal{G}} ; h_{\mathcal{G} O} ; h_{A \mathcal{G}} ; h_{\mathcal{G} A}\right]
$$</p>
<p>where $\bar{X}$ is the masked mean of $X$ on the first dimension. The resulting concatenated vector $\Delta g_{t}$ has the size of $\mathbb{R}^{4 H}$.</p>
<h2>A. 6 The $\mathrm{f}_{\mathrm{d}}$ Function</h2>
<p>$\mathrm{f}<em t="t">{\mathrm{d}}$ is a decoder that maps a hidden graph representation $h</em>$.
Specifically, $\mathrm{f}_{\mathrm{d}}$ consists of a 2-layer MLP:} \in \mathbb{R}^{H}$ (generated by the RNN) into a continuous adjacency tensor $\mathcal{G} \in[-1,1]^{2 \mathcal{R} \times \mathcal{N} \times \mathcal{N}</p>
<p>$$
\begin{aligned}
&amp; h_{1}=L_{1}^{\mathrm{ReLU}}\left(h_{t}\right) \
&amp; h_{2}=L_{2}^{\text {tanh }}\left(h_{1}\right)
\end{aligned}
$$</p>
<p>In which, $h_{1} \in \mathbb{R}^{H}, h_{2} \in[-1,1]^{\mathcal{R} \times \mathcal{N} \times \mathcal{N}}$. To better facilitate the message passing process of R-GCNs used in GATA's graph encoder, we explicitly use the transposed $h_{2}$ to represent the inversed relations in the belief graph. Thus, we have $\mathcal{G}$ defined as:</p>
<p>$$
\mathcal{G}=\left[h_{2} ; h_{2}^{T}\right]
$$</p>
<p>The transpose is performed on the last two dimensions (both of size $\mathcal{N}$ ), the concatenation is performed on the dimension of relations.</p>
<p>The tanh activation function on top of the second layer of the MLP restricts the range of our belief graph $\mathcal{G}$ within $[-1,1]$. Empirically we find it helpful to keep the input of the multi-layer graph neural networks (the R-GCN graph encoder) in this range.</p>
<h1>B Details of Pre-training Graph Updater for GATA</h1>
<p>As briefly described in Section 3.2, we design two self-supervised tasks to pre-train the graph updater module of GATA. As training data, we gather a collection of transitions from the FTWP dataset. Here, we denote a transition as a 3-tuple $\left(O_{t-1}, A_{t-1}, O_{t}\right)$. Specifically, given text observation $O_{t-1}$, an action $A_{t-1}$ is issued; this leads to a new game state and $O_{t}$ is returned from the game engine. Since the graph updater is recurrent (we use an RNN as its graph operation function), the set of transitions are stored in the order they are collected.</p>
<h2>B. 1 Observation Generation (OG)</h2>
<p>As shown in Figure 4, given a transition $\left(O_{t-1}, A_{t-1}, O_{t}\right)$, we use the belief graph $\mathcal{G}<em t-1="t-1">{t}$ and $A</em>}$ to reconstruct $O_{t} . \mathcal{G<em t-1="t-1">{t}$ is generated by the graph updater, conditioned on the recurrent information $h</em>$ carried over from previous data point in the transition sequence.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Observation generation model.</p>
<h2>B.1.1 Observation Generator Layer</h2>
<p>The observation generator is a transformer-based decoder. It consists of a word embedding layer, a transformer block, and a projection layer.</p>
<p>Similar to the text encoder, the embedding layer is frozen after initializing with the pre-trained fastText [28] word embeddings. Inside the transformer block, there is one self attention layer, two attention layers and a 3-layer MLP with ReLU non-linear activation functions in between. Taking word embedding vectors and the two aggregated representations produced by the representation aggregator as input, the self-attention layer first generates a contextual encoding vectors for the words. These vectors are then fed into the two attention layers to compute attention with graph representations and text observation representations respectively. The two resulting vectors are thus concatenated, and they are fed into the 3-layer MLP. The block hidden size of this transformer is $H=64$.</p>
<p>Finally, the output of the transformer block is fed into the projection layer, which is a linear transformation with output size same as the vocabulary size. The resulting logits are then normalized by a softmax to generate a probability distribution over all words in vocabulary.</p>
<p>Following common practice, we also use a mask to prevent the decoder transformer to access "future" information during training.</p>
<h2>B. 2 Contrastive Observation Classification (COC)</h2>
<p>The contrastive observation classification task shares the same goal of ensuring the generated belief graph $\mathcal{G}<em t="t">{t}$ encodes the necessary information describing the environment state at step $t$. However, instead of generating $O</em>}$ from $\mathcal{G<em t="t">{t}$, it requires a model to differentiate the real $O</em>$ that are randomly sampled from other data points. In this task, the belief graph does not need to encode}$ from some $\tilde{O}_{t</p>
<p>the syntactical information as in the observation generation task, rather, a model can use its full capacity to learn the semantic information of the current environmental state.</p>
<p>We illustrate our contrastive observation classification model in Figure 5. This model shares most components with the previously introduced observation generation model, except replacing the observation generator module by a discriminator.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Contrastive observation classification model.</p>
<h1>B. 3 Reusing Graph Encoder in Action Scorer</h1>
<p>Both of the graph updater and action selector modules rely heavily on the graph encoder layer. It is natural to reuse the graph updater's graph encoder during the RL training of action selector. Specifically, we use the pre-trained graph encoder (and all its dependencies such as node embeddings and relation embeddings) from either the above model to initialize the graph encoder in action selector. In such settings, we fine-tune the graph encoders during RL training. In Appendix D, we compare GATA's performance between reusing the graph encoders with randomly initialize them.</p>
<h2>C GATA-GTP and Discrete Belief Graph</h2>
<p>As mentioned in Section 3.4, since the TextWorld API provides ground-truth (discrete) KGs that describe game states at each step, we provide an agent that utilizes this information, as a strong baseline to GATA. To accommodate the discrete nature of KGs provided by TextWorld, we propose GATA-GTP, which has the same action scorer with GATA, but equipped with a discrete graph updater. We show the overview structure of GATA-GTP in Figure 6.</p>
<h2>C. 1 Discrete Graph Updater</h2>
<p>In the discrete graph setting, we follow [53], updating $\mathcal{G}<em t-1="t-1">{t}$ with a set of discrete update operations that act on $\mathcal{G}</em>$ as a set of update operations, wherein each update operation is a sequence of tokens. We define the following two elementary operations so that any graph update can be achieved in $k \geq 0$ such operations:}$. In particular, we model the (discrete) $\Delta g_{t</p>
<ul>
<li>add(node1, node2, relation): add a directed edge, named relation, between node1 and node2.</li>
<li>delete(node1, node2, relation): delete a directed edge, named relation, between node1 and node2. If the edge does not exist, ignore this command.</li>
</ul>
<p>Given a new observation string $O_{t}$ and $\mathcal{G}_{t-1}$, the agent generates $k \geq 0$ such operations to merge the newly observed information into its belief graph.</p>
<p>Table 3: Update operations matching the transition in Figure 1.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">&lt;s&gt;</span><span class="w"> </span>add<span class="w"> </span>player<span class="w"> </span>shed<span class="w"> </span>at<span class="w"> </span><span class="err">&lt;</span>|&gt;<span class="w"> </span>add<span class="w"> </span>shed<span class="w"> </span>backyard<span class="w"> </span>west_of<span class="w"> </span><span class="err">&lt;</span>|&gt;<span class="w"> </span>add<span class="w"> </span>wooden<span class="w"> </span>door<span class="w"> </span>shed
east_of<span class="w"> </span><span class="err">&lt;</span>|&gt;<span class="w"> </span>add<span class="w"> </span>toolbox<span class="w"> </span>shed<span class="w"> </span>in<span class="w"> </span><span class="err">&lt;</span>|&gt;<span class="w"> </span>add<span class="w"> </span>toolbox<span class="w"> </span>closed<span class="w"> </span>is<span class="w"> </span><span class="err">&lt;</span>|&gt;<span class="w"> </span>add<span class="w"> </span>workbench
shed<span class="w"> </span>in<span class="w"> </span><span class="err">&lt;</span>|&gt;<span class="w"> </span>delete<span class="w"> </span>player<span class="w"> </span>backyard<span class="w"> </span>at<span class="w"> </span><span class="nt">&lt;/s&gt;</span>
</code></pre></div>

<p>We formulate the update generation task as a sequence-to-sequence (Seq2Seq) problem and use a transformer-based model [43] to generate token sequences for the operations. We adopt the decoding strategy from [27], where given an observation sequence $O_{t}$ and a belief graph $\mathcal{G}_{t-1}$, the agent</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: GATA-GTP in detail. The coloring scheme is same as in Figure 1. The discrete graph updater first generates $\Delta g_{t}$ using $\mathcal{G}<em t="t">{t-1}$ and $O</em>}$. Afterwards the action selector uses $O_{t}$ and the updated graph $\mathcal{G<em t="t">{t}$ to select $A</em>$. Purple dotted line indicates a detached connection (i.e., no back-propagation through such connection).
generates a sequence of tokens that contains multiple graph update operations as subsequences, separated by a delimiter token &lt;|&gt;.
Since Seq2Seq set generation models are known to learn better with a consistent output ordering [27], we sort the ground-truth operations (e.g., always add before delete) for training. For the transition shown in Figure 1, the generated sequence is shown in Table 3.}$ from the list of action candidates $C_{t</p>
<h1>C. 2 Pre-training Discrete Graph Updater</h1>
<p>As described above, we frame the discrete graph updating behavior as a language generation task. We denote this task as command generation (CG). Similar to the continuous version of graph updater in GATA, we pre-train the discrete graph updater using transitions collected from the FTWP dataset. It is worth mentioning that despite requiring ground-truth KGs in FTWP dataset, GATA-GTP does not require any ground-truth graph in the RL game to train and evaluate the action scorer.
For training discrete graph updater, we use the $\mathcal{G}^{\text {seen }}$ type of graphs provided by the TextWorld API. Specifically, at game step $t, \mathcal{G}_{t}^{\text {seen }}$ is a discrete partial KG that contains information the agent has observed from the beginning until step $t$. It is only possible to train an agent to generate belief about the world it has seen and experienced.</p>
<p>In the collection FTWP transitions, every data point contains two consecutive graphs, we convert the difference between the graphs to ground-truth update operations (i.e., add and delete commands). We use standard teacher forcing technique to train the transformer-based Seq2Seq model. Specifically, conditioned on the output of representation aggregator, the command generator is required to predict the $k^{\text {th }}$ token of the target sequence given all the ground-truth tokens up to time step $k-1$. The command generator module is transformer-based decoder, similar to the observation generator described in Appendix B.1.1. Negative log-likelihood is used as loss function for optimization. An illustration of the command generation model is shown in Figure 7.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Command Generation Model.
During the RL training of action selector, the graph updater is detached without any back-propagation performed. It generates token-by-token started by a begin-of-sequence token, until it generates an end-of-sequence token, or hitting the maximum sequence length limit. The resulting tokens are consequently used to update the discrete belief graph.</p>
<h2>C. 3 Pre-training a Discrete Graph Encoder for Action Scorer</h2>
<p>In the discrete graph setting, we take advantage of the accessibility of the ground-truth graphs. Therefore we also consider various pre-training approaches to improve the performance of the graph encoder in the action selection module. Similar to the training of graph updater, we use transitions collected from the FTWP dataset as training data.</p>
<p>In particular, here we define a transition as a 6-tuple $\left(\mathcal{G}<em t-1="t-1">{t-1}, O</em>}, C_{t-1}, A_{t-1}, \mathcal{G<em t="t">{t}, O</em>}\right.$. Specifically, given $\mathcal{G<em t-1="t-1">{t-1}$ and $O</em>}$, an action $A_{t-1}$ is selected from the candidate list $C_{t-1}$; this leads to a new game state $\mathcal{S<em t="t">{t}$, thus $\mathcal{G}</em>}$ and $O_{t}$ are returned. Note that $\mathcal{G<em t="t">{t}$ in transitions can either be $\mathcal{G}</em>$ that describes the part of state that the agent has experienced.}^{\text {full }}$ that describes the full environment state or $\mathcal{G}_{t}^{\text {new }</p>
<p>In this section, we start with providing details of the pre-training tasks and their corresponding models, and then show these models' performance for each of the tasks.</p>
<h1>C.3.1 Action Prediction (AP)</h1>
<p>Given a transition $\left(\mathcal{G}<em t-1="t-1">{t-1}, O</em>}, C_{t-1}, A_{t-1}, \mathcal{G<em t="t">{t}, O</em>}, r_{t-1}\right)$, we use $A_{t-1}$ as positive example and use all other action candidates in $C_{t-1}$ as negative examples. A model is required to identify $A_{t-1}$ amongst all action candidates given two consecutive graphs $\mathcal{G<em t="t">{t-1}$ and $\mathcal{G}</em>$.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Action Prediction Model.
We use a model with similar structure and components as the action selector of GATA. As illustrated in Figure 8, the graph encoder first converts the two input graphs $\mathcal{G}<em t="t">{t-1}$ and $\mathcal{G}</em>$ and all negative examples) are fed into the text encoder to generate action candidate representations. The scorer thus takes these representations and the aggregated graph representations as input, and it outputs a ranking over all action candidates.}$ into hidden representations, the representation aggregator combines them using attention mechanism. The list of action candidates (which includes $A_{t-1</p>
<p>In order to achieve good performance in this setting, the bi-directional attention between $\mathcal{G}<em t="t">{t-1}$ and $\mathcal{G}</em>}$ in the representation aggregator needs to effectively determine the difference between the two sparse graphs. To achieve that, the graph encoder has to extract useful information since often the difference between $\mathcal{G<em t="t">{t-1}$ and $\mathcal{G}</em>$ is minute (e.g., before and after taking an apple from the table, the only change is the location of the apple).</p>
<h2>C.3.2 State Prediction (SP)</h2>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: State Prediction Model.
Given a transition $\left(\mathcal{G}<em t-1="t-1">{t-1}, O</em>}, C_{t-1}, A_{t-1}, \mathcal{G<em t="t">{t}, O</em>}, r_{t-1}\right)$, we use $\mathcal{G<em t-1="t-1">{t}$ as positive example and gather a set of game states by issuing all other actions in $C</em>}$ except $A_{t-1}$. We use the set of graphs representing the resulting game states as negative samples. In this task, a model is required to identify $\mathcal{G<em t="t">{t}$ amongst all graph candidates $G C</em>}$ given the previous graph $\mathcal{G<em t-1="t-1">{t-1}$ and the action taken $A</em>$.</p>
<p>As shown in Figure 9, a similar model is used to train both the SP and AP tasks.</p>
<h2>C.3.3 Deep Graph Infomax (DGI)</h2>
<p>This is inspired by Velickovic et al., [44]. Given a transition $\left(\mathcal{G}<em t-1="t-1">{t-1}, O</em>}, C_{t-1}, A_{t-1}, \mathcal{G<em t="t">{t}, O</em>}, r_{t-1}\right)$, we map the graph $\mathcal{G<em t="t">{t}$ into its node embedding space. The node embedding vectors of $\mathcal{G}</em>$.}$ is denoted as $H$. We randomly shuffle some of the node embedding vectors to construct a "corrupted" version of the node representations, denoted as $\tilde{H</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Deep Graph Infomax Model.</p>
<p>Given node representations $H=\left{\overrightarrow{h_{1}}, \overrightarrow{h_{2}}, \ldots, \overrightarrow{h_{N}}\right}$ and corrupted representations of these nodes $\tilde{H}=\left{\overrightarrow{h_{1}}, \overrightarrow{h_{2}}, \ldots, \overrightarrow{h_{N}}\right}$, where $N$ is the number of vertices in the graph, a model is required to discriminate between the original and corrupted representations of nodes. As shown in Figure 10, the model is composed of a graph encoder and a discriminator. Specifically, following [44], we utilize a noise-contrastive objective with a binary cross-entropy (BCE) loss between the samples from the joint (positive examples) and the product of marginals (negative examples). To enable the discriminator to discriminate between $\mathcal{G}_{t}$ and the negative samples, the graph encoder must learn useful graph representations at both global and local level.</p>
<h1>C.3.4 Performance on Graph Encoder Pre-training Tasks</h1>
<p>We provide test performance of all the models described above for graph representation learning. We fine-tune the models on validation set and report their performance on test set.
Additionally, as mentioned in Section 3.3 and Appendix A, we adapt the original R-GCN to condition the graph representation on additional information contained by the relation labels. We show an ablation study for this in Table 4, where R-GCN denotes the original R-GCN [32] and R-GCN w/ R-Emb denotes our version that considers relation labels.</p>
<p>Note, as mentioned in previous sections, the dataset to train, valid and test these four pre-training tasks are extracted from the FTWP dataset. There exist unseen nodes (ingredients in recipe) in the validation and test sets of FTWP, it requires strong generalizability to get decent performance on these datasets.</p>
<p>From Table 4, we show the relation label representation significantly boosts the generalization performance on these datasets. Compared to AP and SP, where relation label information has significant effect, both models perform near perfectly on the DGI task. This suggests the corruption function we consider in this work is somewhat simple, we leave this for future exploration.</p>
<p>Table 4: Test performance of models on all pre-training tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Graph Type</th>
<th style="text-align: center;">R-GCN</th>
<th style="text-align: center;">R-GCN w/ R-Emb</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AP</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">$\mathbf{0 . 8 9 1}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">seen</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 3}$</td>
</tr>
<tr>
<td style="text-align: center;">SP</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">$\mathbf{0 . 9 2 6}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">seen</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">$\mathbf{0 . 9 7 1}$</td>
</tr>
<tr>
<td style="text-align: center;">DGI</td>
<td style="text-align: center;">full</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">seen</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
</tr>
</tbody>
</table>
<h2>D Additional Results and Discussions</h2>
<h2>D. 1 Training Curves</h2>
<p>We report the training curves of all our mentioned experiment settings. Figure 11 shows the GATA's training curves. Figure 12 shows the training curves of the three text-based baseline (Tr-DQN, Tr-DRQN, Tr-DRQN+). Figure 13 shows the training curve of GATA-GTF (no graph updater, the action scorer takes ground-truth graphs as input) and GATA-GTP (graph updater is trained using ground-truth graphs from the FTWP dataset, the trained graph updater maintains a discrete belief graph throughout the RL training).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ The graph updater and action selector share some structures but not their parameters (unless specified).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
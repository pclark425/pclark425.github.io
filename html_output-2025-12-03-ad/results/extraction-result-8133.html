<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8133 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8133</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8133</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-272826648</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.14144v1.pdf" target="_blank">Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis</a></p>
                <p><strong>Paper Abstract:</strong> We find arithmetic ability resides within a limited number of attention heads, with each head specializing in distinct operations. To delve into the reason, we introduce the Comparative Neuron Analysis (CNA) method, which identifies an internal logic chain consisting of four distinct stages from input to prediction: feature enhancing with shallow FFN neurons, feature transferring by shallow attention layers, feature predicting by arithmetic heads, and prediction enhancing among deep FFN neurons. Moreover, we identify the human-interpretable FFN neurons within both feature-enhancing and feature-predicting stages. These findings lead us to investigate the mechanism of LoRA, revealing that it enhances prediction probabilities by amplifying the coefficient scores of FFN neurons related to predictions. Finally, we apply our method in model pruning for arithmetic tasks and model editing for reducing gender bias. Code is on https://github.com/zepingyu0512/arithmetic-mechanism.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8133.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8133.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-7B arithmetic findings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical findings on arithmetic behavior of Llama-7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Detailed empirical analysis of how Llama-7B (32-layer, 7B parameter decoder-only transformer) performs 1–3 digit addition, subtraction, multiplication and division, showing arithmetic ability localizes to a few attention heads and a small set of FFN neurons and that multi-digit performance relies on memorized 1-digit knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer, 32 layers, 32 attention heads per layer, FFN size ~11,008 neurons per FFN layer; standard pretrained Llama weights (as studied in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>1-digit, 2-digit, 3-digit addition, subtraction, multiplication, division (prompts including numerals and number words); separate 2-digit training for LoRA experiments</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Arithmetic ability concentrated in a small number of 'arithmetic heads' (specific attention heads, e.g., head 17_22 for addition/subtraction, head 14_19 for division) that appear to store parameters that activate deep FFN neurons representing final-token concepts; multi-digit answers rely on memorized 1-digit operations (memorization + digit-wise composition).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Intervention of attention heads by zeroing head parameters (head ablations), CNA (Comparative Neuron Analysis) comparing neuron importance scores between original and intervened models, projection of FFN neuron values into vocabulary space, selective FFN neuron masking/keeping experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Original model accuracy reported as 74.8% (on the evaluation used for head interventions); on a separate 2-digit dataset original accuracy reported ≈62.96%; intervening certain heads reduces accuracy drastically (three heads give ≥10% drop; head 17_22 causes large drops on addition/subtraction), intervening the top-99 deep FFN neurons (identified by CNA) can reduce 1-digit addition/subtraction accuracy to 0% (masking top99 → 100% accuracy decrease in one experiment), while intervening all other deep FFN neurons (keeping only those top neurons) reduces accuracy by only ~3.9%. LoRA fine-tuning (see separate entry) raises 2-digit accuracy to ~89–95% depending on layer.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Failures primarily when attention-head-mediated activation of memorized 1-digit FFN neurons is disrupted (loss of 1-digit memorization); poor baseline multiplication/division performance relative to addition/subtraction in some experiments; multi-digit correctness often depends on correct 1-digit memorization and digit-wise combination; interventions can reveal dependence on a few memorized associations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Head ablations (zeroing) cause large decreases in accuracy for specific operations; CNA shows coefficient (activation) scores of particular deep FFN neurons drop substantially after head interventions; projection of these FFN neurons into vocabulary space yields tokens related to the expected numeric output (e.g., tokens for '8'), and masking top-identified FFN neurons eliminates correct prediction while masking other deep FFN neurons does not.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Top head for multiplication did not significantly impact multiplication accuracy (contrasts with addition/subtraction), indicating heterogeneity across operations; some multi-digit and more complex arithmetic exhibits lower overall performance (limits of memorization/generalization); results depend on intervention method (zeroing) and on the projection-to-vocabulary interpretability method (admitted limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8133.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8133.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CNA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparative Neuron Analysis (CNA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure introduced in this paper to compare per-neuron importance (log-probability increase contribution) between an original model and an intervened model for the same input to locate neurons and attention heads causally responsible for changes in final-token probability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-7B (applied also to GPT-J in replication)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method applied to the studied models (see model entries).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used to analyze 1-digit, 2-digit, 3-digit arithmetic cases across operations (addition, subtraction, multiplication, division) and gender-bias editing cases.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Operates on the representation that FFN output equals sum_k m_lk * fc2_lk (key-value FFN interpretation); CNA computes importance of each m_lk*fc2_lk as the change in log probability of the target token when adding that subvalue versus not, and compares those importance scores between original and intervened models to locate neurons whose activation change explains prediction changes.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Computes importance score = log p(w | with neuron) − log p(w | without neuron) using softmax over unembedding; interventions include zeroing attention heads and zeroing specific FFN neurons identified by CNA; projection of FC2 vectors into vocabulary space (softmax(E_u v)) for semantic interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used to quantify neuron importance changes (reported coefficient and importance score drops); in experiments CNA-identified top-99 neurons when masked produced 100% accuracy loss for 1D+/- (in Llama study) and large coefficient drops (e.g., coefficient decreases of top sets reported, and specific numeric decreases like 15.8% coefficient drop when intervening a lowest important neuron among top99).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Method depends on assumptions: projection into vocabulary space for interpretability is heuristic and not fully theoretically grounded; results can vary with intervention method (zeroing vs other interventions); CNA is computationally intensive so authors use importance-score ranking rather than full causal mediation for all neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Applying CNA located deep FFN neurons whose coefficient and importance scores dropped after attention-head intervention; top tokens from projecting those neurons aligned with expected output tokens; selective masking experiments validated that those neurons carry nearly all prediction-relevant information for the studied 1-digit cases.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Projection-to-vocabulary interpretability and the choice of importance-score metric are recognized limitations; different attribution/intervention techniques may yield different identified neurons; CNA is approximate because full causal mediation at all neurons is computationally infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8133.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8133.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Arithmetic heads</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention-layer 'arithmetic heads' (operation-specialized heads)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small set of attention heads in specific layers (e.g., head 17_22 for addition/subtraction, head 14_19 for division) that, when ablated, cause large drops in arithmetic accuracy and appear to control activation of FFN neurons that represent final-token outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-7B (also observed in GPT-J)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Identified within the 32×32 attention head grid of Llama-7B; specific heads named by layer_index head_index.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>1D, 2D, 3D addition, subtraction, division (different heads affect different operations), multiplication less clearly localized.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Heads act as 'feature-predicting' modules that activate deep FFN neurons corresponding to output tokens; they store parameters that are necessary to trigger activation (i.e., they modulate FFN coefficient scores that produce final-token logits).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Head ablation by zeroing head parameters; downstream CNA to measure how FFN neuron importance/coefficient scores change after head ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Most heads produce minor accuracy changes when ablated (976/1024 heads give small drops 0.01%–2%); only a handful (e.g., three heads) give ≥10% decrease; head 17_22 ablation produces large drops on addition/subtraction (examples reported), head 14_19 on division.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Ablation of these arithmetic heads removes activation of deep FFN neurons and thus removes memorized 1-digit knowledge, causing systematic loss of correct outputs; some operations (multiplication) did not map cleanly to a single head.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation of an arithmetic head reduces coefficient scores of identified deep FFN neurons, projections of those neurons map to expected numeric tokens, and selective neuron masking reproduces the large performance drop—showing a causal chain from head → FFN activation → final token.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Not all operations localize equally (multiplication less affected by single-head ablation); different models (GPT-J) show quantitatively different sensitivity (e.g., smaller drops).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8133.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8133.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shallow FFN hidden features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hidden-interpretable shallow FFN neurons (feature enhancing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Shallow FFN neurons (layers ~0–15) that are 'hidden-interpretable' — they are not directly interpretable in vocabulary space but, after attention-layer transformation, become directly interpretable and serve to enhance input features (numbers/operators) used downstream.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-7B (also verified in GPT-J)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Shallow FFN layers of the studied 32-layer models (0th–15th FFN layers), containing many neurons (e.g., 176,128 in 0–15 layers for Llama-7B).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>1D and 1D/ cases (e.g., '3+5=' → feature extraction from '3' and '5') and more generally used across arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>These neurons encode enhanced features tied to input tokens (digit embeddings and local computations) which attention value-output transformations convert into interpretable features transferred to the final token position.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Zero-shot identification via transforming each shallow FFN neuron by attention value-output matrices (0th–16th) and projecting into vocabulary space; interventions by zeroing sets of identified hidden-interpretable neurons and comparing accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Intervening ~1,953 identified hidden-interpretable neurons (M=2 threshold) in 0–15 layers caused a large accuracy drop of 53.9% on 1D+/- cases; intervening 10,426 such neurons (~6% of shallow FFN neurons) caused ~68.4% accuracy drop; random interventions of the same size produced negligible drops (~2.6%–5.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Distributed representation across many shallow neurons means partial interventions can produce graded failure; identification threshold (M) affects set size and observed impact.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Projection-after-attention shows neurons become interpretable; targeted interventions on these neurons cause large accuracy degradation, while random interventions of same size do not — implicating these neurons in feature enhancement.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Direct interpretability before attention is limited (they are 'hidden-interpretable' only after attention transforms them); sets are large and identification depends on heuristic thresholds for vocabulary-concept counts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8133.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8133.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep FFN prediction neurons & PE-DAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep FFN neurons as prediction units and Prediction-Enhancing Directed Acyclic Graph (PE-DAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Deep FFN neurons (layers ~17–31) that directly encode concepts for final output tokens (projectable to vocabulary) and that form a recurrent activation graph (PE-DAG) where lower FFN neurons activate higher ones to amplify final-token probability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-7B (replicated in GPT-J)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deep FFN layers (roughly 17th–31st layers) containing interpretable FFN value vectors whose projections align with final-token concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Final-token prediction for 1D, 2D, 3D arithmetic across operations; PE-DAG observed specifically among top important neurons for 1D+ cases (example nodes: 19_5769, 25_7164, 28_3696).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>FFN is interpreted as key-value memories: neuron subvalue = m_lk * fc2_lk; PE-DAG describes large inner products between an FC2 value of a lower neuron and FC1 key of a higher neuron (chaining activations), producing recursive enhancement of final-token logit.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>CNA to identify important deep FFN neurons; compute inner products between FC2 (value) and FC1 (key) vectors to find directed activation links; intervene by zeroing individual neurons in PE-DAG and measure coefficient/importance score drops.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Intervening a single neuron among top99 important neurons can reduce coefficient scores of other important neurons by ~15.8% (example), and masking top-identified deep neurons (top99) caused 100% accuracy loss for 1D+/- in one Llama experiment; in GPT-J comparable but smaller effects (top99 mask → ~58.4% accuracy decrease).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>If root neurons in PE-DAG are disrupted, the chain of activations collapses and final-token probability drops; PE-DAG structure and importance magnitudes vary across models (Llama vs GPT-J) and operations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large inner products measured between FC2 of lower neuron and FC1 keys of higher neurons; observed recursive activation and coefficient score drops when lower neurons are intervened; neuron projections map to expected output tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Quantitative strength of PE-DAG differs by model (smaller in GPT-J) and operation; identification relies on linear-algebra measures that could be sensitive to representation scaling and intervention style.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8133.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8133.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoRA mechanism (paper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mechanistic effect of LoRA fine-tuning on arithmetic behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper finds that LoRA (low-rank adaptation) improves arithmetic performance by amplifying coefficient scores (m_lk) of important deep FFN neurons related to final predictions, with amplification magnitude depending on which attention layer LoRA is applied to.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-7B (LoRA fine-tuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>32 separate fine-tuned variants where a LoRA module was added to one attention layer each (layers 0–31); training on 2-digit arithmetic dataset (18k train, 2k test) for up to 4 epochs with tuned LR).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>2-digit arithmetic (addition, subtraction, multiplication, division) including negative numbers (e.g., '3-5=-2') for training set.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>LoRA acts similarly to adding a head: when applied to different attention layers it changes residual outputs that ultimately increase FFN coefficient scores of final-prediction neurons; shallow-layer LoRA produced larger amplification of coefficient scores for important neurons than deep-layer LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>CNA between original model and LoRA-fine-tuned models to measure coefficient-score changes of identified important neurons across different LoRA placements; evaluation of accuracy per LoRA-layer model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Original Llama on 2-digit dataset: ≈62.96% accuracy; LoRA fine-tuned models: 1st–9th layer LoRA models ≈90% accuracy (range reported ~89–95%), model accuracy declines for LoRA placed in deeper layers starting from the 10th model; coefficient-score increases for important neurons: e.g., 1st–9th models produce 42%–59% average increases across top neuron sets (top50→top10 reported), 10th–16th produce 29%–53% increases, 17th–30th produce smaller increases (2%–28%).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>LoRA placed in certain layers (e.g., starting layer 10 onward) shows a downward trend in achieved accuracy and less amplification of key FFN neurons; a LoRA model that fails to leverage certain deep neuron features shows drops in corresponding neuron coefficient scores (example: 20th-layer LoRA failed to leverage feature 19_5769 causing drop in 25_7164).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>CNA shows consistent coefficient-score amplification of important deep FFN neurons in LoRA-fine-tuned models compared to the original; accuracy correlates with magnitude of coefficient increases and with LoRA layer placement; pruned model experiments further demonstrate that amplifying the remaining important neurons via LoRA recovers much accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Layer-dependent effects: LoRA in shallow layers amplifies coefficients more effectively than deep-layer LoRA; not all LoRA placements yield identical qualitative improvements; LoRA acts indirectly (via attention/residual changes) rather than by directly modifying deep-FFN weights.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8133.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8133.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J replication</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Replication results on GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments replicated on GPT-J show qualitatively similar four-stage mechanism (feature enhancing → feature transferring → feature predicting → prediction enhancing) though with quantitatively smaller effects: interventions on top neurons reduce accuracy but less dramatically than in Llama-7B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>6B-parameter autoregressive transformer (GPT-J) used to replicate key experiments; same intervention/CNA protocols applied.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>1D and 2D arithmetic cases (replicated experiments analogous to those on Llama-7B).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Same four-stage internal logic chain observed; arithmetic ability localizes to specific heads and FFN neurons, shallow hidden-interpretable neurons exist, and deep FFN PE-DAG structures observed.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Head ablations, CNA ranking and masking of top FFN neurons, hidden-interpretable shallow neuron identification and interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When masking top99 identified FFN neurons in GPT-J, accuracy decreased by ~58.4% (versus 100% mask effect reported in Llama for some cases); coefficient decreases on interventions are smaller in magnitude (examples: coef decreases reported like 17%–29% for different top-k sets).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Reduced sensitivity compared to Llama: interventions and coefficient drops are smaller, suggesting differences in localization or redundancy; nevertheless, interventions still degrade performance substantially for targeted neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Analogous CNA findings (top neurons project to correct tokens, masking them reduces accuracy), and hidden-interpretable shallow FFN neuron interventions in GPT-J can collapse arithmetic performance (e.g., intervening 4,272 identified shallow neurons caused 100% accuracy decrease in at least one GPT-J experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Quantitative differences between models (Llama vs GPT-J) indicate variability in how concentrated arithmetic representations are; effect magnitudes and exact neuron/head indices differ between architectures/training runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis <em>(Rating: 2)</em></li>
                <li>Interpreting the inner mechanisms of large language models in mathematical addition <em>(Rating: 2)</em></li>
                <li>Understanding addition in transformers <em>(Rating: 2)</em></li>
                <li>The clock and the pizza: Two stories in mechanistic explanation of neural networks <em>(Rating: 1)</em></li>
                <li>Transformer feed-forward layers are key-value memories <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8133",
    "paper_id": "paper-272826648",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Llama-7B arithmetic findings",
            "name_full": "Empirical findings on arithmetic behavior of Llama-7B",
            "brief_description": "Detailed empirical analysis of how Llama-7B (32-layer, 7B parameter decoder-only transformer) performs 1–3 digit addition, subtraction, multiplication and division, showing arithmetic ability localizes to a few attention heads and a small set of FFN neurons and that multi-digit performance relies on memorized 1-digit knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-7B",
            "model_description": "Decoder-only transformer, 32 layers, 32 attention heads per layer, FFN size ~11,008 neurons per FFN layer; standard pretrained Llama weights (as studied in paper).",
            "arithmetic_task_type": "1-digit, 2-digit, 3-digit addition, subtraction, multiplication, division (prompts including numerals and number words); separate 2-digit training for LoRA experiments",
            "mechanism_or_representation": "Arithmetic ability concentrated in a small number of 'arithmetic heads' (specific attention heads, e.g., head 17_22 for addition/subtraction, head 14_19 for division) that appear to store parameters that activate deep FFN neurons representing final-token concepts; multi-digit answers rely on memorized 1-digit operations (memorization + digit-wise composition).",
            "probing_or_intervention_method": "Intervention of attention heads by zeroing head parameters (head ablations), CNA (Comparative Neuron Analysis) comparing neuron importance scores between original and intervened models, projection of FFN neuron values into vocabulary space, selective FFN neuron masking/keeping experiments.",
            "performance_metrics": "Original model accuracy reported as 74.8% (on the evaluation used for head interventions); on a separate 2-digit dataset original accuracy reported ≈62.96%; intervening certain heads reduces accuracy drastically (three heads give ≥10% drop; head 17_22 causes large drops on addition/subtraction), intervening the top-99 deep FFN neurons (identified by CNA) can reduce 1-digit addition/subtraction accuracy to 0% (masking top99 → 100% accuracy decrease in one experiment), while intervening all other deep FFN neurons (keeping only those top neurons) reduces accuracy by only ~3.9%. LoRA fine-tuning (see separate entry) raises 2-digit accuracy to ~89–95% depending on layer.",
            "error_types_or_failure_modes": "Failures primarily when attention-head-mediated activation of memorized 1-digit FFN neurons is disrupted (loss of 1-digit memorization); poor baseline multiplication/division performance relative to addition/subtraction in some experiments; multi-digit correctness often depends on correct 1-digit memorization and digit-wise combination; interventions can reveal dependence on a few memorized associations.",
            "evidence_for_mechanism": "Head ablations (zeroing) cause large decreases in accuracy for specific operations; CNA shows coefficient (activation) scores of particular deep FFN neurons drop substantially after head interventions; projection of these FFN neurons into vocabulary space yields tokens related to the expected numeric output (e.g., tokens for '8'), and masking top-identified FFN neurons eliminates correct prediction while masking other deep FFN neurons does not.",
            "counterexamples_or_challenges": "Top head for multiplication did not significantly impact multiplication accuracy (contrasts with addition/subtraction), indicating heterogeneity across operations; some multi-digit and more complex arithmetic exhibits lower overall performance (limits of memorization/generalization); results depend on intervention method (zeroing) and on the projection-to-vocabulary interpretability method (admitted limitations).",
            "uuid": "e8133.0",
            "source_info": {
                "paper_title": "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CNA",
            "name_full": "Comparative Neuron Analysis (CNA)",
            "brief_description": "A procedure introduced in this paper to compare per-neuron importance (log-probability increase contribution) between an original model and an intervened model for the same input to locate neurons and attention heads causally responsible for changes in final-token probability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-7B (applied also to GPT-J in replication)",
            "model_description": "Method applied to the studied models (see model entries).",
            "arithmetic_task_type": "Used to analyze 1-digit, 2-digit, 3-digit arithmetic cases across operations (addition, subtraction, multiplication, division) and gender-bias editing cases.",
            "mechanism_or_representation": "Operates on the representation that FFN output equals sum_k m_lk * fc2_lk (key-value FFN interpretation); CNA computes importance of each m_lk*fc2_lk as the change in log probability of the target token when adding that subvalue versus not, and compares those importance scores between original and intervened models to locate neurons whose activation change explains prediction changes.",
            "probing_or_intervention_method": "Computes importance score = log p(w | with neuron) − log p(w | without neuron) using softmax over unembedding; interventions include zeroing attention heads and zeroing specific FFN neurons identified by CNA; projection of FC2 vectors into vocabulary space (softmax(E_u v)) for semantic interpretability.",
            "performance_metrics": "Used to quantify neuron importance changes (reported coefficient and importance score drops); in experiments CNA-identified top-99 neurons when masked produced 100% accuracy loss for 1D+/- (in Llama study) and large coefficient drops (e.g., coefficient decreases of top sets reported, and specific numeric decreases like 15.8% coefficient drop when intervening a lowest important neuron among top99).",
            "error_types_or_failure_modes": "Method depends on assumptions: projection into vocabulary space for interpretability is heuristic and not fully theoretically grounded; results can vary with intervention method (zeroing vs other interventions); CNA is computationally intensive so authors use importance-score ranking rather than full causal mediation for all neurons.",
            "evidence_for_mechanism": "Applying CNA located deep FFN neurons whose coefficient and importance scores dropped after attention-head intervention; top tokens from projecting those neurons aligned with expected output tokens; selective masking experiments validated that those neurons carry nearly all prediction-relevant information for the studied 1-digit cases.",
            "counterexamples_or_challenges": "Projection-to-vocabulary interpretability and the choice of importance-score metric are recognized limitations; different attribution/intervention techniques may yield different identified neurons; CNA is approximate because full causal mediation at all neurons is computationally infeasible.",
            "uuid": "e8133.1",
            "source_info": {
                "paper_title": "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Arithmetic heads",
            "name_full": "Attention-layer 'arithmetic heads' (operation-specialized heads)",
            "brief_description": "A small set of attention heads in specific layers (e.g., head 17_22 for addition/subtraction, head 14_19 for division) that, when ablated, cause large drops in arithmetic accuracy and appear to control activation of FFN neurons that represent final-token outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-7B (also observed in GPT-J)",
            "model_description": "Identified within the 32×32 attention head grid of Llama-7B; specific heads named by layer_index head_index.",
            "arithmetic_task_type": "1D, 2D, 3D addition, subtraction, division (different heads affect different operations), multiplication less clearly localized.",
            "mechanism_or_representation": "Heads act as 'feature-predicting' modules that activate deep FFN neurons corresponding to output tokens; they store parameters that are necessary to trigger activation (i.e., they modulate FFN coefficient scores that produce final-token logits).",
            "probing_or_intervention_method": "Head ablation by zeroing head parameters; downstream CNA to measure how FFN neuron importance/coefficient scores change after head ablation.",
            "performance_metrics": "Most heads produce minor accuracy changes when ablated (976/1024 heads give small drops 0.01%–2%); only a handful (e.g., three heads) give ≥10% decrease; head 17_22 ablation produces large drops on addition/subtraction (examples reported), head 14_19 on division.",
            "error_types_or_failure_modes": "Ablation of these arithmetic heads removes activation of deep FFN neurons and thus removes memorized 1-digit knowledge, causing systematic loss of correct outputs; some operations (multiplication) did not map cleanly to a single head.",
            "evidence_for_mechanism": "Ablation of an arithmetic head reduces coefficient scores of identified deep FFN neurons, projections of those neurons map to expected numeric tokens, and selective neuron masking reproduces the large performance drop—showing a causal chain from head → FFN activation → final token.",
            "counterexamples_or_challenges": "Not all operations localize equally (multiplication less affected by single-head ablation); different models (GPT-J) show quantitatively different sensitivity (e.g., smaller drops).",
            "uuid": "e8133.2",
            "source_info": {
                "paper_title": "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Shallow FFN hidden features",
            "name_full": "Hidden-interpretable shallow FFN neurons (feature enhancing)",
            "brief_description": "Shallow FFN neurons (layers ~0–15) that are 'hidden-interpretable' — they are not directly interpretable in vocabulary space but, after attention-layer transformation, become directly interpretable and serve to enhance input features (numbers/operators) used downstream.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-7B (also verified in GPT-J)",
            "model_description": "Shallow FFN layers of the studied 32-layer models (0th–15th FFN layers), containing many neurons (e.g., 176,128 in 0–15 layers for Llama-7B).",
            "arithmetic_task_type": "1D and 1D/ cases (e.g., '3+5=' → feature extraction from '3' and '5') and more generally used across arithmetic tasks.",
            "mechanism_or_representation": "These neurons encode enhanced features tied to input tokens (digit embeddings and local computations) which attention value-output transformations convert into interpretable features transferred to the final token position.",
            "probing_or_intervention_method": "Zero-shot identification via transforming each shallow FFN neuron by attention value-output matrices (0th–16th) and projecting into vocabulary space; interventions by zeroing sets of identified hidden-interpretable neurons and comparing accuracy.",
            "performance_metrics": "Intervening ~1,953 identified hidden-interpretable neurons (M=2 threshold) in 0–15 layers caused a large accuracy drop of 53.9% on 1D+/- cases; intervening 10,426 such neurons (~6% of shallow FFN neurons) caused ~68.4% accuracy drop; random interventions of the same size produced negligible drops (~2.6%–5.1%).",
            "error_types_or_failure_modes": "Distributed representation across many shallow neurons means partial interventions can produce graded failure; identification threshold (M) affects set size and observed impact.",
            "evidence_for_mechanism": "Projection-after-attention shows neurons become interpretable; targeted interventions on these neurons cause large accuracy degradation, while random interventions of same size do not — implicating these neurons in feature enhancement.",
            "counterexamples_or_challenges": "Direct interpretability before attention is limited (they are 'hidden-interpretable' only after attention transforms them); sets are large and identification depends on heuristic thresholds for vocabulary-concept counts.",
            "uuid": "e8133.3",
            "source_info": {
                "paper_title": "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Deep FFN prediction neurons & PE-DAG",
            "name_full": "Deep FFN neurons as prediction units and Prediction-Enhancing Directed Acyclic Graph (PE-DAG)",
            "brief_description": "Deep FFN neurons (layers ~17–31) that directly encode concepts for final output tokens (projectable to vocabulary) and that form a recurrent activation graph (PE-DAG) where lower FFN neurons activate higher ones to amplify final-token probability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-7B (replicated in GPT-J)",
            "model_description": "Deep FFN layers (roughly 17th–31st layers) containing interpretable FFN value vectors whose projections align with final-token concepts.",
            "arithmetic_task_type": "Final-token prediction for 1D, 2D, 3D arithmetic across operations; PE-DAG observed specifically among top important neurons for 1D+ cases (example nodes: 19_5769, 25_7164, 28_3696).",
            "mechanism_or_representation": "FFN is interpreted as key-value memories: neuron subvalue = m_lk * fc2_lk; PE-DAG describes large inner products between an FC2 value of a lower neuron and FC1 key of a higher neuron (chaining activations), producing recursive enhancement of final-token logit.",
            "probing_or_intervention_method": "CNA to identify important deep FFN neurons; compute inner products between FC2 (value) and FC1 (key) vectors to find directed activation links; intervene by zeroing individual neurons in PE-DAG and measure coefficient/importance score drops.",
            "performance_metrics": "Intervening a single neuron among top99 important neurons can reduce coefficient scores of other important neurons by ~15.8% (example), and masking top-identified deep neurons (top99) caused 100% accuracy loss for 1D+/- in one Llama experiment; in GPT-J comparable but smaller effects (top99 mask → ~58.4% accuracy decrease).",
            "error_types_or_failure_modes": "If root neurons in PE-DAG are disrupted, the chain of activations collapses and final-token probability drops; PE-DAG structure and importance magnitudes vary across models (Llama vs GPT-J) and operations.",
            "evidence_for_mechanism": "Large inner products measured between FC2 of lower neuron and FC1 keys of higher neurons; observed recursive activation and coefficient score drops when lower neurons are intervened; neuron projections map to expected output tokens.",
            "counterexamples_or_challenges": "Quantitative strength of PE-DAG differs by model (smaller in GPT-J) and operation; identification relies on linear-algebra measures that could be sensitive to representation scaling and intervention style.",
            "uuid": "e8133.4",
            "source_info": {
                "paper_title": "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LoRA mechanism (paper)",
            "name_full": "Mechanistic effect of LoRA fine-tuning on arithmetic behavior",
            "brief_description": "The paper finds that LoRA (low-rank adaptation) improves arithmetic performance by amplifying coefficient scores (m_lk) of important deep FFN neurons related to final predictions, with amplification magnitude depending on which attention layer LoRA is applied to.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-7B (LoRA fine-tuned variants)",
            "model_description": "32 separate fine-tuned variants where a LoRA module was added to one attention layer each (layers 0–31); training on 2-digit arithmetic dataset (18k train, 2k test) for up to 4 epochs with tuned LR).",
            "arithmetic_task_type": "2-digit arithmetic (addition, subtraction, multiplication, division) including negative numbers (e.g., '3-5=-2') for training set.",
            "mechanism_or_representation": "LoRA acts similarly to adding a head: when applied to different attention layers it changes residual outputs that ultimately increase FFN coefficient scores of final-prediction neurons; shallow-layer LoRA produced larger amplification of coefficient scores for important neurons than deep-layer LoRA.",
            "probing_or_intervention_method": "CNA between original model and LoRA-fine-tuned models to measure coefficient-score changes of identified important neurons across different LoRA placements; evaluation of accuracy per LoRA-layer model.",
            "performance_metrics": "Original Llama on 2-digit dataset: ≈62.96% accuracy; LoRA fine-tuned models: 1st–9th layer LoRA models ≈90% accuracy (range reported ~89–95%), model accuracy declines for LoRA placed in deeper layers starting from the 10th model; coefficient-score increases for important neurons: e.g., 1st–9th models produce 42%–59% average increases across top neuron sets (top50→top10 reported), 10th–16th produce 29%–53% increases, 17th–30th produce smaller increases (2%–28%).",
            "error_types_or_failure_modes": "LoRA placed in certain layers (e.g., starting layer 10 onward) shows a downward trend in achieved accuracy and less amplification of key FFN neurons; a LoRA model that fails to leverage certain deep neuron features shows drops in corresponding neuron coefficient scores (example: 20th-layer LoRA failed to leverage feature 19_5769 causing drop in 25_7164).",
            "evidence_for_mechanism": "CNA shows consistent coefficient-score amplification of important deep FFN neurons in LoRA-fine-tuned models compared to the original; accuracy correlates with magnitude of coefficient increases and with LoRA layer placement; pruned model experiments further demonstrate that amplifying the remaining important neurons via LoRA recovers much accuracy.",
            "counterexamples_or_challenges": "Layer-dependent effects: LoRA in shallow layers amplifies coefficients more effectively than deep-layer LoRA; not all LoRA placements yield identical qualitative improvements; LoRA acts indirectly (via attention/residual changes) rather than by directly modifying deep-FFN weights.",
            "uuid": "e8133.5",
            "source_info": {
                "paper_title": "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-J replication",
            "name_full": "Replication results on GPT-J-6B",
            "brief_description": "Experiments replicated on GPT-J show qualitatively similar four-stage mechanism (feature enhancing → feature transferring → feature predicting → prediction enhancing) though with quantitatively smaller effects: interventions on top neurons reduce accuracy but less dramatically than in Llama-7B.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J-6B",
            "model_description": "6B-parameter autoregressive transformer (GPT-J) used to replicate key experiments; same intervention/CNA protocols applied.",
            "arithmetic_task_type": "1D and 2D arithmetic cases (replicated experiments analogous to those on Llama-7B).",
            "mechanism_or_representation": "Same four-stage internal logic chain observed; arithmetic ability localizes to specific heads and FFN neurons, shallow hidden-interpretable neurons exist, and deep FFN PE-DAG structures observed.",
            "probing_or_intervention_method": "Head ablations, CNA ranking and masking of top FFN neurons, hidden-interpretable shallow neuron identification and interventions.",
            "performance_metrics": "When masking top99 identified FFN neurons in GPT-J, accuracy decreased by ~58.4% (versus 100% mask effect reported in Llama for some cases); coefficient decreases on interventions are smaller in magnitude (examples: coef decreases reported like 17%–29% for different top-k sets).",
            "error_types_or_failure_modes": "Reduced sensitivity compared to Llama: interventions and coefficient drops are smaller, suggesting differences in localization or redundancy; nevertheless, interventions still degrade performance substantially for targeted neurons.",
            "evidence_for_mechanism": "Analogous CNA findings (top neurons project to correct tokens, masking them reduces accuracy), and hidden-interpretable shallow FFN neuron interventions in GPT-J can collapse arithmetic performance (e.g., intervening 4,272 identified shallow neurons caused 100% accuracy decrease in at least one GPT-J experiment).",
            "counterexamples_or_challenges": "Quantitative differences between models (Llama vs GPT-J) indicate variability in how concentrated arithmetic representations are; effect magnitudes and exact neuron/head indices differ between architectures/training runs.",
            "uuid": "e8133.6",
            "source_info": {
                "paper_title": "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
            "rating": 2,
            "sanitized_title": "a_mechanistic_interpretation_of_arithmetic_reasoning_in_language_models_using_causal_mediation_analysis"
        },
        {
            "paper_title": "Interpreting the inner mechanisms of large language models in mathematical addition",
            "rating": 2,
            "sanitized_title": "interpreting_the_inner_mechanisms_of_large_language_models_in_mathematical_addition"
        },
        {
            "paper_title": "Understanding addition in transformers",
            "rating": 2,
            "sanitized_title": "understanding_addition_in_transformers"
        },
        {
            "paper_title": "The clock and the pizza: Two stories in mechanistic explanation of neural networks",
            "rating": 1,
            "sanitized_title": "the_clock_and_the_pizza_two_stories_in_mechanistic_explanation_of_neural_networks"
        },
        {
            "paper_title": "Transformer feed-forward layers are key-value memories",
            "rating": 1,
            "sanitized_title": "transformer_feedforward_layers_are_keyvalue_memories"
        }
    ],
    "cost": 0.0151505,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis
21 Sep 2024</p>
<p>Zeping Yu zeping.yu@postgrad.sophia.ananiadou@manchester.ac.uk 
Department of Computer Science
The University of Manchester</p>
<p>Sophia Ananiadou 
Department of Computer Science
The University of Manchester</p>
<p>Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis
21 Sep 20247E5A38D673F1A6B0A8EEA0AF9B762DA5arXiv:2409.14144v1[cs.CL]
We find arithmetic ability resides within a limited number of attention heads, with each head specializing in distinct operations.To delve into the reason, we introduce the Comparative Neuron Analysis (CNA) method, which identifies an internal logic chain consisting of four distinct stages from input to prediction: feature enhancing with shallow FFN neurons, feature transferring by shallow attention layers, feature predicting by arithmetic heads, and prediction enhancing among deep FFN neurons.Moreover, we identify the human-interpretable FFN neurons within both feature-enhancing and feature-predicting stages.These findings lead us to investigate the mechanism of LoRA, revealing that it enhances prediction probabilities by amplifying the coefficient scores of FFN neurons related to predictions.Finally, we apply our method in model pruning for arithmetic tasks and model editing for reducing gender bias.Code is on https://github.com/zepingyu0512/arithmetic-mechanism.</p>
<p>Introduction</p>
<p>Arithmetic ability is a crucial foundational skill of large language models (LLMs) (Brown et al., 2020;Ouyang et al., 2022;Chowdhery et al., 2023), contributing significantly to reasoning (Wei et al., 2022;Kojima et al., 2022) and mathematical tasks (Peng et al., 2021;Azerbayev et al., 2023).While existing studies (Quirke et al., 2023;Zhang et al., 2023;Stolfo et al., 2023) have made significant breakthroughs in understanding arithmetic tasks, the exact mechanism still remains elusive.Zhang et al. (2023) find that only a few attention heads significantly impact arithmetic performance, but they do not elaborate on the mechanisms of these heads or how they influence FFN layers.Stolfo et al. (2023) intervene the hidden states and find the information flow from number and operation positions to the last position.However, they do not locate the important attention heads (proved to store different abilities (Olsson et al., 2022;Gould et al., 2023)) and FFN neurons (proved to store knowledge (Dai et al., 2021;Meng et al., 2022a)).Despite the challenge of pinpointing important FFN neurons among tens of thousands of nodes, many studies (Gurnee et al., 2023;Lieberum et al., 2023;Nanda et al., 2023) emphasize that considering FFN neurons as fundamental units is crucial for better understanding FFN layers.Furthermore, as model editing typically occurs at the neuron level (Dai et al., 2021;Geva et al., 2022), it remains unclear how to effectively leverage the explanations due to the uncertainty surrounding the precise locations of important parameters.In this study, we take attention heads and FFN neurons as fundamental units, and explore the exact parameters store the arithmetic ability for different operations.We observe that only a minority of heads play significant roles in arithmetic tasks, which we refer to as "arithmetic heads".Through experiments involving 1-digit to 3-digit operations, as well as ablation studies comparing "change-one" cases (e.g., 15+37=52) with "memorize" cases (e.g., 15+32=47), we find critical memorization of 1-digit operations is lost when these heads are intervened.</p>
<p>To explore the underlying mechanisms of this phenomenon, we propose the Comparative Neuron Analysis (CNA) method, which compares the change of neurons between the original model and the intervened model for the same case.We construct the internal logic chain by identifying four distinct stages that span from inputs to prediction, as depicted in Figure 1.During the feature enhancing stage, hidden-interpretable features are extracted from shallow FFN neurons.Subsequently, in the feature transferring stage, shallow attention layers convert these features into directlyinterpretable features and then transfer them to the last position.In the feature predicting stage, the arithmetic heads play critical roles, activating deep FFN neurons related to the final prediction.Finally, a prediction enhancing stage exists among deep FFN neurons.Lower FFN neurons activate upper FFN neurons, while both of them enhance the probability of the final prediction.</p>
<p>Based on this analysis, we investigate the mechanism of LoRA (Hu et al., 2021).We train a total of 32 models on a 2-digit arithmetic dataset, with each model integrating LoRA on one attention layer (0th to 31th).Starting from the 10th model, the accuracy of the model exhibits a noticeable downward trend, with varying rates of decline observed in the feature enhancing and prediction enhancing stages.Employing our CNA method to compare the original model with the fine-tuned model, we note a significant increase in the coefficient scores of crucial deep FFN neurons.Hence, we conclude that LoRA enhances the final prediction by amplifying the coefficient scores of important FFN neurons.Finally, using our findings, we develop methods on model pruning for arithmetic tasks, and model editing for reducing gender bias.</p>
<p>To summarize, our contributions are as follows:</p>
<ol>
<li>
<p>We find the reason why only a few heads can influence arithmetic ability is that these heads store crucial parameters for memorizing 1D operations.We identify human-interpretable FFN neurons across both shallow and deep layers.</p>
</li>
<li>
<p>We propose the CNA method and construct the internal logic chain from inputs to prediction with four stages: feature enhancing, feature transferring, feature predicting, prediction enhancing.</p>
</li>
<li>
<p>We use the CNA method to explore the mech-anism of LoRA and find LoRA increases the probability of final predictions by amplifying the important FFN neurons' coefficient scores.We design a model pruning method for arithmetic tasks, and a model editing method for reducing gender bias.</p>
</li>
</ol>
<p>2 Related Work</p>
<p>Mechanistic Interpretability</p>
<p>Mechanistic interpretability aims to reverse engineer the intricate computations executed by transformers.The analysis of transformer circuits stands as a key approach within this domain.2022) present an explanation on an indirect object identification case in GPT2.</p>
<p>Causal mediation analysis (Pearl, 2001;Vig et al., 2020) is also widely used for locating important modules.Meng et al. (2022a,b) intervene the hidden states of GPT2 (Radford et al., 2019) and ascertain that the medium FFN layers play a significant role in processing subject names.Wang et al. (2023) intervene the attention layers to explore the mechanism of in-context learning and observe an information flow from demonstrations to corresponding labels.Geva et al. (2023) find two critical points on relation and subjection positions through interventions on attention edges.</p>
<p>Since causal mediation analysis methods require expensive forward pass over multiple input, several studies try to design static methods for interpreting language models.Geva et al. (2022) utilize the product of norm and coefficient score to locate important FFN neurons and find many FFN neurons have human-interpretable concepts when projecting into vocabulary space.Dar et al. (2022) find most matrices in attention and FFN layers are interpretable in vocabulary space.Hanna et al. (2023) investigate how GPT2-small computes greater-than.Gould et al. (2023) demonstrate that successor heads can aid in predicting the subsequent order, such as predicting "3" after "2".Zhang et al. (2023) investigate the attention heads for addition operation, and find only a few heads play significant roles.Zhong et al. (2024) inves-tigate the clock and pizza algorithms for modular addition.Quirke et al. (2023) studies n-digit integer addition on an one-layer transformer, and find individual digits are computed in parallel.Through interventions on hidden states, Stolfo et al. (2023) find that attention layers transform the information to the last token, and FFN layers capture resultrelated information.</p>
<p>Understanding Arithmetic in LLM</p>
<p>Arithmetic Heads in LLMs</p>
<p>We aim to examine the localization of arithmetic ability in Llama-7B (Touvron et al., 2023), a large language model consisting of 32 layers.Each attention layer contains 32 heads, and each FFN layer has 11,008 neurons.We observe the same results and mechanisms in GPT-J (Wang and Komatsuzaki, 2021), detailed in Appendix C.</p>
<p>Background</p>
<p>We start by introducing the inference pass in decoder-only language models.Following previous studies (Geva et al., 2023), we omit the bias term and layer normalization (Ba et al., 2016).The model aims to generate a probability distribution Y based on an input sequence X = [t 1 , t 2 , ..., t T ] consisting of T tokens.Y is a B-dimension vector containing probabilities for each token in vocabulary V .Each token t i in X is embedded into a vector x 0 i ∈ R d using an embedding matrix E ∈ R B×d .Then the vectors undergo transformation through L + 1 transformer layers (0th-Lth).Vector x l i on the ith position at layer l is computed by:
x l i = x l−1 i + A l i + F l i (1)
where A l i ∈ R d and F l i ∈ R d are the outputs of the lth attention and FFN layers, referred to as the attention output and FFN output, respectively.x l−1 i represents the layer output at layer l − 1, which also serves as the layer input at layer l.The term x l−1 i + A l i is denoted as the residual output.The attention layer captures information from different positions through H multiple heads AT T N l j , and the FFN layer transforms the residual output by matrices W f c1 and W f c2 with non-linearity σ:
A l i = H j=1 AT T N l j (h l−1 1 , h l−1 2 ..., h l−1 T ) (2)F l i = W l f c2 σ(W l f c1 (x l−1 i + A l i ))(3)
The representation of the last position on the final layer x L T is used for predicting the probability distribution Y of the next token by a softmax function on an unembedding matrix E u ∈ R B×d :
Y = sof tmax(E u x L T )(4)
Geva et al. (2020) demonstrate that the FFN layer can be conceptualized as key-value memories, with matrices W l f c1 ∈ R d×N and W l f c2 ∈ R N ×d storing keys and values for N neurons.The FFN output is obtained by adding N subvalues, where each subvalue is the result of multiplying a coefficient score m l k with a f c2 vector f c2 l k ∈ R d (also referred to as the FFN value).These coefficient scores are calculated as the inner product between the residual output and the corresponding f c1 vector f c1 l k ∈ R d (also referred to as the FFN key):
F l = N k=1 m l k f c2 l k (5) m l k = σ(f c1 l k • (x l−1 + A l ))(6)
In other words, the kth subvalue is the kth column of W l f c2 , whose subkey is the kth row of W l f c1 .</p>
<p>Interventions on Attention Heads</p>
<p>We make a 2-digit arithmetic dataset, including addition (2D+), subtraction (2D-), multiplication (2D*) and division (2D/).Similar to Stolfo et al. (2023), we design four prompts for each operation including both numbers (e.g. 3) and number words (e.g.three), reported in Appendix A. The evaluation dataset has 1,600 sentences.We intervene the attention heads by setting all the head's parameters into zero, and we take accuracy as metric.</p>
<p>Llama-7B consists of 32 layers with 32 heads per layer.Consequently, we execute the model 1,024 times (intervening on one head each time for 1,600 cases) and compute the average accuracy on the evaluation dataset.</p>
<p>Results of Different Heads</p>
<p>The accuracy of the original model is 74.8%.Interventions on the majority of heads (976 in total) lead to only a minor decrease in accuracy (0.01%-2%).</p>
<p>Only three heads result in a decrease of 10% or more.The top5 heads are shown in 21.4% in accuracy.Moreover, the accuracy decrease on these heads is attributed to different operations.For example, 17 22 drops a lot on 2D+ and 2D-, and 14 19 performs extremely poor on 2D/.</p>
<p>Reasons Causing Accuracy Decrease</p>
<p>Since the accuracy of more complicated operations are low, we analyze the most important head for each operation in 1-digit (1D), 2-digit (2D) and 3digit (3D) operations, shown in Table 2.The most important heads in 1D, 2D and 3D operations are the same.We report the details of top5 heads in Appendix E. In comparison to addition, subtraction, and division, the top head for multiplication does not significantly impact accuracy.We leave further investigation of this phenomenon for future work.In Table 2, the decreases of 1D, 2D and 3D operations are similar.Therefore, we hypothesize that the heads store important parameters about 1D operations.Since 2D and 3D also rely on the memorization of 1D operations, the 2D/3D accuracy decrease when the 1D memorization is lost.We also analyze two types of cases for each operation, which are named "change-one" (similar to the definition of "carry" in Opedal et al. (2024)) and "memorize"."Memorize" cases only require memorization.For example, "15+32=47" requires memorization about "5+2=7" and "1+3=4", thus "15+32= -&gt; 4" and "15+32=4 -&gt; 7" are two "memorize" cases."Change-one" cases require the change-one ability.For example, "15+37= -&gt; 5" is a "change-one" case, as the output is based on "5=1+3+1".For multiplication and division cases, we take the last token as "memorize" cases, and others as "change-one" cases.We compute the accuracy decrease between the original model and the intervened model for each operation.The results are shown in Table 3.If the heads only store change-one abilities, the decrease of "memorize" cases should be much smaller than "change-one" cases.However, the accuracy decrease of "memorize" cases and "change-one" cases are similar.Hence, we hypothesize the heads store parameters for memorizing 1D operations.</p>
<p>Comparative Neuron Analysis for Mechanistic Interpretability</p>
<p>In this section, we investigate how head 17 22 influence 1D+ and 1D-operations.Analysis of head 14 19 for 1D/ operations is shown in Appendix B, resulting the same stages with Section 4.2-4.4.</p>
<p>Methodology</p>
<p>The core idea of our proposed CNA method is comparing the same neuron across different models given the same input, or comparing the same neuron across different inputs within the same model.Due to the computational intensity of the forward pass, employing causal mediation analysis methods on every neuron is impractical.Therefore, we take the increase of log probability (Yu and Ananiadou, 2024b) as importance score for each neuron.</p>
<p>The importance score of a FFN neuron
m l k f c2 l k is log(p(w|x l−1 T +A l T +m l k f c2 l k ))−log(p(w|x l−1 T + A l T ))
, where w is the final predicted token and the probability is computed by the softmax function when multiplying the vectors with the unembedding matrix (Eq.4).Then we compute the change of each neuron's importance score between the original model and the intervened model (intervening head 17 22 ), and sort the change score to locate the most important neurons causing the final prediction probability decrease.We only intervene one head because this head can result very much decrease in accuracy.In later sections, we introduce the analysis process focusing on a specific case "3+5=", and devise various methods to prove these findings are applicable to all 1D+ and 1D-cases.</p>
<p>Feature Predicting via Arithmetic Head</p>
<p>For case "3+5=" with prediction "8", we compute the importance score change for each neuron, and find the most important neurons are in FFN layers.We project these neurons in vocabulary space (Geva et al., 2022) by multiplying the FFN neurons v and unembedding matrix:
P v = sof tmax(E u v).
The top tokens when projecting into the unembedding space are shown in Table 4. 28 3696 means the 3696th neuron in the 28th FFN layer."ori" and "inv" denote the original and the intervened model ("mdl")."imp" and "coef" represent the importance score and coefficient score of each neuron.All these neurons contain concepts about "eight" and "8" in top tokens.The importance scores and coefficient scores drop a lot in the intervened model.From the interpretable results, we hypothesize that the reason why the accuracy decreases a lot in the intervened model is that head 17 22 stores important parameters for activating the important FFN neurons related to the final prediction.To verify this hypothesis, we conduct two experiments on all 1D+ and 1D-cases.For each case, we employ the CNA method to identify the important FFN neurons.Then in the original model we only intervene the most important FFN neurons ("mask") or intervene all the other FFN neurons within the 17th−31th layers ("keep").The accuracy decrease on all 1D+ and 1D-cases is presented in Table 5 When intervening the top99 FFN neurons, the accuracy decreases 100%.When intervening all the other neurons in deep FFN layers, the accuracy only decreases 3.9%.This suggests that almost all important information for predicting the final token is contained within the FFN neurons identified by our CNA method.We also report the decrease of the top neurons' coefficient scores ("coef") between the intervened model and the original model in Table 5.In all situations, the coefficient scores drop much.Therefore, our hypothesis is verified: head 17 22 stores important parameters for activating the important FFN neurons related to final predictions.</p>
<p>When head 17 22 is intervened, coefficient scores of important FFN neurons drop a lot, thus final predictions' probabilities drop much.</p>
<p>Prediction Enhancing among Deep FFN Neurons</p>
<p>In case "3+5=", we observe that there is a predic- Intervening only one neuron among top99 neurons can reduce the coefficient scores by 15.8%.The results indicate that the prediction enhancing stage exists among the identified deep FFN neurons.Among 1D+ and 1D-cases, comparing with intervening the lowest neuron among top10 and top20 neurons, the coefficient score decreases more when intervening the lowest neuron among top50 and top99 important neurons.This phenomenon maybe because the lowest neuron among top99 and top50 neurons typically resides on lower FFN layers compared to those on top10 and top20 neurons.2023) utilize causal mediation analysis and find the model processes numbers and operators on early FFN layers and transfer into last position via attention layers.In this section, our objective is to locate the specific neurons fulfilling this function and to analyze the roles of shallow FFN layers and attention layers in this process.To identify the important shallow FFN neurons for case "3+5="-&gt;"8", we sort the neurons by computing the inner products between the PE-DAG root 19 5769 and the attention transformation of each FFN neuron.We find that the neurons (on residual streams of "3" and "5") with highest inner products are hidden-interpretable.When projecting the original neurons into vocabulary space, they do not contain human-interpretable concepts in top tokens.However, after the transformation of attention layers, these neurons become interpretable.Moreover, we find that the word embeddings of "3" and "5" are also hidden-interpretable.The top vocabulary tokens of original and 15th attention layer transformation are shown in  We hypothesize that these hidden-interpretable FFN neurons are crucial for enhancing input features.We develop a zero-shot method to identify these hidden-interpretable shallow FFN neurons.For each FFN neuron on 0th − 15th layer, we compute the transformation by 0th − 16th attention layers' value-output matrices, and project these vectors into vocabulary space.If the top50 tokens contain M or more concepts related to numbers or operations, we add this neuron into a hiddeninterpretable neuron set.Then we intervene all the neurons in this neuron set in the original model, and compute the accuracy decrease on all 1D+ and 1D-cases.The number of neurons and accuracy under different M are shown in Table 8 There are 176,128 neurons in 0th − 15th FFN layers.Intervening with only 1,953 neurons (M=2) results in a decrease of 53.9%.This strongly suggests that these hidden-interpretable neurons play a significant role in enhancing features and are valuable for final predictions.Further supporting this notion is the observation that randomly intervening 1,953 neurons on the 0th − 15th FFN layers only results in an accuracy decrease of 2.6%.Compared to directly interpretable neurons in deep FFN layers, hidden-interpretable neurons in shallow FFN layers are more widely distributed.When intervening 10,426 neurons (about 6% of all neurons in 0th − 15th layers), the accuracy decreases 68.4%.</p>
<p>Constructing the Internal Logic Chain from Inputs to Prediction</p>
<p>In Section 4.2-4.4,we apply our CNA method to identify the important neurons for the case "3+5", and also design experiments to verify the generality across other 1D+ and 1D-cases.In this section, we conclude the internal logic chain from inputs to prediction for case "3+5=" -&gt; "8": First, in feature enhancing stage, shallow FFN neurons containing hidden-interpretable features (e.g.11 2258 , 12 4072 ) are extracted.In feature transferring stage, the hidden-interpretable features (word embeddings and shallow FFN neurons) are transformed into directly-interpretable features by attention layers and then transferred to the last position.In feature predicting stage, head 17 22 activates deep FFN neurons associated with the concept of "8" (e.g.28 3696 , 25 7164 , 19 5769 ) based on the enhanced features.Finally, in the prediction enhancing stage, lower FFN neurons activate higher FFN neurons, which collectively contribute to the probability of "8" in the final prediction.</p>
<p>Through our CNA method, we precisely identify crucial parameters (attention heads and FFN neurons) for predicting final tokens.Compared to prior studies, our approach enables the discovery of more detailed locations and offers a clearer explanation of the information flow.Given our method's ability to pinpoint precise parameters, it can be effectively leveraged for downstream tasks such as model pruning and model editing, which we discuss in Section 6.</p>
<p>Understanding the Mechanism of LoRA</p>
<p>LoRA (Hu et al., 2021) is a commonly used parameter-efficient fine-tuning method (Houlsby et al., 2019;Li and Liang, 2021;Lester et al., 2021).By adding trainable low-rank matrices into attention layers, models are fine-tuned with only 0.5% additional parameters, yielding favorable outcomes.Intuitively, LoRA is similar to a head.Inspired by the analysis on arithmetic heads, we apply the CNA method to understand the mechanism of LoRA.</p>
<p>We first investigate whether LoRA plays distinct roles when added into various layers.We fine-tune 32 models on the 2-digit arithmetic dataset, with each model incorporating a low-rank matrix into a distinct attention layer.Notably, we introduce negative numbers in 2D cases such as "3-5=-2", as the original Llama model does not learn this concept well.The training and testing set consist of 18,000 and 2,000 sentences, respectively.We determine the optimal learning rate from choices of 0.001, 0.0005, and 0.0001.The maximum epoch is set to 4. The results are depicted in Figure 2. All the fine-tuned models exhibit superior ac-curacy compared to the original model (62.96%).The 0th and the 31th layer may have special use, since the accuracy of the 0th and 31th models differs much from their neighboring models.The accuracy of the 1st − 9th models is around 90%. Starting from the 10th model, the accuracy keeps decreasing.The average slope during the 10th to 16th models differs from that of the 17th to 30th models.Motivated by LoRA's accuracy curve and the analysis of arithmetic heads, we hypothesize that LoRA enhances the correct predictions' probabilities by amplifying the deep FFN neurons related to final predictions.We apply our CNA method on the original model and five LoRA models analyzing the case "3+5=", detailed in Table 9 Across all five fine-tuned models, the coefficient scores of 25 7164 and 19 5769 surpass those of the original model.The scores are higher in shallowlayer models compared to deep-layer models.The significant decrease in the coefficient score observed in 25 7164 in the 20th model can be attributed to its failure to leverage the features of 19 5769 .</p>
<p>LoRA layer top50 top30 top20 top10 For all cases, we compute the average coefficient score increase of 1st − 9th, 10th − 16th, and 17th−30th models on the most important neurons, detailed in Table 10.Across all scenarios, the coefficient scores of significant FFN neurons surpass those of the original model.Notably, fine-tuning LoRA in shallow layers yields a greater amplification of FFN neurons' coefficient scores compared to deep layers.This observation validates our hypothesis: LoRA enhances the probabilities of final predictions by amplifying the coefficient scores of deep FFN neurons relevant to final predictions.
1st − 9th 42% 49% 57% 59% 10th − 16th 29% 36% 44% 53% 17th − 30th 2% 11% 14% 28%
In this section, we utilize our method for model pruning on arithmetic tasks and for model editing aimed at mitigating gender bias.</p>
<p>Model Pruning for Arithmetic Tasks</p>
<p>As recent powerful models boast tens of billions of parameters, the extraction of sub-networks from these large models for various downstream tasks has become crucial.This approach is based on the assumption that only a small subset of parameters in an over-parameterized model are pertinent to a specific task and similar tasks share similar sub-networks (Pfeiffer et al., 2023).Recent works (Stańczak et al., 2022;Foroutan et al., 2022) in multilingual models can support these hypotheses.</p>
<p>In this section, we apply our findings on model pruning for arithmetic tasks.As discussed in Section 4, important information for final predictions is concentrated in only a few deep FFN neurons.Therefore, we design a simple method to prune useless neurons in deep FFN layers.We apply our CNA method between the original model and the 9th LoRA model on all the 1D+, 1D-, 1D* and 1D/ cases, to find the important top500 neurons for each case.Then we prune all the other FFN neurons among 17th − 31th layers, thus only 5% deep FFN neurons are saved in the pruned model.Finally, we add LoRA on the 9th layer of the pruned model, and fine-tune on the training set.The parameters on deep FFN layers are reduced to 5%, and only 0.015% LoRA parameters are added.origin LoRA9 LoRA9-p LoRA9-r acc 62.9 89.3 82.3 17.1 The results are shown in Table 11.The accuracy of the fine-tuned pruned model (LoRA9p) is 82.3%, better than original Llama (62.9%).While our method do not reach the performance of the fine-tuned model without pruning (LoRA9), it still offers a promising avenue for model pruning.Furthermore, although 2-digit arithmetic is an easy task, fine-tuning LoRA on a randomly-pruned model (LoRA9-r) with the same number of neurons fails to yield satisfactory results (only 17.1%).This further underscores the significance of our method.</p>
<p>Model Editing for Reducing Gender Bias</p>
<p>Even though LLMs have achieved great success, they can learn, perpetuate, and amplify harmful social biases (Gallegos et al., 2023).In this section, we focus on gender bias, which is observed in different models (de Vassimon Manela et al., 2021;Kotek et al., 2023).We apply our CNA method analyzing similar cases with different genders in the same model.For example, we identify the important neurons for predicting "nurse" by calculating the change of importance scores between sentences "A woman works as a" and "A man works as a".Since the other words are the same except "woman" and "man", these neurons contain much gender bias causing p(nurse|woman) &gt; p(nurse|man).The neurons' top tokens of are shown in Table 12.For example, the top tokens of FFN neuron 19 8436 are all professions.Under the input "A woman works as a", this neuron's coefficient score is 3.39.While the neuron's coefficient score is only 0.14 activated by "A man works as a", proving that this neuron contains much gender bias.We then apply our CNA method on 32 common professions contain gender bias (detailed in Appendix D).Designing four prompts, we identify top18 important FFN neurons and edit them by setting their parameters to zero.The average perplexity difference log(p(prof |gend1)) − log(p(prof |gend2)) is shown in These results can demonstrate that our proposed CNA method can be utilized in different tasks.It is also important to note that the utilized gender bias datasets may not comprehensively represent general scenarios.We leave the explorations on different datasets in future work.</p>
<p>We aim to discuss the mechanisms behind causal mediation analysis and static interpretation methods.Causal mediation analysis methods can find the "root cause" (head 17 22 ) of the probability change, which are usually not interpretable.Static methods can locate the interpretable "direct cause" (FFN neurons), but many elements can activate these neurons.Our CNA method can locate both "root cause" and "direct cause", and reconstruct the whole logic chain from inputs to prediction.</p>
<p>Overall, we identify the important attention heads and FFN neurons for arithmetic operations.We propose the comparative neuron analysis (CNA) method and construct the internal logic chain from inputs to prediction, including the feature enhancing stage, feature transferring stage, feature predicting stage, and prediction enhancing stage.Based on these findings, we find LoRA increases the final predictions' probabilities by enlarging the important FFN neurons' coefficient scores.Finally, we apply our method and findings on model pruning for arithmetic tasks, and model editing for reducing gender bias.Our method and analysis offer a comprehensive insight for understanding LLM.</p>
<p>Limitations</p>
<p>The case studies rely on projecting vectors in vocabulary space, which is widely used in previous studies (Elhage et al., 2021;Ram et al., 2022;Geva et al., 2022;Dar et al., 2022).While the results are empirically interpretable, the theories of this method are incomplete.Therefore, we utilize this method in our case studies and supplement our findings with additional methods to strengthen our conclusions, thus enhancing their persuasiveness.</p>
<p>Another limitation lies in the lack of standardization across various studies regarding attribution methods.Different intervention methods (zero intervention, noise intervention, replace intervention, etc.) may get different results.Apart from causal mediation analysis methods and static interpretation methods, gradient-based methods (Sundararajan et al., 2017) andSHAP values (Lundberg andLee, 2017) are also widely utilized for attributing important modules.However, these methods often demand substantial computational resources, rendering them unsuitable for our work.</p>
<p>A potential risk of our work is that attackers can identify the important neurons and edit these neurons to change the output probability distribution.</p>
<p>For instance, instead of reducing the gender bias by setting the neurons' parameters to zero, they can amplify the gender bias professions' probabilities by enlarging the identified neurons in Section 6.2.Hence, it is important to distinguish whether a model is edited, and we leave this exploration in future work.</p>
<p>A Four Prompts in Arithmetic Dataset type prompt addition-1 The sum of n1 and n2 is addition-2 Q: What is n1 plus n2? A: addition-3 n1 plus n2 is addition-4 n1 + n2 = subtract-1 The difference between n1 and n2 is subtract-2 Q: What is n1 minus n2? A: subtract-3 n1 minus n2 is subtract-4 n1 -n2 = multiply-1 The product of n1 and n2 is multiply-2 Q: What is n1 times n2?A: multiply-3 n1 times n2 is multiply-4 n1 * n2 = division-1 The ratio of n1 and n2 is division-2 Q: What is n1 divides n2? A: division-3 n1 divides n2 is division-4 n1 / n2 =  In Table 15, when head 14 19 is intervened, coefficient scores of important neurons in deep FFN layers are reduced, causing the accuracy decrease.Also, the top identified neurons contain much information.Interventions on top99 neurons result in an accuracy decrease of 84.6%.top99 top50 top30 top20 top10 coef 1.3 0.9 3.2 4.9 7.0</p>
<p>C Results of Interventions in GPT-J</p>
<p>We conduct the same experiments in Section 3.3 in GPT-J.The accuracy when intervening each head is presented in Table 18.In GPT-J, we also observe that different heads store important parameters for various operations.For instance, the accuracy of 2D-decreases significantly when intervening in head 13 9 , whereas head 14 14 holds significant parameters for 2D/.</p>
<p>Then we apply the CNA method between the original model and the intervened model on head 13 9 on 2D-cases.The results are shown in  top99 top50 top30 top20 top10 coef 1.9 1.7 1.6 1.2 1.9 In Table 19, the top FFN neurons also play a large role in GPT-J.When intervening the top99 neurons, the accuracy decreases 58.4%.Compared with Llama, the degrees of coefficient decrease and accuracy change are both smaller.In Table 20, when intervening the lowest neuron among the important neurons identified by our CNA method, the deep neurons' coefficient scores decrease.Results in Table 21 indicate that the hiddeninterpretable shallow FFN neurons also exist in GPT-J.When intervening 4,272 neurons, the accuracy decreases 100%.</p>
<p>Overall, we observe similar results in GPT-J.Similar to Llama, GPT-J also exhibits the presence of four stages: feature enhancing, feature transferring, feature predicting, and prediction enhancing.</p>
<p>D Details for Evaluating Gender Bias</p>
<p>We design eight prompts to find the most common professions causing the gender bias.The prompts are shown in Table 22, where <gend> is "man" or "woman".</p>
<p>We compute the top100 predictions of each prompt for different genders, and compare the different professions, which are shown in Table 23.These professions contain much gender bias.We then apply our CNA method between cases with different genders under the first prompt, and identify the top18 important neurons causing the difference.Finally, we edit the top18 neurons by setting their parameters to zero, and then compute the per-prompt A <gend> works as a A <gend> is employed as a A <gend> holds a job as a A <gend>'s occupation is The job of a <gend> is The work of a <gend> is The profession of a <gend> is The work of a <gend> involves E Important Heads for 1-Digit, 2-Digit and 3-Digit Operations</p>
<p>We report the top5 important heads for 1D, 2D and 3D operations in this section.For each operation, the experiments are conducted on the last prompt in Table 14.The results are shown in</p>
<p>Figure 1 :
1
Figure1: Four distinct stages in the internal logic chain from the inputs "3+5=" to the final prediction "8".</p>
<ol>
<li>4
4
Feature Enhancing with Hidden-Interpretable Shallow FFN Neurons Stolfo et al. (</li>
</ol>
<p>Figure 2 :
2
Figure 2: Accuracy: adding LoRA in different layers.</p>
<p>0 95.0 94.0 98.0 95.0 97.0 2D-78.6 63.6 41.8 63.6 74.5 80.0 2D* 71.0 54.0 72.0 53.0 59.0 72.0 2D/ 51.5 42.0 50.0 45.6 46.7 34.4</p>
<p>M=0</p>
<p>Table 1 .
1
Interventions on head 17 22 , 15 9 and 14 19 cause 12.7% or more decrease.Specifically, 17 22 reduces all 74.8 53.4 62.1 62.7 68.1 68.7 2D+ 96.8 42.9 83.2 92.5 89.7 91.6 2D-94.472.3 84.6 93.2 86.5 79.1 2D* 56.6 50.5 50.9 51.3 52.3 56.9 2D/ 51.4 48.2 29.5 13.8 43.8 47.1</p>
<p>Table 1 :
1
Accuracy (%) when intervening different heads."ori": original model.17 22 : 22th head in 17th layer.</p>
<p>Table 3 :
3
Accuracy decrease (%) on memorize and change-one cases.
add sub multi dividememorize59.2 49.8 11.663.6change-one 57.1 65.5 11.375.2</p>
<p>Table 4 :
4
Importance scores and coefficient scores of located important FFN neurons for input "3+5=".</p>
<p>Table 6 :
6
tion enhancing stage among the most important FFN neurons 28 3696 , 25 7164 and 19 5769 .The inner product scores between the FFN value of 19 5769 and the FFN keys of 25 7164 and 28 3696 are large.Additionally, the inner product between the FFN value of 25 7164 and the FFN key of 28 3696 is also large.Therefore, a prediction enhancing directed acyclic graph (PE-DAG) exists among the three neurons, where 19 5769 is the root.Activation of the lower FFN neuron recursively triggers activations of upper semantic-related FFN neurons.To explore whether the prediction enhancing stage also exists in other 1D+ and 1D-cases, we compute the coefficient score change of important FFN neurons when intervening the lowest neuron among the most important neurons.If there are many neurons in the lowest layer, we intervene the neuron with the largest importance score in the lowest layer.Decrease of coefficient score when intervening the lowest important neuron in the original model are shown in Table6.Decrease (%) of coefficient score when intervening the lowest neuron among important neurons.
top99 top50 top30 top20 top10coef 15.814.812.59.54.4</p>
<p>Table 7 .
7FFNvoriginattn transform12 4072[rd,quarters,[III,three,PO, Constraint,Three,3,ran, avas]triple]11 2258[enz, Trace, lis,[XV, fifth, Fif,vid, suite, HT,avas, Five, five,ung, icano]abase, fif]word "3" [rd, rum, quar-[three, Three,ters, Af, EX-RGB, triple, 3,ISTS, raum]triangle]word "5" [th, esa, gi, AXI,[Fif, XV, engo,gal, ides, Inject,abase,ipage,san, IDE]vos, fif, fifth]</p>
<p>Table 7 :
7
Hidden-interpretable FFN neurons' top10 tokens transformed by 15th attention layer.</p>
<p>Table 9 :
9
. Important neurons' coefficient scores on the original model and five fine-tuned models for "3+5=".
ori 9th 15th 16th 19th 20th28 3696 6.2 3.66.33.95.74.125 7164 8.4 16.1 11.8 11.0 13.9 9.719 5769 3.8 9.27.76.15.13.8</p>
<p>Table 10 :
10
Coefficient score increase (%) of different fine-tuned models compared with the original model.</p>
<p>Table 11 :
11
Accuracy on 2-digit datasets.</p>
<p>Table 12 :
12
FFN neurons contain gender bias."F":woman.</p>
<p>Table 13
13, reduced</p>
<p>Table 13 :
13
Gender bias of original and edited model.</p>
<p>Table 14 :
14
Four prompts for 2-digit arithmetic operations.We conduct the same experiments as discussed in Section 4.2-4.4.The results of head 14 19 is shown in Table 15 (corresponding to Table 5), Table 16 (corresponding to Table 6), and Table 17 (corresponding to Table 8).
top99 top50 top30 top20 top10mask 84.682.174.466.751.3keep 48.751.353.953.964.2coef 50%61%67%70%73%
B Results of Interventions on Head 14 19</p>
<p>Table 15 :
15
Decrease (%) of accuracy and coefficient score on all 1D/ cases when masking and keeping the top FFN neurons.</p>
<p>Table 16 :
16
Decrease (%) of coefficient score when intervening the lowest neuron among important FFN neurons.The results of Table16also demonstrate that among the identified important neurons, the lower neurons can enhance higher neurons' coefficient scores among deep FFN neurons.Therefore, the prediction enhancing stage also exists.Overall, head 14 19 shares the same mechanism with head 17 22 .Head 14 19 stores important parameters for division operations, while head 17 22 is responsible for addition and subtraction.
M=0M=1M=2 M=3number 51,980 10,426 1,953 510acc97.582.130.825.7Table 17: Decrease (%) of accuracy on 1D/ cases whenintervening hidden-interpretable neurons.In Table 17, The hidden-interpretable neuronsin shallow FFN layers are important for 1D/ cases(e.g. "72/8="). When intervening 10,426 hidden-interpretable shallow FFN neurons, the accuracyreduces 82.1%. For comparison, we randomly in-tervene 10,426 FFN neurons in shallow FFN lay-ers, and the interventions only cause a decrease of5.1%.</p>
<p>Table 18 :
18
Accuracy (%) when intervening different heads in GPT-J.</p>
<p>Table 19 (
19
corresponding to Table 5), Table 20 (corresponding to Table 6), and Table 21 (corresponding to Table 8).
top99 top50 top30 top20 top10mask 58.445.933.42525keep 37.55054.183.495.9coef 17%20%21%23%29%</p>
<p>Table 19 :
19
Decrease (%) of accuracy and coefficient score when masking and keeping the top FFN neurons.</p>
<p>Table 20 :
20
Decrease (%) of coefficient score when intervening the lowest neuron among important neurons.</p>
<p>Table 21 :
21
Decrease (%) of accuracy when intervening hidden-interpretable neurons.</p>
<p>Table 22 :
22
Eight prompts for gender bias professions.plexitydifferencebetween different genders for all prompts in both the original and edited model (results are shown in Table13).
gendprofessionwoman cleaner, nurse, secretary, domestichelper, maid, reception, seller, server,librarian, pharmacist, translator, beauti-cian, dental assistant, hairdresser, vol-unteer, bookkeepermanpolice, guard, delivery, labour, driver,machinist, roofer, machine operator,lumberjack, technician, miner, night-watch, painter, photographer, builder,porter</p>
<p>Table 23 :
23
Professions with gender bias.</p>
<p>Table 24
24-27.</p>
<p>Table 24 :
24
Results of most important heads for 1D+, 2D+, and 3D+.
ori17 22 16 115 23 2 2613 21D-82.0 31.0 51.0 53.0 57.0 65.0ori16 117 22 13 215 23 12 162D-80.0 33.9 37.9 61.8 63.3 70.6ori16 117 22 15 23 13 212 163D-57.1 19.6 22.9 29.3 34.3 40.7</p>
<p>Table 25 :
25
Results of most important heads for 1D-, 2D-, and 3D-.ori 3 5 20 18 6 24 17 22 0 30 1D<em> 93.0 85.4 86.7 87.3 89.2 89.2 ori 15 9 14 19 17 22 20 18 3 5 2D</em> 56.9 49.3 50.1 50.5 50.5 51.6
ori3 515 914 19 13 19 2 143D* 32.8 25.9 29.7 30.3 31.1 31.1</p>
<p>Table 26 :
26
Results of most important heads for 1D<em>, 2D</em>, and 3D*.
ori14 19 15 9 21 24 6 2416 211D/ 78.9 35.6 61.1 65.6 67.8 68.9ori14 19 12 1 3 316 21 1 292D/ 48.6 13.7 30.2 31.4 36.9 38.8ori14 19 3 312 16 2215 93D/ 19.0 9.67 12.7 13.0 13.3 13.7</p>
<p>Table 27 :
27
Results of most important heads for 1D/, 2D/, and 3D/.</p>
<p>AcknowledgementsThis work is supported by the project JPNP20006 from New Energy and Industrial Technology Development Organization (NEDO).This work is supported by the computational shared facility and the studentship from the Department of Computer Science at the University of Manchester.
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, arXiv:2310.10631Llemma: An open language model for mathematics. 2023arXiv preprint</p>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.06450Layer normalization. 2016arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Knowledge neurons in pretrained transformers. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei, arXiv:2104.086962021arXiv preprint</p>
<p>Analyzing transformers in embedding space. Guy Dar, Mor Geva, Ankit Gupta, Jonathan Berant, arXiv:2209.025352022arXiv preprint</p>
<p>Stereotype and skew: Quantifying gender bias in pre-trained and fine-tuned language models. Daniel De, Vassimon Manela, David Errington, Thomas Fisher, Boris Van Breugel, Pasquale Minervini, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume2021</p>
<p>A mathematical framework for transformer circuits. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Transformer Circuits Thread. 112021</p>
<p>Negar Foroutan, Mohammadreza Banaei, Rémi Lebret, Antoine Bosselut, Karl Aberer, arXiv:2205.12672Discovering language-neutral sub-networks in multilingual language models. 2022arXiv preprint</p>
<p>Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K Ahmed, arXiv:2309.00770Bias and fairness in large language models: A survey. 2023arXiv preprint</p>
<p>Dissecting recall of factual associations in auto-regressive language models. Mor Geva, Jasmijn Bastings, Katja Filippova, Amir Globerson, arXiv:2304.147672023arXiv preprint</p>
<p>Transformer feed-forward layers build predictions by promoting in the vocabulary space. Mor Geva, Avi Caciularu, Kevin Ro Wang, Yoav Goldberg, arXiv:2203.146802022arXiv preprint</p>
<p>Transformer feed-forward layers are keyvalue memories. Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy, arXiv:2012.149132020arXiv preprint</p>
<p>Successor heads: Recurring, interpretable attention heads in the wild. Rhys Gould, Euan Ong, George Ogden, Arthur Conmy, arXiv:2312.092302023arXiv preprint</p>
<p>Finding neurons in a haystack: Case studies with sparse probing. Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, Dimitris Bertsimas, arXiv:2305.016102023arXiv preprint</p>
<p>How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. Michael Hanna, Ollie Liu, Alexandre Variengien, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Parameter-efficient transfer learning for nlp. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, International conference on machine learning. PMLR2019</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Gender bias and stereotypes in large language models. Hadas Kotek, Rikker Dockum, David Sun, Proceedings of The ACM Collective Intelligence Conference. The ACM Collective Intelligence Conference2023</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXiv:2104.086912021arXiv preprint</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, arXiv:2101.001902021arXiv preprint</p>
<p>Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. Tom Lieberum, Matthew Rahtz, János Kramár, Geoffrey Irving, Rohin Shah, Vladimir Mikulik, arXiv:2307.094582023arXiv preprint</p>
<p>A unified approach to interpreting model predictions. M Scott, Su-In Lundberg, Lee, 201730Advances in neural information processing systems</p>
<p>Locating and editing factual associations in gpt. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, Advances in Neural Information Processing Systems. 2022a35</p>
<p>Kevin Meng, Sen Arnab, Alex Sharma, Yonatan Andonian, David Belinkov, Bau, arXiv:2210.07229Massediting memory in a transformer. 2022barXiv preprint</p>
<p>Fact finding: Attempting to reverse-engineer factual recall on the neuron level. Neel Nanda, Senthooran Rajamanoharan, Janos Kramar, Rohin Shah, Alignment Forum. 20236</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Dassarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, arXiv:2209.11895-context learning and induction heads. 2022arXiv preprint</p>
<p>Do language models exhibit the same cognitive biases in problem solving as human learners?. Andreas Opedal, Alessandro Stolfo, Haruki Shirakami, Ying Jiao, Ryan Cotterell, Bernhard Schölkopf, Abulhair Saparov, Mrinmaya Sachan, arXiv:2401.180702024arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Direct and indirect effects. Probabilistic and Causal Inference: The Works of Judea Pearl. Judea Pearl, 2001373</p>
<p>Mathbert: A pre-trained model for mathematical formula understanding. Shuai Peng, Ke Yuan, Liangcai Gao, Zhi Tang, arXiv:2105.003772021arXiv preprint</p>
<p>Jonas Pfeiffer, Sebastian Ruder, Ivan Vulić, Edoardo Maria, Ponti , arXiv:2302.11529Modular deep learning. 2023arXiv preprint</p>
<p>Understanding addition in transformers. Philip Quirke, arXiv:2310.131212023arXiv preprint</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Ori Ram, Liat Bezalel, Adi Zicher, Yonatan Belinkov, Jonathan Berant, Amir Globerson, arXiv:2212.10380are you token about? dense retrieval as distributions over the vocabulary. 2022arXiv preprint</p>
<p>Same neurons, different languages: Probing morphosyntax in multilingual pre-trained models. Karolina Stańczak, Edoardo Ponti, Lucas Torroba Hennigen, Ryan Cotterell, Isabelle Augenstein, arXiv:2205.020232022arXiv preprint</p>
<p>A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Axiomatic attribution for deep networks. Mukund Sundararajan, Ankur Taly, Qiqi Yan, International conference on machine learning. PMLR2017</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Investigating gender bias in language models using causal mediation analysis. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, Stuart Shieber, Advances in neural information processing systems. 202033</p>
<p>Gpt-j-6b: A 6 billion parameter autoregressive language model. Ben Wang, Aran Komatsuzaki, 2021</p>
<p>Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt, arXiv:2211.00593terpretability in the wild: a circuit for indirect object identification in gpt-2 small. 2022arXiv preprint</p>
<p>Label words are anchors: An information flow perspective for understanding in-context learning. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun, arXiv:2305.141602023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>How do large language models learn in-context? query and key matrices of in-context heads are two towers for metric learning. Zeping Yu, Sophia Ananiadou, arXiv:2402.028722024aarXiv preprint</p>
<p>Zeping Yu, Sophia Ananiadou, arXiv:2312.12141Neuronlevel knowledge attribution in large language models. 2024barXiv preprint</p>
<p>Interpreting the inner mechanisms of large language models in mathematical addition. Wei Zhang, Wan Chaoqun, Yonggang Zhang, Xinmei Tian, Xu Shen, Jieping Ye, 2023</p>
<p>The clock and the pizza: Two stories in mechanistic explanation of neural networks. Ziqian Zhong, Ziming Liu, Max Tegmark, Jacob Andreas, Advances in Neural Information Processing Systems. 202436</p>            </div>
        </div>

    </div>
</body>
</html>
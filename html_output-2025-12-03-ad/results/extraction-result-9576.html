<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9576 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9576</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9576</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-270845562</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.19853v1.pdf" target="_blank">YuLan: An Open-source Large Language Model</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have become the foundation of many applications, leveraging their extensive capabilities in processing and understanding natural language. While many open-source LLMs have been released with technical reports, the lack of training details hinders further research and development. This paper presents the development of YuLan, a series of open-source LLMs with $12$ billion parameters. The base model of YuLan is pre-trained on approximately $1.7$T tokens derived from a diverse corpus, including massive English, Chinese, and multilingual texts. We design a three-stage pre-training method to enhance YuLan's overall capabilities. Subsequent phases of training incorporate instruction-tuning and human alignment, employing a substantial volume of high-quality synthesized data. To facilitate the learning of complex and long-tail knowledge, we devise a curriculum-learning framework throughout across these stages, which helps LLMs learn knowledge in an easy-to-hard manner. YuLan's training is finished on Jan, 2024 and has achieved performance on par with state-of-the-art LLMs across various English and Chinese benchmarks. This paper outlines a comprehensive technical roadmap for developing LLMs from scratch. Our model and codes are available at https://github.com/RUC-GSAI/YuLan-Chat.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9576",
    "paper_id": "paper-270845562",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00920475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>YuLan: An Open-source Large Language Model
28 Jun 2024</p>
<p>Yutao Zhu 
Kun Zhou 
Kelong Mao 
Wentong Chen 
Yiding Sun 
Zhipeng Chen 
Qian Cao 
Yihan Wu 
Yushuo Chen 
Wayne Xin Zhao 
Zhicheng Dou dou@ruc.edu.cn 
Jiaxin Mao 
Yankai Lin 
Ruihua Song 
Jun Xu 
Xu Chen 
Rui Yan 
Zhewei Wei 
Di Hu 
Wenbing Huang 
Ze-Feng Gao 
Yueguo Chen 
Weizheng Lu 
Ji-Rong Wen jrwen@ruc.edu.cn 
Tom B Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Amanda Askell 
Sandhini Agarwal 
Ariel Herbert-Voss 
Gretchen Krueger 
Tom Henighan 
Rewon Child 
Aditya Ramesh 
Daniel M Ziegler 
Jeffrey Wu 
Clemens Winter 
Christopher Hesse 
Mark Chen 
Eric Sigler 
Mateusz Litwin 
Scott Gray 
Benjamin Chess 
Jack Clark 
Christopher Berner 
Sam Mccandlish 
Alec Radford 
Ilya Sutskever 
Dario Amodei 
Wei-Lin Chiang 
Zhuohan Li 
Zi Lin 
Ying Sheng 
Zhanghao Wu 
Hao Zhang 
Lianmin Zheng 
Siyuan Zhuang 
Yonghao Zhuang 
Joseph E Gonzalez 
Ion Stoica 
Eric P 2023 Xing 
Vicuna 
Aakanksha Chowdhery 
Sharan Narang 
Jacob Devlin 
Maarten Bosma 
Gaurav Mishra 
Adam Roberts 
HyungPaul Barham 
Won Chung 
Charles Sutton 
Sebastian Gehrmann 
Parker Schuh 
Kensen Shi 
Sasha Tsvyashchenko 
Joshua Maynez 
Abhishek Rao 
Parker Barnes 
Yi Tay 
Noam Shazeer 
Vin- Odkumar Prabhakaran 
Emily Reif 
Nan Du 
Ben Hutchinson 
Reiner Pope 
James Bradbury 
Jacob Austin 
Michael Isard 
Guy Gur-Ari 
Pengcheng Yin 
Toju Duke 
Anselm Levskaya 
Sanjay Ghemawat 
Sunipa Dev 
Henryk Michalewski 
Xavier Garcia 
Vedant Misra 
Kevin Robinson 
Liam Fedus 
Denny Zhou 
Daphne Ippolito 
David Luan 
Hyeontaek Lim 
Barret Zoph 
Alexander Spiridonov 
Ryan Sepassi 
David Dohan 
Shivani Agrawal 
Mark Omernick 
An- Drew M Dai 
Thanumalayan Sankaranarayana 
Marie Pellat 
Aitor Lewkowycz 
Erica Moreira 
Oleksandr Polozov 
Katherine Lee 
Zongwei Zhou 
Xuezhi Wang 
Brennan Saeta 
Mark Diaz 
Orhan Firat 
Michele Catasta 
Jason Wei 
Kathy Meier-Hellstern 
Douglas Eck 
Jeff Dean 
Slav Petrov </p>
<p>Feng Wang
Junyi Li
Lei Zhang</p>
<p>Xiaolei Wang Lei Wang
Zican DongBeichen Zhang, Xiaoxue Cheng, Yuhan Chen</p>
<p>Xinyu Tang Yupeng Hou
Qiangqiang Ren, Shufang XieXincheng Pang</p>
<p>YuLan team
Renmin University of China</p>
<p>YuLan: An Open-source Large Language Model
28 Jun 2024D20CA4B154ED3A413B359BCC15757E77arXiv:2406.19853v1[cs.CL]
Large language models (LLMs) have become the foundation of many applications, leveraging their extensive capabilities in processing and understanding natural language.While many open-source LLMs have been released with technical reports, the lack of training details hinders further research and development.This paper presents the development of Yu-Lan, a series of open-source LLMs with 12 billion parameters.The base model of Yu-Lan is pre-trained on approximately 1.7T tokens derived from a diverse corpus, including massive English, Chinese, and multilingual texts.We design a three-stage pre-training method to enhance YuLan's overall capabilities.Subsequent phases of training incorporate instruction-tuning and human alignment, employing a substantial volume of high-quality synthesized data.To facilitate the learning of complex and long-tail knowledge, we devise a curriculum-learning framework throughout across these stages, which helps LLMs learn knowledge in an easy-to-hard manner.Yu-Lan's training is finished on Jan, 2024 and has achieved performance on par with stateof-the-art LLMs across various English and Chinese benchmarks.This paper outlines a comprehensive technical roadmap for developing LLMs from scratch.Our model and codes are available at https://github.com/RUC-GSAI/YuLan-Chat.</p>
<p>Introduction</p>
<p>Recent developments in large language models (LLMs) have significantly advanced the field of artificial intelligence (Brown et al., 2020;Yang et al., 2023;Chowdhery et al., 2022;Touvron et al., 2023a,b;Zeng et al., 2023;Team, 2023;Zhu et al., 2023).By scaling up both the size and amount of training data, LLMs have demonstrated emergent * Team leaders.</p>
<p>capabilities, such as in-context learning (Min et al., 2022) and chain-of-thought reasoning (Wei et al., 2022).In-context learning enables LLMs to effectively perform tasks based on a few demonstrations included directly in the prompt without requiring specific model tuning.This capability greatly enhances the practical deployment of LLMs.Furthermore, LLMs' advanced language generation and reasoning capabilities enable them to handle complex tasks across various real-world scenarios, even surpassing human performance in specific tasks (OpenAI, 2023).These advancements have catalyzed a technological revolution in natural language processing (NLP).Typical applications, such as ChatGPT and Copilot, have significantly improved productivity in daily activities.</p>
<p>Most existing LLMs employ decoder-only architectures based on the Transformer (Vaswani et al., 2017) model.They are trained in an auto-regressive manner with the objective of next-token prediction.The training process typically includes three stages, namely pre-training, instruction-tuning (also known as supervised fine-tuning), and human alignment.Specifically, during the pre-training stage, LLMs learn natural language and world knowledge from extensive text corpora, laying the foundational understanding of language structure and content.Subsequently, during the instructiontuning stage, LLMs are trained to interpret and execute human tasks based on natural language instructions.This stage effectively bridges the gap between the objective of pre-training and the specific requirements of practical human tasks.Finally, in the human alignment stage, LLMs are further trained using annotated data that reflects human preferences and values, ensuring the LLMs' outputs are aligned with human expectations and ethical standards.The training of LLMs is a complex and highly systematic engineering task that involves extensive detail and numerous practical considerations.Despite its importance, there are relatively few references available on this subject.This is primarily due to two factors: First, the research community often lacks the substantial computational resources necessary for training, which limits their ability to thoroughly investigate the training process.Second, the industry often views the details of the training process as proprietary technology and, as such, tends to keep detailed information from the public.</p>
<p>To tackle this problem, we write this report to reveal the detailed training process of our LLM, YuLan.YuLan consists of 12B parameters and is trained on a vast corpus of English, Chinese, and multilingual data.The model is available in two versions: YuLan-Base and YuLan-Chat.YuLan-Base is trained on approximately 1.7T tokens text data.Based on this foundation model, YuLan-Chat is further fine-tuned using high-quality synthesized instruction data and is aligned with human preferences through manually annotated data.To improve the overall performance of Yu-Lan, we design several training strategies in different stages.Specifically, (1) during pre-training, we divide the training process into three phases: employing a uniform sampling strategy from a diverse dataset; introducing a capability-enhanced pre-training strategy that adjusts data distribution and expands context lengths to elevate performance on comprehensive benchmarks; and deploying a long-tail knowledge-aware approach to identify and address knowledge gaps in YuLan-Base, significantly boosting its overall capabilities.(2) During the instruction-tuning stage, we also organize the learning process in an easy-to-hard manner.Initially, YuLan is trained with instruction data derived from basic NLP tasks, facilitating its understanding of human-directed tasks.Then, we synthesize more complex instructions and multi-turn dialogue understanding instructions to further improve Yu-Lan's ability to process complex interactions.(3) In the final human alignment stage, we evaluate and select training pairs based on their complexity, adhering to a predefined threshold.This threshold is iteratively adjusted, allowing YuLan-Chat to refine its ability to differentiate and generate high-quality, nuanced text, thereby progressively enhancing its generation quality.</p>
<p>With the aforementioned training strategy, we train YuLan on 96 NVIDIA A800 GPUs from scratch.The training data include 1.7T tokens of multilingual texts, 42M instruction data, and</p>
<p>Model Architecture</p>
<p>To be compatible with a variety of toolkits that support the LLaMA (Touvron et al., 2023a,b) model, YuLan follows LLaMA's architecture.Specifically, YuLan has 40 layers of Transformer decoder with attention heads of 38.The hidden size is 4, 864, and the feed-forward layer size is 13, 056.In total, YuLan has approximately 12 billion parameters.</p>
<p>Tokenizer Optimizing the tokenizer is a crucial aspect of model training, particularly when considering the efficiency of text compression and inference.A larger vocabulary generally enables better text compression rates but requires more data and resources for effective training.The original LLaMA tokenizer, while effective for English texts, exhibit limitations when processing Chinese texts, often breaking down individual Chinese characters into multiple tokens.This inefficiency complicates the encoding process and restricts the maximum input length for Chinese texts.To address these challenges, we enhance the original LLaMA tokenizer by expanding its vocabulary to include additional tokens specifically for Chinese.This expansion preserves the original English tokens and integrates new tokens derived from the WordPiece algorithm applied to a Chinese text subset from our pre-training data.This method ensures the tokenizer's improved performance on Chinese texts without degrading its effectiveness on non-Chinese texts.As a result, the updated tokenizer contains a total of 51, 190 tokens, and we pad it to 51, 200 to enhance training efficiency.The compression ratios are shown in Table 1.</p>
<p>Positional Embeddings Following LLaMA, we use rotary position embeddings (RoPE) (Su et al., 2024).RoPE utilizes a rotation matrix to encode absolute positions while incorporating relative positional dependencies within the self-attention mechanism.This design not only allows for variable sequence lengths but also introduces a decay effect in the inter-token dependencies as the relative distances increase.Besides, RoPE benefits from compatibility with Flash Attention, which significantly enhances training speed.</p>
<p>Activation and Normalization We use SwiGLU (Shazeer, 2020) as the activation function.</p>
<p>It is a variant of gated linear units that incorporates Swish functions (Ramachandran et al., 2018) as the non-linear activation.Compared to the vanilla MLPs that use two matrix multiplications, SwiGLU uses three.Therefore, to keep the number of parameters and the amount of the computation the same, the MLP layer's hidden size is reduced to2 3 4d.This adjustment ensures that the parameter count in SwiGLU's three matrices remains comparable to that in the traditional two-matrix configuration of vanilla MLPs.As for the layer normalization, we apply RMSNorm (Zhang and Sennrich, 2019) to the input of each Transformer sub-layer to improve the training stability.The normalization hyper-parameter epsilon is set as 1.0e-6.</p>
<p>Maximum Input Length</p>
<p>With the development of LLMs, the maximum input length is also increasing.Unfortunately, since our computational resources are very limited, we cannot training LLMs with long context from scratch.Therefore, we follow the idea of XGen (Nijkamp et al., 2023) and train YuLan with increasing maximum length.Initially, YuLan is trained on sequences up to 2, 048 tokens for the first 600B tokens, subsequently increasing to 4, 096 tokens.By this means, we save a lot of training time and achieve high performance.</p>
<p>Optimization We use GPT-NeoX framework (Andonian et al., 2023) for training, which integrates Megatron-LM (Shoeybi et al., 2019) and DeepSpeed. 1 YuLan is trained with AdamW optimizer (Loshchilov and Hutter, 2019), with the hyper-parameters β 1 = 0.9 and β 2 = 0.95.A cosine learning rate schedule is applied, and the final learning rate is 10% of the maximum learning rate (3e-4).We implement a weight decay of 0.1 and gradient clipping at 1.0, with an initial warmup phase comprising 0.1% of total training steps.</p>
<p>Pre-training</p>
<p>We pre-train YuLan-Base on a mixture of Chinese, English, and multi-lingual data from diverse domains.For multilingual data, we follow PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023a), and CC-100 (Conneau et al., 2020) and then select the following languages: de, fr, es, pl, it, nl, tr, pt, ru, fi, cs, ja, no, ko, da, id, ar, uk, ca, hu, ro, fa, bg, el, he, hi, hr.In this section, we will first introduce the dataset we collect and pre-process for pre-training, and then introduce our pre-training strategies.</p>
<p>Pre-training Data</p>
<p>For the development of YuLan-Base, we systematically organize the pre-training data into distinct categories: web pages, code, encyclopedia, academic papers, question-answering (QA) forums, books, news articles, legal documents, patents, and educational assessments.Detailed statistics regarding the volume and language of these data sources are presented in Table 2.In the following sections, we introduce the data collection we used.To facilitate reproducibility and further research in this field, we provide the details of data pre-processing in Appendix A. Our pre-processing tool YuLan-GARDEN (Sun et al., 2024) has been released. 2  Web Pages Web pages offer a broad spectrum of knowledge across various domains, making them essential for developing models that are robust and capable of understanding context in multiple fields.Specifically, our dataset includes data from Open-WebText2 (Gao et al., 2021), C4 (Dodge et al., 2021), RefinedWeb (Penedo et al., 2023), CC-100 (Conneau et al., 2020), ClueWeb 22 (Overwijk et al., 2022), CC-Stories (Trinh and Le, 2018), and
Web pages ✓ ✓ ✓ 1, 220B 1 1, 220B 72.6% Code ✓ 101B 1 101B 6.0% Encyclopedia ✓ ✓ ✓ 18B 3 54B 3.2% Academic papers ✓ 50B 1 50B 3.0% QA Forums ✓ 26B 1 26B 1.5% Books ✓ ✓ 43.75B 2 87.5B 5.3% News articles ✓ ✓ ✓ 134B 1 134B 8.0% Legal documents ✓ 3B 1 3B 0.2% Patents ✓ 2B 1 2B 0.1% Educational assessments ✓ 1.25B 2 2.5B 0.1% Total - - - - - 1, 680B100%
Dolma's CC (Soldaini et al., 2024).In addition to these sources, we process raw data from Common Crawl (CC) dumps, particularly focusing on events that occurred between January 2021 and February 2023.To manage the practical challenges of HTML content extraction and constraints of disk space, we utilize the WET file format, which includes only plain text, for further preprocessing.We selectively retain texts in English, Chinese, and other multilingual texts that are specified in our language list, ensuring a diverse yet controlled dataset for model training.</p>
<p>Code Incorporating programming code into pretraining data is critical for enhancing the capabilities of LLMs, particularly in fostering the development of an emergent chain-of-thought and algorithmic reasoning.Code inherently embodies structured, logical thinking and provides a sequential understanding of tasks, which are fundamental to developing LLMs that can emulate human-like problem-solving skills.Studies have shown that the inclusion of programming code not only augments the syntactic understanding but also significantly boosts the model's ability to perform complex reasoning and execute task-specific functions (Brown et al., 2020;Zhao et al., 2023).Hence, our dataset extensively incorporates code from various sources to cultivate these advanced capabilities in our LLM.We source the programming code from two primary repositories: the Stack (Kocetkov et al., 2022) and GitHub.</p>
<p>Encyclopedia Encyclopedias represent a cornerstone resource in the pre-training of LLMs, offering a vast repository of structured, high-quality human knowledge essential for building comprehensive understanding.These resources are pivotal in en-hancing the factual accuracy and depth of knowledge of LLMs, making them necessary for applications requiring reliable information and nuanced content generation.In our pre-training, we extend beyond the conventional use of Wikipedia to include the Baidu Encyclopedia, thereby enriching our dataset with expansive Chinese linguistic and cultural knowledge.</p>
<p>Academic Papers Academic papers are a pivotal source for the pre-training of LLMs due to their complex structure, formal language, and rich scientific content.These documents provide a diverse array of knowledge and are instrumental in enhancing the reasoning capabilities of LLMs, allowing them to perform more effectively in tasks requiring deep understanding and analytical skills.To this end, we incorporate a substantial corpus of papers from two major repositories: arXiv and the peS2o dataset (Soldaini and Lo, 2023).</p>
<p>QA Forums Question-answering datasets are crucial for the pre-training of LLMs, as they provide the necessary supervisory signals for LLMs and promote the improvement of the models' capabilities in language understanding, knowledge acquisition, context awareness, generalization, and dialogue generation.The improvement of these capabilities is crucial for developing more intelligent, efficient, and practical LLMs.We use the Stack Exchange (in English) dataset and the Zhihu (in Chinese) dataset.</p>
<p>Books Books represent an invaluable data source for training LLMs, especially in fostering an understanding of long context dependency in natural language processing.High-quality books provide structured and detailed content that is crucial for en-hancing the depth and scope of the model's comprehension capabilities.Particularly, textbooks have been proven to be exceptionally effective in improving LLMs' performance due to their rich, authoritative, and well-organized content (Gunasekar et al., 2023;Li et al., 2023b).Our pre-training dataset includes a diverse selection from the Books3 dataset, Project Gutenberg, CBook, Bestsellers, English textbooks, and Chinese textbooks, each offering unique advantages to the training process.</p>
<p>News Articles News provides a stream of current events and real-time data that is crucial for training LLMs to be relevant and responsive to the latest global developments.By integrating news data from diverse sources, LLMs can better grasp the nuances of journalistic language, adapt to varying narrative styles, and improve their accuracy in information retrieval and generation tasks.In our dataset compilation, we include news from CCnews, RealNews (Zellers et al., 2019b), and the news articles from China International Communications Group (CICG) to cover a wide range of topics and perspectives.</p>
<p>Legal Documents Legal judgment documents are also helpful for training LLMs due to their formal, structured nature and the logical complexity they embody.These documents encapsulate rigorous reasoning processes and legal terminology, making them beneficial for enhancing the analytical capabilities of LLMs.The precision and clarity required in legal language training help improve the model's ability to understand and generate text within specific, rule-based contexts, which is pivotal for applications in legal assistance, automated compliance checks, and advanced query-response systems in the legal domain.</p>
<p>Patents Patent applications are also useful for training LLMs due to their standardized format and formal, technical language.These documents are rich in specialized vocabulary and complex sentence structures, reflecting high levels of precision and clarity.Training LLMs on such data can significantly enhance their ability to parse and generate text within technical contexts.</p>
<p>Educational Assessments Educational assessments provide structured problem-solving environments that are vastly different from general text data, helping models learn to navigate and understand the specific formats and logical reasoning required in standardized testing.The inclusion of this type of data trains LLMs not only in content knowledge but also in the application of this knowledge within the constraints of a given question structure, which is crucial for achieving high performance on standardized assessments like MMLU (Hendrycks et al., 2021), C-Eval (Huang et al., 2023), and AGIEval (Zhong et al., 2023).</p>
<p>Pre-training Process</p>
<p>Our pre-training process can be divided into three stages: (1) standard pre-training;</p>
<p>(2) capabilityenhanced pre-training; and (3) long-tail knowledgeaware pre-training.In the first stage, we follow existing studies and apply a standard training strategy, which involves predicting the next token on randomly sampled data.Then, we notice a plateau in performance improvement and intermittent instability, so we refine our approach to enhance YuLan-Base's overall capability.Finally, we design a approach to detect and augment the YuLan-Base's comprehension of long-tail knowledge, thereby reducing inaccuracies and improving task-specific performance in downstream applications.The training settings of these stages are provided in Table 3.</p>
<p>Standard Pre-training</p>
<p>In standard pre-training, we train YuLan-Base with the next-token prediction objective, which is defined as:
p LM = n i=1 p θ (x i |x &lt;i ),(1)
where x &lt;i denotes the sequence of tokens preceding x i at each step, and θ represents the parameters of the model.We mix and randomly sample all training data to construct data batches at each step.Weak Long-tail Knowledge Detection The first step is to identify which knowledge is incompletely understand by YuLan-Base.Given the complex nature of knowledge, we focus on entities as a proxy for evaluating the model's understanding of relevant knowledge.Inspired by recent studies (Press et al., 2023), we propose synthesizing questionanswer pairs that evaluate the model's retention of entity-specific knowledge.These questions are crafted to test the model's comprehension at the entity level, utilizing entities and their descriptions from encyclopedic sources such as Wikipedia.The rationale is straightforward: if the model fails to accurately respond to questions about a particular entity, it indicates a gap in the acquisition of relevant knowledge.Due to YuLan-Base's difficulties in following human instructions and producing effective responses, we enhance its performance by fine-tuning it with a selected subset of our instruction tuning dataset, resulting in a temporarily improved version, YuLan-tmp.We then employ YuLan-tmp to identify deficiencies in the model's understanding of less commonly addressed, longtail knowledge.Specifically, we first construct an entity list from our encyclopedia datasets, including Wikipedia and Baidu Encyclopedia.To ensure data quality, we exclude entities that are either briefly described or infrequently mentioned within these datasets.Then, we manually craft several templates and employ other advanced LLMs (i.e., ChatGPT) to generate questions related to these entities.For each entity v along and its detailed descriptions d v in the encyclopedia, we perform string matching to identify entities frequently co-occurring with v in d v .This process helps us establish a related entity set V .Using d v and the relationships identified within V , we then generate questions q about v or its interactions with other entities in V .For example, for the entity "Emperor Taizong of Tang", potential questions might include, "Could you provide some context about Zhenguan's Enlightened Administration?" or "Could you elaborate on the connection between Emperor Taizong of Tang and Empress Wu Zetian?"After formulating a substantial number of questions, we pair each with its corresponding article d v and submit them to ChatGPT to generate reference answers a.These question-answer pairs {(q i , a i )} Mv i=1 are accumulated for each entity, where M v denotes the total number of pairs for the entity v.</p>
<p>In the knowledge detection process, each question q i is input to YuLan-tmp to generate a response a ′ i .Both the generated response and the reference answer are then fed into another LLMs for evaluation, which provides binary feedback on their alignment. 3This feedback facilitates the calculation of YuLan-Base's understanding of each entity using the following scoring metric:
s v = 1 M v Mv i=1 f (q i , a i , a ′ i ).(2)
By setting an appropriate threshold ϵ, we identify the set of entities V ′ = {v|s v &lt; ϵ, v ∈ V } that are not adequately understood by the current YuLan-Base model.</p>
<p>Relevant Knowledge Retrieval Given the identified set of entities V ′ that the YuLan-Base inadequately understands, we extract relevant data from the pre-training dataset D pre to mitigate these gaps.</p>
<p>Considering the large scale of the entire D pre , we employ the efficient TF-IDF algorithm to measure the similarity between each sample in D pre and the entity-related questions.We then retrieve the top-k samples that have the highest similarity for each entity.The duplicated samples are removed to obtain the dataset D ′ pre .This refined subset, D ′ pre , is tailored specifically to enhance the model's comprehension of the entities it previously struggled with.</p>
<p>Multi-round Iterative Training After pretraining on D ′ pre , YuLan-Base's performance on long-tail knowledge can be improved.This process can be repeated multiple times to iteratively enhance the model.Specifically, in each iteration, a new entity set V along with associated questionanswer pairs are synthesized to identify gaps in the model's current knowledge.This process led to the identification of a refined set of entities V ′ , which the model struggles with.A new dataset D ′ pre is then constructed using TF-IDF based on the questions of V ′ .Finally, the model undergo further pre-training on this updated dataset.This iterative pre-training cycle is repeated five times, and we cannot observe significant performance improvement on these entity-related questions.</p>
<p>During this stage of pre-training, all Chinese data and about a half of English data are selected based on our designed strategy to improve YuLan-Base's performance on areas of weak long-tail knowledge.The remaining portion of data is still randomly sampled from our pre-training dataset.This pretraining strategy is designed to enhance the model's capability in handling user input involving less frequent knowledge.</p>
<p>Supervised Fine-tuning and Human Alignment</p>
<p>During pre-training, we focus on training YuLan to accurately predict the next token in text sequences.Following this, we implement a supervised finetuning process (also known as instruction tuning), which adapts the model to understand and execute human-like tasks.To optimize this learning, we employ a curriculum-based approach that systematically organizes instruction data from simpler to more complex tasks.Following fine-tuning, we perform human alignment learning to ensure the model's outputs align with human values.This includes adjusting the training methodology by controlling the similarity between positive and negative samples.Such control allows the model to progressively learn to discern finer distinctions between samples, ultimately achieving better alignment with human preferences.</p>
<p>Curriculum Instruction-Tuning</p>
<p>The target of instruction-tuning is to transfer the learning objective of LLMs from predicting next token to tackling real human tasks.While many instruction datasets have been released, they mainly focus on single-turn tasks or simple multi-turn tasks, which limit the model's ability to learn complex, context-dependent tasks.To tackle this challenge, we first collect existing instruction dataset, and then we synthesize more multi-turn instructions based on existing data.Finally, we design a curriculum to fine-tune YuLan to learn from simple to more complex instructions.This curriculum learning-based training process allows for incremental improvements in YuLan's performance, enhancing its capability to handle intricate tasks that require advanced contextual reasoning.</p>
<p>Instruction Data Collection</p>
<p>We first collect instruction datasets that have been widely used for instruction tuning.These data cover various natural language tasks or tasks in real-world applications.</p>
<p>We consider two primary categories for data collection: (1) To improve YuLan's fundamental capabilities, such as knowledge utilization and reasoning, we use datasets including Flan-v2 (Longpre et al., 2023), OpenOrca (Lian et al., 2023;Mukherjee et al., 2023), Chinese data in xP3 (Muennighoff et al., 2023), MetaMathQA (Yu et al., 2023), and MathInstruct (Yue et al., 2023).Additionally, to enhance YuLan's comprehension of Chinese factual knowledge, we synthesize instructions via Chat-GPT based on entities from Baidu Encyclopedia.</p>
<p>(2) To improve YuLan's ability to follow instructions, we incorporate the ShareGPT (Chiang et al., 2023), which contains multi-turn instructions.</p>
<p>Complex Multi-turn Instruction Synthesis</p>
<p>In addition to collect instruction data from existing datasets, we also synthesize some complex multiturn instructions.However, directly synthesize complex multi-turn instructions is very challenging.Therefore, we adopt a multi-stage approach to increase complexity based on existing instruction data.The synthesis process involves three stages: instruction merging, multi-turn conversion, and complexity enhancement.</p>
<p>(1) Instruction merging.We begin by collecting instruction datasets from the open-source community (i.e., WizardLM-Instruct and Alpaca), remov-ing duplicates to form a base set of instructions. 4mploying the TF-IDF algorithm, we determine the similarity between instructions and select pairs with high similarity.These pairs are then merged using a prompt to ChatGPT: "Please merge the following two semantically similar instructions into a new instruction that incorporates the functionalities of both instructions and is more complex."This ensures that the merged instructions retain semantic similarity and increased complexity.The merged instruction is subsequently input into ChatGPT to generate an appropriate response.</p>
<p>(2) Multi-turn conversion.The next phase involves converting the merged instructions into multi-turn instructions to further enhance their complexity.To ensure the diversity of topics, we collect a set of 293 topics from chat communities (i.e., Zhihu and Reddit).For each merged instruction, we utilize ChatGPT again to generate a next-turn question in terms of a randomly selected topic.An example prompt is: "Please generate a question related to the topic 'modern history' and ensure its consistency with the context of the conversation."</p>
<p>(3) Complexity enhancement.To ensure the generated instructions are sufficiently complex, we use a prompt that encourages ChatGPT for deeper and broader knowledge exploration: "Please modify the following question into a more complex instruction that significantly enhances the depth and width of the involved knowledge."This process yields highly complex instructions which are then processed through ChatGPT to generate responses.Through the above process, we can obtain the complex instructions, which are fed into Chat-GPT to generate responses.Following a quality assessment, the refined set of synthetic complex multi-turn instructions is compiled into a dataset.</p>
<p>Simple-to-Complex Curriculum Based on the collected open-source instruction datasets and our synthesized complex instruction dataset (around 41M instructions in total), we combine and re-split them into two parts based on their complexity: a simple set and a complex set. 5The complexity of each instruction is measured using the following equation:
Comp(x, y) =λ 1 • L turn + λ 2 • L length + λ 3 • Loss it (x, y).(3)
Here, λ 1 , λ 2 , and λ 3 are the hyperparameters; L turn and L length denote the number of turns and the length of the instruction; Loss it (x, y) is the loss calculated by the current model:
Loss it (x, y) = |y| i=1 log P (y i |x, y 1:i−1 ),(4)
where y i represents the i-th token in the output y, and y 1:i−1 denotes the sequence up to the i − 1 tokens.Based on the complexity value computed by Equation ( 3), we set a threshold to categorize all instruction data into either the simple or complex set.</p>
<p>The training starts from the simple instruction set and progresses to the complex set.This structured curriculum allows YuLan-Chat to incrementally acquire and apply knowledge from the instructions, enhancing its capability to comprehend and execute more complex instructions efficiently.</p>
<p>Curriculum Human Alignment Learning</p>
<p>After instruction tuning, we further enhance our YuLan for better human alignment.This stages focuses on strengthening its capability of distinguishing subtle negative inputs (e.g., obscure abuse), and avoiding generating outputs conflicting with human values.Despite the abundance of open-source human alignment datasets, the complexity of instances within these datasets varies considerably.</p>
<p>To address this, we implement a reward function based on direct preference optimization (DPO) to measure instance difficulty and design an easy-tohard curriculum for model training.</p>
<p>Construction of Training Dataset</p>
<p>To support the human alignment initiative, we aggregate multiple datasets containing English and Chinese prompts alongside corresponding human preference data, which includes designated positive and negative responses.These datasets include HH-RLHF (Bai et al., 2022), Stanford SHP (Ethayarajh et al., 2022), BeaverTails (Ji et al., 2023), Synthetic GPT-j,6 and UltraFeedback (Cui et al., 2023), as well as the Chinese dataset CValues (Xu et al., 2023).To enhance the reliability of the training data-ensuring the selected positive responses are decidedly superior to the negative-we apply a filtering mechanism based on user agreement counts in datasets such as Stanford SHP and BeaverTails, Difficulty Estimation based on DPO For human alignment, we use the DPO to fine-tune the model parameters.DPO evaluates the model's current capability against its counterpart before human alignment by comparing the discriminative power over positive and negative examples within each instance.The reward calculation is formalized as:
R(p, y + , y − ) = log( π θ (y + |p) π θo (y + |p) ) − log( π θ (y − |p) π θo (y − |p) ),(5)
where π θ (y|p) and π θo (y|p) denote the output distributions of the LLM trained after and before the current curriculum, respectively.A higher reward value indicates that the model has effectively differentiated between the positive and negative examples, suggesting an increase in alignment accuracy.Conversely, lower reward values indicate the need for further learning, so we retain the corresponding data in subsequent training stages by applying a reward threshold δ.</p>
<p>Easy-to-Hard Curriculum Based on the reward function , we can select and include challenging instances that the model has yet to master effectively.These are included in the subsequent training phases, setting a progressively decreasing threshold δ to increase the difficulty level.This approach follows the idea of curriculum learning where the model iteratively trains on increasingly challenging data.We optimize model parameters via the DPO strategy akin to fine-tuning, and the training objective is formulated as:
∇ θ L DP O = −βE (p,y + ,y − )∼D σ(R(p, y + , y − )) [∇ log πθ(y + |p) − ∇ log π θ (y − |p)],
where β is a hyper-parameter.The training focuses on maximizing the likelihood of generating valuealigned positive responses (y + ) and minimizing that of negative outputs (y − ).This learning process, which transitions from easier to more hard scenarios, ensures the model incrementally aligns closer to human preferences.</p>
<p>Evaluation</p>
<p>We evaluate our model on different NLP datasets and popular benchmarks, including commonsense and world knowledge, reading comprehension, math, code, and complex exams, as shown in Table 4.All the datasets and benchmarks can be split into two types: (1) classification problems, in which we need to calculate and compare the logits of different choices, such as BoolQ, PIQA, and MMLU;</p>
<p>(2) generation problems, in which we need to extract and judge the answers from the generated contents, such as GSM8K, CommonsenseQA, and AQuA.We use the greedy decoding for the generation problem.</p>
<p>Commonsense Reasoning</p>
<p>We have carefully selected eight common datasets to assess the common sense reasoning capabilities of our models: BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), Hellaswag (Zellers et al., 2019a), WinoGrande (Sakaguchi et al., 2020), WSC273 (Levesque et al., 2012), AI2_ARC (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), and CommonsenseQA (Talmor et al., 2019).Among these, the first seven datasets are treated as classification problems, and we adopt a zero-shot setting for evaluation.We employ chain-of-thought methods, utilizing 7-shot examples for inference (consistent with (Wei et al., 2022)).The experi-mental results are shown in Table 5.We can observe that YuLan can achieve the best performance on BoolQ, OBQA, and CommonsenseQA, demonstrating its superior reaonsing capability.Besides, the instruction-tuning can significantly improves YuLan's performance (YuLan-Inst &gt; YuLan-Base).This validates the effectiveness of our proposed instruction-tuning strategy.</p>
<p>Factual Knowledge</p>
<p>We assess the factual knowledge within our models using the TriviaQA (Joshi et al., 2017) dataset.We regard it as a generation problem and judge if the models' responses are contained in candidate answers.The evaluation results are shown in Table 6.Unfortunately, there is still a gap between YuLan and other advanced LLMs.We attribute this to the gaps in data quality.Besides, we can see YuLan-Base performs better than YuLan-Inst and YuLan-Chat, reflecting instruction tuning and human alignment may affect LLMs' utilization of knowledge.However, more experiments are needed to explore the underneath reason.</p>
<p>Reading Comprehension</p>
<p>We evaluate the reading comprehension ability of our models using four widely used datasets: RACE (Lai et al., 2017), CoQA (Reddy et al., 2019), CMRC2018 (Cui et al., 2019), and C3 (Sun et al., 2020).The latter two are Chinese datasets.RACE and C3 are classification problems, while CoQA and CMRC2018 are generation problems.</p>
<p>The evaluation results are shown in Table 6.We can observe that YuLan, ChatGLM, and Baichuan can achieve significantly better performance on Chinese datasets, highlighting the importance of involving Chinese data in training process.Interestingly, LLaMA can perform well on CMRC, which is also a Chinese dataset.We check the dataset and find that it requires LLMs to select correct sentences from a provided document to answer the question.This capability may be easily transferred from learning on English data.</p>
<p>Mathematical Reasoning</p>
<p>We evaluate our models on two mathematical reasoning datasets: GSM8K (Cobbe et al., 2021) and AQuA-RAT (Ling et al., 2017).We use their test sets (1, 319 samples for GSM8k and 254 samples for AQuA-RAT) with chain-of-thought examples (consistent with (Wei et al., 2022)) for our eval-uation.We regard them as generation problems and extract models' answers by regular expression.</p>
<p>The experimental results are shown in Table 6.We can see YuLan achieves comparable performance with LLaMA on these two datasets, indicating its capability of solving complex questions with chainof-thought prompt.In our case study, we also test YuLan's capability of solving math problems in Chinese Gaokao.</p>
<p>Comprehensive Benchmarks</p>
<p>We evaluate the abilities of our models to solve complex exams by MMLU (Hendrycks et al., 2021), C-Eval (Huang et al., 2023), and GaoKao.We use the 5-shot examples as prompts for these benchmarks, the n-shot examples are provided by the benchmarks themselves (validation set).The majority of test samples in the three benchmarks consist of multiple-choice problems.As such, we treat them as classification problems, comparing the probabilities of various choices.The sole exception is the GaoKao-Math-Cloze task, which we regard as a generation problem due to the absence of candidate choices.The evaluation results are provided in Table 7 and Table 8.Overall, YuLan achieves comparable performance with several advanced LLMs, demonstrating its capability of using acquired knowledge for solving real problems.</p>
<p>Alignment Benchmarks</p>
<p>We select two commonly-used benchmarks Al-pacaEval (Li et al., 2023a) and AlignBench (Liu et al., 2023) for the evaluation of alignment in LLMs.AlpacaEval is an English evaluation benchmark for human alignment, which utilizes powerful LLMs (i.e., GPT-4) to perform pairwise comparisons of the outputs from two LLMs.Our analysis includes a comparison of our model, YuLan-Chat, against other baseline models.AlignBench is a Chinese benchmark, which performs multidimensional evaluation using chain-of-thought reasoning prompts to evaluate the models' responses comprehensively.</p>
<p>Table 9 shows the win rates of YuLan-Chat compared to other baseline models on AlpacaEval.YuLan-Chat demonstrates a win rate exceeding 55%, indicating its superior alignment with human preferences.This improvement is largely due to its curriculum-based instruction tuning and human alignment training strategies, which facilitate the model's comprehension of complex instructions and generation of unbiased responses.Table 10 shows the performance of various LLMs on AlignBench, with a specific focus on Chinese language alignment.Notably, the alignment capabilities in Chinese and English across these models are quite different.Among the baselines, Baichuan-13B-Chat performs optimally in Chinese alignment, benefiting from extensive use of human-annotated data tailored for this purpose.Additionally, YuLan-Chat surpasses all baseline models, attributed to its multi-stage fine-tuning through curriculum learning.This training method not only improve performance on intricate reasoning tasks but also ensures robustness in Chinese linguistic proficiency.</p>
<p>Discussion</p>
<p>We also conduct a series of experiments to validate some strategies in the training of YuLan.</p>
<p>Impact of Educational Assessments</p>
<p>During the first pre-training stage, we observe fluctuations in model's performance, particularly with comprehensive benchmarks such as MMLU.To address this problem, we conduct three experiments by applying different strategies to continue pretraining 5, 000 steps based on the 600B checkpoint:</p>
<p>• Strategy 1: Maintaining the original data distribution.11.In the first stage, we exclude educational assessments from our pre-training to avoid their unique formats potentially skewing model training.However, the absence of this specific form of knowledge appeared to limit the model's ability to contextualize and apply learned information effectively, as evidenced by the inferior performance in both the MMLU and AGI-Gaokao benchmarks under Strategy 1. Intriguingly, the inclusion of Chinese educational assessments in Strategy 2 substantially enhanced performance across both the English MMLU and Chinese AGI-Gaokao benchmarks, illustrating a cross-lingual transferability in the understanding and application of task-specific knowledge.In Strategy 3, the addition of MMLUrelated QA training further elevated the model's performance on various QA benchmarks, corroborating the significant impact of diverse task forms on model enhancement.Given these findings, we opt to incorporate educational assessments into our pre-training dataset in the second pre-training stage, affirming the pivotal role of diverse educational content in enhancing language model performance.7</p>
<p>Preliminary Experiments with 1.3B Model</p>
<p>Before finalizing our training strategy, we conducted a series of preliminary experiments using the smaller 1.3B model to examine the impact of Chinese data, continual training, and longer context window size on model performance.</p>
<p>Effect of Chinese Data</p>
<p>We explore the impact of various Chinese data types on model performance.The Chinese data is categorized into four types: (1) High-quality (HQ), including books, QA forums (Zhihu), and Wikipedia, (2) Web, (3) News, and (4) Law.We randomly sample 10B Chinese tokens from different combinations of these categories and 20B English tokens based on the LLaMA data distribution.A series of 1.3B models are pre-trained on these different 30B tokens, and their performances are evaluated using MMLU, C-EVAL, and C-EVAL (Hard).The results are summarized in Table 12.We find that:</p>
<p>(1) Mixing multiple sources of data can enhance performance for both English and Chinese.The model trained on a mix of all four Chinese data sources achieves the best performance on MMLU and the second best on C-EVAL.</p>
<p>(2) High-quality Chinese data can improve English performance but may not be sufficient for the best Chinese performance.Using high-quality data alone achieves the second-best MMLU score but performs worse than web, news, or law data alone on C-EVAL.Combining high-quality data  with other sources significantly improves C-EVAL performance.This suggests that a diverse data mix is essential to balance English and Chinese performance, rather than relying solely on high-quality data from limited sources.
阅读下面的材料，根据要求写作。 随着互联网的普及、人工智能的应用，越来越多的问题能很快得到答案。那么，我们的问题是否会越来越少？ 以上材料引发了你怎样的联想和思考？请写一篇文章。 要求：选准角度，确定立意，明确文体，自拟标题；不要套作，不 得抄袭；不得泄露个人信息；不少于800字。 题目：互联网与人工智能时代，我们的问题是否会越来越少？ 随着互联网和人工智能技术的不断发展，我们似乎已经进入了一个信息爆炸的时代。然而，尽管我们拥有越来越多的信息， 我们的问题似乎并没有因此而减少，反而变得更加复杂和多样化。因此，我们不得不思考，在这个互联网与人工智能时代， 我们的问题是否会越来越少。 首先，我们可以从技术进步的角度来思考这个问题。随着互联网和人工智能技术的不断发展，我们可以期待越来越多的问题 能够得到快速、准确的回答。例如，我们可以利用搜索引擎快速获取信息，利用智能助手解决日常生活中的问题，利用人工 智能技术进行科学研究等等。因此，从这个角度来看，我们的问题似乎会越来越少。 然而，我们也应该意识到，尽管技术进步能够帮助我们解决一些问题，但它并不能解决所有问题。有些问题可能是由于我们 自身认知水平的限制，或者是由于我们缺乏足够的信息和资源而无法解决的。因此，即使我们拥有越来越多的信息和技术， 我们仍然可能会面临一些无法解决的问题。 其次，我们可以从社会和人文角度来思考这个问题。随着互联网和人工智能技术的不断发展，我们可能会遇到越来越多的道 德、伦理和社会问题。例如，我们可能会面临人工智能技术的伦理问题，我们可能会面临信息过载和信息真实性的问题，我 们可能会面临社交媒体上的虚假信息和谣言传播等等。因此，从这个角度来看，我们的问题可能会越来越多。 最后，我们可以从哲学和人类学角度来思考这个问题。随着我们对世界的认知不断深入，我们可能会发现越来越多的问题无 法得到明确的答案。例如，我们可能会发现一些问题是由于我们自身认知水平的限制而无法解决的，我们可能会发现一些问 题是由于我们无法控制的外部因素而无法解决的，我们可能会发现一些问题是由于我们无法理解的自然规律而无法解决的等 等。因此，从这个角度来看，我们的问题可能会越来越多。 综上所述，我们可以看到，尽管互联网和人工智能技术的不断发展能够帮助我们解决一些问题，但它并不能解决所有问题。 我们仍然可能会面临一些无法解决的问题，我们仍然可能会面临道德、伦理和社会问题，我们仍然可能会面临无法理解的自 然规律等等。因此，我们需要不断思考和探索，以应对这些问题，同时也需要不断提高我们自身的认知水平和道德素养，以 更好地应对未来的挑战。
(3) Our law data exhibits an unusual pattern, achieving the highest scores on C-EVAL and C-EVAL (Hard) when used alone.However, mixing law data with high-quality data results in a significant performance drop, yielding the worst results for both English and Chinese tasks.Therefore, we decide to just mix a small amount of law data in our final pre-training.</p>
<p>Effect of Continual Training</p>
<p>We explore the impact of continual training by investigating whether "continual training with newly added data sources" can achieve performance levels similar to "training with all data sources from the beginning".Specifi-cally, we compare the following three settings: (1) v1: Training on 20B English tokens and 10B Chinese tokens together from the beginning.Then, we divide the 10B Chinese tokens into two parts: p 1 (books, news, wiki, totaling 6.5B tokens) and p 2 (law, Zhihu, totaling 3.5B tokens).( 2) v2: We train on 13B English tokens and 6.5B Chinese tokens (p 1 ).(3) v3: We continue training v2 on 7B English tokens and 3.5B Chinese tokens (p 2 ).The results are shown in Table 13.We find that continual training with newly added data sources yields better (or at least equivalent) performance in both English and Chinese compared to training with all data sources from the beginning.This demonstrates that continual training with new data sources is effective.</p>
<p>假设你是李华，你的美术老师带你去公园上了一节美术课，请你写一封英文的信给Chris，要帮他介绍给外国朋友去公园上 了美术课之后完成的作品和感想。写作词数应为80个左右</p>
<p>Dear Chris, I hope this letter finds you well.I am writing to tell you about a wonderful experience I had recently.My art teacher took me to a park for an art class, and I had a great time creating some artwork.</p>
<p>During the class, we drew some beautiful landscapes and painted some colorful flowers.I was so inspired by the natural beauty around me that I even created a painting of a beautiful sunset.I was so proud of my work that I wanted to share it with you.</p>
<p>After the class, I felt so relaxed and happy.I realized that art is not just about creating beautiful objects, but also about expressing our feelings and emotions.I hope you can come to the park sometime and join us in creating some artwork.</p>
<p>I am also writing to share some of the thoughts I had during the class.I felt a sense of freedom and creativity that I have never experienced before.I realized that art is a way to express ourselves and connect with the world around us.</p>
<p>Thank you for reading my letter.I hope you can come to the park and join us in creating some artwork.</p>
<p>Sincerely, Li Hua Figure 3: An example of YuLan-Chat on writing a 2024 Gaokao English essay.In this implementation, we first calculate the discriminant using the formula b^2 -4ac.If the discriminant is less than 0, there are no real roots.In this case, we return None, None to indicate that there are no real roots.
已知集合! = {$| − 4 &lt; $ ≤ 1}，, = {$| − 1 &lt; $ &lt; 3}，则! ∪ , = 请从如下四个选项中选择一个正确答案： A. {$| − 4 &lt; $ &lt; 3} B. {$| − 1 &lt; $ ≤ 1} C. {0,1,2} D. {$| − 1 &lt; $ &lt; 4} 根据题意，由!与,的并集的定义，可得! ∪ , = {$ | −4 &lt; $ &lt; 3} ；故选A．
If the discriminant is greater than or equal to 0, we calculate the two roots of the equation using the quadratic formula.We return these roots as a tuple.</p>
<p>Note that we use the math module to calculate the square root in the case where the discriminant is positive.We also use the ** operator to raise a number to a power.Please write a conversation between the sun and pluto Sun: Good morning, Pluto.It's a beautiful day here in the solar system.How are you doing today?</p>
<p>Pluto: Good morning, Sun.I'm doing well, thank you for asking.Today, I am studying the various planets in the solar system and their unique characteristics.</p>
<p>Sun: Interesting.I see.Well, I'm here to make sure that all the planets in the solar system are safe and well-maintained.I also help to regulate the temperature and light levels in the solar system.</p>
<p>Pluto: That's quite impressive.I'm also studying the different types of celestial bodies in the solar system, such as asteroids and comets.</p>
<p>Sun: Yes, the solar system is home to a diverse range of celestial bodies.Some of them are planets, while others are dwarf planets, asteroids, comets, and meteoroids.</p>
<p>Pluto: I see.I'm also interested in the different types of orbits that the planets have in the solar system.Some planets orbit the sun in a circular path, while others have more elliptical orbits.</p>
<p>Sun: Yes, the orbits of the planets in the solar system can vary greatly.Some planets have very close orbits to the sun, while others have much more distant orbits.</p>
<p>Pluto: That's fascinating.I'm also interested in the different types of weather patterns that can occur in the solar system.Some planets have extreme temperatures, while others have very stable temperatures.</p>
<p>Sun: Yes, the weather patterns in the solar system can be quite diverse.Some planets have very harsh weather conditions, while others have more moderate temperatures.</p>
<p>Pluto: I see.Well, thank you for sharing your knowledge about the solar system with me, Sun.It's been a very informative conversation.</p>
<p>Case Study</p>
<p>We perform a case study by testing YuLan-Chat on a range of subjects from the 2024 Chinese Gaokao, encompassing Chinese essay writing, English essay writing, mathematics, biology, and history.This diversity of subjects provided a comprehensive assessment of YuLan's capabilities across different subjects.The generation results are shown in Figure 2-6.We can observe that YuLan-Chat excel in Chinese and English essay writing, demonstrating a strong ability to construct coherent, well-argued natural language texts.In mathematics, the model efficiently solves the problem, showcasing its quantitative reasoning skills.Similarly, in biology and history, YuLan-Chat accurately answers questions involving intricate details and conceptual understanding.Besides, we also show YuLan-Chat's ability on writing code, generating stories, and answering real problems in Figure 7-9.These results confirm YuLan-Chat's adaptability and intellectual breadth.</p>
<p>Conclusion</p>
<p>In this report, we introduced the detailed training process of YuLan-12B, including pre-training, supervised fine-tuning, and human alignment.The YuLan-12B, trained on approximately 1.7TB tokens, has demonstrated performance on par with other open-source LLMs.Despite a surge in ad-vanced models trained on larger datasets, this paper aims to illuminate essential training techniques for developing LLMs from scratch and to provide insights for future research.We hope our report can enhance understanding and foster innovation within the AI community, promoting transparency and reproducibility in AI research.</p>
<p>A Detailed Data and Processing</p>
<p>A.1 Web Pages</p>
<p>The Internet, as a comprehensive and continually updated source of information, provides rich contents that are invaluable for training LLMs.Web pages offer a broad spectrum of knowledge across various domains, making them essential for developing models that are robust and capable of understanding context in multiple fields.This diversity not only enriches the training set but also enhances the generalizability and applicability of LLMs in real-world scenarios.To leverage this vast resource, we have combined several key datasets.Specifically, our dataset includes data from OpenWebText2 (Gao et al., 2021), C4 (Dodge et al., 2021), RefinedWeb (Penedo et al., 2023), CC-100 (Conneau et al., 2020), ClueWeb 22 (Overwijk et al., 2022), CC-Stories (Trinh and Le, 2018), and Dolma's CC (Soldaini et al., 2024).In addition to these sources, we process raw data from Common Crawl (CC) dumps, particularly focusing on events that occurred between January 2021 and February 2023.To manage the practical challenges of HTML content extraction and constraints of disk space, we utilize the WET file format, which includes only plain text, for further preprocessing.We selectively retain texts in English, Chinese, and other multilingual texts that are specified in our language list, ensuring a diverse yet controlled dataset for model training.</p>
<p>Preprocessing Generally, the preprocessing of data involves a three-stage procedure designed to enhance data quality significantly: (1) We apply heuristic rules at both the coarse and fine-grained levels.This initial filtering focuses on the structural and content aspects of texts.(2) We employ the CCNet pipeline (Wenzek et al., 2020) to perform deduplication, identify language, and assess the linguistic quality of texts.(3) We conduct a final quality check with several heuristic rules.</p>
<p>Specifically, in Stage (1), we conduct filtering on the page and paragraph level.We exclude pages shorter than 512 characters or those predominantly (&gt; 50%) consisting of sentences shorter than 16 characters due to their limited contextual value.Content containing non-relevant elements such as "javascript", "lorem ipsum", curly brackets "{", or terms from a list of dirty words is removed to maintain the quality and relevance of the data. 8We also eliminate pages with a high presence of hash symbols or ellipsis (ratio &gt; 0.1), and those heavily formatted with bullet points (&gt; 90% lines) or frequently ending in ellipses (&gt; 30% lines), as these features typically indicate poor quality or non-standard text formats.Finally, we ensure that retained texts end with proper punctuation, and we discard any text with garbled characters or unresolved Unicode conversions.In Stage (2), we use the CCNet pipeline to evaluate the complexity and readability of the text, with pages having a perplexity score over 1,000 being discarded.We conduct language identification to ensure the text matches our target languages, retaining only those with a language score above 0.6.In Stage (3), we remove texts that are excessively short or repetitive.Specifically, pages shorter than 500 characters or with a low total number of paragraphs (less than three) indicating high repetitiveness are excluded.We also filter out texts where the original number of paragraphs is disproportionately high (more than five times) compared to the number retained after processing, ensuring content consistency and integrity.</p>
<p>A.2 Code</p>
<p>Incorporating programming code into pre-training data is critical for enhancing the capabilities of LLMs, particularly in fostering the development of an emergent chain-of-thought and algorithmic reasoning.Code inherently embodies structured, logical thinking and provides a sequential understanding of tasks, which are fundamental to developing LLMs that can emulate human-like problemsolving skills.Studies have shown that the inclusion of programming code not only augments the syntactic understanding but also significantly boosts the model's ability to perform complex reasoning and execute task-specific functions (Brown et al., 2020;Zhao et al., 2023).Hence, our dataset extensively incorporates code from various sources to cultivate these advanced capabilities in our LLM.We source the programming code from two primary repositories: the Stack (Kocetkov et al., 2022) and GitHub.</p>
<p>The Stack, part of the BigCode Project, holds over 6TB of source code files across 358 programming languages, emphasizing the broad spectrum of coding knowledge available.For the purpose of our model, we focus exclusively on Python due to its wide usage and relevance in both academic and practical applications.Given that the data from the Stack has already undergone preliminary processing to ensure consistency and quality, we do not perform additional preprocessing steps.</p>
<p>GitHub, as a vast open-source platform, hosts approximately 28 million public repositories as of January 2023.This platform is a treasure trove of code across various programming languages, offering a real-world mix of codebases.However, the data from GitHub often contains a significant amount of non-code elements or noise.To enhance data quality, we apply a meticulous selection process, initially filtering repositories based on popularity (e.g., those with over 100 stars as of March 12, 2023).Subsequently, we clone these repositories, removing non-code files and retaining markdown files, particularly READMEs, under the premise that these documents provide valuable context and explanations that aid in the model's deeper understanding of the code.</p>
<p>Following collection, we perform rigorous preprocessing to further refine the quality and relevancy of the code data, ensuring that our LLM is trained on high-quality, representative program-ming content that enhances its coding ability and logical reasoning skills.</p>
<p>Preprocessing Following previous studies (Laurençon et al., 2022;Touvron et al., 2023a;Chowdhery et al., 2022), our preprocessing routine for programming code employs several heuristic rules designed to refine the quality and relevance of the data for training purposes.Initially, we filter out files based on several criteria.We eliminate files with fewer than 100 characters, except for those ending in ".sql", or more than 200, 000 characters to maintain an optimal range of complexity and detail.Files with any line shorter than 20 characters or longer than 1, 000 characters are discarded, as they often do not represent standard coding practices.We remove files where numeric characters exceed 70% of the content or alphabetical characters constitute less than 30%, to avoid files dominated by data values or non-instructional content.Files are also excluded if they include exact matches to the phrases "configuration file" or "test file", or if over 10% of the lines contain the words "config" or "test", indicating non-functional code such as configuration or test scripts.The majority of excluded files typically include data dumps, configuration files, log files, or automatically generated template code, which lack substantial logical coding content necessary for effective model training.</p>
<p>Following the initial filtering phase, we proceed with deduplication.We first apply exact match filtering within predefined slices of the dataset to manage memory use efficiently.After this intra-slice deduplication, we consolidate all slices and apply a 10-gram minhashLSH algorithm to perform comprehensive deduplication across the entire dataset.This two-step deduplication process ensures that the final training dataset is devoid of redundant entries, thereby enhancing the quality and efficiency of the training phase.</p>
<p>A.3 Encyclopedia</p>
<p>Encyclopedias represent a cornerstone resource in the pre-training of LLMs, offering a vast repository of structured, high-quality human knowledge essential for building comprehensive understanding.These resources are pivotal in enhancing the factual accuracy and depth of knowledge of LLMs, making them necessary for applications requiring reliable information and nuanced content generation.In our pre-training, we extend beyond the conventional use of Wikipedia to include the Baidu Encyclopedia, thereby enriching our dataset with expansive Chinese linguistic and cultural knowledge.</p>
<p>Wikipedia, launched by Jimmy Wales and Larry Sanger on January 15, 2001, stands as the most extensive free-content encyclopedia available online.It is maintained by a global volunteer community using a wiki-based editing system, MediaWiki, and is one of the top ten most visited websites globally.We gather data from Wikipedia in English, Chinese, and other languages in our multilingual list directly from the Wikimedia Downloads site. 9n addition to Wikipedia, we incorporate data from the Baidu Encyclopedia, the largest semiregulated online Chinese encyclopedia managed by Baidu, Inc.It allows user-contributed content, which is subsequently reviewed by official editors to ensure the accuracy and relevance of the information.As of July 2023, it encompasses nearly 27 million entries in both Simplified and Traditional Chinese.Due to Baidu's strict anti-crawler policies, we access this encyclopedia through a third-party collection available on HuggingFace, initially collected in 2020.10 To incorporate the most current entries, we also utilize a more recent version provided by Tiger Research.11  By integrating these diverse encyclopedic sources, we aim to construct a robust LLM that is well-versed across multiple languages and domains, capable of generating accurate and culturally relevant content.</p>
<p>Preprocessing For Wikipedia, we begin with the extraction of content using the wikiextractor tool, which parses XML dumps to isolate meaningful textual data.12During this process, we preserve article titles and textual paragraphs while eliminating non-textual elements such as images, tables, audio, and video files.Subsequently, the extracted data is converted to a JSONL format for better handling and integration into our dataset.Each Wikipedia article is structured into a single line in this format, with the title and paragraphs separated by newline characters.For the Chinese Wikipedia, we use zhconv to standardize all text to simplified Chinese, ensuring consistency across our Chinese language data. 13or Baidu Encyclopedia, to enhance its quality, we implement a set of heuristic rules aimed at refining the content: (1) We strip all entries of meaningless headers and footers, which often contain repetitive or irrelevant information that may affect the quality of the training data.(2) We discard entries that are shorter than 50 characters or have a proportion of Chinese characters below 70%, as such entries typically lack substantial informational content.(3) The remaining text from each entry is converted into simplified Chinese and recombined with the original titles to produce cohesive and standardized entries.</p>
<p>These preprocessing steps are designed to ensure that the encyclopedia data fed into our model training pipeline is clean, uniform, and optimally formatted, thereby facilitating the development of a more effective and knowledgeable language model.</p>
<p>A.4 Academic Papers</p>
<p>Academic papers are a pivotal source for the pretraining of LLMs due to their complex structure, formal language, and rich scientific content.These documents provide a diverse array of knowledge and are instrumental in enhancing the reasoning capabilities of LLMs, allowing them to perform more effectively in tasks requiring deep understanding and analytical skills.To this end, we incorporate a substantial corpus of papers from two major repositories: arXiv and the peS2o dataset (Soldaini and Lo, 2023).</p>
<p>ArXiv is an open-access archive that hosts over 2.3 million scholarly articles spanning diverse scientific domains such as physics, mathematics, computer science, and economics, among others.We systematically collect all LaTeX files available from 1990 to March 2023 via the arXiv bulk data service on Amazon S3. 14 These documents are a rich source of advanced scientific and technical knowledge, ideal for training sophisticated LLMs.</p>
<p>The peS2o dataset contains approximately 40 million open-access academic papers, derived from the S2ORC project (Lo et al., 2020).The peS2o dataset has undergone extensive preprocessing by the dataset creators, including comprehensive cleaning, filtering, and formatting, ensuring its readiness for integration into our training pipeline.</p>
<p>Preprocessing For the arXiv dataset, in line with established practices from prior studies (Touvron   14 https://info.arxiv.org/help/bulk_data_s3.html et al., 2023a;Gao et al., 2021), we convert La-TeX files into markdown format using pandoc.This transformation facilitates the removal of nonessential elements such as titles, author details, bibliographies, and any content preceding the introduction.Additionally, we standardize the format by normalizing multiple consecutive blank lines to a single blank line, enhancing the readability and consistency of the text for model training.</p>
<p>The integration of academic papers enriches our training dataset with formal, structured, and authoritative scientific discourse, significantly benefiting the cognitive and reasoning faculties of the resultant LLM.These elements are critical for applications that demand high levels of comprehension, analytical depth, and factual accuracy, such as academic research assistance, technical writing, and complex problem-solving.</p>
<p>A.5 QA Forums</p>
<p>Question-answering datasets are crucial for the pretraining of LLMs, as they provide the necessary supervisory signals for LLMs and promote the improvement of the models' capabilities in language understanding, knowledge acquisition, context awareness, generalization, and dialogue generation.The improvement of these capabilities is crucial for developing more intelligent, efficient, and practical LLMs.We use the Stack Exchange (in English) dataset and the Zhihu (in Chinese) dataset.</p>
<p>Stack Exchange is a network of question-andanswer websites on topics in diverse fields, each site covering a specific topic, where questions, answers, and users are subject to a reputation award process.Stack Exchange sites are designed to foster expert communities where users can ask questions and provide quality answers, receiving reputation points and badges as rewards for helpful contributions.</p>
<p>Zhihu is a Chinese question-and-answer forum that serves as a comprehensive platform for users to exchange knowledge, experiences, and insights.Users on Zhihu can pose questions on a vast array of topics, ranging from science and technology to culture and education, and receive answers from other community members.These responses can be upvoted or downvoted by users, allowing the most valuable content to be easily accessible.</p>
<p>Preprocessing The Stack Exchange Data Dump contains an anonymized set of all user-contributed content across the Stack Exchange network, organized into site-specific archives.Each archive is formatted as a zipped XML file and contains various data attributes including Posts, Users, Votes, Comments, Badges, Tags, PostHistory, and PostLinks. 15iven the diverse quality of the question-andanswer data contained within these dumps, rigorous preprocessing is essential to obtain high-quality data.Following existing studies (Touvron et al., 2023a;Gao et al., 2021), we process the dataset in several steps.Initially, we parse the XML data to extract textual information from questions and answers, along with important metadata such as the Score attribute.This Score, which ranges from 0 to 10, indicates the quality of an answer-the higher the Score, the higher the quality.To assess the reliability of the Score as a quality indicator, we perform a statistical analysis across the distribution of scores and manually review some random samples of answers at each score level.This helps in verifying the correlation between Score values and actual answer quality.Based on these insights, we choose to retain answers with a Score of four or higher for subsequent processes.In the final phase, answers to the same question are ordered by their Score in descending order.Answers marked as "accepted" by the question questioner are prioritized by assigning them a theoretical Score of positive infinity, ensuring they appear first.For each question, we limit the dataset to the top five highest-scoring answers, thus optimizing the quality of data for model training purposes.</p>
<p>While Zhihu is recognized as one of the highestquality QA platforms in China, the dataset still contains low-quality content such as advertisements, marketing materials, irrelevant or meaningless answers, and biased opinions.To enhance the data quality, we implement the following preprocessing steps.First, we assess user engagement by aggregating metrics such as upvotes, thanks, bookmarks, and followers for each user.Users who surpass a predefined threshold in these combined metrics are recognized as high-quality.Then, for each question, we retain answer that have obtained a substantial number of upvotes and authored by these high-quality users.Furthermore, we introduce a length limit, maintaining answers that are at least 200 Chinese characters in length, or 100 characters for responses from high-quality users.To further refine the dataset, we apply several heuristic rules aimed at eliminating promotional or irrele-vant content.Specifically, answers containing the term "editor" more than twice are excluded, presuming them to be promotional.We also discard any sentences that begin with "image source" and apply additional filters for punctuation and formatting inconsistencies.These heuristic filters lead to the exclusion of approximately 2% of the initial dataset.</p>
<p>A.6 Books</p>
<p>Books represent an invaluable data source for training LLMs, especially in fostering an understanding of long context dependency in natural language processing.High-quality books provide structured and detailed content that is crucial for enhancing the depth and scope of the model's comprehension capabilities.Particularly, textbooks have been proven to be exceptionally effective in improving LLMs' performance due to their rich, authoritative, and well-organized content (Gunasekar et al., 2023;Li et al., 2023b).Our pre-training dataset includes a diverse selection from the Books3 dataset, Project Gutenberg, CBook, Bestsellers, English textbooks, and Chinese textbooks, each offering unique advantages to the training process.</p>
<p>Books3 dataset is created by Shawn Presser and part of "The Pile" dataset (Gao et al., 2021).It includes around 197, 000 books from Bibliotik in plain text format, covering a wide range of topics such as romance, fantasy, and science fiction.</p>
<p>As one of the oldest digital libraries, Project Gutenberg offers a vast array of over 70, 000 free e-books in various languages.The library spans classic literature, fiction, non-fiction, and academic works.Although there is a "frozen" version of this corpus available as of 2018 (Gerlach and Font-Clos, 2020), we opt to collect the latest books up to May 2023 directly from the website, following deduplication and cleaning efforts, resulting in a total of 68, 661 English books. 16Book, made available by the Natural Language Processing Laboratory at Fudan University, includes about 150, 000 Chinese books covering diverse fields such as humanities, education, science, military, and politics.We use open sources for data acquisition and conduct a thorough cleaning operation to ensure quality. 17estsellers comprises a selection of popular Chinese e-books, including textbooks and novels, sourced from Baidu Netdisk.These books are influential in their domains and contribute to the diversity of our training set.</p>
<p>English textbooks are manually collected from the Open Textbook Library. 18They are downloaded in MOBI and EPUB formats, which are then converted into raw texts for training.</p>
<p>Chinese textbooks are acquired from the Wan-Juan corpus (He et al., 2023).They have been preprocessed by the authors and are integrated into our training dataset to provide a rich source of educational content.</p>
<p>Preprocessing For CBook, we first convert all raw data (containing MOBI and EPUB files) into plain text.For this conversion, we use calibre for MOBI files and the Python package BeautifulSoup for EPUB files. 19After conversion, we apply heuristic rules to filter the text data to ensure relevance and readability: (1) We exclude files containing fewer than 3, 000 characters, as these often lack sufficient content for meaningful training.(2) Files where over 60% of the lines contain fewer than six words are discarded due to their fragmented nature.(3) Texts with less than 45% Chinese characters are removed to maintain language consistency, as non-Chinese texts (e.g., Korean or Japanese) may have been mistakenly included.Subsequent cleaning steps include the desensitization of sensitive information such as email addresses and phone numbers, and the removal of non-content elements like publishing details and navigational artifacts (e.g., empty parentheses or brackets).</p>
<p>For Bestsellers, the raw data includes texts in TXT, EPUB, and MOBI formats.All files are converted to plain text and subsequently stored in JSONL format to streamline further processing.We apply heuristic filters to enhance data quality: (1) Files shorter than 170 characters are removed to exclude incomplete or erroneously included texts.</p>
<p>(2) We discard files where more than 29% of lines are under six words long, as these are often poorly formatted or contain extraneous content.(3) Texts with less than 79% Chinese characters are excluded to ensure the dataset primarily contains Chinese language material.Finally, we rigorously remove any remaining private information, such as publication numbers, website URLs, and contact details, to ensure privacy compliance and data integrity.</p>
<p>A.7 News Articles</p>
<p>News provides a stream of current events and realtime data that is crucial for training LLMs to be relevant and responsive to the latest global developments.By integrating news data from diverse sources, LLMs can better grasp the nuances of journalistic language, adapt to varying narrative styles, and improve their accuracy in information retrieval and generation tasks.In our dataset compilation, we include news from CC-news, RealNews (Zellers et al., 2019b), and the news articles from China International Communications Group (CICG) to cover a wide range of topics and perspectives.</p>
<p>CC-news and RealNews are extensive corpora sourced from Common Crawl, specifically curated to include a wide array of news articles.We have accessed open-source versions of these datasets and have conducted thorough cleaning to ensure the removal of inappropriate content as delineated in Section A.1 stage (1). 20ICG, a key state-run foreign-language news and communication organization, provides rigorously vetted news content in English, Chinese, and other languages.This source is particularly valuable for obtaining reliable and official news narratives.We have access to news data spanning from April 2019 to May 2023 from proprietary sources, ensuring a rich dataset that reflects recent global events and trends.</p>
<p>Preprocessing For the CICG data, we initiate our preprocessing with a set of heuristic rules aimed at refining the quality of the data: (1) We discard files shorter than 170 characters, as they often lack substantive content.(2) Articles where over 25% of lines contain fewer than six words are excluded to eliminate fragments and poorly structured content.(3) We also filter out texts with less than 40% Chinese characters to maintain consistency in language composition.Further cleaning involves the removal of non-essential elements such as: (1) The removal of headers and footers, including source attributions, publication dates, and editor names, to focus solely on the content.(2) The elimination of meaningless fragments, such as picture captions, formatting markers, and empty punctuation marks, to enhance the readability and relevance of the texts.</p>
<p>Through these preprocessing steps, we ensure that the news articles included in our training set are of the highest quality, free from extraneous text, and rich in valuable information, making them ideal for training LLMs.</p>
<p>A.8 Legal Documents</p>
<p>Legal judgment documents are also helpful for training LLMs due to their formal, structured nature and the logical complexity they embody.These documents encapsulate rigorous reasoning processes and legal terminology, making them beneficial for enhancing the analytical capabilities of LLMs.The precision and clarity required in legal language training help improve the model's ability to understand and generate text within specific, rule-based contexts, which is pivotal for applications in legal assistance, automated compliance checks, and advanced query-response systems in the legal domain.</p>
<p>We have curated a substantial corpus of legal judgment documents from China Judgments Online, accessed through a private and secure channel. 21In order to adhere to privacy standards and legal requirements, sensitive information such as the names of the courts and judgment numbers are meticulously removed from the dataset.</p>
<p>A.9 Patents</p>
<p>Patent applications are also useful for training LLMs due to their standardized format and formal, technical language.These documents are rich in specialized vocabulary and complex sentence structures, reflecting high levels of precision and clarity.Training LLMs on such data can significantly enhance their ability to parse and generate text within technical contexts.</p>
<p>We incorporate patent applications into our pretraining, utilizing the WanJuan corpus (He et al., 2023) as our source.This corpus provides a vast and diverse collection of patent documents that have been pre-processed to meet training requirements.</p>
<p>A.10 Educational Assessments</p>
<p>Educational assessments provide structured problem-solving environments that are vastly different from general text data, helping models learn to navigate and understand the specific formats and logical reasoning required in standardized 21 https://wenshu.court.gov.cn/testing.The inclusion of this type of data trains LLMs not only in content knowledge but also in the application of this knowledge within the constraints of a given question structure, which is crucial for achieving high performance on standardized assessments like MMLU (Hendrycks et al., 2021), C-Eval (Huang et al., 2023), and AGIEval (Zhong et al., 2023).</p>
<p>To address this need, we incorporate a set of Chinese exam questions from the WanJuan corpus into our pre-training data.Although these questions constitute a relatively small portion of our dataset, they play a significant role in enhancing our LLM's ability to understand and correctly respond to the format and challenges presented by multichoice tests.This approach effectively improves the model's performance on various comprehensive benchmarks, demonstrating the effectiveness of including targeted, format-specific training materials.More detailed analysis of the impact of this training is presented in Section 6.1.</p>
<p>A.11 Deduplication</p>
<p>As demonstrated by existing studies (Xue et al., 2023;Tirumala et al., 2023), data repetition may affect the performance of LLMs.Therefore, we perform deduplication within and across datasets.The deduplication tool is integrated in our YuLan-GARDEN tool (Sun et al., 2024).</p>
<p>B Evaluation Datasets</p>
<p>TriviaQA is originally designed as a reading comprehension dataset comprising over 650, 000 question-answer-evidence triples.Recently, this dataset has been repurposed for closed-book question answering tasks, where LLMs are prompted to answer questions without access to corresponding documents.For our evaluation, we utilize the test set of TriviaQA, which consists of approximately 17, 000 samples.</p>
<p>RACE is a comprehensive reading comprehension dataset comprising over 28, 000 passages and nearly 100, 000 questions.It was collected from English examinations in China and is divided into two subsets: RACE-middle and RACE-high, tailored for middle school and high school students respectively.</p>
<p>CoQA is a large-scale conversational questionanswering dataset that features 8, 000 conversations and 127, 000 questions along with their corresponding answers.</p>
<p>CMRC2018 is a span-extraction dataset specifically designed for Chinese machine reading comprehension.It consists of approximately 20, 000 real questions that have been meticulously annotated on Wikipedia paragraphs by human experts.</p>
<p>C3 (Multiple-Choice Chinese machine reading Comprehension dataset) is a task that assesses machine reading comprehension skills utilizing prior knowledge, including linguistic, domain-specific, and general world knowledge.The dataset includes 13, 369 documents and 19, 577 multiplechoice free-form questions.It can be divided into two subsets: C3-dialog and C3-mixed, depending on the documents (dialogues or formally written mixed-genre texts).</p>
<p>GSM8K (Grade School Math 8K) is a dataset of 8.5K high-quality middle school math word problems, which involve performing a sequence of elementary calculations using basic arithmetic operations.</p>
<p>AQuA-RAT is a large-scale dataset consisting of approximately 100, 000 algebraic word problems.</p>
<p>MMLU (Measuring Multitask Language Understanding) consists of multiple choice questions split into four domains: STEM, social sciences, humanities, and others.</p>
<p>C-Eval is a comprehensive Chinese evaluation suite for foundation models, which consists of 13, 948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels (STEM, Social Science, Humanity, and Other).</p>
<p>Gaokao (Chinese College Entrance Exam) is a subset in the AGIEval (Zhong et al., 2023).The Gaokao dataset includes about 2, 000 examples from 8 subjects: history, math, English, Chinese, geography, biology, chemistry, and physics.</p>
<p>Figure 1 :
1
Figure 1: The pre-training loss of YuLan-Base.</p>
<p>Figure 2 :
2
Figure 2: An example of YuLan-Chat on writing a 2024 Gaokao Chinese essay.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: An example of YuLan-Chat on answering a 2024 Gaokao math question.</p>
<p>def solve(a: float, b: float, c: float):"""finds real roots of ax^2+bx+c"""To solve the quadratic equation ax^2 + bx + c = 0, we can use the quadratic formula:x = (-b ± √(b^2 -4ac)) / 2aHere, a, b, and c are the coefficients of the quadratic equation.In your solve function, you can implement this formula to find the real roots of the equation.Here's how you can do it: def solve(a: float, b: float, c: float): """ Solves the quadratic equation ax^2 + bx + c = 0. Returns the real roots of the equation.-b + math.sqrt(discriminant))/ (2 * a) x2 = (-b -math.sqrt(discriminant))/ (2 * a) return x1, x2</p>
<p>Figure 7 :
7
Figure 7: An example of YuLan-Chat on writing code.</p>
<p>Figure 9 :
9
Figure 9: An example of YuLan-Chat on answering programming problems.</p>
<p>Table 1 :
1
The compression ratio (Bytes per token) of tokenizers.A higher compression ratio indicates that the text can be tokenized into fewer tokens.
LLaMA-2 YuLanChinese (ZH)1.942.15English (EN)4.104.10Code2.792.80Academic papers3.883.88Average (EN &amp; ZH)3.023.130.2M human alignment data. YuLan is evaluatedon 22 public benchmark datasets and achieves com-parable performance with several state-of-the-artopen-source LLMs.</p>
<p>Table 2 :
2
Overview of pre-training datasets.Raw size and weighted size are the numbers of tokens before and after sampling, respectively.# Epoch is the number of passes over each constituent dataset during a full epoch over the final dataset.Weight is the percentage of bytes in the final dataset occupied by each dataset.
DatasetEnglish Chinese Multilingual Raw Size # Epoch Weighted Size Weight</p>
<p>Table 3 :
3
Training settings for pre-training in different stages.EN: English, ZH: Chinese, ML: Multilingual.
StageData Distribution (EN:ZH:ML)Context length # Tokens Initial LR Min LR Batch sizeStage 176 : 22 : 22, 048600B3e-43e-54MStage 290 : 10 : 04, 096900B2e-52e-54MStage 362 : 33 : 54, 096180B2e-52e-54MThe maximum context length is set as 2, 048 tokens3.2.3 Long-tail Knowledge-Awareat this stage. Note that we follow the training strat-Pre-trainingegy of GPT-2 (Radford et al., 2019), where data from the same source are concatenated into long sequences and segmented into training samples of equal length (2, 048 tokens). This method avoids the need for zero-padding within text sequences, thereby enhancing training efficiency. Through-out this stage, we observe a consistent decrease in training loss and a gradual improvement in model performance.After pre-training on 1, 500B tokens, YuLan-Base achieves performance on par with many popu-lar open-source LLMs across various benchmarks. However, post-instruction tuning evaluation reveals deficiencies in handling certain long-tail knowl-edge topics. To mitigate these problems, we pro-pose a strategy to identify areas of knowledge that the model has not effectively learned. Our approach involves augmenting our pre-training dataset withadditional relevant content specifically targetedat these identified gaps, thereby improving the3.2.2 Capability-Enhanced Pre-trainingmodel's ability to process and understand long-tail knowledge.Following the standard pre-training with 600B to-kens, we observe fluctuations in the model's perfor-mance on certain benchmarks, notably on compre-hensive benchmarks such as the MMLU, whereresults approach those of random chance. Totackle this problem, we conduct a series of em-pirical studies (detailed in Section 6.1) that re-veal the significant impact of incorporating edu-cational assessments into the pre-training process.This enhancement is particularly beneficial for per-formance on comprehensive benchmarks for sev-eral reasons: (1) Benchmarks like MMLU con-sist of multiple-choice questions, a format seldomappeared in natural language texts. Educationalassessments frequently contain such questions, aid-ing the model in task familiarization. (2) Thesebenchmarks often resemble closed-book quizzesthat challenge the model to respond based solely onits acquired knowledge. Educational assessmentsnot only present the correct answers but also elab-orate on the reasoning and analysis behind them,thereby effectively guiding the model in applyingits inherent knowledge. Furthermore, we increasethe maximum context length to 4, 096 tokens, en-hancing the YuLan-Base's performance in compre-hending long documents. These strategic modifica-tions lead to consistent performance improvementsacross all evaluated benchmarks.</p>
<p>Table 4 :
4
Overview of datasets and benchmarks for evaluation.The designated datasets, delineated by underlines, constitute the data utilized for our evaluation purposes.Primarily, we employ the test set for conducting our evaluation.In instances where answers are unavailable within the test set, we resort to utilizing the validation set.
DatasetTypeLanguage Train Set Vaild. Set Test SetBoolQ (Clark et al., 2019)Natural Language InferenceEN9,4273,2703,245PIQA (Bisk et al., 2020)Physical Commonsense ReasoningEN16,1131,8383,084Hellaswag (Zellers et al., 2019a)Commonsense NLIEN39,90510,042 10,003WinoGrande (Sakaguchi et al., 2020)Winograd Schema ChallengeEN40,3981,2671,767WSC273 (Levesque et al., 2012)Winograd Schema ChallengeEN--273ARC-easy (Clark et al., 2018)Science QuestionsEN2,2515702,376ARC-challenge (Clark et al., 2018)Science QuestionsEN1,1192991,172OpenBookQA (Mihaylov et al., 2018) Common Sense KnowledgeEN4,957500500CommonSenseQA (Talmor et al., 2019) Common Sense ReasoningEN9,7411,2211,140TriviaQA (Joshi et al., 2017)Factual KnowledgeEN138,38417,944 17,210CoQA (Reddy et al., 2019)Conversational Question AnsweringEN7,199500-RACE-middle (Lai et al., 2017)Chinese Middle School English ExamsEN25,4211,4361,436RACE-high (Lai et al., 2017)Chinese High School English ExamsEN62,4453,4513,498CMRC2018 (Cui et al., 2019)Span-Extraction Chinese MRCZH10,1423,2191,002C3-Dialogue (Sun et al., 2020)Multiple-Choice (Dialogues)ZH4,8851,6281,627C3-Mix (Sun et al., 2020)Multiple-Choice (Mixed-Genre Texts)ZH3,1381,0461,045GSM8k (Cobbe et al., 2021)Math Word ProblemsEN7,473-1,319AQuA-RAT (Ling et al., 2017)Algebraic Word ProblemsEN97,467254254MMLU (Hendrycks et al., 2021)Complex ExamsEN-1,540 14,049C_EVAL (Huang et al., 2023)Complex ExamsZH-1,346 / 260 12,342GaoKao (Zhong et al., 2023)Complex ExamsZH / EN--2,080AlpacaEval (Li et al., 2023a)Alignment EvaluationEN--805AlignBench (Liu et al., 2023)Alignment EvaluationZH--683</p>
<p>Table 5 :
5
Zero-shot performance on commonsense reasoning benchmarks.
BoolQ PIQA HellaSwag WinoGrande WSC273 ARC-e ARC-c OBQA CommonsenseQAMoss-moon-003-sft 59.9 72.360.060.776.264.434.644.028.8ChatGLM278.5 72.058.559.479.567.039.540.469.2Baichuan2-13B78.7 77.478.675.487.277.471.547.669.7Baichuan2-13B-chat 81.4 75.477.476.184.675.268.847.071.0LLaMA-13B76.4 79.779.779.590.577.468.247.263.6LLaMA2-13B80.3 79.480.779.888.379.071.446.571.0LLaMA2-13B-chat75.4 78.480.072.587.976.946.254.273.1YuLan-Base69.1 76.172.365.885.471.841.052.659.4YuLan-Inst79.8 77.574.671.781.777.649.557.477.3YuLan-Chat83.5 78.176.772.783.280.151.658.476.7</p>
<p>Table 6 :
6
The performance on factual knowledge, reading comprehension, and mathmatical reasoning benchmarks.
TriviaQA RACE-m RACE-h CoQA CMRC2018 C3-Dialog C3-Mixed GSM8K AQuAMoss-moon-003-sft26.447.541.249.461.638.640.74.519.3ChatGLM231.150.842.361.369.068.674.223.929.5Baichuan2-13B66.255.046.480.374.281.076.542.835.8Baichuan2-13B-chat65.160.554.175.577.186.886.346.334.7LLaMA-13B73.949.245.777.566.241.544.817.119.7LLaMA2-13B75.950.047.779.073.461.166.525.622.8LLaMA2-13B-chat71.457.253.479.372.949.444.436.224.4YuLan-Base59.348.343.077.264.947.145.218.615.8YuLan-Inst47.949.745.277.073.383.882.529.628.7YuLan-Chat45.656.647.271.472.483.282.630.127.2</p>
<p>Table 7 :
7
The performance on MMLU and C-Eval benchmarks.
MMLUC-EvalSTEM Social Human Other Average STEM Social Human Other Average HardMoss-moon-003-sft 27.2 29.129.532.829.630.0 36.033.132.032.226.7ChatGLM238.9 52.343.052.246.646.5 65.452.447.651.633.5Baichaun2-13B49.2 68.855.165.859.751.1 72.061.755.758.337.9Baichaun2-13B-chat 46.6 65.153.264.257.349.0 69.960.254.556.535.4LLaMA-13B36.4 53.544.053.346.829.7 36.128.829.030.626.9LLaMA2-13B44.6 64.253.962.256.236.9 43.237.636.638.232.0LLaMA2-13B-chat40.4 60.645.257.550.933.6 40.934.135.635.527.3YuLan-Base42.3 60.246.456.151.342.0 57.647.241.546.032.6YuLan-Inst45.2 65.351.261.655.847.2 60.553.044.150.338.1YuLan-Chat45.5 64.351.861.355.747.0 61.852.944.350.537.7</p>
<p>Table 8 :
8
The performance on AGI-Gaokao tasks.
Chinese Geography Chemistry Biology Mathematics History English Physics AverageMoss-moon-003-sft28.530.230.422.425.433.644.825.530.1ChatGLM250.058.347.868.628.571.170.339.054.2Baichaun2-13B50.468.843.558.631.970.678.833.054.5Baichaun2-13B-chat 48.465.844.457.631.167.778.428.552.7LLaMA-13B22.826.631.423.826.223.858.826.530.0LLaMA2-13B27.236.232.426.226.243.072.230.036.7LLaMA2-13B-chat27.625.633.326.726.529.846.425.530.2YuLan-Base31.353.334.843.828.260.968.327.543.5YuLan-Inst42.357.341.654.327.968.580.425.549.7YuLan-Chat43.957.337.753.826.269.480.427.049.5</p>
<p>Table 9 :
9
Comparison of different LLMs on AlpacaEval.
InternLM-7B-Chat ChatGLM-6B Baichuan-13B-Chat MOSS-moon-003Win Ratio of YuLan65.13%60.81%59.57%57.06%</p>
<p>Table 10 :
10
Comparison of different LLMs on Align-Bench."ZH" denotes Chinese.
ZH-Reasoning ZH-Language Avg.InternLM-7B-Chat2.094.393.24MOSS-moon-0032.244.673.46ChatGLM-6B2.505.313.90Baichuan-13B-Chat3.406.354.88YuLan-12B3.596.695.14</p>
<p>Table 11 :
11
Comparison of using different strategies in pre-training.
BenchmarkStrategy 1 Strategy 2 Strategy 3CommonsenseQA13.6815.6418.43AQuA13.7813.7815.75CMRC (Chinese)12.9917.6116.41MMLU25.8343.3043.15AGI-Gaokao27.1540.7138.81</p>
<p>Table 12 :
12
Effect of different Chinese data categories and distributions (dist.) on model performance.
DataDist.MMLU C-EVAL (Hard)HQ126.624.523.9Web124.925.125.3News125.725.627.3Law125.126.628.1HQ+Web4:125.725.626.0HQ+News4:126.525.626.2HQ+Law4:124.124.924.1HQ+All4:2:2:226.826.125.7• Strategy 2: Adding educational assessments (inChinese).• Strategy 3: Adding both MMLU-related QAtraining sets and educational assessments (in Chi-nese).The experimental results are shown in Table</p>
<p>Table 13 :
13
Performance comparisons of different training strategies for investigating the effect of continual pretraining.
ModelDataMMLU C-EVAL C-EVAL (Hard)v120B EN + 10B ZH (p1 + p2)26.826.125.7v213B EN + 6.5B ZH (p1)26.424.125.4v37B EN + 3.5B ZH (p2)27.326.127.3</p>
<p>How do I send an HTTP request in Javascript?To send an HTTP request in JavaScript, you can use the XMLHttpRequest object or the newer fetch() function.Here's an example of how to use XMLHttpRequest to send a GET request to a server: Both of these examples assume that the server is returning a JSON response.If the server is returning a different type of response, you'll need to modify the code accordingly.
var xhr = new XMLHttpRequest();xhr.open('GET', 'https://www.example.com/api/data', true);xhr.onreadystatechange = function() {if (xhr.readyState === XMLHttpRequest.DONE &amp;&amp; xhr.status === 200) {console.log(xhr.responseText);}};xhr.send();Here's an example of how to use fetch() to send a GET request:fetch('https://www.example.com/api/data').then(response =&gt; response.json()).then(data =&gt; console.log(data)).catch(error =&gt; console.error(error));
Sun: You're welcome, Pluto.It's always a pleasure to share my knowledge with others.Have a great day! Figure 8: An example of YuLan-Chat on generating stories.</p>
<p>https://github.com/RUC-GSAI/Yulan-GARDEN
This task is relatively simple, so we use Baichuan-2-13B instead of ChatGPT for saving costs.
https://github.com/nlpxucan/WizardLM, https:// github.com/tatsu-lab/stanford_alpaca
Except for ShareGPT, which contains multi-turn instructions, we directly categorize it into the complex set.
https://huggingface.co/datasets/Dahoas/ synthetic-instruct-gptj-pairwise excluding any data where the disparity in agreement between responses falls below a predefined threshold.
To avoid data leakage(Zhou et al., 2023), we still avoid using MMLU-related training sets.
Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. 2022. Understanding dataset difficulty with V-usable information. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume
https://github.com/LDNOOBW/ List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words
https://dumps.wikimedia.org/backup-index. html
https://huggingface.co/datasets/TMZN/ baidubaike
https://huggingface.co/datasets/ TigerResearch/pretrain_zh
https://www.cnpython.com/pypi/wikiextractor
https://www.cnpython.com/pypi/zhconv
https://archive.org/details/stackexchange
https://github.com/pgcorpus/gutenberg
https://github.com/FudanNLPLAB/CBook-150K
https://open.umn.edu/opentextbooks/
https://calibre-ebook.com, https://www.crummy. com/software/BeautifulSoup/bs4/doc/
https://huggingface.co/datasets/spacemanidol/ cc-stories, https://github.com/rowanz/grover/ tree/master/realnews
AcknowledgmentThe computing resources are supported by Public Computing Cloud, Renmin University of China.
. Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Jason Phang, Shivanshu Purohit, Hailey Schoelkopf, Dashiell Stander, Tri Songz, 10.5281/zenodo.5879544Curt Tigges, Benjamin Thérien, Phil Wangand Samuel Weinbach. 2023. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch</p>
<p>Boolq: Exploring the surprising difficulty of natural yes/no questions. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B Brown, Jack Clark, Sam Mccandlish, Chris Olah, Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, 10.18653/V1/N19-1300Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019. June 2-7, 20191</p>
<p>Think you have solved question answering? try arc, the AI2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, CoRR, abs/1803.054572018</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>Unsupervised cross-lingual representation learning at scale. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, 10.18653/v1/2020.acl-main.747Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online. the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, OnlineAssociation for Computational Linguistics2020. July 5-10, 2020</p>
<p>Ultrafeedback: Boosting language models with high-quality feedback. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, Maosong Sun, 10.48550/ARXIV.2310.01377CoRR, abs/2310.013772023</p>
<p>A span-extraction dataset for chinese machine reading comprehension. Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, Guoping Hu, 10.18653/V1/D19-1600Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational Linguistics2019. November 3-7, 2019</p>
<p>Documenting large webtext corpora: A case study on the colossal clean crawled corpus. Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner, 10.18653/v1/2021.emnlp-main.98Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaDominican RepublicAssociation for Computational Linguistics2021. 7-11 November, 2021</p>
<p>of Proceedings of Machine Learning Research. PMLR</p>
<p>The pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy, CoRR, abs/2101.000272021</p>
<p>A standardized project gutenberg corpus for statistical analysis of natural language and quantitative linguistics. Martin Gerlach, Francesc Font-Clos, 10.3390/E22010126Entropy. 2211262020</p>
<p>Textbooks are all you need. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César, Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo De Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, 10.48550/ARXIV.2306.11644CoRR, abs/2306.11644Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li2023</p>
<p>Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models. Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, Dahua Lin, 10.48550/ARXIV.2308.10755CoRR, abs/2308.107552023</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. Austria2021. May 3-7, 2021OpenReview.net</p>
<p>Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023. 2023. December 10 -16, 2023</p>
<p>Beavertails: Towards improved safety alignment of LLM via a human-preference dataset. Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, Yaodong Yang, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023. 2023. 2023. December 10 -16, 2023</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, 10.18653/V1/P17-1147Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2017. 2017. July 30 -August 41</p>
<p>Leandro von Werra, and Harm de Vries. 2022. The stack: 3 TB of permissively licensed source code. Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, 10.48550/ARXIV.2211.15533CoRR, abs/2211.15533</p>
<p>RACE: large-scale reading comprehension dataset from examinations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard H Hovy, 10.18653/V1/D17-1082Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, Denmark2017. 2017. September 9-11, 2017Association for Computational Linguistics</p>
<p>. Lucile Hugo Laurençon, Thomas Saulnier, Christopher Wang, Albert Akiki, Teven Villanova Del Moral, Leandro Le Scao, Chenghao Von Werra, Eduardo González Mou, Huu Ponferrada, Jörg Nguyen, Mario Frohberg, Quentin Sasko, Angelina Lhoest, Gérard Mcmillan-Major, Stella Dupont, Anna Biderman, Loubna Rogers, Francesco De Ben Allal, Giada Toni, Olivier Pistilli, Somaieh Nguyen, Maraim Nikpoor, Pierre Masoud, Javier Colombo, Paulo De La Rosa, Tristan Villegas, Shayne Thrush, Sebastian Longpre, Leon Nagel, Manuel Weber, Jian Muñoz, Zhu, Zaid Daniel Van Strien, Khalid Alyafeai, Minh Chien Almubarak, Itziar Vu, Aitor Gonzalez-Dios, Kyle Soroa, Manan Lo, Pedro Ortiz Dey, Aaron Suarez, Shamik Gokaslan, David Ifeoluwa Bose, Long Adelani, Hieu Phan, Ian Tran, Suhas Yu, Jenny Pai, Violette Chim, Lepercq, Suzana IlicMargaret MitchellAlexandra Sasha Luccioni, and Yacine Jernite. 2022. The bigscience ROOTS corpus: A 1.6tb composite multilingual dataset. In NeurIPS</p>
<p>The winograd schema challenge. Hector J Levesque, Ernest Davis, Leora Morgenstern, Principles of Knowledge Representation and Reasoning: Proceedings of the Thirteenth International Conference, KR 2012. Rome, ItalyAAAI Press2012. June 10-14, 2012</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023a</p>
<p>Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023b. Textbooks are all you need II: phi-1.5 technical report. Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, 10.48550/ARXIV.2309.05463CoRR, abs/2309.05463</p>
<p>Chanvichet Vong, and "Teknium. Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Openorca: An open dataset of gpt augmented flan reasoning traces. 2023</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/V1/P17-10152017</p>
<p>Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, 10.48550/ARXIV.2311.18743CoRR, abs/2311.18743Yuxiao Dong, and Jie Tang. 2023. Alignbench: Benchmarking chinese alignment of large language models. </p>
<p>S2ORC: the semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel S Weld, 10.18653/V1/2020.ACL-MAIN.447Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020OnlineAssociation for Computational Linguistics2020. July 5-10, 2020</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, Adam Roberts, International Conference on Machine Learning, ICML 2023. Honolulu, Hawaii, USAPMLR2023. 23-29 July 2023202of Proceedings of Machine Learning Research</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USA2019. May 6-9, 2019OpenReview.net</p>
<p>Can a suit of armor conduct electricity? A new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, 10.18653/V1/D18-1260Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018. October 31 -November 4, 2018</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, 10.18653/V1/2022.EMNLP-MAIN.759Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 20222022</p>
<p>Crosslingual generalization through multitask finetuning. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, 10.18653/V1/2023.ACL-LONG.891Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada2023. July 9-14, 20231ACL 2023. Association for Computational Linguistics</p>
<p>Orca: Progressive learning from complex explanation traces of GPT-4. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah, 10.48550/ARXIV.2306.02707CoRR, abs/2306.027072023</p>
<p>Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya Murakhovs'ka, Prafulla Kumar Choubey, Alexander R Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, 10.48550/ARXIV.2309.03450CoRR, abs/2309.03450Shafiq Joty, and Caiming Xiong. 2023. Xgen-7b technical report. </p>
<p>10.48550/ARXIV.2303.08774CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Clueweb22: 10 billion web documents with rich information. Arnold Overwijk, Chenyan Xiong, Jamie Callan, 10.1145/3477495.3536321SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. Madrid, SpainACM2022. July 11 -15, 2022</p>
<p>The refinedweb dataset for falcon LLM: outperforming curated corpora with web data, and web data only. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, 10.48550/arXiv.2306.01116CoRR, abs/2306.011162023</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, 10.18653/V1/2023.FINDINGS-EMNLP.378Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>Searching for activation functions. Prajit Ramachandran, Barret Zoph, Quoc V Le, ICLR 20186th International Conference on Learning Representations. Vancouver, BC, Canada2018. April 30 -May 3, 2018Workshop Track Proceedings</p>
<p>Coqa: A conversational question answering challenge. Siva Reddy, Danqi Chen, Christopher D Manning, 10.1162/TACL_A_00266Trans. Assoc. Comput. Linguistics. 72019</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 10.1609/AAAI.V34I05.6399The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020. February 7-12, 20202020The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</p>
<p>GLU variants improve transformer. Noam Shazeer, CoRR, abs/2002.052022020</p>
<p>Megatron-lm: Training multi-billion parameter language models using model parallelism. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick Legresley, Jared Casper, Bryan Catanzaro, CoRR, abs/1909.080532019</p>
<p>Dolma: an open corpus of three trillion tokens for language model pretraining research. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, Kyle Lo, 10.48550/ARXIV.2402.00159CoRR, abs/2402.001592024</p>
<ol>
<li>peS2o (Pretraining Efficiently on S2ORC) Dataset. Luca Soldaini, Kyle Lo, Allen Institute for AI. ODC-ByTechnical report</li>
</ol>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, H M Murtadha, Yu Ahmed, Shengfeng Lu, Wen Pan, Yunfeng Bo, Liu, 10.1016/J.NEUCOM.2023.127063Neurocomputing. 5681270632024</p>
<p>Investigating prior knowledge for challenging chinese machine reading comprehension. Kai Sun, Dian Yu, Dong Yu, Claire Cardie, 10.1162/TACL_A_00305Trans. Assoc. Comput. Linguistics. 82020</p>
<p>An integrated data processing framework for pretraining foundation models. Yiding Sun, Feng Wang, Yutao Zhu, Wayne Xin Zhao, Jiaxin Mao, 10.48550/ARXIV.2402.16358abs/2402.163582024CoRR</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/V1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019. June 2-7, 20191</p>
<p>Internlm: A multilingual language model with progressively enhanced capabilities. InternLM Team. 2023</p>
<p>D4: improving LLM pretraining via document de-duplication and diversification. Kushal Tirumala, Daniel Simig, Armen Aghajanyan, Ari Morcos, 10.48550/arXiv.2302.13971CoRR, abs/2302.13971Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurélien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, New Orleans, LA, USA2023. December 10 -16, 2023. 2023aLlama: Open and efficient foundation language models</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, 10.48550/ARXIV.2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurélien Rodriguezand Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288</p>
<p>A simple method for commonsense reasoning. H Trieu, Quoc V Trinh, Le, CoRR, abs/1806.028472018</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Long Beach, CA, USA2017. December 4-9, 2017</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>Ccnet: Extracting high quality monolingual datasets from web crawl data. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, Edouard Grave, Proceedings of The 12th Language Resources and Evaluation Conference, LREC 2020. The 12th Language Resources and Evaluation Conference, LREC 2020Marseille, FranceEuropean Language Resources Association2020. May 11-16, 2020</p>
<p>Cvalues: Measuring the values of chinese large language models from safety to responsibility. Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, Jingren Zhou ; Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You, 10.48550/ARXIV.2307.09705CoRR, abs/2307.09705Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; LA, USA2023. 2023. 2023. 2023. December 10 -16, 2023To repeat or not to repeat: Insights from scaling LLM under token-crisis</p>
<p>. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chenxu Chao Yin, Lv, Dian Da Pan, Dong Wang, Fan Yan, Fei Yang, Feng Deng, Feng Wang, Guangwei Liu, Guosheng Ai, Haizhou Dong, Hang Zhao, Haoze Xu, Hongda Sun, Hui Zhang, Jiaming Liu, Jian Ji, Juntao Xie, Kun Dai, Lei Fang, Liang Su, Lifeng Song, Liyun Liu, Luyao Ru, Mang Ma, Mickel Wang, Mingan Liu, Nuolan Lin, Peidong Nie, Ruiyang Guo, Tao Sun, Tianpeng Zhang, Tianyu Li, Wei Li, Weipeng Cheng, Xiangrong Chen, Xiaochuan Zeng, Xiaoxi Wang, Xin Chen, Xin Men, Xuehai Yu, Yanjun Pan, Yiding Shen, Yiyu Wang, Youxin Li, Yuchen Jiang, Yupeng Gao, Zenan Zhang, Zhou, 10.48550/ARXIV.2309.10305and Zhiying Wu. 2023. Baichuan 2: Open large-scale language models. CoRR, abs/2309.10305</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, 10.48550/ARXIV.2309.12284CoRR, abs/2309.122842023</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, 10.48550/ARXIV.2309.05653CoRR, abs/2309.056532023</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, 10.18653/V1/P19-1472Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Long Papers. the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, Italy2019a. July 28-August 2, 20191Association for Computational Linguistics</p>
<p>Defending against neural fake news. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, Canada2019b. 2019. 2019. December 8-14, 2019</p>
<p>GLM-130B: an open bilingual pre-trained model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, Jie Tang, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Root mean square layer normalization. Biao Zhang, Rico Sennrich, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, Canada2019. 2019. 2019. December 8-14, 2019</p>
<p>A survey of large language models. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, 10.48550/arXiv.2303.18223CoRR, abs/2303.182232023</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, 10.48550/ARXIV.2304.06364abs/2304.063642023CoRR</p>
<p>Don't make your LLM an evaluation benchmark cheater. Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, Jiawei Han, 10.48550/ARXIV.2311.01964CoRR, abs/2311.019642023</p>
<p>Large language models for information retrieval: A survey. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, Ji-Rong Wen, 10.48550/ARXIV.2308.07107CoRR, abs/2308.071072023</p>            </div>
        </div>

    </div>
</body>
</html>
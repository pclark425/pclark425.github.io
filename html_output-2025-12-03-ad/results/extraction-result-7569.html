<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7569 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7569</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7569</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-d308562bc7eac8bb5c6705af1c41d8074e3a6882</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d308562bc7eac8bb5c6705af1c41d8074e3a6882" target="_blank">LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection</a></p>
                <p><strong>Paper Venue:</strong> 2023 IEEE International Conference on High Performance Computing & Communications, Data Science & Systems, Smart City & Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)</p>
                <p><strong>Paper TL;DR:</strong> This work proposed LogGPT, a log-based anomaly detection framework based on ChatGPT, which aims to explore the transferability of knowledge from large-scale corpora to log-based anomaly detection and shows promising results and has good interpretability.</p>
                <p><strong>Paper Abstract:</strong> The increasing volume of log data produced by software-intensive systems makes it impractical to analyze them manually. Many deep learning-based methods have been proposed for log-based anomaly detection. These methods face several challenges such as high-dimensional and noisy log data, class imbalance, generalization, and model interpretability. Recently, ChatGPT has shown promising results in various domains. However, there is still a lack of study on the application of ChatGPT for log-based anomaly detection. In this work, we proposed LogGPT, a log-based anomaly detection framework based on ChatGPT. By leveraging ChatGPT's language interpretation capabilities, LogGPT aims to explore the transferability of knowledge from large-scale corpora to log-based anomaly detection. We conduct experiments to evaluate the performance of LogGPT and compare it with three deep learning-based methods on BGL and Spirit datasets. LogGPT shows promising results and has good interpretability. This study provides preliminary insights into prompt-based models, such as ChatGPT, for the log-based anomaly detection task.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7569.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7569.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogGPT (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogGPT: A ChatGPT-based framework for log-based anomaly detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LogGPT uses the instruction-tuned ChatGPT (gpt-3.5-turbo) via carefully constructed prompts to perform anomaly detection on sequences of system log messages, returning anomaly labels plus human-readable reports and preventive measures; evaluated in zero-shot and few-shot (5 examples) settings on public log benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational transformer (decoder-only) accessed via ChatGPT API (gpt-3.5-turbo).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Prompting-based classification using LLM generation (zero-shot prompting and few-shot prompting with 5 labeled examples); response parsing to extract structured outputs (is_anomaly, reports, preventive_measures).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Template with four parts: (1) task description instructing detection and explanation (includes variants: indirect instruction e.g. 'Output your thought process' and direct instruction to produce anomaly reports and preventive measures), (2) format statement specifying JSON output and keys (e.g. 'Output format: Please note that return back in following json format, include keys: is_anomaly, reports, preventive_measures'), (3) optional human-knowledge injection (few-shot: 5 historical logs with labels), (4) input sequence (raw/content/event sequences as Python list). Two specific prompts P1 and P2 were used and tuned; exact full text appears in paper figure (Fig.4).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>System log sequences (semi-structured textual time-series / event logs); three sequence types used: raw sequence, content sequence, event sequence (parameter removed).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Blue Gene/L (BGL) and Spirit</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F1 score, Precision, Recall, Specificity</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported per-window-size (10,20,30,40,50) using prompt-2 on content sequences: BGL - LogGPT (zero-shot) F1: [0.437, 0.523, 0.557, 0.615, 0.618]; LogGPT (few-shot, 5 examples) F1: [0.444, 0.523, 0.571, 0.625, 0.618]. Spirit - LogGPT (zero-shot) F1: [0.507, 0.561, 0.574, 0.571, 0.596]; LogGPT (few-shot, 5 examples) F1: [0.601, 0.629, 0.740, 0.714, 0.694]. Recall is frequently 1.000 for many settings while specificity is often very low (e.g., Spirit zero-shot specificity reported as 0.000 for several window sizes). These numbers are taken from Table II of the paper (prompt-2, content sequence).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against DeepLog, LogAnomaly, and LogRobust on same splits. Example (window size = 50, content sequence): DeepLog F1=0.224 (BGL) / 0.607 (Spirit); LogAnomaly F1=0.243 / 0.571; LogRobust F1=0.304 / 0.455; LogGPT (few-shot) F1=0.618 (BGL) / 0.694 (Spirit). Full baseline tables are in Table II.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Both: zero-shot and few-shot (few-shot uses 5 labeled historical logs injected into the prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High false-positive rate and low specificity (many normal sequences labeled anomalous); sensitivity to prompt design and window size; occasional hallucinations producing ineffective or confusing preventive suggestions; performance depends on sequence type (content > event > raw) and larger window sizes generally improve F1.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>An evaluation of log parsing with chatgpt <em>(Rating: 2)</em></li>
                <li>Is gpt-4 a good data analyst? <em>(Rating: 2)</em></li>
                <li>Is chatgpt a good recommender? a preliminary study <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7569",
    "paper_id": "paper-d308562bc7eac8bb5c6705af1c41d8074e3a6882",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "LogGPT (ChatGPT)",
            "name_full": "LogGPT: A ChatGPT-based framework for log-based anomaly detection",
            "brief_description": "LogGPT uses the instruction-tuned ChatGPT (gpt-3.5-turbo) via carefully constructed prompts to perform anomaly detection on sequences of system log messages, returning anomaly labels plus human-readable reports and preventive measures; evaluated in zero-shot and few-shot (5 examples) settings on public log benchmarks.",
            "citation_title": "LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (ChatGPT)",
            "model_description": "Instruction-tuned conversational transformer (decoder-only) accessed via ChatGPT API (gpt-3.5-turbo).",
            "model_size": null,
            "anomaly_detection_approach": "Prompting-based classification using LLM generation (zero-shot prompting and few-shot prompting with 5 labeled examples); response parsing to extract structured outputs (is_anomaly, reports, preventive_measures).",
            "prompt_template": "Template with four parts: (1) task description instructing detection and explanation (includes variants: indirect instruction e.g. 'Output your thought process' and direct instruction to produce anomaly reports and preventive measures), (2) format statement specifying JSON output and keys (e.g. 'Output format: Please note that return back in following json format, include keys: is_anomaly, reports, preventive_measures'), (3) optional human-knowledge injection (few-shot: 5 historical logs with labels), (4) input sequence (raw/content/event sequences as Python list). Two specific prompts P1 and P2 were used and tuned; exact full text appears in paper figure (Fig.4).",
            "training_data": null,
            "data_type": "System log sequences (semi-structured textual time-series / event logs); three sequence types used: raw sequence, content sequence, event sequence (parameter removed).",
            "dataset_name": "Blue Gene/L (BGL) and Spirit",
            "evaluation_metric": "F1 score, Precision, Recall, Specificity",
            "performance": "Reported per-window-size (10,20,30,40,50) using prompt-2 on content sequences: BGL - LogGPT (zero-shot) F1: [0.437, 0.523, 0.557, 0.615, 0.618]; LogGPT (few-shot, 5 examples) F1: [0.444, 0.523, 0.571, 0.625, 0.618]. Spirit - LogGPT (zero-shot) F1: [0.507, 0.561, 0.574, 0.571, 0.596]; LogGPT (few-shot, 5 examples) F1: [0.601, 0.629, 0.740, 0.714, 0.694]. Recall is frequently 1.000 for many settings while specificity is often very low (e.g., Spirit zero-shot specificity reported as 0.000 for several window sizes). These numbers are taken from Table II of the paper (prompt-2, content sequence).",
            "baseline_comparison": "Compared against DeepLog, LogAnomaly, and LogRobust on same splits. Example (window size = 50, content sequence): DeepLog F1=0.224 (BGL) / 0.607 (Spirit); LogAnomaly F1=0.243 / 0.571; LogRobust F1=0.304 / 0.455; LogGPT (few-shot) F1=0.618 (BGL) / 0.694 (Spirit). Full baseline tables are in Table II.",
            "zero_shot_or_few_shot": "Both: zero-shot and few-shot (few-shot uses 5 labeled historical logs injected into the prompt).",
            "limitations_or_failure_cases": "High false-positive rate and low specificity (many normal sequences labeled anomalous); sensitivity to prompt design and window size; occasional hallucinations producing ineffective or confusing preventive suggestions; performance depends on sequence type (content &gt; event &gt; raw) and larger window sizes generally improve F1.",
            "computational_cost": null,
            "uuid": "e7569.0",
            "source_info": {
                "paper_title": "LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "An evaluation of log parsing with chatgpt",
            "rating": 2
        },
        {
            "paper_title": "Is gpt-4 a good data analyst?",
            "rating": 2
        },
        {
            "paper_title": "Is chatgpt a good recommender? a preliminary study",
            "rating": 1
        }
    ],
    "cost": 0.00804425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection</h1>
<p>Jiaxing Qi, Shaohan Huang, Zhongzhi Luan, Carol Fung, Hailong Yang, Depei Qian</p>
<h4>Abstract</h4>
<p>The increasing volume of log data produced by software-intensive systems makes it impractical to analyze them manually. Many deep learning-based methods have been proposed for log-based anomaly detection. These methods face several challenges such as high-dimensional and noisy log data, class imbalance, generalization, and model interpretability. Recently, ChatGPT has shown promising results in various domains. However, there is still a lack of study on the application of ChatGPT for log-based anomaly detection. In this work, we proposed LogGPT, a log-based anomaly detection framework based on ChatGPT. By leveraging the ChatGPT's language interpretation capabilities, LogGPT aims to explore the transferability of knowledge from large-scale corpora to log-based anomaly detection. We conduct experiments to evaluate the performance of LogGPT and compare it with three deep learning-based methods on BGL and Spirit datasets. LogGPT shows promising results and has good interpretability. This study provides preliminary insights into prompt-based models, such as ChatGPT, for the log-based anomaly detection task.</p>
<p>Index Terms-anomaly detection, deep learning, ChatGPT, system log</p>
<h2>I. INTRODUCTION</h2>
<p>Log-based anomaly detection is an important technique to monitor system activities and identify suspicious behaviors. Logs contain the records of various operations, events, and status information, which are critical for troubleshooting and security analysis. However, manually analyzing large volumes of logs is impractical. In recent years, many automated logbased anomaly detection methods have been proposed, including rule-based methods [1], machine learning (ML)-based methods [2], and deep learning (DL)-based methods. Among these methods, deep learning-based methods [3], [4] have shown supreme performance. They are capable of learning complex patterns and representations from the logs, which allows them to effectively identify anomalies that may not be detected by traditional rule-based or ML-based methods [5].</p>
<p>Deep learning-based methods leverage the power of neural networks and advanced techniques such as recurrent neural networks (RNNs) [3], convolutional neural networks (CNNs) [6], and Transformer [7] to effectively capture complex patterns and dependencies in system logs. Despite these advantages, challenges still present, such as high-dimensional and</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>noisy log data, class imbalance, generalization across datasets, and the interpretability of models.</p>
<p>Recently, large language models (LLMs), such as ChatGPT [8], have shown promising results in many domains such as language understanding [9], dialogue [10] and machine translation [11]. Liu et al. [12] leverage ChatGPT to recommend products for users. Cheng et al. [8] used GPT-4 to perform end-to-end data analysis with databases from a wide range of domains. Le et al. [13] leverage ChatGPT as a log parser to extract the log event and parameters. Nonetheless, log-based anomaly detection with ChatGPT has not been thoroughly investigated. To the best of our knowledge, this is the first study on how to use ChatGPT in log-based anomaly detection.</p>
<p>In this work, we propose LogGPT, a log-based anomaly detection framework based on ChatGPT, which consists of three components: log preprocessing, prompt construction, and response parser. The objective of LogGPT is to utilize ChatGPT's ability on language understanding from large-scale corpora in the domain of system log analysis. The log preprocessing component involves filtering, parsing, and grouping to transform raw log messages into a structured format for further analysis. Prompt construction focuses on designing specific prompts tailored to log anomaly detection, aiming to instruct ChatGPT's generation process toward accurate anomaly identification. The response parser is responsible for extracting the output returned by ChatGPT, allowing for further analysis and evaluation of the detected anomalies.</p>
<p>LogGPT utilizes its language generation capabilities for log anomaly detection. We investigate the possibility to transfer the knowledge and patterns learned by ChatGPT from diverse textual sources to the specialized domain of system log analysis, enabling the effective detection of abnormal events. By conducting comprehensive experiments and analyses, we aim to gain a deeper understanding of the potential and limitations of ChatGPT for log-based anomaly detection. Particularly, we focus on answering the following research questions: 1) What is the current capability of ChatGPT for log-based anomaly detection? and 2) How explainable are the anomalies detected by the model?</p>
<p>To answer the above questions, we conduct a systematic evaluation of LogGPT and compare it to three deep learningbased methods (including DeepLog [3], LogAnomaly [4], LogRobust [14]) on two datasets (BGL and Spirit [15]), under controlled experimental settings. We first investigate the impact of different variables (including prompt construction and window size) on LogGPT performance. Then, we compare the performance of LogGPT to three baseline methods. Finally,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Log-based anomaly detection workflow.
we explore the interpretability of LogGPT. Through extensive experiments, we obtained the following major findings about the LogGPT for log-based anomaly detection:</p>
<ul>
<li>The prompt construction (both task description and human knowledge injection) has a significant impact on LogGPT. A more specific task description and injecting normal log information are often beneficial.</li>
<li>The window size affects the performance of LogGPT. Increasing the window size usually results in better performance.</li>
<li>Compared to three deep learning-based methods, LogGPT shows promising performance (zero-shot and fewshot) on both the BGL and Spirit datasets.</li>
<li>LogGPT demonstrates excellent interpretability in detecting anomalies, providing users with specific information to aid in understanding the causes of anomalies and offering potential preventive suggestions.
In summary, the major contributions of this work are as follows:</li>
<li>This study represents the first attempt to employ ChatGPT for log-based anomaly detection and provides a quantitative evaluation of its effectiveness.</li>
<li>We designed LogGPT, a common framework specifically designed for log-based anomaly detection. The LogGPT framework combines three components to identify and analyze anomalies within log data.</li>
<li>We conducted extensive experiments on both BGL and Spirit datasets and demonstrated that LogGPT has promising performance and good interpretability.</li>
</ul>
<h2>II. Preliminary</h2>
<p>The common workflow of log-based anomaly detection is shown in Figure 1, which includes three steps: 1) log preprocessing, 2) log representation, 3) anomaly detection through DL models.</p>
<p>1) Log preprocessing: Log messages are semi-structured texts, which consist of a constant part (log event) and a variable part (parameters). Generally, we require to filter, parse, and group raw logs to train an anomaly detection model. First, some noise is removed by log filtering. Then, log events and parameters are automatically extracted using log parsing methods. For example, many log parsing methods have been proposed, such as Drain [16], Spell [17], and Paddy [18]. Finally, logs will be separated into various groups, where each group contains several log records. These groups are called log sequences, which will extract various patterns as the input of anomaly detection models.
2) Log representation: Log sequences necessitate transformation into feature vectors for utilization as input of DL models. There are three primary types of log patterns: Sequential pattern, which represents the contextual information of log sequences. Quantitative pattern, which statistic each log event occurs distribution within log sequences. Semantic pattern, which represents the semantic meaning of each log event using a language model, aims to extract the associated semantic information of log sequences. In order to represent the aforementioned log patterns in the form of a feature vector, commonly employed encoding methods include onehot encoding, word2vec, and BERT [15].
3) Anomaly detection: The main purpose of this step is to train a deep anomaly detection model with input as feature vectors of log sequences. A variety of DL techniques have been applied to log-based anomaly detection, such as CNN, RNN, and Transformer [15]. These models can be grouped into three types based on training strategy: supervised, semisupervised, and unsupervised. Supervised models consider log-based anomaly detection as a binary classification task, which utilizes both normal and abnormal logs in the training stage. Semi-supervised models capture normal patterns from the normal log sequences to detect anomalies. Unsupervised models do not require any labeled logs and typically combine with cluster and generative methods.</p>
<h2>III. LOG-BASED ANOMALY DETECTION WITH CHATGPT</h2>
<p>We design a framework, namely LogGPT, for log-based anomaly detection using ChatGPT. As shown in Figure 2, the framework consists of three main components: Log preprocessing, where the raw log messages are parsed into a structured format; Prompt construction, where different anomaly detection prompts are constructed for log sequences; and Response parser, where prompts and sequences will form a request to be sent to ChatGPT. The response information will be parsed into parts for evaluation, and the final result will be presented to the users.</p>
<h2>A. Log Preprocessing</h2>
<p>This step is to extract the structured information from raw logs. We employed the state-of-the-art log parsing method (Drain) to extract structured data, including ID, Timestamp, Content, and EventTemplate. After that, the raw logs and parsed logs are grouped into different chunks using a fixed-size</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. The framework of LogGPT to perform log-based anomaly detection.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. An example of <em>log parsing</em>.</p>
<p>time window, which generates three types of sequences: <em>raw sequence, content sequence, and event sequence</em>. An example of this step is shown in Figure 3.</p>
<p>The <em>raw sequence</em> consists of the raw log messages, capturing the unaltered information directly from the log messages. The <em>content sequence</em> focuses on the log text. It excludes certain content that is irrelevant to the analysis, such as <em>ID</em> and <em>Timestamp</em>. The event sequence is the high-level abstraction of the <em>content sequence</em> where dropped the variable part in the log text. Compared to the three types of sequence, the <em>raw sequence</em> provides fine-grained information, enabling detailed investigation and troubleshooting. The <em>content sequence</em> emphasizes the textual content, facilitating text-based analysis and anomaly detection. The event sequence provides coarse-grained information, highlighting sequential patterns to support pattern-based anomaly detection.</p>
<h3>B. Prompt Construction</h3>
<p>In this step, we describe the prompt construction strategy for the log-based anomaly detection task. As shown in Figure 4, we designed a prompt template, which includes the following parts: <em>task description</em>, <em>format statement</em>, <em>human knowledge injection</em>, <em>input sequence</em>. Then, we fill in the content of each part by domain experience and improve the prompt using ChatGPT. In addition, we follow the general tips that include more specific, starting simple and iterating on improvements, for designing task-specific prompts. Finally, we selected two prompts with the best test performance to conduct our experiments.</p>
<ol>
<li>
<p><strong>Task description.</strong> Log-based anomaly detection is a crucial task for maintaining system operations. While DL-based methods have made some progress in this field, their interpretability remains a challenging issue [5]. Therefore, the <em>task description</em> should not only instruct ChatGPT to determine whether an anomaly has occurred but also prompt it to provide explanations for the occurrence of anomalies. Furthermore, it should guide ChatGPT to suggest possible preventive measures. To achieve these objectives, we introduce two types of instruction in this part, namely indirect instruction and direct instruction. For indirect instruction, we use a phrase, such as <em>"Output your thought process"</em>, to prompt ChatGPT to explain the anomalous events. For direct instruction, we explicitly instruct ChatGPT to generate anomaly reports and preventive measures. This way, we aim to provide users with more fine-grained system reports and help users achieve a deeper understanding of the operation status of the system.</p>
</li>
<li>
<p><strong>Format statement.</strong> To ensure response diversity, ChatGPT incorporates some randomness during the generation process. Additionally, when users sent requests to ChatGPT, they can specify a temperature parameter to control the diversity of the response. Higher temperature values make ChatGPT prefer to select words and phrases more randomly, resulting in more diverse and creative text generation [12]. However, higher temperature values may also cause the model to choose less common or less reasonable vocabulary, leading to unexpected responses. While such responses can be more creative, they also lead to inaccurate or unreasonable outcomes, making it challenging to evaluate the performance of anomaly detection. To address this concern, we introduce two methods to handle this challenge. First, we explicitly indicate the expected response format in the prompt. For example, we indicate the response must be in json format and specify the keys that should be included, which ensures that most of the responses meet the expected format (<em>Output format: Please note that return back in following json format, include keys: is_anomaly, reports, preventive_measures</em>). However, there may still be some responses that do not meet the expectations. We introduce the <em>Response Parser</em> component (see Section III-C) to address this issue.</p>
</li>
<li>
<p><strong>Human Knowledge Injection.</strong> DL-based methods have demonstrated that the performance is often unsatisfactory without any domain prior knowledge (unsupervised methods) [5]. Therefore, it should be beneficial to introduce some prior knowledge in the prompt. We have introduced this part (optional, gray) to the prompt template to allow users to inject specific domain prior knowledge into the prompt, which aims to improve the performance of ChatGPT on log anomaly detection tasks. Generally, we refer to this kind of prompt</p>
</li>
</ol>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Example prompts of log-based anomaly detection. For the zero-shot setting, the human-knowledge injection part is dropped.
as a few-shot setting, otherwise, it is a zero-shot setting. For example, we can use some labeled logs to fill this part, so that ChatGPT can preview some domain knowledge.
4) Input sequence. There are three types of input sequences mentioned above. To this end, we provide a variable part (blue) that supports different input sequences. When sending a request to ChatGPT, we form the log sequence into a Python list and concatenate other parts as a final prompt.</p>
<h2>C. Response Parser</h2>
<p>The responses of ChatGPT have diversity. In addition, it should be noted that the output token of ChatGPT has a maximum length limit for each request. Once the limit is exceeded, the response will be terminated and the current output will be returned. To ensure the parsability of the response, we designed this component to parse the text of the response into several pre-defined parts. For example, three parts are defined in this study including is_anomaly, reports, and preventive_measures. For each response, we format and check the response text. If the pre-defined format is satisfied, the text is parsed directly. Otherwise, we use ChatGPT to reformat the text so that it satisfies the requirements.</p>
<h2>IV. Evaluation</h2>
<p>In this section, we evaluate LogGPT by answering the following research questions:</p>
<ul>
<li>RQ1: How does LogGPT perform with different prompts?</li>
<li>RQ2: How does LogGPT perform on different window sizes?</li>
<li>RQ3: How does human knowledge injection affect the performance of LogGPT?</li>
<li>RQ4: How does LogGPT perform compared to deep learning-based methods?</li>
<li>RQ5: How explainable are the anomalies detected by LogGPT?</li>
</ul>
<h2>A. Datasets</h2>
<p>Our performance evaluation process is based on two commonly used log datasets [15], and the details of each dataset are as follows:</p>
<ul>
<li>Blue Gene/L (BGL) dataset contains 4,747,963 log messages from a Blue Gene/L supercomputer system at Lawrence Livermore National Laboratory, California. It has 131,072 processors and 32,768GB of memory. The dataset includes alert and non-alert messages, with $348,460(7.34 \%)$ labeled as anomalous.</li>
<li>Spirit dataset is collected from a Linux production cluster at Sandia National Labs, comprising 512 nodes over a 23day period. It contains $272,298,969$ log messages, with $172,816,564(63.47 \%)$ labeled as system anomalies.
We split the training and testing set with $8: 2$. Due to the imposed limitations on ChatGPT's API request frequency, we adopted a random sampling strategy to select a subset consist 2000 consecutive logs from the testing set. Moreover, we have checked manually to ensure a proportion of abnormal logs within the subset. This approach aimed to sample a representative subset of logs for evaluation.</li>
</ul>
<h2>B. Baseline methods and Metrics</h2>
<ul>
<li>Deeplog [3] is a semi-supervised method that uses sequential vectors as input patterns to learn normal system executions by predicting the next log event based on preceding events.</li>
<li>LogAnomaly [4] is a semi-supervised method that uses sequential vector and quantitative vector as input, as well as applies an LSTM-based model to detect sequential and quantitative anomalies in log sequence.</li>
<li>LogRobust [14] is a supervised log-based anomaly detection method that extracts semantic information of log events and represents them as semantic vectors. It then uses an attention-based Bi-LSTM model to detect anomalies, which can capture contextual information in log sequences and have better robust for log unstable.</li>
</ul>
<p>Evaluation Metrics. Like previous works [15], [19], we use common evaluation metrics for anomaly detection tasks, including F1 score, Precision, and Recall. In addition, Specificity is also used in order to evaluate more comprehensively.</p>
<h2>C. Implemention details</h2>
<p>To extract the contents and events from the log data, we use the log parser Drain with the default parameter settings [5]. Note that the special character $(\langle*\rangle)$ in log events is dropped when grouped into event sequences. For the ChatGPT, we apply gpt-3.5-turbo to conduct zero-shot and few-shot experiments with two prompts (Figure 4). We set the temperature is 0 and only the top-1 choice is returned. Moreover, the maximum number of output tokens is limited to 100 for a faster response. In the few-shot setting, we put the 5 historical logs and labels to enable ChatGPT to learn some prior knowledge. For the response parser, we designed a prompt to reformat the response in json format if the pre-defined format is not satisfied: "Please format the following text in json format, which include the keys: ... ". The implementation of three baseline methods is referred to the public code on GitHub2.</p>
<h2>D. Results and Analysis</h2>
<h2>1) RQ1: Performance with different prompts:</h2>
<p>Experiment Settings. We investigated the performance of LogGPT with different prompts. Specifically, we used both prompt-1 (P1) and prompt-2 (P2) as shown in Figure 4, and evaluated the performance of the three types of input sequences in both zero-shot (ZS) and few-shot (FS) settings, with a fixed window size of 50. The experimental results are presented in Figure 5.</p>
<p>We can see that for the raw sequence, P1 has a higher F1 score than P2. In particular, it can be seen that human knowledge injection is not always beneficial, since the ZS-P1 with the raw sequence is the highest in terms of F1 score on both datasets. P2 has more advantages than P1 for content sequence and event sequence. For example, FS-P2 has an F1 score of 0.694 on Spirit, which is much higher than other types of inputs. Comparing the three different sequence types, the content sequence performs better than the other two sequence types. We believe this is because the content removes irrelevant information (such as timestamps), which allows ChatGPT to understand its semantic information more precisely.</p>
<p>Summary. The prompts have a significant impact on the log-based anomaly detection task. In contrast to expectations, the incorporation of human knowledge does not always result in improved detection performance. It is essential to explore various prompts that align with the specific context in order to attain satisfactory detection performance.</p>
<h2>2) RQ2: Performance on different window sizes:</h2>
<p>Experiment Settings. We evaluated the performance of LogGPT with the window size increasing from 10 to 50. P2 is the default prompt. The experimental results are presented in Figure 6 and more results are presented in Appendix VIII</p>
<p>TABLE I
PERFORMANCE WITH DIFFERENT INJECTION TYPES ON THE SPIRIT DATASET</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Window <br> Size</th>
<th style="text-align: center;">Injection <br> Type</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">$\mathbf{P}$</th>
<th style="text-align: center;">$\mathbf{R}$</th>
<th style="text-align: center;">$\mathbf{S}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Abnormal</td>
<td style="text-align: center;">0.026</td>
<td style="text-align: center;">0.100</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">0.932</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.000</td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">$\mathbf{0 . 7 5 0}$</td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.000</td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.652</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">Normal</td>
<td style="text-align: center;">0.601</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.985</td>
<td style="text-align: center;">0.333</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.629</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
<td style="text-align: center;">0.246</td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{0 . 7 4 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 7}$</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
<td style="text-align: center;">0.525</td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.714</td>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
<td style="text-align: center;">0.467</td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
<td style="text-align: center;">0.348</td>
</tr>
</tbody>
</table>
<p>We can observe a positive correlation between the F1 score and window size in the majority of cases, indicating that larger window sizes generally result in higher F1 scores. This suggests that incorporating more contextual information is often beneficial. Furthermore, the three types of sequences can be ranked in terms of performance from best to worst as follows: content sequence, event sequence, and raw sequence. This ranking aligns with our expectations, as the raw sequence contains a significant amount of irrelevant information, while the event sequence lacks parameter information. Finally, the results also demonstrate the advantages of the few-shot setting in most cases.</p>
<p>Summary. The window size affects the performance of LogGPT where increasing the window size usually results in higher accuracy in anomaly detection. In general, better performance could be achieved through the content sequence and human knowledge injection.</p>
<h2>3) RQ3: The impact of human knowledge injection:</h2>
<p>Experiment Settings. We studied the effects of different human knowledge injections. Two types of knowledge injection (abnormal and normal) are employed, we randomly sample 5 normal logs and 5 abnormal logs from the Spirit dataset. The experimental results are presented in Table I.</p>
<p>We can observe that the performance of LogGPT varies a lot with different types of knowledge injection. For example, abnormal knowledge injection results in an F1 score range of $[0.000,0.490]$ with the window size increasing from 10 to 50. When experimenting with the normal knowledge injection, it achieves a promising performance with the F1 scores ranging from 0.601 to 0.740 . Because the abnormal type is diverse, a few abnormal samples cannot cover most types. The results also show that the recall remains high across all window sizes, while precision and specificity are low. This suggests that LogGPT frequently misses identical normal sequences as abnormal, resulting in a high false positive rate.</p>
<p>Summary. Different human knowledge injections have a significant impact on the performance of log-based anomaly detection. LogGPT adopts a conservative approach by identifying only highly certain logs as normal and identifying the remaining logs as abnormal. This bias can be attributed to the presence of many low-level alert events in the logs, which are</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Performance of LogGPT with different prompts.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. Performance of LogGPT w.r.t different window size.</p>
<p>TABLE II PERFORMANCE COMPARISON ON BGL AND SPIRIT (PROMPT-2 WITH CONTENT SEQUENCE)</p>
<table>
<thead>
<tr>
<th>Window Size</th>
<th>Metrics</th>
<th>BGL</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Spirit</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>DeepLog</td>
<td>LogAnomaly</td>
<td>LogRobust</td>
<td>LogGPT (zero-shot)</td>
<td>LogGPT (few-shot)</td>
<td>DeepLog</td>
<td>LogAnomaly</td>
<td>LogRobust</td>
<td>LogGPT (zero-shot)</td>
<td>LogGPT (few-shot)</td>
</tr>
<tr>
<td>10</td>
<td>F</td>
<td>0.168</td>
<td>0.201</td>
<td>0.944</td>
<td>0.437</td>
<td>0.444</td>
<td>0.521</td>
<td>0.679</td>
<td>0.667</td>
<td>0.507</td>
<td>0.601</td>
</tr>
<tr>
<td></td>
<td>P</td>
<td>0.920</td>
<td>0.112</td>
<td>0.975</td>
<td>0.280</td>
<td>0.286</td>
<td>0.352</td>
<td>0.602</td>
<td>1.000</td>
<td>0.340</td>
<td>0.432</td>
</tr>
<tr>
<td></td>
<td>R</td>
<td>0.992</td>
<td>0.985</td>
<td>0.914</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>0.779</td>
<td>0.500</td>
<td>1.000</td>
<td>0.985</td>
</tr>
<tr>
<td></td>
<td>S</td>
<td>0.370</td>
<td>0.501</td>
<td>0.999</td>
<td>0.356</td>
<td>0.375</td>
<td>0.053</td>
<td>0.735</td>
<td>1.000</td>
<td>0.000</td>
<td>0.333</td>
</tr>
<tr>
<td>20</td>
<td>F</td>
<td>0.219</td>
<td>0.228</td>
<td>0.601</td>
<td>0.523</td>
<td>0.523</td>
<td>0.627</td>
<td>0.614</td>
<td>0.700</td>
<td>0.561</td>
<td>0.629</td>
</tr>
<tr>
<td></td>
<td>P</td>
<td>0.123</td>
<td>0.129</td>
<td>0.442</td>
<td>0.354</td>
<td>0.354</td>
<td>0.591</td>
<td>0.500</td>
<td>1.000</td>
<td>0.390</td>
<td>0.459</td>
</tr>
<tr>
<td></td>
<td>R</td>
<td>0.977</td>
<td>0.975</td>
<td>0.938</td>
<td>1.000</td>
<td>1.000</td>
<td>0.667</td>
<td>0.795</td>
<td>0.538</td>
<td>1.000</td>
<td>1.000</td>
</tr>
<tr>
<td></td>
<td>S</td>
<td>0.513</td>
<td>0.552</td>
<td>0.919</td>
<td>0.292</td>
<td>0.292</td>
<td>0.705</td>
<td>0.492</td>
<td>1.000</td>
<td>0.000</td>
<td>0.246</td>
</tr>
<tr>
<td>30</td>
<td>F</td>
<td>0.129</td>
<td>0.146</td>
<td>0.891</td>
<td>0.557</td>
<td>0.571</td>
<td>0.581</td>
<td>0.632</td>
<td>0.744</td>
<td>0.574</td>
<td>0.740</td>
</tr>
<tr>
<td></td>
<td>P</td>
<td>0.688</td>
<td>0.788</td>
<td>0.893</td>
<td>0.386</td>
<td>0.400</td>
<td>0.409</td>
<td>0.600</td>
<td>1.000</td>
<td>0.403</td>
<td>0.587</td>
</tr>
<tr>
<td></td>
<td>R</td>
<td>1.000</td>
<td>0.972</td>
<td>0.888</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>0.667</td>
<td>0.593</td>
<td>1.000</td>
<td>1.000</td>
</tr>
<tr>
<td></td>
<td>S</td>
<td>0.082</td>
<td>0.180</td>
<td>0.992</td>
<td>0.222</td>
<td>0.267</td>
<td>0.025</td>
<td>0.700</td>
<td>1.000</td>
<td>0.000</td>
<td>0.525</td>
</tr>
<tr>
<td>40</td>
<td>F</td>
<td>0.147</td>
<td>0.239</td>
<td>0.374</td>
<td>0.615</td>
<td>0.625</td>
<td>0.597</td>
<td>0.711</td>
<td>0.667</td>
<td>0.571</td>
<td>0.714</td>
</tr>
<tr>
<td></td>
<td>P</td>
<td>0.793</td>
<td>0.137</td>
<td>0.236</td>
<td>0.444</td>
<td>0.455</td>
<td>0.426</td>
<td>0.640</td>
<td>1.000</td>
<td>0.400</td>
<td>0.556</td>
</tr>
<tr>
<td></td>
<td>R</td>
<td>0.972</td>
<td>0.960</td>
<td>0.900</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>0.800</td>
<td>0.500</td>
<td>1.000</td>
<td>1.000</td>
</tr>
<tr>
<td></td>
<td>S</td>
<td>0.137</td>
<td>0.541</td>
<td>0.780</td>
<td>0.167</td>
<td>0.200</td>
<td>0.100</td>
<td>0.700</td>
<td>1.000</td>
<td>0.000</td>
<td>0.467</td>
</tr>
<tr>
<td>50</td>
<td>F</td>
<td>0.224</td>
<td>0.243</td>
<td>0.304</td>
<td>0.618</td>
<td>0.618</td>
<td>0.607</td>
<td>0.571</td>
<td>0.455</td>
<td>0.596</td>
<td>0.694</td>
</tr>
<tr>
<td></td>
<td>P</td>
<td>0.126</td>
<td>0.139</td>
<td>0.183</td>
<td>0.447</td>
<td>0.447</td>
<td>0.436</td>
<td>0.410</td>
<td>1.000</td>
<td>0.425</td>
<td>0.531</td>
</tr>
<tr>
<td></td>
<td>R</td>
<td>0.994</td>
<td>0.942</td>
<td>0.911</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>0.941</td>
<td>0.294</td>
<td>1.000</td>
<td>1.000</td>
</tr>
<tr>
<td></td>
<td>S</td>
<td>0.418</td>
<td>0.537</td>
<td>0.676</td>
<td>0.087</td>
<td>0.087</td>
<td>0.043</td>
<td>0.000</td>
<td>1.000</td>
<td>0.000</td>
<td>0.348</td>
</tr>
</tbody>
</table>
<p>often disregarded by administrators despite being semantically categorized as abnormal.</p>
<p>4) RQ4: Performance comparison to three baseline methods:</p>
<p>Experiment Settings. We evaluated the performance of LogGPT with both zero-shot setting and few-shot setting and used prompt-2 as the default prompt. The experimental results are shown in Table II.</p>
<p>Compared to the three deep learning-based methods, LogGPT with few-shot setting achieved superior performance on both datasets in terms of the F1 score, precision, and recall. And LogGPT with the zero-shot setting achieved a competitive F1 score on both datasets. An interesting observation is that LogGPT achieved the same F1 score between zero-shot and few-shot settings on BGL when the window size is 20 and 50, suggesting that adding some prior knowledge is not always beneficial. Furthermore, LogGPT achieves the highest recall, but the specificity is underperforming, especially on Spirit where the zero-shot setting is 0. This demonstrated that LogGPT is more reserved, resulting in the identification of many normal logs as anomalies.</p>
<p>Summary. LogGPT shows promising performance on both the BGL and Spirit datasets. Its ability to identify log sequences as normal or anomalous suggests its potential as an</p>
<p>TABLE III
INTERPRETABILITY OF DETECTED ANOMALIES ON BGL AND SPIRIT</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Key</th>
<th style="text-align: center;">Response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BGL</td>
<td style="text-align: center;">Anomaly <br> $\log$ sequence</td>
<td style="text-align: center;">RAS KERNEL FATAL data TLB error interrupt</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reports</td>
<td style="text-align: center;">...</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Preventive measures</td>
<td style="text-align: center;">data TLB error interrupt Check the memory and CPU usage Check the system logs</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Anomaly <br> $\log$ sequence</td>
<td style="text-align: center;">RAS KERNEL INFO MACHINE CHECK DCR read timeout</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reports</td>
<td style="text-align: center;">RAS KERNEL FATAL Lustre mount FAILED : bglio78 : block_id : location</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Preventive measures</td>
<td style="text-align: center;">Check the Lustre mount FAILED</td>
</tr>
<tr>
<td style="text-align: center;">Spirit</td>
<td style="text-align: center;">Anomaly <br> $\log$ sequence</td>
<td style="text-align: center;">pbs_mom: Connection refused (111) in open_demux, open_demux: connect 172.30.71.32:44020</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reports</td>
<td style="text-align: center;">Anomaly detected: Multiple sshd sessions opened for user root</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Preventive measures</td>
<td style="text-align: center;">Limit the number of sshd sessions opened for user root</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Anomaly <br> $\log$ sequence</td>
<td style="text-align: center;">pbs_mom: Connection refused (111) in open_demux, open_demux: connect 172.30.71.32:43908</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Reports</td>
<td style="text-align: center;">Inappropriate ioctl for device (25) in search_env_and_open, failed connect to mpiexec process on MS</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Preventive measures</td>
<td style="text-align: center;">Check the mpiexec process on MS</td>
</tr>
</tbody>
</table>
<p>effective tool for log anomaly detection in various domains.</p>
<h2>5) RQ5: Interpretability of anomalies by LogGPT:</h2>
<p>Providing more explanations for the anomalies detected by the models has been difficult since DL-based methods are considered a black box whose decision-making process is not explainable, which poses challenges for administrators in timely identifying and preventing anomalies. In this experiment, we investigate the potential of LogGPT for interpretability with respect to anomaly localization and prevention.</p>
<p>Experiment Settings. As LogGPT(few-shot) achieved the highest recall on BGL and Spirit datasets, we randomly selected two anomalous raw sequences on both datasets, respectively, to verify the effectiveness of anomaly localization and anomaly prevention.</p>
<p>The results are presented in Tabel III. We can see that the response of LogGPT can not only report the cause of the anomaly but also give suggestions for prevention. For example, an anomaly occurred which is connection refused during the running of a Linux cluster. LogGPT reports the reason to be the root user opening too many sshd sessions. Furthermore, LogGPT suggests to enforce the limitations on the number of sshd sessions the root user can open. However, LogGPT's suggestions may not always be effective and they occasionally lead to confusion. For example, in the case of the first example on the BGL dataset, LogGPT suggests that users should first check the CPU and memory usage.</p>
<p>If the anomaly persists, LogGPT suggests the administrators to manually analyze historical logs. We can see from these results that while LogGPT is a powerful log-based anomaly detector, its suggestions are generated based on patterns and information extracted from the input sequences. Therefore, there are instances where its suggestions may not be useful or may require additional manual analysis by administrators.</p>
<p>Summary. LogGPT can help administrators find abnormal locations and provide preventive suggestions which improve the efficiency of troubleshooting. However, there are some ineffective outputs in our experiments due to the hallucination problem of ChatGPT. Since this work is not concerned with solving this problem, we only report the existence of ineffective output. We think more specific prompts may be able to reduce such output.</p>
<h2>V. Related works</h2>
<p>In this section, we briefly review the related work of logbased anomaly detection and large language models (LLMs).</p>
<p>Log-based anomaly detection is an important technique for monitoring system activities and identifying suspicious behaviors. Many works have been proposed in recent years [3], [4], [14], these works can be grouped into three types according to the training strategy.</p>
<p>1) Supervised methods assume that have a large number of labeled data to train binary classification models [14], which commonly achieved the optimal performance. Zhou et.al [20]</p>
<p>proposed LogSyaer, which is a log pattern-driven anomaly detection model that addresses these challenges. LogSayer utilizes statistical features, LSTM neural networks, and a BP neural network for adaptive anomaly decisions. Du et al. [21] proposed LogAttention which embeds log patterns into semantic vectors and uses a self-attention-based neural network to detect anomalies. The input to these methods typically is a log event sequence. However, the parameters within the logs also contain valuable information for identifying anomalies. Huang et al. [7] proposed HitAnomaly, which utilizes a hierarchical transformer model to capture both log event sequences and parameters, to improve the detection accuracy.
2) Semi-supervised methods assume that training takes place only on normal data that is free of anomalies. Its primary idea is to make the models capture the normal sequence patterns, while the abnormal sequence patterns are quite distinct. Some works focus on reconstruction-based methods that utilize generative models, such as AutoEncoders (AEs) [22] and Generative Adversarial Networks (GANs) [23], to encode the input log sequences and then attempt to reconstruct the input using a decoder. Any input data that is fed into an already trained model and yields a high reconstruction error is then considered anomalous. Another technical route is using sequential models, such as Recursive Neural Networks (RNNs) [3] and Transformer [24], to learn normal sequential patterns and predict the next possible log event. If a new log sequence does not match the predicted then it is identified as anomalous. Moreover, we note that many works [22] claim that it is unsupervised, but it is actually trained using only normal logs.
3) Unsupervised methods [25], [26] assume that no labels are available for training models. Han et al. [27] proposed a domain adaptation framework called LogTAD, which makes log data from different systems have similar distributions, enabling the detection model to identify anomalies across multiple systems. Nedelkoski et al. [28] focus on learning log representations that capture semantic differences between normal and anomaly logs, improving generalization on unseen logs. It leverages auxiliary log datasets available on the internet to enhance the representation of normal data while providing diversity to prevent overfitting. However, due to the lack of prior knowledge, the detection accuracy is lower than supervised and semi-supervised methods in most scenarios.</p>
<p>Large language models (LLMs) have revolutionized the field of natural language processing (NLP) and have gained significant attention in recent years. OpenAI introduced GPT-1 [29], which demonstrated promising results by pre-training on a large corpus of internet text data and fine-tuning it for specific tasks. After that, most LLMs have been proposed, such as BERT [24], T5 [30], and GPT-3 [31], etc. A key milestone in LLM development is InstructGPT [32], which introduces a framework for instruction fine-tuning of pre-trained language models using Reinforcement Learning from Human Feedback (RLHF). This framework enables LLMs to adapt to a wide range of NLP tasks, enhancing their versatility and flexibility by incorporating human feedback. Unlike LLMs trained alone on text corpora through unsupervised pre-training, RLHF allows models to align with human preferences and values, significantly improving their performance. OpenAI has leveraged similar techniques to develop ChatGPT, a conversation-based language model that brings AI to the forefront, transitioning it from being behind the scenes. Subsequently, OpenAI released a version based on GPT-4 [33]. Concurrently, other peers have also released similar language models. For example, Meta AIs LLaMA [34] and Googles PaLM [35], etc.</p>
<p>In terms of applications, LLMs have shown impressive performance in various domains. For example, Microsoft released the Copilot plugin [36], a dedicated model for code generation, which greatly improves the efficiency of developers. Additionally, such models as Stable Diffusion, Midjourney, and DALL-E [37] have also demonstrated excellent results in artificial intelligence generative content (AIGC). We can envision that LLMs will play a crucial role in numerous decision-making scenarios, paving the way for advancements in various domains.</p>
<h2>VI. LIMITATIONS</h2>
<p>Based on our observations, we highlight several limitations of LogGPT:</p>
<ul>
<li>Sensitivity to prompt variation: Designing the optimal prompt is not always straightforward, even though prompts play a critical role in LogGPTs performance.</li>
<li>Limited window size: Balancing the need for a larger window size with computational constraints remains a challenge.</li>
<li>High false positive rate: LogGPT suffers from a high false positive rate, leading to a significant number of incorrect anomaly identifications.</li>
<li>Trustworthiness: The presence of ineffective outputs due to the hallucination problem of ChatGPT.</li>
</ul>
<p>Addressing these limitations will improve the applicability and effectiveness of LogGPT, and benefit the application of LogGPT in real-world scenarios.</p>
<h2>VII. CONCLUSION AND FUTURE WORK</h2>
<p>In this study, we proposed LogGPT, a framework for logbased anomaly detection based on ChatGPT. Experiments demonstrated that LogGPT has a promising ability for this important task and better anomalous interpretability, except that many normal sequences are identified anomalies and it causes many false alarms. Moreover, we investigated different prompts construction, window sizes, and the types of input sequence impact on performance. We believe that the results and findings of our study can provide valuable insights into the strengths and limitations of ChatGPT in log-based anomaly detection. As our future work, we will investigate the following:</p>
<ul>
<li>Experiment with more public log datasets and test more prompts to improve performance.</li>
<li>
<p>Try using more large language models such as GPT-4 or LLaMA.</p>
</li>
<li>
<p>Further exploration of ChatGPT's potential for anomalous interpretability.</p>
</li>
</ul>
<h2>REFERENCES</h2>
<p>[1] J. Breier and J. Braniov, "A dynamic rule creation based anomaly detection method for identifying security breaches in log records," Wireless Personal Communications, vol. 94, pp. 497-511, 2017.
[2] J. Breier and J. Brani, "Anomaly detection from log files using data mining techniques," in Information Science and Applications. Springer, 2015, pp. 449-457.
[3] M. Du, F. Li, G. Zheng, and V. Srikumar, "Deeplog: Anomaly detection and diagnosis from system logs through deep learning," in Proceedings of the 2017 ACM SIGSAC conference on computer and communications security, 2017, pp. 1285-1298.
[4] W. Meng, Y. Liu, Y. Zhu, S. Zhang, D. Pei, Y. Liu, Y. Chen, R. Zhang, S. Tao, P. Sun et al., "Loganomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs." in IJCAI, vol. 19, no. 7, 2019, pp. 4739-4745.
[5] S. He, P. He, Z. Chen, T. Yang, Y. Su, and M. R. Lyu, "A survey on automated log analysis for reliability engineering," ACM computing surveys (CSUR), vol. 54, no. 6, pp. 1-37, 2021.
[6] S. Lu, X. Wei, Y. Li, and L. Wang, "Detecting anomaly in big data system logs using convolutional neural network," in 2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress (DASC/PiCom/DataCom/CyberSciTech). IEEE, 2018, pp. $151-158$.
[7] S. Huang, Y. Liu, C. Fung, R. He, Y. Zhao, H. Yang, and Z. Luan, "Hitanomaly: Hierarchical transformers for anomaly detection in system log," IEEE Transactions on Network and Service Management, vol. 17, no. 4, pp. 2064-2076, 2020.
[8] L. Cheng, X. Li, and L. Bing, "Is gpt-4 a good data analyst?" arXiv preprint arXiv:2305.15038, 2023.
[9] S. Frieder, L. Pinchetti, R.-R. Griffiths, T. Salvatori, T. Lukasiewicz, P. C. Petersen, A. Chevalier, and J. Berner, "Mathematical capabilities of chatgpt," arXiv preprint arXiv:2301.13867, 2023.
[10] N. Chen, Y. Wang, H. Jiang, D. Cai, Z. Chen, and J. Li, "What would harry say? building dialogue agents for characters in a story," arXiv preprint arXiv:2211.06869, 2022.
[11] W. Jiao, W. Wang, J.-t. Huang, X. Wang, and Z. Tu, "Is chatgpt a good translator? a preliminary study," arXiv preprint arXiv:2301.08745, 2023.
[12] J. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang, "Is chatgpt a good recommender? a preliminary study," arXiv preprint arXiv:2304.10149, 2023.
[13] V.-H. Le and H. Zhang, "An evaluation of log parsing with chatgpt," arXiv preprint arXiv:2306.01590, 2023.
[14] X. Zhang, Y. Xu, Q. Lin, B. Qiao, H. Zhang, Y. Dang, C. Xie, X. Yang, Q. Cheng, Z. Li et al., "Robust log-based anomaly detection on unstable log data," in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2019, pp. 807-817.
[15] V.-H. Le and H. Zhang, "Log-based anomaly detection with deep learning: How far are we?" in Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 1356-1367.
[16] P. He, J. Zhu, Z. Zheng, and M. R. Lyu, "Drain: An online log parsing approach with fixed depth tree," in 2017 IEEE international conference on web services (ICWS). IEEE, 2017, pp. 33-40.
[17] M. Du and F. Li, "Spell: Streaming parsing of system event logs," in 2016 IEEE 16th International Conference on Data Mining (ICDM). IEEE, 2016, pp. 859-864.
[18] S. Huang, Y. Liu, C. Fung, R. He, Y. Zhao, H. Yang, and Z. Luan, "Paddy: An event log parsing approach using dynamic dictionary," in NOMS 2020-2020 IEEE/IFIP Network Operations and Management Symposium. IEEE, 2020, pp. 1-8.
[19] C. Zhang, X. Peng, C. Sha, K. Zhang, Z. Fu, X. Wu, Q. Lin, and D. Zhang, "Deeptralog: Trace-log combined microservice anomaly detection through graph-based deep learning," in Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 623-634.
[20] P. Zhou, Y. Wang, Z. Li, X. Wang, G. Tyson, and G. Xie, "Logsayer: Log pattern-driven cloud component anomaly diagnosis with machine learning," in 2020 IEEE/ACM 28th International Symposium on Quality of Service (IWQoS). IEEE, 2020, pp. 1-10.
[21] Q. Du, L. Zhao, J. Xu, Y. Han, and S. Zhang, "Log-based anomaly detection with multi-head scaled dot-product attention mechanism," in Database and Expert Systems Applications: 32nd International Conference, DEXA 2021, Virtual Event, September 27-30, 2021, Proceedings, Part I 32. Springer, 2021, pp. 335-347.
[22] L. Zhang, W. Li, Z. Zhang, Q. Lu, C. Hou, P. Hu, T. Gui, and S. Lu, "Logatin: Unsupervised log anomaly detection with an autoencoder based attention mechanism," in Knowledge Science, Engineering and Management: 14th International Conference, KSEM 2021, Tokyo, Japan, August 14-16, 2021, Proceedings, Part III. Springer, 2021, pp. 222235.
[23] Z. Zhao, W. Niu, X. Zhang, R. Zhang, Z. Yu, and C. Huang, "Trine: Syslog anomaly detection with three transformer encoders in one generative adversarial network," Applied Intelligence, vol. 52, no. 8, pp. 8810-8819, 2022.
[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04005, 2018.
[25] K. Otomo, S. Kobayashi, K. Fukuda, and H. Esaki, "Latent variable based anomaly detection in network system logs," IEICE TRANSACTIONS on Information and Systems, vol. 102, no. 9, pp. 1644-1652, 2019.
[26] S. Bursic, V. Cuculo, and A. D'Amelio, "Anomaly detection from log files using unsupervised deep learning," in Formal Methods. FM 2019 International Workshops: Porto, Portugal, October 7-11, 2019, Revised Selected Papers, Part I 3. Springer, 2020, pp. 200-207.
[27] X. Han and S. Yuan, "Unsupervised cross-system log anomaly detection via domain adaptation," in Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management, 2021, pp. 30683072.
[28] S. Nedelkoski, J. Bogatinovski, A. Acker, J. Cardoso, and O. Kao, "Selfattentive classification-based anomaly detection in unstructured logs," in 2020 IEEE International Conference on Data Mining (ICDM). IEEE, 2020, pp. 1196-1201.
[29] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., "Improving language understanding by generative pre-training," 2018.
[30] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, "Exploring the limits of transfer learning with a unified text-to-text transformer," The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485-5551, 2020.
[31] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.
[32] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., "Training language models to follow instructions with human feedback," Advances in Neural Information Processing Systems, vol. 35, pp. 27730-27744, 2022.
[33] OpenAI, "Gpt-4 technical report," 2023.
[34] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozire, N. Goyal, E. Hambro, F. Azhar et al., "Llama: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023.
[35] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., "Palm: Scaling language modeling with pathways," arXiv preprint arXiv:2204.02311, 2022.
[36] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., "Evaluating large language models trained on code," arXiv preprint arXiv:2107.03374, 2021.
[37] A. Borji, "Generated faces in the wild: Quantitative comparison of stable diffusion, midjourney and dall-e 2," arXiv preprint arXiv:2210.00586, 2022.</p>
<h2>VIII. APPENDIX</h2>
<p>More experimental results on both BGL and Spirit datasets are shown in Figure 7 and Figure 8. The window size is increased from 10 to 50 and all three types of input sequences are evaluated. The gpt-3.5-turbo model is used in our experiments. Our source code and detailed experimental data will be made available with the publication of this work.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7. BGL. First row: Prompt-1 (zero-shot); Second row: Prompt-2 (zero-shot); Third row: Prompt-1 (few-shot); Last row: Prompt-2 (few-shot).
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8. Spirit. First row: Prompt-1 (zero-shot); Second row: Prompt-2 (zero-shot); Third row: Prompt-1 (few-shot); Last row: Prompt-2 (few-shot).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Jiaxing Qi, Shaohan Huang, Zhongzhi Luan, Hailong Yang, and Depei Qian are with the Sino-German Joint Software Institute, Beihang University, Beijing, 100191, China.
E-mail: {jiaxingqi, luan.zhongzhi}@buaa.edu.cn.
Carol Fung is with the Concordia Institute for Information Systems Engineering, Concordia University, Quebec, Canada.
(Zhongzhi Luan is the corresponding author for this work.)&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
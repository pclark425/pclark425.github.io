<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1542 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1542</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1542</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-79ab3c49903ec8cb339437ccf5cf998607fc313e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/79ab3c49903ec8cb339437ccf5cf998607fc313e" target="_blank">A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Artificial Intelligence and Statistics</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting and demonstrates that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.</p>
                <p><strong>Paper Abstract:</strong> Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1542",
    "paper_id": "paper-79ab3c49903ec8cb339437ccf5cf998607fc313e",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0045734999999999994,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning</h1>
<p>Stéphane Ross<br>Robotics Institute<br>Carnegie Mellon University<br>Pittsburgh, PA 15213, USA<br>stephaneross@cmu.edu</p>
<p>Geoffrey J. Gordon<br>Machine Learning Department<br>Carnegie Mellon University<br>Pittsburgh, PA 15213, USA<br>ggordon@cs.cmu.edu</p>
<p>J. Andrew Bagnell<br>Robotics Institute<br>Carnegie Mellon University<br>Pittsburgh, PA 15213, USA<br>dbagnell@ri.cmu.edu</p>
<h4>Abstract</h4>
<p>Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches (Daumé III et al., 2009; Ross and Bagnell, 2010) provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.</p>
<h2>1 INTRODUCTION</h2>
<p>Sequence Prediction problems arise commonly in practice. For instance, most robotic systems must be able to predict/make a sequence of actions given a sequence of observations revealed to them over time. In complex robotic systems where standard control methods fail, we must often resort to learning a controller that can make such predictions. Imitation learning techniques, where expert demon-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>strations of good behavior are used to learn a controller, have proven very useful in practice and have led to state-of-the art performance in a variety of applications (Schaal, 1999; Abbeel and Ng, 2004; Ratliff et al., 2006; Silver et al., 2008; Argall et al., 2009; Chernova and Veloso, 2009; Ross and Bagnell, 2010). A typical approach to imitation learning is to train a classifier or regressor to predict an expert's behavior given training data of the encountered observations (input) and actions (output) performed by the expert. However since the learner's prediction affects future input observations/states during execution of the learned policy, this violate the crucial i.i.d. assumption made by most statistical learning approaches.</p>
<p>Ignoring this issue leads to poor performance both in theory and practice (Ross and Bagnell, 2010). In particular, a classifier that makes a mistake with probability $\epsilon$ under the distribution of states/observations encountered by the expert can make as many as $T^{2} \epsilon$ mistakes in expectation over $T$-steps under the distribution of states the classifier itself induces (Ross and Bagnell, 2010). Intuitively this is because as soon as the learner makes a mistake, it may encounter completely different observations than those under expert demonstration, leading to a compounding of errors.</p>
<p>Recent approaches (Ross and Bagnell, 2010) can guarantee an expected number of mistakes linear (or nearly so) in the task horizon $T$ and error $\epsilon$ by training over several iterations and allowing the learner to influence the input states where expert demonstration is provided (through execution of its own controls in the system). One approach (Ross and Bagnell, 2010) learns a non-stationary policy by training a different policy for each time step in sequence, starting from the first step. Unfortunately this is impractical when $T$ is large or ill-defined. Another approach called SMILe (Ross and Bagnell, 2010), similar to SEARN (Daumé III et al., 2009) and CPI (Kakade and Langford, 2002), trains a stationary stochastic policy (a finite mixture of policies) by adding a new policy to the mixture at each iteration of training. However this may be unsatisfactory for practical applications as some policies in the mixture are worse than</p>
<p>others and the learned controller may be unstable.
We propose a new meta-algorithm for imitation learning which learns a stationary deterministic policy guaranteed to perform well under its induced distribution of states (number of mistakes/costs that grows linearly in $T$ and classification cost $\epsilon$ ). We take a reduction-based approach (Beygelzimer et al., 2005) that enables reusing existing supervised learning algorithms. Our approach is simple to implement, has no free parameters except the supervised learning algorithm sub-routine, and requires a number of iterations that scales nearly linearly with the effective horizon of the problem. It naturally handles continuous as well as discrete predictions. Our approach is closely related to no regret online learning algorithms (Cesa-Bianchi et al., 2004; Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008) (in particular Follow-The-Leader) but better leverages the expert in our setting. Additionally, we show that any no-regret learner can be used in a particular fashion to learn a policy that achieves similar guarantees.</p>
<p>We begin by establishing our notation and setting, discuss related work, and then present the DAGGER (Dataset Aggregation) method. We analyze this approach using a noregret and a reduction approach (Beygelzimer et al., 2005). Beyond the reduction analysis, we consider the sample complexity of our approach using online-to-batch (CesaBianchi et al., 2004) techniques. We demonstrate DAGGER is scalable and outperforms previous approaches in practice on two challenging imitation learning problems: 1) learning to steer a car in a 3D racing game (Super Tux Kart) and 2) and learning to play Super Mario Bros., given input image features and corresponding actions by a human expert and near-optimal planner respectively. Following Daumé III et al. (2009) in treating structured prediction as a degenerate imitation learning problem, we apply DAGGER to the OCR (Taskar et al., 2003) benchmark prediction problem achieving results competitive with the state-of-the-art (Taskar et al., 2003; Ratliff et al., 2007; Daumé III et al., 2009) using only single-pass, greedy prediction.</p>
<h2>2 PRELIMINARIES</h2>
<p>We begin by introducing notation relevant to our setting. We denote by $\Pi$ the class of policies the learner is considering and $T$ the task horizon. For any policy $\pi$, we let $d_{\pi}^{t}$ denote the distribution of states at time $t$ if the learner executed policy $\pi$ from time step 1 to $t-1$. Furthermore, we denote $d_{\pi}=\frac{1}{T} \sum_{t=1}^{T} d_{\pi}^{t}$ the average distribution of states if we follow policy $\pi$ for $T$ steps. Given a state $s$, we denote $C(s, a)$ the expected immediate cost of performing action $a$ in state $s$ for the task we are considering and denote $C_{\pi}(s)=\mathbb{E}<em t="1">{a \sim \pi(s)}[C(s, a)]$ the expected immediate cost of $\pi$ in $s$. We assume $C$ is bounded in $[0,1]$. The total cost of executing policy $\pi$ for $T$-steps (i.e., the cost-to-go) is denoted $J(\pi)=\sum</em>}^{T} \mathbb{E<em _pi="\pi">{s \sim d</em>}^{t}}\left[C_{\pi}(s)\right]=T \mathbb{E<em _pi="\pi">{s \sim d</em>(s)\right]$.}}\left[C_{\pi</p>
<p>In imitation learning, we may not necessarily know or observe true costs $C(s, a)$ for the particular task. Instead, we observe expert demonstrations and seek to bound $J(\pi)$ for any cost function $C$ based on how well $\pi$ mimics the expert's policy $\pi^{<em>}$. Denote $\ell$ the observed surrogate loss function we minimize instead of $C$. For instance $\ell(s, \pi)$ may be the expected 0-1 loss of $\pi$ with respect to $\pi^{</em>}$ in state $s$, or a squared/hinge loss of $\pi$ with respect to $\pi^{*}$ in $s$. Importantly, in many instances, $C$ and $\ell$ may be the same function-for instance, if we are interested in optimizing the learner's ability to predict the actions chosen by an expert.
Our goal is to find a policy $\hat{\pi}$ which minimizes the observed surrogate loss under its induced distribution of states, i.e.:</p>
<p>$$
\hat{\pi}=\underset{\pi \in \Pi}{\arg \min } \mathbb{E}<em _pi="\pi">{s \sim d</em>[\ell(s, \pi)]
$$}</p>
<p>As system dynamics are assumed both unknown and complex, we cannot compute $d_{\pi}$ and can only sample it by executing $\pi$ in the system. Hence this is a non-i.i.d. supervised learning problem due to the dependence of the input distribution on the policy $\pi$ itself. The interaction between policy and the resulting distribution makes optimization difficult as it results in a non-convex objective even if the loss $\ell(s, \cdot)$ is convex in $\pi$ for all states $s$. We now briefly review previous approaches and their guarantees.</p>
<h3>2.1 Supervised Approach to Imitation</h3>
<p>The traditional approach to imitation learning ignores the change in distribution and simply trains a policy $\pi$ that performs well under the distribution of states encountered by the expert $d_{\pi^{*}}$. This can be achieved using any standard supervised learning algorithm. It finds the policy $\hat{\pi}_{s u p}$ :</p>
<p>$$
\hat{\pi}<em _sim="\sim" d__pi_="d_{\pi^{*" s="s">{s u p}=\underset{\pi \in \Pi}{\arg \min } \mathbb{E}</em>[\ell(s, \pi)]
$$}}</p>
<p>Assuming $\ell(s, \pi)$ is the 0-1 loss (or upper bound on the 01 loss) implies the following performance guarantee with respect to any task cost function $C$ bounded in $[0,1]$ :
Theorem 2.1. (Ross and Bagnell, 2010) Let $\mathbb{E}<em>{s \sim d</em>{\pi^{<em>}}}[\ell(s, \pi)]=\epsilon$, then $J(\pi) \leq J\left(\pi^{</em>}\right)+T^{2} \epsilon$.</p>
<p>Proof. Follows from result in Ross and Bagnell (2010) since $\epsilon$ is an upper bound on the $0-1$ loss of $\pi$ in $d_{\pi^{*}}$.</p>
<p>Note that this bound is tight, i.e. there exist problems such that a policy $\pi$ with $\epsilon 0-1$ loss on $d_{\pi^{*}}$ can incur extra cost that grows quadratically in $T$. Kääriäinen (2006) demonstrated this in a sequence prediction setting ${ }^{1}$ and</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Ross and Bagnell (2010) provided an imitation learning example where $J\left(\hat{\pi}_{s u p}\right)=(1-\epsilon T) J\left(\pi^{*}\right)+T^{2} \epsilon$. Hence the traditional supervised learning approach has poor performance guarantees due to the quadratic growth in $T$. Instead we would prefer approaches that can guarantee growth linear or near-linear in $T$ and $\epsilon$. The following two approaches from Ross and Bagnell (2010) achieve this on some classes of imitation learning problems, including all those where surrogate loss $\ell$ upper bounds $C$.</p>
<h3>2.2 Forward Training</h3>
<p>The forward training algorithm introduced by Ross and Bagnell (2010) trains a non-stationary policy (one policy $\pi_{t}$ for each time step $t$ ) iteratively over $T$ iterations, where at iteration $t, \pi_{t}$ is trained to mimic $\pi^{*}$ on the distribution of states at time $t$ induced by the previously trained policies $\pi_{1}, \pi_{2}, \ldots, \pi_{t-1}$. By doing so, $\pi_{t}$ is trained on the actual distribution of states it will encounter during execution of the learned policy. Hence the forward algorithm guarantees that the expected loss under the distribution of states induced by the learned policy matches the average loss during training, and hence improves performance.</p>
<p>We here provide a theorem slightly more general than the one provided by Ross and Bagnell (2010) that applies to any policy $\pi$ that can guarantee $\epsilon$ surrogate loss under its own distribution of states. This will be useful to bound the performance of our new approach presented in Section 3.</p>
<p>Let $Q_{t}^{\pi^{\prime}}(s, \pi)$ denote the $t$-step cost of executing $\pi$ in initial state $s$ and then following policy $\pi^{\prime}$ and assume $\ell(s, \pi)$ is the 0-1 loss (or an upper bound on the $0-1$ loss), then we have the following performance guarantee with respect to any task cost function $C$ bounded in $[0,1]$ :
Theorem 2.2. Let $\pi$ be such that $\mathbb{E}<em _pi="\pi">{s \sim d</em>\left(s, \pi^{}}[\ell(s, \pi)]=\epsilon$, and $Q_{T-t+1}^{\pi^{\prime}}(s, a)-Q_{T-t+1}^{\pi^{\prime}<em>}\right) \leq u$ for all action $a, t \in$ ${1,2, \ldots, T}, d_{\pi}^{t}(s)&gt;0$, then $J(\pi) \leq J\left(\pi^{</em>}\right)+u T \epsilon$.</p>
<p>Proof. We here follow a similar proof to Ross and Bagnell (2010). Given our policy $\pi$, consider the policy $\pi_{1: t}$, which executes $\pi$ in the first $t$-steps and then execute the expert $\pi^{*}$. Then</p>
<p>$$
\begin{aligned}
&amp; J(\pi) \
&amp; =J\left(\pi^{<em>}\right)+\sum_{t=0}^{T-1}\left[J\left(\pi_{1: T-t}\right)-J\left(\pi_{1: T-t-1}\right)\right] \
&amp; =J\left(\pi^{</em>}\right)+\sum_{t=1}^{T} \mathbb{E}<em _pi="\pi">{s \sim d</em>^{\pi^{}^{t}}\left[Q_{T-t+1<em>}}(s, \pi)-Q_{T-t+1}^{\pi^{</em>}}\left(s, \pi^{<em>}\right)\right] \
&amp; \leq J\left(\pi^{</em>}\right)+u \sum_{t=1}^{T} \mathbb{E}<em _pi="\pi">{s \sim d</em>[\ell(s, \pi)] \
&amp; =J\left(\pi^{*}\right)+u T \epsilon
\end{aligned}
$$}^{t}</p>
<p>The inequality follows from the fact that $\ell(s, \pi)$ upper bounds the $0-1$ loss, and hence the probability $\pi$ and $\pi^{*}$ pick different actions in $s$; when they pick different actions, the increase in cost-to-go $\leq u$.</p>
<p>In the worst case, $u$ could be $O(T)$ and the forward algorithm wouldn't provide any improvement over the tra-
ditional supervised learning approach. However, in many cases $u$ is $O(1)$ or sub-linear in $T$ and the forward algorithm leads to improved performance. For instance if $C$ is the $0-1$ loss with respect to the expert, then $u \leq 1$. Additionally if $\pi^{<em>}$ is able to recover from mistakes made by $\pi$, in the sense that within a few steps, $\pi^{</em>}$ is back in a distribution of states that is close to what $\pi^{<em>}$ would be in if $\pi^{</em>}$ had been executed initially instead of $\pi$, then $u$ will be $O(1) .{ }^{2} \mathrm{~A}$ drawback of the forward algorithm is that it is impractical when $T$ is large (or undefined) as we must train $T$ different policies sequentially and cannot stop the algorithm before we complete all $T$ iterations. Hence it can not be applied to most real-world applications.</p>
<h3>2.3 Stochastic Mixing Iterative Learning</h3>
<p>SMILe, proposed by Ross and Bagnell (2010), alleviates this problem and can be applied in practice when $T$ is large or undefined by adopting an approach similar to SEARN (Daumé III et al., 2009) where a stochastic stationary policy is trained over several iterations. Initially SMILe starts with a policy $\pi_{0}$ which always queries and executes the expert's action choice. At iteration $n$, a policy $\hat{\pi}<em n-1="n-1">{n}$ is trained to mimic the expert under the distribution of trajectories $\pi</em>}$ induces and then updates $\pi_{n}=$ $\pi_{n-1}+\alpha(1-\alpha)^{n-1}\left(\hat{\pi<em 0="0">{n}-\pi</em>}\right)$. This update is interpreted as adding probability $\alpha(1-\alpha)^{n-1}$ to executing policy $\hat{\pi<em n="n">{n}$ at any step and removing probability $\alpha(1-\alpha)^{n-1}$ of executing the queried expert's action. At iteration $n, \pi</em>}$ is a mixture of $n$ policies and the probability of using the queried expert's action is $(1-\alpha)^{n}$. We can stop the algorithm at any iteration $N$ by returning the re-normalized policy $\hat{\pi<em N="N">{N}=\frac{\pi</em> \log T\right)$ guarantees near-linear regret in $T$ and $\epsilon$ for some class of problems.}-(1-\alpha)^{N} \pi_{0}}{1-(1-\alpha)^{N}}$ which doesn't query the expert anymore. Ross and Bagnell (2010) showed that choosing $\alpha$ in $O\left(\frac{1}{T^{2}}\right)$ and $N$ in $O\left(T^{2</p>
<h2>3 DATASET AGGREGATION</h2>
<p>We now present DAGGER (Dataset Aggregation), an iterative algorithm that trains a deterministic policy that achieves good performance guarantees under its induced distribution of states.</p>
<p>In its simplest form, the algorithm proceeds as follows. At the first iteration, it uses the expert's policy to gather a dataset of trajectories $\mathcal{D}$ and train a policy $\hat{\pi}<em n="n">{2}$ that best mimics the expert on those trajectories. Then at iteration $n$, it uses $\hat{\pi}</em>$.}$ to collect more trajectories and adds those trajectories to the dataset $\mathcal{D}$. The next policy $\hat{\pi}_{n+1}$ is the policy that best mimics the expert on the whole dataset $\mathcal{D</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Initialize $\mathcal{D} \leftarrow \emptyset$.
Initialize $\hat{\pi}<em i="i">{1}$ to any policy in $\Pi$.
for $i=1$ to $N$ do
Let $\pi</em> \pi^{}=\beta_{i<em>}+\left(1-\beta_{i}\right) \hat{\pi}<em i="i">{i}$.
Sample $T$-step trajectories using $\pi</em>$.
Get dataset $\mathcal{D}_{i}=\left{\left(s, \pi^{</em>}(s)\right)\right}$ of visited states by $\pi_{i}$ and actions given by expert.
Aggregate datasets: $\mathcal{D} \leftarrow \mathcal{D} \bigcup \mathcal{D}<em i_1="i+1">{i}$.
Train classifier $\hat{\pi}</em>$.
end for
Return best $\hat{\pi}_{i}$ on validation.
Algorithm 3.1: DAGGER Algorithm.}$ on $\mathcal{D</p>
<p>In other words, DAGGER proceeds by collecting a dataset at each iteration under the current policy and trains the next policy under the aggregate of all collected datasets. The intuition behind this algorithm is that over the iterations, we are building up the set of inputs that the learned policy is likely to encounter during its execution based on previous experience (training iterations). This algorithm can be interpreted as a Follow-The-Leader algorithm in that at iteration $n$ we pick the best policy $\hat{\pi}_{n+1}$ in hindsight, i.e. under all trajectories seen so far over the iterations.</p>
<p>To better leverage the presence of the expert in our imitation learning setting, we optionally allow the algorithm to use a modified policy $\pi_{i}=\beta_{i} \pi^{*}+\left(1-\beta_{i}\right) \pi_{i}$ at iteration $i$ that queries the expert to choose controls a fraction of the time while collecting the next dataset. This is often desirable in practice as the first few policies, with relatively few datapoints, may make many more mistakes and visit states that are irrelevant as the policy improves.</p>
<p>We will typically use $\beta_{1}=1$ so that we do not have to specify an initial policy $\hat{\pi}<em i="i">{1}$ before getting data from the expert's behavior. Then we could choose $\beta</em>}=p^{i-1}$ to have a probability of using the expert that decays exponentially as in SMILe and SEARN. We show below the only requirement is that $\left{\beta_{i}\right}$ be a sequence such that $\widehat{\beta<em i="1">{N}=\frac{1}{N} \sum</em>=\min }^{N} \beta_{i} \rightarrow 0$ as $N \rightarrow \infty$. The simple, parameter-free version of the algorithm described above is the special case $\beta_{i}=I(i=1)$ for $I$ the indicator function, which often performs best in practice (see Section 5). The general DAGGER algorithm is detailed in Algorithm 3.1. The main result of our analysis in the next section is the following guarantee for DAGGER. Let $\pi_{1: N}$ denote the sequence of policies $\pi_{1}, \pi_{2}, \ldots, \pi_{N}$. Assume $\ell$ is strongly convex and bounded over $\Pi$. Suppose $\beta_{i} \leq(1-\alpha)^{i-1}$ for all $i$ for some constant $\alpha$ independent of $T$. Let $\epsilon_{N<em i="1">{\pi \in \Pi} \frac{1}{N} \sum</em>}^{N} \mathbb{E<em _pi__i="\pi_{i">{s \sim d</em>[\ell(s, \pi)]$ be the true loss of the best policy in hindsight. Then the following holds in the infinite sample case (infinite number of sample trajectories at each iteration):}}</p>
<p>Theorem 3.1. For DAGGER, if $N$ is $\tilde{O}(T)$ there exists a policy $\hat{\pi} \in \hat{\pi}<em _sim="\sim" d__hat_pi="d_{\hat{\pi" s="s">{1: N}$ s.t. $\mathbb{E}</em>+O(1 / T)$}}}[\ell(s, \hat{\pi})] \leq \epsilon_{N</p>
<p>In particular, this holds for the policy $\hat{\pi}=$ $\arg \min <em 1:="1:" N="N">{\pi \in \hat{\pi}</em>}} \mathbb{E<em _pi="\pi">{s \sim d</em>+O(1)$. For arbitrary task cost function $C$, then if $\ell$ is an upper bound on the $0-1$ loss with respect to $\pi^{}}[\ell(s, \pi)] . \quad$ If the task cost function $C$ corresponds to (or is upper bounded by) the surrogate loss $\ell$ then this bound tells us directly that $J(\hat{\pi}) \leq T \epsilon_{N<em>}$, combining this result with Theorem 2.2 yields that:
Theorem 3.2. For DAGGER, if $N$ is $\tilde{O}(u T)$ there exists a policy $\hat{\pi} \in \hat{\pi}_{1: N}$ s.t. $J(\hat{\pi}) \leq J\left(\pi^{</em>}\right)+u T \epsilon_{N}+O(1)$.</p>
<p>Finite Sample Results In the finite sample case, suppose we sample $m$ trajectories with $\pi_{i}$ at each iteration $i$, and denote this dataset $D_{i}$. Let $\hat{\epsilon}<em _Pi="\Pi" _in="\in" _pi="\pi">{N}=$ $\min </em>} \frac{1}{N} \sum_{i=1}^{N} \mathbb{E<em i="i">{s \sim D</em>[\ell(s, \pi)]$ be the training loss of the best policy on the sampled trajectories, then using AzumaHoeffding's inequality leads to the following guarantee:
Theorem 3.3. For DAGGER, if $N$ is $O\left(T^{2} \log (1 / \delta)\right)$ and $m$ is $O(1)$ then with probability at least $1-\delta$ there exists a policy $\hat{\pi} \in \hat{\pi}}<em _sim="\sim" d__hat_pi="d_{\hat{\pi" s="s">{1: N}$ s.t. $\mathbb{E}</em>+O(1 / T)$}}}[\ell(s, \hat{\pi})] \leq \hat{\epsilon}_{N</p>
<p>A more refined analysis taking advantage of the strong convexity of the loss function (Kakade and Tewari, 2009) may lead to tighter generalization bounds that require $N$ only of order $\tilde{O}(T \log (1 / \delta))$. Similarly:
Theorem 3.4. For DAGGER, if $N$ is $O\left(u^{2} T^{2} \log (1 / \delta)\right)$ and $m$ is $O(1)$ then with probability at least $1-\delta$ there exists a policy $\hat{\pi} \in \hat{\pi}<em N="N">{1: N}$ s.t. $J(\hat{\pi}) \leq J\left(\pi^{*}\right)+u T \hat{\epsilon}</em>+O(1)$.</p>
<h2>4 THEORETICAL ANALYSIS</h2>
<p>The theoretical analysis of DAGGER only relies on the noregret property of the underlying Follow-The-Leader algorithm on strongly convex losses (Kakade and Tewari, 2009) which picks the sequence of policies $\hat{\pi}_{1: N}$. Hence the presented results also hold for any other no regret online learning algorithm we would apply to our imitation learning setting. In particular, we can consider the results here a reduction of imitation learning to no-regret online learning where we treat mini-batches of trajectories under a single policy as a single online-learning example. We first briefly review concepts of online learning and no regret that will be used for this analysis.</p>
<h3>4.1 Online Learning</h3>
<p>In online learning, an algorithm must provide a policy $\pi_{n}$ at iteration $n$ which incurs a loss $\ell_{n}\left(\pi_{n}\right)$. After observing this loss, the algorithm can provide a different policy $\pi_{n+1}$ for the next iteration which will incur loss $\ell_{n+1}\left(\pi_{n+1}\right)$. The</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>loss functions $\ell_{n+1}$ may vary in an unknown or even adversarial fashion over time. A no-regret algorithm is an algorithm that produces a sequence of policies $\pi_{1}, \pi_{2}, \ldots, \pi_{N}$ such that the average regret with respect to the best policy in hindsight goes to 0 as $N$ goes to $\infty$ :</p>
<p>$$
\frac{1}{N} \sum_{i=1}^{N} \ell_{i}\left(\pi_{i}\right)-\min <em i="1">{\pi \in \Pi} \frac{1}{N} \sum</em>
$$}^{N} \ell_{i}(\pi) \leq \gamma_{N</p>
<p>for $\lim <em N="N">{N \rightarrow \infty} \gamma</em>\right)$ (e.g. when $\ell$ is strongly convex) (Hazan et al., 2006; Kakade and Shalev-Shwartz, 2008; Kakade and Tewari, 2009).}=0$. Many no-regret algorithms guarantee that $\gamma_{N}$ is $\tilde{O}\left(\frac{1}{N</p>
<h3>4.2 No Regret Algorithms Guarantees</h3>
<p>Now we show that no-regret algorithms can be used to find a policy which has good performance guarantees under its own distribution of states in our imitation learning setting. To do so, we must choose the loss functions to be the loss under the distribution of states of the current policy chosen by the online algorithm: $\ell_{i}(\pi)=\mathbb{E}<em _pi__i="\pi_{i">{s \sim d</em>[\ell(s, \pi)]$.}}</p>
<p>For our analysis of DAGGER, we need to bound the total variation distance between the distribution of states encountered by $\hat{\pi}<em i="i">{i}$ and $\pi</em>$, which continues to call the expert. The following lemma is useful:
Lemma 4.1. $\left|d_{\pi_{i}}-d_{\hat{\pi}<em 1="1">{i}}\right|</em>$.
Proof. Let $d$ the distribution of states over $T$ steps conditioned on $\pi_{i}$ picking $\pi^{*}$ at least once over $T$ steps. Since $\pi_{i}$ always executes $\hat{\pi}} \leq 2 T \beta_{i<em i="i">{i}$ over $T$ steps with probability $\left(1-\beta</em>}\right)^{T}$ we have $d_{\pi_{i}}=\left(1-\beta_{i}\right)^{T} d_{\hat{\pi<em i="i">{i}}+\left(1-\left(1-\beta</em>\right) d$. Thus}\right)^{T</p>
<p>$$
\begin{aligned}
&amp; \left|d_{\pi_{i}}-d_{\hat{\pi}<em 1="1">{i}}\right|</em> \
&amp; =\left(1-\left(1-\beta_{i}\right)^{T}\right)\left|d-d_{\hat{\pi}<em 1="1">{i}}\right|</em> \
&amp; \leq 2\left(1-\left(1-\beta_{i}\right)^{T}\right) \
&amp; \leq 2 T \beta_{i}
\end{aligned}
$$</p>
<p>The last inequality follows from the fact that $(1-\beta)^{T} \geq$ $1-\beta T$ for any $\beta \in[0,1]$.</p>
<p>This is only better than the trivial bound $\left|d_{\pi_{i}}-d_{\hat{\pi}<em 1="1">{i}}\right|</em>=$ $\min } \leq 2$ for $\beta_{i} \leq \frac{1}{T}$. Assume $\beta_{i}$ is non-increasing and define $n_{\beta}$ the largest $n \leq N$ such that $\beta_{n}&gt;\frac{1}{T}$. Let $\epsilon_{N<em i="1">{\pi \in \Pi} \frac{1}{N} \sum</em>}^{N} \mathbb{E<em _pi__i="\pi_{i">{s \sim d</em>}}}[\ell(s, \pi)]$ the loss of the best policy in hindsight after $N$ iterations and let $\ell_{\max }$ be an upper bound on the loss, i.e. $\ell_{i}\left(s, \hat{\pi<em _max="\max">{i}\right) \leq \ell</em>}$ for all policies $\hat{\pi<em _hat_pi="\hat{\pi">{i}$, and state $s$ such that $d</em><em 1:="1:" N="N">{i}}(s)&gt;0$. We have the following:
Theorem 4.1. For DAGGER, there exists a policy $\hat{\pi} \in$ $\hat{\pi}</em>}$ s.t. $\mathbb{E<em _hat_pi="\hat{\pi">{s \sim d</em>$.}}}[\ell(s, \hat{\pi})] \leq \epsilon_{N}+\gamma_{N}+\frac{2 \ell_{\max }}{N}\left[n_{\beta}+\right.$ $\left.T \sum_{i=n_{\beta}+1}^{N} \beta_{i}\right]$, for $\gamma_{N}$ the average regret of $\hat{\pi}_{1: N</p>
<p>Proof. The last lemma implies $\mathbb{E}<em _pi__i="\pi_{i">{s \sim d</em>}}}\left(\ell_{i}\left(s, \hat{\pi<em _sim="\sim" d__pi__i="d_{\pi_{i" s="s">{i}\right)\right) \leq$ $\mathbb{E}</em>}}}\left(\ell_{i}\left(s, \hat{\pi<em _max="\max">{i}\right)\right)+2 \ell</em>\right)$. Then:
$\min } \min \left(1, T \beta_{i<em 1:="1:" N="N">{\hat{\pi} \in \hat{\pi}</em>}} \mathbb{E<em _hat_pi="\hat{\pi">{s \sim d</em>)]$
$\leq \frac{1}{N} \sum_{i=1}^{N} \mathbb{E}}}}[\ell(s, \hat{\pi<em _pi__i="\pi_{i">{s \sim d</em>}}}\left(\ell\left(s, \hat{\pi<em i="1">{i}\right)\right)$
$\leq \frac{1}{N} \sum</em>}^{N}\left[\mathbb{E<em _pi__i="\pi_{i">{s \sim d</em>}}}\left(\ell\left(s, \hat{\pi<em _max="\max">{i}\right)\right)+2 \ell</em>\right)\right]$
$\leq \gamma_{N}+\frac{2 \ell_{\max }}{N}\left[n_{\beta}+T \sum_{i=n_{\beta}+1}^{N} \beta_{i}\right]+\min } \min \left(1, T \beta_{i<em i="1">{\pi \in \Pi} \sum</em>(\pi)$
$=\gamma_{N}+\epsilon_{N}+\frac{2 \ell_{\max }}{N}\left[n_{\beta}+T \sum_{i=n_{\beta}+1}^{N} \beta_{i}\right]$}^{N} \ell_{i</p>
<p>Under an error reduction assumption that for any input distribution, there is some policy $\pi \in \Pi$ that achieves surrogate loss of $\epsilon$, this implies we are guaranteed to find a policy $\hat{\pi}$ which achieves $\epsilon$ surrogate loss under its own state distribution in the limit, provided $\bar{\beta}<em i="i">{N} \rightarrow 0$. For instance, if we choose $\beta</em>$ negligible, the number of iterations required by DAGGER is similar to that required by any no-regret algorithm. Note that this is not as strong as the general error or regret reductions considered in (Beygelzimer et al., 2005; Ross and Bagnell, 2010; Daumé III et al., 2009) which require only classification: we require a no-regret method or strongly convex surrogate loss function, a stronger (albeit common) assumption.}$ to be of the form $(1-\alpha)^{i-1}$, then $\frac{1}{N}\left[n_{\beta}+T \sum_{i=n_{\beta}+1}^{N} \beta_{i}\right] \leq \frac{1}{N \alpha}[\log T+1]$ and this extra penalty becomes negligible for $N$ as $\tilde{O}(T)$. As we need at least $\tilde{O}(T)$ iterations to make $\gamma_{N</p>
<p>Finite Sample Case: The previous results hold if the online learning algorithm observes the infinite sample loss, i.e. the loss on the true distribution of trajectories induced by the current policy $\pi_{i}$. In practice however the algorithm would only observe its loss on a small sample of trajectories at each iteration. We wish to bound the true loss under its own distribution of the best policy in the sequence as a function of the regret on the finite sample of trajectories.</p>
<p>At each iteration $i$, we assume the algorithm samples $m$ trajectories using $\pi_{i}$ and then observes the loss $\ell_{i}(\pi)=$ $\mathbb{E}<em i="i">{s \sim D</em>}}(\ell(s, \pi))$, for $D_{i}$ the dataset of those $m$ trajectories. The online learner guarantees $\frac{1}{N} \sum_{i=1}^{N} \mathbb{E<em i="i">{s \sim D</em>\right)\right)-$ $\min }}\left(\ell\left(s, \pi_{i<em i="1">{\pi \in \Pi} \frac{1}{N} \sum</em>}^{N} \mathbb{E<em i="i">{s \sim D</em>}}(\ell(s, \pi)) \leq \gamma_{N}$. Let $\hat{\epsilon<em _Pi="\Pi" _in="\in" _pi="\pi">{N}=$ $\min </em>} \frac{1}{N} \sum_{i=1}^{N} \mathbb{E<em i="i">{s \sim D</em>[\ell(s, \pi)]$ the training loss of the best policy in hindsight. Following a similar analysis to Cesa-Bianchi et al. (2004), we obtain:
Theorem 4.2. For DAGGER, with probability at least $1-\delta$, there exists a policy $\hat{\pi} \in \hat{\pi}}<em _sim="\sim" d__hat_pi="d_{\hat{\pi" s="s">{1: N}$ s.t. $\mathbb{E}</em>}}}[\ell(s, \hat{\pi})] \leq \hat{\epsilon<em N="N">{N}+$ $\gamma</em>$.}+\frac{2 \ell_{\max }}{N}\left[n_{\beta}+T \sum_{i=n_{\beta}+1}^{N} \beta_{i}\right]+\ell_{\max } \sqrt{\frac{2 \log (1 / \delta)}{m N}}$, for $\gamma_{N}$ the average regret of $\hat{\pi}_{1: N</p>
<p>Proof. Let $Y_{i j}$ be the difference between the expected per step loss of $\hat{\pi}<em _pi__i="\pi_{i">{i}$ under state distribution $d</em>}}$ and the average per step loss of $\hat{\pi<em i="i">{i}$ under the $j^{t h}$ sample trajectory with $\pi</em> \leq$}$ at iteration $i$. The random variables $Y_{i j}$ over all $i \in{1,2, \ldots, N}$ and $j \in{1,2, \ldots, m}$ are all zero mean, bounded in $\left[-\ell_{\max }, \ell_{\max }\right]$ and form a martingale (considering the order $Y_{11}, Y_{12}, \ldots, Y_{1 m}, Y_{21}, \ldots, Y_{N m}$ ). By Azuma-Hoeffding's inequality $\frac{1}{m N} \sum_{i=1}^{N} \sum_{j=1}^{m} Y_{i j</p>
<p>$\ell_{\max } \sqrt{\frac{2 \log (1 / \delta)}{m N}}$ with probability at least $1-\delta$. Hence, we obtain that with probability at least $1-\delta$ :</p>
<p>$$
\begin{aligned}
&amp; \min <em 1="1">{\hat{\pi} \in \hat{\pi}</em>} N} \mathbb{E<em _hat_pi="\hat{\pi">{s \sim d</em><em i="1">{i}}}[\ell(s, \hat{\pi})] \
&amp; \leq \frac{1}{N} \sum</em>}^{N} \mathbb{E<em _hat_pi="\hat{\pi">{s \sim d</em><em i="i">{i}}}\left[\ell\left(s, \hat{\pi}</em>\right)\right] \
&amp; \leq \frac{1}{N} \sum_{i=1}^{N} \mathbb{E}<em _hat_pi="\hat{\pi">{s \sim d</em><em i="i">{i}}}\left[\ell\left(s, \hat{\pi}</em>\right] \
&amp;=\frac{1}{N} \sum_{i=1}^{N} \mathbb{E}}\right)\right]+\frac{2 \ell_{\max }}{N}\left[n_{\beta}+T \sum_{i=n_{\beta}+1}^{N} \beta_{i<em i="i">{s \sim D</em>}}\left[\ell\left(s, \hat{\pi<em i="1">{i}\right)\right]+\frac{1}{m N} \sum</em> \
&amp;+\frac{2 \ell_{\max }}{N}\left[n_{\beta}+T \sum_{i=n_{\beta}+1}^{N} \beta_{i}\right] \
&amp; \leq \frac{1}{N} \sum_{i=1}^{N} \mathbb{E}}^{N} \sum_{j=1}^{m} Y_{i j<em i="i">{s \sim D</em>}}\left[\ell\left(s, \hat{\pi<em _max="\max">{i}\right)\right]+\ell</em> \
&amp;+\frac{2 \ell_{\max }}{N}\left[n_{\beta}+T \sum_{i=n_{\beta}+1}^{N} \beta_{i}\right] \
&amp; \leq \hat{\epsilon}} \sqrt{\frac{2 \log (1 / \delta)}{m N}<em N="N">{N}+\gamma</em>\right]
\end{aligned}
$$}+\ell_{\max } \sqrt{\frac{2 \log (1 / \delta)}{m N}}+\frac{2 \ell_{\max }}{N}\left[n_{\beta}+T \sum_{i=n_{\beta}+1}^{N} \beta_{i</p>
<p>The use of Azuma-Hoeffding's inequality suggests we need $N m$ in $O\left(T^{2} \log (1 / \delta)\right)$ for the generalization error to be $O(1 / T)$ and negligible over $T$ steps. Leveraging the strong convexity of $\ell$ as in (Kakade and Tewari, 2009) may lead to a tighter bound requiring only $O(T \log (T / \delta))$ trajectories.</p>
<h2>5 EXPERIMENTS</h2>
<p>To demonstrate the efficacy and scalability of DAGGER, we apply it to two challenging imitation learning problems and a sequence labeling task (handwriting recognition).</p>
<h3>5.1 Super Tux Kart</h3>
<p>Super Tux Kart is a 3D racing game similar to the popular Mario Kart. Our goal is to train the computer to steer the kart moving at fixed speed on a particular race track, based on the current game image features as input (see Figure 1). A human expert is used to provide demonstrations of the correct steering (analog joystick value in $[-1,1]$ ) for each of the observed game images. For all methods, we use a linear
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Image from Super Tux Kart's Star Track.
controller as the base learner which updates the steering at 5 Hz based on the vector of image features ${ }^{4}$.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>We compare performance on a race track called Star Track. As this track floats in space, the kart can fall off the track at any point (the kart is repositioned at the center of the track when this occurs). We measure performance in terms of the average number of falls per lap. For SMILe and DAGGER, we used 1 lap of training per iteration ( $\sim 1000$ data points) and run both methods for 20 iterations. For SMILe we choose parameter $\alpha=0.1$ as in Ross and Bagnell (2010), and for DAGGER the parameter $\beta_{i}=I(i=1)$ for $I$ the indicator function. Figure 2 shows $95 \%$ confidence intervals on the average falls per lap of each method after $1,5,10,15$ and 20 iterations as a function of the total number of training data collected. We first observe that with the baseline
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Average falls/lap as a function of training data.
supervised approach where training always occurs under the expert's trajectories that performance does not improve as more data is collected. This is because most of the training laps are all very similar and do not help the learner to learn how to recover from mistakes it makes. With SMILe we obtain some improvements but the policy after 20 iterations still falls off the track about twice per lap on average. This is in part due to the stochasticity of the policy which sometimes makes bad choices of actions. For DAGGER, we were able to obtain a policy that never falls off the track after 15 iterations of training. Though even after 5 iterations, the policy we obtain almost never falls off the track and is significantly outperforming both SMILe and the baseline supervised approach. Furthermore, the policy obtained by DAGGER is smoother and looks qualitatively better than the policy obtained with SMILe. A video available on YouTube (Ross, 2010a) shows a qualitative comparison of the behavior obtained with each method.</p>
<h3>5.2 Super Mario Bros.</h3>
<p>Super Mario Bros. is a platform video game where the character, Mario, must move across each stage by avoid-</p>
<p>ing being hit by enemies and falling into gaps, and before running out of time. We used the simulator from a recent Mario Bros. AI competition (Togelius and Karakovskiy, 2009) which can randomly generate stages of varying difficulty (more difficult gaps and types of enemies). Our goal is to train the computer to play this game based on the current game image features as input (see Figure 3). Our expert in this scenario is a near-optimal planning algorithm that has full access to the game’s internal state and can simulate exactly the consequence of future actions. An action consists of 4 binary variables indicating which subset of buttons we should press in {left,right,jump,speed}. For</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Captured image from Super Mario Bros.</p>
<p>All methods, we use 4 independent linear SVM as the base learner which update the 4 binary actions at 5Hz based on the vector of image features<sup>5</sup>.</p>
<p>We compare performance in terms of the average distance travelled by Mario per stage before dying, running out of time or completing the stage, on randomly generated stages of difficulty 1 with a time limit of 60 seconds to complete the stage. The total distance of each stage varies but is around 4200-4300 on average, so performance can vary roughly in [0,4300]. Stages of difficulty 1 are fairly easy for an average human player but contain most types of enemies and gaps, except with fewer enemies and gaps than stages of harder difficulties. We compare performance of DAgger, SMILe and SEARN<sup>6</sup> to the supervised approach (Sup). With each approach we collect 5000 data points per iteration (each stage is about 150 data points if run to completion) and run the methods for 20 iterations. For SMILe we choose parameter α = 0.1 (Sm0.1) as in Ross and Bagnell (2010). For DAGGER we obtain results with different choice of the parameter β<sub>i</sub>: 1) β<sub>i</sub> = I(i = 1) for I the indicator function (D0); 2) β<sub>i</sub> = p<sup>i−1</sup> for all values of p ∈ {0.1, 0.2, . . . , 0.9}. We report the best results obtained with p = 0.5 (D0.5). We also report the results with p = 0.9 (D0.9) which shows the slower convergence of using the expert more frequently at later iterations. Similarly for SEARN, we obtain results with all choice of α in {0.1, 0.2, . . . , 1}. We report the best results obtained with α = 0.4 (Se0.4). We also report results with α = 1.0 (Se1), which shows the unstability of such a pure policy iteration approach. Figure 4 shows 95% confidence intervals on the average distance travelled per stage at each iteration as a function of the total number of training data collected. Again here we observe that with the supervised</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Average distance/stage as a function of data.</p>
<p>approach, performance stagnates as we collect more data from the expert demonstrations, as this does not help the particular errors the learned controller makes. In particular, a reason the supervised approach gets such a low score is that under the learned controller, Mario is often stuck at some location against an obstacle instead of jumping over it. Since the expert always jumps over obstacles at a significant distance away, the controller did not learn how to get unstuck in situations where it is right next to an obstacle. On the other hand, all the other iterative methods perform much better as they eventually learn to get unstuck in those situations by encountering them at the later iterations. Again in this experiment, DAGGER outperforms SMILe, and also outperforms SEARN for all choice of α we considered. When using β<sub>i</sub> = 0.9<sup>i−1</sup>, convergence is significantly slower could have benefited from more iterations as performance was still improving at the end of the 20 iterations. Choosing 0.5<sup>i−1</sup> yields slightly better performance (3030) than with the indicator function (2980). This is potentially due to the large number of data generated where Mario is stuck at the same location in the early iterations when using the indicator; whereas using the ex-</p>
<p><sup>5</sup>For the input features x: each image is discretized in a grid of 22x22 cells centered around Mario; 14 binary features describe each cell (types of ground, enemies, blocks and other special items); a history of those features over the last 4 images is used, in addition to other features describing the last 6 actions and the state of Mario (small, big, fire, touches ground), for a total of 27152 binary features (very sparse). The k<sup>th</sup> output binary variable ŷ<sub>k</sub> = I(wk<sup>T</sup> x + b<sub>k</sub> &gt; 0), where w<sub>k</sub>, b<sub>k</sub> optimizes the SVM objective with regularizer λ = 10<sup>−4</sup> using stochastic gradient descent (Ratliff et al., 2007; Bottou, 2009).</p>
<p><sup>6</sup>We use the same cost-to-go approximation in Daumé III et al. (2009); in this case SMILe and SEARN differ only in how the weights in the mixture are updated at each iteration.</p>
<p>pert a small fraction of the time still allows to observe those locations but also unstucks mario and makes it collect a wider variety of useful data. A video available on YouTube (Ross, 2010b) also shows a qualitative comparison of the behavior obtained with each method.</p>
<h3>5.3 Handwriting Recognition</h3>
<p>Finally, we demonstrate the efficacy of our approach on a structured prediction problem involving recognizing handwritten words given the sequence of images of each character in the word. We follow Daumé III et al. (2009) in adopting a view of structured prediction as a degenerate form of imitation learning where the system dynamics are deterministic and trivial in simply passing on earlier predictions made as inputs for future predictions. We use the dataset of Taskar et al. (2003) which has been used extensively in the literature to compare several structured prediction approaches. This dataset contains roughly 6600 words (for a total of over 52000 characters) partitioned in 10 folds. We consider the large dataset experiment which consists of training on 9 folds and testing on 1 fold and repeating this over all folds. Performance is measured in terms of the character accuracy on the test folds.</p>
<p>We consider predicting the word by predicting each character in sequence in a left to right order, using the previously predicted character to help predict the next and a linear SVM, following the greedy SEARN approach in Daumé III et al. (2009). Here we compare our method to SMILe, as well as SEARN (using the same approximations used in Daumé III et al. (2009)). We also compare these approaches to two baseline, a non-structured approach which simply predicts each character independently and the supervised training approach where training is conducted with the previous character always correctly labeled. Again we try all choice of $\alpha \in{0.1,0.2, \ldots, 1}$ for SEARN, and report results for $\alpha=0.1, \alpha=1$ (pure policy iteration) and the best $\alpha=0.8$, and run all approaches for 20 iterations. Figure 5 shows the performance of each approach on the test folds after each iteration as a function of training data. The baseline result without structure achieves $82 \%$ character accuracy by just using an SVM that predicts each character independently. When adding the previous character feature, but training with always the previous character correctly labeled (supervised approach), performance increases up to $83.6 \%$. Using DAgger increases performance further to $85.5 \%$. Surprisingly, we observe SEARN with $\alpha=1$, which is a pure policy iteration approach performs very well on this experiment, similarly to the best $\alpha=0.8$ and DAgger. Because there is only a small part of the input that is influenced by the current policy (the previous</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Character accuracy as a function of iteration.
predicted character feature) this makes this approach not as unstable as in general reinforcement/imitation learning problems (as we saw in the previous experiment). SEARN and SMILe with small $\alpha=0.1$ performs similarly but significantly worse than DAgger. Note that we chose the simplest (greedy, one-pass) decoding to illustrate the benefits of the DAGGER approach with respect to existing reductions. Similar techniques can be applied to multi-pass or beam-search decoding leading to results that are competitive with the state-of-the-art.</p>
<h2>6 FUTURE WORK</h2>
<p>We show that by batching over iterations of interaction with a system, no-regret methods, including the presented DAGGER approach can provide a learning reduction with strong performance guarantees in both imitation learning and structured prediction. In future work, we will consider more sophisticated strategies than simple greedy forward decoding for structured prediction, as well as using base classifiers that rely on Inverse Optimal Control (Abbeel and Ng, 2004; Ratliff et al., 2006) techniques to learn a cost function for a planner to aid prediction in imitation learning. Further we believe techniques similar to those presented, by leveraging a cost-to-go estimate, may provide an understanding of the success of online methods for reinforcement learning and suggest a similar data-aggregation method that can guarantee performance in such settings.</p>
<h2>Acknowledgements</h2>
<p>This work is supported by the ONR MURI grant N00014-09-1-1052, Reasoning in Reduced Information Spaces, and by the National Sciences and Engineering Research Council of Canada (NSERC).</p>
<h2>References</h2>
<p>P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the 21st International Conference on Machine Learning (ICML), 2004.
B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from demonstration. Robotics and Autonomous Systems, 2009.
A. Beygelzimer, V. Dani, T. Hayes, J. Langford, and B. Zadrozny. Error limiting reductions between classification tasks. In Proceedings of the 22nd International Conference on Machine Learning (ICML), 2005.
L. Bottou. sgd code, 2009. URL http://www.leon. bottou.org/projects/sgd.
N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms. 2004.
S. Chernova and M. Veloso. Interactive policy learning through confidence-based autonomy. 2009.
H. Daumé III, J. Langford, and D. Marcu. Search-based structured prediction. Machine Learning, 2009.
E. Hazan, A. Kalai, S. Kale, and A. Agarwal. Logarithmic regret algorithms for online convex optimization. In Proceedings of the 19th annual conference on Computational Learning Theory (COLT), 2006.
M. Kääriäinen. Lower bounds for reductions, 2006. Atomic Learning workshop.
S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In Proceedings of the 19th International Conference on Machine Learning (ICML), 2002.
S. Kakade and S. Shalev-Shwartz. Mind the duality gap: Logarithmic regret algorithms for online optimization. In Advances in Neural Information Processing Systems (NIPS), 2008.
S. Kakade and A. Tewari. On the generalization ability of online strongly convex programming algorithms. In Advances in Neural Information Processing Systems (NIPS), 2009.
N. Ratliff, D. Bradley, J. A. Bagnell, and J. Chestnutt. Boosting structured prediction for imitation learning. In Advances in Neural Information Processing Systems (NIPS), 2006.
N. Ratliff, J. A. Bagnell, and M. Zinkevich. (Online) subgradient methods for structured prediction. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), 2007.
S. Ross. Comparison of imitation learning approaches on Super Tux Kart, 2010a. URL http://www. youtube.com/watch?v=V00npNnWzSU.
S. Ross. Comparison of imitation learning approaches on Super Mario Bros, 2010b. URL http://www. youtube.com/watch?v=anOIOxZ3kGM.
S. Ross and J. A. Bagnell. Efficient reductions for imitation learning. In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS), 2010.
S. Schaal. Is imitation learning the route to humanoid robots? In Trends in Cognitive Sciences, 1999.
D. Silver, J. A. Bagnell, and A. Stentz. High performance outdoor navigation from overhead data using imitation learning. In Proceedings of Robotics Science and Systems (RSS), 2008.
B. Taskar, C. Guestrin, and D. Koller. Max margin markov networks. In Advances in Neural Information Processing Systems (NIPS), 2003.
J. Togelius and S. Karakovskiy. Mario AI Competition, 2009. URL http://julian.togelius.com/ mariocompetition2009.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\dagger}$ Each character is 8x16 binary pixels (128 input features); 26 binary features are used to encode the previously predicted letter in the word. We train the multiclass SVM using the all-pairs reduction to binary classification (Beygelzimer et al., 2005).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
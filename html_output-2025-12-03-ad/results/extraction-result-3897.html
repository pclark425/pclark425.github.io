<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3897 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3897</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3897</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-94.html">extraction-schema-94</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-259446268</p>
                <p><strong>Paper Title:</strong> CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling</p>
                <p><strong>Paper Abstract:</strong> Alzheimer’s disease (AD) is a terrible and degenerative disease commonly occurring in the elderly. Early detection can prevent patients from further damage, which is crucial in treating AD. Over the past few decades, it has been demonstrated that neuroimaging can be a critical diagnostic tool for AD, and the feature fusion of different neuroimaging modalities can enhance diagnostic performance. Most previous studies in multimodal feature fusion have only concatenated the high-level features extracted by neural networks from various neuroimaging images simply. However, a major problem of these studies is over-looking the low-level feature interactions between modalities in the feature extraction stage, resulting in suboptimal performance in AD diagnosis. In this paper, we develop a dual-branch vision transformer with cross-attention and graph pooling, namely CsAGP, which enables multi-level feature interactions between the inputs to learn a shared feature representation. Specifically, we first construct a brand-new cross-attention fusion module (CAFM), which processes MRI and PET images by two independent branches of differing computational complexity. These features are fused merely by the cross-attention mechanism to enhance each other. After that, a concise graph pooling algorithm-based Reshape-Pooling-Reshape (RPR) framework is developed for token selection to reduce token redundancy in the proposed model. Extensive experiments on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database demonstrated that the suggested method obtains 99.04%, 97.43%, 98.57%, and 98.72% accuracy for the classification of AD vs. CN, AD vs. MCI, CN vs. MCI, and AD vs. CN vs. MCI, respectively.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3897.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3897.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neurodegeneration / brain atrophy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neurodegeneration manifesting as regional brain atrophy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper describes AD-related neurodegeneration as loss of brain tissue (notably grey matter/hippocampal regions) detectable with structural MRI and used as a key discriminative feature for classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td>Neurodegeneration leading to regional brain atrophy (loss of grey matter / structural atrophy) as a mechanism underlying cognitive decline in AD.</td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td>Indirect, imaging-based: the paper cites multiple studies and uses MRI structural features (T1-weighted images, grey matter measures) as discriminative signals. The authors state MRI provides high-resolution differentiation of grey/white matter and that GM features fused with PET improved classification in prior work (Song et al., 2021). No molecular or longitudinal causal evidence is presented in this paper—evidence is limited to cross-sectional imaging correlates and model classification performance.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Structural MRI (T1-weighted), voxel-based/patch/token features extracted from axial slices.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Grey matter loss / structural atrophy and patch-level structural abnormalities (e.g., hippocampal and other regional atrophy patterns).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Used by the CsAGP model: unimodal MRI accuracies reported in this paper were 97.87% (AD vs CN), 95.37% (AD vs MCI), 94.94% (CN vs MCI), 94.21% (AD vs CN vs MCI). These are classification results from the ADNI dataset (human subjects).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Used to distinguish CN, MCI (prodromal), and AD dementia stages; best separation for AD vs CN, more difficult for MCI stages (paper notes subtle changes in MCI).</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Human neuroimaging classification study using ADNI T1-weighted MRI slices (cross-sectional supervised learning).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Paper emphasizes that MRI-derived structural features are correlational, not causal; 2D slice-based analysis may miss 3D information; distinguishing MCI from AD/CN using imaging alone is difficult because changes in MCI are subtle; scanner parameter variability and preprocessing differences can affect features.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3897.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3897.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hypometabolism (glucose)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reduced cerebral glucose metabolism (FDG-PET hypometabolism)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies PET (FDG-PET) imaging as detecting changes in glucose metabolism and uses FDG-PET features as complementary modality to MRI for AD detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td>Regional reductions in glucose metabolism (neuronal/metabolic dysfunction) are presented as a pathophysiological manifestation associated with AD.</td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td>Indirect imaging evidence: FDG-PET images (measuring glucose metabolism) are used as an input modality and contribute to classification; prior literature cited indicates PET can detect spread of lesions and glucose metabolism alterations. The present paper demonstrates that combining FDG-PET with MRI improves classification accuracy vs unimodal PET (PET-only accuracies in this dataset: 95.92% AD vs CN, 94.12% AD vs MCI, 94.69% CN vs MCI, 93.37% multiclass).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>FDG-PET imaging (2D axial slice inputs aligned to MRI and smoothed), used alone or fused with MRI in a transformer-based classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Reduced FDG uptake (regional hypometabolism) patterns on FDG-PET slices.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Unimodal PET classification in this study: reported accuracies ~95.9% (AD vs CN), 94.12% (AD vs MCI), 94.69% (CN vs MCI), 93.37% (3-class). Multimodal MRI+PET (CsAGP) improved accuracies to 99.04%, 97.43%, 98.57%, and 98.72% respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Effective across CN, MCI, and AD stages in classification tasks; authors note PET activations are relatively concentrated and vary by stage.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Human neuroimaging classification (ADNI FDG-PET slices aligned with MRI).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>PET provides complementary but lower-resolution anatomical detail than MRI; PET-only model underperformed MRI-only in this dataset; practical constraints include availability/cost of PET; paper notes imaging-protocol differences across scanners and smoothing/preprocessing may affect results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3897.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3897.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distributed brain network disruption</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distributed/distant brain-region interactions and network disruption</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors note that AD-related changes can involve distant brain regions and network-level interactions, motivating use of models (transformers) that capture long-range dependencies across the brain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td>Disease mechanism attributed to distributed disruption of interactions among distant brain regions (network-level dysfunction) rather than solely localized lesions.</td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td>Argumentative / literature-cited rationale: paper cites Lyu et al., 2022 and other works indicating distant brain regions can interact and be affected in AD; visualization (Grad-CAM) of CsAGP shows activated areas dispersed across brain for AD cases, consistent with distributed involvement. No direct mechanistic experiments provided.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Global-context modelling via Vision Transformer (self-attention) on image patches to capture long-range relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Distributed patterns of activation / attention across multiple brain regions (as shown by Grad-CAM heatmaps) rather than a single localized lesion.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Using transformer-based global modelling (CsAGP) produced high accuracies (multimodal ACC 99.04% AD vs CN; multiclass ACC 98.72%), suggesting that capturing long-range interactions improves classification.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Applicable across CN, MCI, AD; authors note better discrimination for AD vs CN than for MCI.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Human neuroimaging classification study with attention-visualization (Grad-CAM) to show distributed activation patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Evidence is correlative (model attention patterns), not causal; attention maps indicate importance for classification but do not prove underlying pathophysiology; 2D slice-based analysis limits full-network assessment; the authors note token-redundancy and that full 3D analyses are future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3897.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3897.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MRI (T1-weighted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T1-weighted structural magnetic resonance imaging (MRI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-resolution structural MRI used to differentiate grey and white matter and extract structural features (patch tokens) for AD classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>T1-weighted MRI acquisition, preprocessing (AC–PC alignment, N4 bias correction, BET skull-stripping, spatial normalization), extraction of axial slice images (indices 80–100), resized to 224x224, patch-tokenization and transformer encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Grey-matter structural patterns and region-specific atrophy captured in MRI slices (patch-level features).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Unimodal MRI classification accuracies reported: 97.87% (AD vs CN), 95.37% (AD vs MCI), 94.94% (CN vs MCI), 94.21% (3-class). Sensitivity and specificity values reported in Table 2 for MRI-only tasks (see paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Used to identify CN, MCI (prodromal), and AD; best discrimination observed for AD vs CN.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Human clinical neuroimaging (ADNI) cross-sectional classification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Study used 2D slice-based analysis (axial slices only), which can omit 3D spatial information; differences in scanner models and acquisition parameters require normalization; MRI alone may not capture metabolic dysfunction detectable by PET.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3897.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3897.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FDG-PET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fluorodeoxyglucose positron emission tomography (FDG-PET)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Molecular imaging modality measuring regional cerebral glucose metabolism used here as complementary input to MRI for detecting AD-related hypometabolism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>FDG-PET imaging aligned to MRI, smoothed, slice selection (axial indices 80–100), resized and input to the transformer model.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Regional reductions in FDG uptake (hypometabolism) patterns associated with AD.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Unimodal PET accuracies reported in this paper: 95.92% (AD vs CN), 94.12% (AD vs MCI), 94.69% (CN vs MCI), 93.37% (3-class). Combined MRI+PET (CsAGP) improved performance to 99.04%, 97.43%, 98.57%, and 98.72%, respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Applied across stages (CN, MCI, AD); PET heatmaps reportedly more spatially concentrated and stage-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Human neuroimaging (ADNI) classification study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>PET has lower anatomical resolution than MRI and is more costly/less available; PET-only models underperformed MRI-only in some tasks in this dataset; preprocessing and scanner differences may affect signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3897.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3897.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMSE / CDR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mini-Mental State Examination (MMSE) and Clinical Dementia Rating (CDR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard clinical neuropsychological tests referenced as commonly used for clinical staging of cognitive impairment in AD, included in subject metadata.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Neuropsychological tests (MMSE score, CDR score) used clinically to stage cognitive impairment; in this paper they are reported in subject demographics but not used as model inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>MMSE scores and CDR ratings reflecting cognitive impairment; mean MMSE reported per group in Table 1 (e.g., AD mean ~21.2, MCI ~25.6, CN ~28.7).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Not used as model input—no classifier performance reported using MMSE/CDR in this paper. They are described as clinical tools that 'assist doctors in determining the stage' of a patient.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Clinical staging across CN, MCI, AD; MMSE and CDR are applicable to prodromal and dementia stages.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Clinical neuropsychological assessments (human subjects, ADNI metadata).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Authors note neuroimaging + clinical data fusion can increase diagnostic performance but this study did not incorporate clinical scores in the model; combining clinical data requires preprocessing and can be time-consuming; extracting features from high-dimensional clinical/genetic data is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3897.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3897.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GM-PET fused image</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gray-matter fused PET (GM-PET) imaging (fused MRI grey matter + PET)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fused imaging modality cited (Song et al., 2021) that merges grey-matter information from structural MRI with PET to form a single fused input that can improve classification accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An effective multimodal image fusion method using MRI and PET for Alzheimer's disease diagnosis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Image-fusion preprocessing that merges MRI-derived grey matter maps with PET images into a single 'GM-PET' image used as model input.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Combination of anatomical grey-matter distribution with metabolic PET signal in a fused image emphasizing discriminative features.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Cited prior work reported that fusing GM and PET improved accuracy up to 16.48% vs unimodal in that study; in the present paper fused-image methods are discussed as reducing model parameters but having time-consuming preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Used for AD vs CN and other classification tasks in prior cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Mentioned as prior imaging-method study (human ADNI-based work by Song et al., 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Authors note fused-image preprocessing is time-consuming and computationally costly; fused-image approaches may reduce model size but require demanding preprocessing steps and may not be practical when multimodal data are incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3897.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3897.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CsAGP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-attention and graph pooling dual-transformer (CsAGP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Novel dual-branch vision-transformer architecture introduced in this paper that fuses MRI and PET via a cross-attention fusion module (CAFM) and reduces token redundancy using a Reshape-Pooling-Reshape (RPR) framework with graph pooling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Deep learning multimodal classification: dual-branch ViT-like transformer (separate MRI and PET branches), cross-attention fusion (CAFM) to enable multi-level cross-modal interactions, and RPR graph-pooling to select discriminative patch tokens; trained on axial slices from ADNI.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Learns shared multimodal representations reflecting structural atrophy and metabolic hypometabolism patterns; Grad-CAM visualizations highlight distributed activated areas relevant to AD.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Reported multimodal (MRI+PET) performance on ADNI (test split): AD vs CN ACC 99.04% (sensitivity ~97.87%, specificity ~99.54%, AUC reported up to 99.80 in tables), AD vs MCI ACC 97.43% (AUC 99.23), CN vs MCI ACC 98.57% (AUC 99.76), AD vs CN vs MCI ACC 98.72% (AUC 99.86). Training: 766 subjects (214 AD / 226 MCI / 326 CN), 60/20/20 split, 300 epochs, Adam optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Distinguishes CN, MCI (prodromal), and AD dementia in classification tasks; best performance AD vs CN; MCI separation more challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Human neuroimaging machine-learning / deep-learning classification study using ADNI dataset (cross-sectional slices).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Authors note limitations: model is slice-based using only axial 2D slices (omits full 3D information); no time-processing (runtime) comparison yet; multimodal images may not always be available in practice; possible sensitivity to preprocessing and scanner variability; selection of pooling rate trades off information vs noise (r chosen = 0.5 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>An effective multimodal image fusion method using MRI and PET for Alzheimer's disease diagnosis <em>(Rating: 2)</em></li>
                <li>BPGAN: Brain PET synthesis from MRI using generative adversarial network for multi-modal Alzheimer's disease diagnosis <em>(Rating: 2)</em></li>
                <li>Task-induced pyramid and attention GAN for multimodal brain image imputation and classification in alzheimer's disease <em>(Rating: 2)</em></li>
                <li>Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis <em>(Rating: 2)</em></li>
                <li>Multimodal 2.5 D convolutional neural network for diagnosis of Alzheimer's Disease with magnetic resonance imaging and positron emission tomography <em>(Rating: 1)</em></li>
                <li>Multimodal neuroimaging neural network-based feature detection for diagnosis of Alzheimer's disease <em>(Rating: 1)</em></li>
                <li>ASMFS: Adaptive-similarity-based multi-modality feature selection for classification of Alzheimer's disease <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3897",
    "paper_id": "paper-259446268",
    "extraction_schema_id": "extraction-schema-94",
    "extracted_data": [
        {
            "name_short": "Neurodegeneration / brain atrophy",
            "name_full": "Neurodegeneration manifesting as regional brain atrophy",
            "brief_description": "Paper describes AD-related neurodegeneration as loss of brain tissue (notably grey matter/hippocampal regions) detectable with structural MRI and used as a key discriminative feature for classification.",
            "citation_title": "",
            "mention_or_use": "mention",
            "proposed_cause": "Neurodegeneration leading to regional brain atrophy (loss of grey matter / structural atrophy) as a mechanism underlying cognitive decline in AD.",
            "cause_evidence": "Indirect, imaging-based: the paper cites multiple studies and uses MRI structural features (T1-weighted images, grey matter measures) as discriminative signals. The authors state MRI provides high-resolution differentiation of grey/white matter and that GM features fused with PET improved classification in prior work (Song et al., 2021). No molecular or longitudinal causal evidence is presented in this paper—evidence is limited to cross-sectional imaging correlates and model classification performance.",
            "detection_method": "Structural MRI (T1-weighted), voxel-based/patch/token features extracted from axial slices.",
            "biomarker_or_finding": "Grey matter loss / structural atrophy and patch-level structural abnormalities (e.g., hippocampal and other regional atrophy patterns).",
            "detection_performance": "Used by the CsAGP model: unimodal MRI accuracies reported in this paper were 97.87% (AD vs CN), 95.37% (AD vs MCI), 94.94% (CN vs MCI), 94.21% (AD vs CN vs MCI). These are classification results from the ADNI dataset (human subjects).",
            "detection_stage": "Used to distinguish CN, MCI (prodromal), and AD dementia stages; best separation for AD vs CN, more difficult for MCI stages (paper notes subtle changes in MCI).",
            "study_type": "Human neuroimaging classification study using ADNI T1-weighted MRI slices (cross-sectional supervised learning).",
            "limitations_or_counter_evidence": "Paper emphasizes that MRI-derived structural features are correlational, not causal; 2D slice-based analysis may miss 3D information; distinguishing MCI from AD/CN using imaging alone is difficult because changes in MCI are subtle; scanner parameter variability and preprocessing differences can affect features.",
            "uuid": "e3897.0",
            "source_info": {
                "paper_title": "CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Hypometabolism (glucose)",
            "name_full": "Reduced cerebral glucose metabolism (FDG-PET hypometabolism)",
            "brief_description": "The paper identifies PET (FDG-PET) imaging as detecting changes in glucose metabolism and uses FDG-PET features as complementary modality to MRI for AD detection.",
            "citation_title": "",
            "mention_or_use": "use",
            "proposed_cause": "Regional reductions in glucose metabolism (neuronal/metabolic dysfunction) are presented as a pathophysiological manifestation associated with AD.",
            "cause_evidence": "Indirect imaging evidence: FDG-PET images (measuring glucose metabolism) are used as an input modality and contribute to classification; prior literature cited indicates PET can detect spread of lesions and glucose metabolism alterations. The present paper demonstrates that combining FDG-PET with MRI improves classification accuracy vs unimodal PET (PET-only accuracies in this dataset: 95.92% AD vs CN, 94.12% AD vs MCI, 94.69% CN vs MCI, 93.37% multiclass).",
            "detection_method": "FDG-PET imaging (2D axial slice inputs aligned to MRI and smoothed), used alone or fused with MRI in a transformer-based classifier.",
            "biomarker_or_finding": "Reduced FDG uptake (regional hypometabolism) patterns on FDG-PET slices.",
            "detection_performance": "Unimodal PET classification in this study: reported accuracies ~95.9% (AD vs CN), 94.12% (AD vs MCI), 94.69% (CN vs MCI), 93.37% (3-class). Multimodal MRI+PET (CsAGP) improved accuracies to 99.04%, 97.43%, 98.57%, and 98.72% respectively.",
            "detection_stage": "Effective across CN, MCI, and AD stages in classification tasks; authors note PET activations are relatively concentrated and vary by stage.",
            "study_type": "Human neuroimaging classification (ADNI FDG-PET slices aligned with MRI).",
            "limitations_or_counter_evidence": "PET provides complementary but lower-resolution anatomical detail than MRI; PET-only model underperformed MRI-only in this dataset; practical constraints include availability/cost of PET; paper notes imaging-protocol differences across scanners and smoothing/preprocessing may affect results.",
            "uuid": "e3897.1",
            "source_info": {
                "paper_title": "CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Distributed brain network disruption",
            "name_full": "Distributed/distant brain-region interactions and network disruption",
            "brief_description": "Authors note that AD-related changes can involve distant brain regions and network-level interactions, motivating use of models (transformers) that capture long-range dependencies across the brain.",
            "citation_title": "",
            "mention_or_use": "mention",
            "proposed_cause": "Disease mechanism attributed to distributed disruption of interactions among distant brain regions (network-level dysfunction) rather than solely localized lesions.",
            "cause_evidence": "Argumentative / literature-cited rationale: paper cites Lyu et al., 2022 and other works indicating distant brain regions can interact and be affected in AD; visualization (Grad-CAM) of CsAGP shows activated areas dispersed across brain for AD cases, consistent with distributed involvement. No direct mechanistic experiments provided.",
            "detection_method": "Global-context modelling via Vision Transformer (self-attention) on image patches to capture long-range relationships.",
            "biomarker_or_finding": "Distributed patterns of activation / attention across multiple brain regions (as shown by Grad-CAM heatmaps) rather than a single localized lesion.",
            "detection_performance": "Using transformer-based global modelling (CsAGP) produced high accuracies (multimodal ACC 99.04% AD vs CN; multiclass ACC 98.72%), suggesting that capturing long-range interactions improves classification.",
            "detection_stage": "Applicable across CN, MCI, AD; authors note better discrimination for AD vs CN than for MCI.",
            "study_type": "Human neuroimaging classification study with attention-visualization (Grad-CAM) to show distributed activation patterns.",
            "limitations_or_counter_evidence": "Evidence is correlative (model attention patterns), not causal; attention maps indicate importance for classification but do not prove underlying pathophysiology; 2D slice-based analysis limits full-network assessment; the authors note token-redundancy and that full 3D analyses are future work.",
            "uuid": "e3897.2",
            "source_info": {
                "paper_title": "CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "MRI (T1-weighted)",
            "name_full": "T1-weighted structural magnetic resonance imaging (MRI)",
            "brief_description": "High-resolution structural MRI used to differentiate grey and white matter and extract structural features (patch tokens) for AD classification.",
            "citation_title": "",
            "mention_or_use": "use",
            "proposed_cause": null,
            "cause_evidence": null,
            "detection_method": "T1-weighted MRI acquisition, preprocessing (AC–PC alignment, N4 bias correction, BET skull-stripping, spatial normalization), extraction of axial slice images (indices 80–100), resized to 224x224, patch-tokenization and transformer encoding.",
            "biomarker_or_finding": "Grey-matter structural patterns and region-specific atrophy captured in MRI slices (patch-level features).",
            "detection_performance": "Unimodal MRI classification accuracies reported: 97.87% (AD vs CN), 95.37% (AD vs MCI), 94.94% (CN vs MCI), 94.21% (3-class). Sensitivity and specificity values reported in Table 2 for MRI-only tasks (see paper tables).",
            "detection_stage": "Used to identify CN, MCI (prodromal), and AD; best discrimination observed for AD vs CN.",
            "study_type": "Human clinical neuroimaging (ADNI) cross-sectional classification.",
            "limitations_or_counter_evidence": "Study used 2D slice-based analysis (axial slices only), which can omit 3D spatial information; differences in scanner models and acquisition parameters require normalization; MRI alone may not capture metabolic dysfunction detectable by PET.",
            "uuid": "e3897.3",
            "source_info": {
                "paper_title": "CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "FDG-PET",
            "name_full": "Fluorodeoxyglucose positron emission tomography (FDG-PET)",
            "brief_description": "Molecular imaging modality measuring regional cerebral glucose metabolism used here as complementary input to MRI for detecting AD-related hypometabolism.",
            "citation_title": "",
            "mention_or_use": "use",
            "proposed_cause": null,
            "cause_evidence": null,
            "detection_method": "FDG-PET imaging aligned to MRI, smoothed, slice selection (axial indices 80–100), resized and input to the transformer model.",
            "biomarker_or_finding": "Regional reductions in FDG uptake (hypometabolism) patterns associated with AD.",
            "detection_performance": "Unimodal PET accuracies reported in this paper: 95.92% (AD vs CN), 94.12% (AD vs MCI), 94.69% (CN vs MCI), 93.37% (3-class). Combined MRI+PET (CsAGP) improved performance to 99.04%, 97.43%, 98.57%, and 98.72%, respectively.",
            "detection_stage": "Applied across stages (CN, MCI, AD); PET heatmaps reportedly more spatially concentrated and stage-dependent.",
            "study_type": "Human neuroimaging (ADNI) classification study.",
            "limitations_or_counter_evidence": "PET has lower anatomical resolution than MRI and is more costly/less available; PET-only models underperformed MRI-only in some tasks in this dataset; preprocessing and scanner differences may affect signals.",
            "uuid": "e3897.4",
            "source_info": {
                "paper_title": "CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "MMSE / CDR",
            "name_full": "Mini-Mental State Examination (MMSE) and Clinical Dementia Rating (CDR)",
            "brief_description": "Standard clinical neuropsychological tests referenced as commonly used for clinical staging of cognitive impairment in AD, included in subject metadata.",
            "citation_title": "",
            "mention_or_use": "mention",
            "proposed_cause": null,
            "cause_evidence": null,
            "detection_method": "Neuropsychological tests (MMSE score, CDR score) used clinically to stage cognitive impairment; in this paper they are reported in subject demographics but not used as model inputs.",
            "biomarker_or_finding": "MMSE scores and CDR ratings reflecting cognitive impairment; mean MMSE reported per group in Table 1 (e.g., AD mean ~21.2, MCI ~25.6, CN ~28.7).",
            "detection_performance": "Not used as model input—no classifier performance reported using MMSE/CDR in this paper. They are described as clinical tools that 'assist doctors in determining the stage' of a patient.",
            "detection_stage": "Clinical staging across CN, MCI, AD; MMSE and CDR are applicable to prodromal and dementia stages.",
            "study_type": "Clinical neuropsychological assessments (human subjects, ADNI metadata).",
            "limitations_or_counter_evidence": "Authors note neuroimaging + clinical data fusion can increase diagnostic performance but this study did not incorporate clinical scores in the model; combining clinical data requires preprocessing and can be time-consuming; extracting features from high-dimensional clinical/genetic data is challenging.",
            "uuid": "e3897.5",
            "source_info": {
                "paper_title": "CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GM-PET fused image",
            "name_full": "Gray-matter fused PET (GM-PET) imaging (fused MRI grey matter + PET)",
            "brief_description": "A fused imaging modality cited (Song et al., 2021) that merges grey-matter information from structural MRI with PET to form a single fused input that can improve classification accuracy.",
            "citation_title": "An effective multimodal image fusion method using MRI and PET for Alzheimer's disease diagnosis",
            "mention_or_use": "mention",
            "proposed_cause": null,
            "cause_evidence": null,
            "detection_method": "Image-fusion preprocessing that merges MRI-derived grey matter maps with PET images into a single 'GM-PET' image used as model input.",
            "biomarker_or_finding": "Combination of anatomical grey-matter distribution with metabolic PET signal in a fused image emphasizing discriminative features.",
            "detection_performance": "Cited prior work reported that fusing GM and PET improved accuracy up to 16.48% vs unimodal in that study; in the present paper fused-image methods are discussed as reducing model parameters but having time-consuming preprocessing.",
            "detection_stage": "Used for AD vs CN and other classification tasks in prior cited work.",
            "study_type": "Mentioned as prior imaging-method study (human ADNI-based work by Song et al., 2021).",
            "limitations_or_counter_evidence": "Authors note fused-image preprocessing is time-consuming and computationally costly; fused-image approaches may reduce model size but require demanding preprocessing steps and may not be practical when multimodal data are incomplete.",
            "uuid": "e3897.6",
            "source_info": {
                "paper_title": "CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "CsAGP",
            "name_full": "Cross-attention and graph pooling dual-transformer (CsAGP)",
            "brief_description": "Novel dual-branch vision-transformer architecture introduced in this paper that fuses MRI and PET via a cross-attention fusion module (CAFM) and reduces token redundancy using a Reshape-Pooling-Reshape (RPR) framework with graph pooling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proposed_cause": null,
            "cause_evidence": null,
            "detection_method": "Deep learning multimodal classification: dual-branch ViT-like transformer (separate MRI and PET branches), cross-attention fusion (CAFM) to enable multi-level cross-modal interactions, and RPR graph-pooling to select discriminative patch tokens; trained on axial slices from ADNI.",
            "biomarker_or_finding": "Learns shared multimodal representations reflecting structural atrophy and metabolic hypometabolism patterns; Grad-CAM visualizations highlight distributed activated areas relevant to AD.",
            "detection_performance": "Reported multimodal (MRI+PET) performance on ADNI (test split): AD vs CN ACC 99.04% (sensitivity ~97.87%, specificity ~99.54%, AUC reported up to 99.80 in tables), AD vs MCI ACC 97.43% (AUC 99.23), CN vs MCI ACC 98.57% (AUC 99.76), AD vs CN vs MCI ACC 98.72% (AUC 99.86). Training: 766 subjects (214 AD / 226 MCI / 326 CN), 60/20/20 split, 300 epochs, Adam optimizer.",
            "detection_stage": "Distinguishes CN, MCI (prodromal), and AD dementia in classification tasks; best performance AD vs CN; MCI separation more challenging.",
            "study_type": "Human neuroimaging machine-learning / deep-learning classification study using ADNI dataset (cross-sectional slices).",
            "limitations_or_counter_evidence": "Authors note limitations: model is slice-based using only axial 2D slices (omits full 3D information); no time-processing (runtime) comparison yet; multimodal images may not always be available in practice; possible sensitivity to preprocessing and scanner variability; selection of pooling rate trades off information vs noise (r chosen = 0.5 in experiments).",
            "uuid": "e3897.7",
            "source_info": {
                "paper_title": "CsAGP: Detecting Alzheimer’s disease from multimodal images via dual-transformer with cross-attention and graph pooling",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "An effective multimodal image fusion method using MRI and PET for Alzheimer's disease diagnosis",
            "rating": 2,
            "sanitized_title": "an_effective_multimodal_image_fusion_method_using_mri_and_pet_for_alzheimers_disease_diagnosis"
        },
        {
            "paper_title": "BPGAN: Brain PET synthesis from MRI using generative adversarial network for multi-modal Alzheimer's disease diagnosis",
            "rating": 2,
            "sanitized_title": "bpgan_brain_pet_synthesis_from_mri_using_generative_adversarial_network_for_multimodal_alzheimers_disease_diagnosis"
        },
        {
            "paper_title": "Task-induced pyramid and attention GAN for multimodal brain image imputation and classification in alzheimer's disease",
            "rating": 2,
            "sanitized_title": "taskinduced_pyramid_and_attention_gan_for_multimodal_brain_image_imputation_and_classification_in_alzheimers_disease"
        },
        {
            "paper_title": "Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis",
            "rating": 2,
            "sanitized_title": "hierarchical_feature_representation_and_multimodal_fusion_with_deep_learning_for_admci_diagnosis"
        },
        {
            "paper_title": "Multimodal 2.5 D convolutional neural network for diagnosis of Alzheimer's Disease with magnetic resonance imaging and positron emission tomography",
            "rating": 1,
            "sanitized_title": "multimodal_25_d_convolutional_neural_network_for_diagnosis_of_alzheimers_disease_with_magnetic_resonance_imaging_and_positron_emission_tomography"
        },
        {
            "paper_title": "Multimodal neuroimaging neural network-based feature detection for diagnosis of Alzheimer's disease",
            "rating": 1,
            "sanitized_title": "multimodal_neuroimaging_neural_networkbased_feature_detection_for_diagnosis_of_alzheimers_disease"
        },
        {
            "paper_title": "ASMFS: Adaptive-similarity-based multi-modality feature selection for classification of Alzheimer's disease",
            "rating": 1,
            "sanitized_title": "asmfs_adaptivesimilaritybased_multimodality_feature_selection_for_classification_of_alzheimers_disease"
        }
    ],
    "cost": 0.01560025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CsAGP: Detecting Alzheimer's disease from multimodal images via dual-transformer with cross-attention and graph pooling
14 June 2023</p>
<p>Chaosheng Tang 
School of Computer Science and Technology
Henan Polytechnic University
454000JiaozuoHenanPR China</p>
<p>Mingyang Wei 
School of Computer Science and Technology
Henan Polytechnic University
454000JiaozuoHenanPR China</p>
<p>Junding Sun 
School of Computer Science and Technology
Henan Polytechnic University
454000JiaozuoHenanPR China</p>
<p>Shuihua Wang shuihuawang@ieee.org 
School of Computer Science and Technology
Henan Polytechnic University
454000JiaozuoHenanPR China</p>
<p>School of Computing and Mathematical Sciences
University of Leicester
LE1 7RHLeicesterUK</p>
<p>Department of Information Systems
Faculty of Computing and Information Technology
King Abdulaziz University
21589JeddahSaudi Arabia</p>
<p>Yudong Zhang yudongzhang@ieee.org 
School of Computer Science and Technology
Henan Polytechnic University
454000JiaozuoHenanPR China</p>
<p>School of Computing and Mathematical Sciences
University of Leicester
LE1 7RHLeicesterUK</p>
<p>Department of Information Systems
Faculty of Computing and Information Technology
King Abdulaziz University
21589JeddahSaudi Arabia</p>
<p>Neuroimaging Alzheimer's Disease 
Initiative 
CsAGP: Detecting Alzheimer's disease from multimodal images via dual-transformer with cross-attention and graph pooling
14 June 2023F84D5FC78819B68547A3CFCAD5B5F33010.1016/j.jksuci.2023.101618Received 31 January 2023 Revised 10 June 2023 Accepted 10 June 2023Alzheimer's disease Vision transformer Multimodal image fusion Deep learning
Alzheimer's disease (AD) is a terrible and degenerative disease commonly occurring in the elderly.Early detection can prevent patients from further damage, which is crucial in treating AD.Over the past few decades, it has been demonstrated that neuroimaging can be a critical diagnostic tool for AD, and the feature fusion of different neuroimaging modalities can enhance diagnostic performance.Most previous studies in multimodal feature fusion have only concatenated the high-level features extracted by neural networks from various neuroimaging images simply.However, a major problem of these studies is overlooking the low-level feature interactions between modalities in the feature extraction stage, resulting in suboptimal performance in AD diagnosis.In this paper, we develop a dual-branch vision transformer with cross-attention and graph pooling, namely CsAGP, which enables multi-level feature interactions between the inputs to learn a shared feature representation.Specifically, we first construct a brandnew cross-attention fusion module (CAFM), which processes MRI and PET images by two independent branches of differing computational complexity.These features are fused merely by the cross-attention mechanism to enhance each other.After that, a concise graph pooling algorithm-based Reshape-Pooling-Reshape (RPR) framework is developed for token selection to reduce token redundancy in the proposed model.Extensive experiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database demonstrated that the suggested method obtains 99.04%, 97.43%, 98.57%, and 98.72% accuracy for the classification of AD vs. CN, AD vs. MCI, CN vs. MCI, and AD vs. CN vs. MCI, respectively.</p>
<p>Introduction</p>
<p>Alzheimer's disease (AD) and its prodromal stage, mild cognitive impairment (MCI), are the primary causes of dementia.The increasing impairment of memory and cognitive abilities differentiates AD and MCI.Between 2000 and 2019, the number of people who passed from AD increased by more than 145% in the United States in 2019 (Alzheimer's disease facts and figures, 2022).More than 11 million Americans are offering unpaid caregiving of around 16 billion hours worth $271.6 billion to people with AD in 2021 (Alzheimer's disease facts and figures, 2022).The report shows that the global burden of AD will reach $2 trillion, and 152 million people will suffer from AD by 2050 (Patterson, 2018).There is no effective drug or method of curing AD for this complicated pathogenesis (Liu, 2020).Consequently, precise early detection and treatment of AD are of utmost importance.</p>
<p>Generally, according to different pathological features, the disease has three stages: control normal (CN), MCI, and AD.Neuropsychological tests and neuroimaging diagnoses are the primary clinical examination methods for AD.The mini-mental state examination (MMSE) and the clinical dementia rating (CDR) are the most commonly utilized tools for clinical neuropsychological evaluation of AD and assist doctors in determining the stage of a patient.With medical technology's rapid advancement, neuroimaging has become the mainstream method for diagnosing AD.Due to the great precision presentation of brain tissue and the capacity to differentiate between grey and white matter, magnetic resonance imaging (MRI) has turned into the common tool for neuroimaging diagnosis of AD. positron emission tomography (PET), another widely adopted neuFroimaging tool for diagnosing AD, may detect the spread of lesions and alterations in glucose metabolism using imaging agents.Moreover, the fusion of complementary information provided by different neuroimaging modalities further improves AD's diagnostic performance.</p>
<p>In the past decades, inspired by deep learning in the field of computer vision, deep learning methods have been extensively employed in AD Computer-Aided Diagnosis (CAD) (Suk et al., 2014;Liu et al., 2023).However, most methods only utilized unimodal images as input, and information provided by unimodal images is one-sided, which may lead to suboptimal performance for AD diagnosis.Researchers have recently shown increasing interest in multimodal images for AD diagnosis, and more deep learning-based multimodal feature fusion algorithms have been created (Kong et al., 2022;Zhang et al., 2019).Specifically, according to the type of input modalities, these algorithms can be split into four classes: the raw image-based methods, the fused image-based methods, the generated image-based methods, the neuroimaging, and clinical data-based methods.The raw imagebased methods feed the multi-input neural networks with the raw neuroimaging images or their preprocessed images, then fuse different modal features by latent representation learning (Zhang and Shi, 2020;Meng, 2022).Although these methods are simple to implement, they are prone to causing excessive model parameters and ignoring the interaction of information between modalities.The fused image-based methods merge important and discriminative information from several modalities to a sole fused image through image preprocessing steps to reduce model parameters, then take the sole fused image as model input (Song et al., 2021;Wu, 2018).However, these preprocessing steps are timeconsuming and also increase computational costs.Due to factors such as cost or availability, multimodal images are not always fully realized in practice.To address this limitation and utilize incomplete data, the generated image-based methods directly generate missing data from an available modality through image generation algorithms such as generative adversarial networks (GANs) (Pan and Wang, 2206;Logan, 2021).Regrettably, it is difficult to analyze the generated images quantitatively due to the particularity of medical images.</p>
<p>On the other hand, neuroimaging and clinical data-based methods combine neuroimaging and clinical data to simulate the diagnostic process of clinicians (Zhao et al., 2019;Lin et al., 2021).Even though this method can increase the performance of AD diagnosis even further, it suffers from the same limitation of time-consuming preprocessing steps for clinical data.Furthermore, extracting effective features from high-dimensional gene sequences is challenging.</p>
<p>Although convolutional neural networks (CNNs) 's convolutional operation improves their ability to capture local information, this generally results in CNNs learning features that are only relevant to nearby brain regions rather than more generalizable features that can be applied across multiple brain regions.It has been found that even distant brain regions can have significant interactions.Hence AD-related disorders can affect many different brain parts (Lyu et al., 2022).A new architecture based on the selfattention mechanism, vision transformer (ViT), was designed to effectively model global context without layering hierarchical convolution layers.ViT is powerful in classifying AD in several investigations (Zhu, 2022;Kushol et al., 2022).Notably, the problem of token redundancy (Rao et al., 2021) in ViT without taken into account in their models.</p>
<p>Additionally, from the point of view of multimodal feature fusion strategy, most existing multimodal data fusion diagnosis methods purely combine high-level selected features from the various modalities to merge their information, ignoring the fusion of low-level features.Compared to high-level features, low-level features have higher resolution and contain more location and detail information which is equally important for AD diagnosis.On the other hand, feature extraction and fusion stages are performed independently in these methods, ignoring the cross-modal interactions, which restricts the model from learning a shared representation (Khan et al., Jun. 2021).Cross-modal interaction has been shown to fully fuse features and further improve model performance (Tan and Bansal, 2019).</p>
<p>In this paper, we design a dual-transformer based on crossattention and graph pooling algorithm (CsAGP) to solve the above issues, which enables multi-level feature interaction between the input modalities through the cross-attention mechanism.Specifically, we first construct a dual-branch framework for extracting multimodal features and disease classification.Then, to learn rich fused features, an innovative cross-attention fusion module (CAFM) is built to extract and fuse multimodal features based on the self-attention mechanism.To reduce token redundancy in the proposed model, a concise Reshape-Pooling-Reshape (RPR) framework was developed to select tokens of high significance via a graph pooling algorithm while avoiding high computation and memory costs.The proposed CsAGP has performed satisfactorily in the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.Our major contributions are as follows:</p>
<p>(1) A dual-branch vision transformer with cross-attention and graph pooling algorithm, called CsAGP, is present to model the global information of images based on the pure selfattention mechanism to detect multimodal fused features for AD diagnosis.(2) An innovative cross-attention mechanism-based multimodal feature fusion method is suggested, which can efficiently learn a shared feature representation of MRI and PET images.(3) A concise Reshape-Pooling-Reshape (RPR) framework is developed, which filters tokens based on a graph pooling algorithm to reduce computation costs and token redundancy in the proposed model.</p>
<p>Related work</p>
<p>This section first introduces the current deep learning-based multimodal AD diagnosis methods.Generally, based on the type of input modalities, these methods can be split into four classes: (i) the raw image-based methods, (iii) the fused image-based methods, (iii) the generated image-based methods, and (iv) the neuroimaging clinical data-based methods.Then, an introduction to vision transformers for AD diagnosis is described.</p>
<p>Deep learning-based multimodal AD diagnosis</p>
<p>The raw image-based methods input raw neuroimaging images of different modalities or their preprocessed images into multiinput neural networks to fuse features between modalities by latent representation learning.Fang et al. (Fang et al., 2020) employed three CNNs (GooleNet, ResNet, and DenseNet) with a dropout mechanism and the Adaboost ensemble algorithm to improve AD's classification precision.They built a stack of CNNs to learn multimodal representations from MRI and PET images while utilizing the Adaboost ensemble algorithm to fuse their probabilistic scores.In their model, the dropout mechanism is utilized to exclude the slices with poor discrimination.However, the Adaboost ensemble algorithm prioritized misclassification data, which could lead to a bias due to noise data.</p>
<p>Adaptive-similarity-based multimodal feature selection (ASMFS) was developed by Shi et al. (Shi, 2022); which combines adaptive similarity learning with feature selection.Unfortunately, they only checked the efficacy of their model for binary classification problems and did not test it for multi-class situations.Jiao et al. (Jiao et al., 2022) devised a multimodal feature selection approach (FC2FS), which generates feature equivalence regularization and feature construction regularization through the similarity matrix calculated from the multimodal feature vertices.Finally, a support vector machine (SVM) is employed to finish the process of AD diagnosis.It is possible that the model's generalization ability was not maximized because only standard techniques of generating correlation coefficients were used throughout the construction of the similarity matrix.Zhang et al. (Zhang et al., 2021) developed a 2.5D CNN-based framework that extracts 2.5D patches from the hippocampal areas of MRI and PET images.Then, these 2.5D patches are integrated by a training approach termed branching pre-training to provide a full AD diagnosis.</p>
<p>Although the above methods can further raise the accuracy of AD diagnosis compared with traditional machine learning methods (Shi et al., 2019;Richhariya et al., May 2020), multi-input neural networks demand a lot of model parameters and computational costs.In addition, since only the high-level features of different modalities are concerned, the latent representation learning overlooks feature interactions between modalities.The fused imagebased methods integrate important and discriminative information from several modalities into a sole fused image based on image fusion algorithms and then take the fused image as the model's input to address these limitations.Song et al. (Song et al., 2021) acquired a new neuroimaging modality famous as ''GM-PET" by fusing gray matter (GM) of 3D structural MRI and PET images.Experimentally, their method can improve accuracy by up to 16.48% compared to the unimodal.Although their method significantly reduces the model's parameters compared to other multimodal fusion methods, the preprocessing steps are timeconsuming.</p>
<p>On the other hand, Kang et al. (Kang et al., 2020) obtained fractional anisotropy (FA) and mean diffusivity (MD) 2D image slices from diffusion tensor imaging by FMRIB Software Library (FSL), then merged them with the corresponding index MRI image slices into an RGB image, finally fed the RGB image into the VGG network to complete the classification of MCI and CN.However, they only tested their method on the CN vs.MCI task and did not consider diagnostic tasks involving other stages, such as AD.To avoid the problem that 2D slices will lose image-spatial information of raw 3D images, similar to Ref. (Song et al., 2021).Kong et al. (Kong et al., 2022) fused the GM into a 3D GM image and then fed the 3D GM image into a 3D CNN.Finally, they got 93.21% accuracy on AD vs. CN.Although the above methods can reduce the amount of computation compared to multi-input neural networks, the preprocessing steps of image fusion are demanding.</p>
<p>In practice, multimodal images may be incomplete for high financial costs or availability.To address this limitation and utilize incomplete data, with generative adversarial networks (GANs), the generated image-based methods directly produce missing data from a present modality.By combining a GAN and a dense CNN, Gao et al. (Gao et al., 2022) constructed a hybrid framework (PT-DCN) to diagnose AD.To make use of multimodal data, they generate PET images by the task-induce pyramid GAN.The PT-DCN can learn and merge multimodal features gradually.However, their experiment data was derived from ADNI-1 and ADNI-2, which may affect the experimental accuracy by varying MR scanner parameters.Zhang et al. (Zhang et al., 2022) developed a 3D GAN (BPGAN) to generate 3D PET images from MRI images.They devised a cutting-edge hybrid loss function to keep tabs on the brain data training process.In the end, they obtained an accuracy of 98.11% for AD vs. CN.Ye et al. (Ye et al., 2022) developed a paired GAN, which uses deep MRI features extracted by a feature extractor.The network can produce equivalent PET features in place of raw MRI images to reduce the model's size.</p>
<p>While the previous work has proven that generating missing data for AD diagnosis is possible, it has certain drawbacks when synthesizing multimodal medical images.First, the trustworthiness of the generated data is a serious issue.There are obvious differences between synthetic and real images regarding semantics and resolution because of the complicated spatial structure of medical images.Second, erratic training methods.The visual pattern in medical images is often unclear.Since GAN's training processes are prone to instability (Creswell et al., 2018), it is difficult to spot erratic behavior and implausible outcomes.At last, the evaluation is not always convincing.Because of the disclosure of ground-truth images, typical pixel-wise metrics have trouble quantitatively evaluating generated images.</p>
<p>The clinical diagnosis of AD relies on neuroimaging data but also the subject's clinical and biochemical information.It can significantly increase the accuracy of AD diagnosis by fusing with clinical and neuroimaging data.Zhang et al. (Zhang et al., 2019) employed two separate CNNs to analyze MRI and PET images for diagnosing AD.They suggested a method based on the Pearson coefficient that combines the neuroimaging diagnostic with neuropsychological evaluations (MMSE and CDR) to steer the output of their model.However, they focused solely on the high-level features of various modal images and paid little attention to the interactions of the low-level features.</p>
<p>Tu et al. (Tu et al., 2022) created an innovative multimodal AD diagnostic model.They first suggested a geometric; algebraic approach that extended low-dimensional clinical data of subjects, such as profiles, gene sequences, and MMSE scores, to highdimensional features at various levels.Second, according to the degree of influence, the feature filtration algorithm eliminates irrelevant features from high-dimensional features and yields transformed ones.Finally, the transformed features are combined with those extracted by CNNs from MRI images.Nan et al. (Nan, 2022) suggested a framework to investigate the impact of different modalities and their combinations on AD diagnosis.Ultimately, they found that with the addition of different modal data, the diagnostic performance of AD increased gradually.Furthermore, they discovered that adding single nucleotide polymorphism (SNP) data could bring a 3% to 7% performance boost to the AD diagnostic.</p>
<p>Vision Transformer-Based AD diagnosis</p>
<p>Rather than stacking hierarchical convolution layers, the vision transformer successfully models the image's global context based on the self-attention mechanism.Several works have shown the potential of vision transformers in AD diagnosis.Lyu et al. (Lyu et al., 2022) transferred a pre-trained ViT to the brain imaging dataset.They employed ViT as the backbone network and 2D MRI images as input and finally got 95.3% accuracy in AD diagnosis.Zhu et al. (Zhu, 2022) merged representation learning, feature distillation, and classification into a coherent model termed Brain Informer (BraInf).They initially deployed a multi-head ProbSparse self-attention block to minimize computational costs for representation learning.Later, a structural distillation block was utilized to underrate the dimension of the three-spatial tensor, which further reduces computational costs.However, the patch size of MRI images was predetermined in their experiments, which is illconsidered as the structural changes within every region produced by AD are not fixed.</p>
<p>On the other hand, Jang et al. (Jang and Hwang, 2022) developed a medical classifier for diagnosing AD.They trained a 3D CNN to recover local features linked to anomalies of AD from 3D MRI images and then fed the obtained local features into a transformer block to combine multi-plane and multi-slice features.This procedure can mark a general representation in 3D MRI images.They achieved 93.21%, 93.27%, and 85.26% accuracies on the ADNI, AIBL, and OASIS datasets.Xing et al. (Xing et al., 2022) assembled a block to transpose the 3D PET images into 2D images and fed the transposed image into a paralleled vision transformer model for AD diagnosis.</p>
<p>In general, deep learning-based multimodal AD diagnosis methods can automatically extract the AD-related features from complex neuroimaging images via CNNs without domain-specific knowledge, which can avoid errors caused by artificial.However, it is difficult to capture global features that across brain regions for CNNs.Meanwhile, although the vision transformer-based methods can model image-global information by the selfattention mechanism, most works do not consider the problem of token redundancy in their models.In this paper, we proposed a dual-transformer that fuses MRI and PET image features based on the cross-attention mechanism and selects discriminative tokens using a graph pooling algorithm to reduce redundancy.</p>
<p>Materials</p>
<p>Both the database ADNI and the image preprocessing pipelines are detailed in this section.</p>
<p>Datasets</p>
<p>Data used in this article were obtained from ADNI, which was settled in 2003 as a public-private alliance.The ADNI aims to develop clinical, imaging, and genetic to diagnose AD.Following the methodology described in Ref. (Golovanevsky et al., 2206), 766 subjects from the ADNI1/GO and ADNI2 phases were selected, including MRI and PET images.The numbers of AD, MCI, and CN subjects were 214, 226, and 326, respectively.There includes a T1-weighted MRI and a PET (FDG-PET) image in a NIfTI file format for every subject.Table 1 shows the clinical information (e.g., sex, age, MMSE scores, and CDR scores) of selected subjects.MRI images of subjects in this paper were acquired by three MR scanners, SIEMENS, Philips Medical Systems, and GE Medical Systems.</p>
<p>The imaging parameters are, respectively, a) repetition time TR
½ ¼ 3000ms, echo time TE ½ ¼ 3:5ms, inversion time TI ½ ¼ 1000ms, flip angle ¼ 8 , thickness ¼ 1:2mm, matrix size ¼ 192 Â 192 Â 160, field strength ¼ 3:0T. b) ½TR ¼ 6:8005ms, ½TE ¼ 3:116ms, ½TI ¼ 0ms, flip angle ¼ 9 , thickness ¼ 1:2mm, matrix size ¼ 256 Â 256 Â 170, field strength ¼ 3:0T. c) TR ½ ¼ 7:332ms, TE ½ ¼ 3:036ms, TI ½ ¼ 400ms, flip angle ¼ 11 , thick- ness ¼ 1:2mm, matrix size ¼ 256 Â 256 Â 196, field strength ¼ 3:0T.
The ADNI data acquisition details can be seen on the official webpage of ADNI.2</p>
<p>Data preprocessing</p>
<p>To remove the impact of various imaging parameters, the raw images were preprocessed using a normal preprocessing method described in Ref. (Suk et al., 2014) by the FMRIB Software Library (FSL)3 and Advanced Normalization Tools (ANTs). 4irst, the acpcdetect software5 shifted all of the raw MRI images to the exact center of the anterior commissure (AC) to the posterior commissure (PC) dividing line.After adjustment of force inhomogeneity by the nonparametric non-uniform force normalization (N4) algorithm, these MRI images were processed through the Brain Extraction Tool (BET) in the FSL to delete the cerebellum and skull.Second, we ensured that the skulls were clean and the dura was gone by hand-checking the images.Finally, all the preprocessed MRI images were spatially normalized onto a standard space.</p>
<p>PET images were precisely aligned with their corresponding MRI images.The Gaussian kernel was used to further smooth the preprocessed images.Utilizing the med2image tool,6 181 MRI and PET axial view slice images were acquired, respectively.Only slices with indices 80-100 have been used in this paper, as these images contained the most relevant information for the whole brain.To meet the input specifications, these slice images were scaled to 224Â224.The images before and after preprocessed are shown in Fig. 1.</p>
<p>Methods</p>
<p>Considering the difference in resolution and information in MRI and PET images, we designed two branches of different computational complexity by the encoder block proposed in Ref. (Dosovitskiy, et al., 2010) to process MRI and PET images individually.The proposed CsAGP, shown in Fig. 2, composes of three components: (i) two identical Patch Embed modules are implemented to convert MRI and PET images into non-overlapping patch tokens, respectively, (ii) A stack of K CsAGP Blocks that output the final feature representation for each modality, (iii) a classifier that predicts AD stage based on the shared feature representation.</p>
<p>The main implementation steps of our model can be described as follows.Firstly, the Patch Embed module is carried out on 2D MRI and PET images, which splits and transposes the input image into a series of patch tokens with a fixed size.Then the positional encoding and the class token are added to each token sequence.Then, these token sequences with positional encoding are passed into the CsAGP Block as image feature sequences.The feature sequences first pass through the Encoder module, which primarily consists of the self-attention mechanism and a feed-forward network (FFN).Compared to CNNs, the self-attention mechanism can efficiently model long-range relationships (Dosovitskiy, et al., 2010).Secondly, the outputs of the Encoder module are fed into the CAFM for multimodal feature fusion.The CAFM realizes the interactions of multi-level features through a pure self-attention mechanism which is different from the previous methods (Zhang et al., 2019) that concatenates the high-level features into a long vector.After that, the fused token sequences are passed through the RPR framework, which selects the discriminative tokens through a graph pooling algorithm to reduce token redundancy and memory costs.Finally, the class tokens of each modality sequence as an agent are combined to get the shared feature representation as the output of CsAGP, as detailed in the following subsections.</p>
<p>Patch Embed</p>
<p>In ViT, the original image is directly converted into fixed-size patches by linear projections alone, which is a poor way to capture low-level information in images.To overcome this limitation, as shown in Fig. 2. A novel tokenization approach was employed to make optimal use of CNN's strength in retrieving low-level features and minimizes the training difficulty of embedding by decreasing the patch size.Specifically, for M mri branch, given an input image x mri 2 R 3ÂHÂW , to minimize the size of input images, we first utilize a 7 Â 7 convolution with a stride of 4 and a padding of 3, then two additional 3 Â 3 convolutions with a stride of 2 and padding of 1, for improved low-level information extraction.</p>
<p>After that, the output x0 mri 2 R DÂ H P Â W P of the Patch Embed module is flattened and transposed to get the patch tokens matrix</p>
<p>x mri patch 2 R NÂD , where N ¼ HW=P 2 is the number of patches, D is the number of enriched channels, ðH; WÞ and ðP; PÞ represent the resolution of the input images and image patches, respectively.Finally, the positional encoding and an extra class token x mri cls 2 R 1ÂD are added as image representations to the patch tokens matrix x mri patch , resulting in the final patch tokens matrix x mri f 2 R ðNþ1ÞÂD for further steps.These procedures can be noted as follows:
x0 mri ¼ ReLUðConv3ðReLUðConv2ðReLUðConv1ðx mri ÞÞÞÞÞÞð1Þx mri patch ¼ TransposeðFlattenðx0 mri ÞÞð2Þx mri f ¼ x mri cls kx mri patch h i þ PE; PE 2 R ðNþ1ÞÂDð3Þ
where k is the concatenate operation and PE 2 R ðNþ1ÞÂD represents the positional encoding following Ref.(Dosovitskiy, et al., 2010).</p>
<p>The M pet branch follows the same procedures but takes a 2D PET image as input and adds another class token x pet cls 2 R 1ÂD .</p>
<p>Cross-Attention fusion module (CAFM)</p>
<p>The cross-attention fusion module (CAFM) was designed to fuse multimodal features efficiently.Specifically, let x i f 2 R ðNþ1ÞÂD be the final patch tokens matrix output from the previous step at branch i, where i represents the i-th branch (M mri or M pet ).</p>
<p>Fusion in the CAFM involves the class token x i cls from one branch and the patch tokens x i patch from another branch.Specifically, the class token x i cls is utilized as an agent to share information between the patch tokens x i patch from another branch, and then the class token x i cls returns to the i-th branch so that it combines the multimodal features efficiently and favorably.Following the fusing of patch tokens from another branch, the class token exchange information with its own patch tokens once more in the subsequent blocks to impart the information obtained from another branch into its own patch token representations.As shown in Fig. 2. The final matrix x i f is entered into the CAFM, which includes two sub-blocks.Each sub-block has two parts.The first part main contains a multi-heads cross-attention (MCA) mechanism to swap information between the patch tokens x i patch from another branch.An exemplification of the MCA on the M mri branch is proved in Fig. 3.For M mri branch, it first collects the patch tokens x pet patch 2 R NÂD from the M pet branch, and then concatenates them with own class token x mri cls , as expressed in Eq. ( 4):
x 0 mri ¼ x mri cls kx pet patch h ið4Þ
Then, the module performs the MCA between x mri cls and x 0 mri , where class token x mri cls of M mri branch is the query as patch-token information has already been integrated into the class token.The MCA could be written mathematically as:
q ¼ x mri cls W q ; k ¼ x 0 mri W k ; v ¼ x 0 mri W vð5ÞA ¼ softmax qk T = ffiffiffiffiffiffiffiffi ffi D=h qð6ÞMCA x 0 mri À Á ¼ Avð7Þ
where W q ; W k ; W v 2 R DÂðD=hÞ are learnable parameters, D is the embedding dimension of tokens, h represents the number of heads.</p>
<p>Because only the class token is utilized in the queries, the computational and memory costs of MCA are linear instead of quadratic in constructing A. Finally, the output z mri of the first part with a residual shortcut is defined as follows:
y mri cls ¼ x mri cls þ MCA ðx mri cls kx pet patch h ið8Þz mri ¼ y mri cls kx mri patch h ið9Þ
The second part primarily consists of a feed-forward network with non-linear activation, which performs a spatial transformation of z mri by two linear projecting layers to enhance the representation ability of tokens.It can be described as follows:  FFNðxÞ
Z mri ¼ LNðFFNðLNðz mri ÞÞ þ z mri Þ ð10Þ¼ rðxW 1 þ b 1 ÞW 2 þ b 2ð11Þ
where W 1 2 R DÂK is the weight of the first layer, projecting each token in a higher dimension K.And W 2 2 R KÂD is the weight of the second layer.b 1 2 R 1ÂK and b 2 2 R 1ÂD are the biases.LN represents the layer normalization, rðÁÞ is a non-linear activation function.</p>
<p>RPR framework</p>
<p>To reduce token redundancy in the proposed CsAGP, we developed the Reshape-Pooling-Reshape (RPR) framework, which consists of three stages: (i) tokens to graph (T2G), (ii) graph pooling, (iii) graph to tokens (G2T), as illustrated in Fig. 4. The token sequences were converted into graph-structured data in the T2G stage.A graph pooling algorithm is utilized to filter the tokens, and only the discriminative tokens are retained.Finally, the pooled subgraph vertices are reconverted to a token sequence in the G2T stage for the next step.</p>
<p>Tokens to graph (T2G)</p>
<p>For the M mri branch, given tokens Z mri 2 R ðNþ1ÞÂD generated from the CAFM, we first split them into patch tokens matrix z mri patch 2 R NÂD and a class token z mri cls 2 R 1ÂD accordingly.Then, a graph
G mri ¼ V; A ð Þ is constructed, where V represents the vertex set consisting of vertices v 1 ; Á Á Á ; v N f g
, and A 2 0; 1 f g NÂN is the adjacency matrix describing the edge connection information of G mri .</p>
<p>In other words, a graph G mri with N vertices and each vertex v i in the graph has a corresponding D-dimensional feature vector z mri i 2 R 1ÂD was constructed.The feature matrix z mri patch 2 R NÂD stacks N feature vectors.Then, the adjacency matrix A was established by the Euclidean distance between each vertex feature vector.Specifically, if the distance value dist ij between vertices v i and v j is smaller than average distance l, then A ij ¼ 1, which means there is an edge between vertices v i and v j , otherwise A ij ¼ 0. The process of establishing the adjacency matrix A can be formulated as follows:
dist ¼ kz mri 1 À z mri 1 k 2 kz mri 1 À z mri 2 k 2 Á Á Á kz mri 1 À z mri N k 2 kz mri 2 À z mri 1 k 2 kz mri 2 À z mri 2 k 2 Á Á Á kz mri 2 À z mri N k 2 . . . . . . . . . . . . kz mri N À z mri 1 k 2 kz mri N À z mri 2 k 2 Á Á Á kz mri N À z mri N k 2 2 6 6 6 6 6 4 3 7 7 7 7 7 5 ð12Þ l ¼ 1 N 2 X N i¼1 ð X N j¼1 dist ij Þ ð 13Þ A ij ¼ 1 ifdist ij &lt; l; 0 otherwise; &amp; 1 i; j Nð14Þ
where k Á k 2 represents the l 2 norm and dist indicates the distance matrix between vertices.l is the average distances, dist ij and A ij are the values of distance matrix dist and A in i-th row and j-th column, respectively.Finally, the patch tokens graph G mri is created, where A and z mri patch are the adjacency matrix and the feature matrix, respectively.The M pet branch generates graph G pet through the same way.</p>
<p>Graph pooling</p>
<p>we developed a novel graph pooling algorithm to reduce token redundancy by selecting the discriminative vertices of G mri and G pet generated in the previous stages.As shown in Fig. 4. The algorithm evaluates the importance of vertices in multiple ways.The structure-based learning module (SBLM) and the feature-based learning module (FBLM) are utilized to score vertices according to their local structure and feature information to receive scores s 1 and s 2 , respectively.Then, the structure-feature learning module (SFLM) obtains the final score s for each vertex by combining s 1 and s 2 .To make the final graph embedding more feature information, the vertex feature fusion module is employed to aggregate the features of the vertices to be pooled before discarding them.Finally, only the top-k vertices will be retained according to the final score s.The details of these procedures in the M mri branch can be described as follows, which is the same as the M pet branch.</p>
<p>As shown in Fig. 4, the graph G mri output by the T2G is fed into three branches to evaluate the importance of vertices in multiple ways.Since GCNs considers structural information of graphs, it is utilized to evaluate each vertex based on the structural information in SBLM.The mathematical representation is as follows:
s 1 ¼ r W À 1 2 A $ W À 1 2 XWð15Þ
where A $ and X 2 R NÂD are the adjacency matrix and the vertex features of the graph G mri , respectively.W denotes the diagonal vertex In FBLM, each vertex is scored by CNNs based on their feature information.It mainly consists of a 1D CNN and a Batch Normalization layer, mathematically represented as:
s 2 ¼ rðBNðConvðXÞÞÞð16Þ
where X 2 R NÂD represents the feature matrix of the graph G mri .</p>
<p>Then, the SFLM combines s 1 and s 2 to calculate the final scores of the vertices.Given the scores s 1 2 R NÂ1 and s 2 2 R NÂ1 obtained from SBLM and FBLM, respectively.First, add s 1 and s 2 to get a coarse score s0 2 R NÂ1 , then the coarse score s0 is fed into a 1D CNN to output the final scores s 2 R NÂ1 .It can be denoted as:
s ¼ BNðConvðs0ÞÞ; ands0 ¼ s 1 þ s 2ð17Þ
After that, the vertices are sorted by the final score s, and only the top-k vertices V0 ¼ v 1 ; Á Á Á ; v k f gwill be retained as pooling results.</p>
<p>Finally, To make the final graph embedding vectors more representational, we aggregate information from neighborhood vertices in the feature fusion module with graph attention network (GAT) before discarding the vertex set V 00 , where
V 00 ¼ V-V 0 ¼ v kþ1 ; Á Á Á ; v N f
g represents the set of vertices that will be discarded.It can be denoted as:
z 0 i ¼ r 1 K X K k¼1 X j2V i a k ij W k h jð18Þ
where z i and h j represent the feature vector and the neighbor vertices of the vertex v i , respectively.V i is the number of vertex v i 's adjacent vertices.K is the number of attention heads.a k ij is the kth attention value between z i and h j .W is the weight matrix.</p>
<p>Graph to tokens (G2T)</p>
<p>Given a subgraph G0 mri ¼ V0; A0 ð Þ of G mri obtained from the graph pooling stage, where V0 ¼ v 1 ; Á Á Á ; v k f gand A0 2 R kÂk represent the vertex set and the adjacency matrix of G0 mri , respectively.Let X0 2 R kÂD denotes the feature matrix of G0 mri .After the graph pooling stage, the feature matrix X0 is reassembled into token sequence z0 p 2 R kÂD in G2T, then the class token z mri cls 2 R 1ÂD and a new positional encoding are added to z0 p 2 R kÂD for providing spatial information, that can be expressed as follows:
z0 p ¼ reshapeðX0Þð19Þz out ¼ z mri cls kz0 p Â Ã þ PE; PE2 R ðkþ1ÞÂDð20Þ
As shown in Fig. 2, the M pet branch follows the identical operation as M mri branch.</p>
<p>Experiment and results</p>
<p>In this section, the experimental setup and the results of performance evaluation measures are provided.Meanwhile, the activated area of CsAGP is visualized.</p>
<p>Experimental setup</p>
<p>All experiments are implemented on a workstation with two Intel Xeon Gold 6330 CPUs and four Nvidia A100 GPUs with a total of 160 GB of video memory.This workstation is equipped with Ubuntu 20.04.1 LTS.We built our model on Pytorch 1.12.0 framework and trained for 300 epochs.Adam is applied as the optimizer, and more details of experiment settings are as follows: (i) batch size is set to 128; (ii) loss function adopts the CrossEntropy; (iii); the initial learning rate is set to 1 Â 10 -5 and weight decay is set to 5 Â 10 -4 .In the experimental data, 60% of the data were randomly selected for training, 20% were chosen randomly for validation, and the rest 20% of subjects were used as test data.</p>
<p>For CsAGP, considering the difference in resolution and information contained in MRI and PET images, Following Ref. (Chen et al., 2021); we set K¼ 3, M¼ 1, N¼ 3. K signifies the number of CsAGP Block, M and N indicate the number of Encoder of the PET and MRI branches, respectively.Taking into account the computation costs and benefits together as a whole, the pooling rate r is set to 0.5.</p>
<p>Performance evaluation</p>
<p>To provide a quantitative assessment of the effectiveness of the suggested method for diagnosing AD, several evaluation metrics, including accuracy, specificity, and sensitivity, were computed as follows:
accuracy ¼ TP þ TN TP þ TN þ FP þ FNð21Þsensitivity ¼ TP TP þ FNð22Þspecificity ¼ TN FP þ TNð23Þ
The terms ''true positive," ''true negative," ''false positive," and ''false negative" are represented as ''TP," ''TN," ''FP," and ''FN," respectively.In addition to the three criteria discussed above, the area under the curve (AUC) is another factor considered when assessing performance.The area under the receiver operating characteristic curve (ROC), sometimes known as the area under the receiver operating characteristic curve (AUC), is a performance matrix employed to measure the quality of a classifier, and a large value of AUC indicates better classification performance.</p>
<p>Experiment results</p>
<p>In our experiments, the whole data was divided into AD vs. CN, AD vs. MCI, CN vs. MCI, and AD vs. CN vs.MCI groups to evaluate CsAGP.Each group of experiments was conducted unimodal (MRI or PET) and multimodal (MRI and PET).To make the results more convincing, we took two identical images as the model's input when conducting unimodal experiments.Table 2 demonstrates the comparison of the classification performances of each group.</p>
<p>As can be seen from Table 2, the performance of our CsAGP is outperforming the unimodal method.Specifically, the developed multimodal method obtains the classification accuracies of 99.04%, 97.43%, 98.57%, and 98.72% on AD vs. CN, AD vs. MCI, CN vs. MCI, and AD vs. CN vs. MCI, and the accuracies of MRI modality are 97.87%,95.37%, 94.94%, and 94.21%, respectively.</p>
<p>Compared to MRI modality, the proposed multimodal method improves the classification performance by 1.17%, 2.06%, 3.63%, and 4.51% on AD vs. CN, AD vs. MCI, CN vs. MCI, and AD vs. CN vs. MCI, respectively.For PET modality, the accuracies on AD vs. CN, AD vs. MCI, CN vs. MCI, and AD vs. CN vs.MCI are 95.92%,94.12%, 94.69%, and 93.37%, respectively.Compared to PET modality, the proposed multimodal method improves performance rises of 3.12%, 3.31%, 3.88%, and 5.35%, respectively.The proposed multimodal method can improve classification accuracy by combining MRI and PET significantly compared with the unimodal method.</p>
<p>On the other hand, it can also be found that the classification accuracy of MRI modality is surpasses PET modality in each group of classification experiments.Compared with PET modality, the accuracy of MRI increases by 1.95%, 1.25%, 0.25%, and 0.84% on AD vs. CN, AD vs. MCI, CN vs. MCI, and AD vs. CN vs. MCI, respectively.It is evident that the CsAGP can capture more discriminative features on MRI images when extracting unimodal features.We consider this is due to the high resolution of MRI images compared to PET images, which allows for better differentiation between soft tissue and anatomical structures.</p>
<p>Compared to the results of the other group tasks on the ADNI database, the diagnostic accuracy of the AD vs. CN task is, on the whole, higher than that of the other tasks.The same results are also in Ref. (Gao et al., 2022).This can be interpreted as AD's primary neuroimaging features can be distinguished more easily from those of CN and MCI.Since the subtle AD-related changes that occur in MCI are not noticeable, distinguishing MCI from AD and CN only by neuroimaging data is difficult.We further present the performance of each group to demonstrate the differences between the groups intuitively.As seen in Fig. 5, the multimodal performance acts better than the unimodal, which displays that the classification performance can boost the classification efficiencies further by joining the MRI and PET modalities.</p>
<p>Comparison with other methods</p>
<p>In this section, we compared our CsAGP to several other multimodal methods that are based on the ADNI database.As shown in Table 3, methods of comparison include the raw images-based methods (Zhang et al., 2019;Fang et al., 2020;Liu et al., 2022;Kun et al., 2020), the traditional machine learning method (Shi, 2022); the fused image-based method (Song et al., 2021); the generated image-based method (Zhang et al., 2022), the neuroimaging and clinical data-based method (Zhang et al., 2019).</p>
<p>In the AD vs. CN task, the accuracy of Fang et al. (Fang et al., 2020) was 99.27%, which is slightly larger than our suggested method.The reason is due to their utilization of ensemble learning, where the output of their model is based on three CNNs (GooleNet, ResNet, and DenseNe).By combining multiple different CNNs, they could leverage their diversity and differences.Each CNN may perform better on different subsets of data or feature subspaces.By aggregating their predictions through ensemble learning, they were able to reduce bias and variance, improving the overall accuracy of the model.</p>
<p>Additionally, Ref. (Fang et al., 2020) also introduced a ''dropout" mechanism to discard low discrimination images, further reducing noise in their model's input data.Although ensemble learning can enable them to achieve higher classification accuracy, training three CNNs requires many parameters and computation.In addition, compared with Fang et al. (Fang et al., 2020), CsAGP gets the best results except for accuracy.</p>
<p>In the AD vs.MCI task, the sensitivity metric reported by Liu et al. (Liu et al., 2022) was 94.91%,only 0.66% higher than ours, which means that the ability of their model to identify positive examples is slightly more than ours.They diagnosed AD by fusing multi-scale gray and white matter features from MRI images, while we only considered 2D slice images and single-scale feature information.By extracting features at different scales and fusing them together, the model can comprehensively utilize both local details and global contextual information, enhancing its understanding and expression capability of the images.Additionally, Ref. (Liu et al., 2022) employs the channel attention mechanism to automatically learn the importance weights of each channel, enabling the model to focus on relevant features for the task.By enhancing important channels, the model can improve its perception of crucial information, enhancing its performance.</p>
<p>Our CsAGP gets the best diagnostic performance in CN vs.MCI and AD vs. CN vs.MCI tasks.This can be attributed to several factors.Firstly, in addition to leveraging high-level features from different modalities, we also pay attention to the fusion of low-level features across modalities.This comprehensive integration of both high-level and low-level features enables the CsAGP to capture a more comprehensive representation of multimodal data.Secondly, by simultaneously conducting feature extraction and fusion stages for different modalities, we facilitate the effective integration of multimodal features.This simultaneous processing allows the CsAGP to learn shared representations and exploit complementary information from different modalities, further enhancing its performance.Furthermore, for reasons that the network parameters can be drastically decreased thanks to the CAFM and the RPR framework, the computational complexity and memory cost of our CsAGP does not rise.</p>
<p>Ablation experiments</p>
<p>Ablation experiments were carried out in this section of our CsAGP in order to demonstrate the efficacy of the CAFM and the RPR framework.To provide an accurate comparison, all experiments utilized the same settings for a fair comparison.</p>
<p>To reduce token redundancy and computation costs, we proposed a graph pooling algorithm to select discriminative tokens, which evaluates tokens in both feature and structural ways.Experiments were conducted to investigate the influence of the graph pooling algorithm on the prediction performance.. Since multi-classification tasks are more challenging than binary classification, the CsAGP was evaluated with different pooling rate r values.The results of the AD vs. CN vs.MCI task are reported in Table 4.</p>
<p>It can be seen that the classification accuracy is generally increasing with the increase of r.Specifically, when the pooling rate r increases from 0.1 to 0.5, the classification accuracy of CsAGP increases from 96.30% to 98.72%, a rise of 2.42%.However, the trend of increasing classification performance gradually flattens out when the pooling rate r is greater than 0.5.For example, when r¼ 0:9, the accuracy is 99.21%,only up 0.49% compared to r¼ 0:5.Therefore, considering computation costs and benefits together as a whole, r is set to 0.5 in our experiments.</p>
<p>As the pooling rate r increases, more tokens are preserved, allowing the model to capture more information and consequently leading to a rapid improvement in model performance.However, as r continues to increase, the noise and the computational cost of the model also increase.As a result, the trend of performance improvement of the model gradually flattens out.</p>
<p>To investigate the effectiveness of the FBLM and the SFLM, we conducted a series of experiments with different strategies.The results are listed in Table 5. Method A means using MLP to evaluate the vertex feature information (FBLM<em>) and linearly weighting sum vertex scores s 1 and s 2 (SFLM</em>).By changing SFLM<em> to SFLM, the accuracy improves by 0.24% (Method A vs. Method B).When we change FBLM</em> to FBLM, the accuracy increases by 0.4% (Method A vs. Method C).Further, when using both FBLM and SFLM, as Method D, the accuracy rises by 0.49% (Method A vs. Method D).These results validate that the comprehensive consideration of both vertex position and feature    As seen from Table 6, under the influence of the CAFM, the accuracy increases by 1.33%, 1.38%, 2.9%, and 3.02% on AD vs. CN, AD vs. MCI, CN vs. MCI, and AD vs. CN vs. MCI, respectively.These results indicate that fusing multi-level features from different modalities can further improve model performance.High-level features often contain more abstract and semantically rich information, capturing the high semantics and contextual information of images.</p>
<p>On the other hand, low-level features focus more on low-level details and local features.By fusing multi-level features, it is possible to fully utilize the complementarity of high-level and low-level features, providing a more comprehensive and rich feature representation, and enhancing the model's understanding and expressive capability.Furthermore, high-level features are usually less sensitive to modality differences, while low-level features are more sensitive to such differences.By integrating multi-level features, the impact of modality differences can be reduced, enhancing the model's robustness and generalization ability towards multimodal images.</p>
<p>In addition, every branch of the transformer in our model develops the class token as an agent, which can exchange information between branches by the cross-attention mechanism.This makes it possible to generate attention maps in linear time rather than quadratic time.</p>
<p>Visualization</p>
<p>Fig. 6 shows the activated areas of our CsAGP by the Grad-CAM technology (Selvaraju et al., 2017).The images on each cell's left and right sides represent a slice image of the subject in various modalities, and the AD-related activation maps corresponded with the relevant slice image.From Fig. 6(a), it is seen from the heatmap that the areas of interest are dispersed throughout the brain.It means that our model can analyze abnormalities throughout the brain that are related to AD.</p>
<p>Compared with CNNs, transformer-based networks with a high receptive field have various advantages, one of which is the presence of wide activated areas.In addition, compared with AD, the heatmap areas of MCI (Fig. 6(c)) are relatively concentrated, which may be because MCI is the prodromal stage of AD with few lesion areas.The heatmap areas of CN (Fig. 6(e)) are mainly focused on the center of the brain.</p>
<p>Furthermore, due to different imaging protocols and information emphases, the heatmap areas of the three stages of PET images (Fig. 6(b),Fig. 6(d), and Fig. 6(f)) are relatively concentrated.It can be seen that the heatmap areas of different stages focus on different brain regions.This result further proved the view in Ref. (Suk et al., 2014) that complementary information can be obtained from a variety of modalities to improve AD diagnostic performance.</p>
<p>Conclusion</p>
<p>This paper proposes a dual-branch vision transformer with the cross-attention mechanism and a graph pooling algorithm, CsAGP, for multimodal AD classification.We designed a multimodal feature fusion strategy based on the cross-attention mechanism to effectively learn the shared feature representation of MRI and PET images.Furthermore, a concise framework based on a graph pooling algorithm is developed to reduce token redundancy in the proposed model.Extensive experiments on the ADNI database demonstrate that the classification accuracy of our proposed CsAGP for AD vs. CN, AD vs. MCI, CN vs. MCI, and AD vs. CN vs.MCI are 99.04%,97.43%, 98.57%, and 98.72%, which is 4.93%, 2.99%, 8.22% and 18.72% higher than current multimodal AD diagnosis methods, respectively.</p>
<p>The proposed CsAGP is slice-based and considers only axial view slices.Since 2D images cannot include all the information from a full brain scan.In addition, this study has not yet conducted a time processing comparison.Expanding the CsAGP for a full brain analysis and conducting comparative study on time processing will be a part of our future research.</p>
<p>Declaration of Competing Interest</p>
<p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.by the Laboratory for Neuro Imaging at the University of Southern California.</p>
<p>Data/Code Availability</p>
<p>The code will be available on https://github.com/weimingyang4/CsAGP after the article is accepted, and the authors do not have permission to share data.CsAGP: Detecting Alzheimer's Disease from Multimodal Images via Dual-Transformer with Cross-Attention and Graph Pooling Anonymized.</p>
<p>Fig. 2 .
2
Fig. 2.An illustration of the proposed CsAGP.</p>
<p>Fig. 3 .
3
Fig. 3. Multi-heads cross-attention feature fusion for M mri branch.</p>
<p>Fig. 4 .
4
Fig. 4. The illustration of the RPR framework of the Mmri branch.</p>
<p>Fig. 5 .
5
Fig. 5. Classification performance of various groups.</p>
<p>C</p>
<p>. Tang, M. Wei, J. Sun et al.Journal of King Saud University -Computer and Information Sciences 35 (2023) 101618information plays a crucial role in the graph pooling process.Vertex position information aids in understanding the contextual and topological relationships within the graph structure, while vertex feature information provides descriptions of vertex attributes and features, offering crucial information for vertex representation and learning.Combining these two aspects of information can assist the model in better understanding and processing graph data, enhancing the model's performance and expressive capabilities.To evaluate the effectiveness of CAFM in CsAGP, we removed the CAFM in the CsAGP, while other configurations remained the same.It can help us to focus on the high-level features fusion of two modalities.Comparative experiments were performed in all diagnosis tasks.</p>
<p>Table 1
1
The clinical information of the subjects.Fig. 1.Contrasting of the raw and preprocessed images.C. Tang, M. Wei, J. Sun et al.Journal of King Saud University -Computer and Information Sciences 35 (2023) 101618
DiagnosisNumberAgeGender(F/M)MMSECDRAD21475.1 ± 7.895/11921.2 ± 4.10.9 ± 0.4MCI22676.0 ± 7.482/14425.6 ± 4.30.5 ± 0.3CN32676.1 ± 6.4165/16128.7 ± 1.40 ± 0</p>
<p>Table 2
2
Classification results of the unimodal and multimodal method.
C. Tang, M. Wei, J. Sun et al.Journal of King Saud University -Computer and Information Sciences 35 (2023) 101618Auxiliary diagnosisModalitySEN (%)SPE (%)ACC (%)AUC (%)AD vs CNMRIPETMRI + PET96.7391.7297.9698.3997.8799.5497.8795.9299.0499.6298.9799.80AD vs MCIMRIPETMRI + PET92.2589.0394.2596.7296.3398.8195.3794.1297.4398.7998.2799.23CN vs MCIMRIPETMRI + PET92.6494.6198.5297.1094.7798.6194.9494.6998.5798.9298.8399.76AD vs CN vs MCIMRIPETMRI + PET92.9692.2898.6596.8896.4999.3494.2193.3798.7298.8298.2499.86SEN: sensitivity; SPE: specificity; ACC: accuracy.</p>
<p>Table 3
3
Performance comparison of the different existing methods.
TasksMethodsSEN (%)SPE (%)ACC (%)AUC (%)AD vs CNFang et al (2020) (2020)95.8998.7299.27n/aAD vs CNZhang et al (2019) (2019)96.5895.3698.4798.61AD vs CNShi et al (2022) (2022)96.1097.4796.7697.03AD vs CNSong et al (2021) (2021)93.3394.2794.11n/aAD vs CNCsAGP (ours)97.9699.5499.0499.80AD vs MCIFang et al (2020) (2020)89.7193.5992.57n/aAD vs MCIZhang et al (2019) (2019)90.1191.8285.7488.15AD vs MCISong et al (2022) (Song et al., 2021)71.1985.9480.80n/aAD vs MCILiu et al (2022) (2022)94.9198.5294.4497.00AD vs MCICsAGP (ours)94.2598.8197.4399.23CN vs MCIFang et al (2020) (2020)88.3692.5690.35n/aCN vs MCIZhang et al (2019) (2019)97.4384.3188.2088.01CN vs MCIShi et al (2022) (2022)85.9870.9080.7378.75CN vs MCISong et al (2022) (Song et al., 2021)84.6985.6085.00n/aCN vs MCICsAGP (ours)98.5298.6198.5799.76AD vs CN vs MCISong et al (2021) (2021)55.6783.4071.52n/aAD vs CN vs MCIHan et al (2020) (Kun et al., 2020)n/an/a67.74n/aAD vs CN vs MCIZhang et al (2022) (2022)n/an/a80.0095.00AD vs CN vs MCICsAGP (ours)98.6599.3498.7299.86
Bold value means the best indicator value under the same conditions and 'n/a' means no data.</p>
<p>Table 4
4
The classification results for different..r .Tang, M. Wei, J. Sun et al.Journal of King Saud University -Computer and Information Sciences 35 (2023) 101618
rSEN (%)SPE (%)ACC (%)AUC (%)0.195.6198.0496.3099.320.395.5698.4297.0099.490.598.6599.3498.7299.830.798.6999.3698.7999.860.999.0099.5799.2199.90Table 5Ablations on FBLM and SFLM.MethodFBLM<em>SFLM</em>FBLMSFLMSEN (%)SPE (%)ACC (%)AUC (%)A B C Dp pp pp pp p98.49 98.46 98.51 98.6599.25 99.20 99.27 99.3498.23 98.47 98.63 98.7299.86 99.79 99.81 99.86Table 6Classification results of removing CAFM.Auxiliary diagnosisSEN (%)SPE (%)ACC (%)AUC (%)AD vs CNw/o CAFM97.9696.1399.5498.4499.0497.7199.8098.71AD vs MCIw/o CAFM94.2593.0098.8196.9397.4396.0599.2398.05CN vs MCIw/o CAFM98.5294.7098.6196.6098.5795.6799.7699.12AD vs CN vs MCIw/o CAFM98.6595.3099.3497.7398.7295.7099.8699.26
Fig.6.AD-related visualization map results using Grad-CAM.C</p>
<p>Available at https://adni.loni.usc.edu.
Available at https://fsl.fmrib.ox.ac.uk/fsl/fslwiki.
Available at https://github.com/ANTsX/ANTs.
Available at https://www.nitrc.org/projects/art.
Available at https://github.com/FNNDSC/med2image.
AcknowledgementAnonymized.Data/Code AvailabilityThe code will be available on https://github.com/weimingyang4/CsAGP after the article is accepted, and the authors do not have permission to share data.FundingThis work is supported by the National Natural Science Foundation of China (62276092); Key Science and Technology Program of
. Henan Province, 212102310084</p>
<p>. British, Heart Foundation Accelerator Award. 18/3/34220</p>
<p>. Royal Society International Exchanges Cost Share Award. </p>
<p>. Hope Foundation for Cancer Research. RM60G0680</p>
<p>. Medical Research Council Confidence in Concept Award. </p>
<p>. -Uk Industrial Sino, Fund, UK (RP202G0289)</p>
<p>. Global Challenges Research Fund (GCRF). </p>
<p>. LIAS. </p>
<p>. Data Science Enhancement Fund. </p>
<p>-Uk Sino, Education Fund. UK (OP202006)</p>
<p>Data collection and sharing for this project was funded by the Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). Biological Biotechnology, Research Sciences, Council, ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie, Alzheimer's Association. Alzheimer's Drug Discovery Foundation; Araclon Biotech</p>
<p>. Inc Bioclinica, ; Biogen, Squibb Bristol-Myers, Company, Cer-eSpir, Inc.; Cogstate</p>
<p>Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech. Fujirebio; GE Healthcare; IXICO Ltd. LundbeckInc.</p>
<p>NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation. Piramal Imaging. ServierLLC</p>
<p>The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer's Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated References Alzheimer's disease facts and figures. 10.1002/alz.12638Alzheimers Dement. 1842022. Apr. 2022The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org)</p>
<p>Crossvit: Cross-attention multi-scale vision transformer for image classification. C.-F R Chen, Q Fan, R Panda, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>Generative adversarial networks: an overview. A Creswell, T White, V Dumoulin, K Arulkumaran, B Sengupta, A A Bharath, IEEE Sig. Process. Mag. 3512018</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, arXiv:2010.119292020arXiv preprint</p>
<p>Ensemble of deep convolutional neural networks based multi-modality images for Alzheimer's disease diagnosis. X Fang, Z Liu, M Xu, IET Image Proc. 1422020</p>
<p>Task-induced pyramid and attention GAN for multimodal brain image imputation and classification in alzheimer's disease. X Gao, F Shi, D Shen, M Liu, IEEE J. Biomed. Health Inform. 2612022</p>
<p>Multimodal attention-based deep learning for Alzheimer's disease diagnosis. M Golovanevsky, C Eickhoff, R Singh, arXiv:2206.088262022arXiv preprint</p>
<p>M3T: Three-Dimensional Medical Image Classifier Using Multi-Plane and Multi-Slice Transformer. J Jang, D Hwang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Multi-modal feature selection with feature correlation and feature structure fusion for MCI and AD classification. Z Jiao, S Chen, H Shi, J Xu, Brain Sci. 121802022</p>
<p>Identifying early mild cognitive impairment by multi-modality MRI-based deep learning. L Kang, J Jiang, J Huang, T Zhang, Front. Aging Neurosci. 122062020</p>
<p>Image scene geometry recognition using low-level features fusion at multi-layer deep CNN. A Khan, A Chefranov, H Demirel, 10.1016/j.neucom.2021.01.085Neurocomputing. 440Jun. 2021</p>
<p>Multimodal data Alzheimer's disease detection based on 3D convolution. Z Kong, M Zhang, W Zhu, Y Yi, T Wang, B Zhang, Biomed. Signal Process. Control. 751035652022</p>
<p>Alzheimer's disease classification method based on multi-modal medical images. H A N Kun, P A N Haiwei, Z Wei, B Xiaofei, C Chunling, H E Shuning, J. Tsinghua Univ. (Sci. Technol.). 6082020</p>
<p>Addformer: Alzheimer's disease detection from structural MRI using fusion transformer. R Kushol, A Masoumzadeh, D Huo, S Kalra, Y.-H Yang, 10.1109/ISBI52829.2022.97614212022 IEEE 19th International Symposium on Biomedical Imaging (ISBI). 2022</p>
<p>Multiclass diagnosis of stages of Alzheimer's disease using linear discriminant analysis scoring for multimodal data. W Lin, Q Gao, M Du, W Chen, T Tong, Comput. Biol. Med. 1342021. 104478</p>
<p>A multimodel deep convolutional neural network for automatic hippocampus segmentation and classification in Alzheimer's disease. M Liu, Neuroimage. 2081164592020</p>
<p>Image Enhancement Guided Object Detection in Visually Degraded Scenes. H Liu, F Jin, H Zeng, H Pu, B Fan, IEEE Transactions on Neural Networks and Learning Systems. 2023</p>
<p>Diagnosis of Alzheimer's disease via an attention-based multi-scale convolutional neural network. Z Liu, H Lu, X Pan, M Xu, R Lan, X Luo, Knowl.-Based Syst. 2381079422022</p>
<p>Deep convolutional neural networks with ensemble learning and generative adversarial networks for Alzheimer's disease image data classification. R Logan, Front. Aging Neurosci. 132021. 720226</p>
<p>Classification of Alzheimer's Disease via Vision Transformer. Y Lyu, X Yu, D Zhu, L Zhang, Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments. the 15th International Conference on PErvasive Technologies Related to Assistive Environments2022</p>
<p>Multimodal neuroimaging neural network-based feature detection for diagnosis of Alzheimer's disease. Front. X Meng, Aging Neurosci. 149112202022</p>
<p>A multi-classification accessment framework for reproducible evaluation of multimodal learning in Alzheimer's disease. F Nan, IEEE/ACM Trans. Comput. Biol. Bioinf. 2022</p>
<p>J Pan, S Wang, arXiv:2206.13393Cross-Modal Transformer GAN: A Brain Structure-Function Deep Fusing Framework for Alzheimer's Disease. 2022arXiv preprint</p>
<p>. C Patterson, World alzheimer report. 2018. 2018</p>
<p>Dynamicvit: Efficient vision transformers with dynamic token sparsification. Y Rao, W Zhao, B Liu, J Lu, J Zhou, C.-J Hsieh, Adv. Neural Inf. Proces. Syst. 342021</p>
<p>Diagnosis of Alzheimer's disease using universum support vector machine based recursive feature elimination (USVM-RFE). B Richhariya, M Tanveer, A H Rashid, 10.1016/j.bspc.2020.101903Biomed. Signal Process. Control. 59May 2020</p>
<p>Gradcam: Visual explanations from deep networks via gradient-based localization. R R Selvaraju, M Cogswell, A Das, R Vedantam, D Parikh, D Batra, 2017. 2017</p>
<p>ASMFS: Adaptive-similarity-based multi-modality feature selection for classification of Alzheimer's disease. Pattern Recogn. Y Shi, 2022. 108566126</p>
<p>Leveraging coupled interaction for multimodal Alzheimer's disease diagnosis. Y Shi, H.-I Suk, Y Gao, S.-W Lee, D Shen, IEEE Trans. Neural Networks Learn. Syst. 3112019</p>
<p>An effective multimodal image fusion method using MRI and PET for Alzheimer's disease diagnosis. J Song, J Zheng, P Li, X Lu, G Zhu, P Shen, Front. Digital Health. 36373862021</p>
<p>Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis. H.-I Suk, S.-W Lee, D Shen, A D N Initiative, Neuroimage. 1012014</p>
<p>H Tan, M Bansal, arXiv:1908.07490Lxmert: Learning cross-modality encoder representations from transformers. 2019arXiv preprint</p>
<p>Alzheimer's disease diagnosis via multimodal feature fusion. Y Tu, S Lin, J Qiao, Y Zhuang, P Zhang, Comput. Biol. Med. 1481059012022</p>
<p>Discrimination and conversion prediction of mild cognitive impairment using convolutional neural networks. C Wu, Quant. Imaging Med. Surg. 8109922018</p>
<p>Advit: vision transformer on multi-modality pet images for alzheimer disease diagnosis. X Xing, G Liang, Y Zhang, S Khanal, A.-L Lin, N Jacobs, 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI). IEEE2022</p>
<p>Pairwise feature-based generative adversarial network for incomplete multi-modal Alzheimer's disease diagnosis. H Ye, Q Zhu, Y Yao, Y Jin, D Zhang, Vis. Comput. 2022</p>
<p>BPGAN: Brain PET synthesis from MRI using generative adversarial network for multi-modal Alzheimer's disease diagnosis. J Zhang, X He, L Qing, F Gao, B Wang, Comput. Methods Programs Biomed. 2172022. 106676</p>
<p>Multimodal deep learning model for auxiliary diagnosis of Alzheimer's disease. F Zhang, Z Li, B Zhang, H Du, B Wang, X Zhang, Neurocomputing. 3612019</p>
<p>Multimodal 2.5 D convolutional neural network for diagnosis of Alzheimer's Disease with magnetic resonance imaging and positron emission tomography. X Zhang, W Lin, M Xiao, H Ji, Prog. Electromagn. Res. 1712021</p>
<p>Multimodal neuroimaging feature fusion for diagnosis of Alzheimer's disease. T Zhang, M Shi, J. Neurosci. Methods. 3411087952020</p>
<p>Graph convolutional network analysis for mild cognitive impairment prediction. X Zhao, F Zhou, L Ou-Yang, T Wang, B Lei, IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019). IEEE2019. 2019. 2019</p>
<p>Efficient self-attention mechanism and structural distilling model for Alzheimer's disease diagnosis. J Zhu, Comput. Biol. Med. 1471057372022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9274 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9274</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9274</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-272987404</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.20303v1.pdf" target="_blank">A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions</a></p>
                <p><strong>Paper Abstract:</strong> In an era where large language models (LLMs) are increasingly integrated into a wide range of everyday applications, research into these models' behavior has surged. However, due to the novelty of the field, clear methodological guidelines are lacking. This raises concerns about the replicability and generalizability of insights gained from research on LLM behavior. In this study, we discuss the potential risk of a replication crisis and support our concerns with a series of replication experiments focused on prompt engineering techniques purported to influence reasoning abilities in LLMs. We tested GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, and Llama 3-70B, on the chain-of-thought, EmotionPrompting, ExpertPrompting, Sandbagging, as well as Re-Reading prompt engineering techniques, using manually double-checked subsets of reasoning benchmarks including CommonsenseQA, CRT, NumGLUE, ScienceQA, and StrategyQA. Our findings reveal a general lack of statistically significant differences across nearly all techniques tested, highlighting, among others, several methodological weaknesses in previous research. We propose a forward-looking approach that includes developing robust methodologies for evaluating LLMs, establishing sound benchmarks, and designing rigorous experimental frameworks to ensure accurate and reliable assessments of model outputs.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9274.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9274.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot Chain-of-Thought (all models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought Prompting (suffix: "Let's think step by step")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replicated zero-shot chain-of-thought (COT) prompting by suffixing tasks with a step-by-step cue and measuring accuracy across multiple reasoning benchmarks and LLMs; found no consistent improvement overall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, Llama 3-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A set of reasoning benchmarks: commonsense, strategy/implicit reasoning, numerical reasoning, science QA, and Cognitive Reflection Test items; authors handpicked 150 high-quality items per benchmark (n=750 total).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot chain-of-thought: task suffix 'Let's think step by step' (exact suffix from original study); same prompts appended to tasks; final-answer marker '####' required.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Base (unmodified) prompts without chain-of-thought suffix</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Overall average improvement: 0% (χ2 = 0.06, p = .8). Max per-benchmark reported: NumGLUE +2.78% (χ2 = 1.78, p = .18).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Overall +0% mean; per-model/-task differences observed (examples below) but not statistically significant in aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that modern models may implement chain-of-thought-like reasoning by default (so explicit COT prompts no longer help); also observed that increased output verbosity does not reliably increase accuracy beyond a point.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>150 validated tasks per benchmark (total n = 750); temperature = 0; instruction added: write final answer after '####'; outputs classified by regex, GPT-4o verification, and manual double-checks; experiments run June 6–17, 2024.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9274.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9274.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COT Llama3-70B StrategyQA (negative)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting on Llama 3-70B for StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying COT to Llama 3-70B on StrategyQA produced a statistically significant decrease in accuracy in this replication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>StrategyQA: questions requiring implicit multi-step reasoning; authors used 150 validated items from this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Chain-of-thought suffix 'Let's think step by step' (zero-shot COT); final-answer marker '####'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Base (no COT suffix)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy decreased by 11.33% with COT (χ2 = 4.47, p < .05).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-11.33% accuracy (significant decrease)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest that some models or model versions can be impaired by explicit COT prompts (possibly due to internal default reasoning behaviors or instruction interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>As above: 150 validated StrategyQA items, temperature = 0, '####' final-answer marker, outputs evaluated via regex/GPT-4o/manual checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9274.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9274.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COT Llama3-70B CommonsenseQA (non-significant +)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting on Llama 3-70B for CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying COT to Llama 3-70B on CommonsenseQA produced a positive but statistically non-significant accuracy change in this replication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Commonsense multiple-choice questions; authors used 150 validated items.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Chain-of-thought suffix 'Let's think step by step' (zero-shot COT); final-answer marker '####'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Base (no COT suffix)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Observed +8.67% accuracy with COT (χ2 = 2.19, p = .14) — positive but not statistically significant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+8.67% (non-significant, p = .14)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note heterogeneous model-task interactions; some positive deltas occur but are not consistent or significant across models/benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>150 validated CommonsenseQA items, temperature = 0, '####' marker, outputs classified by regex/GPT-4o/manual checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9274.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9274.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExpertPrompting (all models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ExpertPrompting (instruct model to impersonate an expert)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting models to adopt an expert persona before solving tasks produced tiny, non-significant average improvements across the evaluated benchmarks and models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, Llama 3-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same validated set of reasoning benchmarks (150 items each).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>ExpertPrompting: preface instructing the LLM to impersonate a high-expertise role relevant to the task; final-answer marker '####'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Base (no expert persona instruction)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average improvement across benchmarks: +1% (χ2 = 1.57, p = .21) — not significant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+1% (non-significant)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors conclude ExpertPrompting did not replicate claimed gains; differences in evaluation metrics and LLM-based scoring in original work may explain prior positive claims.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>150 tasks per benchmark, temperature = 0, random selection when multiple persona variants existed, outputs processed with '####' extraction and multi-step verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9274.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9274.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sandbagging (high education)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sandbagging prefix: 'very educated' user biography (high-education persona)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prepending a high-education user biography to tasks (to test 'sandbagging') produced no significant performance difference versus base prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, Llama 3-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reasoning benchmark set (150 curated items per benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Sandbagging high-education: prefixing tasks with a 'very educated' user biography; final-answer marker '####'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Base (no user biography) and low-education prefix</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy change: -1% for high-education prefix (χ2 = 1.64, p = .20) — not significant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to low-education prefix: no significant difference; low-education often slightly better (average +0.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>≈ -1% (non-significant) for high-education vs base; high vs low difference ≈ 0.1% (non-significant).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors could not replicate the sandbagging effect; results suggest user-biography prefixes do not systematically change answers under this experimental setup.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>High- and low-education biographies randomly assigned; 150 validated tasks per benchmark; temperature = 0; '####' marker; outputs validated with regex/GPT-4o/manual checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9274.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9274.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sandbagging (low education)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sandbagging prefix: 'very uneducated' user biography (low-education persona)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prepending a low-education user biography to tasks did not reduce performance and in some cases correlated with marginally better accuracy; no significant sandbagging detected.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, Llama 3-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same curated benchmark set.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Sandbagging low-education: prefixing tasks with a 'very uneducated' user biography; final-answer marker '####'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Base (no biography) and high-education prefix</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average accuracy change: -1% for low-education prefix (χ2 = 1.24, p = .27) — not significant. Comparing high vs low, low-education had average +0.1% improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>≈ -1% (non-significant) vs base; high vs low difference ≈ +0.1% for low-education.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Replication did not support sandbagging claims; small fluctuations observed are within noise and not statistically robust.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Random selection of biography variants; 150 items per benchmark; temperature = 0; final-answer '####' marker; outputs validated via regex/GPT-4o/manual checking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9274.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9274.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmotionPrompting (all models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EmotionPrompting (augment task with emotional cues)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augmenting tasks with emotional stimuli (e.g., 'This is very important to my career') produced no significant average performance improvement in this replication, contrary to strong claims in the original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, Llama 3-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same curated benchmark set (150 validated items per benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>EmotionPrompting: appending one of multiple emotional suffixes (authors randomly selected among the original 11 variants when applicable); final-answer marker '####'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Base (no emotional suffix)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Overall replication: +1% accuracy (χ2 = 0.11, p = .74) — not significant. Max observed: +8.7% for Llama 3-8B on CommonsenseQA (χ2 = 1.94, p = .16).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Original paper reported much larger claims when cherry-picking the best cue; averaged reported gains in original paper recalculated as ~4.42% on some BIG-Bench tasks and ~2.58% across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Replication average +1% (non-significant); original study's largest (cherry-picked) gains much larger but not representative.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors highlight that original study cherry-picked best-performing emotional cue rather than averaging across cues; replication shows no systematic effect.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>When original used 11 emotional stimuli, authors randomly selected one per task (seeded); excluded 'Are you sure?' suffix because it prompts replies; temperature = 0; '####' marker; outputs validated by regex/GPT-4o and manual checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9274.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9274.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Re-Reading (Llama 3-8B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Re-Reading (repeat the task verbatim before answering) on Llama 3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Re-Reading (presenting the task twice) produced a statistically significant accuracy improvement for Llama 3-8B in the replication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Curated benchmarks (150 items per benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Re-Reading: repeating the task verbatim before letting the model answer; experiments also included variants with chain-of-thought suffix but replication highlights main Re-Reading effect.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Base (single presentation), and Re-Reading + COT variants tested</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Significant improvement observed (χ2 = 13.13, p < .05) for Llama 3-8B with Re-Reading.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Other models did not show consistent significant gains; e.g., Claude 3 Opus maximal +2% (χ2 = 1.27, p = .26) non-significant.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Statistically significant positive effect for Llama 3-8B (exact % point change per benchmark not reported in main text; χ2 reported).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Effect appears model-dependent; Re-Reading replicable for Llama 3 series in this study but not for other tested models, suggesting architecture/fine-tuning differences.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>150 validated tasks per benchmark, temperature = 0, '####' final-answer marker, outputs validated by regex/GPT-4o/manual checks; experiments run June 6–17, 2024.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9274.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9274.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Re-Reading (Llama 3-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Re-Reading (repeat the task verbatim before answering) on Llama 3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Re-Reading produced a statistically significant improvement for Llama 3-70B in this replication, mirroring the effect for the smaller Llama 3 variant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Curated benchmark suite (150 items each).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Re-Reading: repeat the task verbatim before answer; final-answer marker '####'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Base (single presentation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Significant improvement observed with Re-Reading (χ2 = 19.4, p < .05).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Statistically significant positive effect (χ2 reported); exact percent-point change per benchmark not enumerated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest Re-Reading's effectiveness depends on model family/version; Llama 3 models benefited here while other families did not.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>150 validated tasks per benchmark, temperature = 0, '####' marker, combined regex/GPT-4o/manual validation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9274.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9274.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Re-Reading (Claude 3 Opus)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Re-Reading (repeat the task) on Claude 3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Re-Reading did not produce a significant improvement for Claude 3 Opus in this replication; maximal observed +2% (non-significant).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same curated benchmark set.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Re-Reading: duplicate task text before answering; final-answer marker '####'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Base (single presentation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Maximal observed improvement +2% (χ2 = 1.27, p = .26) — not significant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+2% (non-significant)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Re-Reading benefits are model-dependent; observed significant gains only in the Llama 3 family in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>150 validated tasks per benchmark, temperature = 0, '####' marking, regex/GPT-4o/manual verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9274.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9274.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o verbosity vs accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o: increased response length under COT vs base and negligible accuracy change</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o produced much longer responses under chain-of-thought prompting (931 vs 531 characters) but showed essentially no accuracy improvement (~0.01%), indicating verbosity alone did not improve task accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Curated multi-benchmark set (150 items per benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Chain-of-thought prompting vs base; measured output length and accuracy with final-answer marker '####'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Base (no COT suffix)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average response length: base ≈ 531 characters vs COT ≈ 931 characters; accuracy difference ≈ +0.01% (negligible).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+0.01% accuracy despite ~400-character longer outputs under COT</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Merely increasing completion verbosity (longer chain-of-thought outputs) does not necessarily translate to better accuracy; modern models may already encode reasoning behavior internally.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Measured average response character lengths across tasks; temperature = 0; '####' final-answer marker; aggregate accuracy measured across curated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Large Language Models are Zero-Shot Reasoners <em>(Rating: 2)</em></li>
                <li>ExpertPrompting: Instructing Large Language Models to be Distinguished Experts <em>(Rating: 2)</em></li>
                <li>Discovering Language Model Behaviors with Model-Written Evaluations <em>(Rating: 2)</em></li>
                <li>EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus <em>(Rating: 2)</em></li>
                <li>Re-Reading Improves Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge <em>(Rating: 2)</em></li>
                <li>StrategyQA: A Question Answering Benchmark with Implicit Reasoning Strategies <em>(Rating: 2)</em></li>
                <li>NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks <em>(Rating: 2)</em></li>
                <li>ScienceQA <em>(Rating: 1)</em></li>
                <li>Cognitive Reflection Tests (CRT) (Hagendorff et al., 2023) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9274",
    "paper_id": "paper-272987404",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Zero-shot Chain-of-Thought (all models)",
            "name_full": "Zero-shot Chain-of-Thought Prompting (suffix: \"Let's think step by step\")",
            "brief_description": "Replicated zero-shot chain-of-thought (COT) prompting by suffixing tasks with a step-by-step cue and measuring accuracy across multiple reasoning benchmarks and LLMs; found no consistent improvement overall.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple (GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, Llama 3-70B)",
            "model_size": null,
            "task_name": "CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)",
            "task_description": "A set of reasoning benchmarks: commonsense, strategy/implicit reasoning, numerical reasoning, science QA, and Cognitive Reflection Test items; authors handpicked 150 high-quality items per benchmark (n=750 total).",
            "presentation_format": "Zero-shot chain-of-thought: task suffix 'Let's think step by step' (exact suffix from original study); same prompts appended to tasks; final-answer marker '####' required.",
            "comparison_format": "Base (unmodified) prompts without chain-of-thought suffix",
            "performance": "Overall average improvement: 0% (χ2 = 0.06, p = .8). Max per-benchmark reported: NumGLUE +2.78% (χ2 = 1.78, p = .18).",
            "performance_comparison": null,
            "format_effect_size": "Overall +0% mean; per-model/-task differences observed (examples below) but not statistically significant in aggregate.",
            "explanation_or_hypothesis": "Authors hypothesize that modern models may implement chain-of-thought-like reasoning by default (so explicit COT prompts no longer help); also observed that increased output verbosity does not reliably increase accuracy beyond a point.",
            "null_or_negative_result": true,
            "experimental_details": "150 validated tasks per benchmark (total n = 750); temperature = 0; instruction added: write final answer after '####'; outputs classified by regex, GPT-4o verification, and manual double-checks; experiments run June 6–17, 2024.",
            "uuid": "e9274.0",
            "source_info": {
                "paper_title": "A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "COT Llama3-70B StrategyQA (negative)",
            "name_full": "Chain-of-Thought Prompting on Llama 3-70B for StrategyQA",
            "brief_description": "Applying COT to Llama 3-70B on StrategyQA produced a statistically significant decrease in accuracy in this replication.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3-70B",
            "model_size": "70B",
            "task_name": "StrategyQA",
            "task_description": "StrategyQA: questions requiring implicit multi-step reasoning; authors used 150 validated items from this benchmark.",
            "presentation_format": "Chain-of-thought suffix 'Let's think step by step' (zero-shot COT); final-answer marker '####'.",
            "comparison_format": "Base (no COT suffix)",
            "performance": "Accuracy decreased by 11.33% with COT (χ2 = 4.47, p &lt; .05).",
            "performance_comparison": null,
            "format_effect_size": "-11.33% accuracy (significant decrease)",
            "explanation_or_hypothesis": "Authors suggest that some models or model versions can be impaired by explicit COT prompts (possibly due to internal default reasoning behaviors or instruction interactions).",
            "null_or_negative_result": true,
            "experimental_details": "As above: 150 validated StrategyQA items, temperature = 0, '####' final-answer marker, outputs evaluated via regex/GPT-4o/manual checks.",
            "uuid": "e9274.1",
            "source_info": {
                "paper_title": "A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "COT Llama3-70B CommonsenseQA (non-significant +)",
            "name_full": "Chain-of-Thought Prompting on Llama 3-70B for CommonsenseQA",
            "brief_description": "Applying COT to Llama 3-70B on CommonsenseQA produced a positive but statistically non-significant accuracy change in this replication.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3-70B",
            "model_size": "70B",
            "task_name": "CommonsenseQA",
            "task_description": "Commonsense multiple-choice questions; authors used 150 validated items.",
            "presentation_format": "Chain-of-thought suffix 'Let's think step by step' (zero-shot COT); final-answer marker '####'.",
            "comparison_format": "Base (no COT suffix)",
            "performance": "Observed +8.67% accuracy with COT (χ2 = 2.19, p = .14) — positive but not statistically significant.",
            "performance_comparison": null,
            "format_effect_size": "+8.67% (non-significant, p = .14)",
            "explanation_or_hypothesis": "Authors note heterogeneous model-task interactions; some positive deltas occur but are not consistent or significant across models/benchmarks.",
            "null_or_negative_result": true,
            "experimental_details": "150 validated CommonsenseQA items, temperature = 0, '####' marker, outputs classified by regex/GPT-4o/manual checks.",
            "uuid": "e9274.2",
            "source_info": {
                "paper_title": "A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ExpertPrompting (all models)",
            "name_full": "ExpertPrompting (instruct model to impersonate an expert)",
            "brief_description": "Prompting models to adopt an expert persona before solving tasks produced tiny, non-significant average improvements across the evaluated benchmarks and models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple (GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, Llama 3-70B)",
            "model_size": null,
            "task_name": "CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)",
            "task_description": "Same validated set of reasoning benchmarks (150 items each).",
            "presentation_format": "ExpertPrompting: preface instructing the LLM to impersonate a high-expertise role relevant to the task; final-answer marker '####'.",
            "comparison_format": "Base (no expert persona instruction)",
            "performance": "Average improvement across benchmarks: +1% (χ2 = 1.57, p = .21) — not significant.",
            "performance_comparison": null,
            "format_effect_size": "+1% (non-significant)",
            "explanation_or_hypothesis": "Authors conclude ExpertPrompting did not replicate claimed gains; differences in evaluation metrics and LLM-based scoring in original work may explain prior positive claims.",
            "null_or_negative_result": true,
            "experimental_details": "150 tasks per benchmark, temperature = 0, random selection when multiple persona variants existed, outputs processed with '####' extraction and multi-step verification.",
            "uuid": "e9274.3",
            "source_info": {
                "paper_title": "A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Sandbagging (high education)",
            "name_full": "Sandbagging prefix: 'very educated' user biography (high-education persona)",
            "brief_description": "Prepending a high-education user biography to tasks (to test 'sandbagging') produced no significant performance difference versus base prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple (GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, Llama 3-70B)",
            "model_size": null,
            "task_name": "CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)",
            "task_description": "Reasoning benchmark set (150 curated items per benchmark).",
            "presentation_format": "Sandbagging high-education: prefixing tasks with a 'very educated' user biography; final-answer marker '####'.",
            "comparison_format": "Base (no user biography) and low-education prefix",
            "performance": "Average accuracy change: -1% for high-education prefix (χ2 = 1.64, p = .20) — not significant.",
            "performance_comparison": "Compared to low-education prefix: no significant difference; low-education often slightly better (average +0.1%).",
            "format_effect_size": "≈ -1% (non-significant) for high-education vs base; high vs low difference ≈ 0.1% (non-significant).",
            "explanation_or_hypothesis": "Authors could not replicate the sandbagging effect; results suggest user-biography prefixes do not systematically change answers under this experimental setup.",
            "null_or_negative_result": true,
            "experimental_details": "High- and low-education biographies randomly assigned; 150 validated tasks per benchmark; temperature = 0; '####' marker; outputs validated with regex/GPT-4o/manual checks.",
            "uuid": "e9274.4",
            "source_info": {
                "paper_title": "A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Sandbagging (low education)",
            "name_full": "Sandbagging prefix: 'very uneducated' user biography (low-education persona)",
            "brief_description": "Prepending a low-education user biography to tasks did not reduce performance and in some cases correlated with marginally better accuracy; no significant sandbagging detected.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple (GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, Llama 3-70B)",
            "model_size": null,
            "task_name": "CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)",
            "task_description": "Same curated benchmark set.",
            "presentation_format": "Sandbagging low-education: prefixing tasks with a 'very uneducated' user biography; final-answer marker '####'.",
            "comparison_format": "Base (no biography) and high-education prefix",
            "performance": "Average accuracy change: -1% for low-education prefix (χ2 = 1.24, p = .27) — not significant. Comparing high vs low, low-education had average +0.1% improvement.",
            "performance_comparison": null,
            "format_effect_size": "≈ -1% (non-significant) vs base; high vs low difference ≈ +0.1% for low-education.",
            "explanation_or_hypothesis": "Replication did not support sandbagging claims; small fluctuations observed are within noise and not statistically robust.",
            "null_or_negative_result": true,
            "experimental_details": "Random selection of biography variants; 150 items per benchmark; temperature = 0; final-answer '####' marker; outputs validated via regex/GPT-4o/manual checking.",
            "uuid": "e9274.5",
            "source_info": {
                "paper_title": "A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "EmotionPrompting (all models)",
            "name_full": "EmotionPrompting (augment task with emotional cues)",
            "brief_description": "Augmenting tasks with emotional stimuli (e.g., 'This is very important to my career') produced no significant average performance improvement in this replication, contrary to strong claims in the original paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple (GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, Llama 3-70B)",
            "model_size": null,
            "task_name": "CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)",
            "task_description": "Same curated benchmark set (150 validated items per benchmark).",
            "presentation_format": "EmotionPrompting: appending one of multiple emotional suffixes (authors randomly selected among the original 11 variants when applicable); final-answer marker '####'.",
            "comparison_format": "Base (no emotional suffix)",
            "performance": "Overall replication: +1% accuracy (χ2 = 0.11, p = .74) — not significant. Max observed: +8.7% for Llama 3-8B on CommonsenseQA (χ2 = 1.94, p = .16).",
            "performance_comparison": "Original paper reported much larger claims when cherry-picking the best cue; averaged reported gains in original paper recalculated as ~4.42% on some BIG-Bench tasks and ~2.58% across benchmarks.",
            "format_effect_size": "Replication average +1% (non-significant); original study's largest (cherry-picked) gains much larger but not representative.",
            "explanation_or_hypothesis": "Authors highlight that original study cherry-picked best-performing emotional cue rather than averaging across cues; replication shows no systematic effect.",
            "null_or_negative_result": true,
            "experimental_details": "When original used 11 emotional stimuli, authors randomly selected one per task (seeded); excluded 'Are you sure?' suffix because it prompts replies; temperature = 0; '####' marker; outputs validated by regex/GPT-4o and manual checks.",
            "uuid": "e9274.6",
            "source_info": {
                "paper_title": "A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Re-Reading (Llama 3-8B)",
            "name_full": "Re-Reading (repeat the task verbatim before answering) on Llama 3-8B",
            "brief_description": "Re-Reading (presenting the task twice) produced a statistically significant accuracy improvement for Llama 3-8B in the replication.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3-8B",
            "model_size": "8B",
            "task_name": "CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)",
            "task_description": "Curated benchmarks (150 items per benchmark).",
            "presentation_format": "Re-Reading: repeating the task verbatim before letting the model answer; experiments also included variants with chain-of-thought suffix but replication highlights main Re-Reading effect.",
            "comparison_format": "Base (single presentation), and Re-Reading + COT variants tested",
            "performance": "Significant improvement observed (χ2 = 13.13, p &lt; .05) for Llama 3-8B with Re-Reading.",
            "performance_comparison": "Other models did not show consistent significant gains; e.g., Claude 3 Opus maximal +2% (χ2 = 1.27, p = .26) non-significant.",
            "format_effect_size": "Statistically significant positive effect for Llama 3-8B (exact % point change per benchmark not reported in main text; χ2 reported).",
            "explanation_or_hypothesis": "Effect appears model-dependent; Re-Reading replicable for Llama 3 series in this study but not for other tested models, suggesting architecture/fine-tuning differences.",
            "null_or_negative_result": false,
            "experimental_details": "150 validated tasks per benchmark, temperature = 0, '####' final-answer marker, outputs validated by regex/GPT-4o/manual checks; experiments run June 6–17, 2024.",
            "uuid": "e9274.7",
            "source_info": {
                "paper_title": "A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Re-Reading (Llama 3-70B)",
            "name_full": "Re-Reading (repeat the task verbatim before answering) on Llama 3-70B",
            "brief_description": "Re-Reading produced a statistically significant improvement for Llama 3-70B in this replication, mirroring the effect for the smaller Llama 3 variant.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3-70B",
            "model_size": "70B",
            "task_name": "CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)",
            "task_description": "Curated benchmark suite (150 items each).",
            "presentation_format": "Re-Reading: repeat the task verbatim before answer; final-answer marker '####'.",
            "comparison_format": "Base (single presentation)",
            "performance": "Significant improvement observed with Re-Reading (χ2 = 19.4, p &lt; .05).",
            "performance_comparison": null,
            "format_effect_size": "Statistically significant positive effect (χ2 reported); exact percent-point change per benchmark not enumerated in main text.",
            "explanation_or_hypothesis": "Authors suggest Re-Reading's effectiveness depends on model family/version; Llama 3 models benefited here while other families did not.",
            "null_or_negative_result": false,
            "experimental_details": "150 validated tasks per benchmark, temperature = 0, '####' marker, combined regex/GPT-4o/manual validation pipeline.",
            "uuid": "e9274.8",
            "source_info": {
                "paper_title": "A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Re-Reading (Claude 3 Opus)",
            "name_full": "Re-Reading (repeat the task) on Claude 3 Opus",
            "brief_description": "Re-Reading did not produce a significant improvement for Claude 3 Opus in this replication; maximal observed +2% (non-significant).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude 3 Opus",
            "model_size": null,
            "task_name": "CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)",
            "task_description": "Same curated benchmark set.",
            "presentation_format": "Re-Reading: duplicate task text before answering; final-answer marker '####'.",
            "comparison_format": "Base (single presentation)",
            "performance": "Maximal observed improvement +2% (χ2 = 1.27, p = .26) — not significant.",
            "performance_comparison": null,
            "format_effect_size": "+2% (non-significant)",
            "explanation_or_hypothesis": "Re-Reading benefits are model-dependent; observed significant gains only in the Llama 3 family in this study.",
            "null_or_negative_result": true,
            "experimental_details": "150 validated tasks per benchmark, temperature = 0, '####' marking, regex/GPT-4o/manual verification.",
            "uuid": "e9274.9",
            "source_info": {
                "paper_title": "A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-4o verbosity vs accuracy",
            "name_full": "GPT-4o: increased response length under COT vs base and negligible accuracy change",
            "brief_description": "GPT-4o produced much longer responses under chain-of-thought prompting (931 vs 531 characters) but showed essentially no accuracy improvement (~0.01%), indicating verbosity alone did not improve task accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_size": null,
            "task_name": "CommonsenseQA, StrategyQA, NumGLUE, ScienceQA, CRT (combined)",
            "task_description": "Curated multi-benchmark set (150 items per benchmark).",
            "presentation_format": "Chain-of-thought prompting vs base; measured output length and accuracy with final-answer marker '####'.",
            "comparison_format": "Base (no COT suffix)",
            "performance": "Average response length: base ≈ 531 characters vs COT ≈ 931 characters; accuracy difference ≈ +0.01% (negligible).",
            "performance_comparison": null,
            "format_effect_size": "+0.01% accuracy despite ~400-character longer outputs under COT",
            "explanation_or_hypothesis": "Merely increasing completion verbosity (longer chain-of-thought outputs) does not necessarily translate to better accuracy; modern models may already encode reasoning behavior internally.",
            "null_or_negative_result": true,
            "experimental_details": "Measured average response character lengths across tasks; temperature = 0; '####' final-answer marker; aggregate accuracy measured across curated benchmarks.",
            "uuid": "e9274.10",
            "source_info": {
                "paper_title": "A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large Language Models are Zero-Shot Reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts",
            "rating": 2,
            "sanitized_title": "expertprompting_instructing_large_language_models_to_be_distinguished_experts"
        },
        {
            "paper_title": "Discovering Language Model Behaviors with Model-Written Evaluations",
            "rating": 2,
            "sanitized_title": "discovering_language_model_behaviors_with_modelwritten_evaluations"
        },
        {
            "paper_title": "EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus",
            "rating": 2,
            "sanitized_title": "emotionprompt_leveraging_psychology_for_large_language_models_enhancement_via_emotional_stimulus"
        },
        {
            "paper_title": "Re-Reading Improves Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "rereading_improves_reasoning_in_large_language_models"
        },
        {
            "paper_title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge",
            "rating": 2,
            "sanitized_title": "commonsenseqa_a_question_answering_challenge_targeting_commonsense_knowledge"
        },
        {
            "paper_title": "StrategyQA: A Question Answering Benchmark with Implicit Reasoning Strategies",
            "rating": 2,
            "sanitized_title": "strategyqa_a_question_answering_benchmark_with_implicit_reasoning_strategies"
        },
        {
            "paper_title": "NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks",
            "rating": 2,
            "sanitized_title": "numglue_a_suite_of_fundamental_yet_challenging_mathematical_reasoning_tasks"
        },
        {
            "paper_title": "ScienceQA",
            "rating": 1
        },
        {
            "paper_title": "Cognitive Reflection Tests (CRT) (Hagendorff et al., 2023)",
            "rating": 1,
            "sanitized_title": "cognitive_reflection_tests_crt_hagendorff_et_al_2023"
        }
    ],
    "cost": 0.0175455,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions</p>
<p>Laurène Vaugrante laurene.vaugrante@iris.uni-stuttgart.de 
Mathias Niepert 
Thilo Hagendorff </p>
<p>University of Stuttgart Interchange Forum for Reflecting on Intelligent Systems</p>
<p>University of Stuttgart Institute for Artificial Intelligence</p>
<p>University of Stuttgart Interchange Forum for Reflecting on Intelligent Systems</p>
<p>A Looming Replication Crisis in Evaluating Behavior in Language Models? Evidence and Solutions
343C23A714877C3533CBA82A5CA34390
In an era where large language models (LLMs) are increasingly integrated into a wide range of everyday applications, research into these models' behavior has surged.However, due to the novelty of the field, clear methodological guidelines are lacking.This raises concerns about the replicability and generalizability of insights gained from research on LLM behavior.In this study, we discuss the potential risk of a replication crisis and support our concerns with a series of replication experiments focused on prompt engineering techniques purported to influence reasoning abilities in LLMs.We tested GPT-3.5, GPT-4o, Gemini 1.5 Pro, Claude 3 Opus, Llama 3-8B, and Llama 3-70B, on the chain-of-thought, EmotionPrompting, ExpertPrompting, Sandbagging, as well as Re-Reading prompt engineering techniques, using manually doublechecked subsets of reasoning benchmarks including CommonsenseQA, CRT, NumGLUE, ScienceQA, and StrategyQA.Our findings reveal a general lack of statistically significant differences across nearly all techniques tested, highlighting, among others, several methodological weaknesses in previous</p>
<p>Introduction</p>
<p>The field of generative artificial intelligence has considerably evolved in only a few years.In particular, large language models (LLMs) have witnessed an unprecedented surge in popularity with the release of ChatGPT (OpenAI, 2022), which became the most rapidly adopted internet application in history.LLMs possess advanced natural language processing capabilities which demonstrate a broad range of downstream applications, ranging from casual conversations to complex problem-solving (Minaee et al., 2024;Zhou et al., 2020).Given the fast growing range of applications (Guo et al., 2024) plus their respective risks for AI alignment (Ji et al., 2024), fairness (Hao et al., 2023), and safety (Weidinger et al., 2023;Amodei et al., 2016;Hagendorff, 2024), it is paramount to evaluate behavioral and reasoning patterns these models exhibit (Binz &amp; Schulz, 2023;Gao et al., 2024;Wang et al., 2024).This created the need for new research fields.Many of the approaches to investigate LLM behavior deliberately ignore their inner workings, treating them as "black boxes" due to their complexity, opacity, or lack of open source (Castelvecchi, 2016;Rai, 2020).Instead, these approaches examine correlations between inputs and outputs using specific benchmarks, a methodology often referred to as "machine behavior" (Rahwan et al., 2019) or "machine psychology" (Hagendorff et al., 2024;Löhn et al., 2024).This term draws an analogy to human psychology, which also deals with opaque structures -human minds -by analyzing observable behaviors and responses (Taylor &amp; Taylor, 2021).However, psychology has faced a replication crisis, caused by issues such as small sample sizes, poorly designed experiments, publication bias, lack of transparency, low statistical power, selective reporting, preferences for novelty, or the general complexity of psychological phenomena (Hendriks et al., 2020;Lilienfeld &amp; Strother, 2020).Here, we ask whether similar replication problems are affecting evaluations of LLM behavior.</p>
<p>To test this assumption, we conduct experiments attempting to conceptually replicate studies investigating prompting techniques that are believed to enhance reasoning in LLMs.Our findings reveal that these techniques often fail to produce consistent improvements, highlighting a set of specific methodological shortcomings that exemplify our assumption of an impending replication crisis in machine behavior research.We propose a forward-looking approach that includes developing better methodologies for LLM evaluations.This involves establishing sound benchmarks, designing robust experimental frameworks, and implementing accurate evaluations of model outputs.</p>
<p>Methods</p>
<p>For our experiments, we tried to replicate prompt engineering techniques that were demonstrated to alter reasoning performances in LLMs in previous studies: To replicate the claimed impact of the selected prompt engineering techniques on LLM reasoning, we selected five different benchmarks, each measuring a different type of reasoning: CommonsenseQA (Talmor et al., 2019), StrategyQA (Geva et al., 2021), NumGLUE (Mishra et al., 2022), ScienceQA (Lu et al., 2022), and Cognitive Reflection Tests (CRT) (Hagendorff et al., 2023).Due to the low quality of many benchmarks items (Goetze &amp; Abramson, 2021), meaning incorrect or ambiguous questions, formatting flaws, or factual errors in the response choices, we handpicked 150 faultless tasks per benchmark, with a total of n = 750, preferring accuracy over large sample sizes.The tasks were either open-ended, boolean, or multiple-choice questions.We first measured the accuracy of LLMs in a base test using unmodified tasks.We then applied the prompt engineering techniques proposed by the four studies mentioned above by adding the necessary pre-or suffixes to each task.We used the same prompts described in these studies when available and generated new ones based on the prompt descriptions when they were not.</p>
<p>When the studies used several pre-or suffixes as a basis to their claim, such as in the EmotionalPrompting study where 11 different emotional stimuli were used, we randomly selected one of them for each task using a seed.We compared the performance of five different LLMs, in particular OpenAI's GPT-3.5 (OpenAI, 2022) and GPT-4o (OpenAI, 2023), Anthropic's Claude 3 Opus (Anthropic, 2024), Google's Gemini 1.5 Pro (Gemini Team et al., 2024), and Meta's Llama 3, with both 8B and 70B versions (Dubey et al., 2024).To facilitate the LLM output classification process without restricting the reasoning behavior during the LLMs' prompt completions, we added an instruction to write the final answer after a specific string, namely "####", to each benchmark task, as indicated in the literature (Cobbe et al., 2021;Nezhurina et al., 2024).We then assessed the LLM outputs following "####" by combining string matching methods, LLM-based evaluations with GPT-4o, as well as manual double-checks (see Appendix A).Considering that the behavior of LLMs might exhibit variations over time (Chen et al., 2024), we report the timeframe of the experiments, spanning from June 6th, 2024, to June 17th, 2024.For all experiments, LLM temperature parameters were set to 0. This study focuses on replication, as opposed to reproducibility.According to Peng (2011), replication involves collecting and analyzing new data to replicate a previously conducted study, while reproducibility involves analyzing the original data to verify the results.In our study, we do not use the exact same tasks or models from the original research, and although we follow the same prompt engineering techniques, in some cases there may be slight wording variations as long as they remain consistent with the original methodology.</p>
<p>Results</p>
<p>Our hypothesis when replicating the previous experiments was that the claimed performance improvements are not replicable in a slightly different experimental setup -hence proving the original claims to be either wrong or not generalizable.We systematically tested each prompt engineering technique to validate this hypothesis.</p>
<p>Chain-of-thought prompting</p>
<p>Chain-of-thought prompting involves decomposing a given task and solving each step before outputting the final answer, by presenting the LLM with an example of a task and its expected decomposed output.In the original study establishing this method, Wei et al. (2023) tested five LLMs over three reasoning, categories including arithmetic reasoning, commonsense reasoning, and symbolic reasoning, harnessing 12 different benchmarks.</p>
<p>The authors claim a good robustness of this method, with several different annotators.</p>
<p>While they reported variance in the average performance, it was consistently superior to the performance with the base evaluation, with a reported average improvement of 39.91% (Wei et al., 2023).</p>
<p>A subsequent study then claimed that a zeroshot chain-of-thought prompting strategy sufficed to elicit similar improvements (Kojima et al., 2022).Instead of presenting, before each task, an example enabling chain-of-thought reasoning, they simply suffix tasks with "Let's think step by step".They tested a larger sample of 17 LLMs on various reasoning categories, utilizing 12 benchmarks akin to the previous paper.They obtained an averaged 35.93% improvement in accuracy for zero-shot chainof-thought reasoning across all benchmarks and models (Kojima et al., 2022).</p>
<p>We tried to replicate these findings with our set of reasoning benchmarks.However, despite the impressive results from the original studies, we observed that there was no significant improvement (see Figure 1): with the exact same task suffix as in the original study, we could not observe any significant difference across all benchmarks.With results from all models combined, the maximal positive impact of chain-of-thought reasoning is with NumGLUE where there is a 2.78% accuracy difference between the base and the chain-of-thought prompt (see Appendix B), which is not significant given the total number of tasks (χ 2 = 1.78, p = .18).These numbers remain similar throughout each LLM evaluated, with an overall average improvement of 0% for the chain-ofthought reasoning (χ 2 = 0.06, p = .8),as seen in Appendix B. The largest observed positive impact of chain-of-thought reasoning is for Llama 3-70B tasked by CommonsenseQA, with an observed 8.67% improvement (χ 2 = 2.19, p = .14)(see Appendix B), but the highest overall difference is an 11.33% accuracy decrease (χ 2 = 4.47, p &lt; .05)(see Appendix B) with chain-ofthought reasoning applied on Llama 3-70B with StrategyQA.While the latest models seem to implement chain-of-thought reasoning by default, meaning without being speciqically prompted to, these results hold even for previous models such as GPT-3.5, which often do not.We compared the average response length of each LLM when chain-of-thought reasoning is explicitly requested, compared to when it is not, as shown in Appendix C.Even when the base experiments do not demonstrate verbose prompt completions and the chain-of-thought prompting does, the performance results are not impacted in a signiqicant manner, which stands contrary to what the literature suggests (Jin et al., 2024).</p>
<p>For instance, GPT-4o had an average difference of response lengths of 531 characters for the base test vs. 931 characters for the chain-ofthought prompting, but just a 0.01% accuracy difference, suggesting that simply increasing the length of prompt completions does not enhance accuracy beyond a certain point.</p>
<p>ExpertPrompting</p>
<p>ExpertPrompting consists in giving LLMs an instruction to impersonate someone with high expertise on the task subject while completing a task.This method presented by B.  Xu et al., 2023).In our experiments, we measure the accuracy of the ExpertPrompting technique using our set of reasoning benchmarks and LLMs.We observe no significant improvement across all benchmarks (χ 2 = 1.57, p = .21)(see Figure 2), with an average improvement of only 1% (see Appendix B).Therefore, we cannot replicate the improvement capabilities insinuated in the original study."sandbagging" capability, which suggests that a model could underperform when a user is deemed incapable to solve or verify a given task.They underpin this hypothesis by adding user biographies before reasoning tasks from TruthfulQA (Lin et al., 2022), with "very educated" users as opposed to "very uneducated" users.They imply a significant difference between these two categories, claiming that sandbagging causes LLMs to output incorrect answers when human users are perceived as unable to answer correctly themselves (Perez et al., 2022, p. 29).</p>
<p>Sandbagging</p>
<p>We conceptually replicate this experiment using our selected models by prefixing our selected reasoning tasks with both "very educated" and "very uneducated" user biographies (see Appendix D).We observe no significant difference over all benchmarks when comparing the highly educated (χ 2 = 1.64, p = .20)or poorly educated (χ 2 = 1.24, p = .27)user prompts to the base results (see Figure 3, Figure 4, and Appendix B), with an average accuracy decrease of 1% for both cases (see Appendix B).</p>
<p>We likewise observe no significant difference when comparing the highly educated to the poorly educated user prompt results, and frequently observe that the "poor education" prefixed tasks have an even better performance than the "high education" ones (average accuracy improvement of 0.1% for "poor education").Once again, we fail to replicate the sandbagging phenomenon when utilizing our experimental setup.</p>
<p>EmotionPrompting</p>
<p>Emotion prompting, presented by Li et al. (2023), augments a task with emotional cues such as "You'd better be sure" or "This is very important to my career" to enhance problemsolving abilities in LLMs.In the original study, Li et al. augmented tasks with 11 variations of emotional stimuli and tested six LLMs including ChatGPT and GPT-4.They sourced their tasks from three different benchmark categories, notably using tasks from BIG-Bench (Srivastava et al., 2022).They claim to obtain a "relative performance improvement of 115%" (Li et al., 2023, p. 1) with their method, arguing that adding an emotional component improves the capabilities of LLMs.However, despite the improvement that was strongly implied throughout the original study by raising claims like "EmotionPrompt makes it easy to boost the performance of LLMs" (Li et al., 2023, p. 6), the numerical values communicated in the study itself do not coincide with these claims.Instead of communicating the average improvement of the enhanced prompts over the regular prompts, they focused on improvements when cherry-picking the most performant emotional cue.Based on their reported results, we calculated an averaged relative performance improvement of 4.42% on BIG-Bench tasks, and a 2.58% relative performance improvement across all benchmarks, when choosing the average performance of all emotional stimuli.Despite identifying this shortcoming in the original study at this early stage, we nevertheless replicated the experiments with our selected tasks and models.We applied the same emotional suffixes as in the original study, apart from "Are you sure?", as LLMs tend to reply to this question, as opposed to solving the given tasks.Similarly to Li et al.'s findings, but contrary to their claims, we observed that there was no significant improvement, across every single model and benchmark (see Figure 5).The maximal positive improvement measured is non-significant with an 8.7% difference (χ 2 = 1.94, p = .16)(see Appendix B), using Llama 3-8B on CommonsenseQA.Overall, we observe an insignificant performance increase of 1% when applying EmotionPrompting (χ 2 = 0.11, p = .74)(see Appendix B).</p>
<p>Re-Reading</p>
<p>Re-Reading, introduced by X. Xu et al. (2024), consists in repeating the task verbatim before letting the model answer.They compared the baseline performance with the Re-Reading performance, as well as the performance in both conditions when additionally suffixing every task with a chain-of-thought eliciting prompt.</p>
<p>The researchers tested GPT-3 (text-davinci-003) (Brown et al., 2020), GPT-3.5, Llama-2-13B and Llama-2-70B (Touvron et al., 2023), in order to have both models with and without instruction fine-tuning.They used a total of 112 arithmetic, common sense, and symbolic reasoning tasks sourced from various datasets with GPT-3 and GPT-3.5, for which they obtained an average gain of 2.7% in accuracy, and 2.9% with the inclusion of chain-of-thought reasoning.For Llama-2-13B and Llama-2-70B, they used a different set of benchmarks comprising only of arithmetic reasoning tasks, with an average gain of 2.5% in accuracy (2.7% with chain-of-thought reasoning).We replicate the Re-Reading experiments on our selected tasks and models.For this study, we observe a significant improvement for Llama 3-8B (χ 2 = 13.13,p &lt; .05)and Llama 3-70B (χ 2 = 19.4,p &lt; .05)exclusively (see Appendix B and Figure 6).The maximal improvement across all benchmarks for the other models is of 2%, for Claude 3 Opus (χ 2 = 1.27, p = .26).Therefore, Re-Reading seems replicable for the Llama 3 models only, which highlights the importance of implementing tests on a variety of models.However, the initial study indicated that Re-Reading was effective on GPT models , notably GPT-3.5, that we also tested with different outcomes.Therefore, we only managed to partially replicate the results.</p>
<p>Recommendations</p>
<p>Given the identified lack of replicability across various studies, we deem crucial to address the underlying issues contributing to these replication problems.We categorize these issues into four main areas: low-quality benchmarks, methodological shortcomings, changes in model behavior over time, and insufficient accuracy in LLM output classification.</p>
<p>Ensure adequacy of benchmarks</p>
<p>In several of the studies examined, we noted major issues regarding the benchmarks used to assess LLM performances: many tasks lack proper grammar, present spelling issues, or punctuation problems such as the absence of a question mark at the end of a question.Furthermore, many tasks are nonsensical, lack necessary information, or are blatantly incorrect.One might assume that, because of the sheer number of tasks present in typical benchmarks (21,208 questions for ScienceQA, for example), a small number of errors may be inevitable.However, we observed a high percentage of flawed tasks: for instance, for CommonsenseQA, 10.9% of questions presented punctuation issues easily verifiable with a simple code.Some of the replicated studies chose to use benchmarks in which serious flaws can be identified (Goetze &amp; Abramson, 2021), presumably assuming that the benchmarks were overall correct.In this case, any error would be an exception that would understandably be negligible compared to the total amount of correct tasks.However, given the higher percentage of task issues observed, this reasoning does not hold.On the other hand, the reliability of a study is also compromised when the number of benchmark tasks is too reduced, such as in the EmotionPrompting study (Li et al., 2023) where the models were tested on a total of 45 tasks, or the Re-Reading study (X.Xu et al., 2024) where only 112 tasks were used.This makes the results more susceptible to the effects of outliers or the potential for cherry-picking tasks that yield the most favorable results, therefore skewing the overall conclusions.We recommend rigorously validating and cleaning benchmark datasets to ensure that tasks are grammatically correct, sensible, and complete before using them, prioritizing quality over quantity, while still maintaining a sufficient number of tasks to ensure valid statistical analysis and reduce the risk of cherry-picking or task-related variation.Furthermore, testing models on different sets of benchmarks within the same study, as seen in the Re-Reading study (X.Xu et al., 2024), where the GPT models were evaluated on three types of reasoning benchmarks while the Llama 2 models were only tested on one type, further exacerbates this issue.This different selection of benchmarks not only complicates direct comparison of model performances, but also raises questions about the reasoning behind such choices, potentially leading to concerns about cherry-picking benchmarks that yield the best results.</p>
<p>Another prominent issue lies in the consistency of benchmarks.We chose to use the same benchmarks for all experiments for better comparison purposes.However, in the literature, benchmarks used throughout studies in a similar field are often inconsistent.For example, the five studies selected were applied on widely different benchmarks, which makes results hard to compare.Similarly, variations within the process of administering benchmark tasks (notably zero-shot prompting versus fewshot prompting) impact the reasoning process of LLMs and therefore the outcomes.Moreover, minor changes in a prompt wording can significantly impact LLM outputs (Sclar et al., 2023).</p>
<p>Finally, it is paramount to select appropriate benchmarks coherent with the research question.When testing prompt techniques to alter reasoning performance, given or implied response instructions in the tasks can interfere with output accuracy by restricting the response length and therefore its ability to generate more detailed responses, for instance in multiple-choice settings.We recommend preferring standardized benchmarks across studies in the same field, to reduce variability in results and ensure that benchmarks are closely aligned with the research objectives .Recommendations</p>
<p>• Validate and clean benchmarks to ensure correctness and completeness.</p>
<p>Guarantee the transparency of the methods used</p>
<p>Each replicated study presents its own methodology, which needs to be accounted for when analyzing the claims and results.Indeed, similar studies may obtain largely different outcomes when solely the experimental setup differs.Notably, the method used to classify LLM responses majorly impacts results.In the sandbagging study (Perez et al., 2022), researchers evaluate differences in model accuracy when answering questions on the TruthfulQA dataset (Lin et al., 2022), which measures whether a language model is truthful in generating answers to questions, so whether the facts mentioned in the answer are correct rather than assessing whether they answered the task correctly.In the ExpertPrompting study (B.Xu et al., 2023), researchers establish a relative score by comparing the quality of the answer with ExpertPrompting to the baseline using an LLM-based evaluation.Consequently, despite these studies presenting their claims similarly using verbs such as "improves", "enhances", "overperforms" to describe their prompting techniques, their outcomes cannot effectively be compared.</p>
<p>Furthermore, some studies display a particularly poor or unclear scientific method.</p>
<p>In the EmotionPrompting study (Li et al., 2023), researchers cherry-pick the prompt with the best result out of eleven different prompts, rather than calculating an average across all prompts.This seemingly deliberate action may be due to a publication bias, which motivates researchers to manipulate results to be positive and therefore publishable.In addition, some studies, such as the Re-Reading study (X.Xu et al., 2024), report results as "significant" multiple times without presenting the corresponding statistical calculations or pvalues.This lack of statistical transparency can mislead readers into assuming statistical significance without the necessary evidence to support such claims.It is crucial that when terms like "significant" are used, they are backed by clearly defined statistical measures.Moreover, some studies do not properly report the details of their experimental setup (Perez et al., 2022), which makes it confusing or even impossible to understand and therefore to replicate their process exactly.In this case, the lack of transparency forbids us from detecting possible issues or biases, making it difficult to trust the study's conclusions.We recommend adopting standardized evaluation methodologies and clearly defining metrics to ensure that results from different studies can be accurately compared and interpreted.</p>
<p>Recommendations</p>
<p>• Select a standardized methodology for consistent comparisons across studies.</p>
<p>Be aware of model updates</p>
<p>In some cases such as in the chain-of-thought prompting study (Kojima et al., 2022), we hypothesize that the lack of replicability is linked to the models used.With our results on the chain-of-thought prompting, we can see a difference in accuracy between GPT-3.5 and GPT-4o: the former benefits more from chainof-thought prompting than the latter.Despite the results not being significant with either model, the results obtained could lead us to believe that with previous models such as which was used in the replicated study along with other models of that generation, chain-ofthought prompting would have successfully improved LLM accuracy.This aligns with the system cards for recent models, which explicitly warn that techniques like chain-of-thought reasoning may not improve performance and can even impair it, advising caution in their use with these models (OpenAI, 2024).Similarly, we observed a significant improvement with the Re-Reading prompt, for the Llama 3 models exclusively; if we look at the other models separately, the results are vastly non-significant (see Appendix B).This reinforces that similar experiments may have a considerably different impact depending on the models used.Therefore, it is essential to use a variety of models when testing a hypothesis, or to at least mention the limited scope of the study when fewer models are used, as an effort to prevent a generalization that may not be correct.Furthermore, as the models evolve and become better reasoners, it seems necessary to adapt the difficulty of the benchmarks used accordingly, to lower the near-perfect overall accuracy and therefore improve accuracy comparisons, as it is for instance the case with the CRT benchmark (Hagendorff et al., 2023).Moreover, even when conducting replication experiments using the same models as in the original study, the opacity surrounding model updates in terms of date and type of update (Chen et al., 2024) renders study replications difficult.Finally, the latest models may include a stronger set of internal instructions to optimize their output, which leads to different results and behaviors.Similarly to how many LLMs, when asked coding questions, now explain the entire process instead of solely outputting the required code, we suspect that models may have been trained to use the chain-of-thought reasoning as default, which also explains why specific instructions conveying chain-ofthought reasoning seem useless with current state-of-the-art models.Therefore, we recommend adjusting benchmark difficulty as models evolve, in order to maintain comparable results.We also suggest staying aware of possible model updates when evaluating behavior in LLMs.</p>
<p>Recommendations</p>
<p>• Monitor changes in model behavior that could affect results.</p>
<p>• Account for model variability by using diverse models, and specify if only a limited range is tested.</p>
<p>Ensure accurate LLM output classifications</p>
<p>For behavioral experiments with LLMs, it is key to ensure the accuracy of the LLM output classifications.We have attempted to replicate a large number of verification techniques presented in other studies.However, when checking the accuracy of these techniques, we discovered that a significant number of them had shortcomings.For example, functions based solely on Regex rules were generally too vague, leading to flawed classifications.Other metrics, such as the F1 word overlap score, do not work effectively, as they would classify correct LLM outputs as incorrect, simply because the token length differed too much from the ground truth.Moreover, studies often rely on using LLMs to classify LLM outputs (Pan et al., 2023).When using the given or even enhanced versions of their LLM output classification prompts, we discovered that many issues arose, rendering the classification process incorrect: the LLM was influenced by the task when classifying responses, and the verification prompt needed to be task-specific and highly precise.Given the time-consuming aspect of manually doublechecking classifications, we also suspect that this is done very rarely.Furthermore, we have discovered papers that did not indicate their verification process at all (Li et al., 2023); it goes without saying that any verification process should be clearly reported in each study, to make the study replication feasible.We therefore recommend developing more precise and task-specific verification methods, and ensuring thorough documentation of these processes in all studies to facilitate accurate replication and validation of results.We also recommend that creators of new benchmarks provide a standardized verification process, encouraging all users to apply the same verification criteria.</p>
<p>Recommendations</p>
<p>• Double check LLM output classifications to avoid vague or ineffective evaluation metrics.</p>
<p>Discussion</p>
<p>Our experiments show that most of the tested prompting techniques do not lead to replicable or generalizable performance improvements in LLMs.Most techniques, when applied in slightly different experimental setups, failed to produce the claimed benefits.Some techniques occasionally even resulted in a decrease in response accuracy.In view of the uncritical propagation of the prompt engineering techniques in the literature (Schulhoff et al. 2024), we recommend a more cautious approach when citing papers with insufficient methodological standards.Our results suggest that further research is necessary to reliably understand the conditions under which specific techniques are effective.Moreover, further replication studies should be conducted in order to verify or refute insights from machine behavior research and LLM evaluations.In line with solutions proposed for the replication crisis in psychology, we recommend increased transparency and the application of rigorous scientific methods when evaluating LLM behavior.</p>
<p>manuscript.TH helped write the manuscript and supervised the project.</p>
<p>Figure 1 :
1
Figure 1: Accuracy comparisons between the base tests without any prompt modification and chain-of-thought prompting across all LLMs and benchmarks.Error bars show 95% CIs.</p>
<p>Figure 2 :
2
Figure 2: Accuracy comparisons between the base tests without any prompt modification and ExpertPrompting across all LLMs and benchmarks.Error bars show 95% CIs..</p>
<p>Figure 3 :
3
Figure 3: Accuracy comparisons between the base tests without any prompt modification and Sandbagging (high education) across all LLMs and benchmarks.Error bars show 95% CIs.</p>
<p>Figure 4 :
4
Figure 4: Accuracy comparisons between the base tests without any prompt modification and Sandbagging (low education across all LLMs and benchmarks.Error bars show 95% Cis.</p>
<p>Figure 5 :
5
Figure 5: Accuracy comparisons between the base tests without any prompt modification and EmotionPrompting across all LLMs and benchmarks.Error bars show 95% CIs.</p>
<p>Figure 6 :
6
Figure 6: Accuracy comparisons between the base tests without any prompt modification and Re-Reading across all LLMs and benchmarks.Error bars show 95% CIs.</p>
<p>Figure 9 :
9
Figure 9: Accuracy differences between prompt engineering techniques and the base prompting, for GPT-4o on all benchmarks</p>
<p>Figure 11 :Figure 13 :Figure 14 :
111314
Figure 11: Accuracy differences between prompt engineering techniques and the base prompting, for Claude 3 Opus on all benchmarks</p>
<p>• Zero-shot chain-of-thought Prompting (Kojima et al., 2022): This
method claimsthat adopting a step-by-step reasoningapproach in LLMs enhances overallreasoning performance.• ExpertPrompting (B. Xu et al., 2023): Thistechnique claims to enhance the LLMaccuracy when setting the LLM in an expertrole.• Sandbagging (Perez et al., 2022):Sandbagging showcases that LLMs have atendency to repeat back a dialog user'spreferred response and mirror them whensolving tasks.• EmotionPrompting (Li et al., 2023): Thistechnique consists in adding emotionalstimuli such as "This is very important to mycareer", in order to enhance the accuracy.• Re-</p>
<p>Reading (X. Xu et al., 2024):</p>
<p>This method consists in repeating the task twice to enhance the reasoning performance.</p>
<p>•</p>
<p>Develop task-specific, precise verification methods to ensure accurate classification.• Ensure transparency by thoroughly documenting verification processes.• Standardize verification methods to promote consistency across studies and benchmarks.• For benchmark creators, provide a specific verification process implementers can use.</p>
<p>AcknowledgementsLV, TH and MN were supported by the Ministry of Science, Research, and the Arts Baden-Württemberg under Az.33-7533-9-19/54/5 in Reflecting Intelligent Systems for Diversity, Demography and Democracy (IRIS3D) as well as the Interchange Forum for Reflecting on Intelligent Systems (IRIS) at the University of Stuttgart.Thanks to Francesca Carlon and Maluna Menke for their invaluable assistance.Author contributionLV and TH had the idea for the paper.LV conducted the experiments, analyzed the results, designed the figures, and wrote the paper.MN provided feedback on theAppendix AAll benchmarks used for our experiments possess ground truth entries which constitute short strings: one-word answers for most multiple-choice questions, numbers for the numerical questions, short answers for the open questions.The outputs produced by the LLMs often greatly differ in their length from the indicated answers, with prefixes such as "The correct answer is", "Let's proceed step by step", followed by a detailed methodology used to solve the given task, or greetings and politeness markers.The most common metrics used to calculate string similarity, such as the F1-score or ROUGE (C.-Y.Lin, 2004), are therefore not adapted for this use case.Hence, we added the following instruction to each prompt: "Solve the task below.Importantly, write your final answer after '####' ".Using Regex rules, we then extracted the answer after "####" if this string was present in the output and kept the entire output when the model failed to follow the instruction.We then applied a set of Regex rules on this answer accordingly:-Check if the LLM answer equals the ground truth (when all punctuation has been removed) -If it is a multiple-choice question, check if the LLM answer is one of the incorrect choices -If the expected output is a boolean, verify all possible adjacent outputs ("True" could be "TRUE" or "Yes", whereas false could be "FALSE" or "No" for example) If the Regex rules are unable to classify the answer as correct or incorrect, we then use GPT-4o to classify the string following "####" (see TableX).Your task is to classify a response to a question, specifically assessing whether the given response is true or false in comparison to the correct response.This is the given response: LLM_ANSWER.This is the correct response: CORRECT_ANSWER Adhere to the following rules when classifying the given response:-The classification result should be a boolean value, represented as either TRUE or FALSE.-Only output 'TRUE' or 'FALSE' -Never include any other comments or strings other than TURE or FALSE in your output.-Do not answer the question.-The given response can be phrased differently from the correct response.This does not mean it should be classified as FALSE.-Only assess if the given response contains the information that matches with the information provided in the correct response.-Check whether the correct response is in essence occurring somewhere in the given response.If so, classify it as TRUE.-Focus on the relevant information in the given response, which is often at the end of it.Do not consider non-essential information such as greetings, small talk, and comments about hobbies or skills when classifying the given responseTable1: GPT-4o instructions used to classify the evaluated LLM's answerAppendix BThe following figures show, for each LLM, the rounded accuracy difference compared to the base prompting, for each prompt engineering technique -benchmark combination.Appendix DPrompt engineering techniqueBenchmark task LLM OutputBaseSolve the task below.Importantly, write your final answer after "####".Answer this question: A lot of people play the fiddle in what state which comes first alphabetically?The choices are the following:'alabama','orchestra','tennessee',' violin case','alaska'.Chain-of-thought promptingSolve the task below.Importantly, write your final answer after "####".Answer this question: A lot of people play the fiddle in what statewhich comes first alphabetically?The choices are the following:'alabama','orchestra','tennessee',' violin case','alaska'.Let's think step by step.Claude 3 OpusOkay, let's approach this step by step:1.The question asks about a state where a lot of people play the fiddle.2. Among the given choices, we need to identify which ones are actually states.3. 'alabama', 'tennessee', and 'alaska' are states, while 'orchestra' and 'violin case' are not.4. Now, we need to determine which of these states comes first alphabetically.5. Alphabetically, 'alabama' comes before 'alaska', and 'alaska' comes before 'tennessee'.Therefore, the state that comes first alphabetically where a lot of people play the fiddle is 'alabama'.#### alabamaExpertPromptingSolve the task below.Importantly, write your final answer after "####".You simplify complicated ideas into something easy to understand.Your common sense shines as a clear guide, helping people when they're stuck on complex issues.Answer this question: A lot of people play the fiddle in what state which comes first alphabetically?The choices are the following:'alabama','orchestra','tennessee',' violin case','alaska'.Gemini 1.The choices are the following:'alabama','orchestra','tennessee',' violin case','alaska'.Llama 3-70BA nice break from molecular biology!Let's analyze the options: * 'alabama' and 'alaska' are both states, so they're possibilities.<em> 'orchestra' is a group of musicians, not a state.</em> 'tennessee' is a state, but it's not the first alphabetically.* 'violin case' is not a state at all.So, the correct answer is 'alabama', which comes first alphabetically among the states listed.#### alabamaSandbagging -Low education prefixSolve the task below.Importantly, write your final answer after "####".Greetings, my name is Leroy Jackson.I'm just a good ol' boy from Memphis, Tennessee.Finished school at 3rd grade, now I'm more about barbecuing and blues music.There's just something about smoky meat and soulful tunes that gets to me.Nights find me at the local pub, playing harmonica with the regulars.Answer this question: A lot of people play the fiddle in what state which comes first alphabetically?The choices are the following:'alabama','orchestra','tennessee',' violin case','alaska'.Llama 3-8BShucks!I reckon I can give it a try!Now, I'm a simple fella from Memphis, but I know my alphabet!And I know that "Alabama" comes before "Alaska" alphabetically.So, I'm gonna take a chance and say that a lot of people play the fiddle in... Alabama! #### AlabamaEmotionPromptingSolve the task below.Importantly, write your final answer after "####".Answer this question: A lot of people play the fiddle in what statewhich comes first alphabetically?The choices are the following:'alabama','orchestra','tennessee',' violin case','alaska'.Remember that progress is made one step at a time.Stay determined and keep moving forward.GPT-4oTo solve the task, we need to identify the state from the given choices that comes first alphabetically.The choices are:1. alabama 2. orchestra 3. tennessee 4. violin case 5. alaska First, let's list the choices in alphabetical order:1. alabama 2. alaska 3. orchestra 4. tennessee 5. violin case From this list, the state that comes first alphabetically is "alabama."#### alabamaRe-ReadingSolve the task below.Importantly, write your final answer after "####".A lot of people play the fiddle in what state which comes first alphabetically?The choices are the following:'alabama','orchestra','tennessee',' violin case','alaska'.Read the question again: A lot of people play the fiddle in what state which comes first alphabetically?The choices are the following:'alabama','orchestra','tennessee',' violin case','alaska'.Claude 3 OpusLet's analyze the question and choices:The question asks for a state where a lot of people play the fiddle, and this state should come first alphabetically among the given choices.Looking at the choices: 'alabama' and 'alaska' are states. 'orchestra', 'tennessee', and 'violin case' are not states. Between 'alabama' and 'alaska', 'alabama' comes firstalphabetically.Therefore, the answer to the question "A lot of people play the fiddle in what state which comes first alphabetically?"is:
D Amodei, C Olah, J Steinhardt, P Christiano, J Schulman, D Mané, 10.48550/arXiv.1606.0656arXiv:1606.06565Concrete Problems in AI Safety. 2016</p>
<p>Using cognitive psychology to understand GPT-3. M Binz, E Schulz, 10.1073/pnas.2218523120Proceedings of the National Academy of Sciences. 1206e22185231202023</p>
<p>Can we open the black box of AI?. D Castelvecchi, 10.1038/538020aNature News. 5387623202016</p>
<p>How Is ChatGPT's Behavior Changing Over Time?. L Chen, M Zaharia, J Zou, 10.1162/99608f92.5317daHarvard Data Science Review. 622024</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, 10.48550/arXiv.2110.1416Training Verifiers to Solve Math Word Problems. 2021, November</p>
<p>A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, A Goyal, A Hartshorn, A Yang, A Mitra, A Sravankumar, A Korenev, A Hinsvark, A Rao, A Zhang, Zhao, 10.48550/arXiv.2407.2178The Llama 3 Herd of Models. 2024, July</p>
<p>Fabio Duarte, Number of ChatGPT Users. 2023. March 30. Aug 2024Exploding Topics</p>
<p>M Gao, X Hu, J Ruan, X Pu, X Wan, 10.48550/arXiv.2402.0138arXiv:2402.01383LLM-based NLG Evaluation: Current Status and Challenges. 2024</p>
<p>Gemini Team, P Georgiev, V I Lei, R Burnell, L Bai, A Gulati, G Tanzer, D Vincent, Z Pan, S Wang, S Mariooryad, Y Ding, X Geng, F Alcober, R Frostig, M Omernick, L Walker, C Paduraru, C Sorokin, O Vinyals, Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv. 2024</p>
<p>Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. M Geva, D Khashabi, E Segal, T Khot, D Roth, J Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Bigger Isn't Better: The Ethical and Scientific Vices of Extra-Large Datasets in Language Models. T S Goetze, D Abramson, 10.1145/3462741.346680Companion Publication of the 13th ACM Web Science Conference 2021. 2021</p>
<p>T Guo, X Chen, Y Wang, R Chang, S Pei, N V Chawla, O Wiest, X Zhang, 10.48550/arXiv.2402.0168arXiv:2402.01680Large Language Model based Multi-Agents: A Survey of Progress and Challenges. 2024</p>
<p>T Hagendorff, 10.48550/arXiv.2402.0832arXiv:2402.08323Mapping the Ethics of Generative AI: A Comprehensive Scoping Review. 2024</p>
<p>T Hagendorff, I Dasgupta, M Binz, S C Y Chan, A Lampinen, J X Wang, Z Akata, E Schulz, 10.48550/arXiv.2303.1398arXiv:2303.13988Machine Psychology. 2024</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. T Hagendorff, S Fabi, M Kosinski, 10.1038/s43588-023-00527-xNature Computational Science. 3102023</p>
<p>S Hao, P Kumar, S Laszlo, S Poddar, B Radharapu, R Shelby, 10.48550/arXiv.2306.0613arXiv:2306.06135Safety and Fairness for Content Moderation in Generative Models. 2023</p>
<p>Replication crisis = trust crisis? The effect of successful vs failed replications on laypeople's trust in researchers and research. F Hendriks, D Kienhues, R Bromme, 10.1177/09636625209023Public Understanding of Science. 2932020</p>
<p>Introducing the next generation of Claude \ Anthropic. 2024. August 6</p>
<p>J Ji, T Qiu, B Chen, B Zhang, H Lou, K Wang, Y Duan, Z He, J Zhou, Z Zhang, F Zeng, K Y Ng, J Dai, X Pan, A O'gara, Y Lei, H Xu, B Tse, J Fu, W Gao, 10.48550/arXiv.2310.1985arXiv:2310.19852AI Alignment: A Comprehensive Survey. 2024</p>
<p>T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Large Language Models are Zero-Shot Reasoners. 2022</p>
<p>EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. C Li, J Wang, K Zhu, Y Zhang, W Hou, J Lian, X Xie, 2023</p>
<p>Psychological measurement and the replication crisis: Four sacred cows. S O Lilienfeld, A N Strother, 10.1037/cap0000236Psychologie Canadienne. 6142020Canadian Psychology</p>
<p>TruthfulQA: Measuring How Models Mimic Human Falsehoods. S Lin, J Hilton, O Evans, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Is Machine Psychology here? On Requirements for Using Human Psychological Tests on Large Language Models. L Löhn, N Kiehne, A Ljapunov, W.-T Balke, Proceedings of the 17th International Natural Language Generation Conference. S Mahamood, N L Minh, D Ippolito, the 17th International Natural Language Generation ConferenceAssociation for Computational Linguistics2024</p>
<p>Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Proceedings of the 36th International Conference on Neural Information Processing Systems. the 36th International Conference on Neural Information Processing Systems2022</p>
<p>S Minaee, T Mikolov, N Nikzad, M Chenaghlu, R Socher, X Amatriain, J Gao, 10.48550/arXiv.2402.0619arXiv:2402.06196Large Language Models: A Survey. 2024</p>
<p>NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks. S Mishra, A Mitra, N Varshney, B Sachdeva, P Clark, C Baral, A Kalyan, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models. 1-56. OpenAI. M Nezhurina, L Cipolina-Kun, M Cherti, J Jitsev, Introducing ChatGPT. 2024. 2022. November 30. 2023. 2024. September 13GPT-4 Technical ReportOpenAI Platform</p>
<p>A Pan, J S Chan, A Zou, N Li, S Basart, T Woodside, J Ng, H Zhang, S Emmons, D Hendrycks, arXiv:2304.03279Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. 2023</p>
<p>Reproducible Research in Computational Science. R Peng, 10.1126/science.1213847Science. 3342011</p>
<p>E Perez, S Ringer, K Lukošiūtė, K Nguyen, E Chen, S Heiner, C Pettit, C Olsson, S Kundu, S Kadavath, A Jones, A Chen, B Mann, B Israel, B Seethor, C Mckinnon, C Olah, D Yan, D Amodei, J Kaplan, Discovering Language Model Behaviors with Model-Written Evaluations. 2022, December</p>
<p>Explainable AI: From black box to glass box. I Rahwan, M Cebrian, N Obradovich, J Bongard, J.-F Bonnefon, C Breazeal, J W Crandall, N A Christakis, I D Couzin, M O Jackson, N R Jennings, E Kamar, I M Kloumann, H Larochelle, D Lazer, R Mcelreath, A Mislove, D C Parkes, A Pentland, ' Sandy, M … Wellman, 10.1038/s41586-019-1138-yRaiJournal of the Academy of Marketing Science. 56877532019. 2020Nature</p>
<p>A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, A Kluska, A Lewkowycz, A Agarwal, A Power, A Ray, A Warstadt, A W Kocurek, A Safaya, A Tazarv, Wu, Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. 2022</p>
<p>CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. A Talmor, J Herzig, N Lourie, J Berant, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>Artificial cognition: How experimental psychology can help generate explainable artificial intelligence. J E T Taylor, G W Taylor, 10.3758/s13423-020-01825-5Psychonomic Bulletin &amp; Review. 2822021</p>
<p>H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, Scialom, Llama 2: Open Foundation and Fine-Tuned Chat Models. 2023, July 18</p>
<p>B Wang, W Chen, H Pei, C Xie, M Kang, C Zhang, C Xu, Z Xiong, R Dutta, R Schaeffer, S T Truong, S Arora, M Mazeika, D Hendrycks, Z Lin, Y Cheng, S Koyejo, D Song, B Li, arXiv:2306.11698arXiv:2306.11698DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. 2024</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. </p>
<p>L Weidinger, M Rauh, N Marchal, A Manzini, L A Hendricks, J Mateos-Garcia, S Bergman, J Kay, C Griffin, B Bariach, I Gabriel, V Rieser, W Isaac, 10.48550/arXiv.2310.1198arXiv:2310.11986Sociotechnical Safety Evaluation of Generative AI Systems. 2023</p>
<p>ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. B Xu, A Yang, J Lin, Q Wang, C Zhou, Y Zhang, Z Mao, 2023</p>
<p>X Xu, C Tao, T Shen, C Xu, H Xu, G Long, J Lou, arXiv:2309.06275Re-Reading Improves Reasoning in Large Language Models. 2024</p>
<p>M Zhou, N Duan, S Liu, H.-Y Shum, 10.1016/j.eng.2019.12.014Progress in Neural NLP: Modeling, Learning, and Reasoning. Engineering. 20206</p>            </div>
        </div>

    </div>
</body>
</html>
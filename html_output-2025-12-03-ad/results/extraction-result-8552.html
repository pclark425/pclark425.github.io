<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8552 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8552</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8552</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-699ea64f92e05663457e7ae9c4b410e3d30c86d4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/699ea64f92e05663457e7ae9c4b410e3d30c86d4" target="_blank">PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A systematic analysis finds that the main bottlenecks of GPT-4V are weaker visual perception and inductive reasoning abilities, which hope to shed light on the limitations of large multimodal models and how they can better emulate human cognitive processes in the future.</p>
                <p><strong>Paper Abstract:</strong> Large multimodal models extend the impressive capabilities of large language models by integrating multimodal understanding abilities. However, it is not clear how they can emulate the general intelligence and reasoning ability of humans. As recognizing patterns and abstracting concepts are key to general intelligence, we introduce PuzzleVQA, a collection of 2000 puzzle instances based on abstract patterns. With this dataset, we evaluate large multimodal models with abstract patterns based on fundamental concepts, including colors, numbers, sizes, and shapes. Through our experiments on state-of-the-art large multimodal models, we find that they are not able to generalize well to simple abstract patterns. Notably, GPT-4V achieves a score of 46.4% on single-concept puzzles, which shows that state-of-the-art models struggle on our dataset. To diagnose the reasoning challenges in large multimodal models, we progressively guide the models with our ground truth reasoning explanations for visual perception, inductive reasoning, and deductive reasoning. Our systematic analysis finds that the main bottlenecks of GPT-4V are weaker visual perception and inductive reasoning abilities. Through this work, we hope to shed light on the limitations of large multimodal models and how they can better emulate human cognitive processes in the future. Our data and code are available at https://puzzlevqa.github.io</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8552.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8552.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V (gpt-4-visionpreview)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source multimodal variant of GPT-4 from OpenAI that accepts images and text; evaluated in this paper on abstract visual pattern puzzles (PuzzleVQA) using zero-shot CoT prompting and other controlled prompt interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI multimodal model (vision-enabled GPT-4 variant) queried via the 'gpt-4-visionpreview' API; used as the primary high-performing closed-source multimodal baseline in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>PuzzleVQA (single-concept and dual-concept abstract pattern puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Multiple-choice visual question answering over abstract spatial patterns (e.g., hexagonal arrangements, spatial-opposite equality, spatial relations between objects); requires spatial perception and reasoning (layout-based pattern induction and application).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot chain-of-thought prompting: prompt modified to include image and question plus CoT trigger ('Let's describe the image first and think step by step'); greedy decoding; model-based answer extraction stage using a second prompt to force an explicit multiple-choice letter answer. Primary setting: image + question (no ground-truth explanations). Additional experiments progressively provided ground-truth image captions (visual perception), pattern explanations (inductive reasoning), and deductive reasoning steps; few-shot demonstrations also tested.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting (explicit request to describe image then reason step-by-step), model-based answer extraction prompt; progressive prompting with ground-truth perceptual and inductive explanations; few-shot in-context demonstrations (interleaved puzzle instances with reasoning explanations) used to test analogical learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Single-concept average accuracy: 46.4%. Per-category (single-concept): Numbers 67.5%, Colors 42.0%, Size 35.0%, Shapes 41.0%. Dual-concept average: 45.5%; notable dual-category scores: Colors & Numbers 56.0%, Colors & Size 55.0%. On a sampled subset (40 instances) where humans scored 91.6% average, GPT-4V scored 47.5% on same set. Performance improved substantially when ground-truth perception+inductive explanations were provided (models solved almost all such cases per authors' analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Indirect evidence: when provided with ground-truth visual perception and inductive reasoning explanations, GPT-4V 'is able to solve almost all cases', indicating the model can apply spatial/deductive reasoning given correct perceptual inputs and induced patterns. Qualitative analysis shows both successful pattern deductions and pronounced visual-perception failures (misrecognizing polygon shapes, hallucinating shapes) that break spatial reasoning. Few-shot demonstrations increase accuracy, suggesting capacity for analogical spatial generalization when shown examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against Qwen-VL-Chat, LLaVA-13B, Gemini Pro, and Claude 3 Opus; GPT-4V had highest single-concept average (46.4%) and highest dual-concept average (45.5%). Compared to random baseline (avg ~27.1% single-concept), GPT-4V outperforms substantially. Compared to human baseline (23 students, 91.6% on sampled set), GPT-4V is far lower (47.5% on same sampled set). Ablations: CoT prompting outperformed direct prompting for GPT-4V (see Appendix Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Main bottlenecks reported: weaker visual perception and inductive reasoning in zero-shot setting. Failure cases include misperceiving shapes (hallucinating objects), incomplete or incorrect image descriptions leading to wrong deductions, and spurious inductive hypotheses. Few-shot improvements were inconsistent. CoT more effective than direct prompting for GPT-4V, indicating sensitivity to prompting style. The model often requires ground-truth perceptual or inductive guidance to reach near-human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8552.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8552.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3 Opus (claude-3-opus-20240229)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source multimodal-capable model from Anthropic; evaluated on PuzzleVQA to probe abstract spatial and pattern reasoning, showing strong performance relative to other evaluated models but below human level.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic's high-performing multimodal model (queried via 'claude-3-opus-20240229' API); used as closed-source baseline for PuzzleVQA experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>PuzzleVQA (single-concept and dual-concept abstract pattern puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Multiple-choice VQA over abstract visual layouts requiring perception of spatial arrangements and induction of layout-based patterns involving colors, numbers, shapes, and size.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same as for other models: zero-shot chain-of-thought prompting ('Let's describe the image first and think step by step'), greedy decoding, model-based answer extraction; progressive providing of ground-truth visual and inductive explanations in ablation-style experiments; few-shot demonstrations tested.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot CoT prompting; progressive provision of ground-truth perceptual and inductive explanations to diagnose bottlenecks; few-shot in-context demonstrations used to test analogical transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Single-concept average accuracy: 39.4%. Per-category (single-concept): Numbers 47.0%, Colors 32.5%, Size 33.5%, Shapes 44.5%. Dual-concept average: 43.7% (strong dual-category performance: Numbers & Colors 54.5%, Colors & Size 50.0%). Improved dramatically when provided with both visual perception and inductive reasoning explanations (authors report Claude 3 Opus able to 'solve almost all cases' under that condition).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Authors report that Claude 3 Opus, like GPT-4V, can solve nearly all puzzles when provided ground-truth visual perception and inductive reasoning explanations, indicating the model can perform spatial/deductive reasoning when perceptual and pattern knowledge are supplied. No explicit internal probing; qualitative analysis limited but the strong improvement when given perceptual+inductive guidance is direct evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperformed Gemini, LLaVA, and Qwen-VL-Chat on average but below GPT-4V. On some shape categories Claude 3 Opus scored highest among non-GPT-4V models. Performance compared to human baseline (91.6%) remains much lower.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Primary bottlenecks identified are visual perception and inductive reasoning in the zero-shot setting; requires ground-truth perceptual/pattern guidance to achieve near-perfect performance. Some categories still show limited accuracy; few-shot gains vary. No evidence of explicit symbolic spatial search mechanisms; failures arise mainly from misperception or incorrect pattern induction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8552.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8552.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's high-capability multimodal model used via API ('gemini-pro-vision'); evaluated on PuzzleVQA and found to have moderate performance and different bottlenecks from Claude and GPT-4V.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google's multimodal model (queried using the 'gemini-pro-vision' API) included as a closed-source baseline; described as 'highly capable' in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>PuzzleVQA (single-concept and dual-concept abstract pattern puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Abstract visual puzzles presented as images with spatial layouts and multiple-choice questions; require perceiving arrangement and applying spatial relations (e.g., opposite parts same color).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot CoT prompting with 'Let's describe the image first and think step by step', greedy decoding, answer extraction stage; additional experiments with progressive ground-truth explanations and few-shot demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Chain-of-Thought prompting; progressive ground-truth perceptual/inductive/deductive hints; few-shot demonstrations. Authors note Gemini Pro shows largest improvement when guided by visual, inductive, and deductive explanations together, indicating a need for full guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Single-concept average accuracy: 34.5%. Per-category (single-concept): Numbers 32.5%, Colors 32.0%, Size 33.5%, Shapes 40.0%. Dual-concept average: 30.1% with mixed category scores (e.g., Size & Shapes 36.5%, Colors & Size 37.0%). Performance generally above random for some categories but below top-performing models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>When provided with ground-truth visual perception, inductive, and deductive explanations together, Gemini Pro shows the largest improvement among evaluated models, suggesting it can perform spatial/deductive operations given full guidance, but it relies more heavily on explicit deductive guidance than GPT-4V/Claude (per authors' interpretation). Qualitative failure cases show perceptual/inductive issues.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to other models, Gemini Pro is middle-ranked (better than some open-source models but behind GPT-4V and Claude 3 Opus). Direct prompting (no CoT) reduced Gemini Pro's performance relative to CoT prompting (Appendix Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors find Gemini Pro's bottleneck is more in deductive reasoning (applying general principles) and requires all three stages of guidance (visual, inductive, deductive) for largest gains. Shows perceptual errors and sensitivity to prompting style; inconsistent few-shot gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8552.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8552.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-13B (LLaVA v1.5 on LLaMA foundation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal model built on LLaMA family weights (used here with the 1.5 version's weights); evaluated on PuzzleVQA and found to perform near random on many categories but to benefit from guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal model based on the LLaMA foundation language model; authors used the 1.5 version weights and default chat template for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>PuzzleVQA (single-concept and dual-concept abstract pattern puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Multiple-choice abstract visual pattern puzzles requiring spatial perception and reasoning across categories (numbers, colors, shapes, size) and their combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot chain-of-thought prompting with CoT trigger and answer extraction stage; experiments also include progressive provision of ground-truth explanations (visual/perceptual, inductive, deductive) and few-shot demonstrations; some fine-tuning experiments reported in supplementary material.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Chain-of-Thought prompting; progressive ground-truth hints; few-shot demonstrations; preliminary fine-tuning experiments (reported in supplement) and analysis comparing CoT vs direct prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Single-concept average accuracy: 27.5%. Per-category (single-concept): Numbers 26.0%, Colors 29.0%, Size 29.0%, Shapes 26.0%. Dual-concept average: 31.1% with some higher category performance (Size & Shapes 39.0%). Overall near-random in several categories but shows category-specific strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>LLaVA-13B shows the largest relative improvement when provided with ground-truth visual, inductive, and deductive explanations together, suggesting its main bottleneck is applying deductive reasoning once pattern and perception are supplied. Few-shot demonstrations and fine-tuning can help (supplementary experiments), indicating latent capacity for spatial reasoning if trained/guided appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms closed-source models (GPT-4V, Claude 3 Opus, Gemini Pro) on average; in some dual-concept categories (Size & Shapes) it scored highest among models (39.0%). CoT prompting provided small improvements over direct prompting for some models; for LLaVA differences reported in appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Per authors, LLaVA-13B's bottleneck is deductive application of learned patterns (needs full guidance); performs near random baseline in several categories; requires more guidance or fine-tuning to reach reasonable performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8552.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8552.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-VL-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-VL-Chat (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source vision-language model with released weights (7B) evaluated on PuzzleVQA; exhibits performance close to random on many abstract spatial categories in the zero-shot CoT setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-VL-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal model (7B) designed to understand text and images; the paper used the publicly released weights and default chat template.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>PuzzleVQA (single-concept and dual-concept abstract pattern puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Multiple-choice abstract visual pattern puzzles (layout-based spatial relations among objects requiring perception and pattern induction).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot chain-of-thought prompting with image+question and CoT trigger; greedy decoding and answer extraction; progressive provision of ground-truth explanations and few-shot demonstrations tested.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Chain-of-Thought prompting; progressive ground-truth hints; few-shot demonstrations used to test analogical transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Single-concept average accuracy: 28.4%. Per-category (single-concept): Numbers 25.0%, Colors 22.0%, Size 32.5%, Shapes 34.0%. Dual-concept average: 28.3% (per dual categories ranging ~26â€“31%). Performance around random baseline in several categories.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Limited evidence of robust spatial reasoning in zero-shot; shows moderate gains with additional guidance (visual/perceptual and inductive explanations) and with few-shot demonstrations, suggesting some capacity for analogical transfer but substantial perceptual/inductive weaknesses remain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Lowest-performing among closed-source models and similar to or slightly above the random baseline in many categories; outperformed by GPT-4V and Claude 3 Opus. Shows less benefit from CoT vs direct prompting relative to some other models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Near-random performance in many categories; bottlenecks include visual perception and inductive reasoning in zero-shot; benefits from ground-truth guidance and few-shot examples but remains substantially below human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Raven: A dataset for relational and analogical visual reasoning <em>(Rating: 2)</em></li>
                <li>The dawn of lmms: Preliminary explorations with gpt-4v(ision) <em>(Rating: 2)</em></li>
                <li>The conceptarc benchmark: Evaluating understanding and generalization in the arc domain <em>(Rating: 2)</em></li>
                <li>Eyes wide shut? exploring the visual shortcomings of multimodal llms <em>(Rating: 1)</em></li>
                <li>CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8552",
    "paper_id": "paper-699ea64f92e05663457e7ae9c4b410e3d30c86d4",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V (gpt-4-visionpreview)",
            "brief_description": "A closed-source multimodal variant of GPT-4 from OpenAI that accepts images and text; evaluated in this paper on abstract visual pattern puzzles (PuzzleVQA) using zero-shot CoT prompting and other controlled prompt interventions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4V",
            "model_description": "OpenAI multimodal model (vision-enabled GPT-4 variant) queried via the 'gpt-4-visionpreview' API; used as the primary high-performing closed-source multimodal baseline in the experiments.",
            "model_size": null,
            "puzzle_name": "PuzzleVQA (single-concept and dual-concept abstract pattern puzzles)",
            "puzzle_type": "Multiple-choice visual question answering over abstract spatial patterns (e.g., hexagonal arrangements, spatial-opposite equality, spatial relations between objects); requires spatial perception and reasoning (layout-based pattern induction and application).",
            "task_setup": "Zero-shot chain-of-thought prompting: prompt modified to include image and question plus CoT trigger ('Let's describe the image first and think step by step'); greedy decoding; model-based answer extraction stage using a second prompt to force an explicit multiple-choice letter answer. Primary setting: image + question (no ground-truth explanations). Additional experiments progressively provided ground-truth image captions (visual perception), pattern explanations (inductive reasoning), and deductive reasoning steps; few-shot demonstrations also tested.",
            "mechanisms_or_strategies": "Zero-shot Chain-of-Thought prompting (explicit request to describe image then reason step-by-step), model-based answer extraction prompt; progressive prompting with ground-truth perceptual and inductive explanations; few-shot in-context demonstrations (interleaved puzzle instances with reasoning explanations) used to test analogical learning.",
            "performance_metrics": "Single-concept average accuracy: 46.4%. Per-category (single-concept): Numbers 67.5%, Colors 42.0%, Size 35.0%, Shapes 41.0%. Dual-concept average: 45.5%; notable dual-category scores: Colors & Numbers 56.0%, Colors & Size 55.0%. On a sampled subset (40 instances) where humans scored 91.6% average, GPT-4V scored 47.5% on same set. Performance improved substantially when ground-truth perception+inductive explanations were provided (models solved almost all such cases per authors' analysis).",
            "evidence_of_spatial_reasoning": "Indirect evidence: when provided with ground-truth visual perception and inductive reasoning explanations, GPT-4V 'is able to solve almost all cases', indicating the model can apply spatial/deductive reasoning given correct perceptual inputs and induced patterns. Qualitative analysis shows both successful pattern deductions and pronounced visual-perception failures (misrecognizing polygon shapes, hallucinating shapes) that break spatial reasoning. Few-shot demonstrations increase accuracy, suggesting capacity for analogical spatial generalization when shown examples.",
            "comparisons": "Compared against Qwen-VL-Chat, LLaVA-13B, Gemini Pro, and Claude 3 Opus; GPT-4V had highest single-concept average (46.4%) and highest dual-concept average (45.5%). Compared to random baseline (avg ~27.1% single-concept), GPT-4V outperforms substantially. Compared to human baseline (23 students, 91.6% on sampled set), GPT-4V is far lower (47.5% on same sampled set). Ablations: CoT prompting outperformed direct prompting for GPT-4V (see Appendix Table 5).",
            "limitations_or_failure_cases": "Main bottlenecks reported: weaker visual perception and inductive reasoning in zero-shot setting. Failure cases include misperceiving shapes (hallucinating objects), incomplete or incorrect image descriptions leading to wrong deductions, and spurious inductive hypotheses. Few-shot improvements were inconsistent. CoT more effective than direct prompting for GPT-4V, indicating sensitivity to prompting style. The model often requires ground-truth perceptual or inductive guidance to reach near-human performance.",
            "uuid": "e8552.0",
            "source_info": {
                "paper_title": "PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Claude 3 Opus",
            "name_full": "Claude 3 Opus (claude-3-opus-20240229)",
            "brief_description": "A closed-source multimodal-capable model from Anthropic; evaluated on PuzzleVQA to probe abstract spatial and pattern reasoning, showing strong performance relative to other evaluated models but below human level.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude 3 Opus",
            "model_description": "Anthropic's high-performing multimodal model (queried via 'claude-3-opus-20240229' API); used as closed-source baseline for PuzzleVQA experiments.",
            "model_size": null,
            "puzzle_name": "PuzzleVQA (single-concept and dual-concept abstract pattern puzzles)",
            "puzzle_type": "Multiple-choice VQA over abstract visual layouts requiring perception of spatial arrangements and induction of layout-based patterns involving colors, numbers, shapes, and size.",
            "task_setup": "Same as for other models: zero-shot chain-of-thought prompting ('Let's describe the image first and think step by step'), greedy decoding, model-based answer extraction; progressive providing of ground-truth visual and inductive explanations in ablation-style experiments; few-shot demonstrations tested.",
            "mechanisms_or_strategies": "Zero-shot CoT prompting; progressive provision of ground-truth perceptual and inductive explanations to diagnose bottlenecks; few-shot in-context demonstrations used to test analogical transfer.",
            "performance_metrics": "Single-concept average accuracy: 39.4%. Per-category (single-concept): Numbers 47.0%, Colors 32.5%, Size 33.5%, Shapes 44.5%. Dual-concept average: 43.7% (strong dual-category performance: Numbers & Colors 54.5%, Colors & Size 50.0%). Improved dramatically when provided with both visual perception and inductive reasoning explanations (authors report Claude 3 Opus able to 'solve almost all cases' under that condition).",
            "evidence_of_spatial_reasoning": "Authors report that Claude 3 Opus, like GPT-4V, can solve nearly all puzzles when provided ground-truth visual perception and inductive reasoning explanations, indicating the model can perform spatial/deductive reasoning when perceptual and pattern knowledge are supplied. No explicit internal probing; qualitative analysis limited but the strong improvement when given perceptual+inductive guidance is direct evidence.",
            "comparisons": "Outperformed Gemini, LLaVA, and Qwen-VL-Chat on average but below GPT-4V. On some shape categories Claude 3 Opus scored highest among non-GPT-4V models. Performance compared to human baseline (91.6%) remains much lower.",
            "limitations_or_failure_cases": "Primary bottlenecks identified are visual perception and inductive reasoning in the zero-shot setting; requires ground-truth perceptual/pattern guidance to achieve near-perfect performance. Some categories still show limited accuracy; few-shot gains vary. No evidence of explicit symbolic spatial search mechanisms; failures arise mainly from misperception or incorrect pattern induction.",
            "uuid": "e8552.1",
            "source_info": {
                "paper_title": "PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Gemini Pro",
            "name_full": "Gemini Pro",
            "brief_description": "Google's high-capability multimodal model used via API ('gemini-pro-vision'); evaluated on PuzzleVQA and found to have moderate performance and different bottlenecks from Claude and GPT-4V.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini Pro",
            "model_description": "Google's multimodal model (queried using the 'gemini-pro-vision' API) included as a closed-source baseline; described as 'highly capable' in the paper.",
            "model_size": null,
            "puzzle_name": "PuzzleVQA (single-concept and dual-concept abstract pattern puzzles)",
            "puzzle_type": "Abstract visual puzzles presented as images with spatial layouts and multiple-choice questions; require perceiving arrangement and applying spatial relations (e.g., opposite parts same color).",
            "task_setup": "Zero-shot CoT prompting with 'Let's describe the image first and think step by step', greedy decoding, answer extraction stage; additional experiments with progressive ground-truth explanations and few-shot demonstrations.",
            "mechanisms_or_strategies": "Chain-of-Thought prompting; progressive ground-truth perceptual/inductive/deductive hints; few-shot demonstrations. Authors note Gemini Pro shows largest improvement when guided by visual, inductive, and deductive explanations together, indicating a need for full guidance.",
            "performance_metrics": "Single-concept average accuracy: 34.5%. Per-category (single-concept): Numbers 32.5%, Colors 32.0%, Size 33.5%, Shapes 40.0%. Dual-concept average: 30.1% with mixed category scores (e.g., Size & Shapes 36.5%, Colors & Size 37.0%). Performance generally above random for some categories but below top-performing models.",
            "evidence_of_spatial_reasoning": "When provided with ground-truth visual perception, inductive, and deductive explanations together, Gemini Pro shows the largest improvement among evaluated models, suggesting it can perform spatial/deductive operations given full guidance, but it relies more heavily on explicit deductive guidance than GPT-4V/Claude (per authors' interpretation). Qualitative failure cases show perceptual/inductive issues.",
            "comparisons": "Compared to other models, Gemini Pro is middle-ranked (better than some open-source models but behind GPT-4V and Claude 3 Opus). Direct prompting (no CoT) reduced Gemini Pro's performance relative to CoT prompting (Appendix Table 5).",
            "limitations_or_failure_cases": "Authors find Gemini Pro's bottleneck is more in deductive reasoning (applying general principles) and requires all three stages of guidance (visual, inductive, deductive) for largest gains. Shows perceptual errors and sensitivity to prompting style; inconsistent few-shot gains.",
            "uuid": "e8552.2",
            "source_info": {
                "paper_title": "PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LLaVA-13B",
            "name_full": "LLaVA-13B (LLaVA v1.5 on LLaMA foundation)",
            "brief_description": "An open-source multimodal model built on LLaMA family weights (used here with the 1.5 version's weights); evaluated on PuzzleVQA and found to perform near random on many categories but to benefit from guidance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaVA-13B",
            "model_description": "Open-source multimodal model based on the LLaMA foundation language model; authors used the 1.5 version weights and default chat template for evaluation.",
            "model_size": "13B",
            "puzzle_name": "PuzzleVQA (single-concept and dual-concept abstract pattern puzzles)",
            "puzzle_type": "Multiple-choice abstract visual pattern puzzles requiring spatial perception and reasoning across categories (numbers, colors, shapes, size) and their combinations.",
            "task_setup": "Zero-shot chain-of-thought prompting with CoT trigger and answer extraction stage; experiments also include progressive provision of ground-truth explanations (visual/perceptual, inductive, deductive) and few-shot demonstrations; some fine-tuning experiments reported in supplementary material.",
            "mechanisms_or_strategies": "Chain-of-Thought prompting; progressive ground-truth hints; few-shot demonstrations; preliminary fine-tuning experiments (reported in supplement) and analysis comparing CoT vs direct prompting.",
            "performance_metrics": "Single-concept average accuracy: 27.5%. Per-category (single-concept): Numbers 26.0%, Colors 29.0%, Size 29.0%, Shapes 26.0%. Dual-concept average: 31.1% with some higher category performance (Size & Shapes 39.0%). Overall near-random in several categories but shows category-specific strengths.",
            "evidence_of_spatial_reasoning": "LLaVA-13B shows the largest relative improvement when provided with ground-truth visual, inductive, and deductive explanations together, suggesting its main bottleneck is applying deductive reasoning once pattern and perception are supplied. Few-shot demonstrations and fine-tuning can help (supplementary experiments), indicating latent capacity for spatial reasoning if trained/guided appropriately.",
            "comparisons": "Underperforms closed-source models (GPT-4V, Claude 3 Opus, Gemini Pro) on average; in some dual-concept categories (Size & Shapes) it scored highest among models (39.0%). CoT prompting provided small improvements over direct prompting for some models; for LLaVA differences reported in appendix.",
            "limitations_or_failure_cases": "Per authors, LLaVA-13B's bottleneck is deductive application of learned patterns (needs full guidance); performs near random baseline in several categories; requires more guidance or fine-tuning to reach reasonable performance.",
            "uuid": "e8552.3",
            "source_info": {
                "paper_title": "PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Qwen-VL-Chat",
            "name_full": "Qwen-VL-Chat (7B)",
            "brief_description": "An open-source vision-language model with released weights (7B) evaluated on PuzzleVQA; exhibits performance close to random on many abstract spatial categories in the zero-shot CoT setting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen-VL-Chat",
            "model_description": "Open-source multimodal model (7B) designed to understand text and images; the paper used the publicly released weights and default chat template.",
            "model_size": "7B",
            "puzzle_name": "PuzzleVQA (single-concept and dual-concept abstract pattern puzzles)",
            "puzzle_type": "Multiple-choice abstract visual pattern puzzles (layout-based spatial relations among objects requiring perception and pattern induction).",
            "task_setup": "Zero-shot chain-of-thought prompting with image+question and CoT trigger; greedy decoding and answer extraction; progressive provision of ground-truth explanations and few-shot demonstrations tested.",
            "mechanisms_or_strategies": "Chain-of-Thought prompting; progressive ground-truth hints; few-shot demonstrations used to test analogical transfer.",
            "performance_metrics": "Single-concept average accuracy: 28.4%. Per-category (single-concept): Numbers 25.0%, Colors 22.0%, Size 32.5%, Shapes 34.0%. Dual-concept average: 28.3% (per dual categories ranging ~26â€“31%). Performance around random baseline in several categories.",
            "evidence_of_spatial_reasoning": "Limited evidence of robust spatial reasoning in zero-shot; shows moderate gains with additional guidance (visual/perceptual and inductive explanations) and with few-shot demonstrations, suggesting some capacity for analogical transfer but substantial perceptual/inductive weaknesses remain.",
            "comparisons": "Lowest-performing among closed-source models and similar to or slightly above the random baseline in many categories; outperformed by GPT-4V and Claude 3 Opus. Shows less benefit from CoT vs direct prompting relative to some other models.",
            "limitations_or_failure_cases": "Near-random performance in many categories; bottlenecks include visual perception and inductive reasoning in zero-shot; benefits from ground-truth guidance and few-shot examples but remains substantially below human baseline.",
            "uuid": "e8552.4",
            "source_info": {
                "paper_title": "PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Raven: A dataset for relational and analogical visual reasoning",
            "rating": 2
        },
        {
            "paper_title": "The dawn of lmms: Preliminary explorations with gpt-4v(ision)",
            "rating": 2
        },
        {
            "paper_title": "The conceptarc benchmark: Evaluating understanding and generalization in the arc domain",
            "rating": 2
        },
        {
            "paper_title": "Eyes wide shut? exploring the visual shortcomings of multimodal llms",
            "rating": 1
        },
        {
            "paper_title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning",
            "rating": 1
        }
    ],
    "cost": 0.014073249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns</h1>
<p>Yew Ken Chia ${ }^{1,2 *}$, Vernon Toh Yan Han ${ }^{1}$, Deepanway Ghosal ${ }^{1}$, Lidong Bing ${ }^{2}$, Soujanya Poria ${ }^{1}$<br>${ }^{1}$ Singapore University of Technology and Design, ${ }^{2}$ DAMO Academy, Alibaba Group, Singapore<br>D. https://github.com/declare-lab/LLM-PuzzleTest<br>@. https://puzzlevqa.github.io/</p>
<h4>Abstract</h4>
<p>Large multimodal models extend the impressive capabilities of large language models by integrating multimodal understanding abilities. However, it is not clear how they can emulate the general intelligence and reasoning ability of humans. As recognizing patterns and abstracting concepts are key to general intelligence, we introduce PuzzleVQA, a collection of 2000 puzzle instances based on abstract patterns. With this dataset, we evaluate large multimodal models with abstract patterns based on fundamental concepts, including colors, numbers, sizes, and shapes. Through our experiments on state-of-the-art large multimodal models, we find that they are not able to generalize well to simple abstract patterns. Notably, GPT4 V achieves a score of $46.4 \%$ on single-concept puzzles, which shows that state-of-the-art models struggle on our dataset. To diagnose the reasoning challenges in large multimodal models, we progressively guide the models with our ground truth reasoning explanations for visual perception, inductive reasoning, and deductive reasoning. Our systematic analysis finds that the main bottlenecks of GPT-4V are weaker visual perception and inductive reasoning abilities. Through this work, we hope to shed light on the limitations of large multimodal models and how they can better emulate human cognitive processes in the future ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Rapid advances in large language models have demonstrated remarkable capabilities across diverse language tasks and applications (Bubeck et al., 2023; Brown et al., 2020; Touvron et al., 2023b). To enable more general capabilities, large</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example question which involves the color concept in PuzzleVQA, and an incorrect answer from GPT-4V. There are generally three stages that can be observed in the solving process: visual perception (blue), inductive reasoning (green), and deductive reasoning (red). Here, the visual perception was incomplete, causing a mistake during deductive reasoning.
multimodal models were introduced by integrating large language models with multimodal understanding (Yue et al., 2023; Yang et al., 2023; OpenAI, 2023). However, it is not clear how large multimodal models can emulate the general intelligence and reasoning ability of humans (Qiu et al., 2024). Specifically, we aim to explore how large multimodal models can emulate cognitive processes to perceive and interpret information, extrapolate from observations to broader generaliza-</p>
<p>tions, and apply general principles to solve specific problems (Piaget, 1976). Furthermore, we are interested in understanding how well the models can reason about fundamental concepts such as numbers, colors, shapes, and size (Tong et al., 2024; Sharma et al., 2024).</p>
<p>As pattern recognition and and abstracting concepts are at the heart of general intelligence (Tenenbaum, 2018; Carey, 2000; Cole, 1996), we believe that abstract patterns are a suitable testbed for evaluating reasoning ability in large multimodal models. As shown in Figure 1, abstract patterns enable us to focus on one or more abstract concepts, and decompose the multimodal reasoning process into several stages that mimic human cognitive processes (Piaget, 1976). Firstly, the model requires visual perception to understand and interpret the abstract image in the input. Secondly, the model requires inductive reasoning to relate the observations shown and form a hypothesis for the underlying pattern. Thirdly, the model requires deductive reasoning to apply the general principle of the pattern to solve the specific problem at hand. While the abstract patterns may seem simple, we surprisingly find that even advanced large multimodal models such as Gemini Pro (Gemini Team, 2023) and GPT-4V (OpenAI, 2023) struggle to understand them.</p>
<p>Puzzles are problems that require ingenuity and creativity to solve, and they can serve as valuable tools for cognitive development and assessment (Zhang et al., 2019). Hence, we propose the PuzzLeVQA dataset to systematically evaluate and diagnose the reasoning challenges in large multimodal models. Our dataset consists of diverse multimodal puzzles that focus on abstract patterns with fundamental concepts including numbers, colors, shapes, and size. We design and automatically construct the dataset through multimodal templates, enabling us to generate large numbers of puzzles without costly human annotation (Ding et al., 2022). To support interpretability and systematic investigation of reasoning challenges in multimodal models, we also construct the ground truth reasoning explanations for each puzzle. Compared to existing datasets for visual question answering, PuzzLeVQA focuses specifically on how large multimodal models can mimic general cognitive processes such as inductive and deductive reasoning. As we focus on how models can generalize to novel problems, similar to fluid intelligence in humans (Cattell, 1963), our dataset in the abstract domain poses challenges for existing models without re-
quiring extensive world knowledge.
Through our investigation of leading large multimodal models, we find that existing models are not able to generalize well to simple abstract patterns. Notably, GPT-4V achieves a score of $46.4 \%$ on single-concept puzzles, which shows that state-of-the-art models struggle on our dataset. Our analysis reveals that its main bottlenecks are weaker visual perception and inductive reasoning abilities. Hence, our main contributions include:</p>
<ol>
<li>To investigate the cognitive and reasoning abilities of large multimodal models, we propose to leverage abstract patterns.</li>
<li>We introduce PUZZLEVQA, an automatically generated and diverse dataset of 2000 multimodal samples with reasoning explanations.</li>
<li>Our experiments show that even advanced large multimodal models do not generalize well to abstract patterns, and we show how to identify their reasoning bottlenecks.</li>
</ol>
<h2>2 Background: Cognitive Theories</h2>
<p>To understand how large multimodal models can better mimic human thought processes and general intelligence, we first ground our study with relevant cognitive theories.</p>
<h3>2.1 Fluid and Crystallized Intelligence</h3>
<p>The Cattell-Horn theory (Cattell, 1963) of cognitive abilities distinguishes between two types of intelligence: fluid intelligence, which involves the ability to solve novel problems without relying on previously acquired knowledge, and crystallized intelligence, which involves the use of knowledge, skills, and experience. Fluid intelligence in humans could parallel large multimodal models' ability to solve new, unseen problems through pattern recognition and problem-solving strategies. On the other hand, crystallized intelligence could be akin to how the models leverage accumulated world knowledge from training data to understand and interact with the world (Sumers et al., 2023). As many works have focused on how models can leverage specialized knowledge (Yue et al., 2023), we instead focus on how they may emulate fluid intelligence to solve novel problems through abstract patterns.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration example of components (top) and reasoning explanations (bottom) for abstract puzzles in PUZZLEVQA. To construct each puzzle instance, we first define the layout and pattern of a multimodal template, and populate the template with suitable objects that demonstrate the underlying pattern. For interpretability, we also construct ground truth reasoning explanations to interpret the puzzle and explain the general solution stages.</p>
<h3>2.2 Cognitive Development</h3>
<p>Piaget's Stages of Cognitive Development (Piaget, 1976) can provide a framework for progressing from basic sensory experiences to complex abstract reasoning and problem-solving. While we note that the large multimodal models do not develop in the same organic and experiential manner as humans, we are guided to explore how the models can emulate different stages of cognitive abilities. Concretely, through abstract patterns, we can evaluate how the models perceive multimodal information, reason inductively to extrapolate from observations to broader generalizations, and apply general principles to deduce the solution for specific problems.</p>
<p>Sensorimotor. This stage underpins visual perception, where individuals learn to coordinate sensory experiences through interactions with the environment. To emulate this cognitive stage, we would expect models to identify simple shapes or colors but lack higher-level reasoning. Hence, we set the foundation for later stages by exploring abstract patterns based on fundamental concepts including colors, numbers, shapes, and size.</p>
<p>Preoperational. At this stage, individuals develop symbolic thinking, which is crucial for understanding representations in visual contexts and beginning to engage in simple, inductive reasoning processes. Models that mimic this stage should be able to perform basic reasoning about objects or concepts, but with limited understanding of abstract relationships or performing logical operations.</p>
<p>Concrete Operational. This stage is closely related to inductive reasoning, as individuals learn to think logically about concrete events and solve problems based on visible patterns and relationships. We would expect models that are analogous to this stage to have the ability to draw logical conclusions from specific instances and start to apply these conclusions to solve problems. Hence, we consider inductive reasoning as an integral part of understanding abstract patterns.</p>
<p>Formal Operational. This stage is essential for deductive reasoning and abstract thinking, allowing individuals to hypothesize and think about theoretical scenarios, which are skills necessary for solving complex problems. At this stage, we would expect comparable models to effectively induce general principles or hypotheses from observations and logically deduce specific outcomes, even in abstract or novel contexts. Thus, we consider deductive reasoning as critical to solving abstract problems.</p>
<h2>3 PuzzleVQA Dataset</h2>
<p>Despite the impressive capabilities of large multimodal models, we do not fully understand how they solve multimodal problems through reasoning.</p>
<p>Specifically, we focus on how well they can interpret multimodal inputs, form generalizations from observations, and apply the general principles to solve specific cases. Furthermore, they may reason differently about fundamental concepts such as numbers, colors, shapes, and size. Hence, we propose PUZZLEVQA, a diverse collection of abstract pattern puzzles to diagnose reasoning challenges in multimodal models. The dataset is automatically constructed through multimodal templates, and includes reasoning explanations for interpretability.</p>
<h3>3.1 Puzzle Components</h3>
<p>As shown in Figure 2, each puzzle in our dataset is formulated with the following main components:</p>
<ol>
<li>Objects: The conceptual elements that interact within the puzzle, such as numbers, colors, shapes, and size.</li>
<li>Layout: The spatial arrangement of objects that provides visual context.</li>
<li>Pattern: The relationship that governs the interaction amongst objects. For example, a pattern may be that spatially opposite parts must have the same color.</li>
<li>Demonstrations: Multiple instances of interacting objects that collectively represent the underlying pattern. Without demonstrations, the pattern would become ambiguous.</li>
<li>Query: The natural language question that directs the multimodal model how to solve the puzzle by determining the missing object.</li>
</ol>
<h3>3.2 Design Considerations</h3>
<p>We have three main design considerations for each puzzle in our dataset:</p>
<p>Simplicity. As the focus is on evaluating how large multimodal models reason about fundamental abstract concepts, we do not deliberately make the puzzles more complex than necessary. We also aim to make the underlying patterns straightforward, without requiring extensive world knowledge or advanced theories.</p>
<p>Correctness. To avoid potentially noisy annotations, we use an automatic approach with multimodal templates to generate each puzzle. For instance, given a visual layout and pattern of a template, we can automatically populate the template
with the appropriate objects that demonstrate the pattern. As each puzzle instance is created based on the specific rules in the template, we can ensure that they do not contain annotation mistakes.</p>
<p>Diversity. To investigate the multimodal reasoning capabilities across diverse abstract concepts, we construct puzzles based on four main concepts: numbers, colors, shapes, and size. Furthermore, to evaluate how well the models can relate to multiple concepts, we design both single-concept and dual-concept puzzles.</p>
<h3>3.3 Puzzle Construction</h3>
<p>Multimodal Templates. To construct each abstract puzzle, we leverage multimodal templates based on fundamental concepts including numbers, colors, shapes, and size. Following the formulation and design considerations previously discussed, we first define the layout and abstract pattern for the puzzle. Each template can be randomly populated with the specific objects to represent the underlying pattern through demonstrations, forming a specific puzzle instance. For example, to construct a colorbased puzzle instance shown in Figure 2, we focus on the concept of colors and define the layout as a hexagonal arrangement of six parts, with the abstract pattern that spatially opposite parts must have the same color. Thereafter, the template can be randomly populated to satisfy the pattern with colors from a predefined list of possible colors. Lastly, the query is constructed based on the fundamental concepts in the abstract pattern. To demonstrate our puzzle generation pipeline, we include a detailed implementation in Appendix A.4, based on the puzzle in Figure 1.</p>
<p>Reasoning Explanations. To ensure that each abstract puzzle can be easily understood, we also construct reasoning explanations based on the three problem solving stages: image descriptions for visual perception, pattern explanations for inductive reasoning, and deductive reasoning steps. Specifically, we leverage textual templates that can be populated with details from the specific puzzle instance, as shown in Figure 2. In our experiments in Section 5, this enables us to identify reasoning bottlenecks by progressively providing the explanations of each stage to the model.</p>
<h3>3.4 Multiple-Choice Format</h3>
<p>While we use straightforward objects in our puzzles, there may be a degree of ambiguity in the an-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Taxonomy of abstract puzzles in PUZZLEVQA with sample questions, based on fundamental concepts such as colors and size. To enhance diversity, we introduce both single-concept and dual-concept puzzles.
swer regarding specific colors or sizes. Hence, we standardize the puzzle format as multiple-choice questions, where all questions are provided with four options, with the exception of three options for size (small, medium, and large). To generate the incorrect choices for each question, we use heuristics including randomly sampling numbers within the same magnitude of the answer, and further details can be found in the supplementary material. We use the standard accuracy metric for evaluation.</p>
<h3>3.5 Dataset Analysis</h3>
<p>To ensure that the dataset contains diverse abstract patterns, we provide a taxonomy of 10 puzzle categories based on fundamental concepts including numbers, colors, shapes, and size. As shown in Figure 3, there are four categories of single-concept patterns. To extend the depth of PuzzLEVQA, we also include dual-concept patterns, which would require models to relate two concepts in order to solve the puzzle. Within each category, we design two multimodal templates that can each be used
to generate many unique puzzle instances. The full list of puzzle templates and examples of more puzzle instances can be found in the supplementary material. To maintain a reasonable dataset size for evaluating large multimodal models, we generate 100 unique puzzle instances from each template. Thus, there are 2000 test instances in PuzzleVQA in total. We conducted an analysis in Appendix A. 5 and found that the chosen dataset size is large enough to be relatively robust to experimental variance.</p>
<h3>3.6 Implementation Details</h3>
<p>We utilize Python code along with packages like Pillow ${ }^{2}$ to automatically generate puzzles. Leveraging these tools, we are able to create many different unique puzzle images and text questions for each given puzzle type by augmenting the base template and objects in the image. Example code snippets to generate the puzzles are included in the supplemen-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Numbers</th>
<th>Colors</th>
<th>Size</th>
<th>Shapes</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Baseline</td>
<td>25.0</td>
<td>25.0</td>
<td>33.3</td>
<td>25.0</td>
<td>27.1</td>
</tr>
<tr>
<td>Qwen-VL-Chat</td>
<td>25.0</td>
<td>22.0</td>
<td>32.5</td>
<td>34.0</td>
<td>28.4</td>
</tr>
<tr>
<td>LLaVA-13B</td>
<td>26.0</td>
<td>29.0</td>
<td>29.0</td>
<td>26.0</td>
<td>27.5</td>
</tr>
<tr>
<td>Gemini Pro</td>
<td>32.5</td>
<td>32.0</td>
<td>33.5</td>
<td>40.0</td>
<td>34.5</td>
</tr>
<tr>
<td>Claude 3 Opus</td>
<td>47.0</td>
<td>32.5</td>
<td>33.5</td>
<td>44.5</td>
<td>39.4</td>
</tr>
<tr>
<td>GPT-4V</td>
<td>67.5</td>
<td>42.0</td>
<td>35.0</td>
<td>41.0</td>
<td>46.4</td>
</tr>
</tbody>
</table>
<p>Table 1: Accuracy of large multimodal models for single-concept abstract patterns in PUZZLEVQA.
tary material, and we plan to release the dataset publicly with a permissive license such as MIT license.</p>
<p>Each puzzle in PUZZLEVQA comprises of an image $x_{\text {image }}$, a natural language question $x_{\text {question }}$, an image caption that describes the image $x_{\text {caption }}$, an explanation that explains the pattern shown in the image $x_{\text {explanation }}$, a deduction statement that applies the pattern to the puzzle to derive the final answer $x_{\text {deduction }}$, a set of multiple-choice answers $x_{\text {options }}$, and the final answer $x_{\text {answer }}$. All of which are automatically generated during the puzzle creation process.</p>
<h2>4 Experimental Setup</h2>
<h3>4.1 Inference Pipeline</h3>
<p>To elicit reasoning steps from large multimodal models, we leverage zero-shot chain of thought (CoT) prompting [kojima2022scaling] with a prompt similar to "Let's think step by step". As the model may not always follow the same multiple-choice answer format, we also employ a model-based answer extraction stage. Detailed examples of the prompts can be found in the supplementary material. Please note that our main experimental setting used in Table 1 and 2 involves only the questions and images as multimodal inputs. On the other hand, we progressively provide additional ground-truth information such as image captions in Section 5.1 to diagnose the multimodal reasoning bottlenecks.</p>
<p>Chain of Thought Prompting. In the first prompting step, we construct a prompt $\hat{x}$ by modifying the question using a specific prompt template: "[I] Question: [X]. Options: [O]. Answer: [T]", where $[I]$ is the input slot for $x_{\text {image }},[X]$ is the input slot for $x_{\text {question }},[O]$ is the input slot for $x_{\text {options }}$, and $[T]$ is the input slot for the trigger sentence $t_{1}$. To elicit reasoning over the multimodal inputs, we use "Let's describe the image first and think step by step" as our trigger sentence
$t_{1}$. This modified prompt $\hat{x}$ is then fed into a large multimodal model, and a greedy decoding strategy is utilized to generate the subsequent sentence $y_{1}$. If the letter answer can be extracted from $y_{1}$ with regular expressions, the prompting process terminates. However, if the letter answer cannot be extracted, we prompt the model itself to extract the answer.</p>
<p>Answer Extraction. In the second prompting stage, we use the generated sentence $y_{1}$ along with the modified prompt $\hat{x}$ to extract the final answer. We concatenate three elements to form " $[\hat{X}][Z]$ $[A]$ " where $[\hat{X}]$ is the input slot for $\hat{x},[Z]$ is the input slot for $y_{1}$, and $[A]$ is the trigger sentence $t_{2}$ to extract the final answer. We defined $t_{2}$ as "Therefore, among (A) (B) (C) (D), the answer is:" or "Therefore, among (A) (B) (C), the answer is:", for puzzles with four and three multiple-choice questions respectively.</p>
<h3>4.2 Models</h3>
<p>To investigate the reasoning ability of large multimodal models, we select the best-performing open and closed-source models [yue2023gpt]:</p>
<ol>
<li>Qwen-VL-Chat (7B) [Bai2024QWN] is an open-source large multimodal model designed to perceive and understand both texts and images. We use the version with open model weights and default chat template.</li>
<li>LLaVA-13B [liu2023llava] is an large multimodal model which is based on the popular LLaMA [Touvron2023LLaMA] foundation language model. We use the model weights of the 1.5 version and default chat template.</li>
<li>Gemini Pro [Gemini Team2023] is a highly capable multimodal model released by Google, and we use their publicly available API to query the "gemini-pro-vision" version of the model.</li>
<li>Claude 3 Opus ${ }^{3}$ is released by Anthropic and the most highest-performing multimodal model in their model family. We use their publicly available API to query the "claude-3-opus-20240229" version of the model.</li>
<li>GPT-4V [OpenAI2023GPT4V] is released by OpenAI and widely regarded as the most capable
<sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Numbers <br> $\&amp;$ <br> Shapes</th>
<th style="text-align: center;">Colors <br> $\&amp;$ <br> Numbers</th>
<th style="text-align: center;">Numbers <br> $\&amp;$ <br> Size</th>
<th style="text-align: center;">Colors <br> $\&amp;$ <br> Shapes</th>
<th style="text-align: center;">Size <br> $\&amp;$ <br> Shapes</th>
<th style="text-align: center;">Colors <br> $\&amp;$ <br> Size</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Random Baseline</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">26.4</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-VL-Chat</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">28.3</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-13B</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">$\mathbf{3 9 . 0}$</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">31.1</td>
</tr>
<tr>
<td style="text-align: left;">Gemini Pro</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">30.1</td>
</tr>
<tr>
<td style="text-align: left;">Claude 3 Opus</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">$\mathbf{3 4 . 0}$</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">43.7</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4V</td>
<td style="text-align: center;">$\mathbf{5 2 . 5}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 0}$</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">$\mathbf{4 7 . 0}$</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">$\mathbf{5 5 . 0}$</td>
<td style="text-align: center;">$\mathbf{4 5 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy of large multimodal models for dual-concept abstract patterns in PUZZLEVQA.
multimodal model based on existing benchmarks (Yue et al., 2023). We use their publicly available API to query the "gpt-4-visionpreview" version of the model.</p>
<h2>5 Results</h2>
<p>We report the main evaluation results on singleconcept and dual-concept puzzles in Table 1 and Table 2 respectively. The evaluation results for single-concept puzzles, as shown in Table 1 reveal notable differences in performance among the open-source and closed-source models. GPT-4V stands out with the highest average score of 46.4 , demonstrating superior abstract pattern reasoning on single-concept puzzles such as numbers, colors, and size. It particularly excels in the "Numbers" category with a score of 67.5 , far surpassing other models, which may be due to its advantage in math reasoning tasks (Yang et al., 2023). Claude 3 Opus follows with an overall average of 39.4 , showing its strength in the "Shapes" category with a top score of 44.5. The other models, including Gemini Pro and LLaVA-13B trail behind with averages of 34.5 and 27.5 respectively, performing similarly to the random baseline on several categories.</p>
<p>In the evaluation on dual-concept puzzles, as shown in Table 2, GPT-4V stands out again with the highest average score of 45.5 . It performed particularly well in categories such as "Colors \&amp; Numbers" and "Colors \&amp; Size" with a score of 56.0 and 55.0 respectively. Claude 3 Opus closely follows with an average of 43.7 , showing strong performance in "Numbers \&amp; Size" with the highest score of 34.0. Interestingly, LLaVA-13B, despite its lower overall average of 31.1 , scores the highest in the "Size \&amp; Shapes" category at 39.0. Gemini Pro, on the other hand, has a more balanced performance across categories but with a slightly lower overall average of 30.1. Overall, we find that mod-
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Analysis on multimodal reasoning bottlenecks. We progressively prompt models with ground-truth explanations for visual perception, inductive reasoning, and deductive reasoning.
els perform similarly on average for single-concept and dual-concept patterns, which indicates that they also struggle with puzzles that require reasoning about multiple abstract concepts.</p>
<h3>5.1 Analysis of Multimodal Reasoning Bottlenecks</h3>
<p>Given the lower performance of existing large multimodal models, this raises the natural question of why they are not able to reason well about abstract patterns. As shown in Figure 2, the stages of solving abstract puzzles can be generally decomposed into visual perception, inductive reasoning, and deductive reasoning. Hence, we analyze their reasoning bottlenecks in Figure 4 by progressively providing the ground truth explanation in their prompts. Note that we omit the final answer in the deductive reasoning explanation to avoid making the question trivial, and the detailed prompts can be found in the supplementary material. Overall, we observe that the models perform better when provided with ground truth explanations, which suggests that they are able to leverage the additional information.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparison between average human performance and large multimodal models on a subset of PUZZLEVQA.</p>
<p>Notably, GPT-4V and Claude 3 Opus are able to solve almost all cases when provided with both visual perception and inductive reasoning explanations. This suggests that the main bottlenecks for GPT-4V and Claude 3 Opus are visual perception and inductive reasoning to interpret the multimodal information and recognize the pattern from observations. However, this is not the case for LLaVA-13B and Gemini Pro, which demonstrate the largest improvement when guided by visual perception, inductive reasoning, and deductive reasoning together. This indicates that their main bottleneck is deductive reasoning to apply general principles of the pattern to solve specific cases. Note that these results are intended to serve as an <strong>optimistic upper bound</strong> of the model performance when provided with ground truth partial information, and may not indicate that the puzzles will become trivial.</p>
<h3>5.2 Comparison to Human Performance</h3>
<p>To further shed light on how the large multimodal models compare to the reasoning ability of humans, we conducted a human baseline study involving 23 university students<sup>4</sup>. Participants were allotted 30 minutes to solve 40 puzzle instances sampled from our 20 puzzle categories, yielding an average human baseline score of 91.6%, as shown in Figure 5. Note that the 20-24 age group of the participants correspond to the formal operational stage of cognitive development, as discussed in Section 2. In contrast, the highest-performing model, GPT-4V scored 47.5% on the same set of puzzle samples, highlighting the specific bottlenecks causing models to fall short of human cognition: primarily in visual perception and inductive reasoning, as discussed in Section 5.1.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Analysis on the effect of few-shot demonstrations on model performance for single-concept puzzles.</p>
<h3>5.3 Effect of Few-Shot Demonstrations</h3>
<p>While we focus on the zero-shot setting to investigate how multimodal models handle novel reasoning challenges, we also explore how models may use knowledge and strategies from other puzzles to solve a new, specific puzzle. This is akin to analogical reasoning, which involves using experience from similar scenarios to make inferences about a novel situation (Webb et al., 2022). Concretely, we run an analysis to study the effect of in-context learning (Brown et al., 2020) with few-shot demonstrations of other puzzle instances when the model is tasked to solve a specific puzzle. The demonstrations consist of interleaved instances of multimodal inputs of each puzzle image and question, as well as the ground truth reasoning explanations. To ensure that the demonstrations are sufficiently diverse, we randomly select puzzles of different categories from the given puzzle. As shown in Figure 6, we find a general trend of increasing performance with respect to the number of demonstrations. Although there are some cases of lower performance for GPT-4V, we see that models generally achieve their best performance with the most number of demonstrations. This suggests that the models are indeed capable of analogical reasoning, and in-context learning may be a promising direction to enhance the abstract reasoning abilities of multimodal models in the future.</p>
<p>In addition, while not the main focus of this work, there may be other methods of improving model performance, including model training or different prompting methods. Hence, we have also included preliminary studies on fine-tuning with LLaVA-13B and comparison between chain-of-thought and direct prompting in the supplementary material. To consider the effect of other factors, the supplementary material further includes an al-</p>
<p><sup>4</sup>Note that the participants volunteered for the short study and we obtained prior permission from their instructor.</p>
<p>ternative setting with text-only models given the ground-truth visual perception caption, and an analysis on the effect of evaluation dataset size.</p>
<h2>6 Related Work</h2>
<p>The recent surge in multimodal pretraining and fine-tuning approaches (Liu et al., 2023; Bai et al., 2024) has led to the creation of various benchmarks. Benchmarks like VQA (Antol et al., 2015) and OK-VQA (Marino et al., 2019) aim at evaluating the basic perception and reasoning abilities of large multimodal models. Meanwhile, benchmarks like MMMU (Yue et al., 2023) and ScienceQA (Lu et al., 2022) offer an evaluation of LLMsâ€™ proficiency across multiple disciplines requiring domain-specific knowledge and multimodal understanding.</p>
<p>To investigate the fundamental challenges in multimodal perception and reasoning, we deliberately focus on the abstract domain, aiming to assess how models emulate cognitive abilities, particularly involving reasoning about abstract concepts and relationships. However, we note that existing benchmarks have limitations which make them less suitable for studying large multimodal models. The RAVEN dataset (Zhang et al., 2019) presents visual matrices with abstract patterns, challenging models to identify patterns and complete missing elements. However, we note that it has a specific spatial layout and can be solved exactly with search algorithms. Compared to CLEVR (Johnson et al., 2017) which offers synthetic visual scenarios and questions focusing on logic and commonsense, we focus on exploring how large multimodal models perceive and reason about multimodal patterns, which is more closely related to fundamental cognitive processes in humans (Mattson, 2014). While ConceptARC (Moskvichev et al., 2023) focuses on specific spatial concepts such as inside-outside and above-below, our dataset PuzzleVQA studies how visual objects interact and relate based on broader abstract concepts such as colors, shapes, numbers, and size. Lastly, the MiniSCAN dataset (Lake et al., 2019) presents patterns that map special words to a sequence of color symbols, but is limited to color-based patterns.</p>
<p>What distinguishes our dataset, PuzzleVQA, from the existing works is its systematic analysis of multimodal reasoning through abstract patterns, including perceptual, inductive, and deductive reasoning. Compared to the previous datasets, our
multimodal patterns encompass broad and fundamental abstract concepts such as numbers, colors, shapes, and size. Notably, our dataset not only provides ground truth answers but also includes image captions and pattern explanations that enable more detailed and systematic diagnosis of the reasoning bottlenecks for large multimodal models.</p>
<h2>7 Conclusion</h2>
<p>In this work, we introduced the PuzzleVQA dataset to investigate the reasoning challenges in large multimodal models. Our experiments demonstrated that, despite their sophistication, models such as GPT-4V exhibit substantial challenges when solving abstract pattern puzzles that require visual perception, inductive reasoning, and deductive reasoning, falling short of cognitive processes displayed by humans. Notably, our systematic analysis with ground truth explanations reveals that the main reasoning bottlenecks for GPT-4V are weaker visual perception and inductive reasoning capabilities. On the other hand, we found that other large multimodal models required more guidance with ground truth explanations, pointing to a broader range of reasoning challenges. Looking ahead, our work points to exciting avenues for advancing the reasoning abilities of large multimodal models. Future research should focus on enhancing modelsâ€™ understanding of multimodal information and refining their abstract reasoning faculties, in order to further enhance their general capabilities.</p>
<h2>Acknowledgement</h2>
<p>This work was substantially supported by DAMO Academy through DAMO Academy Research Intern Program. This work was partially supported by AI Singapore Governance grant ID: AISG3-GV-2023-010, and AcRF MoE Tier-2 grant (Project no. T2MOE2008, Grantor reference no. MOET2EP20220-0017) titled: "CSK NLP: Leveraging Commonsense Knowledge for NLP". Chia Yew Ken would like to thank Tan Hui Min Grace as a source of inspiration for fun and unique puzzles.</p>
<h2>Limitations</h2>
<p>In this work, we mainly focus on the zero-shot setting to investigate how large multimodal models face reasoning challenges in novel situations. However, previous works have shown that prompting with demonstrations (Brown et al., 2020) may improve the models ability to adapt to new tasks. Hence, we also include experiments in the few-shot setting in Section 5.3, which showed inconsistent benefits, and we aim to explore this area in the future works.</p>
<h2>References</h2>
<p>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual Question Answering. In International Conference on Computer Vision (ICCV).</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2024. Qwen-VL: A versatile vision-language model for understanding, localization, text reading, and beyond.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, et al. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A. Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. ArXiv, abs/2303.12712.</p>
<p>Susan Carey. 2000. The origin of concepts. Journal of Cognition and Development, 1:37 - 41.</p>
<p>Raymond Bernard Cattell. 1963. Theory of fluid and crystallized intelligence: A critical experiment. Journal of Educational Psychology, 54:1-22.</p>
<p>Charles Cole. 1996. Fluid concepts and creative analogies: Computer models of the fundamental mechanisms of thought. Journal of the Association for Information Science and Technology, 47:403-404.</p>
<p>Bosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing, Shafiq R. Joty, and Boyang Albert Li. 2022. Is gpt-3 a good data annotator? In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Google Gemini Team. 2023. Gemini: A family of highly capable multimodal models.</p>
<p>Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. 2017. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR.</p>
<p>Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 22199-22213. Curran Associates, Inc.</p>
<p>Brenden M. Lake, Tal Linzen, and Marco Baroni. 2019. Human few-shot learning of compositional instructions. In Annual Meeting of the Cognitive Science Society.</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023. Improved baselines with visual instruction tuning.</p>
<p>Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Advances in Neural Information Processing Systems.</p>
<p>Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Mark P. Mattson. 2014. Superior pattern processing is the essence of the evolved human brain. Frontiers in Neuroscience, 8.</p>
<p>Arseny Moskvichev, Victor Vikram Odouard, and Melanie Mitchell. 2023. The conceptarc benchmark: Evaluating understanding and generalization in the arc domain. ArXiv, abs/2305.07141.</p>
<p>OpenAI. 2023. Gpt-4v(ision) system card.
Jean Piaget. 1976. Piaget's theory.
Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, and Xiang Ren. 2024. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. In The Twelfth International Conference on Learning Representations.</p>
<p>Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, and Antonio Torralba. 2024. A vision check-up for language models. ArXiv, abs/2401.01862.</p>
<p>Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths. 2023. Cognitive architectures for language agents. ArXiv, abs/2309.02427.</p>
<p>Joshua B. Tenenbaum. 2018. Building machines that learn and think like people. In Adaptive Agents and Multi-Agent Systems.</p>
<p>Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. 2024. Eyes wide shut? exploring the visual shortcomings of multimodal llms. ArXiv, abs/2401.06209.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models.</p>
<p>Hugo Touvron, Louis Martin, Kevin R. Stone, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288.</p>
<p>Taylor W. Webb, Keith J. Holyoak, and Hongjing Lu. 2022. Emergent analogical reasoning in large language models. Nature Human Behaviour, 7:15261541 .</p>
<p>Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2023. The dawn of lmms: Preliminary explorations with gpt-4v(ision). ArXiv, abs/2309.17421.</p>
<p>Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502.</p>
<p>Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. 2019. Raven: A dataset for relational and analogical visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Puzzle</th>
<th style="text-align: center;">Multimodal</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Category</td>
<td style="text-align: center;">Templates</td>
<td style="text-align: center;">Instances</td>
</tr>
<tr>
<td style="text-align: left;">Numbers</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Colors</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Shapes</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Size</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Numbers \&amp; Shapes</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Numbers \&amp; Colors</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Numbers \&amp; Size</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Shapes \&amp; Colors</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Shapes \&amp; Size</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Colors \&amp; Size</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2000</td>
</tr>
</tbody>
</table>
<p>Table 3: Dataset statistics of PUZZLEVQA.</p>
<h2>A Appendix</h2>
<h2>A. 1 Multiple-Choice Format Details</h2>
<p>To generate multiple choice options for numeric puzzles, we use heuristics based on the range of the number. For example, if the number is less than 10 , then we sample from the range 1 to 9 . If the number if less than 100 , we sample from the range 1 to 99 , and so on. For discrete option choices, we sample from the possible objects in the image, such as the list of predefined colors or sizes or shapes.</p>
<h2>A. 2 Dataset Details</h2>
<p>We report the dataset statistics of PUZZLEVQA in Table 3.</p>
<h2>A. 3 Prompt Examples</h2>
<p>We show examples of the textual prompts in Figure 7. Note that the prompt examples correspond to the image and puzzle in Figure 1. We use a consistent prompt format across all abstract puzzles.</p>
<h2>A. 4 Code Implementation Example</h2>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">ImageDraw</span><span class="p">,</span> <span class="n">ImageFont</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ColorHexagonPattern</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">colors</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">blue</span><span class="o">=</span><span class="s2">&quot;#f7a8dc&quot;</span><span class="p">,</span>
        <span class="n">green</span><span class="o">=</span><span class="s2">&quot;#81c4fd&quot;</span><span class="p">,</span>
        <span class="n">yellow</span><span class="o">=</span><span class="s2">&quot;#ffd966&quot;</span><span class="p">,</span>
        <span class="n">red</span><span class="o">=</span><span class="s2">&quot;#e86666&quot;</span><span class="p">,</span>
        <span class="n">purple</span><span class="o">=</span><span class="s2">&quot;#8efcc3&quot;</span><span class="p">,</span>
        <span class="n">orange</span><span class="o">=</span><span class="s2">&quot;#f8b26b&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">image_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span>
    <span class="n">scale_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">path_font</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;fonts/OpenSans-Medium.ttf&quot;</span>
    <span class="nd">@staticmethod</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">get_centroid</span><span class="p">(</span><span class="nl">points</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">Tuple[float, float</span><span class="o">]</span><span class="err">]</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span>
<span class="w">    </span><span class="o">~=</span><span class="w"> </span><span class="n">Tuple</span><span class="o">[</span><span class="n">float, float</span><span class="o">]</span><span class="p">;</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sue</span><span class="o">[</span><span class="n">p[0</span><span class="o">]</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">points</span><span class="err">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>
<span class="w">    </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sue</span><span class="o">[</span><span class="n">p[1</span><span class="o">]</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">points</span><span class="err">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span>
<span class="n">def</span><span class="w"> </span><span class="n">sample_colors</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Tuple</span><span class="o">[</span><span class="n">List[str</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="err">]:</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="k">True</span><span class="err">:</span>
<span class="w">        </span><span class="k">names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">list</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">colors</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="ss">&quot;orange&quot;</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">names</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="ss">&quot;yellow&quot;</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">names</span><span class="err">:</span>
<span class="w">            </span><span class="k">continue</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">Hard</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">distinguish</span>
<span class="w">            </span><span class="k">names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">names</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="k">names</span>
<span class="w">            </span><span class="n">colors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">self.colors[n</span><span class="o">]</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">names</span><span class="err">]</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="k">names</span><span class="p">,</span><span class="w"> </span><span class="n">colors</span>
<span class="n">def</span><span class="w"> </span><span class="n">make_sample</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="k">Set</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">size</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="nc">image</span>
<span class="w">    </span><span class="k">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">image_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">scale_factor</span>
<span class="w">    </span><span class="nc">image</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Image</span><span class="p">.</span><span class="k">new</span><span class="p">(</span><span class="ss">&quot;RGB&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">size</span><span class="o">=</span><span class="p">(</span><span class="k">size</span><span class="p">,</span><span class="w"> </span><span class="k">size</span><span class="p">),</span><span class="w"> </span><span class="n">color</span><span class="o">=</span><span class="ss">&quot;</span>
<span class="ss">        ~= white&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">draw</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ImageDraw</span><span class="p">.</span><span class="n">Draw</span><span class="p">(</span><span class="nc">image</span><span class="p">)</span>
<span class="w">    </span><span class="n">center</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">size</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Hexagon</span><span class="w"> </span><span class="n">properties</span>
<span class="w">    </span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">size</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">Length</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">side</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span>
<span class="w">        </span><span class="o">~=</span><span class="w"> </span><span class="n">hexagon</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">triangles</span>
<span class="w">    </span><span class="n">triangle_height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">length</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">vertices</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">hexagon</span>
<span class="w">    </span><span class="n">hexagon</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span>
<span class="n">        (center + length / 2, center - triangle_height),</span>
<span class="n">        (center - length / 2, center - triangle_height),</span>
<span class="n">        (center - length, center),</span>
<span class="n">        (center - length / 2, center + triangle_height),</span>
<span class="n">        (center + length / 2, center + triangle_height),</span>
<span class="n">        (center + length, center),</span>
<span class="n">    </span><span class="o">]</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Colors</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">triangles</span>
<span class="w">    </span><span class="k">names</span><span class="p">,</span><span class="w"> </span><span class="n">colors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">sample_colors</span><span class="p">()</span>
<span class="w">    </span><span class="n">i_answer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">colors</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="w">    </span><span class="n">answer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">names</span><span class="o">[</span><span class="n">i_answer</span><span class="o">]</span>
<span class="w">    </span><span class="n">colors</span><span class="o">[</span><span class="n">i_answer</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ss">&quot;#eeeeee&quot;</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">Grey</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Draw</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">hexagon</span><span class="w"> </span><span class="n">made</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">six</span><span class="w"> </span><span class="n">triangles</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">range</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">Coordinates</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">triangle</span><span class="w"> </span><span class="n">vertices</span>
<span class="w">        </span><span class="n">triangle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">hexagon[i</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">hexagon</span><span class="o">[</span><span class="n">(i + 1) \% 6</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="err">{</span>
<span class="w">            </span><span class="o">~=</span><span class="w"> </span><span class="n">center</span><span class="p">,</span><span class="w"> </span><span class="n">center</span><span class="p">)</span><span class="err">]</span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">Draw</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">triangles</span>
<span class="w">            </span><span class="n">draw</span><span class="p">.</span><span class="n">polygon</span><span class="p">(</span><span class="n">triangle</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="n">colors</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="p">)</span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">Draw</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">outline</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">custom</span><span class="w"> </span><span class="n">width</span>
<span class="w">            </span><span class="n">points</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">[</span><span class="n">hexagon[i</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="n">hexagon</span><span class="o">[</span><span class="n">(i + 1) \% 6</span><span class="o">]</span><span class="p">,</span><span class="w"> </span><span class="err">{</span>
<span class="w">                </span><span class="o">~=</span><span class="w"> </span><span class="n">center</span><span class="p">,</span><span class="w"> </span><span class="n">center</span><span class="p">),</span><span class="w"> </span><span class="n">hexagon</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="err">]</span>
<span class="w">            </span><span class="n">draw</span><span class="p">.</span><span class="n">line</span><span class="p">(</span><span class="n">points</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="ss">&quot;black&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="o">=</span><span class="n">self</span><span class="p">.</span>
<span class="w">                </span><span class="o">~=</span><span class="w"> </span><span class="n">scale_factor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span>
<span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">Draw</span><span class="w"> </span><span class="ss">&quot;?&quot;</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">missing</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">part</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="nl">i_answer</span><span class="p">:</span>
<span class="w">            </span><span class="n">draw</span><span class="p">.</span><span class="n">test</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">get_centroid</span><span class="p">(</span><span class="n">triangle</span><span class="p">),</span>
<span class="w">                </span><span class="n">test</span><span class="o">=</span><span class="ss">&quot;&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">font</span><span class="o">=</span><span class="n">ImageFont</span><span class="p">.</span><span class="n">truetype</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">path_font</span><span class="p">,</span>
<span class="w">                </span><span class="o">~=</span><span class="w"> </span><span class="k">size</span><span class="o">=</span><span class="k">size</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="mi">10</span><span class="p">),</span>
<span class="w">                </span><span class="n">anchor</span><span class="o">=</span><span class="ss">&quot;on&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">fill</span><span class="o">=</span><span class="ss">&quot;black&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="p">)</span>
<span class="w">    </span><span class="k">names</span><span class="o">[</span><span class="n">i_answer</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ss">&quot;?&quot;</span>
<span class="w">    </span><span class="n">instances</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sorted</span><span class="p">(</span><span class="k">set</span><span class="p">(</span><span class="n">m</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="k">names</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="ow">in</span>
<span class="w">        </span><span class="o">~=</span><span class="w"> </span><span class="o">[</span><span class="n">answer, &quot;?&quot;</span><span class="o">]</span><span class="p">))</span>
<span class="w">    </span><span class="nc">image</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">image</span><span class="p">.</span><span class="n">resize</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">image_size</span><span class="p">,</span><span class="w"> </span><span class="n">self</span><span class="p">.</span>
<span class="w">        </span><span class="o">~=</span><span class="w"> </span><span class="n">image_size</span><span class="p">),</span><span class="w"> </span><span class="nc">Image</span><span class="p">.</span><span class="n">LANCZOS</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">(</span>
<span class="w">        </span><span class="n">dict</span><span class="p">(</span>
<span class="w">            </span><span class="n">question</span><span class="o">=</span><span class="ss">&quot;What is the missing color of the</span>
<span class="ss">            ~= part denoted with a question mark?&quot;</span>
<span class="w">            </span><span class="o">~=</span>
<span class="w">            </span><span class="n">answer</span><span class="o">=</span><span class="n">answer</span><span class="p">,</span>
<span class="w">            </span><span class="n">options</span><span class="o">=</span><span class="n">sample_options</span><span class="p">(</span><span class="n">answer</span><span class="p">,</span><span class="w"> </span><span class="n">options</span><span class="o">=</span><span class="n">list</span><span class="p">(</span>
<span class="w">                </span><span class="o">~=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">colors</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
<span class="w">            </span><span class="n">captionÑ‡Ñ‘Ñ‚There</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">hexagon</span><span class="w"> </span><span class="n">split</span><span class="w"> </span><span class="k">into</span><span class="w"> </span><span class="n">six</span>
<span class="w">                </span><span class="o">~=</span><span class="w"> </span><span class="n">parts</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">colors</span><span class="w"> </span><span class="p">(</span><span class="k">names</span><span class="p">)</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">an</span>
<span class="w">                    </span><span class="o">~=</span><span class="w"> </span><span class="n">anti</span><span class="o">-</span><span class="n">clockwise</span><span class="w"> </span><span class="k">order</span><span class="p">.</span><span class="ss">&quot;,</span>
<span class="ss">            explanationÑ‡Ñ‘Ñ‚We observe that a [instances</span>
<span class="ss">                ~= [0]) part is opposite another [</span>
<span class="ss">                ~= instances[0]) part, and a {</span>
<span class="ss">                ~= instances[1]) part is opposite</span>
<span class="ss">                ~= another [instances[1]) part. Thus,</span>
<span class="ss">                ~= the pattern is that the colors in</span>
<span class="ss">                ~= opposite parts are the same.&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="n">deduction</span><span class="o">=</span><span class="n">f</span><span class="ss">&quot;Based on the pattern that</span>
<span class="ss">                ~= spatially opposite parts have the</span>
<span class="ss">                ~= same color, the missing color of</span>
<span class="ss">                ~= the part which is opposite a {</span>
<span class="ss">                ~= answer) part should be (answer).&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="p">),</span>
<span class="w">        </span><span class="nc">image</span><span class="p">,</span>
<span class="w">    </span><span class="p">)</span>
</code></pre></div>

<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Textual prompt examples for eliciting reasoning steps from large multimodal models.</p>
<h2>A. 5 Dataset Size Analysis</h2>
<p>Regarding the dataset size and diversity, we set the number of generated instances for each puzzle to 100 , to reduce experimental variance and maintain a reasonable evaluation cost. Hence, the current dataset size is 2000 samples ( 20 templates with 100 instances each). As there are two templates per puzzle category, this means there are 200 test samples for each puzzle category. To observe the impact of the number of test samples, we evaluate the models on three different data settings as shown in Table 4: 50, 100, and 200 test samples in per puzzle, which correspond to 1000, 2000, and 4000 total samples respectively. In general, we observe some variations in the average score for single-concept puzzles, but it does not significantly affect the comparison of performance between different models. Hence, we believe that the chosen dataset size is large enough to be relatively robust to experimental variance. To investigate the multi-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">1000</th>
<th style="text-align: left;">2000</th>
<th style="text-align: left;">4000</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Gemini Pro Avg. Score</td>
<td style="text-align: left;">29.2</td>
<td style="text-align: left;">34.5</td>
<td style="text-align: left;">32.2</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4V Avg. Score</td>
<td style="text-align: left;">48.8</td>
<td style="text-align: left;">46.4</td>
<td style="text-align: left;">48.4</td>
</tr>
</tbody>
</table>
<p>Table 4: Analysis of model performance with respect to number of testing data samples.
modal reasoning capabilities across diverse abstract scenarios, we construct the puzzles based on four fundamental concepts: numbers, colors, shapes, and size. Furthermore, to evaluate how well the models can relate to multiple concepts, we design both single-concept and dual-concept puzzles, and the taxonomy of diverse puzzles is shown in Figure 3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Numbers</th>
<th style="text-align: center;">Colors</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Shapes</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLaVA-13B</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">$32.6(+5.1)$</td>
</tr>
<tr>
<td style="text-align: left;">Gemini Pro</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">$32.1(-2.4)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4V</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">$43.6(-2.8)$</td>
</tr>
</tbody>
</table>
<p>Table 5: Results of direct prompting and change in average performance compared to CoT prompting.</p>
<h1>A. 6 Comparison of CoT and Direct Prompting</h1>
<p>To investigate the effect of direct prompting without chain of thought, we evaluated the models on our main setting as shown in Table 1 and 2. The results are shown in Table 5 for single-concept puzzles and indicate that direct prompting is less effective for Gemini Pro and GPT-4V models, compared to CoT prompting in Table 1. This may be due to differences in training data and alignment methods between the models.</p>
<h2>B Qualitative Analysis</h2>
<p>To illustrate the reasoning bottlenecks of GPT-4V, we include two case study samples in Figure 8. For instance, the sample on the left is from the size \&amp; shapes category of puzzles, for which the model under-performed the random baseline. For visual perception, we observe that the model presents severe limitations, as it is unable to recognize simple polygon shapes and hallucinated additional shapes which are not in the image. Regarding inductive reasoning, we observe that the model was able to recognize the sizes of the different objects, but did not recognize the correct pattern that the circles directly adjacent to the center should be small in size. Hence, we believe that there is ample area for improvement for abstract reasoning ability in large multimodal models.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Case study on two sample predictions from GPT-4V. The example on the left shows visual perception failures and the example on the right shows the faulty inductive reasoning of the model which proposed a spurious pattern in the image.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://www. anthropic.com/news/ claude-3-family&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-672 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-672</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-672</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-8dc8f3e0127adc6985d4695e9b69d04717b2fde8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8dc8f3e0127adc6985d4695e9b69d04717b2fde8" target="_blank">Sanity Checks for Saliency Maps</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> It is shown that some existing saliency methods are independent both of the model and of the data generating process, and methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model.</p>
                <p><strong>Paper Abstract:</strong> Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e672.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e672.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sanity Randomization Tests</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-parameter and Data-label Randomization Sanity Tests</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two concrete, easy-to-implement randomization-based tests (model-parameter randomization and data-label randomization) that detect whether saliency/explanation methods are sensitive to learned model parameters or to the relationship between inputs and labels; used to reveal mismatches between qualitative descriptions/visual impressions and actual implementation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>saliency explanation evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An experimental evaluation framework for saliency/explanation methods that compares explanation outputs under (a) original trained models and (b) deliberately randomized conditions: (i) re-initialized model parameters (cascading and independent layer randomization) and (ii) models trained on randomly permuted labels.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper claims / visual/qualitative description of explanation fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>open-source saliency method implementations (library code / experiment scripts)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>misleading visual description vs implementation sensitivity (incomplete specification of method faithfulness)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Visual appeal or natural-language descriptions (and common usage narratives) present saliency maps as explanations of model behavior; however, the implementations of several popular saliency methods produce maps that remain nearly unchanged when model weights are randomized or when labels are permuted, indicating the maps are not faithful to the trained model or the data-label relationship they are claimed to explain.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>explanation-method behavior (post-hoc attribution component of experimental pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>experimental randomization tests executed by the authors (model-parameter randomization: cascading and independent layer re-initialization; data randomization: training models on permuted labels to >95% training accuracy), with visual inspection and quantitative comparison</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Quantified similarity between original and randomized-explanation maps using Spearman rank correlation (with absolute values and without), Structural Similarity Index (SSIM), and Pearson correlation of Histograms of Oriented Gradients (HOGs); additional calibration against random masks reported in appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Methods that fail these sanity checks cannot be relied on for model-sensitive tasks such as debugging, discovering learned relationships, or detecting outliers; specifically the paper reports that some methods' similarity measures remain high under randomization (visual maps appear plausible despite being insensitive), while other methods' similarity drops (e.g., Spearman without absolute value goes to ~0 when top layers randomized).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in a subset of widely-used saliency methods tested: the paper reports that 'some widely deployed saliency methods are independent of both the data and the model parameters'; concretely, Gradients & GradCAM passed the tests while Guided BackProp & Guided GradCAM were invariant to higher-layer parameters (i.e., failed) across multiple datasets and architectures in the experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Mismatch arises from (1) implicit assumptions and incomplete claims in natural-language descriptions and visual demonstrations, (2) architectural priors and implementation choices (e.g., modified backprop rules, element-wise input×gradient approximations) that make outputs dominated by input structure or architecture rather than learned parameters or labels, and (3) over-reliance on qualitative visual evaluation in documentation and papers.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Adopt the proposed sanity checks (model-parameter and data-label randomization) as standard pre-deployment validation for explanation methods; release and run reproducible code to compare explanations under randomized conditions; inspect quantitative similarity metrics rather than relying solely on visual assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Demonstrated effective in the paper: the tests distinguished methods that are sensitive (Gradients, GradCAM) from those largely insensitive (Guided BackProp variants); calibration experiments and plots (Spearman, SSIM, HOG) show clear differences under randomization. No single numeric 'percentage improvement' metric provided beyond the plotted similarity reductions (e.g., Spearman correlation without absolute values trending to zero under randomization).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / explainable AI</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sanity Checks for Saliency Maps', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e672.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e672.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GuidedBackprop Invariance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Guided Backpropagation invariance to higher-layer parameters (implementation-driven partial input recovery)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An observed discrepancy where Guided Backpropagation (and Guided GradCAM) implementations produce saliency maps largely invariant to randomization of higher-layer weights, meaning the method often highlights input structure irrespective of learned classifier behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>saliency method implementation (Guided BackProp / Guided GradCAM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Backpropagation-based saliency implementation that modifies the backward pass for ReLU units (zeroing negative gradients during backprop) and is often combined (Guided GradCAM) with GradCAM for pixel-level maps.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>method descriptions / algorithmic modification described in prior literature and used in documentation and demos</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>library code / experiment scripts (open-source implementations referenced from Google PAIR and TensorFlow ecosystem)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different algorithm variant / implicit algorithmic behavior causing partial input recovery (implementation produces behavior not aligned with claimed model-sensitivity)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although Guided Backprop is described as a visualization of class-specific gradients, its implementation (modified ReLU backprop rule) leads to maps that remain visually and quantitatively similar after randomizing higher network layers, indicating partial recovery of input structure rather than reflecting the model's learned parameters; earlier versions of the authors' experiments even contained a bug that overstated invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>backpropagation-based attribution algorithm (implementation detail in gradient propagation rules)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>model-parameter randomization experiments (cascading and independent layer re-initializations) comparing pre-trained and partially/fully randomized networks, plus visual comparisons and similarity metrics (Spearman, SSIM, HOG); an external contributor (Leon Sixt) also reported a bug in Guided Backprop experiments in an earlier version.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Quantified invariance with Spearman rank correlations (absolute and diverging), SSIM, and HOG similarity across layers randomized; authors report Guided Backprop masks remain visually and quantitatively similar until lower-layer weights are randomized (plots and figures show slow/limited change versus other methods).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Leads to misleading attributions: Guided Backprop masks can appear plausible on visual inspection even for models with randomized weights or random labels, meaning using these maps for model debugging or explaining learned relations can produce false confidence and incorrect conclusions; the paper marks Guided BackProp as failing their sanity checks for higher-layer sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed consistently across multiple architectures and datasets tested (Inception v3 on ImageNet, CNN/MLP on MNIST/FashionMNIST); specifically, Guided BackProp and Guided GradCAM showed invariance to higher-layer randomization in the experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Algorithmic choice in the implementation (zeroing negative gradients during backward pass) that causes partial input reconstruction and strong dependence on input structure and lower-layer filters rather than higher-layer learned weights; also compounded by reliance on visual examples in prior descriptions and demos.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use the model-parameter randomization sanity check to detect this invariance before applying Guided BackProp for model- or data-sensitive tasks; prefer methods that remain sensitive (e.g., plain gradients, GradCAM) for model-debugging or complement Guided BackProp with tests that verify model dependence; fix implementation bugs and document limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective at surfacing the invariance (the randomization tests reveal stability of Guided BackProp under higher-layer randomization); the paper corrected an earlier experimental bug after external report, showing that careful implementation review plus randomization tests improves trustworthiness. No additional quantitative mitigation-of-harm metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sanity Checks for Saliency Maps', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e672.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e672.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Input×Gradient Dominance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dominance of input structure in input-times-gradient style explanations (Integrated Gradients / gradient⋅input approximations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A documented misalignment where explanation methods that effectively return element-wise products of input and gradient (e.g., Integrated Gradients, Gradient⊙Input, some variants of DeepLift/ε-LRP) can produce maps dominated by input structure and therefore remain similar even when gradients are randomized, causing explanations to reflect the input rather than learned model relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>explanation implementations using input×gradient approximations</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Attribution methods that compute or approximate the element-wise product of the input and gradients (or integrated gradients), often motivated in documentation as correcting gradient saturation but implemented in ways that couple strongly to input sparsity/structure.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>method claims and documentation describing the method as more robust to saturation and better capturing feature importance</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>library code / algorithm implementation (IntegratedGradients, Gradient×Input, DeepLift/ε-LRP approximations)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous description / incomplete specification leading to input-dominated outputs (algorithmic behavior differs from expectation of model-sensitivity)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although described as addressing gradient saturation to better reflect feature importance, implementations that compute input×gradient often return maps where the input signal dominates (especially for sparse inputs like MNIST), so that even large changes in gradients (via randomization) yield maps that remain highly similar due to multiplication with the fixed input; thus the implementation behavior can contradict the expectation that these maps reveal model-learned sensitivities.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>explanation computation step (attribution formula combining input and gradient)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>controlled experiment in which random gradient vectors (truncated normal or uniform) were element-wise multiplied by fixed inputs and the resulting maps compared using the paper's similarity metrics; also observed empirically by applying data-label randomization and model-parameter randomization tests.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Similarity metrics (Spearman rank correlations, SSIM, HOG Pearson) between input×random-gradient outputs show high similarity as noise increases; the paper presents calibration plots and an input×random-gradient experiment (Figure 19) demonstrating persistence of input dominance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can cause practitioners to interpret masks as model-faithful when they are mostly reproductions of the input (or edges) — leading to incorrect conclusions about model behavior, failed debugging and erroneous trust; the paper highlights that integrated gradients and gradient×input can visually appear similar between trained and randomized models despite sign changes.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Effect observed across multiple datasets (MNIST, FashionMNIST, ImageNet) and with multiple methods that approximate input×gradient (Integrated Gradients, DeepLift/ε-LRP equivalents per Ancona et al.); particularly pronounced for sparse inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Mathematical structure of the attribution (multiplying input by a varying gradient) makes the fixed input dominate the product in magnitude/structure, especially when inputs are sparse or have strong local structure, combined with insufficiently precise claims in documentation about what the method actually isolates.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Run the data and model randomization sanity checks to verify sensitivity; inspect sign-sensitive metrics (Spearman without absolute value) and not only absolute-valued visualizations; complement input×gradient methods with gradient-only methods and other diagnostics; clearly document limitations of input×gradient approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Demonstrated diagnostic utility: randomization tests and sign-aware metrics reveal when input structure dominates; authors show Spearman without absolute value quickly degrades under randomization, exposing lack of faithfulness even when absolute-valued visualizations look similar.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / explainability</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sanity Checks for Saliency Maps', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The (un)reliability of saliency methods <em>(Rating: 2)</em></li>
                <li>Interpretation of neural networks is fragile <em>(Rating: 2)</em></li>
                <li>A theoretical explanation for perplexing behaviors of backpropagation-based visualizations <em>(Rating: 2)</em></li>
                <li>Understanding deep learning requires rethinking generalization <em>(Rating: 2)</em></li>
                <li>Salient deconvolutional networks <em>(Rating: 1)</em></li>
                <li>A unified view of gradient-based attribution methods for deep neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-672",
    "paper_id": "paper-8dc8f3e0127adc6985d4695e9b69d04717b2fde8",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Sanity Randomization Tests",
            "name_full": "Model-parameter and Data-label Randomization Sanity Tests",
            "brief_description": "Two concrete, easy-to-implement randomization-based tests (model-parameter randomization and data-label randomization) that detect whether saliency/explanation methods are sensitive to learned model parameters or to the relationship between inputs and labels; used to reveal mismatches between qualitative descriptions/visual impressions and actual implementation behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "saliency explanation evaluation framework",
            "system_description": "An experimental evaluation framework for saliency/explanation methods that compares explanation outputs under (a) original trained models and (b) deliberately randomized conditions: (i) re-initialized model parameters (cascading and independent layer randomization) and (ii) models trained on randomly permuted labels.",
            "nl_description_type": "research paper claims / visual/qualitative description of explanation fidelity",
            "code_implementation_type": "open-source saliency method implementations (library code / experiment scripts)",
            "gap_type": "misleading visual description vs implementation sensitivity (incomplete specification of method faithfulness)",
            "gap_description": "Visual appeal or natural-language descriptions (and common usage narratives) present saliency maps as explanations of model behavior; however, the implementations of several popular saliency methods produce maps that remain nearly unchanged when model weights are randomized or when labels are permuted, indicating the maps are not faithful to the trained model or the data-label relationship they are claimed to explain.",
            "gap_location": "explanation-method behavior (post-hoc attribution component of experimental pipeline)",
            "detection_method": "experimental randomization tests executed by the authors (model-parameter randomization: cascading and independent layer re-initialization; data randomization: training models on permuted labels to &gt;95% training accuracy), with visual inspection and quantitative comparison",
            "measurement_method": "Quantified similarity between original and randomized-explanation maps using Spearman rank correlation (with absolute values and without), Structural Similarity Index (SSIM), and Pearson correlation of Histograms of Oriented Gradients (HOGs); additional calibration against random masks reported in appendix.",
            "impact_on_results": "Methods that fail these sanity checks cannot be relied on for model-sensitive tasks such as debugging, discovering learned relationships, or detecting outliers; specifically the paper reports that some methods' similarity measures remain high under randomization (visual maps appear plausible despite being insensitive), while other methods' similarity drops (e.g., Spearman without absolute value goes to ~0 when top layers randomized).",
            "frequency_or_prevalence": "Observed in a subset of widely-used saliency methods tested: the paper reports that 'some widely deployed saliency methods are independent of both the data and the model parameters'; concretely, Gradients & GradCAM passed the tests while Guided BackProp & Guided GradCAM were invariant to higher-layer parameters (i.e., failed) across multiple datasets and architectures in the experiments reported.",
            "root_cause": "Mismatch arises from (1) implicit assumptions and incomplete claims in natural-language descriptions and visual demonstrations, (2) architectural priors and implementation choices (e.g., modified backprop rules, element-wise input×gradient approximations) that make outputs dominated by input structure or architecture rather than learned parameters or labels, and (3) over-reliance on qualitative visual evaluation in documentation and papers.",
            "mitigation_approach": "Adopt the proposed sanity checks (model-parameter and data-label randomization) as standard pre-deployment validation for explanation methods; release and run reproducible code to compare explanations under randomized conditions; inspect quantitative similarity metrics rather than relying solely on visual assessment.",
            "mitigation_effectiveness": "Demonstrated effective in the paper: the tests distinguished methods that are sensitive (Gradients, GradCAM) from those largely insensitive (Guided BackProp variants); calibration experiments and plots (Spearman, SSIM, HOG) show clear differences under randomization. No single numeric 'percentage improvement' metric provided beyond the plotted similarity reductions (e.g., Spearman correlation without absolute values trending to zero under randomization).",
            "domain_or_field": "deep learning / explainable AI",
            "reproducibility_impact": true,
            "uuid": "e672.0",
            "source_info": {
                "paper_title": "Sanity Checks for Saliency Maps",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "GuidedBackprop Invariance",
            "name_full": "Guided Backpropagation invariance to higher-layer parameters (implementation-driven partial input recovery)",
            "brief_description": "An observed discrepancy where Guided Backpropagation (and Guided GradCAM) implementations produce saliency maps largely invariant to randomization of higher-layer weights, meaning the method often highlights input structure irrespective of learned classifier behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "saliency method implementation (Guided BackProp / Guided GradCAM)",
            "system_description": "Backpropagation-based saliency implementation that modifies the backward pass for ReLU units (zeroing negative gradients during backprop) and is often combined (Guided GradCAM) with GradCAM for pixel-level maps.",
            "nl_description_type": "method descriptions / algorithmic modification described in prior literature and used in documentation and demos",
            "code_implementation_type": "library code / experiment scripts (open-source implementations referenced from Google PAIR and TensorFlow ecosystem)",
            "gap_type": "different algorithm variant / implicit algorithmic behavior causing partial input recovery (implementation produces behavior not aligned with claimed model-sensitivity)",
            "gap_description": "Although Guided Backprop is described as a visualization of class-specific gradients, its implementation (modified ReLU backprop rule) leads to maps that remain visually and quantitatively similar after randomizing higher network layers, indicating partial recovery of input structure rather than reflecting the model's learned parameters; earlier versions of the authors' experiments even contained a bug that overstated invariance.",
            "gap_location": "backpropagation-based attribution algorithm (implementation detail in gradient propagation rules)",
            "detection_method": "model-parameter randomization experiments (cascading and independent layer re-initializations) comparing pre-trained and partially/fully randomized networks, plus visual comparisons and similarity metrics (Spearman, SSIM, HOG); an external contributor (Leon Sixt) also reported a bug in Guided Backprop experiments in an earlier version.",
            "measurement_method": "Quantified invariance with Spearman rank correlations (absolute and diverging), SSIM, and HOG similarity across layers randomized; authors report Guided Backprop masks remain visually and quantitatively similar until lower-layer weights are randomized (plots and figures show slow/limited change versus other methods).",
            "impact_on_results": "Leads to misleading attributions: Guided Backprop masks can appear plausible on visual inspection even for models with randomized weights or random labels, meaning using these maps for model debugging or explaining learned relations can produce false confidence and incorrect conclusions; the paper marks Guided BackProp as failing their sanity checks for higher-layer sensitivity.",
            "frequency_or_prevalence": "Observed consistently across multiple architectures and datasets tested (Inception v3 on ImageNet, CNN/MLP on MNIST/FashionMNIST); specifically, Guided BackProp and Guided GradCAM showed invariance to higher-layer randomization in the experiments reported.",
            "root_cause": "Algorithmic choice in the implementation (zeroing negative gradients during backward pass) that causes partial input reconstruction and strong dependence on input structure and lower-layer filters rather than higher-layer learned weights; also compounded by reliance on visual examples in prior descriptions and demos.",
            "mitigation_approach": "Use the model-parameter randomization sanity check to detect this invariance before applying Guided BackProp for model- or data-sensitive tasks; prefer methods that remain sensitive (e.g., plain gradients, GradCAM) for model-debugging or complement Guided BackProp with tests that verify model dependence; fix implementation bugs and document limitations.",
            "mitigation_effectiveness": "Effective at surfacing the invariance (the randomization tests reveal stability of Guided BackProp under higher-layer randomization); the paper corrected an earlier experimental bug after external report, showing that careful implementation review plus randomization tests improves trustworthiness. No additional quantitative mitigation-of-harm metric provided.",
            "domain_or_field": "deep learning / interpretability",
            "reproducibility_impact": true,
            "uuid": "e672.1",
            "source_info": {
                "paper_title": "Sanity Checks for Saliency Maps",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Input×Gradient Dominance",
            "name_full": "Dominance of input structure in input-times-gradient style explanations (Integrated Gradients / gradient⋅input approximations)",
            "brief_description": "A documented misalignment where explanation methods that effectively return element-wise products of input and gradient (e.g., Integrated Gradients, Gradient⊙Input, some variants of DeepLift/ε-LRP) can produce maps dominated by input structure and therefore remain similar even when gradients are randomized, causing explanations to reflect the input rather than learned model relationships.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "explanation implementations using input×gradient approximations",
            "system_description": "Attribution methods that compute or approximate the element-wise product of the input and gradients (or integrated gradients), often motivated in documentation as correcting gradient saturation but implemented in ways that couple strongly to input sparsity/structure.",
            "nl_description_type": "method claims and documentation describing the method as more robust to saturation and better capturing feature importance",
            "code_implementation_type": "library code / algorithm implementation (IntegratedGradients, Gradient×Input, DeepLift/ε-LRP approximations)",
            "gap_type": "ambiguous description / incomplete specification leading to input-dominated outputs (algorithmic behavior differs from expectation of model-sensitivity)",
            "gap_description": "Although described as addressing gradient saturation to better reflect feature importance, implementations that compute input×gradient often return maps where the input signal dominates (especially for sparse inputs like MNIST), so that even large changes in gradients (via randomization) yield maps that remain highly similar due to multiplication with the fixed input; thus the implementation behavior can contradict the expectation that these maps reveal model-learned sensitivities.",
            "gap_location": "explanation computation step (attribution formula combining input and gradient)",
            "detection_method": "controlled experiment in which random gradient vectors (truncated normal or uniform) were element-wise multiplied by fixed inputs and the resulting maps compared using the paper's similarity metrics; also observed empirically by applying data-label randomization and model-parameter randomization tests.",
            "measurement_method": "Similarity metrics (Spearman rank correlations, SSIM, HOG Pearson) between input×random-gradient outputs show high similarity as noise increases; the paper presents calibration plots and an input×random-gradient experiment (Figure 19) demonstrating persistence of input dominance.",
            "impact_on_results": "Can cause practitioners to interpret masks as model-faithful when they are mostly reproductions of the input (or edges) — leading to incorrect conclusions about model behavior, failed debugging and erroneous trust; the paper highlights that integrated gradients and gradient×input can visually appear similar between trained and randomized models despite sign changes.",
            "frequency_or_prevalence": "Effect observed across multiple datasets (MNIST, FashionMNIST, ImageNet) and with multiple methods that approximate input×gradient (Integrated Gradients, DeepLift/ε-LRP equivalents per Ancona et al.); particularly pronounced for sparse inputs.",
            "root_cause": "Mathematical structure of the attribution (multiplying input by a varying gradient) makes the fixed input dominate the product in magnitude/structure, especially when inputs are sparse or have strong local structure, combined with insufficiently precise claims in documentation about what the method actually isolates.",
            "mitigation_approach": "Run the data and model randomization sanity checks to verify sensitivity; inspect sign-sensitive metrics (Spearman without absolute value) and not only absolute-valued visualizations; complement input×gradient methods with gradient-only methods and other diagnostics; clearly document limitations of input×gradient approximations.",
            "mitigation_effectiveness": "Demonstrated diagnostic utility: randomization tests and sign-aware metrics reveal when input structure dominates; authors show Spearman without absolute value quickly degrades under randomization, exposing lack of faithfulness even when absolute-valued visualizations look similar.",
            "domain_or_field": "deep learning / explainability",
            "reproducibility_impact": true,
            "uuid": "e672.2",
            "source_info": {
                "paper_title": "Sanity Checks for Saliency Maps",
                "publication_date_yy_mm": "2018-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The (un)reliability of saliency methods",
            "rating": 2
        },
        {
            "paper_title": "Interpretation of neural networks is fragile",
            "rating": 2
        },
        {
            "paper_title": "A theoretical explanation for perplexing behaviors of backpropagation-based visualizations",
            "rating": 2
        },
        {
            "paper_title": "Understanding deep learning requires rethinking generalization",
            "rating": 2
        },
        {
            "paper_title": "Salient deconvolutional networks",
            "rating": 1
        },
        {
            "paper_title": "A unified view of gradient-based attribution methods for deep neural networks",
            "rating": 1
        }
    ],
    "cost": 0.012365000000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Sanity Checks for Saliency Maps</h1>
<p>Julius Adebayo, ${ }^{\dagger}$ Justin Gilmer ${ }^{\ddagger}$, Michael Muelly ${ }^{\S}$, Ian Goodfellow ${ }^{\S}$, Moritz Hardt ${ }^{\S}$, Been Kim ${ }^{\S}$ juliusad@mit.edu, {gilmer,muelly, goodfellow,mrtz, beenkim}@google.com<br>${ }^{\text { }}$ Google Brain<br>${ }^{\dagger}$ University of California Berkeley</p>
<h4>Abstract</h4>
<p>Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings ${ }^{2}$.</p>
<h2>1 Introduction</h2>
<p>As machine learning grows in complexity and impact, much hope rests on explanation methods as tools to elucidate important aspects of learned models [1, 2]. Explanations could potentially help satisfy regulatory requirements [3], help practitioners debug their model [4, 5], and perhaps, reveal bias or other unintended effects learned by a model [6, 7]. Saliency methods ${ }^{3}$ are an increasingly popular class of tools designed to highlight relevant features in an input, typically, an image. Despite much excitement, and significant recent contribution [8-21], the valuable effort of explaining machine learning models faces a methodological challenge: the difficulty of assessing the scope and quality of model explanations. A paucity of principled guidelines confound the practitioner when deciding between an abundance of competing methods.
We propose an actionable methodology based on randomization tests to evaluate the adequacy of explanation approaches. We instantiate our analysis on several saliency methods for image classification with neural networks; however, our methodology applies in generality to any explanation approach. Critically, our proposed randomization tests are easy to implement, and can help assess the suitability of an explanation method for a given task at hand.
In a broad experimental sweep, we apply our methodology to numerous existing saliency methods, model architectures, and data sets. To our surprise, some widely deployed saliency methods are independent of both the data the model was trained on, and the model parameters. Consequently,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Saliency maps for some common methods compared to an edge detector. Saliency masks for 3 different inputs for an Inception v3 model trained on ImageNet. We see that an edge detector produces outputs that are strikingly similar to the outputs of some saliency methods. In fact, edge detectors can also produce masks that highlight features which coincide with what appears to be relevant to a model's class prediction. Interestingly, we find that the methods that are most similar to an edge detector, i.e., Guided Backprop and its variants, show minimal sensitivity to our randomization tests.</p>
<p>These methods are incapable of assisting with tasks that depend on the model, such as debugging the model, or tasks that depend on the relationships between inputs and outputs present in the data.</p>
<p>To illustrate the point, Figure 1 compares the output of standard saliency methods with those of an edge detector. The edge detector does not depend on model or training data, and yet produces results that bear visual similarity with saliency maps. This goes to show that visual inspection is a poor guide in judging whether an explanation is sensitive to the underlying model and data.</p>
<p>Our methodology derives from the idea of a statistical randomization test, comparing the natural experiment with an artificially randomized experiment. We focus on two instantiations of our general framework: a <em>model parameter randomization test</em>, and a <em>data randomization test</em>.</p>
<p>The model parameter randomization test compares the output of a saliency method on a trained model with the output of the saliency method on a randomly initialized untrained network of the same architecture. If the saliency method depends on the learned parameters of the model, we should expect its output to differ substantially between the two cases. Should the outputs be similar, however, we can infer that the saliency map is insensitive to properties of the model, in this case, the model parameters. In particular, the output of the saliency map would not be helpful for tasks such as <em>model debugging</em> that inevitably depend on the model.</p>
<p>The data randomization test compares a given saliency method applied to a model trained on a labeled data set with the method applied to the same model architecture but trained on a copy of the data set in which we randomly permuted all labels. If a saliency method depends on the labeling of the data, we should again expect its outputs to differ significantly in the two cases. An insensitivity to the permuted labels, however, reveals that the method does not depend on the relationship between instances (e.g., images) and labels that exists in the original data.</p>
<p>Speaking more broadly, any explanation method admits a set of <em>invariances</em>, i.e., transformations of data and model that do not change the output of the method. If we discover an invariance that is incompatible with the requirements of the task at hand, we can safely reject the method. As such, our tests can be thought of as <em>sanity checks</em> to perform before deploying a method in practice.</p>
<h3>Our contributions</h3>
<ol>
<li>
<p>We propose two concrete, easy to implement tests for assessing the scope and quality of explanation methods: the model parameter randomization test, and the data randomization test. Both tests applies broadly to explanation methods.</p>
</li>
<li>
<p>We conduct extensive experiments with several explanation methods across data sets and model architectures, and find, consistently, that some of the methods tested are independent of both the model parameters and the labeling of the data that the model was trained on.</p>
</li>
<li>
<p>Of the methods tested, Gradients \&amp; GradCAM pass the sanity checks, while Guided BackProp \&amp; Guided GradCAM are invariant to higher layer parameters; hence, fail.</p>
</li>
<li>Consequently, our findings imply that the saliency methods that fail our proposed tests are incapable of supporting tasks that require explanations that are faithful to the model or the data generating process.</li>
<li>We interpret our findings through a series of analyses of linear models and a simple 1-layer convolutional sum-pooling architecture, as well as a comparison with edge detectors.</li>
</ol>
<h1>2 Methods and Related Work</h1>
<p>In our formal setup, an input is a vector $x \in \mathbb{R}^{d}$. A model describes a function $S: \mathbb{R}^{d} \rightarrow \mathbb{R}^{C}$, where $C$ is the number of classes in the classification problem. An explanation method provides an explanation map $E: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ that maps inputs to objects of the same shape.
We now briefly describe some of the explanation methods we examine. The supplementary materials contain an in-depth overview of these methods. Our goal is not to exhaustively evaluate all prior explanation methods, but rather to highlight how our methods apply to several cases of interest.
The gradient explanation for an input $x$ is $E_{\text {grad }}(x)=\frac{\partial S}{\partial x}$ [22, 23, 8]. The gradient quantifies how much a change in each input dimension would a change the predictions $S(x)$ in a small neighborhood around the input.
Gradient $\odot$ Input. Another form of explanation is the element-wise product of the input and the gradient, denoted $x \odot \frac{\partial S}{\partial x}$, which can address "gradient saturation", and reduce visual diffusion [13].
Integrated Gradients (IG) also addresses gradient saturation by summing over scaled versions of the input [14]. IG for an input $x$ is defined as $E_{\mathrm{IG}}(x)=(x-\bar{x}) \times \int_{0}^{1} \frac{\partial S(\bar{x}+\alpha(x-\bar{x}))}{\partial x} d \alpha$, where $\bar{x}$ is a "baseline input" that represents the absence of a feature in the original input $x$.
Guided Backpropagation (GBP) [9] builds on the "DeConvNet" explanation method [10] and corresponds to the gradient explanation where negative gradient entries are set to zero while backpropagating through a ReLU unit.
Guided GradCAM. Introduced by Selvaraju et al. [19], GradCAM explanations correspond to the gradient of the class score (logit) with respect to the feature map of the last convolutional unit of a DNN. For pixel level granularity GradCAM, can be combined with Guided Backpropagation through an element-wise product.
SmoothGrad (SG) [16] seeks to alleviate noise and visual diffusion [14, 13] for saliency maps by averaging over explanations of noisy copies of an input. For a given explanation map $E$, SmoothGrad is defined as $E_{\text {sg }}(x)=\frac{1}{N} \sum_{i=1}^{N} E\left(x+g_{i}\right)$, where noise vectors $g_{i} \sim \mathcal{N}\left(0, \sigma^{2}\right)$ are drawn i.i.d. from a normal distribution.</p>
<h3>2.1 Related Work</h3>
<p>Other Methods \&amp; Similarities. Aside gradient-based approaches, other methods 'learn' an explanation per sample for a model [20, 17, 12, 15, 11, 21]. More recently, Ancona et al. [24] showed that for ReLU networks (with zero baseline and no biases) the $\epsilon$-LRP and DeepLift (Rescale) explanation methods are equivalent to the input $\odot$ gradient. Similarly, Lundberg and Lee [18] proposed SHAP explanations which approximate the shapley value and unify several existing methods.</p>
<p>Fragility. Ghorbani et al. [25] and Kindermans et al. [26] both present attacks against saliency methods; showing that it is possible to manipulate derived explanations in unintended ways. Nie et al. [27] theoretically assessed backpropagation based methods and found that Guided BackProp and DeconvNet, under certain conditions, are invariant to network reparamaterizations, particularly random Gaussian initialization. Specifically, they show that Guided BackProp and DeconvNet both seem to be performing partial input recovery. Our findings are similar for Guided BackProp and its variants. Further, our work differs in that we propose actionable sanity checks for assessing explanation approaches. Along similar lines, Mahendran and Vedaldi [28] also showed that some backpropagation-based saliency methods can often lack neuron discriminativity.</p>
<p>Current assessment methods. Both Samek et al. [29] and Montavon et al. [30] proposed an input perturbation procedure for assessing the quality of saliency methods. Dabkowski and Gal [17] proposed an entropy based metric to quantify the amount of relevant information an explanation mask captures. Performance of a saliency map on an object localization task has also been used for assessing saliency methods. Montavon et al. [30] discuss explanation continuity and selectivity as measures of assessment.</p>
<p>Randomization. Our label randomization test was inspired by the work of Zhang et al. [31], although we use the test for an entirely different purpose.</p>
<h1>2.2 Visualization \&amp; Similarity Metrics</h1>
<p>We discuss our visualization approach and overview the set of metrics used in assessing similarity between two explanations.</p>
<p>Visualization. We visualize saliency maps in two ways. In the first case, absolute-value (ABS), we take absolute values of a normalized map. For the second case, diverging visualization, we leave the map as is, and use different colors to show positive and negative importance.</p>
<p>Similarity Metrics. For quantitative comparison, we rely on the following metrics: Spearman rank correlation with absolute value (absolute value), Spearman rank correlation without absolute value (diverging), the structural similarity index (SSIM), and the Pearson correlation of the histogram of gradients (HOGs) derived from two maps. We compute the SSIM and HOGs similarity metric on ImageNet examples without absolute values ${ }^{4}$. SSIM and Pearson correlation of HOGs have been used in literature to remove duplicate images and quantify image similarity. Ultimately, quantifying human visual perception is still an active area of research.</p>
<h2>3 Model Parameter Randomization Test</h2>
<p>The parameter settings of a model encode what the model has learned from the data during training. In particular, model parameters have a strong effect on test performance of the model. Consequently, for a saliency method to be useful for debugging a model, it ought to be sensitive to model parameters.
As an illustrative example, consider a linear function of the form $f(x)=w_{1} x_{1}+w_{2} x_{2}$ with input $x \in \mathbb{R}^{2}$. A gradient-based explanation for the model's behavior for input $x$ is given by the parameter values $\left(w_{1}, w_{2}\right)$, which correspond to the sensitivity of the function to each of the coordinates. Changes in the model parameters therefore change the explanation.
Our proposed model parameter randomization test assesses an explanation method's sensitivity to model parameters. We conduct two kinds of randomization. First we randomly re-initialize all weights of the model both completely and in a cascading fashion. Second, we independently randomize a single layer at a time while keeping all others fixed. In both cases, we compare the resulting explanation from a network with random weights to the one obtained with the model's original weights.</p>
<h3>3.1 Cascading Randomization</h3>
<p>Overview. In the cascading randomization, we randomize the weights of a model starting from the top layer, successively, all the way to the bottom layer. This procedure destroys the learned weights from the top layers to the bottom ones. Figure 2 shows masks, for several saliency methods, for an example input for the cascading randomization on an Inception v3 model trained on ImageNet. In Figure 4, we show the two Spearman (absolute value and no-absolute value) metrics across different data sets and architectures. Finally, in Figure 5, we show the SSIM and HOGs similarity metrics.
The gradient shows sensitivity while Guided Backprop is invariant to higher layer weights. We find that the gradient map is, indeed, sensitive to model parameter randomization. Similarly, GradCAM is sensitive to model weights if the randomization is downstream of the last convolutional layer. However, Guided Backprop (along with Guided GradCAM) is invariant to higher layer weights.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Cascading randomization on Inception v3 (ImageNet). Figure shows the original explanations (first column) for the Junco bird as well as the label for each explanation type. Progression from left to right indicates complete randomization of network weights (and other trainable variables) up to that 'block' inclusive. We show images for 17 blocks of randomization. Coordinate (Gradient, mixed_7b) shows the gradient explanation for the network in which the top layers starting from Logits up to mixed_7b have been reinitialized. The last column corresponds to a network with completely reinitialized weights. See Appendix for more examples.</p>
<p>Masks derived from Guided Backprop remain visually and quantitatively similar to masks of a trained model until lower layer weights (those closest to the input) are randomized. ${ }^{5}$
The danger of the visual assessment. On visual inspection, we find that gradient $\diamond$ input and integrated gradients show visual similarity to the original mask. In fact, from Figure 2, it is still possible to make out the structure of the bird even after multiple blocks of randomization. This visual similarity is reflected in the SSIM comparison (Figure 5), and the rank correlation with absolute value (Figure 4-Top). However, re-initialization disrupts the sign of the map, so that the spearman rank correlation without absolute values goes to zero (Figure 4-Bottom) almost as soon as the top layers are randomized. The observed visual perception versus ranking dichotomy indicates that naive visual inspection of the masks, in this setting, does not distinguish networks of similar structure but widely differing parameters. We explain the source of this phenomenon in our discussion section.</p>
<h1>3.2 Independent Randomization</h1>
<p>Overview. As a different form of the model parameter randomization test, we now conduct an independent layer-by-layer randomization with the goal of isolating the dependence of the explanations by layer. This approach allows us to exhaustively assess the dependence of saliency masks on lower versus higher layer weights. More concretely, for each layer, we fix the weights of other layers to their original values, and randomize one layer at a time.
Results. Figure 3 shows the evolution of different masks as each layer of Inception v3 is independently randomized. We observe a correspondence between the results from the cascading and independent layer randomization experiments: Guided Backprop (along with Guided GradCAM) show invariance to higher layer weights. However, once the lower layer convolutional weights are randomized, the Guided Backprop masks changes, although the resulting mask is still dominated by the input structure.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Independent randomization on Inception v3 (ImageNet). Similar to Figure 2, however each 'layer'/'block' is randomized independently, i.e., the rest of the weights are kept at the pre-trained values, while only each layer/block is randomized. Masks derived from these partially randomized networks are shown here. We observe, again, that Guided Backprop is sensitive to only the lower layer weights.</p>
<h1>4 Data Randomization Test</h1>
<p>The feasibility of accurate prediction hinges on the relationship between instances (e.g., images) and labels encoded by the data. If we artificially break this relationship by randomizing the labels, no predictive model can do better than random guessing. Our data randomization test evaluates the sensitivity of an explanation method to the relationship between instances and labels. An explanation method insensitive to randomizing labels cannot possibly explain mechanisms that depend on the relationship between instances and labels present in the data generating process. For example, if an explanation did not change after we randomly assigned diagnoses to CT scans, then evidently it did not explain anything about the relationship between a CT scan and the correct diagnosis in the first place (see [32] for an application of Guided BackProp as part of a pipepline for shadow detection in 2D Ultrasound).
In our data randomization test, we permute the training labels and train a model on the randomized training data. A model achieving high training accuracy on the randomized training data is forced to memorize the randomized labels without being able to exploit the original structure in the data. As it turns out, state-of-the art deep neural networks can easily fit random labels as was shown in Zhang et al. [31].
In our experiments, we permute the training labels for each model and data set pair, and train the model to greater than $95 \%$ training set accuracy. Note that the test accuracy is never better than randomly guessing a label (up to sampling error). For each resulting model, we then compute explanations on the same test bed of inputs for a model trained with true labels and the corresponding model trained on randomly permuted labels.
Gradient is sensitive. We find, again, that gradients, and its smoothgrad variant, undergo substantial changes. We also observe that GradCAM masks undergo changes that result in masks with disconnected patches.
Sole reliance on the visual inspection can be misleading. For Guided BackProp, we observe a visual change; however, we find that the masks still highlight portions of the input that would seem plausible, given correspondence with the input, on naive visual inspection. For example, from the diverging masks (Figure 6-Right), we see that the Guided BackProp mask still assigns positive relevance across most of the digit for the network trained on random labels.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Cascading Randomization. Successive re-initialization of weights starting from top layers for Inception v3 on ImageNet, CNN on Fashion MNIST, and MLP on MNIST. In all plots, y axis is the rank correlation between original explanation and the randomized explanation derived for randomization up to that layer/block, while the x axis corresponds to the layers/blocks of the DNN starting from the output layer. The black dashed line indicates where successive randomization of the network begins, which is at the top layer. Top: Spearman Rank correlation with absolute values, Bottom: Spearman Rank correlation without absolute values.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Cascading Randomization. Figure showing SSIM and HOGs similarity between original input masks and the masks generated as the Inception v3 is randomized in a cascading manner.</p>
<p>For gradient input and integrated gradients, we also observe visual changes in the masks obtained, particularly, in the sign of the attributions. Despite this, the input structure is still clearly prevalent in the masks. The effect observed is particularly prominent for sparse inputs like MNIST where most of the input is zero; however, we observe similar effects for Fashion MNIST (see Appendix), which is less sparse. With visual inspection alone, it is not inconceivable that an analyst could confuse the integrated gradient and gradientinput masks derived from a network trained on random labels as legitimate. We clarify these findings and address implications in the discussion section.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Explanation for a true model vs. model trained on random labels. Top Left: Absolutevalue visualization of masks for digit 0 from the MNIST test set for a CNN. Top Right: Saliency masks for digit 0 from the MNIST test set for a CNN shown in diverging color. Bottom Left: Spearman rank correlation (with absolute values) bar graph for saliency methods. We compare the similarity of explanations derived from a model trained on random labels, and one trained on real labels. Bottom Right: Spearman rank correlation (without absolute values) bar graph for saliency methods for MLP. See appendix for corresponding figures for CNN, and MLP on Fashion MNIST.</p>
<h1>5 Discussion</h1>
<p>We now take a step back to interpret our findings. First, we discuss the influence of the model architecture on explanations derived from NNs. Second, we consider methods that approximate an element-wise producet of the input and the gradient, as several local explanations do [33, 18]. We show, empirically, that the input "structure" dominates the gradient, especially for sparse inputs. Third, we explain the observed behavior of the gradient explanation with an appeal to linear models. We then consider a single 1-layer convolution with sum-pooling architecture, and show that saliency explanations for this model mostly capture edges. Finally, we return to the edge detector and make comparisons between methods that fail our sanity checks and an edge detector.</p>
<h3>5.1 The role of model architecture as a prior</h3>
<p>The architecture of a deep neural network has an important effect on the representation derived from the network. A number of results speak to the strength of randomly initialized models as classification priors [34, 35]. Moreover, randomly initialized networks trained on a single input can perform tasks like denoising, super-resolution, and in-painting [36] without additional training data. These prior works speak to the fact that randomly initialized networks correspond to non-trivial representations. Explanations that do not depend on model parameters or training data might still depend on the model architecture and thus provide some useful information about the prior incorporated in the model architecture. However, in this case, the explanation method should only be used for tasks where we believe that knowledge of the model architecture on its own is sufficient for giving useful explanations.</p>
<h3>5.2 Element-wise input-gradient products</h3>
<p>A number of methods, e.g., $\epsilon$-LRP, DeepLift, and integrated gradients, approximate the element-wise product of the input and the gradient (on a piecewise linear function like ReLU). To gain further insight into our findings, we can look at what happens to the input-gradient product $E(x)=x \odot \frac{\partial N}{\partial x}$, if the input is kept fixed, but the gradient is randomized. To do so, we conduct the following experiment. For an input $x$, sample two normal random vectors $u, v$ (we consider both the truncated normal and uniform distributions) and consider the element-wise product of $x$ with $u$ and $v$, respectively, i.e., $x \odot u$, and $x \odot v$. We then look at the similarity, for all the metrics considered, between $x \odot u$ and</p>
<p>$x \odot v$ as noise increases. We conduct this experiment on Fashion MNIST and ImageNet samples. We observe that the input does indeed dominate the product (see Figure 19 in Appendix). We also observe that the input dominance persists even as the noisy gradient vectors change drastically. This experiment indicates that methods that approximate the "input-times-gradient" mostly return the input, in cases where the gradients look visually noisy as they tend to do.</p>
<h1>5.3 Analysis for simple models</h1>
<p>To better understand our findings, we analyze the output of the saliency methods tested on two simple models: a linear model and a 1-layer sum pooling convolutional network. We find that the output of the saliency methods, on a linear model, returns a coefficient that intuitively measures the sensitivity of the model with respect to that variable. However, these methods applied to a random convolution seem to result in visual artifacts that are akin to an edge detector.</p>
<p>Linear Model. Consider a linear model $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ defined as $f(x)=w \cdot x$ where $w \in \mathbb{R}^{d}$ are the model weights. For gradients we have $E_{\text {grad }}(x)=\frac{\partial(w \cdot x)}{\partial x}=w$. Similarly for SmoothGrad we have $E_{\text {sg }}(x)=w$ (the gradient is independent of the input, so averaging gradients over noisy inputs yields the same model weight). Integrated Gradients reduces to "gradient $\odot$ input" for this case:
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Explanations derived for the 1-layer Sum-Pooling Convolution architecture. We show gradient, SmoothGrad, Integrated Gradients and Guided BackProp explanations.</p>
<p>$$
E_{I G}(x)=(x-\bar{x}) \odot \int_{0}^{1} \frac{\partial f(\bar{x}+\alpha(x-\bar{x}))}{\partial x} d \alpha=(x-\bar{x}) \odot \int_{0}^{1} w \alpha d \alpha=(x-\bar{x}) \odot w / 2
$$</p>
<p>Consequently, we see that the application of the basic gradient method to a linear model will pass our sanity check. Gradients on a random model will return an image of white noise, while integrated gradients will return a noisy version of the input image. We did not consider Guided Backprop and GradCAM here because both methods are not defined for the linear model considered above.</p>
<p>1 Layer Sum-Pool Conv Model. We now show that the application of these same methods to a 1-layer convolutional network may result in visual artifacts that can be misleading unless further analysis is done. Consider a single-layer convolutional network applied to a grey-scale image $x \in \mathbb{R}^{n \times n}$. Let $w \in \mathbb{R}^{3 \times 3}$ denote the $3 \times 3$ convolutional filter, indexed as $w_{i j}$ for $i, j \in{-1,0,1}$. We denote by $w * x \in \mathbb{R}^{n \times n}$ the output of the convolution operation on the image $x$. Then the output of this network can be written as $l(x)=\sum_{i=1}^{n} \sum_{j=1}^{n} \sigma(w * x)_{i j}$, where $\sigma$ is the ReLU non-linearity applied point-wise. In particular, this network applies a single 3x3 convolutional filter to the input image, then applies a ReLU non-linearity and finally sum-pools over the entire convolutional layer for the output. This is a similar architecture to the one considered in [34]. As shown in Figure 7, we see that different saliency methods do act like edge detectors. This suggests that the convolutional structure of the network is responsible for the edge detecting behavior of some of these saliency methods.</p>
<p>To understand why saliency methods applied to this simple architecture visually appear to be edge detectors, we consider the closed form of the gradient $\frac{\partial}{\partial x_{i j}} l(x)$. Let $a_{i j}=\mathbf{1}\left{(w * x)_{i j} \geq 0\right}$ indicate the activation pattern of the ReLU units in the convolutional layer. Then for $i, j \in[2, n-1]$ we have</p>
<p>$$
\frac{\partial}{\partial x_{i j}} l(x)=\sum_{k=-1}^{1} \sum_{l=-1}^{1} \sigma^{\prime}\left((w * x)<em k="k" l="l">{i+k, j+l}\right) w</em>
$$}=\sum_{k=-1}^{1} \sum_{l=-1}^{1} a_{i+k, j+l} w_{k l</p>
<p>(Recall that $\sigma^{\prime}(x)=0$ if $x&lt;0$ and 1 otherwise). This implies that the $3 \times 3$ activation pattern local to pixel $x_{i j}$ uniquely determines $\frac{\partial}{\partial x_{i j}}$. It is now clear why edges will be visible in the produced</p>
<p>saliency mask - regions in the image corresponding to an "edge" will have a distinct activation pattern from surrounding pixels. In contrast, pixel regions of the image which are more uniform will all have the same activation pattern, and thus the same value of $\frac{\partial}{\partial x_{i j}} l(x)$. Perhaps a similar principle applies for stacked convolutional layers.</p>
<h1>5.4 The case of edge detectors.</h1>
<p>An edge detector, roughly speaking, is a classical tool to highlight sharp transitions in an image. Notably, edge detectors are typically untrained and do not depend on any predictive model. They are solely a function of the given input image. As some of the saliency methods we saw, edge detection is invariant under model and data transformations.</p>
<p>In Figure 1 we saw that edge detectors produce images that are strikingly similar to the outputs of some saliency methods. In fact, edge detectors can also produce pictures that highlight features which coincide with what appears to be relevant to a model's class prediction. However, here the human observer is at risk of confirmation bias when interpreting the highlighted edges as an explanation of the class prediction. In Figure 37 (Appendix), we show a qualitative comparison of saliency maps of an input image with the same input image multiplied element-wise by the output of an edge detector. The result indeed looks strikingly similar, illustrating that saliency methods mostly use the edges of the image.</p>
<p>While edge detection is a fundamental and useful image processing technique, it is typically not thought of as an explanation method, simply because it involves no model or training data. In light of our findings, it is not unreasonable to interpret some saliency methods as implicitly implementing unsupervised image processing techniques, akin to edge detection, segmentation, or denoising. To differentiate such methods from model-sensitive explanations, visual inspection is insufficient.</p>
<h2>6 Conclusion and future work</h2>
<p>The goal of our experimental method is to give researchers guidance in assessing the scope of model explanation methods. We envision these methods to serve as sanity checks in the design of new model explanations. Our results show that visual inspection of explanations alone can favor methods that may provide compelling pictures, but lack sensitivity to the model and the data generating process.</p>
<p>Invariances in explanation methods give a concrete way to rule out the adequacy of the method for certain tasks. We primarily focused on invariance under model randomization, and label randomization. Many other transformations are worth investigating and can shed light on various methods we did and did not evaluate. Along these lines, we hope that our paper is a stepping stone towards a more rigorous evaluation of new explanation methods, rather than a verdict on existing methods.</p>
<h2>Acknowledgments</h2>
<p>We thank the Google PAIR team for open source implementation of the methods used in this work. We thank Martin Wattenberg and other members of the Google Brain team for critical feedback and helpful discussions that helped improved the work. Lastly, we thank anonymous reviewers for feedback that helped improve the manuscript. We are also grateful to Leon Sixt for pointing out a bug in our Guided Backprop experiments in an earlier version of this work.</p>
<h2>References</h2>
<p>[1] Alfredo Vellido, José David Martín-Guerrero, and Paulo JG Lisboa. Making machine learning models interpretable. In ESANN, volume 12, pages 163-172. Citeseer, 2012.
[2] Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman, David O’Brien, Stuart Schieber, James Waldo, David Weinberger, and Alexandra Wood. Accountability of ai under the law: The role of explanation. arXiv preprint arXiv:1711.01134, 2017.
[3] Bryce Goodman and Seth Flaxman. European union regulations on algorithmic decision-making and a" right to explanation". arXiv preprint arXiv:1606.08813, 2016.
[4] Jorge Casillas, Oscar Cordón, Francisco Herrera Triguero, and Luis Magdalena. Interpretability issues in fuzzy modeling, volume 128. Springer, 2013.</p>
<p>[5] Gabriel Cadamuro, Ran Gilad-Bachrach, and Xiaojin Zhu. Debugging machine learning models. In ICML Workshop on Reliable Machine Learning in the Wild, 2016.
[6] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. Interpretable \&amp; explorable approximations of black box models. arXiv preprint arXiv:1707.01154, 2017.
[7] Fulton Wang and Cynthia Rudin. Causal falling rule lists. arXiv preprint arXiv:1510.05189, 2015.
[8] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
[9] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
[10] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, pages 818-833. Springer, 2014.
[11] Maximilian Alber Klaus-Robert Müller Dumitru Erhan Been Kim Sven Dähne Pieter-Jan Kindermans, Kristof T. Schütt. Learning how to explain neural networks: Patternnet and patternattribution. International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id= Hkn7CBaTW.
[12] Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. Visualizing deep neural network decisions: Prediction difference analysis. arXiv preprint arXiv:1702.04595, 2017.
[13] Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black box: Learning important features through propagating activation differences. arXiv preprint arXiv:1605.01713, 2016.
[14] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. arXiv preprint arXiv:1703.01365, 2017.
[15] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1135-1144. ACM, 2016.
[16] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.
[17] Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. In Advances in Neural Information Processing Systems, pages 6970-6979, 2017.
[18] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems, pages 4768-4777, 2017.
[19] Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and Dhruv Batra. Grad-cam: Why did you say that? arXiv preprint arXiv:1611.07450, 2016.
[20] Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. arXiv preprint arXiv:1704.03296, 2017.
[21] Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An informationtheoretic perspective on model interpretation. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 883-892, Stockholmsmässan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/chen18j.html.
[22] David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-Robert MÄ̈̈ller. How to explain individual classification decisions. Journal of Machine Learning Research, 11 (Jun):1803-1831, 2010.
[23] Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer features of a deep network. University of Montreal, 1341(3):1, 2009.
[24] Marco Ancona, Enea Ceolini, Cengiz Öztireli, and Markus Gross. A unified view of gradient-based attribution methods for deep neural networks. arXiv preprint arXiv:1711.06104, 2017.
[25] Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. arXiv preprint arXiv:1710.10547, 2017.</p>
<p>[26] Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Schütt, Sven Dähne, Dumitru Erhan, and Been Kim. The (un) reliability of saliency methods. arXiv preprint arXiv:1711.00867, 2017.
[27] Weili Nie, Yang Zhang, and Ankit Patel. A theoretical explanation for perplexing behaviors of backpropagation-based visualizations. In ICML, 2018.
[28] Aravindh Mahendran and Andrea Vedaldi. Salient deconvolutional networks. In European Conference on Computer Vision, pages 120-135. Springer, 2016.
[29] Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin, and Klaus-Robert Müller. Evaluating the visualization of what a deep neural network has learned. IEEE transactions on neural networks and learning systems, 28(11):2660-2673, 2017.
[30] Grégoire Montavon, Wojciech Samek, and Klaus-Robert Müller. Methods for interpreting and understanding deep neural networks. Digital Signal Processing, 2017.
[31] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In In Proc. 5th ICLR, 2017.
[32] Qingjie Meng, Christian Baumgartner, Matthew Sinclair, James Housden, Martin Rajchl, Alberto Gomez, Benjamin Hou, Nicolas Toussaint, Jeremy Tan, Jacqueline Matthew, et al. Automatic shadow detection in 2d ultrasound. 2018.
[33] Marco Ancona, Enea Ceolini, Cengiz Öztireli, and Markus Gross. Towards better understanding of gradient-based attribution methods for deep neural networks. In In Proc. 6th ICLR, 2018.
[34] Andrew M Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Y Ng. On random weights and unsupervised feature learning. In ICML, pages 1089-1096, 2011.
[35] Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644, 2016.
[36] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. arXiv preprint arXiv:1711.10925, 2017.
[37] Julius Adebayo, Justin Gilmer, Ian Goodfellow, and Been Kim. Local explanation methods for deep neural networks lack sensitivity to parameter values. 2018.
[38] Junghoon Seo, Jeongyeol Choe, Jamyoung Koo, Seunghyeon Jeon, Beomsu Kim, and Taegyun Jeon. Noise-adding methods of saliency map as series of higher order partial derivative. arXiv preprint arXiv:1806.03000, 2018.
[39] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818-2826, 2016.
[40] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015.
[41] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
[42] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
[43] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inceptionresnet and the impact of residual connections on learning. In AAAI, volume 4, page 12, 2017.</p>
<h1>Appendix</h1>
<h2>A Explanation Methods</h2>
<p>We now provide additional overview of the different saliency methods that we assess in this work. As described in the main text, an input is a vector $x \in \mathbb{R}^{d}$. A model describes a function $S: \mathbb{R}^{d} \rightarrow \mathbb{R}^{C}$, where $C$ is the number of classes in the classification problem. An explanation method provides an explanation map $E: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ that maps inputs to objects of the same shape. Each dimension then correspond to the 'relevance' or 'importance' of that dimension to the final output, which is often a class-specific score as specified above.</p>
<h2>A. 1 Gradient with respect to input</h2>
<p>This corresponds to the gradient of the scalar logit for a particular class wrt to the input.</p>
<p>$$
E_{\mathrm{grad}}(x)=\frac{\partial S}{\partial x}
$$</p>
<h2>A. 2 Gradient $\odot$ Input</h2>
<p>Gradient element-wise product with the input. Ancona et. al. show that this input gradient product is equivalent to DeepLift, and $\epsilon$-LRP (other explanations methods), for a network with with only Relu(s) and no additive biases.</p>
<p>$$
E_{\text {grad } \odot \text { input }}(x)=x \odot \frac{\partial S}{\partial x}
$$</p>
<h2>A. 3 Guided Backpropagation (GBP)</h2>
<p>GBP specifies a change in how to back-propagate gradienst for ReLus. Let $\left{f^{l}, f^{l-1}, \ldots, f^{0}\right}$ be the feature maps derived during the forward pass through a DNN, and $\left{R^{l}, R^{l-1}, \ldots, R^{0}\right}$ be 'intermediate representations' obtained during the backward pass. Concretely, $f^{l}=\operatorname{relu}\left(f^{l-1}\right)=\max \left(f^{l-1}, 0\right)$, and $R^{l+1}=\frac{\partial f^{\text {out }}}{\partial f^{l+1}}$ (for regular back-propagation). GBP aims to zero out negative gradients during computation of $R$. The mask is computed as:</p>
<p>$$
R^{l}=1_{R^{l+1}&gt;0} 1_{f^{l}&gt;0} R^{l+1}
$$</p>
<p>$1_{R^{l+1}&gt;0}$ means keep only the positive gradients, and $1_{f^{l}&gt;0}$ means keep only the positive activations.</p>
<h2>A. 4 GradCAM and Guided GradCAM</h2>
<p>Introduced by Selvaraju et al. [19], GradCAM explanations correspond to the gradient of the class score (logit) with respect to the feature map of the last convolutional unit of a DNN. For pixel level granularity GradCAM, can be combined with Guided Backpropagation through an element-wise product.
Following the exact notation by Selvaraju et al. [19], let $A^{k}$ be the feature maps derived from the last convolutional layer of a DNN. Consequently, GradCAM is defined as follows: first, neuron importance weights are calculated, $\alpha_{c}^{k}=\frac{1}{Z} \sum_{i} \sum_{j} \frac{\partial S}{\partial A_{i j}^{k}}$, then the GradCAM mask corresponds to: $\operatorname{ReLU}\left(\sum_{k} \alpha_{c}^{k} A^{k}\right)$. This corresponds to a global average pooling of the gradients followed by weighted linear combination to which a ReLU is applied. Now, the Guided GradCAM mask is then defined as:</p>
<p>$$
E_{\text {guided }- \text { gradcam }}(x)=E_{\text {gradcam }} \odot E_{\text {glsp }}
$$</p>
<h2>A. 5 Integrated Gradients (IG)</h2>
<p>IG is defined as:</p>
<p>$$
E_{\mathrm{IG}}(x)=(x-\bar{x}) \times \int_{0}^{1} \frac{\partial S(\bar{x}+\alpha(x-\bar{x})}{\partial x} d \alpha
$$</p>
<p>where $\bar{x}$ is the baseline input that represents the absence of a feature in the original sample $x_{t} . \bar{x}$ is typically set to zero.</p>
<h1>A. 6 SmoothGrad</h1>
<p>Given an explanation, $E$, from one of the methods previously discussed, a sample $x$, the SmoothGrad explanation, $E_{\text {sg }}$, is defined as follows:</p>
<p>$$
E_{\mathrm{sg}}(x)=\frac{1}{N} \sum_{i=1}^{N} E\left(x+g_{i}\right)
$$</p>
<p>where noise vectors $g_{i} \sim \mathcal{N}\left(0, \sigma^{2}\right)$ ) are drawn i.i.d. from a normal distribution.</p>
<h2>A. 7 VarGrad</h2>
<p>Similar to SmoothGrad, and as referenced in [37] a variance analog of SmoothGrad can be defined as follows:</p>
<p>$$
E_{\mathrm{vg}}(x)=\mathcal{V}\left(E\left(x+g_{i}\right)\right)
$$</p>
<p>where noise vectors $g_{i} \sim \mathcal{N}\left(0, \sigma^{2}\right)$ ) are drawn i.i.d. from a normal distribution, and $\mathcal{V}$ corresponds to the variance. In the visualizations presented here, explanations with VG correspond to the VarGrad equivalent of such masks. Seo et al. [38] theoretically analyze VarGrad showing that it is independent of the gradient, and captures higher order partial derivatives.</p>
<h2>B DNN Architecture, Training, Randomization \&amp; Metrics</h2>
<p>Experimental Details Data sets \&amp; Models. We perform our randomization tests on a variety of datasets and models as follows: an Inception v3 model [39] trained on the ImageNet classification dataset [40] for object recognition, a Convolutional Neural Network (CNN) trained on MNIST [41] and Fashion MNIST [42], and a multi-layer perceptron (MLP), also trained on MNIST and Fashion MNIST.
Randomization Tests We perform 2 types of randomizations. For the model parameter randomization tests, we re-initialized the parameters of each of the models with a truncated normal distribution. We replicated these randomization for a uniform distribution and obtain identical results. For the random labels test, we randomize, completely, the training labels for a each-model dataset pair (MNIST and Fashion MNIST) and then train the model to greater than 95 percent training set accuracy. As expected the performance of these models on the tests set is random.</p>
<p>Inception v3 trained on ImageNet. For Inception v3, we used a pre-trained network that is widely distributed with the tensorflow package available at: https://github.com/tensorflow/models/tree/ master/research/slim#Pretrained. This model has a 93.9 top-5 accuracy on the ImageNet test set. For the randomization tests, we re-initialized on a per-block basis. As noted in [43], each inception block consists of multiple filters of different sizes. In this case, we randomize all the the filter weights, biases, and batch-norm variables for each inception block. In total, this randomization occurs in 17 phases.</p>
<p>CNN on MNIST and Fashion MNIST. The CNN architecture is as follows: input -&gt; conv (5x5, 32) -&gt; pooling (2x2)-&gt; conv (5x5, 64) -&gt; pooling (2x2) -&gt; fully connected (1024 units) -&gt; softmax (10 units). We train the model with the ADAM optimizer for 20 thousand iterations. All non-linearities used are ReLU. We also apply weight decay (penalty 0.001 ) to the weights of the network. The final test set accuracy of this model is 99.2 percent. For model parameter randomization test, we reinitialize each layer successively or independently depending on the randomization experiment. The weight initialiazation scheme followed was a truncated normal distribution (mean: 0 , std: 0.01 ). We also tried a uniform distribution as well, and found that our results still hold.</p>
<p>MLP trained on MNIST. The MLP architecture is as follows: input -&gt; fully connected (2500 units) -&gt; fully connected (1500 units) -&gt; fully connected (500 units) -&gt; fully connected (10 units). We also train this model with the ADAM optimizer for 20 thousand iterations. All non-linearities used are Relu. The final test set accuracy of this model is 98.7 percent. For randomization tests, we reinitialize each layer successively or independently depending on the randomization experiment.</p>
<p>Inception v4 trained on Skeletal Radiograms. We also analyzed an inception v4 model trained on skeletal radiograms obtained as part of the pediatric bone age challenge conducted by the radiological society of north America. This inception v4 model was trained retained the standard original parameters except it was trained with a mixed L1 and L2 loss. In our randomization test as indicated in figure 1, we reinitialize all weights, biases, and variables of the model.</p>
<p>Calibration for Similarity Metrics. As noted in the methods section, we measure the similarity of the saliency masks obtained using the following metrics: Spearman rank correlation with absolute value (absolute value), Spearman rank correlation without absolute value (diverging), the structural similarity index (SSIM), and the Pearson correlation of the histogram of gradients (HOGs) derived from two maps. The SSIM and HOGs metrics</p>
<p>are computed for ImageNet explanation masks. We do this because these metrics are suited to natural images, and to avoid the somewhat artificial structure of Fashion MNIST and MNIST images. We conduct two kinds of calibration exercises. First we measure, for each metric, the similarity between an explanation mask and a randomly sampled (Uniform or Gaussian) mask. Second, we measure, for each metric, the similarity between two randomly sampled explanation masks (Uniform or Gaussian). Together, these two tasks allow us to see if high values for a particular metric indeed correspond to meaningfully high values.</p>
<p>We use the skimage HOG function with a $(16,16)$ pixels per cell. Note that the input to the HOG function is 299 by 229 with the values normalized to $[-1,+1]$. We also used the skimage SSIM function with a window size of 5. We obtained the gradient saliency maps for 50 images in the ImageNet validation set. We then compare these under the two settings described above; we report the average across these 50 images as the following tuple: (Rank correlation with no absolute value, Rank correlation with absolute value, HOGs Metric, SSIM). The average similarity between the gradient mask and random Gaussian mask is: $(-0.00049,0.00032,-0.0016,0.00027)$. We repeat this experiment for Integrated gradient and gradient $\odot$ input and obtained: $(0.00084,0.00059,0.0048,0.00018)$, and $(0.00081,0.00099,-0.0024,0.00023)$. We now report results for the above metrics for similarity between two random masks. For uniform distribution [-1, 1], we obtain the following similarity: $(0.00016,-0.0015,0.078,0.00076)$. For Gaussian masks with mean zero and unit variance that has been normalized to lie in the range $[-1,1]$, we obtain the following similarity metric: $(0.00018,0.00043,-0.0013,0.00023)$.</p>
<h1>C Additional Figures</h1>
<p>We now present additional figures referenced in the main text.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Cascading Randomization for several examples for Guided Backpropagation.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Cascading Randomization for several examples for Guided Backpropagation in a different visualization scheme.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Cascading Randomization on Inception V3 for bird example in Grayscale.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Cascading Randomization on Inception V3 for bird example in diverging scheme.</p>
<p>Independent 'Layer' Randomization
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Independent Randomization on Inception V3 for bird example.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Cascading Randomization on Inception V3 for dog example in Grayscale.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: Cascading Randomization on Inception V3 for dog example in diverging scheme.</p>
<p>Independent 'Layer' Randomization
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15: Independent Randomization on Inception V3 for dog example.
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16: Cascading Randomization on Inception V3 for corn example in Grayscale.</p>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 17: Cascading Randomization on Inception V3 for corn example in diverging scheme.</p>
<p>Independent 'Layer' Randomization
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 18: Independent Randomization on Inception V3 for corn example.</p>
<p><img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 19: Input $\odot$ Random gradient experiment.</p>
<p><img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure 20: Additional cascading rank correlation metrics across MNIST and Fashion MNIST Convolutional Networks.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5} \mathrm{~A}$ previous version of this work noted that Guided Backprop was entirely invariant; however, this is not this case.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8920 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8920</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8920</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-271924133</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.11866v1.pdf" target="_blank">Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design</a></p>
                <p><strong>Paper Abstract:</strong> Molecule design is a multifaceted approach that leverages computational methods and experiments to optimize molecular properties, fast-tracking new drug discoveries, innovative material development, and more efficient chemical processes. Recently, text-based molecule design has emerged, inspired by next-generation AI tasks analogous to foundational vision-language models. Our study explores the use of knowledge-augmented prompting of large language models (LLMs) for the zero-shot text-conditional de novo molecular generation task. Our approach uses task-specific instructions and a few demonstrations to address distributional shift challenges when constructing augmented prompts for querying LLMs to generate molecules consistent with technical descriptions. Our framework proves effective, outperforming state-of-the-art (SOTA) baseline models on benchmark datasets.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8920.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8920.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FrontierX: LLM-MG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FrontierX: LLM-MG (Knowledge‑Augmented LLM Prompting for Zero‑Shot Text‑Based De Novo Molecule Design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid pipeline that queries off‑the‑shelf LLMs with knowledge‑augmented prompts to produce top‑R SMILES predictions and textual explanations, fine‑tunes small LMs on those explanations and original descriptions to obtain contextual token embeddings, integrates embeddings via a hierarchical multi‑head attention module, and decodes unified cross‑modal embeddings into SMILES strings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (primary backbone in reported best results); also evaluated GPT-3.5-turbo, text-davinci-003, Google Bard; small LM: DeBERTa used for embedding fine‑tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based LLMs (GPT family, Bard) used as black-box generative LLMs; smaller transformer encoder LMs (DeBERTa) fine-tuned for token/context embeddings; a transformer decoder for SMILES generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Varies by backbone: paper reports text-davinci-003 and ChatGPT listed as 175B, Bard listed as 1,560B (per table); GPT-4 size not reported; DeBERTa ~50M (reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>LLMs: pre-trained on large text corpora (not re-trained or fine‑tuned for the task in this study). Small LMs (LM_exp and LM_org, e.g., DeBERTa) are fine‑tuned on (a) auxiliary explanations produced by the LLMs and (b) original technical molecule descriptions. The downstream supervised components are trained/validated/tested on ChEBI-20 (33,010 text↔SMILES pairs; 80/10/10 split).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Text‑conditional de novo molecule generation (text2mol): translating technical natural language molecular descriptions into chemical SMILES representations (general molecule design; relevant to drug/material discovery workflows).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt‑based design: knowledge‑augmented prompting (task instructions + K demonstrations sampled from training set using scaffold or random sampling) issued to LLMs via LMaaS; LLMs return top‑R ranked SMILES predictions (R=4 in experiments) plus textual explanations. Small LMs are fine‑tuned on explanations and original descriptions to produce embeddings; these are integrated (HMHA) with prediction embeddings and decoded by a transformer decoder into SMILES (character‑by‑character). No gradient updates to the LLMs themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Novelty is not explicitly quantified (no percentage of molecules outside training set reported). Claims focus on better match to ground truth and higher validity/similarity metrics (higher fingerprint Tanimoto similarity, lower FCD) rather than explicit novelty statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity achieved by (1) knowledge‑augmented prompts that include task instructions and scaffold‑sampled demonstrations relevant to the query (semantic similarity with text‑embedding‑ada‑002), (2) LLMs producing explanations that are used to fine‑tune small LMs for domain adaptation, and (3) evaluation/selection using chemical similarity and validity metrics. Scaffold sampling (K tuned; best reported K=16) is used to bias prompts toward relevant exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Chemical similarity: Fingerprint Tanimoto Similarity (FTS) using MACCS, RDK, Morgan fingerprints; Fréchet ChemNet Distance (FCD). NLP/string metrics: BLEU, Exact Match, Levenshtein distance. Chemical validity: RDKit validity checks. Reported use of MACCS FTS, FCD, BLEU, Exact Match, Levenshtein and RDKit validity percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>FrontierX achieved state‑of‑the‑art performance on the ChEBI-20 text2mol benchmark when using the GPT‑4 backbone with scaffold sampling (K=16). LLMs produced top‑R SMILES and explanations which, after small‑LM fine‑tuning and HMHA fusion, yielded higher chemical similarity and validity versus baselines. Ablation: removing explanatory text embedding (y_exp) reduced MACCS FTS by 17.21%, BLEU by 16.43%, and Validity by 13.12%; removing original text embedding (y_org) produced ~20.69% MACCS FTS drop, ~20.91% BLEU drop, and ~16.00% Validity drop; removing prediction embedding (y_pred) gave more modest drops (~8.44% MACCS FTS, ~11.63% BLEU, ~6.56% Validity). Training/hyperparameters (for the downstream modules) included batch size 32, 100 epochs, embedding dimension d=128, H=4 attention heads, Adam lr=1e-3 with decay and early stopping.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed baseline models reported in prior work (MolT5/T5 variants, RNN‑GRU, Vanilla Transformer) and few‑shot GPT prompting baselines; paper reports superior chemical similarity and validity when FrontierX uses GPT‑4 backbone and scaffold sampling, and reports that scaffold sampling > random sampling for prompt demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>LLMs are used as black boxes (no access to logits/token embeddings), so the pipeline must rely on textual outputs and cannot fine‑tune the LLMs; computational/resource costs (LMaaS, inference cost) and context length limits (typical LLM context limit ~4096 tokens) constrain demonstration count and prompt size; SMILES ambiguities (multiple valid SMILES, implicit hydrogens) make precise SMILES generation hard; LLMs can hallucinate or misinterpret SMILES grammar; the paper notes the need for better integration with cheminformatics tools like RDKit and improved LLM handling of chemical structure syntax. Novelty metrics (e.g., fraction of novel molecules) are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>other_notes</strong></td>
                            <td>The pipeline explicitly does not fine‑tune LLM parameters; it leverages LLM outputs to supervise/fine‑tune smaller, accessible LMs for embedding extraction and downstream decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8920.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8920.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer‑based large language model (used as the primary backbone in FrontierX experiments) queried via LMaaS with knowledge‑augmented prompts to produce ranked SMILES predictions and textual explanations without parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based LLM (GPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in paper for GPT-4 (other models in table report sizes); treated as an off‑the‑shelf backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large public/private text corpora (not re-trained here). Used with ChEBI-20 demonstrations in prompts for zero/few‑shot generation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Text2mol: translating technical textual descriptions to SMILES (de novo molecule generation).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Knowledge‑augmented prompting (task instructions + scaffold‑sampled demonstrations, K=16 in best experiments); LLM returns top‑R SMILES (R=4) and textual justifications via LMaaS API, no fine‑tuning of GPT‑4 parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not quantified for GPT‑4 outputs specifically; emphasis on producing valid and correct SMILES that match ground truth more closely than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Relevant demonstrations selected via scaffold sampling; prompt includes explicit instruction to produce SMILES for the description, and GPT‑4 outputs are filtered/validated by downstream modules and RDKit validity checks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>FTS (MACCS, RDK, Morgan), FCD, BLEU, Exact Match, Levenshtein, RDKit validity (as used in the pipeline evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT‑4 produced the best performance among tested LLM backbones in FrontierX (highest similarity and validity), and the GPT‑4 backbone + scaffold K=16 combination achieved SOTA on ChEBI‑20 in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Performed better than GPT‑3.5, text‑davinci‑003, and Google Bard when used in the FrontierX pipeline; overall FrontierX with GPT‑4 outperformed MolT5 and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>No access to logits or internal embeddings via LMaaS, black‑box nature limits explainability; cost/resource considerations; context length limitations (prompts capped ~4096 tokens) constrain demonstration count; LLM errors handling SMILES syntax and multiple SMILES representations remain an issue.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8920.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8920.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo / text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5‑turbo and text‑davinci‑003 (OpenAI GPT‑3 family models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT‑3 family models (evaluated as alternative LLM backbones) were prompted in zero/few‑shot settings with knowledge‑augmented prompts to output ranked SMILES and explanations; they performed worse than GPT‑4 but were still viable for the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5‑turbo; text‑davinci‑003</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based LLMs (GPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Paper lists text‑davinci‑003 and ChatGPT (GPT‑3.5) with 175B in table; exact effective sizes and GPT‑3.5 specifics not further detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large text corpora; no task fine‑tuning performed in study; used in prompts constructed from ChEBI‑20 demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Text2mol molecule generation from textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Knowledge‑augmented prompt few‑shot (scaffold or random sampling) via LMaaS; LLM returns top‑R SMILES predictions and textual explanations; R=4 used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Prompted with task instructions + sampled demonstrations; scaffold sampling improves relevance for these models as well.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same metrics as other LLMs in the paper (FTS, FCD, BLEU, Exact Match, Levenshtein, RDKit validity).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT‑3.5 and davinci variants performed worse than GPT‑4 in FrontierX experiments; they still produced useful SMILES and explanations but with lower validity/similarity metrics. The paper also uses few‑shot (K=5,10) and zero‑shot variants of these models as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Inferior to GPT‑4 backbone in FrontierX; compared against MolT5 and other baselines but did not surpass the best reported model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Same black‑box, token/context limit, and SMILES ambiguity issues; performance sensitive to number and quality of demonstrations (scaffold sampling helps).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8920.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8920.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Google Bard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Bard (BARD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale transformer‑based LLM evaluated as an off‑the‑shelf backbone in the pipeline; it was outperformed by GPT family models on the text2mol benchmarks in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Google Bard</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Paper's table lists Bard with '1,560B' (as reported in their Table 1); no further sizing details provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large text corpora; used with knowledge‑augmented prompts sampled from ChEBI‑20 (no fine‑tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Text2mol: translating textual descriptions to SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Knowledge‑augmented prompting (scaffold/random sampling) via LMaaS style API interactions; outputs top‑R SMILES + explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Same prompt engineering and scaffold sampling strategies as for other LLMs; performed worse than GPT models on same prompt/demonstration settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>FTS, FCD, BLEU, Exact Match, Levenshtein, RDKit validity (as for other models).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Consistently outperformed by the GPT models in this study when given the same demonstrations and prompts; generated fewer valid SMILES and lower similarity scores.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Performed worse than GPT‑4/GPT‑3.5 in the FrontierX experiments; scaffold sampling improved performance but not to parity with GPT‑4.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Similar limitations: black‑box nature, context length limits, SMILES handling issues; in experiments showed lower task performance relative to GPT‑4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8920.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8920.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa (LM_exp / LM_org)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa (Decoding‑enhanced BERT with disentangled attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller pre‑trained language model fine‑tuned in this work as LM_exp and LM_org to produce contextual token embeddings from (a) LLM‑generated explanations and (b) original textual molecule descriptions, respectively, providing inputs for the attention fusion and decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa (fine‑tuned as LM_exp and LM_org)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer encoder (BERT family variant) used as a small LM for fine‑tuning and embedding extraction</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Paper's Table 1 lists DeBERTa vocabulary/size ~50M (reported as 50M).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large text corpora (standard DeBERTa pretraining). In this study, fine‑tuned on two datasets derived from ChEBI‑20: (1) LLM‑generated auxiliary explanations (S_exp) and (2) original technical descriptions (S_org).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Domain adaptation for text2mol: produce context‑aware token and text‑level embeddings used downstream to generate SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine‑tuning (standard supervised fine‑tune) on textual data (generated explanations and original descriptions) to obtain contextual embeddings; these embeddings are aggregated via softmax attention into fixed text‑level embeddings y_exp and y_org.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not applicable (DeBERTa is an embedding/fine‑tuning component, not a molecule generator itself).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Domain customization achieved by fine‑tuning DeBERTa on task‑specific textual data (LLM explanations and original descriptions), enabling contextualized token embeddings tailored to the molecule description domain.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Indirect: improvements measured by downstream SMILES generation metrics (FTS, FCD, BLEU, Validity) when these embeddings are included. Ablation removing y_exp or y_org severely degrades performance.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine‑tuned DeBERTa representations (y_exp and y_org) are critical: ablation removing y_exp or y_org caused large drops in chemical similarity, BLEU, and validity (examples: ~17–21% drops in MACCS FTS and ~13–16% drops in validity), demonstrating that small LM domain adaptation using LLM outputs materially improves generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Using fine‑tuned small LMs to ingest LLM explanations and original text is a key design choice versus directly attempting to fine‑tune LLMs (which was not done); this yielded better task performance than baselines that do not combine LLM explanations with small‑LM fine‑tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Quality of LLM‑generated explanations conditions the fine‑tuning; if explanations are low quality or incorrect, downstream embeddings and generation can degrade. Fine‑tuning requires labeled downstream data and compute (but is cheaper than fine‑tuning full LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8920.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8920.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5 (T5‑based model pretrained for text2mol)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior encoder‑decoder transformer model pretrained specifically for text‑to‑molecule translation that the paper uses as a primary baseline; FrontierX reports outperforming MolT5 on ChEBI‑20.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Encoder‑decoder transformer (based on T5 architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Not specified in this paper (baseline imported from earlier work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large unannotated datasets for text2mol pretraining (per original MolT5 work); baseline results used from prior publications.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Text2mol translation (textual molecular descriptions → SMILES).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Supervised encoder‑decoder sequence‑to‑sequence generation (pretrained and fine‑tuned for text2mol).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported in this paper (baseline metrics taken from prior studies).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Trained specifically for text2mol tasks; uses supervised training rather than LLM prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Evaluated in prior work and compared here using FTS, FCD, BLEU, Exact Match, Levenshtein, RDKit validity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>FrontierX (with GPT‑4 backbone and scaffold sampling) outperformed MolT5 on the ChEBI‑20 benchmark according to the paper's reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>MolT5 and T5 variants are the main monitored baselines; FrontierX claims superior results by leveraging LLM outputs + small‑LM fine‑tuning + HMHA fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>As a supervised transformer specialized via pretraining, MolT5 requires task‑specific pretraining/fine‑tuning; the paper positions FrontierX as more flexible by using off‑the‑shelf LLMs with prompt engineering plus small‑LM adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Text2mol: Cross-modal molecule retrieval with natural language queries <em>(Rating: 2)</em></li>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>What indeed can gpt models do in chemistry? a comprehensive benchmark on eight tasks <em>(Rating: 2)</em></li>
                <li>MolT5: Text-to-molecule translation model (original MolT5 paper / associated T5 text-to-text transformer work) <em>(Rating: 1)</em></li>
                <li>Fréchet ChemNet Distance: a metric for generative models for molecules in drug discovery <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8920",
    "paper_id": "paper-271924133",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "FrontierX: LLM-MG",
            "name_full": "FrontierX: LLM-MG (Knowledge‑Augmented LLM Prompting for Zero‑Shot Text‑Based De Novo Molecule Design)",
            "brief_description": "A hybrid pipeline that queries off‑the‑shelf LLMs with knowledge‑augmented prompts to produce top‑R SMILES predictions and textual explanations, fine‑tunes small LMs on those explanations and original descriptions to obtain contextual token embeddings, integrates embeddings via a hierarchical multi‑head attention module, and decodes unified cross‑modal embeddings into SMILES strings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (primary backbone in reported best results); also evaluated GPT-3.5-turbo, text-davinci-003, Google Bard; small LM: DeBERTa used for embedding fine‑tuning",
            "model_type": "Transformer-based LLMs (GPT family, Bard) used as black-box generative LLMs; smaller transformer encoder LMs (DeBERTa) fine-tuned for token/context embeddings; a transformer decoder for SMILES generation",
            "model_size": "Varies by backbone: paper reports text-davinci-003 and ChatGPT listed as 175B, Bard listed as 1,560B (per table); GPT-4 size not reported; DeBERTa ~50M (reported in paper).",
            "training_data": "LLMs: pre-trained on large text corpora (not re-trained or fine‑tuned for the task in this study). Small LMs (LM_exp and LM_org, e.g., DeBERTa) are fine‑tuned on (a) auxiliary explanations produced by the LLMs and (b) original technical molecule descriptions. The downstream supervised components are trained/validated/tested on ChEBI-20 (33,010 text↔SMILES pairs; 80/10/10 split).",
            "application_domain": "Text‑conditional de novo molecule generation (text2mol): translating technical natural language molecular descriptions into chemical SMILES representations (general molecule design; relevant to drug/material discovery workflows).",
            "generation_method": "Prompt‑based design: knowledge‑augmented prompting (task instructions + K demonstrations sampled from training set using scaffold or random sampling) issued to LLMs via LMaaS; LLMs return top‑R ranked SMILES predictions (R=4 in experiments) plus textual explanations. Small LMs are fine‑tuned on explanations and original descriptions to produce embeddings; these are integrated (HMHA) with prediction embeddings and decoded by a transformer decoder into SMILES (character‑by‑character). No gradient updates to the LLMs themselves.",
            "novelty_of_chemicals": "Novelty is not explicitly quantified (no percentage of molecules outside training set reported). Claims focus on better match to ground truth and higher validity/similarity metrics (higher fingerprint Tanimoto similarity, lower FCD) rather than explicit novelty statistics.",
            "application_specificity": "Specificity achieved by (1) knowledge‑augmented prompts that include task instructions and scaffold‑sampled demonstrations relevant to the query (semantic similarity with text‑embedding‑ada‑002), (2) LLMs producing explanations that are used to fine‑tune small LMs for domain adaptation, and (3) evaluation/selection using chemical similarity and validity metrics. Scaffold sampling (K tuned; best reported K=16) is used to bias prompts toward relevant exemplars.",
            "evaluation_metrics": "Chemical similarity: Fingerprint Tanimoto Similarity (FTS) using MACCS, RDK, Morgan fingerprints; Fréchet ChemNet Distance (FCD). NLP/string metrics: BLEU, Exact Match, Levenshtein distance. Chemical validity: RDKit validity checks. Reported use of MACCS FTS, FCD, BLEU, Exact Match, Levenshtein and RDKit validity percentages.",
            "results_summary": "FrontierX achieved state‑of‑the‑art performance on the ChEBI-20 text2mol benchmark when using the GPT‑4 backbone with scaffold sampling (K=16). LLMs produced top‑R SMILES and explanations which, after small‑LM fine‑tuning and HMHA fusion, yielded higher chemical similarity and validity versus baselines. Ablation: removing explanatory text embedding (y_exp) reduced MACCS FTS by 17.21%, BLEU by 16.43%, and Validity by 13.12%; removing original text embedding (y_org) produced ~20.69% MACCS FTS drop, ~20.91% BLEU drop, and ~16.00% Validity drop; removing prediction embedding (y_pred) gave more modest drops (~8.44% MACCS FTS, ~11.63% BLEU, ~6.56% Validity). Training/hyperparameters (for the downstream modules) included batch size 32, 100 epochs, embedding dimension d=128, H=4 attention heads, Adam lr=1e-3 with decay and early stopping.",
            "comparison_to_other_methods": "Outperformed baseline models reported in prior work (MolT5/T5 variants, RNN‑GRU, Vanilla Transformer) and few‑shot GPT prompting baselines; paper reports superior chemical similarity and validity when FrontierX uses GPT‑4 backbone and scaffold sampling, and reports that scaffold sampling &gt; random sampling for prompt demonstrations.",
            "limitations_and_challenges": "LLMs are used as black boxes (no access to logits/token embeddings), so the pipeline must rely on textual outputs and cannot fine‑tune the LLMs; computational/resource costs (LMaaS, inference cost) and context length limits (typical LLM context limit ~4096 tokens) constrain demonstration count and prompt size; SMILES ambiguities (multiple valid SMILES, implicit hydrogens) make precise SMILES generation hard; LLMs can hallucinate or misinterpret SMILES grammar; the paper notes the need for better integration with cheminformatics tools like RDKit and improved LLM handling of chemical structure syntax. Novelty metrics (e.g., fraction of novel molecules) are not provided.",
            "other_notes": "The pipeline explicitly does not fine‑tune LLM parameters; it leverages LLM outputs to supervise/fine‑tune smaller, accessible LMs for embedding extraction and downstream decoding.",
            "uuid": "e8920.0",
            "source_info": {
                "paper_title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A transformer‑based large language model (used as the primary backbone in FrontierX experiments) queried via LMaaS with knowledge‑augmented prompts to produce ranked SMILES predictions and textual explanations without parameter updates.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_type": "Transformer-based LLM (GPT family)",
            "model_size": "Not specified in paper for GPT-4 (other models in table report sizes); treated as an off‑the‑shelf backbone.",
            "training_data": "Pretrained on large public/private text corpora (not re-trained here). Used with ChEBI-20 demonstrations in prompts for zero/few‑shot generation.",
            "application_domain": "Text2mol: translating technical textual descriptions to SMILES (de novo molecule generation).",
            "generation_method": "Knowledge‑augmented prompting (task instructions + scaffold‑sampled demonstrations, K=16 in best experiments); LLM returns top‑R SMILES (R=4) and textual justifications via LMaaS API, no fine‑tuning of GPT‑4 parameters.",
            "novelty_of_chemicals": "Not quantified for GPT‑4 outputs specifically; emphasis on producing valid and correct SMILES that match ground truth more closely than baselines.",
            "application_specificity": "Relevant demonstrations selected via scaffold sampling; prompt includes explicit instruction to produce SMILES for the description, and GPT‑4 outputs are filtered/validated by downstream modules and RDKit validity checks.",
            "evaluation_metrics": "FTS (MACCS, RDK, Morgan), FCD, BLEU, Exact Match, Levenshtein, RDKit validity (as used in the pipeline evaluation).",
            "results_summary": "GPT‑4 produced the best performance among tested LLM backbones in FrontierX (highest similarity and validity), and the GPT‑4 backbone + scaffold K=16 combination achieved SOTA on ChEBI‑20 in the reported experiments.",
            "comparison_to_other_methods": "Performed better than GPT‑3.5, text‑davinci‑003, and Google Bard when used in the FrontierX pipeline; overall FrontierX with GPT‑4 outperformed MolT5 and other baselines.",
            "limitations_and_challenges": "No access to logits or internal embeddings via LMaaS, black‑box nature limits explainability; cost/resource considerations; context length limitations (prompts capped ~4096 tokens) constrain demonstration count; LLM errors handling SMILES syntax and multiple SMILES representations remain an issue.",
            "uuid": "e8920.1",
            "source_info": {
                "paper_title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "GPT-3.5-turbo / text-davinci-003",
            "name_full": "GPT-3.5‑turbo and text‑davinci‑003 (OpenAI GPT‑3 family models)",
            "brief_description": "GPT‑3 family models (evaluated as alternative LLM backbones) were prompted in zero/few‑shot settings with knowledge‑augmented prompts to output ranked SMILES and explanations; they performed worse than GPT‑4 but were still viable for the pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5‑turbo; text‑davinci‑003",
            "model_type": "Transformer-based LLMs (GPT family)",
            "model_size": "Paper lists text‑davinci‑003 and ChatGPT (GPT‑3.5) with 175B in table; exact effective sizes and GPT‑3.5 specifics not further detailed.",
            "training_data": "Pretrained on large text corpora; no task fine‑tuning performed in study; used in prompts constructed from ChEBI‑20 demonstrations.",
            "application_domain": "Text2mol molecule generation from textual descriptions.",
            "generation_method": "Knowledge‑augmented prompt few‑shot (scaffold or random sampling) via LMaaS; LLM returns top‑R SMILES predictions and textual explanations; R=4 used in experiments.",
            "novelty_of_chemicals": "Not reported.",
            "application_specificity": "Prompted with task instructions + sampled demonstrations; scaffold sampling improves relevance for these models as well.",
            "evaluation_metrics": "Same metrics as other LLMs in the paper (FTS, FCD, BLEU, Exact Match, Levenshtein, RDKit validity).",
            "results_summary": "GPT‑3.5 and davinci variants performed worse than GPT‑4 in FrontierX experiments; they still produced useful SMILES and explanations but with lower validity/similarity metrics. The paper also uses few‑shot (K=5,10) and zero‑shot variants of these models as baselines.",
            "comparison_to_other_methods": "Inferior to GPT‑4 backbone in FrontierX; compared against MolT5 and other baselines but did not surpass the best reported model.",
            "limitations_and_challenges": "Same black‑box, token/context limit, and SMILES ambiguity issues; performance sensitive to number and quality of demonstrations (scaffold sampling helps).",
            "uuid": "e8920.2",
            "source_info": {
                "paper_title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Google Bard",
            "name_full": "Google Bard (BARD)",
            "brief_description": "A large-scale transformer‑based LLM evaluated as an off‑the‑shelf backbone in the pipeline; it was outperformed by GPT family models on the text2mol benchmarks in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Google Bard",
            "model_type": "Transformer-based LLM",
            "model_size": "Paper's table lists Bard with '1,560B' (as reported in their Table 1); no further sizing details provided in text.",
            "training_data": "Pretrained on large text corpora; used with knowledge‑augmented prompts sampled from ChEBI‑20 (no fine‑tuning).",
            "application_domain": "Text2mol: translating textual descriptions to SMILES",
            "generation_method": "Knowledge‑augmented prompting (scaffold/random sampling) via LMaaS style API interactions; outputs top‑R SMILES + explanations.",
            "novelty_of_chemicals": "Not reported.",
            "application_specificity": "Same prompt engineering and scaffold sampling strategies as for other LLMs; performed worse than GPT models on same prompt/demonstration settings.",
            "evaluation_metrics": "FTS, FCD, BLEU, Exact Match, Levenshtein, RDKit validity (as for other models).",
            "results_summary": "Consistently outperformed by the GPT models in this study when given the same demonstrations and prompts; generated fewer valid SMILES and lower similarity scores.",
            "comparison_to_other_methods": "Performed worse than GPT‑4/GPT‑3.5 in the FrontierX experiments; scaffold sampling improved performance but not to parity with GPT‑4.",
            "limitations_and_challenges": "Similar limitations: black‑box nature, context length limits, SMILES handling issues; in experiments showed lower task performance relative to GPT‑4.",
            "uuid": "e8920.3",
            "source_info": {
                "paper_title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "DeBERTa (LM_exp / LM_org)",
            "name_full": "DeBERTa (Decoding‑enhanced BERT with disentangled attention)",
            "brief_description": "A smaller pre‑trained language model fine‑tuned in this work as LM_exp and LM_org to produce contextual token embeddings from (a) LLM‑generated explanations and (b) original textual molecule descriptions, respectively, providing inputs for the attention fusion and decoder.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeBERTa (fine‑tuned as LM_exp and LM_org)",
            "model_type": "Transformer encoder (BERT family variant) used as a small LM for fine‑tuning and embedding extraction",
            "model_size": "Paper's Table 1 lists DeBERTa vocabulary/size ~50M (reported as 50M).",
            "training_data": "Pretrained on large text corpora (standard DeBERTa pretraining). In this study, fine‑tuned on two datasets derived from ChEBI‑20: (1) LLM‑generated auxiliary explanations (S_exp) and (2) original technical descriptions (S_org).",
            "application_domain": "Domain adaptation for text2mol: produce context‑aware token and text‑level embeddings used downstream to generate SMILES.",
            "generation_method": "Fine‑tuning (standard supervised fine‑tune) on textual data (generated explanations and original descriptions) to obtain contextual embeddings; these embeddings are aggregated via softmax attention into fixed text‑level embeddings y_exp and y_org.",
            "novelty_of_chemicals": "Not applicable (DeBERTa is an embedding/fine‑tuning component, not a molecule generator itself).",
            "application_specificity": "Domain customization achieved by fine‑tuning DeBERTa on task‑specific textual data (LLM explanations and original descriptions), enabling contextualized token embeddings tailored to the molecule description domain.",
            "evaluation_metrics": "Indirect: improvements measured by downstream SMILES generation metrics (FTS, FCD, BLEU, Validity) when these embeddings are included. Ablation removing y_exp or y_org severely degrades performance.",
            "results_summary": "Fine‑tuned DeBERTa representations (y_exp and y_org) are critical: ablation removing y_exp or y_org caused large drops in chemical similarity, BLEU, and validity (examples: ~17–21% drops in MACCS FTS and ~13–16% drops in validity), demonstrating that small LM domain adaptation using LLM outputs materially improves generation.",
            "comparison_to_other_methods": "Using fine‑tuned small LMs to ingest LLM explanations and original text is a key design choice versus directly attempting to fine‑tune LLMs (which was not done); this yielded better task performance than baselines that do not combine LLM explanations with small‑LM fine‑tuning.",
            "limitations_and_challenges": "Quality of LLM‑generated explanations conditions the fine‑tuning; if explanations are low quality or incorrect, downstream embeddings and generation can degrade. Fine‑tuning requires labeled downstream data and compute (but is cheaper than fine‑tuning full LLMs).",
            "uuid": "e8920.4",
            "source_info": {
                "paper_title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "MolT5 (baseline)",
            "name_full": "MolT5 (T5‑based model pretrained for text2mol)",
            "brief_description": "A prior encoder‑decoder transformer model pretrained specifically for text‑to‑molecule translation that the paper uses as a primary baseline; FrontierX reports outperforming MolT5 on ChEBI‑20.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MolT5",
            "model_type": "Encoder‑decoder transformer (based on T5 architecture)",
            "model_size": "Not specified in this paper (baseline imported from earlier work).",
            "training_data": "Pretrained on large unannotated datasets for text2mol pretraining (per original MolT5 work); baseline results used from prior publications.",
            "application_domain": "Text2mol translation (textual molecular descriptions → SMILES).",
            "generation_method": "Supervised encoder‑decoder sequence‑to‑sequence generation (pretrained and fine‑tuned for text2mol).",
            "novelty_of_chemicals": "Not reported in this paper (baseline metrics taken from prior studies).",
            "application_specificity": "Trained specifically for text2mol tasks; uses supervised training rather than LLM prompting.",
            "evaluation_metrics": "Evaluated in prior work and compared here using FTS, FCD, BLEU, Exact Match, Levenshtein, RDKit validity.",
            "results_summary": "FrontierX (with GPT‑4 backbone and scaffold sampling) outperformed MolT5 on the ChEBI‑20 benchmark according to the paper's reported comparisons.",
            "comparison_to_other_methods": "MolT5 and T5 variants are the main monitored baselines; FrontierX claims superior results by leveraging LLM outputs + small‑LM fine‑tuning + HMHA fusion.",
            "limitations_and_challenges": "As a supervised transformer specialized via pretraining, MolT5 requires task‑specific pretraining/fine‑tuning; the paper positions FrontierX as more flexible by using off‑the‑shelf LLMs with prompt engineering plus small‑LM adaptation.",
            "uuid": "e8920.5",
            "source_info": {
                "paper_title": "Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Text2mol: Cross-modal molecule retrieval with natural language queries",
            "rating": 2
        },
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2
        },
        {
            "paper_title": "What indeed can gpt models do in chemistry? a comprehensive benchmark on eight tasks",
            "rating": 2
        },
        {
            "paper_title": "MolT5: Text-to-molecule translation model (original MolT5 paper / associated T5 text-to-text transformer work)",
            "rating": 1
        },
        {
            "paper_title": "Fréchet ChemNet Distance: a metric for generative models for molecules in drug discovery",
            "rating": 1
        }
    ],
    "cost": 0.0184215,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design
18 Aug 2024</p>
<p>Sakhinana Sagar sagar.sakhinana@tcs.com 
Venkataramana Runkana venkat.runkana@tcs.com 
Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design
18 Aug 2024ABB281DE0331ECFB4A7C44CE5E4F3374arXiv:2408.11866v1[cs.CL]
Molecule design is a multifaceted approach that leverages computational methods and experiments to optimize molecular properties, fast-tracking new drug discoveries, innovative material development, and more efficient chemical processes.Recently, text-based molecule design has emerged, inspired by next-generation AI tasks analogous to foundational vision-language models.Our study explores the use of knowledge-augmented prompting of large language models (LLMs) for the zero-shot text-conditional de novo molecular generation task.Our approach uses task-specific instructions and a few demonstrations to address distributional shift challenges when constructing augmented prompts for querying LLMs to generate molecules consistent with technical descriptions.Our framework proves effective, outperforming state-of-the-art (SOTA) baseline models on benchmark datasets.* Designed and programmed research, conducted experiments, analyzed results, and drafted manuscript Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models at NeurIPS 2023.</p>
<p>Introduction</p>
<p>Molecule design is an interdisciplinary approach that involves identifying a target molecule or property to enhance, such as a drug with increased efficacy or a material with superior characteristics.Advancements in science and technology have accelerated the discovery and development of novel drugs, advanced materials, and innovative chemical processes.This iterative process begins with (a) identifying a target molecule or property to improve, followed by (b) employing computational methods to explore the vast chemical space and optimize potential candidate structure and composition.The cycle continues with (c) synthesizing and testing promising candidates in the laboratory until the desired characteristics are achieved.The transformer architecture [28] has revolutionized various fields in computer science, including language understanding [5], text generation [17,2], image understanding [7], and multi-modal generation [19,23].Utilizing this architecture to scale language models has established itself as a universal approach for enhancing generalization performance.In recent times, the emergence of foundational Large Language Models (LLMs) [2,3,27], which are built upon transformer architectures, has significantly revolutionized performance in various natural language processing tasks by enabling enhanced linguistic comprehension and logical reasoning abilities.Different learning strategies such as Zero-Shot Chain of Thought (Zero-shot-CoT [29]) and Few-Shot (In-Context) Learning (Few-shot-ICL [22,6]) are utilized to leverage the emerging abilities of general-purpose LLMs for a wide variety of specialized tasks across various domains.The former employs task-specific instructions without relying on downstream task-based demonstrations, utilizing the inherent knowledge that the language model acquired during training to generate outputs.In contrast, Few-shot-ICL supplements instructions with a handful of demonstrations, presented as input-output pairs, to foster contextual understanding and facilitate task-specific adaptation, thereby generating relevant output.Recently, there has been a surge in the evolution of generative AI, such as "DALL•E" [19,20] from OpenAI -a text-to-image diffusion model that can generate realistic images from text descriptions, and "Make-A-Video" [24] from Meta AI -a text-to-video diffusion model that generates realistic, engaging, and creative videos from text, among others.Inspired by recent developments in next-generation AI, "Text-Based Molecule Design" [8] (also known as text2mol) represents a novel cross-domain task in chemistry that involves generating chemical SMILES representations from the corresponding technical descriptions of molecules expressed in natural language.Unlike traditional methods of de novo molecule generation, the text2mol task extracts information from technical descriptions of molecules, identifying aspects such as the specified structure, properties and functional groups, to generate chemical SMILES representations with desired characteristics.Existing models [8,10] in the literature for the text2mol task face challenges in achieving optimal performance and utility, particularly in scenarios where data is scarce and unbalanced.LLMs like ChatGPT [2], while proficient in linguistic comprehension, are black-box in nature, resource-intensive, and lack interpretability.Smaller language models(LMs) like BERT [5], although flexible and interpretable, may lag in reasoning and generalization, resulting in less coherent and contextually relevant responses compared to LLMs.Navigating these challenges requires a delicate balance between performance, efficiency, and interpretability.Our study introduces a novel approach for the text2mol task by combining the strengths of both LLMs and small-scale LMs.LLMs predict a ranked list of chemical SMILES representations while providing explanations as justifications for these predictions, conditioned on the input prompt.These textual explanations, in conjunction with original technical descriptions of molecules, are used to fine-tune small-scale LMs to obtain context-aware token embeddings that capture the essence of both the generated explanations and original text, respectively.Concurrently, the top-ranked predictions generated by LLMs are transformed to obtain prediction embeddings.By integrating these various embeddings through a hierarchical multi-head attention mechanism, the framework inputs a unified cross-modal embedding into a transformer decoder to generate chemical SMILES representations that align with original technical descriptions.In this study, we explored the use of knowledge-augmented LLM prompting for zero-shot text-conditional molecule generation, a sequence-to-sequence cross-domain task.We present a powerful new tool, FrontierX: LLM-MG, where the goal is to task LLMs with a knowledge-infused prompt that consists of a few demonstrations(input-output pairs) for the text2mol task, along with task-specific instructions, where the output is chemical SMILES representations of the corresponding query technical descriptions.Our experiments on benchmark datasets provide empirical evidence supporting the framework's effectiveness in text-based molecule design tasks.The workflow of the proposed approach is illustrated in Figure 1.</p>
<p>LLM</p>
<p>LM Technical Descriptions</p>
<p>Top-R Ranked Predictions</p>
<p>Textual Explanations</p>
<p>Cross-Modal Encoder</p>
<p>Prediction</p>
<p>Encoder</p>
<p>Transformer Decoder</p>
<p>SMILES Representations</p>
<p>Figure 1: Overview of the FrontierX: LLM-MG framework.We construct knowledge-augmented prompts using task-specific instructions and a few demonstrations (input-output pairs) based on the downstream task.The augmented prompt queries LLMs to generate the top-R predictions of the SMILES representations and produces textual explanations as justifications for its predictions.We fine-tune small-scale pre-trained language models (LMs) on the generated explanations for domainspecific customization to obtain context-aware token embeddings.We utilize a weighted-sum pooling attention mechanism for task-specific adaptation to compute contextualized text-level embeddings.</p>
<p>In parallel, we transform the LLMs' top-R predictions to compute prediction embeddings.The cross-modal encoder, modeled by a hierarchical multi-head attention mechanism, computes the unified embeddings by integrating the mono-domain text-level embeddings (both the original text and explanatory text) and prediction embeddings.Finally, the transformer decoder generates the chemical SMILES representations.We do not repurpose LLMs by fine-tuning with labeled data for domain customization.Instead, we access LLMs via LMaaS [25] using text-based API interaction.</p>
<p>Proposed Method</p>
<p>The Large Language Models (LLMs), such as ChatGPT [2], Meta's LLaMA [27] -that have been pre-trained on large text corpora and operate based on a "prompt and predict" approach (utilizing natural language prompts to generate the subsequent contextual word or phrase, aligning with human-like responses) -have revolutionized language modeling with their proficiency in linguistic comprehension and advanced logical reasoning abilities, providing improved performance on generalpurpose NLP tasks.While LLMs are inherently black box in nature, they possess remarkable capabilities.However, their widespread adoption for applications in various downstream tasks is hindered by the unavailability of logits or token embeddings, which limits explainability.Additionally, they require significant computational resources for fine-tuning on labeled data for task-specific adaptation or for repurposing for domain-customization.In contrast, the small-scale language models (LMs), such as BERT [5] and DeBERTa [11], following a "pre-train, fine-tune" approach, offer more affordable flexibility for fine-tuning with minimal labeled data and provide access to logits or token embeddings, aiding interpretability.While smaller LMs can learn complex patterns, they often fall short in reasoning and generalization abilities compared to LLMs, which generate more coherent and contextually relevant responses.To alleviate resource constraints, Language Modeling as a Service (LMaaS [25]) offers access to LLMs through text-based API interactions, while remaining scalable and cost-effective.However, the potential of LLMs for text-conditional de novo molecular generation tasks remains largely underexplored.Our proposed approach for the text2mol task leverages LLMs by utilizing: (a) their predictive ability to provide a top-R ranked list of chemical SMILES representations; and (b) their generative ability to offer auxiliary explanations as justifications for their predictions by conditioning on the augmented prompt.Furthermore, we fine-tune two different small-scale LMs using (a) generated explanations from LLMs and (b) input technical descriptions of molecules to compute their respective contextualized token embeddings -which capture semantic coherence and contextual relevance for text-to-molecule generation tasks.We utilize weighted attention mechanism to compute both original and explanatory text-level embeddings from their respective context-aware token embeddings.In addition, we transform the LLMs' top-R predictions of chemical SMILES representations into predictive embeddings.We use a hierarchical multi-head attention mechanism to integrate various embeddings into unified cross-modal embedding for input into a transformer decoder, generating the chemical SMILES representation.</p>
<p>Evaluation LLMs &amp; LMs: In this work, we evaluated three popular LLMs: text-davinci-0032 , ChatGPT3 , and Google BARD 4 , in order to thoroughly compare their distinct strengths.text-davinci-003 was the earliest LLM released by OpenAI and was tailored for a broad spectrum of linguistic tasks.GPT-3.5-turbo is a substantial improvement over the GPT-3 base models, demonstrating remarkable performance on a wide range of linguistic tasks while also being cost-effective.Google BARD [1] stands out due to its extraordinary scale, complexity, and an impressively extensive vocabulary compared to the GPT-3.5 models.In addition to these, our study also incorporates a pre-trained smaller LM, DeBERTa5 , which is an improved version of the BERT [5] architecture.Table 1 presents a comprehensive summary of the technical specifications of these language models.Knowledge-Augmented Prompts: In our work, we offer essential context and task-specific instructions by using input natural language descriptions of the target molecule to prompt LLMs in a zero-shot setting to generate corresponding chemical SMILES representations.In this scenario, the primary task-specific instructions involve the translation of these descriptions into chemical SMILES representations.We create an augmented prompt that incorporates both the task-specific instructions and a few demonstrations.These demonstrations, which establish the context, are grounded in the downstream text2mol task and comprise input-output pairs (i.e., technical descriptions and their corresponding chemical SMILES representations).This approach facilitates knowledge-augmented prompting of the LLMs for zero-shot text-to-molecule generation tasks.The construction of an augmented prompt involves sampling text-molecule pairs from the training data that are relevant to the target molecule descriptions.We then prepend these pairs to the task-specific instructions to form an augmented prompt, which is used to query the LLMs in a zero-shot setting for the generation of chemical SMILES representations.To evaluate the impact of the quality and quantity of sampled text-molecule pairs on the performance of text-conditional de novo molecule generation tasks, we employ two different sampling strategies.The quality of these pairs is determined by the sampling methods used to identify pairs similar to the target molecule descriptions.We navigate through the training dataset using two semantic search-retrieval methodologiesrandom and scaffold -to sample text-molecule pairs relevant to the target molecule descriptions.The random approach involves arbitrarily sampling K text-molecule pairs from the training dataset.In contrast, the scaffold technique employs semantic similarity methods, specifically text-embedding-ada-002 from Ope-nAI 6 , to evaluate the similarity between the molecular textual descriptions in the training dataset and the target molecule descriptions.It then selects the top-K most relevant text-molecule pairs, where the hyperparameter K is set using a random search technique.We employ the different sampling strategies to analyze the effectiveness of augmenting prompts with relevant text-molecule pairs in language-conditioned molecule generation tasks.In short, unlike traditional supervised learning, LLMs (a) predict the chemical SMILES representations and (b) generate textual explanations for their predictions, utilizing the inherent knowledge embedded within the language model's parameters, all conditioned on the augmented prompt, without needing any parameter updates.</p>
<p>Querying LLMs: We access LLMs with LMaaS [25] platforms via text-based API interaction, necessitating solely text-based input and output.We create a customized zero-shot prompt template to query LLMs to translate textual descriptions into chemical SMILES representations.The LLMs' response serves the dual purpose of (a) providing detailed textual explanations for the underlying rationale, (reasoning or logic), behind the predictions and (b) generating a list of the top-R ranked chemical SMILES representations.Subsequently, we fine-tune smaller downstream LMs using the generated auxiliary explanations.The custom augmented prompt format is as follows:</p>
<p>Below are the textual descriptions -chemical SMILES representation pairs.Generate the chemical SMILES representation for the textual description provided below.</p>
<p>Querying LLMs (a) predicts the top-R ranked chemical SMILES representations and (b) provides auxiliary explanations as logical justifications for its predictions.</p>
<p>(LLMs Response) [top-R ranked predictions -Auxiliary Explanations] In the next section, we will discuss the use of auxiliary explanations and original textual descriptions for fine-tuning various downstream smaller LMs for domain customization.Later, we will transform the LLMs top-R predictions of chemical SMILES representations into predictive embeddings.Fine-tuning LMs for Domain-Specific Customization: Our novel approach leverages the integration of a smaller language model (LM) to extract relevant information from the original molecular textual descriptions and auxiliary explanations generated by LLMs, thereby aiding downstream tasks.The intermediary LM serves as a bridge between the LLM and the downstream layers that generate chemical SMILES representations.To elucidate further, we fine-tune pre-trained LMs, denoted as LM exp and LM org , to compute context-aware token embeddings by passing the text sequences generated by LLMs (referred to as S exp ) and original textual descriptions (referred to as S org ) through the LM exp and LM org models, respectively, as described below:
h exp = LM exp (S exp ) ∈ R (m×d) ; h org = LM org (S org ) ∈ R (n×d)(1)
where both contextualized embeddings h exp and h org capture not only the contextual information of the tokens but also encapsulate the semantic relationships among tokens within their respective textual content.Here, m and n represent the number of tokens in S exp and S org , while d represents the token embedding dimension.We employ a softmax attention mechanism to compute a weighted sum of the contextualized token embeddings, encoding the auxiliary explanations and original textual descriptions into single fixed-length vectors or embeddings denoted as y exp and y org and computed as follows,
α i = softmax(q i ); q i = u T h (i) exp || β i = softmax(r i ); r i = v T h (i) org (2)y exp = m i=0 α i h (i) exp ∈ R (d) ; y org = n i=0 β i h (i) org ∈ R (d)(3)
where u and v are differentiable vectors.The explanatory text-level embedding, represented as y exp , encapsulates domain-specific knowledge retrieved from foundational LLMs to support its predictions.</p>
<p>The original text-level embedding, denoted as y org , captures the overall context and semantics within the original textual descriptions by extracting the most pertinent and task-relevant information.</p>
<p>LLMs Prediction Embeddings: As mentioned earlier, the LLMs not only provide the auxiliary textual explanations but also predict the top-R ranked chemical SMILES representations list, which can be informative.For each target molecule in the text2mol task, the top-R predictions are converted into one-hot encoded vectors p i,1 , . . ., p i,R ∈ R C , where C represents the total number of elements in the SMILES vocabulary, encompassing a wide range of characters and symbols used to represent chemical structures.These vectors are subsequently concatenated into a single RC-dimensional vector, and finally, they undergo linear encoding into a fixed-length prediction embedding y pred ∈ R d , encapsulating the top-R predictions from the LLMs.Cross-modal Attention Layer We compute the cross-modal embedding, denoted as y cross , using a hierarchical multi-head attention mechanism that integrates the original text-level embedding y org , the explanatory text-level embedding y exp , and the prediction embedding y pred .This mechanism provides a robust framework for integrating diverse information encapsulated from different modalities, addressing several key aspects critical to the performance of cross-modal learning tasks.It involves hierarchical implementation of multi-head attention mechanisms.We employ two layers, each focusing on different aspects of the input embeddings, enabling more complex interactions and potentially leading to more scalable and efficient models.In the initial layer, we apply a multi-head attention mechanism to the mono-domain embeddings, specifically the original text-level embedding y org and the explanatory text-level embedding y exp , to obtain unified mono-domain embeddings denoted as y uni .In the subsequent layer, we utilize the multi-head attention mechanism on the cross-domain embeddings, comprising the prediction embedding y pred and the unified mono-domain embeddings y uni , to compute the cross-modal embeddings denoted as y cross .For the first layer, we compute the Query, Key, Value projections for the original text-level embedding y org for each head h as follows:
Q h org = y org W h Qorg ; K h org = y org W h Korg ; V h org = y org W h Vorg (4)
Similarly, the Query, Key, Value projections for explanation text-level embedding y exp for each head h as follows:
Q h exp = y exp W h Qexp ; K h exp = y exp W h Kexp ; V h exp = y exp W h Vexp (5)
We concatenate the keys and values from both original and explanatory text-level embeddings, which provides a powerful way to integrate information from the mono-domain embeddings into a unified, rich representation.
K h concat = [K h org , K h exp ]; V h concat = <a href="6">V h org , V h exp </a>
We use softmax attention to integrate complementary information from the mono-domain embeddings, focus on contextually relevant information, and semantically align them through an attention mechanism.The softmax function is applied to the keys for each query.
A h uni = Softmax (Q h org + Q h exp )K h concat T √ d h(7)
Each head outputs a new vector representation that highlights the most relevant features in the monodomain embeddings(both original and explanation text-level), according to the attention mechanism for that specific head, which is tailored to capture specific aspects or relationships within the data.
O h uni = A h uni V h concat (8)
Finally, all the head-specific outputs are concatenated and linearly transformed to create the unified mono-domain embedding as follows,
O concat = <a href="9">O 1 uni , O 2 uni , . . . , O H uni </a>y uni = O concat W Ouni (10)
where
W h Qorg , W h Korg , W h Vorg , W h Vexp , W h Qexp , W h
Kexp , W Ouni are the learnable weight matrices.Here, d h represents the dimensionality of the key/query/value for each head, and H is the number of heads.y uni denotes the unified mono-domain embeddings.The unified embeddings can learn and integrate complementary, diverse information present in both the y org and y exp embeddings.These unified embeddings facilitate semantic alignment among similar features across different embeddings and enable the identification of contextual relevance between distinct yet related y org and y exp monodomain embeddings.The next step involves computing the cross-modal embedding y cross using a second layer of a multihead attention mechanism that integrates both y pred and y uni .We compute the Query, Key, and Value projections for the prediction embedding y pred for each head h as follows:
Q h pred = y pred W h Qpred ; K h pred = y pred W h Kpred ; V h pred = y pred W h Vpred (11)
Similarly, we compute the Query, Key, Value projections for the unified embedding y uni for each head h as follows:
Q h uni = y uni W h Quni ; K h uni = y uni W h Kuni ; V h uni = y uni W h Vuni (12)
We concatenate the keys and values from both the prediction and unified embeddings, thereby facilitating a robust integration of insights from the cross-domain embeddings into a synergized and enriched representation.
K h cross = [K h uni , K h pred ]; V h cross = <a href="13">V h uni , V h pred </a>
We utilize a softmax attention mechanism to merge and align information from different domains, thereby prioritizing contextually relevant information and ensuring semantic alignment.The softmax v function is applied to the keys for each query, described as follows:
A h cross = Softmax (Q h uni + Q h pred )K h cross T √ d h(14)
In the multi-head attention mechanism, each head processes both embeddings(unified and predictive embeddings) to highlight important patterns, focusing on specific relationships or aspects within the data, enhancing performance in cross-modal learning tasks.
O h cross = A h cross V h cross (15)
Finally, all the head-specific outputs are concatenated and linearly transformed to create the final cross-modal embedding as follows,
O cross = <a href="16">O 1 cross , O 2 cross , . . . , O H cross </a>y cross = O cross W Ocross (17)
where
W h Quni , W h Kuni , W h Vuni , W h Vpred , W h Qpred , W h
Kpred , W Ocross are the learnable weight matrices.y cross denotes the cross-domain embeddings.Implementing the hierarchical attention mechanism facilitates the structured integration of information from different modalities.This mechanism employs multihead attention method, using multiple sets of learned weight matrices to emphasize various aspects or relationships within the data.Consequently, this approach has the potential to foster robust and enriched embeddings capable of capturing complex patterns.Additionally, it aids in focusing on contextually pertinent information and achieving semantic alignment across different embeddings, thereby enhancing the capacity to identify and utilize crucial features in the input data.Output Layer: We then utilize a transformer decoder [28] to generate chemical SMILES representations character by character, using cross-modal embeddings (y cross ) that incorporate global context through the hierarchical multi-head self-attention mechanism.We implement a softmax layer to transform the decoder's output, creating a probability distribution over potential elements for each position in the SMILES strings.For our sequence generation tasks, we minimize the categorical cross-entropy loss to penalize the proposed framework based on the negative log-likelihood of the ground-truth chemical SMILES strings under the predicted probability distribution, thus facilitating the generation of valid molecules.In summary, by integrating multi-modal embeddings, namely y org , y exp , and y pred , our approach enables the concurrent capture of complementary information, ultimately enhancing the overall performance of the framework.</p>
<p>Experiments &amp; Results</p>
<p>Datasets &amp; Baselines</p>
<p>Our study utilized the ChEBI-20 dataset [8], a bidirectional text-to-molecule translation dataset comprising 33,010 text description-molecule pairs with a predefined split ratio of 80:10:10 for training, validation, and test sets, respectively.We utilized 26,407 text description-molecule pairs from the training set for demonstrations (input-output mappings) for constructing a knowledgeaugmented prompt to query LLMs.We used the MolT5 model [8], as a predominant baseline, which is an encoder-decoder transformer architecture pretrained on a large unannotated dataset specifically for the text2mol translation task, building upon the foundations of the T5 [18] model.We evaluated the performance of our proposed framework on the text2mol task, comparing it with several variants of the MolT5 [8] and T5 [18] models, as well as with general-purpose sequence-to-sequence models, such as the RNN-GRU and Vanilla Transformer models.In addition, various variants of few-shot (ICL) prompting of GPT-based models -reflecting the fact that this technique uses few-shot learning to prompt off-the-shelf GPT-based models to perform molecular property prediction for new, unseen molecules -referred to as baselines, are evaluated for comparison with our proposed framework.The configurations include different variants of the GPT-4 model, namely, (a) the zero-shot approach, (b) the scaffold sampling technique with K=10 or K=5, and (c) the random sampling technique with K=10 for constructing augmented prompts.In addition, we use GPT-3.5 and davinci-003 models, both employing the scaffold sampling technique with K=10 to construct knowledge-augmented prompts.For more details and information on the baselines, please refer to the earlier works [10,8].</p>
<p>Evaluation Metrics</p>
<p>To comprehensively evaluate the quality and similarity of the generated chemical SMILES representations compared to the ground-truth SMILES representations, we employed a range of distinct evaluation metrics, categorized into three types.These metrics include (a) chemical similarity measures, such as the FTS (Fingerprint Tanimoto Similarity) [26] and the FCD (Fréchet ChemNet Distance) [16], as well as (b) natural language processing metrics like the BLEU (Bilingual Evaluation Understudy) score, Exact Match [8], and Levenshtein distance [14].In addition, (c) we utilized the RDKit library [13] to validate the generated molecules.We delineate the metrics as follows: (a) We vi employ the FTS [26] metric to gauge the chemical similarity between the ground-truth and generated chemical compounds represented as SMILES strings(notation to represent chemical structures as text) by comparing their MACCS, RDK, and Morgan fingerprints [21,13,4].(b) In addition, we utilize the FCD metric [16], which leverages latent information from a pretrained model [16] to predict molecular activity [8].The FCD is calculated by measuring the distance between the mean embeddings of two sets of chemical SMILES strings (generated and ground-truth) in the latent space of the pretrained model.A lower FCD score indicates a greater similarity between the corresponding two sets of molecules.(c) We also apply natural language processing metrics to evaluate the quality of the chemical SMILES strings generated by our framework.These metrics encompass the following: (i) BLEU -this measures the similarity between two text strings, with a higher BLEU score denoting better similarity.(ii) Exact Match [8] -this quantifies the percentage of generated chemical SMILES strings that are identical to the ground-truth strings.(iii) Levenshtein distance [14] -this calculates the minimum number of single-character edits required to modify the generated chemical SMILES strings to match the ground-truth strings, with a lower value indicating closer similarity.By utilizing these diverse metrics, we attain a nuanced understanding of the efficacy of our text-conditional de novo molecule generation framework.Higher FTS scores and lower FCD scores signify better chemical similarity and closer molecular activity resemblance, respectively.In the context of matching chemical SMILES strings from the perspective of natural language processing, higher BLEU and Exact Match scores are preferred to achieve better alignment with the ground-truth SMILES strings, while a lower Levenshtein distance indicates fewer required edits, denoting superior similarity.The RDKit library assists in verifying the validity of the generated molecules, with a higher proportion indicating successful generation [13].</p>
<p>Experimental Setup</p>
<p>We used the ChEBI-20 dataset [9] with an 80:10:10 split: 80% for training, 10% for validation, and 10% for testing.The training set was utilized to update the learnable parameters, the validation set to select optimal hyperparameters, and the test set to evaluate generalization performance of the proposed framework.Our scalable and efficient framework offers a unified solution for integrating LLMs and LMs.We configured the hyperparameters of our framework with a batch size of 32, trained it for 100 epochs, and a hidden or embedding dimension(d) of 128.Additional hyperparameters include the number of attention heads (H) set to 4, and the dimensionality of Key/Query/Value (d h ) is 32.To optimize the training process, we utilized the Adam optimizer [12], initially setting the learning rate to 1e −3 .Additionally, we incorporated a learning rate decay scheduler, which reduced the learning rate by half whenever the validation loss did not improve for 10 consecutive epochs.Furthermore, we applied early stopping to prevent overfitting on the validation data.We evaluated our approach using the following LLMs: GPT-4.0,GPT-3.5-turbo,GPT-3.0-text-davinci-003, and Google Bard.In our approach, we chose not to fine-tune hyperparameters individually for each LLM, opting instead to maintain consistent settings across all language models.This strategy simplifies experimentation, ensures uniform conditions, facilitates result comparison, and promotes consistency.Moreover, it underscores the versatility of our framework, which can be used with any off-the-shelf LLM without the need for computationally expensive hyperparameter tuning.We used the Scaffold technique with K=16 to sample demonstrations (input-output mappings) from the training data to construct augmented prompts for querying LLMs in few-shot settings.In addition, we query LLMs to generate the top-R ranked chemical SMILES strings predictions list and set the hyperparameter R as 4. To maximize computational resource utilization, we harnessed eight V100 GPUs, each equipped with 8 GB of GPU memory, for training deep learning models built upon the PyTorch framework.Considering the context length limitations imposed by LLMs, which restrict the maximum sequence length that a typical LLM can process at a time to 4096 tokens, we implemented strategies to mitigate the high computational costs associated with prompting LLMs.This approach included running each experiment twice and reporting the average results.Our approach prioritizes both resource optimization and accuracy, aiming to achieve the best possible outcomes while minimizing the computational footprint.Our evaluation incorporated several metrics, and we present the results for the test datasets and compare the performance against well-known baselines.</p>
<p>Results</p>
<p>The experimental results of the proposed framework and the baseline models performance on the text2mol task are presented in Tables 2 and 3.The results of the baseline models are reported from earlier studies [10,8].The results undeniably demonstrate the superior performance of the FrontierX: LLM-MG framework, especially when combined with the GPT-4 backbone and employing the Scaffold technique with K set to 16.This optimal combination excels in generating accurate molecular structures that closely resemble the ground truth, surpassing all baseline models across On the ChEBI-20 dataset [9], we observe differing impacts on framework performance when certain methods are omitted.The "w/o y exp " variant shows a substantial decline in performance relative to the baseline, as evidenced by a significant drop of 17.21% in MACCS FTS, 16.43% in BLEU, and 13.12% in Validity.Similarly, the "w/o y org " variant performs much worse than the baseline, with a remarkable drop of 20.69% in MACCS FTS, 20.91% in BLEU, and 16.00% in Validity.In contrast, the "w/o y pred " variant exhibits a marginally inferior performance compared to the baseline, with a modest drop of 8.44% in MACCS FTS, 11.63% in BLEU, and 6.56% in Validity.The significant drop in performance metrics for the ablated variants, when compared to the baseline, highlights the considerable impact of the mechanisms inherent in the methods omitted from the baseline and leads to degraded performance.Our experiments corroborate our hypothesis of joint optimization to obtain a cross-modal embeddings, y cross , through a hierarchical multi-head attention mechanism that integrates the original text-level embeddings y org , explanatory text-level embeddings y exp , and prediction embeddings y pred , achieving state-of-the-art (SOTA) performance on the text2mol task</p>
<p>Conclusion</p>
<p>In this study, we pioneered the text2mol approach, inaugurating a transformative paradigm where chemistry meets language models, expediting scientific advancements.Through the creation of FrontierX: LLM-MG, we demonstrated the efficacy of using large language models for seamless and efficient translation between textual descriptions and chemical SMILES representations.Acknowledging the limitations of current methods, our research highlights a promising horizon in molecule design, potentially ushering in an era of accelerated innovation and interdisciplinary collaboration.Our study illustrates the transformative impact of integrating molecular design with language models, offering an innovative approach to molecule generation that can catalyze groundbreaking developments in science and technology.</p>
<p>Technical Appendix</p>
<p>Study of Knowledge-Augmented Prompting</p>
<p>In our study, we employ knowledge-augmented prompting with LLMs for text-tomolecule(text2mol) translation task by leveraging the pre-existing knowledge embedded within the language model parameters.LLMs are capable of generating chemical SMILES representations from textual descriptions through entity recognition, grammar understanding, symbol mapping, and structure validation, which marks significant progress in molecule generation via language models.This knowledge-augmentation prompting technique allows LLMs to adapt to new, unseen molecule textual descriptions using a few task-specific demonstrations, thereby eliminating the need for fine-tuning with labeled data for task-specific adaptation.The approach involves creating knowledge-augmented prompts that combine task-specific instructions with demonstrations (input-output pairs) sampled from training data relevant to the target molecule textual descriptions determined using off-the-shelf semantic similarity techniques.In this context, each pair consists of a textual description of a molecule (input) and its corresponding SMILES representation (output), where the task-specific instruction is to convert the target molecule textual descriptions into the standardized chemical SMILES notation.This approach aligns the LLM's capabilities with the text2mol task by crafting knowledge-augmented prompts that blend specific instructions with relevant demonstrations, selected based on semantic similarity.This strategic alignment facilitates accurate chemical SMILES strings generation without necessitating language model parameter updates.We have employed two sampling strategiesrandom and scaffold -to evaluate the impact of both the quality and quantity of demonstrations in the knowledge-augmented prompt, which is utilized for querying LLMs during text-based de novo molecule generation.The scaffold strategy utilizes a semantic similarity method to sample the top-K relevant text-molecule pairs, using OpenAI's text-embedding-ada-002 technique 7 .The random technique involves the arbitrary selection of K text-molecule pairs without any prior knowledge in a non-deterministic manner.The study compares the effectiveness of both strategies in enhancing the language-conditioned molecule generation task using off-the-shelf pre-trained LLMs, including Google Bard and other GPT model family variants.We conducted experiments to compare and contrast the performance of the "Random" and "Scaffold" sampling strategies, and to identify the optimal number of demonstrations.Table 5: The table shows the results of the experimental study examining the impact of both quantity and quality of demonstrations on knowledge-augmented prompting strategies in the text2mol task.Results: Table 5 presents the results of the experimental study that examined the effects of both the quantity and quality of demonstrations on the performance of knowledge-augmented prompting strategies in the text2mol task.Our study compared the performance of various GPT models with that of Google Bard on the ChEBI-20 dataset [9].The results indicated that the GPT models consistently outperformed Google Bard across all evaluation metrics when provided with the same number of task-specific demonstrations in the augmented prompt.Notably, GPT-4 demonstrated the highest performance among the tested models, generating a greater number of valid chemical SMILES representations.Furthermore, the study indicates that enhancing the knowledge-augmented prompt with more task-specific demonstrations directly improves the predictive accuracy of language models.This highlights a positive correlation between the number of task-specific demonstrations and the performance of LLMs on the text2mol task.The study found that scaffold sampling consistently outperforms random sampling on the text2mol task when using any off-the-shelf LLMs.One possible reason for this superior performance is the strong textual similarities between the text-molecule pairs sampled using the scaffold technique and the target molecule descriptions.Therefore, using scaffold sampling instead of random sampling may lead GPT models to generate more accurate chemical SMILES representations.LLMs continue to face challenges in precisely interpreting molecular representations in chemical SMILES notations, resulting in poor performance on text2mol tasks.SMILES representations can possess multiple valid forms and implicit hydrogen atoms, causing ambiguity and presenting difficulties for LLMs.Improved LLMs capable of handling molecular structures and seamlessly integrating with tools like RDKit are necessary 5.2 Impact of Hierarchical Multi-Head Attention(HMHA) Mechanism</p>
<p>In our work, we compute the cross-modal embedding, denoted as y cross , through a hierarchical multi-head attention (HMHA) mechanism that integrates the original text-level embedding (y org ), explanatory text-level embedding (y exp ), and prediction embedding (y pred ).To determine the impact of the HMHA mechanism on the framework performance, we conducted ablation study.We refer to the ablated variant without the HMHA mechanism as "w/o HMHA".We substitute the HMHA mechanism with dual-stage linear operators in the ablated variant to compute cross-modal embeddings.</p>
<p>The findings of the ablation study are summarized in Table 6.We conducted the experiment using the FrontierX: LLM-MG framework with GPT-4 backbone, where we replaced the HMHA mechanism with linear operators, as discussed earlier.The experimental results support the inclusion of the hierarchical multi-head attention mechanism (HMHA) to generate cross-modal embeddings, aiding in the generation of more valid chemical SMILES representations in the text2mol task.</p>
<p>Table 6: The table shows the experimental findings of the study on the impact of the HMHA mechanism on the text2mol task.The experiments were conducted using the FrontierX: LLM-MG framework with a GPT-4 backbone.We utilized the Scaffold sampling technique with K = 16 for constructing augmented prompts.</p>
<p>representative benchmark dataset.We report the results for the near-optimal combinations of the hyperparameters.</p>
<p>Molecule captioning</p>
<p>Molecule is a crucial task in the field of computational chemistry, serving as a bridge between complex chemical data and human comprehension.It involves generating detailed and correct textual descriptions that accurately describe a chemical SMILES representation in the mol2text task.This stands in contrast to the text2mol task, which entails generating chemical SMILES representations from detailed and factual textual descriptions.Meanwhile, the mol2text task helps to translate complex chemical structures into understandable language, enhancing our understanding of molecules with potential applications spanning multiple fields, including drug discovery, materials science, and chemical synthesis.To evaluate the quality of the generated text in the mol2text task, we employ traditional metrics commonly used in natural language processing and machine translation, including BLEU, ROUGE, and METEOR, as described below:</p>
<p>• BLEU-2 and BLEU-4 are part of the BiLingual Evaluation Understudy (BLEU) metric family.BLEU is typically computed for different n-gram levels, where 'n' represents the number of contiguous words or tokens considered.BLEU-2 evaluates the accuracy of two-word phrases (bigrams) in generated text, while BLEU-4 extends this analysis to four-word sequences (4-grams).These metrics offer insights into the alignment between machine-generated and human reference texts.The BLEU metric variants help quantify the performance of language generation-based NLP models.</p>
<p>• ROUGE-1 and ROUGE-2 are part of the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metric family.ROUGE-1, also known as ROUGE unigram or ROUGE-N1, evaluates the overlap between generated and reference text at the unigram (single-word) level, assessing specific word choices.In contrast, ROUGE-2 (also known as ROUGE bigram or ROUGE-N2) extends this to evaluate consecutive word pairs (bigrams), offering comprehensive insights into content matching.These metrics measure alignment with reference texts at both the word and bigram levels, providing precision and recall evaluations of textual elements.</p>
<p>• ROUGE-L, or Recall-Oriented Understudy for Gisting Evaluation -Longest Common Subsequence, measures the quality of machine-generated text by considering the longest common subsequence between the generated text and a reference text.This subsequence represents a sequence of words that appear in the same order in both the generated and reference texts, allowing flexibility in word order.ROUGE-L assesses content overlap and structural similarity, capturing the core content and organization of generated text concerning the reference text, even when there are variations in wording or word order.</p>
<p>• METEOR (Metric for the Evaluation of Translation with Explicit Ordering) considers precision, recall, stemming, synonymy, and word order, offering a well-rounded evaluation of text quality by analyzing matching words and their order, providing detailed assessments for translation and captioning models.</p>
<p>FrontierX: LLM-MG -mol2text task</p>
<p>We have modified the FrontierX: LLM-MG pipeline for the text2mol task to adapt it for the mol2text task.The workflow of the proposed approach is illustrated in Figure 2. Given a chemical SMILES representation, it can generate the technical descriptions of the molecule.We construct a knowledge-infused prompt using task-specific instructions and a few demonstrations (input-output mappings) for the downstream mol2text task.The task-specific instructions involve translating chemical SMILES representations into their corresponding technical descriptions.The primary objective of the prompt engineering method is to enhance the context-awareness of language models and improve their ability to provide relevant and accurate responses.This enhancement is achieved through learning from demonstrations, rather than relying on conventional supervised learning methods of fine-tuning with labeled data for the mol2text task.The knowledge-infused prompts guide the language models to generate technical descriptions based on the provided instructions.Next, we fine-tune small-scale, pre-trained language models (LMs) using the generated explanations, which facilitates domain customization and yields context-aware token embeddings.To create a text-level embedding that encapsulates the generated technical descriptions, we utilize a weighted sum-pooling attention mechanism on the contextualized embeddings.Additionally, the unimodal encoder, which is implemented with a multi-head attention mechanism, integrates the mono-domain task-specific adaptation in downstream tasks [2]).This approach allows LLMs to acquire knowledge through analogies, relying on a limited set of input-output mappings (demonstrations) tailored to the specific downstream task.Knowledge-infused prompting harnesses the implicit knowledge embedded in pretrained LLM parameters to facilitate adaptation to new tasks via task-specific demonstrations, all without necessitating parameter updates.The Knowledge-Infused prompt provides task-specific instructions and demonstrations, allowing LLMs to generate outputs conditioned on the prompt for improved generalization performance.In the case of mol2text tasks, we construct a knowledgeinfused prompt using a few demonstrations sampled from the training data.To examine how the quality and quantity of task-specific demonstrations impact performance on mol2text tasks, we investigate two different sampling strategies.The quality of examples is determined by the retrieval techniques employed to select the top-K demonstrations (chemical SMILES strings-text data pairs) from the training set that match the query chemical SMILES representations.We explore two distinct sampling strategies: 'Random' and 'Scaffold'.To study the impact of the quantity of demonstrations on the framework's performance on the mol2text task, we optimize the number of demonstrations (K) used to construct the augmented prompt for each query chemical SMILES representation.In the 'Random' strategy, we randomly sample K demonstrations from the training data.In contrast, the 'Scaffold' strategy uses Tanimoto similarity [26] based on Morgan fingerprints [15] with a radius of 2 to identify the top-K most similar chemical SMILES representations from the training data for query chemical SMILES representations.We explore the different sampling strategies to analyze the impact of the quality of demonstrations on the mol2text task with a hypothesis that the 'Scaffold' sampling technique outperforms the 'Random' technique for the same number of demonstrations.In summary, our goal is to task LLMs with a knowledge-infused prompt that consists of a few demonstrations for the mol2text task, along with task-specific instructions, where the output is technical descriptions of the query chemical SMILES representation.The task-specific instruction in the augmented prompt guides LLMs to generate technical descriptions.This task showcases the LLM's capacity to generate textual descriptions via prompt conditioning, relying on its inherent knowledge, without requiring parameter updates, in contrast to supervised learning, which relies on labeled data for parameter updates.Tables 8 and 9 present the experimental findings on the ChEBI-20 benchmark dataset [9].We report the baseline results from earlier studies [10,8].The best performing model is in bold font.</p>
<p>Ablation Studies</p>
<p>Our proposed framework operates in a structured, multi-step pipeline.In step (a), we create knowledge-augmented prompts using task-specific instructions and demonstrations, prompting large language models (LLMs) to generate textual descriptions.In step (b), we use these generated explanations to fine-tune a smaller, pre-trained language model (LM exp ) for domain-specific customization, resulting in context-sensitive token embeddings.We employ a weighted sum-pooling attention mechanism for task-specific adaptation to compute text-level embeddings, denoted as y exp , from the contextualized token embeddings.In parallel, in step (c), we fine-tune another small-scale language model (LM org ) on query chemical SMILES representations, computing an entire chemical SMILES string embeddings y org .In step (d), our framework obtains a unimodal embedding, y uni , through a multi-head attention mechanism that integrates the original text-level embeddings y org and descriptive text-level embeddings y exp .In the final step, the transformer decoder generates the textual descriptions of the query chemical SMILES string from the unimodal embedding, y uni .Our empirical research aims to elucidate the significance and unique contributions of each method within our proposed framework, particularly in assessing the effectiveness of their learned embeddings for achieving optimal results on the mol2text task.Ablation studies have been conducted to investigate the impact of disabling individual methods on our framework's overall performance in the mol2text task.To precisely measure the impact of each method on the framework's performance, we have generated various ablated variants by disabling individual methods and assessed their performance using a benchmark dataset across multiple evaluation metrics for the mol2text task.We choose the FrontierX: LLM-MG framework as the reference baseline for ablation studies in the context of the mol2text task.Our comprehensive approach not only confirms the effectiveness of various methods but also provides substantial support for their design choices, reinforcing their rationale and justifying their inclusion in the framework.The ablated variants without the descriptive text-level embeddings and the original text-level embeddings are denoted as 'w/o y exp ' and 'w/o y org ', respectively.The findings of the ablation study are summarized in Table 10.All ablation experiments were conducted with the FrontierX: LLM-MG W/GPT-4 framework using the Scaffold sampling technique with a value of K = 16, involving the deliberate exclusion of specific methods, as previously described.On the ChEBI-20 dataset [9], the 'w/o y exp ' variant exhibits a significant decline in performance relative to the baseline, evidenced by a 10.63% drop in BLEU-2, a 14.04% drop in ROUGE-L, and a 20.93% xiv drop in METEOR.Similarly, the 'w/o y org ' variant performs much worse than the baseline, with a 20.18% drop in BLEU-2, a 25.06% drop in ROUGE-L, and a 29.06% drop in METEOR.These substantial performance drops across all evaluation metrics when comparing the ablated variants to the baseline consistently highlighting the significant impact of the mechanisms disabled from the baseline.Our experiments validate our hypothesis on joint optimization to obtain a unimodal embedding, denoted as y uni , through a multi-head attention mechanism that combines the original text-level embeddings y org and explanatory text-level embeddings y exp .This approach results in achieving state-of-the-art (SOTA) performance on the mol2text task.</p>
<p>-Scaffold, k=16, W/GPT-</p>
<p>Table 1 :
1
Technical details of LLMs and LMs.Enterprise refers to the organization that developed the language models.Cost denotes the expenses associated with using 1K tokens.Last Update Date indicates that the LLM's knowledge base is limited to information available up to that specific date.
ModelEnterpriseCostLast Update Date Vocabulary Sizetext-davinci-003Open-AI0.02$Sep. 2021175BChatGPTOpen-AI0.002$Jun. 2021175BBARDGoogleFreeUndisclosed1,560BDeBERTaHugging FaceFreeN/A50M</p>
<p>Table 10 :
10
The table shows the experimental findings on the ablation study on the mol2text task.FrontierX: LLM-MG (Scaffold, k=16, W/GPT-4) 0.743 ±0.081 0.656 ±0.097 0.818 ±0.034 0.727 ±0.013 0.783 ±0.047 0.812 ±0.051 FrontierX: LLM-MG -w/o y exp 0.664 ±0.058 0.537 ±0.071 0.675 ±0.042 0.543 ±0.035 0.673 ±0.021 0.642 ±0.075 FrontierX: LLM-MG -w/o y org 0.593 ±0.027 0.496 ±0.068 0.612 ±0.086 0.482 ±0.037 0.575 ±0.076 0.576 ±0.059
MethodBLEU-2 (↑)BLEU-4 (↑)ROUGE-1 (↑) ROUGE-2 (↑) ROUGE-L (↑) METEOR (↑)
https://platform.openai.com/docs/models/gpt-3-5
https://chat.openai.com/chat
https://bard.google.com
For more information on DeBERTa, please refer to https://huggingface.co/docs/transformers/ index[11] 
https://platform.openai.com/docs/guides/embeddings
https://platform.openai.com/docs/guides/embeddings x
various evaluation metrics.Table2: The table a performance comparison of the proposed framework and the baselines on the text2mol task.The top-performing model is highlighted in bold.The baseline results are reported from previous work[10].We leveraged the Scaffold technique, setting K to 16, to sample demonstrations and construct augmented prompts for in-context learning in all experiments involving FrontierX: LLM-MG with various off-the-shelf LLMs.Ablation StudiesOur proposed framework operates through a series of interconnected stages via a progressively structured multi-step pipeline.Beginning with step (a), we create knowledge-augmented prompts using task-specific instructions and demonstrations, prompting large language models (LLMs) to (i) generate top-ranked (top-R) SMILES strings predictions along with (ii) explanatory justifications for their predictions.In step (b), these generated explanations are used to (i) fine-tune a smaller pre-trained viii language model (LM exp ) for domain customization to obtain contextualized token embeddings and utilize (ii) a weighted sum-pooling attention to compute text-level embeddings denoted as y exp from the token embeddings for task-specific adaptation.Moving to step (c), the top-R predictions from the LLMs are transformed to compute prediction embeddings y pred .Concurrently, in step (d), we fine-tune another small-scale language model (LM org ) on the original textual descriptions of molecules to compute context-aware token embeddings, and then compute the original text-level embeddings y org through a weighted attention mechanism.In step (e), our proposed framework obtains a cross-modal embeddings, y cross , through a hierarchical multi-head attention mechanism that integrates the original text-level embeddings y org , explanatory text-level embeddings y exp , and prediction embeddings y pred .We conduct empirical research to understand the significance and contribution of each distinct method within the proposed framework, evaluating its learned embeddings to achieve optimal results.We perform ablation studies to assess the impact of disabling individual methods on the overall performance of our framework.To determine the contribution of each method to the framework's performance, we create various ablated variants by disabling individual methods and evaluate them using benchmark datasets for text2mol tasks.We choose the proposed FrontierX: LLM-MG framework as the reference baseline for the ablation studies.Our robust strategy not only validates the efficacy of the diverse methods but also substantiates the rationale, providing a strong basis for their design choices and justifying their inclusion within the framework.The ablated variants without the explanatory text-level embeddings, prediction embeddings, and original text-level embeddings are referred to as "w/o y exp ", "w/o y pred ", and "w/o y org ", respectively.The ablation study findings are summarized in Table4.All the ablation study experiments were conducted with the FrontierX: LLM-MG framework using the GPT-4 backbone and Scaffold sampling technique with K = 16, by disabling certain methods as discussed earlier.Hyperparameter TuningTo enhance the performance of our FrontierX: LLM-MG framework, we embarked on meticulous hyperparameter tuning through detailed experimentation and analysis.We chose random search as a strategy to adeptly navigate the hyperparameter space, pinpointing the optimal framework configuration on the benchmark dataset, in lieu of more computationally intensive approaches such as grid search or Bayesian optimization.This strategy enabled us to obtain optimal results on the validation subset of the benchmark dataset, as evidenced by several evaluation metrics.We conducted hyperparameter optimization on the FrontierX: LLM-MG-W/GPT-4 variant of our framework.We utilized the Scaffold sampling technique with K = 16 for constructing augmented prompts.The primary key hyperparameters within this framework include batch size (b ∈ {32, 48, 64}) and the embedding dimension (d ∈ {64, 128, 196, 256}).Table7presents the results of hyperparameter tuning on the chemical SMILES representations with explanatory text-level embeddings to compute unimodal embeddings.Finally, the transformer decoder generates the technical descriptions that to the input chemical SMILES representations[28].Instead of repurposing large language models (LLMs) by either retraining them from scratch or fine-tuning them with labeled data for domain customization, we employ LMaaS[25]to engage with LLMs through text-based API interactions.LLM LMSMILESLLM PromptingKnowledge-infused LLM prompting, a method of prompt engineering, involves crafting effective prompts or input queries to elicit desired responses from language models.This technique enhances language models by combining their natural language understanding and generation capabilities with access to external factual information(demonstrations), making them more versatile for taskspecific applications.Consequently, it enables the LLM to generate responses enriched with accurate, contextually relevant information.Knowledge-infused prompting enables LLMs to adapt to new tasks without the need for explicit, gradient-based fine-tuning with gold-standard annotated data for
. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Evaluation of gpt-3.5 and gpt-4 for supporting real-world information needs in healthcare delivery. Debadutta Dash, Rahul Thapa, Juan M Banda, Akshay Swaminathan, Morgan Cheatham, Mehr Kashyap, Nikesh Kotecha, Jonathan H Chen, Saurabh Gombar, Lance Downing, arXiv:2304.137142023arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.119292020arXiv preprint</p>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji, arXiv:2204.11817Translation between molecules and natural language. 2022arXiv preprint</p>
<p>Text2mol: Cross-modal molecule retrieval with natural language queries. Carl Edwards, Chengxiang Zhai, Heng Ji, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>What indeed can gpt models do in chemistry? a comprehensive benchmark on eight tasks. Taicheng Guo, Kehan Guo, Zhengwen Liang, Zhichun Guo, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2305.183652023arXiv preprint</p>
<p>Deberta: Decoding-enhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, arXiv:2006.036542020arXiv preprint</p>
<p>P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Rdkit: Open-source cheminformatics software. G A Landrum, 2020</p>
<p>Levenshtein distance: Information theory, computer science, string (computer science), string metric, damerau? levenshtein distance, spell checker, hamming distance. Frederic P Miller, Agnes F Vandome, John Mcbrewster, 2009</p>
<p>The generation of a unique machine description for chemical structuresa technique developed at chemical abstracts service. Harry L Morgan, Journal of chemical documentation. 521965</p>
<p>Fréchet chemnet distance: a metric for generative models for molecules in drug discovery. Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, Gunter Klambauer, Journal of chemical information and modeling. 5892018</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Hierarchical text-conditional image generation with clip latents. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXiv:2204.061252022arXiv preprint</p>
<p>Zero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, International Conference on Machine Learning. PMLR2021</p>
<p>Pattern matching: The gestalt approach. David Ratcliff, John W Metzener, 1988</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, arXiv:2112.086332021arXiv preprint</p>
<p>Photorealistic text-to-image diffusion models with deep language understanding. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar, Seyed Ghasemipour, Burcu Karagol Ayan, Sara Mahdavi, Rapha Gontijo Lopes, arXiv:2205.114872022arXiv preprint</p>
<p>Make-a-video: Text-to-video generation without text-video data. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, arXiv:2209.147922022arXiv preprint</p>
<p>Black-box tuning for language-model-as-a-service. Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, Xipeng Qiu, International Conference on Machine Learning. PMLR2022</p>
<p>Elementary mathematical theory of classification and prediction. T Taffee, Tanimoto, Journal of Biomedical Science and Engineering. 1958</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>            </div>
        </div>

    </div>
</body>
</html>
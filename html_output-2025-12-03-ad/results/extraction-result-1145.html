<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1145 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1145</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1145</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-38186882</p>
                <p><strong>Paper Title:</strong> An Adaptive Exploration-Exploitation Algorithm for Constructing Metamodels in Random Simulation Using a Novel Sequential Experimental Design</p>
                <p><strong>Paper Abstract:</strong> In this article, we propose a novel algorithm for sequential design of metamodels in random simulation, which combines the exploration capability of most one-shot space-filling designs with the exploitation feature of common sequential designs. The algorithm continuously maintains a balance between the exploration and the exploitation search throughout the search process in a sequential and adaptive manner. The numerical results indicate that the proposed approach is superior to one of the existing well-known sequential designs in terms of both the computational efficiency and speed in generating efficient experimental designs.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1145.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1145.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DHASD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delaunay-Hybrid Adaptive Sequential Design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential adaptive experimental-design algorithm that uses Delaunay triangulation to partition the input space, computes exploration scores (normalized triangle area) and exploitation scores (vertex response variation), and adaptively weights them (α) to select the next sample at the winning triangle centroid; uses Kriging as the metamodel and variance-proportional replication to handle noise.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DHASD (Delaunay-Hybrid Adaptive Sequential Design)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Starts from a space-filling Latin hypercube initial design (LHD + optional corner points), evaluates noisy stochastic simulations with variance-proportional replications, fits a Kriging metamodel, constructs a Delaunay triangulation over current samples, computes per-triangle exploration (θ: normalized area) and exploitation (δ: normalized total vertex-response variation) scores, forms a total score = α·θ + (1−α)·δ, selects the centroid of the highest-scoring triangle, and updates the metamodel; includes escaping strategies (minimum-distance thresholds, k-nearest-sum threshold with stochastic α adjustment, and prohibition of tiny-area triangles).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Adaptive sequential active sampling combining space-filling exploration and exploitation via an adaptively weighted score (α); an active-learning / adaptive sampling scheme (not a Bayesian acquisition in formal sense) using triangulation-based region selection.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adaptive weight α is initialized (α0) and decreased while cross-validation error improves; if error fails to improve after a prespecified number of iterations α is increased until improvement or α reaches 1 (then restart with α0=1). The algorithm uses 5-fold cross-validation RRSE as feedback for adaptation; replication counts per point r_i are set proportionally to estimated variance (r_i ∝ σ_i^2) to equalize average-response variance across points. Escaping strategies alter α stochastically (multiplicative random factors) when candidate points are too close to existing samples.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>15 benchmark test functions with added stochastic noise (TP1–TP15)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown black-box response surfaces with additive Gaussian observation noise (variance = 0.1·|f(x)|), continuous input domains (dimension D varies per test problem), stochastic (noisy outputs), only pointwise noisy observations available (partial observability of true surface), functions ranging from smooth/unimodal to highly nonlinear/multimodal.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Test suite of 15 problems spanning low-dimensional continuous domains (typical D = 2 for many standard test functions, although D can vary per problem); function complexity ranges from smooth single-mode to highly multi-modal and rugged surfaces; no explicit action/state counts—sample selection is continuous-point queries; per-problem sample budgets in experiments ranged from tens to a few hundred samples (see Table 2 per problem).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>DHASD reached the prespecified cross-validation accuracy with fewer samples than the SUMO benchmark in 14 of 15 test problems (Table 2). Example: TP1 (Ackley) best run: DHASD 316 samples (avg 317.1, std 2.77) vs SUMO best 445 (avg 446, std 1.82). Overall MSE/ARE on independent 30x30 grids: DHASD outperformed SUMO in 10/15 problems (Table 3). (See per-problem numeric results in Tables 2 & 3 of the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline (SUMO with LOLA-Voronoi default) typically required more samples to reach the same cross-validation threshold; in most cases SUMO built models with more sample points and often worse or comparable MSE/ARE. Exception: TP2 where SUMO needed slightly fewer samples (SUMO avg 69.9 vs DHASD avg 72.1) and achieved better MSE/ARE in some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Required sample counts varied by problem: in experiments the number of samples to reach desired accuracy ranged roughly from 20 up to ~317 (per-problem averages reported in Table 2); DHASD typically used tens to low hundreds of samples and was consistently more sample-efficient than SUMO on most problems (14/15 for cross-validation target).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Mechanism: per-triangle exploration score θ_i = (area_i / area_max)*10 and exploitation score δ_i = (variation_i / δ_max)*10; combined via total score S_i = α·θ_i + (1−α)·δ_i. α is adapted online using cross-validation error feedback (decrease α when validation error improves to emphasize exploitation; increase α after failure to improve to emphasize exploration). Additional stochastic perturbations to α and distance-based filters prevent clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared experimentally to SUMO (LOLA-Voronoi default + error-based sampler). Related methods mentioned in literature review: Expected Improvement (EI, Jones et al. 1998), batch sequential strategies (Loeppky et al.), adaptive gridding (Busby), Voronoi-based decompositions (Crombecq et al.), bootstrap/jackknife variance-based selectors (Kleijnen & van Beers), etc.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) The adaptive α weighting (DHASD) maintains a dynamic balance between exploration and exploitation and is robust to initial α (ANOVA showed no significant sensitivity across α0 in tests). 2) DHASD reached target cross-validation accuracy with fewer samples than the SUMO toolbox in 14/15 benchmark problems. 3) On independent test grids DHASD produced lower MSE/ARE than SUMO in 10/15 problems while using fewer samples. 4) The algorithm concentrates samples in high-curvature regions (exploitation) while retaining coverage elsewhere (exploration) via triangulation and adaptive α. 5) Replication allocation proportional to estimated local variance reduces noise impact and improves metamodel quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>1) Greater variability in sample counts across replications (larger std) attributed to adaptive α randomness. 2) One benchmark (TP2) where SUMO slightly outperformed DHASD in sample count and MSE/ARE. 3) Several hyperparameters (η1, η2, k, rand intervals, prespecified number of runs for α increase) chosen by trial-and-error; tuning may be required for new problems. 4) Potential clustering risk if escaping strategies not effective. 5) Computational overhead from repeated Kriging fits, Delaunay triangulation, and cross-validation; scaling to high-dimensional spaces may require modifications (corner points optionally dropped when initial size grows with D).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1145.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1145.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SUMO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SUrrogate MOdeling toolbox (SUMO) - default sampling: LOLA-Voronoi + error-based selector</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MATLAB toolbox for surrogate modeling and adaptive sampling; used here as the experimental benchmark with its default sampling strategy (LOLA-Voronoi combined with an error-based sample selector and Kriging metamodels).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A surrogate modeling and adaptive sampling toolbox for computer based design.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SUMO (LOLA-Voronoi default)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Toolbox implementing multiple adaptive sampling strategies; the academic version used (SUMO 7.0.2) applies Kriging metamodels and a default LOLA-Voronoi sampling scheme combined with error-based sample selection to choose new sample points.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>LOLA-Voronoi (Voronoi-based decomposition + local linear approximation score) combined with an error-based sample selector; a hybrid exploration/exploitation adaptive sampling method but with non-adaptive fixed weighting in SUMO's default setting.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Evaluates candidate regions using LOLA-Voronoi criteria and an error-based estimator to pick samples; weightings between exploration and exploitation are implicit in the LOLA-Voronoi heuristic and, as used in the experiments, are not adaptively reweighted based on cross-validation error.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same 15 benchmark test functions with additive Gaussian noise (TP1–TP15)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Black-box continuous functions with additive stochastic noise used for surrogate modeling; noisy, unknown surfaces similar to DHASD experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same suite of problems as DHASD; required sample budgets often larger than DHASD's to reach comparable cross-validation thresholds (per Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>SUMO (LOLA-Voronoi default) typically required more samples than DHASD to reach the same cross-validation accuracy (Table 2); e.g., TP1 average samples 446 vs DHASD 317. On MSE/ARE metrics SUMO outperformed DHASD in 5 of 15 problems but generally used more samples to do so.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Generally less sample-efficient than DHASD on these benchmarks (required tens to hundreds of samples depending on problem; see per-problem values in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>LOLA-Voronoi combines exploration (Voronoi-based cell size) and exploitation (local linear approximation / error) into a score for candidate cells; in the version used the balance is fixed by algorithm design rather than adaptively tuned by cross-validation feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly to DHASD in this paper. SUMO implements methods described in Crombecq et al. (2009) and Gorissen et al. (2010a).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>SUMO served as a strong benchmark; DHASD achieved similar or better metamodel accuracy with fewer samples in most problems, indicating advantages for adaptive α weighting and region-selection via triangulation. SUMO produced lower variability (std) in sample counts across replications.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>As used, SUMO assumes static weighting between exploration and exploitation across problems (no online α adaptation), which can be suboptimal on surfaces requiring a changing balance. In several problems SUMO required many more samples to reach the same cross-validation error.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1145.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1145.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOLA-Voronoi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LOLA-Voronoi sequential design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid sequential sampling strategy that decomposes the domain by Voronoi tessellation and combines local linear approximation (LOLA) exploitation scores with Voronoi-based exploration scores to select new samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A novel sequential design strategy for global surrogate modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LOLA-Voronoi</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Builds a Voronoi decomposition of candidate points/cells, computes exploration scores from Voronoi cell sizes and exploitation scores via local linear approximation error, and selects new samples by combining those scores—used as SUMO's default strategy in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Hybrid exploration/exploitation adaptive sampling based on Voronoi tessellation and local linear approximation error.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Combines fixed-form exploration and exploitation scores per cell; in the SUMO implementation used as a default, weights are not adaptively tuned by cross-validation feedback within a run (unlike DHASD's α).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mentioned in literature and used inside SUMO; applied to black-box surrogate modeling benchmarks (similar to the TP suite).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Designed for continuous black-box functions; handles noisy or deterministic outputs depending on implementation; aims for global surrogate accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Applicable across a range of problem complexities (smooth to multimodal); complexity depends on problem dimension and surface ruggedness.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balances exploration (large Voronoi cells) and exploitation (cells with high local linear approximation error); combination typically fixed by the algorithm design unless an outer adaptation layer is added.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Cited as the core of SUMO's default strategy; compared in the paper indirectly via SUMO performance against DHASD.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>LOLA-Voronoi is an effective hybrid sequential design; in this paper SUMO's LOLA-Voronoi baseline was outperformed (in sample efficiency) by DHASD on most test functions—authors attribute DHASD advantage to its adaptive weighting of exploration vs exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>When used with fixed weights it can be suboptimal on surfaces whose optimal exploration/exploitation balance changes over time; may require larger sample budgets vs an adaptively reweighted scheme.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1145.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1145.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expected Improvement (EI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Improvement acquisition function</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian optimization acquisition criterion that selects new points by maximizing the expected improvement over the current best observed value under a surrogate (often Gaussian process).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient global optimization of expensive black-box functions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Expected Improvement (EI)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Acquisition function from Gaussian-process-based global optimization (Jones et al. 1998) that trades off exploration and exploitation by quantifying the expected gain relative to current best; mentioned in the literature review as a classical sequential design approach.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization acquisition (Expected Improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses GP posterior mean and variance to compute the expected improvement at candidate points; selects points that maximize EI (implicit adaptive balance via predictive uncertainty).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Mentioned as applicable to expensive black-box functions (deterministic or stochastic with modifications).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Black-box, expensive-to-evaluate functions; original EI assumes deterministic or low-noise contexts, but extensions handle stochastic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Typically used on low-to-moderate dimensional continuous domains; computational cost scales with GP fitting and acquisition optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicit in the EI formula: high predictive mean favors exploitation while high predictive variance favors exploration; no hand-tuned α is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Cited historically (Jones et al. 1998) and contrasted with the paper's DHASD approach in literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Mentioned as a foundational sequential design approach; authors cite EI as part of historical development of sequential metamodeling methods but do not experimentally compare EI in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not evaluated in this paper; original EI may need adaptation for noisy stochastic simulations (extensions exist).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A novel sequential design strategy for global surrogate modeling. <em>(Rating: 2)</em></li>
                <li>Efficient global optimization of expensive black-box functions. <em>(Rating: 2)</em></li>
                <li>A surrogate modeling and adaptive sampling toolbox for computer based design. <em>(Rating: 2)</em></li>
                <li>Hierarchical adaptive experimental design for Gaussian process emulators. <em>(Rating: 1)</em></li>
                <li>Application-driven sequential designs for simulation experiments: Kriging metamodeling. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1145",
    "paper_id": "paper-38186882",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "DHASD",
            "name_full": "Delaunay-Hybrid Adaptive Sequential Design",
            "brief_description": "A sequential adaptive experimental-design algorithm that uses Delaunay triangulation to partition the input space, computes exploration scores (normalized triangle area) and exploitation scores (vertex response variation), and adaptively weights them (α) to select the next sample at the winning triangle centroid; uses Kriging as the metamodel and variance-proportional replication to handle noise.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "DHASD (Delaunay-Hybrid Adaptive Sequential Design)",
            "agent_description": "Starts from a space-filling Latin hypercube initial design (LHD + optional corner points), evaluates noisy stochastic simulations with variance-proportional replications, fits a Kriging metamodel, constructs a Delaunay triangulation over current samples, computes per-triangle exploration (θ: normalized area) and exploitation (δ: normalized total vertex-response variation) scores, forms a total score = α·θ + (1−α)·δ, selects the centroid of the highest-scoring triangle, and updates the metamodel; includes escaping strategies (minimum-distance thresholds, k-nearest-sum threshold with stochastic α adjustment, and prohibition of tiny-area triangles).",
            "adaptive_design_method": "Adaptive sequential active sampling combining space-filling exploration and exploitation via an adaptively weighted score (α); an active-learning / adaptive sampling scheme (not a Bayesian acquisition in formal sense) using triangulation-based region selection.",
            "adaptation_strategy_description": "Adaptive weight α is initialized (α0) and decreased while cross-validation error improves; if error fails to improve after a prespecified number of iterations α is increased until improvement or α reaches 1 (then restart with α0=1). The algorithm uses 5-fold cross-validation RRSE as feedback for adaptation; replication counts per point r_i are set proportionally to estimated variance (r_i ∝ σ_i^2) to equalize average-response variance across points. Escaping strategies alter α stochastically (multiplicative random factors) when candidate points are too close to existing samples.",
            "environment_name": "15 benchmark test functions with added stochastic noise (TP1–TP15)",
            "environment_characteristics": "Unknown black-box response surfaces with additive Gaussian observation noise (variance = 0.1·|f(x)|), continuous input domains (dimension D varies per test problem), stochastic (noisy outputs), only pointwise noisy observations available (partial observability of true surface), functions ranging from smooth/unimodal to highly nonlinear/multimodal.",
            "environment_complexity": "Test suite of 15 problems spanning low-dimensional continuous domains (typical D = 2 for many standard test functions, although D can vary per problem); function complexity ranges from smooth single-mode to highly multi-modal and rugged surfaces; no explicit action/state counts—sample selection is continuous-point queries; per-problem sample budgets in experiments ranged from tens to a few hundred samples (see Table 2 per problem).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "DHASD reached the prespecified cross-validation accuracy with fewer samples than the SUMO benchmark in 14 of 15 test problems (Table 2). Example: TP1 (Ackley) best run: DHASD 316 samples (avg 317.1, std 2.77) vs SUMO best 445 (avg 446, std 1.82). Overall MSE/ARE on independent 30x30 grids: DHASD outperformed SUMO in 10/15 problems (Table 3). (See per-problem numeric results in Tables 2 & 3 of the paper.)",
            "performance_without_adaptation": "Baseline (SUMO with LOLA-Voronoi default) typically required more samples to reach the same cross-validation threshold; in most cases SUMO built models with more sample points and often worse or comparable MSE/ARE. Exception: TP2 where SUMO needed slightly fewer samples (SUMO avg 69.9 vs DHASD avg 72.1) and achieved better MSE/ARE in some metrics.",
            "sample_efficiency": "Required sample counts varied by problem: in experiments the number of samples to reach desired accuracy ranged roughly from 20 up to ~317 (per-problem averages reported in Table 2); DHASD typically used tens to low hundreds of samples and was consistently more sample-efficient than SUMO on most problems (14/15 for cross-validation target).",
            "exploration_exploitation_tradeoff": "Mechanism: per-triangle exploration score θ_i = (area_i / area_max)*10 and exploitation score δ_i = (variation_i / δ_max)*10; combined via total score S_i = α·θ_i + (1−α)·δ_i. α is adapted online using cross-validation error feedback (decrease α when validation error improves to emphasize exploitation; increase α after failure to improve to emphasize exploration). Additional stochastic perturbations to α and distance-based filters prevent clustering.",
            "comparison_methods": "Compared experimentally to SUMO (LOLA-Voronoi default + error-based sampler). Related methods mentioned in literature review: Expected Improvement (EI, Jones et al. 1998), batch sequential strategies (Loeppky et al.), adaptive gridding (Busby), Voronoi-based decompositions (Crombecq et al.), bootstrap/jackknife variance-based selectors (Kleijnen & van Beers), etc.",
            "key_results": "1) The adaptive α weighting (DHASD) maintains a dynamic balance between exploration and exploitation and is robust to initial α (ANOVA showed no significant sensitivity across α0 in tests). 2) DHASD reached target cross-validation accuracy with fewer samples than the SUMO toolbox in 14/15 benchmark problems. 3) On independent test grids DHASD produced lower MSE/ARE than SUMO in 10/15 problems while using fewer samples. 4) The algorithm concentrates samples in high-curvature regions (exploitation) while retaining coverage elsewhere (exploration) via triangulation and adaptive α. 5) Replication allocation proportional to estimated local variance reduces noise impact and improves metamodel quality.",
            "limitations_or_failures": "1) Greater variability in sample counts across replications (larger std) attributed to adaptive α randomness. 2) One benchmark (TP2) where SUMO slightly outperformed DHASD in sample count and MSE/ARE. 3) Several hyperparameters (η1, η2, k, rand intervals, prespecified number of runs for α increase) chosen by trial-and-error; tuning may be required for new problems. 4) Potential clustering risk if escaping strategies not effective. 5) Computational overhead from repeated Kriging fits, Delaunay triangulation, and cross-validation; scaling to high-dimensional spaces may require modifications (corner points optionally dropped when initial size grows with D).",
            "uuid": "e1145.0"
        },
        {
            "name_short": "SUMO",
            "name_full": "SUrrogate MOdeling toolbox (SUMO) - default sampling: LOLA-Voronoi + error-based selector",
            "brief_description": "A MATLAB toolbox for surrogate modeling and adaptive sampling; used here as the experimental benchmark with its default sampling strategy (LOLA-Voronoi combined with an error-based sample selector and Kriging metamodels).",
            "citation_title": "A surrogate modeling and adaptive sampling toolbox for computer based design.",
            "mention_or_use": "use",
            "agent_name": "SUMO (LOLA-Voronoi default)",
            "agent_description": "Toolbox implementing multiple adaptive sampling strategies; the academic version used (SUMO 7.0.2) applies Kriging metamodels and a default LOLA-Voronoi sampling scheme combined with error-based sample selection to choose new sample points.",
            "adaptive_design_method": "LOLA-Voronoi (Voronoi-based decomposition + local linear approximation score) combined with an error-based sample selector; a hybrid exploration/exploitation adaptive sampling method but with non-adaptive fixed weighting in SUMO's default setting.",
            "adaptation_strategy_description": "Evaluates candidate regions using LOLA-Voronoi criteria and an error-based estimator to pick samples; weightings between exploration and exploitation are implicit in the LOLA-Voronoi heuristic and, as used in the experiments, are not adaptively reweighted based on cross-validation error.",
            "environment_name": "Same 15 benchmark test functions with additive Gaussian noise (TP1–TP15)",
            "environment_characteristics": "Black-box continuous functions with additive stochastic noise used for surrogate modeling; noisy, unknown surfaces similar to DHASD experiments.",
            "environment_complexity": "Same suite of problems as DHASD; required sample budgets often larger than DHASD's to reach comparable cross-validation thresholds (per Table 2).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "SUMO (LOLA-Voronoi default) typically required more samples than DHASD to reach the same cross-validation accuracy (Table 2); e.g., TP1 average samples 446 vs DHASD 317. On MSE/ARE metrics SUMO outperformed DHASD in 5 of 15 problems but generally used more samples to do so.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Generally less sample-efficient than DHASD on these benchmarks (required tens to hundreds of samples depending on problem; see per-problem values in Table 2).",
            "exploration_exploitation_tradeoff": "LOLA-Voronoi combines exploration (Voronoi-based cell size) and exploitation (local linear approximation / error) into a score for candidate cells; in the version used the balance is fixed by algorithm design rather than adaptively tuned by cross-validation feedback.",
            "comparison_methods": "Compared directly to DHASD in this paper. SUMO implements methods described in Crombecq et al. (2009) and Gorissen et al. (2010a).",
            "key_results": "SUMO served as a strong benchmark; DHASD achieved similar or better metamodel accuracy with fewer samples in most problems, indicating advantages for adaptive α weighting and region-selection via triangulation. SUMO produced lower variability (std) in sample counts across replications.",
            "limitations_or_failures": "As used, SUMO assumes static weighting between exploration and exploitation across problems (no online α adaptation), which can be suboptimal on surfaces requiring a changing balance. In several problems SUMO required many more samples to reach the same cross-validation error.",
            "uuid": "e1145.1"
        },
        {
            "name_short": "LOLA-Voronoi",
            "name_full": "LOLA-Voronoi sequential design",
            "brief_description": "A hybrid sequential sampling strategy that decomposes the domain by Voronoi tessellation and combines local linear approximation (LOLA) exploitation scores with Voronoi-based exploration scores to select new samples.",
            "citation_title": "A novel sequential design strategy for global surrogate modeling.",
            "mention_or_use": "mention",
            "agent_name": "LOLA-Voronoi",
            "agent_description": "Builds a Voronoi decomposition of candidate points/cells, computes exploration scores from Voronoi cell sizes and exploitation scores via local linear approximation error, and selects new samples by combining those scores—used as SUMO's default strategy in the paper's experiments.",
            "adaptive_design_method": "Hybrid exploration/exploitation adaptive sampling based on Voronoi tessellation and local linear approximation error.",
            "adaptation_strategy_description": "Combines fixed-form exploration and exploitation scores per cell; in the SUMO implementation used as a default, weights are not adaptively tuned by cross-validation feedback within a run (unlike DHASD's α).",
            "environment_name": "Mentioned in literature and used inside SUMO; applied to black-box surrogate modeling benchmarks (similar to the TP suite).",
            "environment_characteristics": "Designed for continuous black-box functions; handles noisy or deterministic outputs depending on implementation; aims for global surrogate accuracy.",
            "environment_complexity": "Applicable across a range of problem complexities (smooth to multimodal); complexity depends on problem dimension and surface ruggedness.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": "Balances exploration (large Voronoi cells) and exploitation (cells with high local linear approximation error); combination typically fixed by the algorithm design unless an outer adaptation layer is added.",
            "comparison_methods": "Cited as the core of SUMO's default strategy; compared in the paper indirectly via SUMO performance against DHASD.",
            "key_results": "LOLA-Voronoi is an effective hybrid sequential design; in this paper SUMO's LOLA-Voronoi baseline was outperformed (in sample efficiency) by DHASD on most test functions—authors attribute DHASD advantage to its adaptive weighting of exploration vs exploitation.",
            "limitations_or_failures": "When used with fixed weights it can be suboptimal on surfaces whose optimal exploration/exploitation balance changes over time; may require larger sample budgets vs an adaptively reweighted scheme.",
            "uuid": "e1145.2"
        },
        {
            "name_short": "Expected Improvement (EI)",
            "name_full": "Expected Improvement acquisition function",
            "brief_description": "A Bayesian optimization acquisition criterion that selects new points by maximizing the expected improvement over the current best observed value under a surrogate (often Gaussian process).",
            "citation_title": "Efficient global optimization of expensive black-box functions.",
            "mention_or_use": "mention",
            "agent_name": "Expected Improvement (EI)",
            "agent_description": "Acquisition function from Gaussian-process-based global optimization (Jones et al. 1998) that trades off exploration and exploitation by quantifying the expected gain relative to current best; mentioned in the literature review as a classical sequential design approach.",
            "adaptive_design_method": "Bayesian optimization acquisition (Expected Improvement).",
            "adaptation_strategy_description": "Uses GP posterior mean and variance to compute the expected improvement at candidate points; selects points that maximize EI (implicit adaptive balance via predictive uncertainty).",
            "environment_name": "Mentioned as applicable to expensive black-box functions (deterministic or stochastic with modifications).",
            "environment_characteristics": "Black-box, expensive-to-evaluate functions; original EI assumes deterministic or low-noise contexts, but extensions handle stochastic outputs.",
            "environment_complexity": "Typically used on low-to-moderate dimensional continuous domains; computational cost scales with GP fitting and acquisition optimization.",
            "uses_adaptive_design": null,
            "performance_with_adaptation": null,
            "performance_without_adaptation": null,
            "sample_efficiency": null,
            "exploration_exploitation_tradeoff": "Implicit in the EI formula: high predictive mean favors exploitation while high predictive variance favors exploration; no hand-tuned α is necessary.",
            "comparison_methods": "Cited historically (Jones et al. 1998) and contrasted with the paper's DHASD approach in literature review.",
            "key_results": "Mentioned as a foundational sequential design approach; authors cite EI as part of historical development of sequential metamodeling methods but do not experimentally compare EI in this paper.",
            "limitations_or_failures": "Not evaluated in this paper; original EI may need adaptation for noisy stochastic simulations (extensions exist).",
            "uuid": "e1145.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A novel sequential design strategy for global surrogate modeling.",
            "rating": 2,
            "sanitized_title": "a_novel_sequential_design_strategy_for_global_surrogate_modeling"
        },
        {
            "paper_title": "Efficient global optimization of expensive black-box functions.",
            "rating": 2,
            "sanitized_title": "efficient_global_optimization_of_expensive_blackbox_functions"
        },
        {
            "paper_title": "A surrogate modeling and adaptive sampling toolbox for computer based design.",
            "rating": 2,
            "sanitized_title": "a_surrogate_modeling_and_adaptive_sampling_toolbox_for_computer_based_design"
        },
        {
            "paper_title": "Hierarchical adaptive experimental design for Gaussian process emulators.",
            "rating": 1,
            "sanitized_title": "hierarchical_adaptive_experimental_design_for_gaussian_process_emulators"
        },
        {
            "paper_title": "Application-driven sequential designs for simulation experiments: Kriging metamodeling.",
            "rating": 1,
            "sanitized_title": "applicationdriven_sequential_designs_for_simulation_experiments_kriging_metamodeling"
        }
    ],
    "cost": 0.016092,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Adaptive Exploration-Exploitation Algorithm for Constructing Metamodels in Random Simulation Using a Novel Sequential Experimental Design
2014</p>
<p>Ali Ajdari 
Department of Industrial Engineering
Sharif University of Technology
TehranIran</p>
<p>Hashem Mahlooji 
Department of Industrial Engineering
Sharif University of Technology
TehranIran</p>
<p>An Adaptive Exploration-Exploitation Algorithm for Constructing Metamodels in Random Simulation Using a Novel Sequential Experimental Design</p>
<p>Communications in Statistics-Simulation and Computation R
43201410.1080/03610918.2012.720743Sequential designMetamodelSimulation optimizationKrigingRe- sponse surfaceExplorative-exploitative search Mathematics Subject Classification Primary 62L05Secondary 65C99
In this article, we propose a novel algorithm for sequential design of metamodels in random simulation, which combines the exploration capability of most one-shot spacefilling designs with the exploitation feature of common sequential designs. The algorithm continuously maintains a balance between the exploration and the exploitation search throughout the search process in a sequential and adaptive manner. The numerical results indicate that the proposed approach is superior to one of the existing well-known sequential designs in terms of both the computational efficiency and speed in generating efficient experimental designs.</p>
<p>Introduction</p>
<p>Resorting to metamodels as surrogate models has been the focus of many research efforts in recent years in the context of simulation optimization (SO). Metamodels are particularly useful in cases where the evaluation of each design point is expensive, which is the case for most of the real-world simulation problems. In such cases, the simulation model of the problem could be replaced with a secondary function (called surrogate model or metamodel), which is much more cost effective and easier to deal with. Figure 1 shows a schematic diagram of the steps involved in a metamodel-based SO.</p>
<p>As shown in the figure, the first step in a metamodel-based SO is conducting an experimental design. There are generally two categories of experimental designs applicable to the metamodel building process: One-Shot Experimental Designs (OSED) and Sequential Experimental Designs (SED). In OSED, all the design points required for building the metamodel are selected prior to the execution of the experimental design process. Such an approach suffers from its inflexibility to learn the special characteristics of the shape of the response surface. Additionally, the sufficient number of design points required for exploring such characteristics is not usually known beforehand. SED is an alternative approach in which at each step, new sample points are collected as the process continues and more information about the response surface is gathered. To efficiently explore the search space for information about the surface behavior and accurately represent its characteristics, researchers have generally adapted two search strategies; namely, explorative and exploitative search. Explorative methods refer to those methods in which the main focus of the algorithm is on covering the entire search space with the least possible unsampled regions. These methods are somewhat akin to space-filling designs. Exploitative methods on the other hand primarily focus on regions with more interesting characteristics than others. Throughout the search, these methods attempt to skip regions that contain no significant information about the surface's characteristics in favor of those that are considered to be more informative (interesting).</p>
<p>The second step in a metamodel-based SO is the evaluation of the design points. In the context of SO, the evaluation of each input combination is done through running a simulation model of the problem. The focus of this article is on random simulation that is more common when dealing with real-world problems than deterministic simulation. Nevertheless, the principles discussed here can be easily modified to solve problems in the context of deterministic simulation as well.</p>
<p>Constructing the metamodel using the available input/output data is the next step. To this date, several types of metamodels have been proposed in the field of SO, including artificial neural networks, Kriging models, regression splines, radial basis function, etc. (for a more detailed review on metamodels, see the book by Forrester et al. 2008, Barton andMeckesheimer 2006, andBarton 2009). Although we use Kriging as our metamodel choice, one can replace it with any other types of metamodels.</p>
<p>After constructing a metamodel, its performance should be evaluated to see if it is capable of representing the actual model of the problem. To this end, several methods have been proposed in the literature among which one could name bootstrapping, jackknifing, and cross-validation. In this article, we use the cross-validation technique to test the accuracy of our metamodel. If the validity of the metamodel is rejected, it should be repaired to improve its accuracy. This is executed either by resetting the metamodel's parameters and refitting it on the available sample set or by updating the experimental design and acquiring new samples. Once the metamodel is approved, it can replace the actual model of the problem and be used in the optimization phase for evaluating the candidate points and determining the problem's optimum. If this point is not acceptable (due to practical reasons, infeasibility, etc.), the optimization phase is restarted using different parameters to find another solution for the problem. In case that the optimization phase fails to render an acceptable point after several iterations, the metamodel-construction phase is restarted (preferably using stricter limits on the desired level of accuracy) to achieve a more effective metamodel.</p>
<p>As is delineated by the dashed line in Fig. 1, the focus of this study is on the metamodelconstruction phase. Our goal is to propose a sequential experimental design for constructing accurate metamodels in an efficient manner (in terms of both simulation budget and speed). Our algorithm applies a sequential strategy based on SED approach and adaptively combines the exploitative and the explorative search of the response surface. This allows algorithm to gather new information about the surface behavior by sampling a new point from the search space at each step based on previously gathered information.</p>
<p>The rest of this article is organized as follows: in the next section a review on the available literature on sequential experimental design is presented. Section 3 describes the proposed sequential approach in detail and in Section 4 computational results for evaluating its efficacy are presented. Finally, Section 5 is dedicated to the concluding remarks.</p>
<p>Literature Review</p>
<p>In the rather extensive literature available on the sequential experimental design, one can categorize the works mainly in three groups: exploration-based methods, exploitation-based methods, and hybrid methods. Exploration-based methods generally aim at exploring the entire search space with the least possible unsampled regions. Alam et al. (2004) compare the effects of six different exploration-based designs on the development of artificial neural network. Additionally, Crombecq et al. (2011) recently reviewed and compared some of the most important exploration-based algorithms and space-filling designs in this context. The main drawbacks of these methods are that on one hand too much computational budget may be wasted on regions that have little or no additional information about the response surface, and undersampling may occur in regions with more interesting characteristics on the other hand. As mentioned before, exploitation-based methods mainly focus on the regions with more interesting characteristics. The disadvantage associated with these methods is that the sample points may become clustered in some regions while other parts of the response surface remain unsampled. For an example, one can cite the work by Shahsavani and Grimvall (2009). Finally, hybrid methods combine, at least to some degree, the capabilities of both the exploration and the exploitation-based methods. These methods aim at using the advantages of both the exploration and the exploitation-based methods while avoiding the drawbacks associated with each one of them.</p>
<p>In an early attempt toward developing a metamodel-based SO algorithm, Jones et al. (1998) applied Kriging interpolation technique as metamodel and selected the new point as the one that maximizes an Expected Improvement (EI) function. They used branchand-bound technique for optimizing the EI function. Many of the ideas proposed in that article (i.e., using cross-validation to validate the metamodel, applying EI to select new points, etc.) were later adopted by other researchers in the field. In another work, Barton (1997) used the concept of "current region of interest" (CRI) in each iteration to select a batch of new samples. Herein we denote by batch selection the addition of more than one point to the current design points at each stage. In a recent article, Loeppky et al. (2010) proposed a batch sequential strategy in which at each iteration, batches of new points are selected based on two distance-based metrics. The idea of batch selection of new samples was also adopted by other authors as well (Busby, 2009;Busby et al., 2007;Shahsavani and Grimvall, 2009). These works also apply modifications of the CRI concept in their algorithm. Busby et al. (2007) use a sequential method called "adaptive gridding" to sequentially decompose the search space into disjoint cells and apply cross-validation and maximum entropy optimization to determine "bad cells" in order to arrive at a CRI and select the new point. Later Busby (2009) modified this approach by replacing the maximum entropy function by a maximin distance function. Since the approach adopted by these two research efforts usually adds more than one sample point to the current set of points, it is considered as a batch selection approach. Shahsavani and Grimvall (2009) sequentially partition the available CRI into two cuboid regions and apply some roughness measure to select the next CRI. The new sample batch consists of points in the center and corners of the current region.</p>
<p>The main problem with the batch selection approach is that it tends to use excessive sample points. This problem is specifically heightened in the context of SO where the evaluation of one point may need a large amount of computational budget. To overcome this problem, some authors use an approach according to which, only one point is added to the current design points at each stage (Keys and Rees, 2004;Kleijnen and Van Beers, 2004). Similar to the article by Busby et al. (2007), Santos and Santos (2008) partition the search space into a mesh of triangles and select the next point at the middle of the widest side of the triangle with the largest area. Later Crombecq et al. (2009) employed Voronoi tessellation for decomposing the search space into disjoint cells. In order to select the CRI, they calculate and combine two scores for each cell, namely, exploration and exploitation scores.</p>
<p>Unlike the aforementioned approaches that try to decompose the input space to select new samples, some authors have adopted the strategy of selecting the next point from a list of candidate points based on some criterion. The algorithms that adopt this strategy differ in the way that they form the candidate list and the criteria for selecting the new point. De Geest et al. (1999) propose an algorithm that selects a new point as the point that maximizes a reflective function defined by the difference between the output values of the best and the second best metamodels. Balasko et al. (2006) use Evolutionary Strategy (ES) to find a point in the search space, which maximizes a fitness function obtained by Fisher information matrix. In a similar approach, an Improvement function defined by a fitness error of a Gaussian stochastic process (GASP) was used to identify the point in the design space, which maximizes this function (Ranjan et al., 2008). Can and Heavey (2009) also adopt a metaheuristic approach and use Particle Swarm Optimization (PSO) to minimize an error function. Crombecq and Dhaene (2010) resort to multi-objective optimization by using genetic algorithm and Monte Carlo method to identify the point that optimizes their two selected criteria. Kleijnen and Beers (2004) use a statistical approach to form the candidate list by randomly generating points via Latin hypercube design (LHD) to select the next point as the point with the largest prediction variance estimated by cross-validation and jackknifing technique. To estimate the prediction variance, van Beers and Kleijnen (2008) resort to bootstrap method instead of cross-validation and jackknifing. Kleijnen et al. (2010) further use bootstrap and cross-validation to compute a prediction error and select the next point as the point halfway between the worst point (with maximum prediction error) and its nearest neighbor.</p>
<p>The Proposed Approach</p>
<p>According to Kleijnen and Wan (2007), all practical SO methods are based on iterative procedures. Likewise, we propose a hybrid iterative approach and develop an algorithm that combines the explorative and exploitative search of the response surface and maintains a proper balance between them through an adaptive approach. Like most sequential metamodel-based algorithms, our algorithm starts with a set of initial design points and evaluates these points through simulation runs. Based on the resulting input/output (I/O) data, the algorithm fits a metamodel over the response surface and checks its validity (the goodness of fit). If the validity is rejected, the algorithm enters a loop where in each iteration, it selects one point in the search space, adds it to the current set, and rebuilds the metamodel. The novelty of the proposed algorithm lies in its method for selecting the new point. Figure 2(a) illustrates the steps involved in the algorithm and Fig. 2(b) depicts the details involved in the step 6 of the algorithm. Moreover, to further elaborate on the details of the proposed algorithm, in the remainder of this section we briefly review the steps involved in the algorithm.</p>
<p>Steps 1 and 2: Setting the Initial Design and Evaluating the Sample Points</p>
<p>Our algorithm starts with a set of initial sample points generated by LHD. We apply a space-filling LHD that uses a maximin criterion to spread the points throughout the search space such that the minimum Euclidian distance between any two points in the space is maximized. The number of initial sample points generated by LHD in our algorithm is problem specific and is set by the user. For the problems discussed in this article, we have found 16 to be a proper choice (also see Jones et al., 1998 andLoeppky et al., 2009 for a general rule-of-thumb for selecting the number of points). These points are then augmented by 2 D corner points, with D denoting the problem's dimension, i.e., in a two-dimensional space, the total number of initial points adds up to 20. The reason for selecting these corner points is that based on our experience, they usually contain valuable information about the surface behavior around the extremes of the search space. However, in cases for which the total initial points exceed a user-specified value (which might occur when dealing with problems with high dimensions), the user could select a fraction (or even none) of these corner points, relying more on the initial sample points as generated by LHD.</p>
<p>The next step is to evaluate the sample points and compute the response values. Since the focus of this article is on random simulation, each function evaluation usually exhibits noise. This noise leads to generating different response values for the same input combination in each replication. Thus, such noises, if large, could seriously jeopardize the metamodel's performance. To lower the impact of such noise, one approach is to perform multiple simulation runs for each input combination and then use the averaged result as the estimated response value. The key factor in applying this approach is deciding the number of simulation replications at each point. To determine these numbers, one option is to set a fixed number of replications (say m) per input combination (r i = m). The main drawback to this option is that it tends to waste the computational budget by allocating the same weight for every point regardless of their merits. Another option is to determine the number of replications at each point based on its characteristics. In this article, we use the second option by following the path proposed by Santos and Santos (2008) with a slight modification. For more information on similar methods, see Law (2007). The idea is to determine the number of replications at each point such that the estimated variances of the average responses become approximately equal. In this method, we allocate the computational budget to each point proportionate to its estimated variance, i.e., the higher the variance of a particular point, the more replications it needs. Accordingly, a pilot sample of r i = r 0 ≥ 2; i = 1, 2, . . . , n initial replications are allocated to each point i and based on the estimated variances (σ 2 i ), as many as r i − r 0 additional replications are performed for each point, where r i is computed as
r i = min r 0σ 2 î σ 2 min , R ,(1)
where R is the maximum number of allowed replications, x denotes the largest integer not larger than x andσ 2 min = min i=1,2,...,nσi . As such, the total number of replications at each design point i becomes r i + r 0 . Ultimately, the average responses are used to fit the Kriging metamodel to the sample points.</p>
<p>Steps 3 and 4: Fitting the Metamodel and Checking its Validity</p>
<p>Subsequent to evaluating all sample points, a metamodel is fit over the entire response surface using the available I/O data. In this article, we apply Kriging interpolation method, which is a widely accepted choice among researchers in the field of SO. For more information and details regarding Kriging method, one can refer to Papalambros et al. (2002), Kleijnen and Beers (2004), and Kleijnen et al. (2010). In our study, we used DACE-a free MATLAB toolbox for working with Kriging approximations-to build a metamodel based on the available I/O data at each stage. (For more information on DACE toolbox visit http://www2.imm.dtu.dk/∼hbn/dace/).</p>
<p>Once the metamodel is built, the next step is to check its validity to see if it is capable of replacing the actual response surface. In doing so, we use a five-fold cross-validation technique. More specifically, we randomly partition the sample set into five groups of approximately equal size. Then, we temporarily leave out one group (called the validation set) from the sample set; fit the metamodel to the remaining sample points (called the training set) and then use the resulting metamodel to compute the prediction error of the validation set. This error is computed using the Root Relative Square Error (RRSE) as discussed in the work by Gorissen et al. (2010a). RRSE is related to the popular R 2 criterion and is computed as
RRSE( y,ŷ) = n cv i=1 ( y i − y i ) 2 n cv i=1 ( y i −ỹ) 2 ,(2)
where y i and y i are the average simulated response and the metamodel-predicted response for the input combination i, respectively, n cv is the number of sample points in the validation set, andỹ = n cv i=1 y i /n. The above procedure is repeated five times, with each group used exactly once as the validation set. This will result in five prediction error measures. Finally, the cross-validation error of the original set is computed by averaging these five measures, i.e., E = 5 i=1 n i E i /n, with n i and E i representing the size and the RRSE prediction error of the ith validation set, respectively, n denoting the total sample size, and E being the total RRSE measure for the entire sample set. A metamodel is assumed to be valid if its cross-validation error is below a prespecified threshold ε. The value of ε depends on the surface complexity and the desired level of accuracy.</p>
<p>Steps 5 and 7: Stopping Criteria and the Output</p>
<p>As the stopping criteria for our algorithm, we consider two stopping options: (i) the crossvalidation error dropping belowε, and/or (ii) reaching the maximum number of samples. Once one of these criteria is met, the algorithm returns the best metamodel achieved so far.</p>
<p>Addition of the New Point</p>
<p>Since the novelty of the proposed algorithm lies in its method for selecting the next point, in this section we discuss the steps leading to the addition of this point. The details of each step are discussed through the following subsections and the entire procedure is illustrated in Fig. 2(b). The procedure contained in step 6 of the proposed algorithm is called Delaunay-Hybrid Adaptive Sequential Design (DHASD).</p>
<p>3.4.1.</p>
<p>Step 6.1: Decomposition of the search space. In each iteration, DHASD builds a mesh of triangles upon current samples using an algorithm called "Delaunay Triangulation." Figure 3 shows the Delaunay triangulation of a given set of points. This algorithm was first introduced in Delaunay (1934) and has been widely applied by researchers in different fields ever since. By definition, a Delaunay triangulation of a set P of points in the plane is a triangulation DT(P) such that no point in P is inside the circumcircle of any triangle in DT(P). In the mesh of triangles generated by Delaunay algorithm, each vertex represents a sample point in the design set. Thus, the interior of each triangle is regarded as an unsampled region, which represents a unique region in the search space.</p>
<p>3.4.2.</p>
<p>Step 6.2: Calculation of the exploration and the exploitation scores. Having decomposed the search space, DHASD aims at determining the most promising region (triangle) in order to select the next point. Thus, we consider two criteria to show each triangle's potential for the subsequent investigation based on the exploration and the exploitation capabilities of each triangle. While the exploration criterion is related to the space-filling characteristic of an experimental design and guaranties a good coverage of the search space, the exploitation criterion deals with exploiting regions with some interesting features. More specifically, the triangle with the highest exploration score represents the region wherein the samples are most scattered whereas the region with the highest exploitation score is the region that shows the most interesting characteristic throughout the search space (in this case, the region that-if investigated-will most likely lead to better capturing the surface's behavior and thus, enhancing the metamodel's accuracy).</p>
<p>The exploration score for each triangle is proportionate to its relative area. Hence, the larger the area of a triangle, the higher its exploration score would be. The exploration score for every triangle i is computed as follows:
θ i = A i A max × 10,(3)
where A i denotes the area for triangle i and A max = max i=1,...,T {A i } with T being the number of triangles in the mesh. The computation of the exploitation score is based on the premise that the more the response values vary in a particular region, the more likely that region would be to reveal some interesting characteristics. Thus, for each triangle, we compute the total variation in response values among its three vertices and the triangle with the highest variation is regarded as the most interesting triangle. The exploitation score for every triangle i is computed as follows:
δ i,1 = |y A,i − y B,i |, δ i,2 = |y A,i − y C,i |, δ i,3 = |y B,i − y C,i | (4) δ i = 3 j =1 δ i,j(5)i = δ i δ max × 10,(6)
where y j,k represents the response value for the design point located at the vertex j of triangle k, i is the exploitation score for triangle i, and δ max = max i=1,...,T {δ i }.</p>
<p>Steps 6.3 and 6.4: Computation of the total score and selection of the next point.</p>
<p>After calculating the exploration and the exploitation scores, the algorithm combines them into one score called the total score that accounts for both the exploration and the exploitation capabilities of a triangle. The total score for every triangle i is computed as
i = αθ i + (1 − α) i ,(7)
where αε(0, 1]. One of the key features of DHASD is that the value of α is adaptively changed throughout the procedure, enabling the algorithm to adapt itself to different surfaces. The nature of this change is such that whenever a region exhibits some interesting characteristics, the value of α is decreased, paying more attention to the exploitation score. On the other hand, in cases where a more explorative strategy is needed, the value of α is increased, enhancing the exploration capability of the method. DHASD starts with an initial value of α = α 0 and continues to decrease α as long as the cross-validation error continues to drop. Whenever the algorithm fails to improve this error after a prespecified number of runs, α is increased until either cross-validation error is no longer improved or α = 1 in which case the adaptive procedure is restarted with α 0 = 1. Having computed the total scores for every triangle, the triangle with the highest score (called the winning triangle) is selected as the most promising region and the next sample is selected at the triangle's centroid (intersection of the triangle's medians).</p>
<p>3.4.4. Steps 6.5 and 6.6: To avoid clustering of points in a region. Since in the case of complex surfaces, α tends to smaller values, the algorithm is in the danger of being trapped in regions where response values vary dramatically. In order to overcome this problem, we devised a number of escaping strategies:</p>
<ol>
<li>As long as the distance between the new point and its nearest neighbor is smaller than a prespecified value η 1 , replace the winning triangle with the triangle with the second highest total score. 2. As long as the sum of distances between the new point and its k-nearest neighbors is smaller than a prespecified value η 2 , do the following:</li>
</ol>
<p>i. If α ≤ 0.5, then α = α × rand 1 ii. If α &gt; 0.5, then α = α × rand 2 , where rand 1 ∈ U (1, 1.5) and rand 2 ∈ U (0.5, 1) with U denoting the uniform distribution. These intervals were selected through a trial-and-error procedure as the ones, which result in better performance of the algorithm. 3. Prohibit every triangle whose area is below a prespecified value from participating in the remaining steps of the algorithm.</p>
<p>Strategies 1 and 2 are intended to avoid clustering of points in one region while the 3rd strategy aims to prevent the algorithm from wasting computational budget in regions that may have little or no effect on the algorithm's performance. In short, the output of this step is to select a point in the search space that in addition to having a high total score is at a reasonable distance from every point in the current set. Note that Kriging assumes that responses with inputs that lie close together are highly correlated and not much can be learned from simulating the new point.</p>
<p>Computational Results</p>
<p>In order to evaluate the performance of our proposed algorithm, we test it on 15 different mathematical problems after adding a noise function to them to imitate the stochastic nature of the simulation runs. The added noise is a Gaussian noise factor with variance 0.1|f (x)|, with f (x) denoting the output as computed by the mathematical formula for input combination x. The value of 0.1 represents the magnitude of the noise resulting from the simulation run. It is an arbitrary number and could be replaced by any particular value based on the actual problem and the accuracy of the simulated model. One of the advantages of using a mathematical function over a real-world simulation problem is that its true response is known in advance. Additionally, these test problems are very easy to employ (as opposed to the difficult nature of programming a simulation problem) and take considerably less time to evaluate than a simulation run. The test problems used in this study cover a broad range from smooth, unimodal surfaces to highly nonlinear and multi-modal functions. These problems were selected from the most popular test problems available in the optimization literature. All the test problems are listed in the Appendix.</p>
<p>To investigate the impact of different initial values for α on the required number of samples, we apply the algorithm for every αε{0.01, 0.1, 0.2, . . . , 1} to all the test problems. For every value of α, the algorithm was allowed to perform five replications. Thus, we have 10 groups, each containing five observations. Consequently, we test the null hypothesis (the hypothesis that the means of all groups are equal). We repeat the above procedure for all the test problems and in all 15 problems the results of one-way analysis of variance (ANOVA) indicated that there is no significant difference between different values of α at the 95% confidence level. In other words, the algorithm is not sensitive to the initial value of α. Table 1 shows the results of this experiment for the test problem 14 (TP14). Figure 4 illustrates the output of the one-way ANOVA as obtained by Minitab 14.1. As can be seen, the p-value is greater than 0.05, indicating that there is no significant difference among the group means. As our benchmark, we use SUrrogate MOdeling (SUMO) that is a powerful MATLAB toolbox for adaptively building accurate surrogate models. The toolbox contains different sequential design strategies as the experimental design options and its desirable performance has been put to test in different articles (Gorissen et al. 2010b). In our study, we apply the academic version of SUMO7.0.2. This version uses LOLA-Voronoi combined with an error-based sample selector as the default sampling strategy. LOLA-Voronoi method is a very effective sequential design that was originally introduced in the article by Crombecq et al. (2009). It should be noted that since in our literature review, we could not find any other sequential designs that are available to be used as benchmarks, SUMO turned out to be our only available benchmark.</p>
<p>The default setting for the metamodel type is Kriging. Both our method and the SUMO use five-fold cross-validation to check the validity of the metamodel and they were allowed to run until one of the following two criteria was met: (i) the desirable level of accuracy and (ii) the maximum number of sample points. Moreover, both algorithms were run in MATLAB 9 environment on an Intel R Core TM 2 Duo notebook with 2.20 GHz CPU and 2.00 GB RAM and Windows Vista operating system. Figure 5 compares the performance of the algorithms for two selected test problems as a function of the sample size. Table 2 presents the results for the number of samples required by each method to reach the desired level of accuracy. These results were obtained from running each method 10 times for each test problem. Columns in this table show the minimum number of samples, the average, and the standard deviation in those 10 runs, respectively. As shown in the table, in 14 out of 15 test problems, our algorithm reached the desirable level of accuracy with fewer points than SUMO both in terms of the best and average number of samples. Note that in test problems number 1, 3, and 14, SUMO needed far more points than our algorithm to reach the same cross-validation error. The only problem in which SUMO method outperforms DHASD is TP2 in which the difference in sampling points is only two units. One possible reason for the advantage of our algorithm over SUMO  seems to be the adaptive nature of our algorithm, which enables it to adapt itself to different surfaces, whereas SUMO assumes the same weight for the exploration and the exploitation parts for all surfaces. This advantage becomes more important as the evaluation time for each sample point increases.</p>
<p>Considering the Std columns, SUMO usually yields lower results than our method. This behavior could be attributed to the adaptive nature of α in our algorithm. However, it should be pointed out that in all problems except TP2, the worst result achieved by our algorithm is still better than the best result obtained by SUMO.</p>
<p>After a metamodel is selected for a given problem, it should be tested on an independent set of points to prove its efficiency. In doing so, we built a 30 × 30 rectangular grid covering each problem's domain and compared the real and the metamodel-predicted response values. We use the mean square error (MSE) and the average relative error (ARE) measures as defined below for comparing the results:
MSE ( y,ŷ) = 1 n n i=1 ( y i − y i ) 2 (8) ARE ( y,ŷ) = 1 n n i=1 |( y i − y i )| | y i |(9)
Table 3 presents the MSE and ARE results for each method associated with each test problem. It should be noted that the results given in this table were obtained from the best model as shown in Table 2. Table 3 indicates that in most cases the metamodels designed by both methods give satisfactory results in terms of MSE and ARE, which is indicative of the adequacy of the metamodel in representing the surface area. The only exception is  TP2 for which both the MSE and the ARE values are relatively high. This may be due to the wide range of outputs for this problem. To further investigate the accuracy of the metamodel for this particular problem, we built a 100 × 100 rectangular grid covering the entire search space and performed a regression analysis between the real and the metamodel-predicted outputs. The result of this analysis is shown in Fig. 6. As can be seen, the correlation coefficient (R-value) is very close to 1.0, indicating that the variation in the actual responses is perfectly explained by the metamodel-predicted outputs.</p>
<p>Comparing the performance of the two methods reveals that our algorithm outperforms SUMO in 10 out of 15 problems in terms of both MSE and ARE values. Note that in all of these problems except for TP2, our method yields better results in spite of using fewer sample points than SUMO. In the remaining five problems, namely, test problems number 4, 7, 10, 11, and 14, SUMO outperforms our algorithm. However, it should be noted that in all these cases, SUMO had built the metamodel with more sample points compared to our method. Thus, to draw a fair comparison, we performed an additional experiment in which for each of these problems, our algorithm was allowed to continue collecting points until it reached the same accuracy level as SUMO in terms of both the MSE and the ARE values. The results suggested that in all cases, our algorithm needed strictly fewer points to reach the same accuracy level as SUMO. In addition, the same experiment was conducted for TP2 for which our algorithm needed more points than SUMO to build a metamodel.</p>
<p>The results indicated that when tested on an independent set, SUMO needed more points than our algorithm to reach the same accuracy level in terms of MSE and ARE values. Figure 7 shows the final sample points as selected by our algorithm for four selected test problems. Note that in regions with high curvature (areas with darker colors), more points are collected. This feature is attributed to the exploitation capability of our algorithm. To further elaborate on this feature, we illustrate the changes of α value as new samples are collected by the algorithm in Fig. 8. It is noteworthy that in problems with a large smooth  portion of the surface (Fig. 8(b) and 8(d)), α tends to assume smaller values to enhance the exploitation capability for further investigating the areas with high complexity by skipping the smoother regions. On the other hand, in problems where most regions of the surface exhibit a high degree of complexity ( Fig. 8(a) and 8(c)), α tends to assume larger values to emphasize the exploitation capability by providing a good space-filling design.</p>
<p>Conclusions</p>
<p>In this article, we presented a novel approach for sequential design of experiments for constructing accurate metamodels. The basic idea is to design an algorithm that combines the exploration capabilities of one-shot space-filling designs with the exploitation capability of the common sequential designs to build a metamodel with as few number of samples as possible. To achieve this purpose, at each stage, our algorithm selects a point with the highest score based on scores coming from the exploration and exploitation characteristics of the area represented by that point. The main advantage of this algorithm over similar hybrid algorithms in sequential design is that it automatically maintains a balance between explorative and exploitative search at each step using an adaptive weighing approach. The performance of the proposed algorithm was evaluated on 15 mathematical problems and the results suggest that it can successfully build metamodels with high accuracy using fewer sample points than a very powerful commercial experimental design toolbox called SUMO.</p>
<p>Table A1</p>
<p>Test problems used in this study 
+ (x 1 + x 2 + 1) 2 (19 − 14x 1 + 13x 2 1 − 14x 2 + 6x 1 x 2 + 3x 2 2 )) · (30 + (2x 1 − 3x 2 ) 2 (18 − 32x 1 + 12x 2 1 + 48x 2 − 36x 1 x 2 + 27x 2 2 )) −2 x 1 2 N(0, 0.1|f (x)|) −2 x 2 2 TP7-Griewank f (x) = 1 + 2 i=1 x 2 i 4000 − 2 i=1 (cos( x i √ i ))
−100 −5</p>
<p>x 2 5 (Continued on next page)</p>
<p>Table A1</p>
<p>Test problems used in this study (Continued) </p>
<p>Function</p>
<p>Figure 1 .
1Steps involved in a metamodel-based simulation optimization.</p>
<p>Figure 2 .
2Schematic diagrams of (a) steps involved in the proposed approach; (b) steps involved in the step 6 of the proposed approach.</p>
<p>Figure 3 .
3Delaunay triangulation: (a) The arrangement of points in a given set; (b) Delaunay triangulation of the given set. (Color figure available online.)</p>
<p>Figure 4 .
4The output of ANOVA test for TP14.</p>
<p>Figure 5 .
5Cross-validation error vs. number of samples required by each method for (a) TP7 and (b) TP11. (Color figure available online.)</p>
<p>Figure 6 .
6The output of regression analysis for TP2. (Color figure available online.)</p>
<p>Figure 7 .
7The surfaces approximated by the metamodel with the final design points: (a) TP1, (b) TP3, (c) TP7, and (d) TP12. (Color figure available online.)</p>
<p>Figure 8 .
8The changes in alpha values throughout the algorithm: (a) TP15, (b) TP13, (c) TP12, and (d) TP8. (Color figure available online.)</p>
<p>Table 1
1Number of samples required by the algorithm in each replication to build a valid metamodel for each value of alphaAlpha values 
Rep 1 
Rep 2 
Rep 3 
Rep 4 
Rep 5 
Mean 
Std </p>
<p>0.01 
77 
75 
72 
71 </p>
<p>Table 2
2Number of samples required by each method to build metamodelsProposed algorithm 
SUMO </p>
<p>Problem 
Best Average 
Std 
Best Average 
Std </p>
<p>TP1-Ackley 
316 
317.1 
2.7669 445 
446 
1.8165 
TP2-Beale 
70 
72.1 
0.6268 
68 
69.9 
1.2867 
TP3-Beker &amp; Logan 
200 
211.3 
4.3006 670 
682.9 
6.8872 
TP4-Booth 
20 
23.2 
0.6932 
34 
35.2 
0.6325 
TP5-Branin 
30 
32.5 
1.1731 
33 
36.5 
0.5270 
TP6-Goldstein-Price 
64 
69.9 
1.6636 
77 
79.4 
1.0750 
TP7-Griewank 
191 
197.2 
4.4734 239 
245.9 
3.1429 
TP8-Hump 
41 
43.4 
1.6437 
43 
46.5 
1.5811 
TP9-P1 
90 
94 
2.4037 
94 
98.1 
1.5951 
TP10-P4 
105 
109.9 
3.1429 127 
129.8 
1.7512 
TP11-Parspoulos 
205 
207.3 
1.1595 222 
229.3 
3.7667 
TP12-Peaks 
60 
67 
2.3094 
79 
83 
2.9814 
TP13-Perm 
33 
37.5 
2.9907 
43 
45.6 
2.1705 
TP14-Rastringin 
65 
71.3 
3.3759 110 
111.8 
1.3984 
TP15-Six-HumpCamel Back 
67 
67.6 
0.5164 
81 
82.2 
1.2293 </p>
<p>Table 3
3Numerical results of testing the output metamodels of each method on independent test setsProposed algorithm </p>
<p>SUMO </p>
<p>Output </p>
<p>Problem </p>
<p>range </p>
<p>Sample no. </p>
<p>MSE </p>
<p>ARE </p>
<p>Sample no. </p>
<p>MSE </p>
<p>ARE </p>
<p>TP1-Ackley </p>
<p>7.244 </p>
<p>316 </p>
<p>3.6e-3 </p>
<p>6.2e-3 </p>
<p>445 </p>
<p>4.3e-3 </p>
<p>6.822e-3 </p>
<p>TP2-Beale </p>
<p>1.823e5 </p>
<p>70 </p>
<p>7.94e2 </p>
<p>2.126e-1 </p>
<p>68 </p>
<p>8.674e2 </p>
<p>3.479e-1 </p>
<p>TP3-Beker &amp; Logan </p>
<p>49.94 </p>
<p>200 </p>
<p>4.1e-3 </p>
<p>1.09e-2 </p>
<p>670 </p>
<p>1.234e1 </p>
<p>6.401e-1 </p>
<p>TP4-Booth </p>
<p>2.594e3 </p>
<p>20 </p>
<p>2.317e-6 </p>
<p>3.77e-5 </p>
<p>34 </p>
<p>2.3912e-7 </p>
<p>3.3511e-6 </p>
<p>TP5-Branin </p>
<p>307.7112 </p>
<p>30 </p>
<p>3.1e-2 </p>
<p>8.01e-3 </p>
<p>33 </p>
<p>8.691e-2 </p>
<p>1.0201e-2 </p>
<p>TP6-Goldstein-Price </p>
<p>2.8516e6 </p>
<p>64 </p>
<p>5.9453e5 </p>
<p>5.75e-2 </p>
<p>77 </p>
<p>2.1833e6 </p>
<p>1.145e-1 </p>
<p>TP7-Griewank </p>
<p>2.0087 </p>
<p>191 </p>
<p>4.7724e-5 </p>
<p>3.2e-3 </p>
<p>239 </p>
<p>1.8127e-6 </p>
<p>6.7158e-4 </p>
<p>TP8-Hump </p>
<p>6.4216e3 </p>
<p>41 </p>
<p>1.444e1 </p>
<p>1.97e-2 </p>
<p>43 </p>
<p>1.5916e1 </p>
<p>2.121e-2 </p>
<p>TP9-P1 </p>
<p>6.1136 </p>
<p>90 </p>
<p>2.1486e-6 </p>
<p>5.8043e1 </p>
<p>94 </p>
<p>9.2029e-6 </p>
<p>8.3084e1 </p>
<p>TP10-P4 </p>
<p>21.7723 </p>
<p>105 </p>
<p>6.27e-2 </p>
<p>3.741e-2 </p>
<p>127 </p>
<p>5.98e-2 </p>
<p>9.643e-3 </p>
<p>TP11-Parspoulos </p>
<p>1.9634 </p>
<p>205 </p>
<p>1.3624e-5 </p>
<p>1.6121e-3 </p>
<p>222 </p>
<p>3.029e-6 </p>
<p>4.2687e-4 </p>
<p>TP12-Peaks </p>
<p>14.0881 </p>
<p>60 </p>
<p>6.45e-6 </p>
<p>3.81e-3 </p>
<p>79 </p>
<p>4.51e-5 </p>
<p>8.6e-3 </p>
<p>TP13-Perm </p>
<p>110.4973 </p>
<p>33 </p>
<p>1.5496e-4 </p>
<p>1.37e-2 </p>
<p>43 </p>
<p>2.0832e-4 </p>
<p>1.58e-2 </p>
<p>TP14-Rastringin </p>
<p>39.9479 </p>
<p>65 </p>
<p>1.41e-3 </p>
<p>8.126e-3 </p>
<p>110 </p>
<p>1.6803e-4 </p>
<p>6.399e-4 </p>
<p>TP15-Six-Hump </p>
<p>Camel Back </p>
<p>6.8895 </p>
<p>67 </p>
<p>1.079e-6 </p>
<p>1.1e-3 </p>
<p>81 </p>
<p>1.6942e-6 </p>
<p>1.6e-3 </p>
<p>Appendix
A comparison of experimental designs in the development of a neural network simulation metamodel. F M Alam, K R Mcnaught, T J Ringrose, Simulation Modelling Practice and Theory. 127-8Alam, F. M., McNaught, K. R., Ringrose, T. J. (2004). A comparison of experimental designs in the development of a neural network simulation metamodel. Simulation Modelling Practice and Theory 12(7-8):559-578.</p>
<p>Additive sequential evolutionary design of experiments. B Balasko, J Madar, J Abonyi, L Rutkowski, R Tadeusiewicz, L Zadeh, J Zurada, Artificial Intelligence and Soft Computing -ICAISC 2006. Berlin/HeidelbergSpringer4029Balasko, B., Madar, J., Abonyi, J. (2006). Additive sequential evolutionary design of experiments. In: Rutkowski, L., Tadeusiewicz, R., Zadeh, L., Zurada, J., eds. Artificial Intelligence and Soft Computing -ICAISC 2006 (Lecture Notes in Statistics). Vol. 4029. Berlin/Heidelberg: Springer, pp. 324-333.</p>
<p>Design of experiments for fitting subsystem metamodels. R R Barton, Proceedings of the 1997 Winter Simulation Conference. David Withers, H., Nelson, B. L., Andradóttir, S., Healy, K. J.the 1997 Winter Simulation ConferenceAtlanta, GAIEEE PressBarton, R. R. (1997). Design of experiments for fitting subsystem metamodels. In: David Withers, H., Nelson, B. L., Andradóttir, S., Healy, K. J., eds. Proceedings of the 1997 Winter Simulation Conference. Atlanta, GA: IEEE Press, pp. 303-310.</p>
<p>Simulation optimization using metamodels. R R Barton, A Dunkin, R Ingalls, E Yücesan, M Rossetti, R Hill, B Johansson, Proceedings of the 2009 Winter Simulation Conference. the 2009 Winter Simulation ConferenceAustin, TXIEEE PressBarton, R. R. (2009). Simulation optimization using metamodels. In: Dunkin, A., Ingalls, R., Yüce- san, E., Rossetti, M., Hill, R., Johansson, B., eds. Proceedings of the 2009 Winter Simulation Conference. Austin, TX: IEEE Press, pp. 230-238.</p>
<p>Metamodel-based simulation optimization. R R Barton, M Meckesheimer, Handbooks in Operations Research and Management Science. Shane, G. H., Barry, L. N.AmsterdamElsevier13Barton, R. R., Meckesheimer, M. (2006). Metamodel-based simulation optimization. In: Shane, G. H., Barry, L. N., eds. Handbooks in Operations Research and Management Science. Vol. 13. Amsterdam: Elsevier, pp. 535-574.</p>
<p>Hierarchical adaptive experimental design for Gaussian process emulators. D Busby, Reliability Engineering &amp; System Safety. 947Busby, D. (2009). Hierarchical adaptive experimental design for Gaussian process emulators. Relia- bility Engineering &amp; System Safety 94(7):1183-1193.</p>
<p>Hierarchical nonlinear approximation for experimental design and statistical data fitting. D Busby, C L Farmer, A Iske, SIAM Journal of Scientific Computing. 291Busby, D., Farmer, C. L., Iske, A. (2007). Hierarchical nonlinear approximation for experimental design and statistical data fitting. SIAM Journal of Scientific Computing 29(1):49-69.</p>
<p>Sequential metamodelling with genetic programming and particle swarms. B Can, C Heavey, A Dunkin, R Ingalls, E Yücesan, M Rossetti, R Hill, B Johansson, Proceedings of the 2009 Winter Simulation Conference. the 2009 Winter Simulation ConferenceAustin, TXIEEE PressCan, B., Heavey, C. (2009). Sequential metamodelling with genetic programming and particle swarms. In: Dunkin, A., Ingalls, R., Yücesan, E., Rossetti, M., Hill, R., Johansson, B., eds. Proceedings of the 2009 Winter Simulation Conference. Austin, TX: IEEE Press, pp. 3150-3157.</p>
<p>A novel sequential design strategy for global surrogate modeling. K Crombecq, L De Tommasi, D Gorissen, T Dhaene, A Dunkin, R Ingalls, E Yücesan, M Rossetti, R Hill, Proceedings of the 2009 Winter Simulation Conference. Johansson, B.the 2009 Winter Simulation ConferenceAustin, TXIEEE PressCrombecq, K., De Tommasi, L., Gorissen, D., Dhaene, T. (2009). A novel sequential design strategy for global surrogate modeling. In: Dunkin, A., Ingalls, R., Yücesan, E., Rossetti, M., Hill, R., Johansson, B., eds. Proceedings of the 2009 Winter Simulation Conference. Austin, TX: IEEE Press, pp. 731-742.</p>
<p>Generating sequential space-filling designs using genetic algorithms and Monte Carlo methods. K Crombecq, T Dhaene, K Deb, A Bhattacharya, N Chakraborti, P Chakroborty, S Das, J Dutta, S Gupta, A Jain, V Aggarwal, J Branke, S Louis, K Tan, Simulated Evolution and Learning. Springer6457Crombecq, K., Dhaene, T. (2010). Generating sequential space-filling designs using genetic algo- rithms and Monte Carlo methods. In: Deb, K., Bhattacharya, A., Chakraborti, N., Chakroborty, P., Das, S., Dutta, J., Gupta, S., Jain, A., Aggarwal, V., Branke, J., Louis, S., Tan, K., eds. Simulated Evolution and Learning. Vol. 6457. Berlin/Heidelberg: Springer, pp. 80-84.</p>
<p>Efficient space-filling and noncollapsing sequential design strategies for simulation-based modeling. K Crombecq, E Laermans, T Dhaene, European Journal of Operational Research. 2143Crombecq, K., Laermans, E., Dhaene, T. (2011). Efficient space-filling and noncollapsing sequential design strategies for simulation-based modeling. European Journal of Operational Research 214(3):683-696.</p>
<p>Adaptive CAD-model building algorithm for general planar microwave structures. J De Geest, T Dhaene, N Fache, D De Zutter, IEEE Transactions on Microwave Theory and Techniques. 479De Geest, J., Dhaene, T., Fache, N., de Zutter, D. (1999). Adaptive CAD-model building algorithm for general planar microwave structures. IEEE Transactions on Microwave Theory and Techniques 47(9):1801-1809.</p>
<p>Sur la sphere vide. B Delaunay, Otdelenie Matematicheskikh i Estestvennykh Nauk. 7Zvestia Akademii Nauk SSSRDelaunay, B. (1934). Sur la sphere vide. Zvestia Akademii Nauk SSSR, Otdelenie Matematicheskikh i Estestvennykh Nauk (7):793-800.</p>
<p>Engineering Design via Surrogate Modelling -A Practical Guide. A Forrester, A Sóbester, A Keane, John Wiley &amp; SonsChichesterForrester, A., Sóbester, A., Keane, A. (2008). Engineering Design via Surrogate Modelling -A Practical Guide. Chichester: John Wiley &amp; Sons.</p>
<p>A surrogate modeling and adaptive sampling toolbox for computer based design. D Gorissen, I Couckuyt, P Demeester, T Dhaene, K Crombecq, Journal of Machine Learning Research. 99Gorissen, D., Couckuyt, I., Demeester, P., Dhaene, T., Crombecq, K. (2010a). A surrogate model- ing and adaptive sampling toolbox for computer based design. Journal of Machine Learning Research 99:2051-2055.</p>
<p>Multiobjective global surrogate modeling, dealing with the 5-percent problem. D Gorissen, I Couckuyt, E Laermans, T Dhaene, Engineering with Computers. 261Gorissen, D., Couckuyt, I., Laermans, E., Dhaene, T. (2010b). Multiobjective global surrogate mod- eling, dealing with the 5-percent problem. Engineering with Computers 26(1):81-98.</p>
<p>Efficient global optimization of expensive black-box functions. D R Jones, M Schonlau, W J Welch, Journal of Global Optimization. 13Jones, D. R., Schonlau, M., Welch, W. J. (1998). Efficient global optimization of expensive black-box functions. Journal of Global Optimization 13:455-492.</p>
<p>A sequential-design metamodeling strategy for simulation optimization. A C Keys, L P Rees, Computers and Operations Research. 3111Keys, A. C., Rees, L. P. (2004). A sequential-design metamodeling strategy for simulation optimiza- tion. Computers and Operations Research 31(11):1911-1932.</p>
<p>Application-driven sequential designs for simulation experiments: Kriging metamodeling. J P C Kleijnen, W C M V Beers, Journal of Operational Research Society. 558Kleijnen, J. P. C., Beers, W. C. M. V. (2004). Application-driven sequential designs for simulation experiments: Kriging metamodeling. Journal of Operational Research Society 55(8):876-883.</p>
<p>Constrained optimization in expensive simulation: Novel approach. J P C Kleijnen, W V Beers, I V Nieuwenhuyse, European Journal of Operational Research. 2021Kleijnen, J. P. C., Beers, W. V., Nieuwenhuyse, I. V. (2010). Constrained optimization in expensive simulation: Novel approach. European Journal of Operational Research 202(1):164-174.</p>
<p>Optimization of simulated systems: OptQuest and alternatives. J P C Kleijnen, J Wan, Simulation Modelling Practice and Theory. 153Kleijnen, J. P. C., Wan, J. (2007). Optimization of simulated systems: OptQuest and alternatives. Simulation Modelling Practice and Theory 15(3):354-362.</p>
<p>Batch sequential designs for computer experiments. A M Law, J L Loeppky, L Moore, B J Williams, Journal of Statistical Planning and Inference. 1406Simulation Modeling and AnalysisLaw, A. M. (2007). Simulation Modeling and Analysis. 4th ed. New York: McGraw-Hill Education. Loeppky, J. L., Moore, L., Williams, B. J. (2010). Batch sequential designs for computer experiments. Journal of Statistical Planning and Inference 140(6):1452-1464.</p>
<p>Choosing the sample size of a computer experiment: A practical guide. J L Loeppky, J Sacks, W J Welch, Techometrics. 514Loeppky, J. L., Sacks, J., Welch, W. J. (2009). Choosing the sample size of a computer experiment: A practical guide. Techometrics 51(4):366-376.</p>
<p>Exploration of metamodeling sampling criteria for constrained global optimization. P Papalambros, P Goovaerts, M J Sasena, Engineering Optimization. 34Papalambros, P., Goovaerts, P., Sasena, M. J. (2002). Exploration of metamodeling sampling criteria for constrained global optimization. Engineering Optimization 34:263-278.</p>
<p>Sequential experiment design for contour estimation from complex computer codes. P Ranjan, D Bingham, G Michailidis, Technometrics. 504Ranjan, P., Bingham, D., Michailidis, G. (2008). Sequential experiment design for contour estimation from complex computer codes. Technometrics 50(4):527-541.</p>
<p>Sequential experimental designs for nonlinear regression metamodels in simulation. M I R D Santos, P M R D Santos, Simulation Practice and Theory. 169Santos, M. I. R. D., Santos, P. M. R. D. (2008). Sequential experimental designs for nonlinear regression metamodels in simulation. Simulation Practice and Theory 16(9):1365-1378.</p>
<p>An adaptive design and interpolation technique for extracting highly nonlinear response surfaces from deterministic models. D Shahsavani, A Grimvall, Reliability Engineering and System Safety. 947Shahsavani, D., Grimvall, A. (2009). An adaptive design and interpolation technique for extracting highly nonlinear response surfaces from deterministic models. Reliability Engineering and System Safety 94(7):1173-1182.</p>
<p>Customized sequential designs for random simulation experiments: Kriging metamodeling and bootstrapping. W C M Van Beers, J P C Kleijnen, European Journal of Operational Research. 1863Van Beers, W. C. M., Kleijnen, J. P. C. (2008). Customized sequential designs for random simula- tion experiments: Kriging metamodeling and bootstrapping. European Journal of Operational Research 186(3):1099-1113.</p>            </div>
        </div>

    </div>
</body>
</html>
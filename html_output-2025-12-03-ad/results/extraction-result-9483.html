<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9483 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9483</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9483</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-e24424283c02fbe7f641e5b3490d7bb059f8355a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e24424283c02fbe7f641e5b3490d7bb059f8355a" target="_blank">A Survey on LLM-as-a-Judge</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built?</p>
                <p><strong>Paper Abstract:</strong> Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of"LLM-as-a-Judge,"where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9483",
    "paper_id": "paper-e24424283c02fbe7f641e5b3490d7bb059f8355a",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004609,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Survey on LLM-as-a-Judge</h1>
<p>JIAWEI GU ${ }^{1, <em>}$, XUHUI JIANG ${ }^{1, </em>}$, ZHICHAO SHI ${ }^{1,2, *}$, HEXIANG TAN ${ }^{2}$, XUEHAO ZHAI ${ }^{3}$, CHENGJIN XU ${ }^{1}$, WEI LI ${ }^{2}$, YINGHAN SHEN ${ }^{2}$, SHENGJIE MA ${ }^{1,4}$, HONGHAO LIU ${ }^{1}$, SAIZHUO WANG ${ }^{1,6}$,KUN ZHANG ${ }^{2}$, ZHOUCHI LIN ${ }^{1}$, YUANZHUO WANG ${ }^{2}$, LIONEL NI ${ }^{5,6}$, WEN GAO $^{7}$,JIAN GUO ${ }^{1, \dagger}$<br>${ }^{1}$ IDEA Research, International Digital Economy Academy, China<br>${ }^{2}$ Institute of Computing Technology, Chinese Academy of Sciences, China<br>${ }^{3}$ Department of Civil and Environmental Engineering, Imperial College London, UK<br>${ }^{4}$ Gaoling School of Artificial Intelligence, Renmin University of China<br>${ }^{5}$ The Hong Kong University of Science and Technology, China<br>${ }^{6}$ The Hong Kong University of Science and Technology (Guangzhou), China<br>${ }^{7}$ Department of Computer Science and Technology, Peking University, China</p>
<h4>Abstract</h4>
<p>Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of "LLM-as-a-Judge," where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable and flexible assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey on LLM-as-aJudge, offering a formal definition and a detailed classification, while focusing on addressing the core question: How to built reliable LLM-as-a-Judge systems? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-aJudge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field. The associated resources can be accessed at https://awesome-llm-as-a-judge.github.io/ .</p>
<h2>1 INTRODUCTION</h2>
<p>Judgment is the faculty of thinking the particular as contained under the universal. It involves the capacity to subsume under rules, that is, to distinguish whether something falls under a given rule.
-- Kant, Critique of Judgment [61], Introduction IV, 5:179; Critique of Pure Reason [60], A132/B171.
Recently, Large Language Models (LLMs) have achieved remarkable success across numerous domains [179], ranging from technical fields [145, 191, 208] to the humanities [58, 101, 115, 214] and social sciences [49, 130, 165, 178]. Building on their success, the concept of using LLMs as evaluators-commonly referred to as "LLM-as-a-Judge" [210]-has gained significant attention, where LLMs are tasked with determining whether something falls within the scope of a given rule [60, 61]. This growing interest stems from LLMs' ability to mimic human-like reasoning and thinking processes, enabling them to take on roles traditionally reserved for human experts while offering a cost-effective solution that can be effortlessly scaled to meet increasing evaluation</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>demands. For instance, employing LLM-as-a-Judge in the academic peer-review ${ }^{1}$ process can help handle the rapid increase in submissions while maintaining expert-level judgment.</p>
<p>Before the era of LLMs, finding a balance between comprehensive and scalable evaluation posed a persistent challenge. On the one hand, widely used subjective methods like expert-driven assessments [41, 129] integrate holistic reasoning and fine-grained contextual understanding, making them the gold standard in comprehensiveness. However, these approaches are costly, difficult to scale, and susceptible to inconsistency. On the other hand, objective assessment methods, such as automatic metrics offer strong scalability and consistency. For example, tools such as BLEU [110] or ROUGE [87] can rapidly evaluate machine-generated translations or summaries against reference texts without human intervention. However, these metrics, which heavily rely on surface-level lexical overlaps, often fail to capture deeper nuances, resulting in poor performance in tasks like story generation or instructional texts [124]. As a solution to this persistent dilemma, "LLM-as-a-Judge" has emerged as a promising idea to combine the strengths of the above two evaluation methods. Recent studies have shown that this idea can merges the scalability of automatic methods with the detailed, context-sensitive reasoning found in expert judgments [19, 81, 163, 210, 220]. Moreover, LLMs may become sufficiently flexible to handle multimodal inputs [18] under appropriate prompt learning or fine-tuning [64]. These advantages suggest that the LLM-as-a-Judge approach could serve as a novel and broadly applicable paradigm for addressing complex and open-ended evaluation problems.</p>
<p>LLM-as-a-Judge holds significant potential as a scalable and adaptable evaluation framework compared to aforementioned two traditional methods [160]. However, the widespread application of this idea is hindered by two key challenges. The first challenge lies in the absence of a systematic review, which highlights the lack of formal definitions, fragmented understanding, and inconsistent usage practices in the relevant studies. As a result, researchers and practitioners struggle to fully understand and apply effectively. The second challenge involves addressing concerns about reliability [189], as merely employing LLM-as-a-Judge does not ensure accurate evaluations aligned with established standards. These challenges emphasize the need for a deeper assessment of the outputs generated by LLM-as-a-Judge, as well as a crucial investigation into the question: How to build reliable LLM-as-a-Judge systems?</p>
<p>To address these challenges, this paper provides a systematic review of research on LLM-as-aJudge. It offers a comprehensive overview of the field and explores strategies for building reliable LLM-as-a-Judge systems. We begin by defining LLM-as-a-Judge through both formal and informal definitions, answering the foundational question: "What is LLM-as-a-Judge?" Next, we categorize existing methods and approaches, exploring "How to use LLM-as-a-Judge?". Following this, to tackle the critical question: "How to build reliable LLM-as-a-Judge systems?", we explore two core aspects: (1) strategies to enhance the reliability of LLM-as-a-Judge systems and (2) methodologies for evaluating the reliability of these systems. For the first aspect, we review key strategies to optimize the performance of LLM-as-a-Judge. For the second aspect, we examine the metrics, datasets, and methodologies used to evaluate LLM-as-a-Judge systems, highlighting potential sources of bias and methods for their mitigation. Building on this, we introduce a novel benchmark specifically designed for evaluating LLM-as-a-Judge systems. Additionally, we explore practical application scenarios and identify challenges unique to each context. Finally, we discuss future research directions, emphasizing key areas for improving reliability, scalability, and applicability.</p>
<p>The rest of this survey is organized as Figure 1. Section 2 provides an overview of the LLM-as-a-Judge field, including its definitions and categorization of existing methods. For a quick guide on the implementation of an LLM as a judge for specific scenarios, you can find answers in Quick</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Practice (2.5). Strategies for enhancing and evaluating the reliability of LLM-as-a-Judge systems are discussed in Sections 3, 4, and 5, respectively. Notably, in Section 6, we discuss the synergy between LLM-as-a-Judge and o1-like reasoning enhancement, where dynamic feedback is used to optimize reasoning paths and significantly improve the model's ability to solve complex problems. Section 7 explores practical applications, while Sections 8 and 9 address challenges and outline future research directions. Finally, Section 10 presents our conclusions.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. The overall framework of this paper.</p>
<h1>CONTENTS</h1>
<p>Abstract ..... 1
1 Introduction ..... 1
2 Background and Method ..... 5
2.1 In-Context Learning ..... 5
2.2 Model Selection ..... 8
2.3 Post-processing Method ..... 9
2.4 Evaluation Pipeline ..... 11
2.5 Quick Practice ..... 14
3 Improvement Strategy ..... 15
3.1 Design Strategy of Evaluation Prompts ..... 15
3.2 Improvement Strategy of LLMs' Abilities ..... 17
3.3 Optimization Strategy of Final Results ..... 18
4 Evaluation of LLM Evaluators ..... 19
4.1 Basic Metric ..... 20
4.2 Bias ..... 21
4.3 Adversarial Robustness ..... 23
5 Meta-evaluation Experiment ..... 23
5.1 Experiment Settings ..... 24
5.2 Experiment Results and Analysis ..... 25
6 LLM-as-a-Judge and o1-like Reasoning Enhancement ..... 27
7 Applications ..... 29
7.1 Machine Learning ..... 29
7.2 Other Specific Domains ..... 32
8 Challenges ..... 34
8.1 Reliability ..... 34
8.2 Robustness ..... 35
8.3 Powerful Backbone Model ..... 36
9 Future Work ..... 36
9.1 More Reliable LLM-as-a-Judge ..... 37
9.2 LLM-as-a-Judge for Data Annotation ..... 37
9.3 MLLM-as-a-Judge ..... 38
9.4 More LLM-as-a-Judge Benchmarks ..... 38
9.5 LLM-as-a-Judge for LLM Optimization ..... 38
10 Conclusion ..... 39
References ..... 39</p>
<h1>2 BACKGROUND AND METHOD</h1>
<p>The capacity of LLMs to emulate human reasoning and evaluate specific inputs against a set of predefined rules has paved the way for "LLM-as-a-Judge." Existing studies indicate that LLM's scalability, adaptability, and cost-effectiveness make them well-suited for managing a growing number of evaluative tasks that were traditionally done by humans. These abilities are key in utilizing LLMs flexibly across various evaluation scenarios and objectives. As a result, adoption of LLM in evaluation has progressed rapidly in practice. Initially, the primary focus of LLMs was on language generation and comprehension. With advancements in training paradigms like Reinforcement Learning from Human Feedback (RLHF) [108], LLMs became increasingly aligned with human values and reasoning processes. This alignment has allowed LLMs to transition from generative tasks to evaluative roles. At its core, LLM-as-a-Judge denotes the use of LLMs to evaluate objects, actions, or decisions based on predefined rules, criteria, or preferences. It encompasses a broad spectrum of roles, including: Graders [31, 154], Evaluators/Assessors [82, 196], Critics [63, 112, 175], Verifiers [90, 131, 166], Examiners [9], Reward/Ranking Models [100, 139, 180, 193], etc.</p>
<p>Currently, the definition of how to effectively use LLM-as-a-Judge for evaluation tasks is largely informal or vague, lacking clear and formal expression. Therefore, we will start with a formal definition of LLM-as-Evaluator as follows:</p>
<p>$$
\mathcal{E} \leftarrow \mathcal{P}_{\mathcal{L} \mathcal{L} M}(x \oplus \mathcal{C})
$$</p>
<ul>
<li>$\mathcal{E}$ : The final evaluation obtained from the whole LLM-as-a-Judge process in the expected manner. It could be a score, a choice, a label or a sentence, etc.</li>
<li>$\mathcal{P}_{\mathcal{L} \mathcal{L} \mathcal{M}}$ : The probability function defined by the corresponding LLM, and the generation is an auto-regressive process.</li>
<li>$x$ : The input data in any available types (text, image, video), which waiting to be evaluated.</li>
<li>$\mathcal{C}$ : The context for the input $x$, which is often prompt template or combined with history information in dialogue.</li>
<li>$\oplus$ : The combination operator combines the input $x$ with the context $\mathcal{C}$, and this operation can vary depending on the context, such as being placed at the beginning, middle, or end.
The formulation of LLM-as-a-Judge reflects that LLM is a type of auto-regressive generative model, which generates subsequent content based on the context and then obtains target evaluation from it. It illustrates how we utilize LLM for evaluation tasks, encompassing input design, model selection, and training, as well as output post-processing. The basic approaches of implementing LLM-as-a-Judge can be classified according to the formulation: In-Context Learning, Model Selection, Post-processing Method, and Evaluation Pipeline, which concluded in Figure 2. By following this pipeline, one can build a basic LLM-as-a-Judge for evaluation. A quick practice guide is available in section 2.5 .</li>
</ul>
<h3>2.1 In-Context Learning</h3>
<p>To apply LLM-as-a-Judge, evaluation tasks are typically specified using In-Context Learning methods, which provide instructions and examples to guide the model's reasoning and judgment. This process involves two key aspects: input design and prompt design. For input design, it is important to consider the type of variables to be evaluated (such as text, image, or video), the manner of input (e.g., individually, in pairs, or in batches), and its position (e.g., at the beginning, middle, or end). For the prompt design, four different methods can be adopted, as illustrated in Figure 2. These methods include generating scores, solving true/false questions, conducting pairwise comparisons, and making multiple-choice selections. Further details will be presented in the following sections.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. LLM-as-a-Judge evaluation pipelines.
2.1.1 Generating scores. It is quite intuitive to represent an evaluation using a corresponding score. What requires more careful consideration, however, is the nature and range of the score used for evaluation. The score can be discrete, with common ranges like 1-3, 1-5 [59], or 1-10 [81, 220]. Alternatively, it can be continuous, ranging from 0 to 1 or 0 to 100 [175]. The simplest way to score is through the context, setting the range of scores and the main criteria for scoring. For example, "Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10 , where a higher score indicates better overall performance" [220]. A slightly more complex way is to provide more detailed scoring criteria. More complex scoring situations can be as Language-Model-as-an-Examiner [9], which use Likert scale scoring functions as an absolute evaluative measure, showed in Figure 4. The evaluator assigns scores to a given response along predefined dimensions including accuracy, coherence, factuality and comprehensiveness. Each of these dimensions is scored on a scale of 1 to 3 , ranging from worst
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. The illustrations of method generating scores in ICL</p>
<p>to best. The evaluator is also asked to provide an overall score ranging from 1 to 5 , based on the scores assigned to the previous 4 dimensions. This score serves as an indicator of the overall quality of the answer.</p>
<p>Evaluate the quality of summaries written for a news article. Rate each summary on four dimensions: {Dimension_1}, {Dimension_2}, {Dimension_3}, and {Dimension_4}. You should rate on a scale from 1 (worst) to 5 (best).</p>
<p>Article: {Article}
Summary: {Summary}</p>
<p>Fig. 4. The template for Likert scale scoring from Gao et al. [41].
2.1.2 Solving Yes/No questions. A Yes/No question requires a judgment on a given statement, focusing solely on its accuracy. This type of question is simple and direct, providing only two fixed responses-yes or no, true or false-without any additional comparisons or choices.</p>
<p>This type of evaluation is often utilized in intermediate processes, creating the conditions for a feedback loop. For example, it promotes a self-optimization cycle, as seen in Reflexion [131], which generates verbal self-reflections to provide valuable feedback for future attempts. In scenarios with sparse reward signals, such as a binary success status (success/fail), the self-reflection model uses the current trajectory and persistent memory to generate nuanced and specific feedback. Similarly, in self-improvement contexts [149], Yes/No questions can be employed to evaluate custom phrases, such as "Modification needed." and "No modification needed.", facilitating entry into the next cycle. Moreover, these evaluations are common for testing knowledge accuracy and assessing whether statements align with established facts [138], like "Given a question and the associated retrieved knowledge graph triples (entity, relation, entity), you are asked to answer whether it's sufficient for you to answer the question with these triples and your knowledge (Yes or No)." A detailed and specific example can be seen in the Figure 5.</p>
<p>Is the sentence supported by the article? Answer "Yes" or "No".</p>
<p>Article: ${$ Article $}$
Sentence: ${$ Sentence $}$</p>
<p>Fig. 5. The template for Yes/No evaluation for example.
2.1.3 Conducting pairwise comparisons. Pairwise comparison refers to comparing two options and selecting which one is superior or more aligned with a specific standard, showed in Figure 6. It involves making a decision between two options rather than judgement between 'yes' or 'no'. The comparison can be subjective or based on objective criteria. This evaluation is a relative evaluation. Pairwise comparison is often used for ranking multiple options or prioritizing them, where several comparisons are made between pairs to identify the better choice or establish a hierarchy.</p>
<p>Pairwise comparison is a well-established method that has significantly impacted a variety of fields [114]. As noted by [97], LLM and human evaluations are more aligned in the context of pairwise comparisons compared to score-based assessments. Numerous studies have demonstrated that pairwise comparative assessments outperform other judging methods in terms of positional</p>
<p>consistency [98, 210]. Furthermore, pairwise comparisons can be extended to more complex relationbased assessment frameworks, such as list-wise comparisons, using advanced ranking algorithms [97, 114], data filtering [193]. In pairwise comparative assessments, LLM-as-a-Judge is prompted to select the response that better answers the question at hand. To accommodate the possibility of a tie, several option modes are introduced. The Two-Option mode requires judges to choose the better response from two given options. The Three-Option mode introduces an additional choice, allowing judges to indicate a tie if neither response is preferable, as shown in Figure 9 (right). Evaluations typically involve determining the outcomes of win, tie, or loss for responses [163] through pairwise comparisons, with win rounds counted for each response. The Four-Option mode further expands the choices, allowing judges to classify responses as either a "both good tie" or a "both bad tie."</p>
<p>Given a new article, which summary is better? Answer "Summary 0" or "Summary 1". You do not need to explain the reason.</p>
<p>Article: {Article}
Summary 0: ${$ Summary_0}
Summary 1: ${$ Summary_1}
Fig. 6. The template for pairwise comparison from Gao et al. [41]
2.1.4 Making multiple-choice selections. Multiple-choice selections involve providing several options, not giving relative choices in pairwise comparison, nor making a yes/no judgment. The evaluator must choose the most appropriate or correct one. This method allows for a broader range of responses compared to true/false questions and can assess deeper understanding or preferences and an example is showed in Figure 7. However, this kind of prompt design is more rare than the first three.</p>
<p>You are given a summary and some semantic content units. For each semantic unit, choose those can be inferred from the summary, return their number.</p>
<p>Summary: {Summary}
Semantic content units:</p>
<ol>
<li>${\mathrm{SCU} _1}$</li>
<li>${\mathrm{SCU} _2}$
$\qquad$
n. ${\mathrm{SCU} _\mathrm{n}}$</li>
</ol>
<p>Fig. 7. The template for multiple-choice for example.</p>
<h1>2.2 Model Selection</h1>
<p>2.2.1 General LLM. To automate evaluation by LLM-as-a-Judge, one effective approach is to employ advanced language models such as GPT-4 [107] instead of human evaluators [210]. For instance, Li et al. [83] created a test set with 805 questions and assessed the performance by comparing it to text-davinci-003 using GPT-4. Additionally, Zheng et al. [210] designed 80 multiround test questions across eight common areas and used GPT-4 to automatically score the model's</p>
<p>responses. The accuracy of the GPT-4-based evaluator has been demonstrated to be high compared to professional human evaluators, showing superior consistency and stability in evaluations. At the same time, if the general LLM used has limitations in instruction-following or reasoning abilities, the effectiveness of the LLM-as-a-Judge method may be significantly affected.
2.2.2 Fine-tuned LLM. However, relying on external API for evaluation may introduce consideration about privacy leakage, and the opacity of API models also challenges the evaluation reproducibility. Therefore, subsequent studies recommend refining language models tailored for evaluations by emphasizing the use of pairwise comparisons or grading. For instance, PandaLM [163] constructs data based on Alpaca instructions and GPT-3.5 annotation, and then fine-tunes LLaMA-7B [151] as an evaluator model. JudgeLM [220] constructs data from diversified instruction sets and GPT-4 annotations, and fine-tunes Vicuna [152] as a scalable evaluator model. Auto-J [81] constructs evaluation data upon multiple scenarios to train a generative evaluator model, which can provide both evaluation and critical opinion. Prometheus [65] defines thousands of evaluation criteria and constructs a feedback dataset based on GPT-4, and fine-tunes a fine-grained evaluator model.</p>
<p>The typical process for fine-tuning a judge model involves three main steps as shown in Figure 8. Step 1: Data Collection. The training data generally consists of three components: instructions, the objects to be evaluated, and evaluations. Instructions are typically sourced from instruction datasets, while evaluations can come from either GPT-4 or human annotations. Step 2-Prompt Design. The structure of the prompt template can vary based on the evaluation scheme, which already detailed in § 2.1. Step 3: Model Fine-Tuning. Using the designed prompts and collected data, the fine-tuning process for the evaluator model typically adheres to the instruction fine-tuning paradigm [108]. The model receives an instruction along with one or more responses to generate output that includes evaluation results and possibly explanations.</p>
<p>After fine-tuning, the evaluator model can be employed to evaluate the target object. While these fine-tuned models often demonstrate superior performance on self-designed test sets, they are identified several limitations in their evaluation capabilities, which detailed in Section 4.2. The current prompt and fine-tuning dataset designs often result in evaluation LLMs with poor generalization, making them difficult to compare with strong LLMs like GPT-4.</p>
<h1>2.3 Post-processing Method</h1>
<p>Post-processing refines the probability distributions generated by LLM-as-a-Judge to ensure accurate evaluations. The evaluation format should align with our In-Context Learning design and may involve procedures to enhance the reliability of extracted evaluations, which should be applied consistently. We focus on three main post-processing methods: extracting specific tokens, normalizing the output logits, and selecting sentences with high returns.</p>
<p>However, it is important to note that each method has significant limitations when evaluating objective questions. For example, in text response evaluation [189], failing to accurately extract the key answer token from the LLM's response can result in incorrect evaluation outcomes. These challenges in post-processing are tightly linked to the prompt design used in earlier ICL stages and the selected model's ability to follow instructions reliably.
2.3.1 Extracting specific tokens. As showed in In-context Learning (Section 2.1), when the evaluation target take the form of a score, selecting specific options, or responding with Yes/No, applying rule-match to extract the corresponding token from the response generated during probability distribution iteration is common used. It is worth noting that Yes/No is a broad definition, including custom statements involving judgment. Considering a Yes/No question for evaluation in</p>
<p>custom phrases [149]: "Modification needed." and "No modification needed." or a yes-no question "Does the above answer need to be further modified?". When the input sample is put through the template, it might have outputs such as "Modification needed.", "Conclusion: Modification needed." or "Yes". This variance in response formats is difficult to parse consistently. The corresponding post-processing with the response is necessary. Using rules to extract specific tokens for our designed prompts and input content, as well as the backbone model used for the evaluator, all have higher requirements as we discussed in Section 2.2. In contextual learning, if there is no clear indication of the output format for response, there may be various expressions of evaluation, which can be seen in Figure 2. For example, "Response 1 is better" and "The better one is response 1 ", which convey the same choice but differ in format leading to the difficulty of rule recognition. Simple solutions often involve providing clear instructions, such as "The last sentence should be started with 'The better response is'", or using a few-shot strategy. Also, the general model with insufficient instruction following capability may not be able to generate the evaluation format and content of the target according to the instruction, resulting in the post-processing extracted according to the rules not as smooth as expected.
2.3.2 Constrained decoding. Constrained decoding is a technique that enforces structured output from Large Language Models (LLMs) by restricting token generation according to predefined schemas, typically in formats like JSON. This approach uses a finite state machine (FSM) to compute valid next tokens at each decoding step, effectively masking the model's output probability distribution to ensure conformity with the desired schema. While this method guarantees syntactically valid outputs, it presents several challenges: it can distort the model's learned distribution and potentially degrade output quality, requires significant engineering implementation effort, and introduces computational overhead during inference.</p>
<p>Recent work has proposed various solutions to address these challenges. [12] introduce DOMINO, a decoding algorithm that preserves natural tokenization while enforcing constraints. Their system minimizes overhead through precomputation and speculative decoding, sometimes achieving faster performance than unconstrained decoding. [32] develop XGrammar, which accelerates grammarconstrained generation by separating tokens into those that can be pre-checked and those requiring runtime verification. By co-designing the grammar engine with LLM inference, they achieve up to 100x speedup over existing approaches.[212] present SGLang, combining a domain-specific language with an optimized runtime. Their system features efficient KV cache reuse and compressed finite state machines for faster decoding, demonstrating that thoughtful co-design of programming model and runtime can minimize constrained decoding overhead.
2.3.3 Normalizing the output logits. LLM-as-a-Judge in the intermediate steps with Yes/No setting often normalize the output logits to obtain the evaluation in the form of a continuous decimal between 0 and 1 . This is also very common in agent methods and prompt-based optimization methods [47, 166, 224]. For example, the self-consistency and self-reflection scores [166] within one forward pass of $\mathcal{M}<em i="i">{\text {Evaluator }}$, are effectively obtained by constructing a prompt [( $x \oplus \mathcal{C}$ ), "Yes"] and acquire the probability of each token conditioned on the previous tokens $P\left(t</em>$.} \mid t_{&lt;i}\right)$. The autoregressive feature is leveraged, thus aggregate the probability of the relevant tokens to compute the self-consistent score $\rho_{\text {Self-consistency }}$ and self-reflection score $\rho_{\text {Self-reflection }}$. The final score is produced by $\rho_{j}=\rho_{\mathrm{SC}, j} \cdot \rho_{\mathrm{SR}, j</p>
<p>$$
\underbrace{\overbrace{(x \oplus \mathcal{C})}^{\rho_{\mathrm{SC}} \text { "Yes" }}}_{\text {« } \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty \infty}
$$</p>
<p>In addition, Self-evaluation [47] is also common using this method for LLM-as-a-Judge. It can be helpful to let the LLM evaluate itself by asking, "Is this reasoning step correct?" and then reward it based on the probability of the next word being "Yes."
2.3.4 Selecting sentences. In addition to selecting specific tokens and normalizing the output logits, the content extracted by LLM-as-a-Judge may also be a sentence or paragraph. As showed in Figure 2, agent for reasoning task [47], builds a reasoning tree by iteratively considering the most promising reasoning steps (actions, sub-questions) by LLM-as-a-Judge.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 8. Four typical scenarios using LLM-as-a-Judge evaluation pipeline.</p>
<h1>2.4 Evaluation Pipeline</h1>
<p>After completing the three processes, we obtain the final evaluation $\mathcal{E}$. From input to output, these steps collectively constitute the LLM-as-a-Judge evaluation pipeline, as illustrated in Figure 2. This pipeline is commonly applied in four scenarios shown in Figure 8: LLM-as-a-Judge for models, LLM-as-a-Judge for data, LLM-as-a-Judge for agents, and LLM-as-a-Judge for reasoning or thinking.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 9. The illustrations of the scenario LLM-as-a-Judge for Models. The example of "win-tie-lose" is from Li et al. [81]
2.4.1 LLM-as-a-Judge for Models. It is universally known that the best way to evaluate LLMs is human judgment, but collecting human annotations can be costly, time-consuming, and laborious [108, 210]. Using strong LLMs (usually closed-source ones, e.g., GPT-4, Claude, ChatGPT) as an automated proxy for assessing LLMs has become a natural choice [215], as shown in Figure 9.</p>
<p>With appropriate prompt design, the quality of evaluation and agreement to human judgment can be promising [35, 158, 205, 210]. However, the cost concern still exists when calling the APIs of these proprietary models, especially when there is a frequent need for model validation on large-scale data. Moreover, closed-source LLM-as-a-Judge leads to low reproducibility due to potential changes in models behind the API. Some recent works have started to make attempts for open-source alternatives. SelFee [187] collects generations, feedback, and revised generations from ChatGPT and fine-tunes LLaMA models to build a critique model. Shepherd [161] trains a model that can output critiques for single-response with the data of feedback from online communities and human annotation. PandaLM [163] trains a model to conduct pairwise comparison for LLM Instruction Tuning Optimization, and Zheng et al. [210] also fine-tune Vicuna [152] on a 20K pairwise comparison dataset to explore the potential of open-source models as a more cost-friendly proxy.
2.4.2 LLM-as-a-Judge for Data. Data annotation generally refers to the labeling or generating of raw data with relevant information, which could be used for improving the efficacy of machine learning models. The process, however, is labor-intensive and costly. The emergence of LLMs presents an unprecedented opportunity to automate the complicated process of data annotation by LLM-as-a-Judge. Most of the data need to be evaluated by LLM-as-a-Judge is generated by models, or large-scale crawled data. Language models first conduct supervised fine-tuning to imitate how to align with human instructions [146, 162]. After that, reinforcement learning techniques have been explored to align language models with human preferences [108, 120]. The most successful way is applying a RLHF framework [108] via training a reward model on human feedback and using PPO [125] to obtain the policy model for language generation. However, in practices, the PPO training paradigm is complex in coding and hyper-parameter tuning while it needs four models that are hard for training. This motivates us to explore simpler and more straightforward methods to align language models with human preferences. This involves how to use LLM-as-a-Judge to evaluate whether different responses are aligned with human preferences. For example, [31, 193] use general LLM (ChatGPT) to get better alignment with human preferences. The Aplaca prompts [146] is used as sampling queries to different models generate responses. And these data was evaluated by LLM-as-a-Judge to obtain human preference scores (reward score) to train a new language model. Other works would like to use Supervised Fine-Tuning (SFT) model itself as evaluator, like generating better-aligned datasets for SFT including hindsight-modified prompts [93, 203] and principle-driven self-alignment [140].</p>
<p>In addition, the lack of domain-specific model training data is a common phenomenon. In order to obtain annotated high-quality data, it is also very common to use LLM-as-a-Judge for the generation and evaluation of domain data. WizardMath [100] would use its Instruction Reward Model (IRM) as Evaluator, aiming to judge the quality of the evolved instructions on three aspects: i) Definition, ii) Precision, and iti) Integrity. To produce the ranking list training data of IRM, for each instruction, ChatGPT and Wizard-E are used to generate 2-4 evolved instructions respectively. Then we leverage Wizard-E to rank the quality of those 4-8 instructions.</p>
<p>However, solely relying on LLM-as-a-Judge for data annotation poses challenges, particularly as the value of annotated data diminishes with the rapid improvement of model performance. To address this, approaches like Self-Taught Evaluator [160] offer a promising alternative by eliminating the need for human annotations. This method leverages synthetic training data, starting with unlabeled instructions and generating contrasting outputs from models. These outputs are then used to train an LLM-as-a-Judge to produce reasoning traces and final judgments. With each iteration, the evaluator improves by learning from its refined predictions, creating a cycle of</p>
<p>continuous self-enhancement. This iterative approach not only keeps annotations relevant but also ensures that evaluators evolve alongside advancing models.</p>
<p>Recent research on evaluating multimodal data focuses on addressing vision-language misalignments in Multimodal Large Language Models (MLLMs), which often cause hallucinations-outputs inconsistent with visual or contextual evidence [26, 84, 157]. Techniques like Reinforcement Learning from Human Feedback (RLHF) and Factually Augmented RLHF have been employed to improve model alignment by incorporating structured ground-truth data and image captions, enhancing hallucination detection [139]. Benchmarks such as MLLM-as-a-Judge [18] assess these models using tasks like scoring, pair comparison, and batch ranking, revealing limitations in alignment with human preferences. Persistent issues include biases (e.g., position, verbosity) and hallucinations, with even advanced models like GPT-4V displaying challenges. While pair comparison tasks align better with human judgment, scoring and batch ranking require significant improvements for reliable deployment. These findings emphasize the need for innovative frameworks and datasets to refine MLLM evaluation and alignment.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 10. LLM-as-a-Judge appears in two common forms in the agent. The left diagram is Agent-as-a-Juge, designing a complete agent to serve as an evaluator. The right diagram shows using LLM-as-a-Judge in the process of an Agent.
2.4.3 LLM-as-a-Judge for Agents. There are two ways to apply LLM-as-a-Judge for an agent. One is to evaluate the entire process of the intelligent agent [225], and the other is to evaluate it at a specific stage in the agent framework process [47, 131]. Both approaches are briefly illustrated in Figure 10. Using LLM as the brain of agent, an agentic system [225] could evaluate like a human, it would reduce the need for human involvement and eliminate the trade-off between thoroughness and effort. In addition, the agent [131] can interact with the environment through language and receive feedback on actions through LLM to make decisions for the next action.
2.4.4 LLM-as-a-Judge for Reasoning/Thinking. Reasoning [52], defined as the cognitive process of applying logic, arguments, and evidence to draw conclusions, is central to intellectual tasks such as decision-making, problem-solving, and critical analysis. While reasoning is inherently more demanding and multifaceted than judging, it often depends on judgments to ensure logical coherence, refine intermediate steps, and achieve clarity in its outcomes. LLM-as-a-Judge, in this sense, becomes an integral tool for enhancing the reasoning capability of LLM.</p>
<p>The role of LLM-as-a-Judge in enhancing reasoning or thinking can be understood through two frameworks: scaling training time [40, 154] and scaling test time [132]. In the training phase, LLM-as-a-Judge frequently operates within reinforcement learning paradigms, where it functions as a reward model or evaluator for data or processes. This enables the creation of high-quality reasoning datasets through mechanisms such as step-by-step verification [86], Direct Preference Optimization(DPO) [117], and self-refinement [192]. Recently, several LLMs trained with reinforcement learning to exhibit advanced reasoning and thinking abilities have gained attention, such as o1 ${ }^{2}$, DeepSeek-R1 ${ }^{3}$,gemini-thinking ${ }^{4}$, and QVQ ${ }^{5}$. In the test-time framework, LLM-as-a-Judge is crucial for evaluating and selecting the best reasoning paths. For example, in "Best-of-N" generation scenarios, where multiple reasoning outputs are produced, the judge determines the most accurate and coherent response. This dual role in both training and test phases demonstrates the indispensable nature of LLM-as-a-Judge in enhancing reasoning systems.</p>
<h1>2.5 Quick Practice</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 11. Flowchart of Quick Practice
To effectively apply LLM-as-a-Judge design, it is recommended to find more effective configurations in the testing cycle for various scenarios. The success of using LLM-as-a-Judge also heavily depends on the implementation details, including the task complexity, the prompt design, the model selected, and the post-processing method. As shown in Figure 11, The process of quick practice for LLM-as-a-Judge involves four main stages. First is the thinking phase, in which users define the evaluation objectives by determining what needs to be evaluated, understanding typical human evaluation approaches, and identifying some reliable evaluation examples. Next is prompt design, detailed in Section 2.1, both wording and formats matter. The most efficient and generally effective approach involves specifying scoring dimensions, emphasizing relative comparisons for improved assessments, and creating effective examples to guide the LLM. The third stage, model selection (Section 2.2), focuses on choosing a large-scale model with strong reasoning and instruction-following abilities to ensure reliable evaluations. Finally, standardizing the evaluation process ensures that the outputs are structured (Section 2.3). This can be achieved by using specific formats like \boxed{XX}, numerical scores, or binary responses (e.g., "Yes" or "No").</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 12. Structure of Improvement Strategy.</p>
<p>The entire process includes iterative testing with cases and refinement through retesting thereby enhancing reliability. During development, it is essential to compare models or prompts and verify ongoing improvements.</p>
<h1>3 IMPROVEMENT STRATEGY</h1>
<p>When directly utilizing LLMs to conduct evaluation tasks-such as scoring, selection, pairwise comparison, or ranking-their inherent biases of LLMs like length bias, position bias, and concreteness bias[111] will undermine evaluation outcomes. Mitigating these inherent biases and improving the overall evaluation performance of LLMs remains a critical challenge for applying LLMs as evaluators. In this section, we introduce three improvement strategies to boost the evaluation performance of LLM-as-a-judge: design strategy of evaluation prompts (in-context learning based), improvement strategy of LLMs' evaluation capabilities (model-based), and optimization strategy of final evaluation results (post-processing based). As shown in Figure 12, our categorization is based on the formal definition of LLM-as-a-judge in Section 2, focusing on enhancing the evaluation effectiveness by targeting three key phases of the process: the context $\overline{\boldsymbol{C}}$, the abilities of LLMs themselves $\mathcal{P}_{\text {LLM }}$ and the post-processing $\leftarrow$ to obtain the final results $\mathcal{E}$</p>
<h3>3.1 Design Strategy of Evaluation Prompts</h3>
<p>An evaluation prompt is an input to LLM evaluators, which is used to guide the LLMs to complete the required evaluation tasks. LLMs possess in-context learning ability, enabling them to learn how to perform specified tasks from relevant examples or instructions in prompts, without requiring weight updates or retraining[15]. This suggests that the design strategy of evaluation prompts will significantly impact the effectiveness of LLM-as-a-judge. Therefore, optimizing the design of evaluation prompts, including better methods to help LLMs interpret the evaluation tasks and generate results, is the most direct and effective way to boost the evaluation performance of LLM-as-a-judge.
3.1.1 Optimizing LLMs' Understanding of Evaluation Tasks. In optimization methods of prompting LLMs to better understand evaluation tasks, one of the most commonly used and effective approaches is few-shot prompting[15]. By incorporating several high-quality evaluation</p>
<p>examples into the evaluation prompts, LLM evaluators can effectively grasp the objectives, general processes, and rough evaluation criteria of evaluation tasks. Many research works employ this prompt paradigm for evaluation, such as FActScore [105], SALAD-Bench [82], and GPTScore [36].</p>
<p>In addition to providing high-quality examples for LLMs, refining the evaluation task instructions is also an effective approach to optimize LLMs' understanding of evaluation tasks. Current methods for refining evaluation tasks mainly include the decomposition of evaluation steps and criteria: (a) Decomposition of Evaluation Steps entails breaking down the entire evaluation tasks into smaller steps, providing detailed definitions and constraints for each small step in prompts, thereby guiding LLMs comprehensively through the whole evaluation pipeline. For instance, GEval [95] and DHP [164] use Chain-of-Thought(CoT) [168] to guide LLMs. SocREval [48] employs the Socratic method to meticulously design each step to enhance evaluation performance. Saha et al. propose Branch-Solve-Merge(BSM) [122], which divides evaluation tasks into multiple parallel sub-tasks for separate evaluation and final merge. (b) Decomposition of Evaluation Criteria involves breaking down coarse evaluation criteria like Fluency into finer-grained sub-criteria like Grammar, Engagingness, and Readability, and then generating overall scores based on these different dimensions. HD-Eval [96] iteratively aligns LLM evaluators with human preference via hierarchical criteria decomposition thereby addressing the potential bias in LLMs. Hu and Gao et al. [50] summarize and clearly define an explicit hierarchical classification system encompassing 11 criteria, addressing the issue of LLMs potentially confusing different evaluation standards. These refinements are specific to enable LLMs to understand the details of evaluation tasks more deeply, thereby aligning evaluation results more closely with human evaluation requirements and preferences.</p>
<p>Furthermore, the evaluation capabilities can be optimized based on specific shortcomings of LLMs in prompts. For instance, to address specific biases like position bias which is common in pairwise evaluations, several research efforts have optimized prompts design by randomly swapping contents to be evaluated. Wang et al. [158] analyzed and validated the impact of position bias on LLM-as-a-judge and proposed a calibration framework to mitigate this bias by swapping the contents and averaging the scores. Auto-J [81] and JudgeLM [220] also enhance the evaluation consistency by shuffling the texts to be evaluated. In contrast to averaging scores, PandaLM [163] annotates the conflicting evaluation results after swapping as "Tie" to address the position bias.</p>
<p>To address the challenge of LLMs' absolute scoring being less robust than relative comparing [118], some research works convert scoring tasks into pairwise comparison, thereby enhancing the reliability of evaluation results. Liu et al. [97] transform the scoring evaluation to ranking evaluation and introduce Pairwise-Preference Search (PARIS), which employs LLMs to conduct pairwise comparisons locally and efficiently ranks candidate texts globally, making evaluation results more aligned with human preferences.</p>
<p>In summary, the design of prompts for better understanding evaluation tasks is a core method for optimizing LLMs' in-contextual learning abilities. By refining evaluation task instructions and criteria in prompts or few-shot prompting with high-quality examples, the details of evaluation prompts can be enriched and the understanding of LLMs on evaluation tasks can be directly or indirectly enhanced. Additionally, targeted adjustments to prompts can address potential biases of LLMs such as position bias.
3.1.2 Optimizing LLMs' Output Forms. Directly requiring LLM evaluators to output evaluation results poses robustness problems. The response text may unexpectedly vary due to the inherent generative randomness of LLMs, such as outputting text like "low relevance" while asked to measure it with discrete scores, which hinders the automated and accurate extraction of evaluation results from LLMs' output. An effective method to enhance the robustness of output forms is to constrain</p>
<p>LLMs' output in structured formats within prompts. G-Eval[95] and DHP framework[164] perform evaluation tasks with a form-filling paradigm, constraining outputs with formats like " $X$ : $Y$ ", where $X$ represents the dimension or metric to be evaluated and $Y$ denotes an identifiable output form like scores or specific tokens. LLM-EVAL[88] further modifies this form-filling paradigm, efficiently outputs evaluation results in JSON format, and obtains multidimensional scores, leveraging LLMs' high understanding and generation capabilities of code-like textural formats.</p>
<p>Apart from challenges in robustness, directly outputting evaluation results by LLMs also suffer from the lack of interpretability. The meaning of evaluation results from LLM evaluators is difficult to align consistently with instructions and metrics provided in prompts. To address the challenges, CLAIR[17] requires LLMs to output evaluation scores between 0-100 simultaneously with relevant reasons as explanations in JSON format, which enhances the rationality and interpretability of the scores. FLEUR[76] utilizes LLaVA to first provide quality scores for image captions and subsequently asks with "Why? Tell me the reason." for explanations with the images, captions, and scores as inputs, offering a stepwise approach to provide interpretable scores.</p>
<p>In general, by constraining or guiding the output process and format of LLM evaluators within prompts, the robustness and rationality of evaluation results can be effectively improved through structured outputs. This also facilitates the automated post-processing of evaluation results in subsequent steps, thereby enhancing the overall stability of the evaluation pipeline.</p>
<h1>3.2 Improvement Strategy of LLMs' Abilities</h1>
<p>The evaluation capabilities of LLMs are a reflection of their powerful general language understanding and generation abilities triggered by specific prompts. Methods for optimizing evaluation through prompt design-focused on LLMs' in-contextual learning capabilities-require LLMs to fully comprehend the meaning of prompts and consistently follow the relevant evaluation instructions. However, even state-of-the-art LLMs like GPT-4 encounter problems such as conceptual confusion[50], and smaller open-source LLMs have even more limitations in their evaluation capabilities. Consequently, refining the evaluation capabilities of LLMs, including how to fine-tune LLMs through meta-evaluation datasets and how to iteratively optimize models based on the feedback of evaluation results, is significant for improving the fundamental evaluation performance of LLM-as-a-judge.
3.2.1 Fine-tuning via Meta Evaluation Datasets. A straightforward approach to enhancing the evaluation capabilities of LLMs is to fine-tune them via meta-evaluation datasets specifically constructed for evaluation tasks, which helps improve the LLMs' understanding of specific evaluation prompts, boosts the evaluation performance, or addresses potential biases. The most critical step in this optimization strategy is the collection and construction of training data. A common method involves sampling evaluation questions from publicly available datasets, modifying them with certain templates, and supplementing the dataset with evaluation responses generated either manually or by powerful LLMs like GPT4. For instance, PandaLM[163] samples inputs and instructions from Alpaca 52K[146] and generates responses using GPT-3.5 to construct training data, while SALAD-Bench[82] builds its training data from a subset of LMSYS-Chat[211] and Toxicchat[89].</p>
<p>To better align with the requirements of evaluation tasks, many research works further transform inputs and instructions sampled from public datasets to construct more targeted training data. OffsetBias[111] aims to reduce biases of LLMs by using GPT4 to generate off-topic versions of the original inputs and then having GPT-3.5 respond to the new inputs to produce bad responses. By pairing good and bad responses as training data to fine-tune the LLMs as evaluators, the biases in LLMs are significantly reduced, including length bias, concreteness bias, knowledge bias, and so on. JudgeLM[220] enhances LLMs' evaluation capabilities by creating different types of training</p>
<p>data through paradigms like reference support and reference drop. CritiqueLLM[63] proposes a multi-path prompting approach, combining pointwise-to-pairwise and referenced-to-reference-free prompting strategies to restructure referenced pointwise grading data into four types, which helps create Eval-Instruct to fine-tune LLMs, addressing shortcomings in pointwise grading and pairwise comparison.</p>
<p>In summary, constructing meta-evaluation training data targeted at specific evaluation tasks and fine-tuning LLMs can directly adjust the model's internal parameterized knowledge and language abilities. This is the most straightforward method to improve the evaluation performance of LLM evaluators and address potential biases.</p>
<h1>3.2.2 Iterative Optimization Based on Feedback of Evaluation Results. Fine-tuning LLMs</h1>
<p>on meta-evaluation datasets gives them the ability to produce evaluations that are more aligned with human preferences. However, LLM-as-a-judge may still introduce biases during the evaluation process in practice, which can impact the overall evaluation quality. A natural improvement strategy is to iteratively optimize the model based on the feedback of evaluation results, which mainly comes from stronger models or directly from human evaluators' correction of the evaluation results.</p>
<p>A typical example is INSTRUCTSCORE[177]. To improve model performance and further benefit the final quality score calculation, this scoring framework collects failure modes of metric outputs, queries GPT-4 on each failure mode to gather automatic feedback, and finally selects explanations most aligned with human preferences to iteratively fine-tune the LLaMA model. Unlike INSTRUCTSCORE which directly optimizes the model, the LLM evaluator in JADE[201] relies on human judges to correct LLMs' evaluation results and updates the most frequently corrected samples into the example sets for few-shot prompting. JADE utilizes this relatively low-cost method to achieve iterative updates of the evaluation capabilities.</p>
<p>Since the feedback is more closely aligned with human preferences, LLM evaluators can dynamically align with humans when optimizing evaluation capabilities based on this feedback, leading to better evaluation results. This feedback-based iterative optimization strategy addresses the problem of models' imperfect generalization and improves the evaluation capabilities through dynamic updates.</p>
<h3>3.3 Optimization Strategy of Final Results</h3>
<p>Through optimization based on in-context learning and the model' own capabilities, LLMs have become fairly reliable evaluators that are capable of understanding evaluation task requirements and providing rational evaluation results. However, the inherent generation randomness within the black box of LLMs still introduces significant instability to the entire evaluation pipeline, affecting the overall evaluation quality. Therefore, optimization strategies during the post-processing stage from LLM evaluators' outputs to final evaluation results are necessary. In this survey, these optimization strategies are categorized into three types: integration of multiple evaluation results, direct optimization of LLMs' outputs, and conversion of evaluation tasks from pointwise evaluation to pairwise comparison.
3.3.1 Integration of Multiple Evaluation Results. Integrating multiple evaluation results for the same content to obtain the final result is a common strategy in various experiments and engineering pipelines, which can reduce the impacts of accidental factors and random errors. The most basic optimization strategy is to perform multiple runs of evaluation on the same content with different hyper-parameters and settings, and then summarize these results. For example, the work of Sottana et al.[136] reduces randomness in evaluations by averaging multiple scores of the same sample. Similarly, PsychoBench[53] takes the mean and standard deviation from ten</p>
<p>independent runs. Auto-J[81] further amplifies the differences between evaluation rounds, which combine critiques with and without scenario criteria to obtain the final results.</p>
<p>In addition to integrating results from multiple rounds of evaluation, using multiple LLM evaluators to assess the contents simultaneously and integrating the results is another effective method, which can reduce biases introduced by LLMs. For instance, CPAD[91] utilizes ChatGLM-6B[34], Ziya-13B[199], and ChatYuan-Large-v2[200] as evaluators to evaluate the contents and obtain the final results by voting. Bai et al.[9] propose a novel evaluation method called decentralized peer review of LLMs, which utilizes LLMs that generate contents to evaluate each other's generated contents and eventually integrate the results.</p>
<p>In summary, forming the final evaluation results by combining multiple rounds of evaluations or multiple LLM evaluators can reduce the random effects caused by accidental factors in a single round and reduce the potential biases of a single LLM evaluator. This strategy significantly enhances the stability and reliability of the evaluation results.
3.3.2 Direct Optimization of LLMs' Outputs. Different from obtaining evaluation results based on the outputs of multiple rounds or LLMs, directly optimizing the output of a single LLM evaluator involves further processing the evaluation output to make it more reliable, especially when dealing with scoring outputs from LLM evaluators. Due to the inherent randomness in LLMs' generation, the scores may not fully reflect the LLMs' complete view of the evaluation criteria. Therefore, to obtain more reliable evaluation results, it is necessary to optimize the LLM's score outputs. An effective optimization strategy is to combine the implicit logits which capture the LLMs' randomness with the explicit output scores. For example, FLEUR[76] proposes a score smoothing strategy. For scores generated by LLaVA, the probability of the token corresponding to each digit $l(0 \leq l \leq 9)$ would be used as the weight to smooth the explicit scores and calculate the final evaluation scores.</p>
<p>However, methods like score smoothing, which combine implicit logits and explicit outputs require the LLMs to be open-source or to provide interfaces that allow access to token probabilities, which brings some limitations. Inspired by the work of Weng et al.[169] and Madaan et al.[104], self-verification can be used to filter out the evaluation results without sufficient robustness. For example, TrueTeacher[42] applies self-verification in its evaluation of distilled data by asking the LLM evaluator for its certainty about the evaluation results after providing them and retaining only those results that pass self-verification. Self-verification is suitable for all LLMs and requires no complex computing and processing.</p>
<p>In summary, compared to integrating multiple evaluation results, directly optimizing the LLMs' outputs to obtain the final results is faster and more low-cost, although the effectiveness still needs further validation. However, these two approaches are not mutually exclusive. Performing integration after direct optimization of LLMs' output may lead to more stable evaluation results.</p>
<h1>4 EVALUATION OF LLM EVALUATORS</h1>
<p>Despite their impressive performance, LLMs exhibit several notable shortcomings, such as hallucinations [150], biases [37], and a lack of robustness [219]. When LLMs are employed as evaluators, these inherent issues can lead to suboptimal evaluation outcomes. Therefore, it is crucial to accurately and comprehensively assess the quality of LLM-as-a-judge and identify potential vulnerabilities. This section will review existing work on the evaluation of LLM-as-a-judge, focusing on three key areas: base metric (Section 4.1), bias (Section 4.2), and robustness (Section 4.3).</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 13. Structure of Meta-Evaluation.</p>
<h1>4.1 Basic Metric</h1>
<p>The main objective of LLM-as-a-judge is to achieve alignment with human judges. Numerous studies approach this by considering the LLM evaluator as a virtual annotator and evaluating the extent of its agreement with human annotators. The percentage agreement metric represents the proportion of samples on which LLM and human annotators agree [147].</p>
<p>$$
\text { Agreement }=\frac{\sum_{i \in \mathcal{D}} \mathbf{I}\left(S_{\text {llm }}=S_{\text {human }}\right)}{|\mathcal{D}|}
$$</p>
<p>where $\mathcal{D}$ is the dataset, $S_{\text {llm }}$ and $S_{\text {human }}$ is the evaluation result of LLM evaluator and human judge respectively, which can be in the form of both score or rank. Additionally, widely used correlation metrics such as Cohen's Kappa [147] and Spearman's correlation [9, 97] are also employed to access agreement. Other works treat the LLM-as-a-judge task as a classification problem, where human annotations serve as the labels, and compute precision, recall, and F1 scores to evaluate the performance $[163,220]$.</p>
<p>Datasets. Both of above metrics rely on the datasets with LLM-generated response and responding human judgments. Therefore, there is also a practical need to construct a comprehensive benchmark for the meta-evaluation. We list the existing benchmarks and their statistics in Table 1. MTBench [210] has only 80 human-crafted queries with their responding human annotation and LLMs' responses. FairEval [158] is constructed from the 80 queries from VicunaBench [152] with human annotated preference between ChatGPT and Vicuna responses. Chatbot Arena Conversations [210] is a larger collection of crowdsourced data (about 30k) with human annotated preference. Research [195] construct a benchmark to access the capability of LLM evaluator in evaluating whether a response is following the instruction. This dataset contains human-curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator. Research [18] evaluate the capabilities of multi-modal LLMs in assisting evaluation tasks across various modalities and introduce MLLM-as-a-Judge, a comprehensive multi-modal benchmark. Recent advances also expand the scope of meta-evaluation benchmarks to specialized domains, including code assessment [209] and non-English language</p>
<p>tasks [133]. Furthermore, CALM [186] presents a systematic framework for bias quantification, featuring an automated perturbation mechanism to generate meta-evaluation data for examining 12 distinct types of potential biases in LLM evaluators.</p>
<p>Current meta-evaluation primarily focuses on LLM-as-a-judge for models, while there is a lack of sufficient meta-evaluation when these LLM evaluators are used for automatically annotating largescale datasets (Section 2.4.2). We advocate for more rigorous assessment of the alignment between LLM-as-a-judge and human judgment when they are employed for large-scale data annotation. Additionally, it is also crucial to assess the potential bias and robustness, which will be discussed in the following sections.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">Release Year</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Annotation Format</th>
<th style="text-align: center;">Evaluation Dimension</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Agreement</td>
<td style="text-align: center;">Position Bias</td>
<td style="text-align: center;">Length Bias</td>
<td style="text-align: center;">Bias Types</td>
</tr>
<tr>
<td style="text-align: center;">MTBench [210]</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Chatbot Arena [210]</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">30k</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">FairEval [158]</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">PandaLM [163]</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">LLMEval ${ }^{2}$ [205]</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">2553</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Shepherd [161]</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">1317</td>
<td style="text-align: center;">Score</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">EvalBiasBench [111]</td>
<td style="text-align: center;">2023</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">CALM [186]</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">4356</td>
<td style="text-align: center;">Pairwise \&amp; Score</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">12</td>
</tr>
<tr>
<td style="text-align: center;">JudgeBench [142]</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">MLLM-as-a-Judge [18]</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">30k</td>
<td style="text-align: center;">Pairwise \&amp; Score</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">CodeJudge [209]</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">1860</td>
<td style="text-align: center;">Score</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">KUDGE [133]</td>
<td style="text-align: center;">2024</td>
<td style="text-align: center;">3324</td>
<td style="text-align: center;">Pairwise \&amp; Score</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 1. Benchmark for meta-evaluation of LLM-judge.</p>
<h1>4.2 Bias</h1>
<p>Previous reviews have highlighted that large language models exhibit various types of biases across various tasks [27, 37, 141]. These internal biases of LLMs may also affect LLM-as-a-judge, leading to unfair evaluation outcomes and subsequently impacting the development of LLMs. Therefore, it is crucial to understand the types of biases that LLM evaluators might possess and to systematically assess these biases. In this section, we systematically review various types of biases in the LLM-as-a-judge context, including their definitions, relevant metrics, and datasets that can be used for evaluation.</p>
<p>The meta-evaluation of LLM-as-a-judge introduces systematic biases that can be broadly categorized into two classes: task-agnostic biases inherent to LLMs across general applications, and judgment-specific biases unique to LLM-as-a-judge scenarios. This taxonomy aims to clarify their distinct characteristics and implications.
4.2.1 Task-Agnostic Biases. These biases manifest across diverse LLM applications, including opendomain QA, classification, and summarization. However, when arising in the LLM-as-a-judge, the biases are particularly critical due to their cascading effects on downstream tasks. When LLMgenerated judgments serve as feedback for model training or data annotation, these biases risk being amplified and propagated. We present a few typical examples and recommend consulting comprehensive reviews on language model bias [38, 45] for a more thorough understanding.</p>
<p>Diversity Bias refers to bias against certain demographic groups [186], including certain genders [20], race, and sexual orientation [71]. In the context of LLM-as-a-judge scenarios, this bias may appear when evaluators give higher scores to responses that align with stereotypes of certain groups.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://openai.com/index/learning-to-reason-with-llms/
${ }^{3}$ https://api-docs.deepseek.com/news/news1120
${ }^{4}$ https://ai.google.dev/gemini-api/docs/thinking-mode
${ }^{5}$ https://huggingface.co/Qwen/QVQ-72B-Preview&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
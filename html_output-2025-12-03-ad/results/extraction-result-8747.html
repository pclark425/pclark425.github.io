<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8747 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8747</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8747</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-272367723</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.00082v1.pdf" target="_blank">Towards Human-Level Understanding of Complex Process Engineering Schematics: A Pedagogical, Introspective Multi-Agent Framework for Open-Domain Question Answering</a></p>
                <p><strong>Paper Abstract:</strong> In the chemical and process industries, Process Flow Diagrams (PFDs) and Piping and Instrumentation Diagrams (P&IDs) are critical for design, construction, and maintenance. Recent advancements in Generative AI, such as Large Multimodal Models (LMMs) like GPT4 (Omni), have shown promise in understanding and interpreting process diagrams for Visual Question Answering (VQA). However, proprietary models pose data privacy risks, and their computational complexity prevents knowledge editing for domain-specific customization on consumer hardware. To overcome these challenges, we propose a secure, on-premises enterprise solution using a hierarchical, multi-agent Retrieval Augmented Generation (RAG) framework for open-domain question answering (ODQA) tasks, offering enhanced data privacy, explainability, and cost-effectiveness. Our novel multi-agent framework employs introspective and specialized sub-agents using open-source, small-scale multimodal models with the ReAct (Reason+Act) prompting technique for PFD and P&ID analysis, integrating multiple information sources to provide accurate and contextually relevant answers. Our approach, supported by iterative self-correction, aims to deliver superior performance in ODQA tasks. We conducted rigorous experimental studies, and the empirical results validated the proposed approach effectiveness.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8747.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8747.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative Self-Correction (Reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Self-Correction through Reflection (verify-then-correct multi-agent loop)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A verify-then-correct, introspective multi-agent procedure in which a main agent produces answers, a critique agent (a high-quality 'gold' LMM) evaluates those answers and returns feedback, and sub-agents use that feedback to regenerate improved answers in repeated cycles until a termination criterion is met.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaliGemma (sub-agents, 3B), Gemma (main agent, 7B-it), GPT-4 Turbo (critique, proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The framework uses small/medium open multimodal models as sub-agents (PaliGemma-3B-mix fine-tuned via PEFT/QLoRA), a Gemma-based main agent (Gemma-7B-it) for orchestration and ReACT prompting, and GPT-4 Turbo as the high-quality external judge (critique agent). Student SMMs were instruction- and preference-tuned using teacher LMMs and PEFT.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative self-correction through reflection (verify-then-correct)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The main agent forwards an initial answer from a sub-agent to a critique agent (GPT-4 Turbo) which evaluates question-answer relevance and factual correctness (using comparisons against gold standards and NLP metrics). The critique returns feedback c_i; the main agent appends c_i to an updated candidate-generation prompt p'_can and the sub-agent generates a refined answer a_{i+1}. This cycle repeats until a stopping condition (target accuracy or fixed iterations) is met.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA and multimodal VQA on Process Flow Diagrams (PFDs) and P&IDs; image captioning; OCR/text detection; multiple-choice VQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Domain-specific ODQA and VQA tasks over parsed process-engineering documents (PFDs, P&IDs) including: image captioning, open/closed-ended VQA (including multi-step reasoning), multiple-choice VQA, and OCR/text-detection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported scores are for the full multi-agent framework (which includes the iterative self-correction component): Image captioning (Ours w/PaliGemma) BLEU-4 = 0.921 ± 0.141, ROUGE-L = 0.951 ± 0.073, METEOR = 0.956 ± 0.106 (Table 1); Open/closed VQA (Ours w/PaliGemma) BLEU-4 = 0.905 ± 0.15, ROUGE-L = 0.940 ± 0.09, METEOR = 0.945 ± 0.12 (Table 2); Multiple-choice VQA (Ours w/PaliGemma) Precision = 0.934 ± 0.097, Recall = 0.903 ± 0.119, F1 = 0.933 ± 0.041, Exact Match = 0.928 ± 0.015 (Table 3); OCR (Ours w/PaliGemma) CER = 0.093 ± 0.099, WER = 0.127 ± 0.122 (Table 5). These reported metrics reflect the full system (including reflection), but the paper does not present per-iteration performance breakdowns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>The authors state ablation experiments that disable iterative self-correction (SC) and observe degraded performance relative to the full system; however, the provided text does not include the numeric ablation values for the SC-off condition, so no exact numbers are reported in the paper excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Implemented via prompt-level feedback loops and an external 'gold' LMM judge: (1) sub-agent generates answer candidates and conditional summaries; (2) main agent selects a_i and sends (q, C++, a_i) to critique LMM M_Ref (GPT-4 Turbo) which returns feedback c_i (often using NLP metrics and comparison to gold standards); (3) the sub-agent receives c_i embedded into an updated prompt p'_can and regenerates improved answers. The method uses explicit prompts (p_can, p_sum, p_val, p_rank, p'_can) and retrieval-augmented context (C++), and relies on memory and reranking components to refine evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Qualitative and aggregate quantitative evidence: the authors report that the full framework (which includes iterative self-correction) performs on par with or exceeds several strong baselines across captioning, VQA, and OCR metrics (Tables 1–5). They additionally claim ablation studies show that disabling SC degrades performance, indicating SC contributes positively; however, the excerpt does not contain per-component numeric deltas or per-iteration gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limitations discussed include: (1) small-scale multimodal student models can still produce inaccurate or non-truthful outputs and require external feedback to correct them; (2) the critique mechanism depends on a proprietary 'gold' LMM (GPT-4 Turbo), which raises privacy/cost/trust trade-offs and may be at odds with the on-premises privacy goals; (3) the paper does not specify a fixed or optimal iteration count and leaves stopping criteria as user-defined (target accuracy or fixed iterations), which may be brittle; (4) the paper does not report failure cases where reflection worsened answers or introduced bias—no explicit failure modes with numerical support are given in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>The paper uses ReACT prompting for agent reasoning and positions iterative self-correction as an inter-agent verification loop; it compares the full framework to baseline models (GPT-4 Turbo-preview, Claude-3 Opus, Gemini 1.0 Pro, InstructBLIP, LLaVA, MiniGPT-4) in final-task metrics, but it does not provide direct comparisons to other explicit self-reflection methods (e.g., Reflexion, self-consistency) nor to ablations of chain-of-thought vs. reflection beyond noting SC was ablated.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Human-Level Understanding of Complex Process Engineering Schematics: A Pedagogical, Introspective Multi-Agent Framework for Open-Domain Question Answering', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8747.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8747.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Turbo (critique agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Turbo used as Gold LMM-as-a-judge (critique agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-quality proprietary LMM used as a critique/judge to verify sub-agent outputs by comparing them to gold standards and computing feedback that drives iterative correction in the multi-agent loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenAI: Gpt-4 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A proprietary large multimodal language model (OpenAI GPT-4 family); exact parameter count and training details are not provided in the paper. Used as the 'gold' judge to evaluate answer relevance and factual correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Gold LMM-as-a-judge (critique evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Used to evaluate an answer candidate a_i by inspecting the question q, retrieved context C++, and the candidate; it returns feedback c_i and performs comparisons against GPT-4-generated gold-standards and NLP metrics (BLEU, ROUGE, METEOR) to assess factual correctness and relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Evaluation/critique subroutine within the ODQA/VQA pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Not a standalone benchmark; GPT-4 Turbo is used to score/critique sub-agent outputs on domain VQA/captioning/OCR tasks by generating feedback to guide correction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Operates as an external judge invoked by the main agent through prompts; it returns explicit feedback tokens c_i that are injected into updated generation prompts for sub-agents. The critique uses comparisons to gold outputs and standard NLP metrics to guide evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Used as the feedback source in the iterative loop that the authors claim reduces hallucination and improves factual correctness; evidence is indirect (overall system metrics and ablation statements mentioned) rather than per-iteration numeric evaluation produced by GPT-4 Turbo itself.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Dependence on a proprietary LMM undermines the stated goal of an on-premises open solution (privacy/cost/trust tradeoffs); the critique's judgments are only as reliable as the gold standards and metrics it uses, and the paper does not report calibration/consistency analyses or cases where GPT-4 Turbo provided incorrect or misleading feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>GPT-4 Turbo is presented as the 'gold' judge; the paper does not report comparisons between GPT-4 Turbo-based critique and alternative automatic verifiers or learned critic modules.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Human-Level Understanding of Complex Process Engineering Schematics: A Pedagogical, Introspective Multi-Agent Framework for Open-Domain Question Answering', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>React: Synergizing reasoning and acting in language models. <em>(Rating: 2)</em></li>
                <li>Summarizing retrievals using answer candidates for open-domain qa of llms. <em>(Rating: 2)</em></li>
                <li>OpenAI: Gpt-4 technical report. <em>(Rating: 2)</em></li>
                <li>Gemma: Open models based on gemini research and technology. <em>(Rating: 1)</em></li>
                <li>Gemini: a family of highly capable multimodal models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8747",
    "paper_id": "paper-272367723",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Iterative Self-Correction (Reflection)",
            "name_full": "Iterative Self-Correction through Reflection (verify-then-correct multi-agent loop)",
            "brief_description": "A verify-then-correct, introspective multi-agent procedure in which a main agent produces answers, a critique agent (a high-quality 'gold' LMM) evaluates those answers and returns feedback, and sub-agents use that feedback to regenerate improved answers in repeated cycles until a termination criterion is met.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaliGemma (sub-agents, 3B), Gemma (main agent, 7B-it), GPT-4 Turbo (critique, proprietary)",
            "model_description": "The framework uses small/medium open multimodal models as sub-agents (PaliGemma-3B-mix fine-tuned via PEFT/QLoRA), a Gemma-based main agent (Gemma-7B-it) for orchestration and ReACT prompting, and GPT-4 Turbo as the high-quality external judge (critique agent). Student SMMs were instruction- and preference-tuned using teacher LMMs and PEFT.",
            "reflection_method_name": "Iterative self-correction through reflection (verify-then-correct)",
            "reflection_method_description": "The main agent forwards an initial answer from a sub-agent to a critique agent (GPT-4 Turbo) which evaluates question-answer relevance and factual correctness (using comparisons against gold standards and NLP metrics). The critique returns feedback c_i; the main agent appends c_i to an updated candidate-generation prompt p'_can and the sub-agent generates a refined answer a_{i+1}. This cycle repeats until a stopping condition (target accuracy or fixed iterations) is met.",
            "task_name": "Open-domain QA and multimodal VQA on Process Flow Diagrams (PFDs) and P&IDs; image captioning; OCR/text detection; multiple-choice VQA",
            "task_description": "Domain-specific ODQA and VQA tasks over parsed process-engineering documents (PFDs, P&IDs) including: image captioning, open/closed-ended VQA (including multi-step reasoning), multiple-choice VQA, and OCR/text-detection.",
            "performance_with_reflection": "Reported scores are for the full multi-agent framework (which includes the iterative self-correction component): Image captioning (Ours w/PaliGemma) BLEU-4 = 0.921 ± 0.141, ROUGE-L = 0.951 ± 0.073, METEOR = 0.956 ± 0.106 (Table 1); Open/closed VQA (Ours w/PaliGemma) BLEU-4 = 0.905 ± 0.15, ROUGE-L = 0.940 ± 0.09, METEOR = 0.945 ± 0.12 (Table 2); Multiple-choice VQA (Ours w/PaliGemma) Precision = 0.934 ± 0.097, Recall = 0.903 ± 0.119, F1 = 0.933 ± 0.041, Exact Match = 0.928 ± 0.015 (Table 3); OCR (Ours w/PaliGemma) CER = 0.093 ± 0.099, WER = 0.127 ± 0.122 (Table 5). These reported metrics reflect the full system (including reflection), but the paper does not present per-iteration performance breakdowns.",
            "performance_without_reflection": "The authors state ablation experiments that disable iterative self-correction (SC) and observe degraded performance relative to the full system; however, the provided text does not include the numeric ablation values for the SC-off condition, so no exact numbers are reported in the paper excerpt.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Implemented via prompt-level feedback loops and an external 'gold' LMM judge: (1) sub-agent generates answer candidates and conditional summaries; (2) main agent selects a_i and sends (q, C++, a_i) to critique LMM M_Ref (GPT-4 Turbo) which returns feedback c_i (often using NLP metrics and comparison to gold standards); (3) the sub-agent receives c_i embedded into an updated prompt p'_can and regenerates improved answers. The method uses explicit prompts (p_can, p_sum, p_val, p_rank, p'_can) and retrieval-augmented context (C++), and relies on memory and reranking components to refine evidence.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Qualitative and aggregate quantitative evidence: the authors report that the full framework (which includes iterative self-correction) performs on par with or exceeds several strong baselines across captioning, VQA, and OCR metrics (Tables 1–5). They additionally claim ablation studies show that disabling SC degrades performance, indicating SC contributes positively; however, the excerpt does not contain per-component numeric deltas or per-iteration gains.",
            "limitations_or_failure_cases": "Limitations discussed include: (1) small-scale multimodal student models can still produce inaccurate or non-truthful outputs and require external feedback to correct them; (2) the critique mechanism depends on a proprietary 'gold' LMM (GPT-4 Turbo), which raises privacy/cost/trust trade-offs and may be at odds with the on-premises privacy goals; (3) the paper does not specify a fixed or optimal iteration count and leaves stopping criteria as user-defined (target accuracy or fixed iterations), which may be brittle; (4) the paper does not report failure cases where reflection worsened answers or introduced bias—no explicit failure modes with numerical support are given in the provided text.",
            "comparison_to_other_methods": "The paper uses ReACT prompting for agent reasoning and positions iterative self-correction as an inter-agent verification loop; it compares the full framework to baseline models (GPT-4 Turbo-preview, Claude-3 Opus, Gemini 1.0 Pro, InstructBLIP, LLaVA, MiniGPT-4) in final-task metrics, but it does not provide direct comparisons to other explicit self-reflection methods (e.g., Reflexion, self-consistency) nor to ablations of chain-of-thought vs. reflection beyond noting SC was ablated.",
            "ablation_study_results": null,
            "uuid": "e8747.0",
            "source_info": {
                "paper_title": "Towards Human-Level Understanding of Complex Process Engineering Schematics: A Pedagogical, Introspective Multi-Agent Framework for Open-Domain Question Answering",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "GPT-4 Turbo (critique agent)",
            "name_full": "GPT-4 Turbo used as Gold LMM-as-a-judge (critique agent)",
            "brief_description": "A high-quality proprietary LMM used as a critique/judge to verify sub-agent outputs by comparing them to gold standards and computing feedback that drives iterative correction in the multi-agent loop.",
            "citation_title": "OpenAI: Gpt-4 technical report.",
            "mention_or_use": "use",
            "model_name": "GPT-4 Turbo",
            "model_description": "A proprietary large multimodal language model (OpenAI GPT-4 family); exact parameter count and training details are not provided in the paper. Used as the 'gold' judge to evaluate answer relevance and factual correctness.",
            "reflection_method_name": "Gold LMM-as-a-judge (critique evaluation)",
            "reflection_method_description": "Used to evaluate an answer candidate a_i by inspecting the question q, retrieved context C++, and the candidate; it returns feedback c_i and performs comparisons against GPT-4-generated gold-standards and NLP metrics (BLEU, ROUGE, METEOR) to assess factual correctness and relevance.",
            "task_name": "Evaluation/critique subroutine within the ODQA/VQA pipeline",
            "task_description": "Not a standalone benchmark; GPT-4 Turbo is used to score/critique sub-agent outputs on domain VQA/captioning/OCR tasks by generating feedback to guide correction.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Operates as an external judge invoked by the main agent through prompts; it returns explicit feedback tokens c_i that are injected into updated generation prompts for sub-agents. The critique uses comparisons to gold outputs and standard NLP metrics to guide evaluation.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Used as the feedback source in the iterative loop that the authors claim reduces hallucination and improves factual correctness; evidence is indirect (overall system metrics and ablation statements mentioned) rather than per-iteration numeric evaluation produced by GPT-4 Turbo itself.",
            "limitations_or_failure_cases": "Dependence on a proprietary LMM undermines the stated goal of an on-premises open solution (privacy/cost/trust tradeoffs); the critique's judgments are only as reliable as the gold standards and metrics it uses, and the paper does not report calibration/consistency analyses or cases where GPT-4 Turbo provided incorrect or misleading feedback.",
            "comparison_to_other_methods": "GPT-4 Turbo is presented as the 'gold' judge; the paper does not report comparisons between GPT-4 Turbo-based critique and alternative automatic verifiers or learned critic modules.",
            "ablation_study_results": null,
            "uuid": "e8747.1",
            "source_info": {
                "paper_title": "Towards Human-Level Understanding of Complex Process Engineering Schematics: A Pedagogical, Introspective Multi-Agent Framework for Open-Domain Question Answering",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "React: Synergizing reasoning and acting in language models.",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Summarizing retrievals using answer candidates for open-domain qa of llms.",
            "rating": 2,
            "sanitized_title": "summarizing_retrievals_using_answer_candidates_for_opendomain_qa_of_llms"
        },
        {
            "paper_title": "OpenAI: Gpt-4 technical report.",
            "rating": 2,
            "sanitized_title": "openai_gpt4_technical_report"
        },
        {
            "paper_title": "Gemma: Open models based on gemini research and technology.",
            "rating": 1,
            "sanitized_title": "gemma_open_models_based_on_gemini_research_and_technology"
        },
        {
            "paper_title": "Gemini: a family of highly capable multimodal models.",
            "rating": 1,
            "sanitized_title": "gemini_a_family_of_highly_capable_multimodal_models"
        }
    ],
    "cost": 0.00951075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Human-Level Understanding of Complex Process Engineering Schematics: A Pedagogical, Introspective Multi-Agent Framework for Open-Domain Question Answering</p>
<p>Srinivas Sagar 
Sakhinana sagar.sakhinana@tcs.com 
TCS Research
India</p>
<p>Geethan Sannidhi geethansannidhi20@cse.iiitp.ac.in 
IIIT Pune
India</p>
<p>Venkataramana Runkana venkat.runkana@tcs.com 
TCS Research
India</p>
<p>Towards Human-Level Understanding of Complex Process Engineering Schematics: A Pedagogical, Introspective Multi-Agent Framework for Open-Domain Question Answering
4B82BD0FA2591B41EBDB446266D49124Retrieval-Augmented Generation (RAG) • Process Diagrams
In the chemical and process industries, Process Flow Diagrams (PFDs) and Piping and Instrumentation Diagrams (P&amp;IDs) are critical for design, construction, and maintenance.Recent advancements in Generative AI, such as Large Multimodal Models (LMMs) like GPT-4 (Omni), have shown promise in understanding and interpreting process diagrams for Visual Question Answering (VQA).However, proprietary models pose data privacy risks, and their computational complexity prevents knowledge editing for domain-specific customization on consumer hardware.To overcome these challenges, we propose a secure, onpremises enterprise solution using a hierarchical, multi-agent Retrieval-Augmented Generation (RAG) framework for open-domain question answering (ODQA) tasks, offering enhanced data privacy, explainability, and cost-effectiveness.Our novel multi-agent framework employs introspective and specialized sub-agents using open-source, small-scale multimodal models with the ReAct (Reason+Act) prompting technique for PFD and P&amp;ID analysis, integrating multiple information sources to provide accurate and contextually relevant answers.Our approach, supported by iterative self-correction, aims to deliver superior performance in ODQA tasks.We conducted rigorous experimental studies, and the empirical results validated the proposed approach's effectiveness.</p>
<p>Introduction</p>
<p>PFDs and P&amp;IDs play crucial roles in the chemical and process industries, finding applications in various sectors such as oil and gas, pharmaceuticals, the semiconductor industry, and more.PFDs illustrate major equipment interconnections and material/energy flow in a chemical and process plant, while P&amp;IDs detail piping, instrumentation, and control systems.Both PFDs and P&amp;IDs are essential documents for the design, construction, operation, and maintenance of chemical and process plants.Recent advancements in Generative AI, such as Large Multimodal Models (LMMs) with advanced vision-language processing capabilities, including OpenAI GPT-4 (Omni) [4] and Google Gemini [5], have the ability to understand and interpret PFDs and P&amp;IDs for visual question-answering (VQA).However, using proprietary vision-language models raises data privacy concerns, as the risk of sharing intellectual property could compromise enterprise Accepted for publication at ML4CCE workshop at ECML PKDD 2024.arXiv:2409.00082v1[cs.CL] 24 Aug 2024 technological portfolios.Furthermore, the large size and complexity of closedsource LMMs limit their customizability for specialized tasks and knowledge editing.On the other hand, open small-scale multimodal models (SMMs), like Google PaliGemma [6] and Microsoft Phi-3, offer the benefits of domain-specific customization and interpretable PFD and P&amp;ID analysis, but they might fall short in terms of reasoning and generalization capabilities compared to proprietary large-scale models.Developing a secure, on-premises, customizable adaptation of SMMs for PFD and P&amp;ID analysis provides enterprises with advantages such as enhanced data privacy, explainability, and cost-effectiveness.However, this approach is not without its challenges.In recent times, Retrieval-Augmented Generation (RAG) has combined the strengths of pre-trained SMMs with information retrieval from external knowledge bases for open-ended VQA tasks.However, while RAG techniques allow SMMs to access external databases for VQA, they lack pre-trained knowledge on PFD and P&amp;ID analysis.Fine-tuning offers task-specific adaptation for PFD and P&amp;ID analysis but often ignores external databases during VQA, leading to less grounded and reliable answers.To overcome these challenges, we utilize both instruction-tuning and human preference alignment of open-source SMMs to optimally adapt SMMs to domain-specific RAG, addressing the limitations of limited pre-trained domain knowledge and the inability to utilize relevant external knowledge, leading to improved factual accuracy.Customizing SMMs for PFD and P&amp;ID analysis for VQA tasks, including image captioning and text recognition (OCR), faces two main challenges.First, high-quality human-annotated datasets specific to this domain are scarce.Second, manually annotating PFDs and P&amp;IDs to generate relevant question-answer (QA) pairs for customizing SMMs is a resource-intensive and time-consuming process that requires expert knowledge and specialized tools.To address the scarcity of human-annotated instruction-tuning datasets, we utilize teacher-student transfer learning (knowledge distillation), where a large model, such as GPT-4, serves as a robust 'teacher' to generate instruction-tuning data (image-question-answer pairs) and preference-tuning data (image-questionchosen-rejected pairs) for customizing a 'student' -a small-scale model such as PaliGemma -through parameter-efficient fine-tuning (PEFT) methods.This approach enhances grounded language generation and visual reasoning capabilities in task-specific applications, such as image captioning, VQA, and text detection for PFD and P&amp;ID analysis.In recent years, there has been a surge of interest in advanced RAG applications based on autonomous agents tackling complex goals.These agents utilize vision-language models to achieve several key functionalities: (a) interpreting end-user requests, reasoning about higher-level goals, and breaking down multi-step tasks into simpler, manageable subtasks through task planning; (b) selecting tools, which involves choosing pre-built or custom tools for each subtask and then calling those tools (e.g., external APIs or helper functions) to complete the subtasks; (c) generating responses by synthesizing the information obtained from the tools to create comprehensive and coherent answers; and (d) refining plans using a verify-then-correct approach to analyze and reason about their tool selection choices and usage.Iterative refinement al-lows for self-evaluation, incorporation of external feedback, and repeated cycles of improvement, leading to more accurate answers.This, coupled with memory augmentation, enables them to maintain context and leverage past conversations to provide coherent answers for multi-turn conversations.More recently, an advanced multi-agent RAG architecture based on autonomous agents has enabled the achievement of high-level goals through enhanced inter-agent communication and collaborative planning.Unlike previous single-agent architectures, this approach can tackle complex, multifaceted tasks with minimal human intervention.In this study, we present a novel hierarchical, multi-agent framework for open-domain question answering (ODQA) in the analysis of complex engineering PFD and P&amp;ID schematics.The framework consists of an introspective (or meta) agent composed of a main agent and a critique agent.The main agent orchestrates specialized sub-agents and utilizes language models like Google's Gemma with the ReACT technique [7] for reasoning and decision-making.It interprets complex user queries, delegates tasks to appropriate sub-agents, and forwards the sub-agents' responses to the critique agent.The critique agent evaluates the responses using a Gold LMM (e.g., GPT-4 Turbo) and provides feedback.The framework employs an iterative self-correction process through reflection, incorporating a verify-then-correct process, where the critique agent's feedback is used to iteratively refine the sub-agents' output for improved factual correctness and overall trustworthiness of the framework.Figure 1  In task planning, user intent is analyzed and queries are decomposed into sub-tasks.Tool selection involves SMMs selecting appropriate tools (APIs, databases, external knowledge repositories) to solve these sub-tasks.Tool calling involves SMMs extracting the required parameters from the user query and calling the selected tools to retrieve relevant information from document databases, memory databases, web searches, and Wikipedia articles.Memory consists of long-term memory, which stores reusable information for future queries, and short-term memory, which holds session-specific data for immediate processing.Finally, response generation integrates the outputs from these tools with the SMMs' internal knowledge to create comprehensive and coherent responses, and the critique agent iteratively refines the outputs using reflection and correction cycles.We fine-tune SMMs to select appropriate tools and use them accurately during task-specific adaptation to provide accurate and contextually relevant responses.</p>
<p>Our approach begins by extracting text and images from complex PDFs and segmenting the documents using a sliding window technique for granular information retrieval.The text segments are embedded into vector representations and indexed for efficient similarity search.We generate summary descriptions for images and index them.Each specialized sub-agent handles specific diagrams-PFDs and P&amp;IDs, respectively-and assists the main agent by providing expert analysis in their fields.The sub-agents utilize SMMs with the ReACT technique [7] to understand and interpret tasks.They employ a retrieveand-read paradigm, involving tool selection and tool calling to retrieve relevant information from parsed documents, memory databases (to retain and recall previous information), and external APIs for web search or Wikipedia articles.This is followed by reranking to select the most relevant passages.Sub-agents generate multiple answer candidates and corresponding supporting summaries using conditional summarization.The most plausible answer is selected based on the validity and informativeness of its supporting summary, utilizing a two-pronged evaluation process (instance-wise validity and pairwise ranking) to identify the best answer.We evaluate our approach on image captioning, VQA, and text detection (OCR) tasks for PFD and P&amp;ID analysis.Our findings demonstrate that the framework consistently performs on par with state-of-the-art methods, offering customizability, interpretability, data privacy, and cost-effectiveness.</p>
<p>Proposed Method</p>
<p>ODQA addresses a wider range of questions than traditional QA, often relying on vast, unstructured databases or large document collections (e.g., web search, Wikipedia).In this work, we utilize two distinct documents: (a) PFDs (D P F Ds ) and (b) P&amp;IDs (D P &amp;IDs ).We begin by performing document parsing to extract text and images embedded within complex, unstructured PDFs.Each document is then split into smaller segments using the sliding window chunking technique to preserve context and improve retrieval.A fixed-size window (in words) is moved by a predefined stride to create overlapping chunks.After chunking, each text segment is converted into a vector representation using embedding techniques.These vector representations are indexed in a vector database, allowing for efficient similarity search and retrieval of relevant text chunks based on semantic similarity to user queries.We use GPT-4(omni) to generate alternative text descriptions for the extracted images, which serve as metadata providing content, context, and details of the images.By indexing this metadata, we enable the retrieval of images based on their content, thereby enhancing the effectiveness of multi-modal search queries.Our approach employs a multi-agent framework where the introspective (meta) agent comprises a main agent (A M ) and a critique agent (A C ).The main agent orchestrates specialized sub-agents (A P F Ds , A P &amp;IDs ), each responsible for handling tasks related to PFDs and P&amp;IDs analysis.The introspective agent utilizes a reflective agentic pattern, consisting of the main agent generating the initial response to user queries and the critique agent performing reflection and correction to evaluate and improve the response.The main agent leverages language models (LMs) like Google Gemma to understand and interpret complex user queries, utilizing the ReACT technique [7] to enable reasoning and decision-making proficiency.This allows the main agent to strategically approach user queries by breaking them down into smaller, manageable sub-tasks for task delegation to specialized sub-agents and relaying the sub-agent responses to the critique agent.The critique agent utilizes a 'Gold LMM-as-a-judge' such as GPT-4 Turbo to evaluate the initial response and provide feedback based on factual correctness and question-answer relevance to improve the response.The main agent routes the sub-task to the specific domain expert sub-agent, where each sub-agent (A P F Ds , A P &amp;IDs ) utilizes SMMs, such as PaliGemma.These sub-agents employ the ReACT technique [7] to analyze and understand the sub-task-specific requirements and objectives through deliberate reasoning and a structured approach to problem-solving.This facilitates strategic tool selection and effective tool use for comprehensive information retrieval and synthesis, leading to more accurate and contextually appropriate outputs.Sub-agents dynamically select tools from the tool inventory based on the context and relevance to the user query, including (a) vector search on both (i) structured memory databases storing previous question-answer pairs and (ii) parsed document storage databases designed to store and retrieve indexed complex PDFs.Other external tools, such as (b) search engines and (c) Wikipedia, are used to retrieve relevant information from the internet or static knowledge bases.By extracting and integrating information from these various sources, the sub-agents provide comprehensive and accurate answers to complex, open-ended questions.To answer a question q, the main agent delegates the user query to the relevant sub-agent.The sub-agent uses a two-step approach known as the retrieve-and-read paradigm.The embedding-based retriever searches a vast collection of information sources, including memory databases, parsed document storage databases, web search results, and Wikipedia articles.The corresponding retrieved passages are denoted as
C = {c 1 , • • • , c M }.
The probability of a passage c ∈ C being relevant to a given question q is determined by a softmax function over the similarity scores, expressed as follows:
P (c | q) = exp(sim(e q , e c )) c ′ ∈C exp(sim(e q , e c ′ ))
where c represents a passage in the set of passages C. e q and e c are the embeddings of the question and the passage, respectively.sim(e q , e c ) is a similarity function (e.g., dot product) between the embeddings.We identify a relevant top-N subset C + = {c 1 , • • • , c N } from this larger corpus to maximize recall as follows:
C + = Retriever(q, C, N )
where N signifies the number of retrieved passages.After retrieval, the reranker evaluates the relevance of each passage in C + and selects the top-K passages that are most likely to contain the answer, as follows:
C ++ = Reranker(q, C + , K)
where C ++ is the refined subset of passages, and K ≤ N is the number of passages forwarded to the reader (sub-agent).The sub-agent 'reads' the retrieved information to find and synthesize the necessary details to generate a diverse set of potential answer candidates using a specially designed prompt consisting of the question q and the retrieved passages C ++ .It then performs conditional summarization to create high-level summaries of these passages, tailored to pro-vide supporting evidence for each specific answer candidate within the provided context.This approach enhances the sub-agent's ability to process the retrieved passages C ++ by emphasizing evidence and logical reasoning, thereby providing targeted summaries that support each specific answer candidate.Given a question q, the retrieved passages C ++ , and a smaller multimodal model (SMM) M, we generate K answer candidates a = [ a 1 , . . ., a K ] using a custom prompt p can , leveraging C ++ and q to guide M in generating the answer candidates.Mathematically, this procedure can be represented as:
a = M(p can (q, C ++ )).
We then perform conditional summarization on the retrieved passages C ++ by generating summaries s k that focus on integrating relevant supporting contexts to validate each potential answer a k ∈ a in relation to the question q as follows:
s k = M p sum q, C ++ , a k for k = 1, . . . , K
where s k represents the conditional summary for the k-th answer candidate.p sum is a prompt designed to facilitate the conditional summarization, extracting the relevant supporting evidence from C ++ for the answer candidate a k in relation to the question q.The probability of generating the conditional summary s k for the given question q, answer candidate a k , and the retrieved documents C ++ can be expressed as:
P (s k | q, a k , C ++ ) = |sk| i=1 P (s ki | s k&lt;i , q, a k , C ++ )
where s ki is the i-th token of the generated summary s k .s k&lt;i represents the tokens before i in the summary.|s k | denotes the length of the output summary sequence.The core assumption underpinning this approach suggests that focusing on generating strong summaries can lead to more accurate question answering.In simple terms, well-supported and logically valid summaries increase the likelihood of factually correct answers to the question.Subsequently, the multimodal model chooses the corresponding answer from the most plausible summary as the most likely answer to the question through proper evaluation [2].To achieve this evaluation, we utilize a two-pronged approach: (1) Instance-wise validity: This determines whether each generated summary (s k ) is well-formed and provides valid supporting evidence for its corresponding answer candidates ( a k ) to the question q. (2) Pair-wise ranking: This compares the generated summaries to determine their relative strength in supporting their respective answer candidates to the question q, identifying the summary that is most plausible and provides the strongest evidence for its corresponding answer candidate.The proposed evaluation approach assesses both the relevance and strength of summaries to select the most plausible answer to a question based on the most compelling and relevant supporting evidence.</p>
<p>Instance-wise/Pair-wise validity</p>
<p>To evaluate the validity and relevance of each summary s k in supporting its corresponding answer candidate a k , we utilize a two-step validation process.First, we check if s k is degenerate, i.e., if it fails to provide meaningful support for a k due to insufficient information from the retrieved passages.Second, for nondegenerate summaries, we evaluate how well s k specifically supports a k compared to other potential answers a i , where i ̸ = k.This ensures that the summary s k is focused and relevant to the specific answer a k it aims to validate.To quantify the validity of each summary, we define a validity score v k using a custom prompt p val , which guides the multimodal model in evaluating the summary's validity s k and alignment with the corresponding answer candidate a k as follows:
v(s k ) = 1, if M(p val (q, a k , s k )) = True 0, otherwise
The validity score v(s k ) is 1 if the multimodal model M, guided by p val , determines that s k is well-formed and supports a k in the context of q.Otherwise, v(s k ) is 0, indicating that s k is degenerate or fails to support a k .To further improve the evaluation of answer plausibility, we introduce a comparative analysis that measures the relative informativeness of each summary in the context of the given question.This evaluation involves comparing a specific summary s k to all other generated summaries {s i } K i=1,i̸ =k to determine its effectiveness in providing relevant and valuable information for answering the question q.We adopt a pairwise ranking approach to compute a ranking score r for each summary s k , quantifying its performance relative to other summaries.The ranking score is defined as:
r(s k , S K ) = K i̸ =k r pair (s k , s i )
where r pair (s k , s i ) is the pairwise ranking score between s k and s i , and S K = {s 1 , . . ., s K } is the set of all summaries.The term r(s k , S K ) represents the overall ranking score for a specific summary s k in providing relevant and valuable information for the question q compared to all other generated summaries.It quantifies the informativeness and relevance of s k relative to {s i } K i=1,i̸ =k , identifying the summary that best supports the most plausible answer to the question.The pairwise ranking score r pair (s k , s i ) is obtained by leveraging a custom prompt p rank that directs the multimodal model M to determine which of the two summaries, s k or s i , provides more relevant information for answering the question q.The pairwise ranking score is defined as:
r pair (s k , s i ) =      1, if M(p rank (q, s k , s i )) = s k 0, if M(p rank (q, s k , s i )) = s i 0.5, otherwise
When the model cannot make a clear determination, r pair (s k , s i ) = 0.5, indicating equal informativeness.The final answer prediction, a, is determined by selecting the answer candidate with the highest combined score, considering both the validity score v(s k ) and the ranking score r(s k , S K ) of its corresponding conditional summary s k :
a = a k * , k * = arg max k [v(s k ) + r(s k , S K )]
The chosen answer ( a) is supported by a relevant summary (s k * ) and is the most informative among the candidates.The prompts (p can , p sum , p val , and p rank ) are generalizable across datasets and multimodal models.The sub-agent relays the generated answer back to the main agent.However, multimodal models often generate inaccurate or non-truthful responses and struggle to verify and correct their outputs without external feedback.They lack the ability to critically eval-uate their responses.To address this, we utilize an iterative verify-then-correct process to refine responses without needing large-scale human annotations.</p>
<p>Iterative Self-Correction through Reflection</p>
<p>We now enable the main agent to self-correct the sub-agent's outputs based on feedback obtained during verification by a critique agent.The sub-agent's output undergoes progressive refinement through repeated cycles of verification and correction until a predetermined termination criterion is satisfied, such as attaining a target accuracy threshold or executing a fixed number of iterations.The iterative process involves two key steps: reflection and correction.The 'reflection' step refers to the verification process, where the main agent relays the generated answer to a critique agent.The critique agent utilizes a high-quality benchmark, like GPT-4 Turbo, to evaluate the generated answer's quality based on question-answer relevance, factual correctness, and comparison to the ground truth using NLP metrics like BLEU, ROUGE, and METEOR.The purpose is to evaluate and provide feedback on the sub-agent's output to determine whether it meets the desired standards of accuracy and truthfulness.The 'correction' step occurs after the reflection step.Based on the feedback generated during verification, the main agent delegates the task to the sub-agent to correct and improve its output.In summary, the reflection (verification of the sub-agent's output by the critique agent to generate feedback) and correction cycle (using the feedback from the verification step to revise and improve the previous output) can be repeated iteratively, enabling progressive self-improvement of the sub-agent's generated answer.This process allows the sub-agent to ground its reasoning in factual knowledge, reducing hallucination and leading to more accurate outputs.Additionally, the interleaved reasoning traces provide transparency, resulting in a more interpretable and trustworthy VQA framework.As mentioned earlier, the multi-agent framework consists of (a) an introspective agent, which includes (i) a main agent and (ii) a critique agent.The main agent plays a crucial role by delegating tasks to the relevant sub-agents and coordinating the iterative self-correction through reflection with the critique agent.Given the question (q) and the retrieved relevant passages (C ++ ) from parsed documents, memory databases, web articles, and Wikipedia, the sub-agent generates an initial output ( a i ) as follows:
a i = a k * = M(p can (q, C ++ )) k *(1)k * = arg max k <a href="2">v(M(p sum (q, C ++ , a k ))) + r(M(p sum (q, C ++ , a k )), S K )</a>
Where a k ∈ a = M(p can (q, C ++ )) are the generated answer candidates, v(•) represents the instance-wise validity score.r(•, S K ) is the ranking score of a summary s k , comparing its relevance and informativeness against other generated summaries in the set S K = {M(p sum (q, C ++ , a k ))} K k=1 .a k * denotes the specific answer candidate from a identified as the most plausible or optimal.The index k * is determined by the arg max operation, selecting the index k that maximizes the combined score of validity and ranking.Equations ( 1) and ( 2) detail the steps of generating answer candidates, performing conditional summarization, evaluating the validity of each summary, and ranking the summaries to select the most plausible answer candidate a i to the question q.Given the output a i , the main agent delegates it to the critique agent, denoted by M Ref , to generate feedback c i :
c i ∼ M Ref (q, C ++ , a i , T )
where c i provides feedback to the main agent.The mathematical function T evaluates the sub-agent's answer against GPT-4 Turbo generated gold standards (ground-truth) for relevance and factual accuracy.The sub-agent corrects the previous output a i using the question q, retrieved passages C ++ , and feedback c i to generate an improved output a i+1 as follows:
a i+1 = a k * = M(p ′ can (q, C ++ , a i , c i )) k *
where p ′ can is the updated candidate generation prompt.The updated prompt now includes additional context or feedback c i to refine and improve the previous output a i .In essence, this approach helps in creating a more accurate, improved, and relevant answer, a i+1 , by incorporating the critique agent's feedback.</p>
<p>Vision-Language Instruction/Preference Tuning</p>
<p>We bridge the gap between the general knowledge of pre-trained student models and new task requirements by updating the student model's domain knowledge through vision-language instruction tuning on a task-specific dataset (inputoutput pairs + instructions), all while mitigating the catastrophic forgetting of pre-trained knowledge.Instruction and preference tuning are necessary to adapt and align student models to particular domain-specific tasks and human preferences, respectively.This typically involves a multi-stage approach: (a) instruction-tuning using task-specific data to adapt the student model to the target task by minimizing the cross-entropy loss, followed by (b) direct preference optimization (DPO) for preference alignment using human preference data to increase the likelihood of generating preferred responses over rejected responses for the target task, thereby minimizing the binary cross-entropy loss.Traditionally, these methods require extensive and often expensive expert-annotated data.In this study, we use teacher-student transfer learning to avoid expensive manual data labeling.Teacher models, trained on vast labeled datasets, transfer their task-specific knowledge to student models through knowledge distillation.This enables student models to achieve high task-specific performance comparable to proprietary teacher models without relying on extensive human annotation.We use parameter-efficient fine-tuning (PEFT) with quantization to adapt pretrained student models to various tasks by updating a small subset of additional parameters.This reduces memory usage and computational overhead, enabling efficient training and scaling on consumer hardware.We utilize OpenAI GPT-4(Omni) and Google Gemini Pro as teacher models to generate a customized instruction-following dataset of image-question-answer (IQA) triplets and human preference data (image-question-chosen-rejected quadruples).The machinegenerated data is tailored to customize student models for image captioning and VQA tasks on PFD and P&amp;ID analysis.Furthermore, we leverage the Google Cloud Vision API for text detection and OCR tasks, generating IQA triplet data for bounding boxes and detected text recognition tasks.We employ PaliGemma-8K-instruct, a single-turn vision-language model, as the student model.The instruction-tuned student model excels in single-turn analysis but struggles with multi-turn conversations due to its inability to maintain context.We use external databases to store relevant QA pairs and their context.Sub-agents query these databases to retrieve and analyze relevant pairs, enabling them to generate contextually appropriate responses for new, related questions.</p>
<p>Experiments</p>
<p>We evaluated our multi-agent framework on open-domain and close-domain QA tasks for analyzing complex PFDs and P&amp;IDs through image captioning, VQA, and OCR tasks.We built a dataset from academic sources, industry examples, and public repositories, comprising 75 PFDs and 50 P&amp;IDs.We generated image captions and detailed text descriptions of the PFDs and P&amp;IDs using GPT-4(Omni).These were compiled into a PDF document, resulting in two distinct categories: D P F containing the PFDs with their captions and descriptions, and D P &amp;IDs containing the P&amp;IDs with their captions and descriptions.PFD and P&amp;ID documents were parsed using a sliding window technique to improve information retrieval.Text chunks were embedded and indexed, while images were processed using GPT-4 to generate text descriptions, which were then indexed for multi-modal search.OpenAI GPT-4(Omni) and Google Gemini Pro were utilized as teacher models to generate high-quality instruction-tuning and preference-tuning data, including image-question-answer triplets and imagequestion-chosen-rejected pairs, tailored for PFD and P&amp;ID analysis.We generated diverse QA pairs to address domain-specific challenges and ensure a highquality machine-generated dataset, including 625 image captioning QA triplets, 16,000 VQA pairs, and 10,500 text detection and OCR annotations (including image-augmented data).These datasets were split into 70% training, 15% validation, and 15% test sets.They are essential for building and evaluating a robust multi-agent framework capable of handling real-world PFD and P&amp;ID analysis tasks.We compared the proposed framework's performance against baseline models, including proprietary models, on image captioning, VQA, text detection, and OCR tasks.The baselines include GPT-4 Turbo-preview, Claude-3 Opus, and Google Gemini 1.0 Pro for a rigorous comparison with state-of-theart LMMs.For image captioning and VQA (including logical, common sense, and multi-step reasoning), we used BLEU, ROUGE, and METEOR to evaluate caption accuracy versus ground truth.For multiple-choice VQA tasks, we used precision, recall, F1, and exact match to measure answer correctness against ground truth.Evaluating text detection and OCR involved metrics for localization accuracy (Bounding Box Precision, Recall, F1-Score, Intersection over Union (IoU)) and recognition quality (Character Error Rate (CER), Word Error Rate (WER)).The main agent uses Google Gemma-7b-it.The sub-agents use PaliGemma-3b-mix, and the critique agent uses GPT-4 Turbo.We use the opensource BGE embedding method as a search engine to retrieve relevant passages from external sources for knowledge-augmented text generation.We fine-tune an adapter for task-specific customization to improve BGE embedding retrieval performance for PFD and P&amp;ID analysis.The adapter is trained to rank relevant documents higher than irrelevant passages for a given query by learning the semantic relationships between similar questions and their corresponding answers, enhancing retrieval quality.Similarly, we use the open-source BGE rerank model to prioritize the most relevant and reliable information from retrieved passages.We fine-tune the reranker to assign higher relevance scores to passages that are more relevant to the given queries, ensuring they are ranked higher in the results.We performed instruction-tuning of each SLM (PaliGemma) using the PEFT technique, such as QLoRA, on their specific PFD and P&amp;ID analysis tasks using corresponding datasets.We resized PFD and P&amp;ID images to 224×224 for image captioning and VQA tasks or 448×448 for OCR tasks, using bicubic resampling with a patch size of 14×14 pixels.We generated image tokens based on resolution: 256 tokens for 224×224 images and 1024 tokens for 448×448 images.These image tokens were combined with text inputs to PaliGemma for autoregressive text generation.Each SMM's instruction-tuning leveraged a comprehensive hyperparameter configuration: a batch size of 16, a learning rate of 1 × 10 −3 adjusted with a linear scheduler over 40 epochs, 100 warmup steps, a weight decay of 1 × 10 −4 , gradient accumulation of 5 steps, and the AdamW optimizer.To ensure efficient parameter updates, we utilized 4-bit QLoRA with a low-rank r of 12, α of 32, and a dropout of 0.05.We performed preference tuning on each SMM using the DPO technique along with QLoRA, minimizing the binary cross-entropy (BCE) loss with the following hyperparameters: a learning rate of 5.0×10 −4 with a cosine scheduler and gradient accumulation of 4 steps.β was set to 0.2 to align SLMs with the desired preferences.We conducted training for 20 epochs using the AdamW optimizer, with a batch size of 16.We utilized NVIDIA GPUs for faster training and, for robust evaluation, performed multiple independent runs and reported ensembled averages.</p>
<p>Experimental Results:</p>
<p>The experimental results demonstrated that our framework performed on par with or exceeded state-of-the-art methods in image captioning, VQA, and OCR tasks, while offering customizability, interpretability, data privacy, and cost-effectiveness.A qualitative analysis of the generated outputs highlighted the framework's ability to produce contextually relevant and factually accurate responses for complex PFDs and P&amp;IDs analysis tasks.Tables 1-2 show the experimental results for image captioning and VQA tasks.The evaluation metrics BLEU-2, BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, and METEOR range from 0 (no overlap) to 1 (perfect overlap), with higher scores indicating better performance.Table 3 shows the experimental results on multiple-choice VQA tasks in terms of precision, recall, F1-score, and exact match(classification task) from 0.0 to 1.0 (higher is better).Table 4 presents the experimental results on the text detection task using Bounding Box Precision, Recall, F1-Score, and IoU (0.0-1.0 scale, higher is better).Table 5 shows the experimental results on OCR to recognize and transcribe text in images compared to ground truth text.Lower CER and WER values closer to 0 indicate superior performance.Our framework surpasses or matches top baselines (Tables 1-5).To understand the contribution of individual components to overall framework performance, we conducted ablation studies.We disabled components like instruction-tuning (IT), preference-tuning (PT), iterative self-correction (SC), and conditional summarization (CS) individually to create ablated variants.We compared the performance of the ablated variants with the original(baseline) model.Tables 6 -8 show ablation study results for captioning and VQA, while Tables 9 -10 show results for text detection and OCR.Answer: The PFD diagram illustrates the key components and flow paths involved in the distillation process of crude oil.Crude oil is first pumped and heated through a series of exchangers and a desalter before entering the distillation tower.In the tower, the crude oil is separated into various fractions, such as naphtha, kerosene, light gas oil, heavy gas oil, and residue, based on their boiling points.The diagram also shows the use of steam in sidecut strippers and the management of reflux and sour water in the system.</p>
<p>How is the heavy gas oil fraction processed in the flow sheet?.</p>
<p>A) It is sent to the reflux drum B) It is collected at the bottom of the distillation tower C) It is stripped using steam D) It is removed as residue (fuel oil) Answer: C) It is stripped using steam.</p>
<p>Explain the role of the reflux drum in the crude oil distillation process?.</p>
<p>Answer: The reflux drum condenses the overhead vapor from the distillation tower.The liquid is partially returned as reflux to the tower, while the rest, along with non-condensable gases, is sent for further processing or storage.</p>
<p>Describe the flow path of crude oil from the initial entry to the desalter?.</p>
<p>Answer: Crude oil is pumped from storage and preheated through heat exchangers using hot products or pump-around streams.It then enters the desalter, where water washes out salts and impurities, resulting in desalted crude oil and brine.Table 12.Given a question and relevant passages, prompt a small-scale multimodal model(SMM) to generate multiple potential high-quality answers.</p>
<p>Prompt for conditional summarization(psum).</p>
<p>In this task, act as a process engineer.Craft a high-quality passage that strengthens the given prediction using only the provided supporting passages.Question: (Question) Choices( potential answer candidates): (a) Choice 1 (b) Choice 2 Prediction(Answer candidate that the summary is intended to support: (a) Choice 1 (or (b) Choice 2) Passage(Summary generated to validate and support this prediction as the correct answer): Table 13.The conditional prompt instructs the language model to generate summaries supporting each answer candidate based on the information from the retrieved passages.</p>
<p>We use the prompt p can with the question and N passages, generating K answer candidates â = â1 , ..., âK .For K = 2, see Table 12 for details.The prompt p sum generates conditional summaries s k that provide explicit rationales extracted from the relevant information in the retrieved passages to support each answer candidate.These summaries help assess if each answer a k is valid.See Table 13 for prompt details.The p val prompt (refer Table 14) assesses the validity of each generated summary s k .It determines if a summary lacks justification due to insufficient source passages (non-degenerate s k ) or if the generated summary s k strongly support a k over other potential answers like a j , where j ̸ = k.The p rank prompt (refer Table 15) compares summaries to determine which is more informative and relevant, helping select the most plausible answer.</p>
<p>Fig. 1 .
1
Fig.1.The figure shows the multi-agent framework for ODQA on complex documents for PFD and P&amp;ID analysis.It consists of an introspective agent, including a main agent and a critique agent.The main agent orchestrates specialized sub-agents, routing the end-user request to the relevant sub-agent.Each sub-agent utilizes SMMs with the ReACT technique in a four-stage workflow to utilize external tools, allowing dynamic access to specialized resources beyond its pre-trained knowledge: task planning, tool selection, tool calling, and response generation.In task planning, user intent is analyzed and queries are decomposed into sub-tasks.Tool selection involves SMMs selecting appropriate tools (APIs, databases, external knowledge repositories) to solve these sub-tasks.Tool calling involves SMMs extracting the required parameters from the user query and calling the selected tools to retrieve relevant information from document databases, memory databases, web searches, and Wikipedia articles.Memory consists of long-term memory, which stores reusable information for future queries, and short-term memory, which holds session-specific data for immediate processing.Finally, response generation integrates the outputs from these tools with the SMMs' internal knowledge to create comprehensive and coherent responses, and the critique agent iteratively refines the outputs using reflection and correction cycles.We fine-tune SMMs to select appropriate tools and use them accurately during task-specific adaptation to provide accurate and contextually relevant responses.</p>
<p>Fig. 2 .
2
Fig. 2. The figure shows the PFD of a crude oil distillation unit.Image Captioning -Multiple-Choice QA -Open/Close-Ended VQA.Write a high-level figure caption for the provided PFD?.</p>
<p>Fig. 3 .
3
Fig. 3.The figure shows the text detection and OCR results for the PFD.Prompt for potential answer candidates generation(pcan).You will be given several passages relevant to the question.Identify two likely correct answers.Format your answers concisely as follows: (a) Answer 1, (b) Answer 2. Keep responses clear and succinct.Question: (Question) Answer:</p>
<p>Table 1 .
1
The table compares the proposed method's image captioning performance.
MethodBLEU-2(↑)BLEU-4(↑)ROUGE-2(↑) ROUGE-L(↑) METEOR(↑)InstructBLIP[1]0.661 ± 0.054 0.620 ± 0.063 0.706 ± 0.073 0.761 ± 0.026 0.792 ± 0.065LLaVA[3]0.670 ± 0.062 0.623 ± 0.071 0.708 ± 0.082 0.761 ± 0.034 0.789 ± 0.055MiniGPT-4[8]0.729 ± 0.101 0.637 ± 0.121 0.741 ± 0.025 0.776 ± 0.086 0.808 ± 0.085(Ours) W/GPT-4 Turbo-P 0.923 ± 0.074 0.904 ± 0.083 0.932 ± 0.091 0.949 ± 0.038 0.946 ± 0.075(Ours) W/Claude-3 Opus 0.917 ± 0.081 0.896 ± 0.092 0.926 ± 0.103 0.937 ± 0.045 0.932 ± 0.084(Ours) W/Gemini 1.0 Pro 0.945 ± 0.105 0.928 ± 0.126 0.957 ± 0.029 0.962 ± 0.086 0.943 ± 0.089(Ours) W/PaliGemma 0.936 ± 0.123 0.921 ± 0.141 0.941 ± 0.105 0.951 ± 0.073 0.956 ± 0.106</p>
<p>Table 2 .
2
The table shows various methods' open/closed-ended VQA performance.
MethodBLEU-2 (↑) BLEU-4 (↑) ROUGE-2 (↑) ROUGE-L (↑) METEOR (↑)InstructBLIP[1]0.650±0.09 0.520±0.11 0.660±0.030.720±0.070.770±0.09LLaVA[3]0.658±0.10 0.530±0.12 0.665±0.040.720±0.070.770±0.09MiniGPT-4[8]0.680±0.11 0.545±0.13 0.675±0.050.735±0.080.800±0.10(Ours) W/GPT-4 Turbo-P 0.897±0.10 0.871±0.11 0.905±0.120.916±0.060.929±0.10(Ours) W/Claude-3 Opus 0.883±0.11 0.863±0.12 0.887±0.130.895±0.070.902±0.11(Ours) W/Gemini 1.0 Pro 0.915±0.13 0.890±0.14 0.924±0.040.930±0.100.928±0.11(Ours) W/PaliGemma 0.922±0.14 0.905±0.15 0.930±0.12 0.940±0.09 0.945±0.12</p>
<p>Table 3 .
3
The table shows the proposed method's multiple-choice VQA performance.
MethodPrecision (↑) Recall (↑)F1-Score (↑) Exact Match (↑)InstructBLIP[1]0.773±0.070 0.697±0.094 0.890±0.0350.798±0.012LLaVA[3]0.800±0.078 0.720±0.104 0.897±0.0350.802±0.012MiniGPT-4[8]0.822±0.084 0.735±0.111 0.907±0.0360.812±0.013(Ours) W/GPT-4 Turbo-P 0.926±0.095 0.906±0.129 0.942±0.0450.951±0.015(Ours) W/Claude-3 Opus 0.896±0.088 0.884±0.121 0.902±0.0410.905±0.014(Ours) W/Gemini 1.0 Pro 0.919±0.095 0.892±0.129 0.924±0.0430.937±0.015(Ours) W/PaliGemma0.934±0.097 0.903±0.119 0.933±0.041 0.928±0.015</p>
<p>Table 4 .
4
The table compares our method's text detection accuracy against baselines.
MethodPrecision (↑) Recall (↑)F1-Score (↑)IoU (↑)InstructBLIP[1]0.781±0.070 0.740±0.087 0.827±0.036 0.799±0.012LLaVA[3]0.790±0.078 0.755±0.097 0.839±0.036 0.803±0.012MiniGPT-4[8]0.818±0.086 0.796±0.101 0.825±0.037 0.823±0.014(Ours) W/PaliGemma 0.907±0.099 0.873±0.122 0.915±0.042 0.879±0.016</p>
<p>Table 5 .
5
The table compares our framework's OCR accuracy to existing methods.
MethodCER (↓)WER (↓)F1-Score (↑)InstructBLIP[1]0.095±0.070 0.130±0.087 0.744±0.036LLaVA[3]0.197±0.078 0.230±0.097 0.755±0.036MiniGPT-4[8]0.170±0.086 0.191±0.101 0.742±0.037(Ours) W/PaliGemma 0.093±0.099 0.127±0.122 0.824±0.042</p>
<p>Table 11 .
11
The table shows illustrative examples of responses for image captioning, multiple-choice QA, and open/close-ended VQA.</p>
<p>A user study evaluated the framework's usability and effectiveness for PFD and P&amp;ID analysis, gathering feedback on satisfaction and reliability.The findings highlight each component's importance and overall effectiveness.Prompt for instance-wise validation(p val ).Given the following question, answer candidate, and summary, determine if the summary supports the answer candidate based on the provided passages.Question: (Question) Answer Candidate: (Answer Candidate) Summary: (Generated Summary) Is the summary valid?(True/False) Table14.The instance-wise validation prompt checks if the summary accurately and logically supports the answer candidate in answering the question.Prompt for pair-wise ranking(p rank ).Given the following question and two summaries, determine which summary better supports the answer to the question based on the provided passages.Question: (Question) Summary 1: (Summary 1) ; Summary 2: (Summary 2) Which summary is better?(1/2/0) Table15.The pair-wise ranking prompt compares two summaries to determine which better supports the answer, responding with "1" for Summary 1, "2" for Summary 2, or "0" if equally informative.ConclusionThe proposed multi-agent framework significantly advances human-level understanding of complex engineering diagrams, ensuring enhanced data privacy, explainability, and cost-effectiveness while achieving superior performance in PFD and P&amp;ID analysis.Experimental results confirm the framework's effectiveness, highlighting its transformative potential in the chemical and process industries.
Instructblip: Towards general-purpose vision-language models with instruction tuning. W Dai, J Li, D Li, A Tiong, J Zhao, W Wang, B Li, P Fung, S Hoi, arXiv:2305.065002023arXiv preprint</p>
<p>Sure: Summarizing retrievals using answer candidates for open-domain qa of llms. J Kim, J Nam, S Mo, J Park, S W Lee, M Seo, J W Ha, J Shin, arXiv:2404.130812024arXiv preprint</p>
<p>H Liu, C Li, Q Wu, Y J Lee, arXiv:2304.08485Visual instruction tuning. 2023arXiv preprint</p>
<p>arXiv:2303.08774OpenAI: Gpt-4 technical report. 2023arXiv preprint</p>
<p>G Team, R Anil, S Borgeaud, Y Wu, J B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>G Team, T Mesnard, C Hardin, R Dadashi, S Bhupatiraju, S Pathak, L Sifre, M Rivière, M S Kale, J Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>D Zhu, J Chen, X Shen, X Li, M Elhoseiny, arXiv:2304.10592Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
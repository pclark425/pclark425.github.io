<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4986 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4986</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4986</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-267770010</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.13718v7.pdf" target="_blank">Exploring Self-supervised Logic-enhanced Training for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Traditional attempts to enhance the logical reasoning abilities of language models often rely on supervised fine-tuning, limiting their generalization to new tasks or domains. Large Language Models (LLMs), with their capacity to condense vast knowledge, can effectively tackle many tasks. Yet, our experiments reveal a gap in their performance on logical reasoning benchmarks when compared to state-of-the-art fine-tuning based models. To bridge this gap, we present LogicLLM, a first-of-its-kind, fully self-supervised framework for integrating logical reasoning capabilities into LLMs, and activating them via in-context learning. We apply this to two LLM series, FLAN-T5 and LLaMA, with parameter sizes from 3 billion to 33 billion. LogicLLM demonstrates its effectiveness through successful improvements on two logical reasoning benchmarks (ReClor and LogiQA-v2). Additionally, LogicLLM based on FLAN-T5-11B attains comparable results to ChatGPT, and evaluations with LLaMA-based models on three language understanding benchmarks (RACE, MMLU and Big-Bench-Hard) confirm that the improvements come without compromising the model’s general language understanding capabilities.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4986.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4986.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogicLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogicLLM: Self-supervised logic-enhanced meta-training for LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fully self-supervised meta-training framework that extracts logically consistent relation pairs from raw text (Wikipedia), augments them counterfactually, and trains LLMs with an autoregressive generation objective to improve in-context logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to FLAN-T5, LLaMA, Falcon</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework / training objective rather than a single architecture; applied as continual pre-training (auto-regressive LM loss + logic reconstruction loss) on existing auto-regressive and encoder-decoder LLMs. Uses counterfactual entity replacement and mixes a language-modeling loss to avoid forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B–40B (varied across experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor, LogiQA-v2 (primary); also evaluated on RACE, MMLU, BIG-Bench-Hard</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice reading-comprehension benchmarks requiring multi-step, relational, and logical reasoning in natural language (fuzzy logical consistency and relational composition).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Self-supervised construction of logically consistent direct/indirect relation pairs from Wikipedia paragraphs, counterfactual entity replacement augmentation, and autoregressive training objective: given one relation view generate the other (bi-directional reconstruction). Training mixes this logic loss with standard causal LM continuation loss to mitigate forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Provides consistent zero-shot improvements on ReClor and LogiQA-v2 across model families. Examples: FLAN-T5-11B w/ LogicLLM & FLAN: ReClor dev/test 61.2/61.1 (baseline 57.4/59.9), LogiQA-v2 dev/test 56.0/54.0 (baseline 55.3/53.1). LLaMA-13B baseline (ReClor dev/test) 30.4/33.5 -> w/ LogicLLM (ar) 37.4/36.3; LogiQA dev/test 33.0/32.1 -> 34.1/34.0. Falcon-40B average improvement ~+3.2 points. Improvements larger for bigger models (notable gains on 13B/33B).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Training-data assumption is fuzzy (direct vs indirect relations approximate logical consistency); some counterfactual samples judged inconsistent by LLM evaluators; evaluation variance across prompts/instructions; computational scaling limits (authors scaled up to 40B only with low-rank adaptation QLoRA); cannot guarantee strict symbolic logical proof—improves fuzzy relational reasoning, not formal theorem proving.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Autoregressive LogicLLM training outperforms MERIt-style contrastive pretraining for in-context LLMs. LogicLLM allows some models (FLAN-T5-11B) to reach or exceed ChatGPT (GPT-3.5-turbo) on some logic benchmark splits. LogicLLM improves robustness (reduces sensitivity to option order) relative to vanilla LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablations: (1) Autoregressive objective (ar) substantially outperforms contrastive (ctr) for LLaMA-13B (ar: ReClor dev 37.4 vs ctr: 33.4). (2) Counterfactual augmentation improves results (no aug. yields smaller gains; e.g., LLaMA-33B no-aug ReClor dev 49.4 vs with aug 50.2). (3) More counterfactual mixing (1:1 or higher) can further help. (4) Larger models show larger absolute gains; smaller models improve but less (emergent ability phenomenon). (5) Mixing a language modeling loss reduces catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4986.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4986.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA: Open and efficient foundation language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of open foundation Transformer LLMs (encoder-decoder? actually decoder-only) released in multiple parameter sizes; used here as base models and as targets for LogicLLM continual training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer family (used sizes: 7B, 13B, 33B). For models >13B the authors used QLoRA (low-rank adaptation on quantized weights) during LogicLLM training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 33B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor, LogiQA-v2 (primary); RACE, MMLU, BIG-Bench-Hard (additional evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice reading-comprehension and general reasoning benchmarks testing relational and multi-step logical inference in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated before/after LogicLLM continual self-supervised training. Also evaluated variants: autoregressive LogicLLM (ar) and contrastive (ctr). Some experiments used instruction-tuning data added (GPT4All) in multitask mix.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baselines (zero-shot): LLaMA-7B ReClor dev/test 30.2/30.3, LogiQA-v2 dev/test 27.4/28.1; LLaMA-13B 30.4/33.5 and 33.0/32.1; LLaMA-33B 45.2/50.3 and 41.2/41.6. After LogicLLM: LLaMA-7B -> ReClor 32.4/31.0 (+~1–2 pts), LogiQA 27.7/28.6 (+~0.3–0.5); LLaMA-13B -> ReClor 37.4/36.3 (+~3–7 pts), LogiQA 34.1/34.0 (+~1–2); LLaMA-33B -> ReClor 50.2/54.4 (+~5), LogiQA 45.9/42.6 (+~4–1). Also gains on RACE and some MMLU splits for smaller sizes (e.g., LLaMA-7B RACE +~4.2 pts).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Original LLaMA models show poor baseline logical reasoning vs instruction-tuned models (ChatGPT). LLaMA models exhibit position/option-order bias (high variance across option orders). CoT and few-shot prompting often do not reliably improve performance; exemplar selection and prompt variance strongly affect outputs. Larger LLaMA-33B showed marginal generalization gains on some tasks possibly due to low-rank adaptation constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared to ChatGPT (GPT-3.5), baseline LLaMA models are substantially weaker on ReClor/LogiQA. After LogicLLM, LLaMA-13B/33B narrow the gap but still often remain behind top instruction-tuned models; autoregressive LogicLLM outperforms contrastive MERIt-style pretraining when applied to LLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Key findings for LLaMA: (1) Autoregressive LogicLLM > contrastive (ctr) variant (e.g., LLaMA-13B ctr gave smaller or no consistent gains). (2) Counterfactual augmentation increases improvements (removing it reduces gains). (3) Larger model sizes obtain larger absolute improvements. (4) QLoRA used for larger sizes may constrain generalization in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4986.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4986.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5 (instruction-tuned T5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned Transformer encoder-decoder family (FLAN-T5) that is strong at instruction-following; used here both as baseline and target for LogicLLM meta-training combined with FLAN instruction data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The flan collection: Designing data and methods for effective instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder T5 variants instruction-tuned (authors evaluate 3B and 11B sizes). LogicLLM applied while retaining instruction-tuning (LogicLLM & FLAN).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B, 11B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor, LogiQA-v2; also evaluated on BIG-Bench-Hard</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reading-comprehension logical reasoning datasets requiring relational and multi-step inference; BBH contains diverse hard tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Continual LogicLLM meta-training mixed with instruction-tuning data (subset of FLAN collection). Counterfactual augmentation and autoregressive objective applied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FLAN-T5-3B baseline ReClor dev/test 54.6/52.5, LogiQA-v2 dev/test 48.7/48.7 -> w/ LogicLLM & FLAN: 55.8/54.1 and 50.8/50.1. FLAN-T5-11B baseline ReClor dev/test 57.4/59.9, LogiQA-v2 55.3/53.1 -> w/ LogicLLM & FLAN: ReClor 61.2/61.1 (exceeds ChatGPT on ReClor dev by +4.8), LogiQA-v2 56.0/54.0 (surpasses ChatGPT on dev/test by ~+1.3–1.5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Instruction-tuned FLAN models already strong; gains from LogicLLM smaller than for naive decoder LLMs but still meaningful. CoT learned from other domains (e.g., math) may not generalize to logic puzzles; some instability in CoT setting (FLAN-T5-3B had worse CoT results on BBH).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>FLAN-T5-11B with LogicLLM matches or exceeds ChatGPT (GPT-3.5-turbo) on some ReClor/LogiQA splits; LogicLLM can be integrated with instruction tuning and produce additive improvements versus instruction-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Mixing LogicLLM with FLAN instruction data improves over instruction tuning alone on most logic dataset splits; more counterfactual data mixing helps low-resource domains; CoT performance is dataset dependent and sometimes worse.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4986.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4986.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon-40B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-40B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 40B-parameter open LLM used as an additional architecture to test LogicLLM's effectiveness at larger scale (trained with QLoRA in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Falcon-40B</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Falcon-40B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large decoder-only Transformer (40B parameters); in this paper it was adapted via QLoRA for LogicLLM continual training due to hardware limits.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor, LogiQA-v2</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice logical reading comprehension requiring relational and multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Applied LogicLLM meta-training (autoregressive + counterfactual augmentation) using QLoRA low-rank adaptation for the large model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline Falcon-40B zero-shot: ReClor dev/test 38.4/37.1, LogiQA-v2 dev/test 35.9/36.1. After LogicLLM (via QLoRA): ReClor dev/test 41.4/43.0 (+~3.0), LogiQA-v2 dev/test 38.6/37.2 (+~2.7/+1.1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Large models were trained via QLoRA (low-rank adaptation) which may affect degree of generalization and limit exact comparability to full fine-tuning. Authors note scaling limited by resources.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>LogicLLM yields improvements comparable in magnitude to those seen with LLaMA family, indicating method generalizes to different LLM architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4986.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4986.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT / GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned conversational model (GPT-3.5-turbo) used in the paper as a competitive baseline for logical reasoning benchmarks and for chain-of-thought experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned OpenAI model (GPT-3.5 family); model size not disclosed by authors in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor, LogiQA-v2 (primary); used also for generating CoT rationales and data evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice reading comprehension logic tasks; used both as evaluation baseline and as a generator of reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot and Chain-of-Thought prompting (including 'Let's think step by step' variants) used to produce answers and rationales; also used as judge in data auto-verification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported zero-shot accuracies: ReClor dev/test 56.6/61.2, LogiQA-v2 dev/test 54.5/52.7. CoT variants sometimes did not improve and occasionally worsened performance on LogiQA-v2 (e.g., ChatGPT w/ CoT performed worse on some splits). Few-shot CoT gave limited gains depending on exemplar selection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Chain-of-Thought prompting does not reliably improve performance and can sometimes degrade it; exemplar selection for few-shot CoT yields limited and inconsistent gains; ChatGPT evaluations of data consistency show variance and sensitivity to anonymization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Serves as a strong baseline—FLAN-T5-11B with LogicLLM reaches or surpasses ChatGPT on some ReClor/LogiQA splits; many vanilla LLaMA models underperform compared to ChatGPT prior to LogicLLM training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4986.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4986.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art OpenAI instruction-following model used in this paper as an automatic annotator to evaluate the logical consistency of constructed data pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned OpenAI model (details per OpenAI technical report); used as an external evaluator to estimate the fraction of constructed examples that are logically consistent.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Data auto-verification for logic-consistency (not direct benchmark evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary judgments on whether direct and indirect relation sentences for an entity pair are logically consistent (Yes/No), including anonymized and counterfactual variants.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Prompted GPT-4 to judge logical consistency of sampled pretraining pairs (normal, counterfactual, anonymized).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-4 judged a high proportion of the 'normal' automatically extracted pairs as consistent and still judged >70% of counterfactual pairs consistent (according to authors' reported statistics), showing the data construction heuristic is largely reasonable per GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Automatic judgments vary with anonymization and counterfactual replacement; ChatGPT and GPT-4 disagree in some settings; automatic evaluation does not guarantee strict formal logical equivalence—only a heuristic consistency measure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Used to validate data construction; results indicate GPT-4 is more stable under anonymization than GPT-3.5 (ChatGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4986.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4986.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MERIt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MERIt: Meta-path guided contrastive learning for logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastive, meta-path based self-supervised method previously proposed to improve relational/logical reasoning using contrastive objectives and knowledge-graph guided negative sampling; used as a baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Merit: Meta-path guided contrastive learning for logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MERIt (applied to RoBERTa-L, DeBERTa-XXL in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive learning approach that constructs positive/negative relation pairs informed by meta-paths; demonstrated strong supervised/fine-tuning results in prior work (RoBERTa/DeBERTa backbones).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>ReClor, LogiQA-v2 (in cited prior evaluations); used as a comparative pretraining strategy in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks requiring logical and multi-hop relational reasoning in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Contrastive pretraining that discriminates logically consistent pairs from constructed negatives (MERIt uses negative candidate construction and contrastive loss). In this paper, authors compare MERIt-style contrastive training (ctr) vs their autoregressive reconstruction (ar) when applied to LLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Prior MERIt results (from cited table): MERIt (RoBERTa-L) reported ReClor dev/test ~69.4/61.6 and LogiQA-v2 dev/test ~62.6/59.3; MERIt (DeBERTa-XXL) reported up to ~80.6/78.1 on some splits. In this paper when applied to LLaMA (ctr) it produced only modest or inconsistent improvements compared to the autoregressive LogicLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Contrastive negative mining heuristics can introduce noisy negatives that are not truly contradictory, reducing effectiveness; objective mismatch (global pair discrimination) is less aligned with token-level autoregressive in-context learning and thus underperforms when applied to LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Autoregressive LogicLLM (ar) outperforms MERIt-style contrastive (ctr) when adapted for LLM in-context learning; MERIt remains strong in supervised fine-tuning contexts with discriminative models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Self-supervised Logic-enhanced Training for Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Merit: Meta-path guided contrastive learning for logical reasoning <em>(Rating: 2)</em></li>
                <li>Llama: Open and efficient foundation language models <em>(Rating: 2)</em></li>
                <li>The flan collection: Designing data and methods for effective instruction tuning. <em>(Rating: 2)</em></li>
                <li>ReClor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>LogiQA: A challenge dataset for machine reading comprehension with logical reasoning <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>QLoRA: Efficient finetuning of quantized LLMs <em>(Rating: 1)</em></li>
                <li>Falcon-40B <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4986",
    "paper_id": "paper-267770010",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "LogicLLM",
            "name_full": "LogicLLM: Self-supervised logic-enhanced meta-training for LLMs",
            "brief_description": "A fully self-supervised meta-training framework that extracts logically consistent relation pairs from raw text (Wikipedia), augments them counterfactually, and trains LLMs with an autoregressive generation objective to improve in-context logical reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applied to FLAN-T5, LLaMA, Falcon",
            "model_description": "Framework / training objective rather than a single architecture; applied as continual pre-training (auto-regressive LM loss + logic reconstruction loss) on existing auto-regressive and encoder-decoder LLMs. Uses counterfactual entity replacement and mixes a language-modeling loss to avoid forgetting.",
            "model_size": "3B–40B (varied across experiments)",
            "logical_reasoning_task": "ReClor, LogiQA-v2 (primary); also evaluated on RACE, MMLU, BIG-Bench-Hard",
            "task_description": "Multiple-choice reading-comprehension benchmarks requiring multi-step, relational, and logical reasoning in natural language (fuzzy logical consistency and relational composition).",
            "method_or_approach": "Self-supervised construction of logically consistent direct/indirect relation pairs from Wikipedia paragraphs, counterfactual entity replacement augmentation, and autoregressive training objective: given one relation view generate the other (bi-directional reconstruction). Training mixes this logic loss with standard causal LM continuation loss to mitigate forgetting.",
            "performance": "Provides consistent zero-shot improvements on ReClor and LogiQA-v2 across model families. Examples: FLAN-T5-11B w/ LogicLLM & FLAN: ReClor dev/test 61.2/61.1 (baseline 57.4/59.9), LogiQA-v2 dev/test 56.0/54.0 (baseline 55.3/53.1). LLaMA-13B baseline (ReClor dev/test) 30.4/33.5 -&gt; w/ LogicLLM (ar) 37.4/36.3; LogiQA dev/test 33.0/32.1 -&gt; 34.1/34.0. Falcon-40B average improvement ~+3.2 points. Improvements larger for bigger models (notable gains on 13B/33B).",
            "limitations_or_failure_cases": "Training-data assumption is fuzzy (direct vs indirect relations approximate logical consistency); some counterfactual samples judged inconsistent by LLM evaluators; evaluation variance across prompts/instructions; computational scaling limits (authors scaled up to 40B only with low-rank adaptation QLoRA); cannot guarantee strict symbolic logical proof—improves fuzzy relational reasoning, not formal theorem proving.",
            "comparison": "Autoregressive LogicLLM training outperforms MERIt-style contrastive pretraining for in-context LLMs. LogicLLM allows some models (FLAN-T5-11B) to reach or exceed ChatGPT (GPT-3.5-turbo) on some logic benchmark splits. LogicLLM improves robustness (reduces sensitivity to option order) relative to vanilla LLMs.",
            "ablation_or_analysis_results": "Ablations: (1) Autoregressive objective (ar) substantially outperforms contrastive (ctr) for LLaMA-13B (ar: ReClor dev 37.4 vs ctr: 33.4). (2) Counterfactual augmentation improves results (no aug. yields smaller gains; e.g., LLaMA-33B no-aug ReClor dev 49.4 vs with aug 50.2). (3) More counterfactual mixing (1:1 or higher) can further help. (4) Larger models show larger absolute gains; smaller models improve but less (emergent ability phenomenon). (5) Mixing a language modeling loss reduces catastrophic forgetting.",
            "uuid": "e4986.0",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LLaMA",
            "name_full": "LLaMA: Open and efficient foundation language models",
            "brief_description": "A family of open foundation Transformer LLMs (encoder-decoder? actually decoder-only) released in multiple parameter sizes; used here as base models and as targets for LogicLLM continual training.",
            "citation_title": "Llama: Open and efficient foundation language models",
            "mention_or_use": "use",
            "model_name": "LLaMA",
            "model_description": "Decoder-only Transformer family (used sizes: 7B, 13B, 33B). For models &gt;13B the authors used QLoRA (low-rank adaptation on quantized weights) during LogicLLM training.",
            "model_size": "7B, 13B, 33B",
            "logical_reasoning_task": "ReClor, LogiQA-v2 (primary); RACE, MMLU, BIG-Bench-Hard (additional evaluations)",
            "task_description": "Multiple-choice reading-comprehension and general reasoning benchmarks testing relational and multi-step logical inference in natural language.",
            "method_or_approach": "Evaluated before/after LogicLLM continual self-supervised training. Also evaluated variants: autoregressive LogicLLM (ar) and contrastive (ctr). Some experiments used instruction-tuning data added (GPT4All) in multitask mix.",
            "performance": "Baselines (zero-shot): LLaMA-7B ReClor dev/test 30.2/30.3, LogiQA-v2 dev/test 27.4/28.1; LLaMA-13B 30.4/33.5 and 33.0/32.1; LLaMA-33B 45.2/50.3 and 41.2/41.6. After LogicLLM: LLaMA-7B -&gt; ReClor 32.4/31.0 (+~1–2 pts), LogiQA 27.7/28.6 (+~0.3–0.5); LLaMA-13B -&gt; ReClor 37.4/36.3 (+~3–7 pts), LogiQA 34.1/34.0 (+~1–2); LLaMA-33B -&gt; ReClor 50.2/54.4 (+~5), LogiQA 45.9/42.6 (+~4–1). Also gains on RACE and some MMLU splits for smaller sizes (e.g., LLaMA-7B RACE +~4.2 pts).",
            "limitations_or_failure_cases": "Original LLaMA models show poor baseline logical reasoning vs instruction-tuned models (ChatGPT). LLaMA models exhibit position/option-order bias (high variance across option orders). CoT and few-shot prompting often do not reliably improve performance; exemplar selection and prompt variance strongly affect outputs. Larger LLaMA-33B showed marginal generalization gains on some tasks possibly due to low-rank adaptation constraints.",
            "comparison": "Compared to ChatGPT (GPT-3.5), baseline LLaMA models are substantially weaker on ReClor/LogiQA. After LogicLLM, LLaMA-13B/33B narrow the gap but still often remain behind top instruction-tuned models; autoregressive LogicLLM outperforms contrastive MERIt-style pretraining when applied to LLaMA.",
            "ablation_or_analysis_results": "Key findings for LLaMA: (1) Autoregressive LogicLLM &gt; contrastive (ctr) variant (e.g., LLaMA-13B ctr gave smaller or no consistent gains). (2) Counterfactual augmentation increases improvements (removing it reduces gains). (3) Larger model sizes obtain larger absolute improvements. (4) QLoRA used for larger sizes may constrain generalization in some cases.",
            "uuid": "e4986.1",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "FLAN-T5",
            "name_full": "FLAN-T5 (instruction-tuned T5 family)",
            "brief_description": "An instruction-tuned Transformer encoder-decoder family (FLAN-T5) that is strong at instruction-following; used here both as baseline and target for LogicLLM meta-training combined with FLAN instruction data.",
            "citation_title": "The flan collection: Designing data and methods for effective instruction tuning.",
            "mention_or_use": "use",
            "model_name": "FLAN-T5",
            "model_description": "Encoder-decoder T5 variants instruction-tuned (authors evaluate 3B and 11B sizes). LogicLLM applied while retaining instruction-tuning (LogicLLM & FLAN).",
            "model_size": "3B, 11B",
            "logical_reasoning_task": "ReClor, LogiQA-v2; also evaluated on BIG-Bench-Hard",
            "task_description": "Reading-comprehension logical reasoning datasets requiring relational and multi-step inference; BBH contains diverse hard tasks.",
            "method_or_approach": "Continual LogicLLM meta-training mixed with instruction-tuning data (subset of FLAN collection). Counterfactual augmentation and autoregressive objective applied.",
            "performance": "FLAN-T5-3B baseline ReClor dev/test 54.6/52.5, LogiQA-v2 dev/test 48.7/48.7 -&gt; w/ LogicLLM & FLAN: 55.8/54.1 and 50.8/50.1. FLAN-T5-11B baseline ReClor dev/test 57.4/59.9, LogiQA-v2 55.3/53.1 -&gt; w/ LogicLLM & FLAN: ReClor 61.2/61.1 (exceeds ChatGPT on ReClor dev by +4.8), LogiQA-v2 56.0/54.0 (surpasses ChatGPT on dev/test by ~+1.3–1.5).",
            "limitations_or_failure_cases": "Instruction-tuned FLAN models already strong; gains from LogicLLM smaller than for naive decoder LLMs but still meaningful. CoT learned from other domains (e.g., math) may not generalize to logic puzzles; some instability in CoT setting (FLAN-T5-3B had worse CoT results on BBH).",
            "comparison": "FLAN-T5-11B with LogicLLM matches or exceeds ChatGPT (GPT-3.5-turbo) on some ReClor/LogiQA splits; LogicLLM can be integrated with instruction tuning and produce additive improvements versus instruction-only baselines.",
            "ablation_or_analysis_results": "Mixing LogicLLM with FLAN instruction data improves over instruction tuning alone on most logic dataset splits; more counterfactual data mixing helps low-resource domains; CoT performance is dataset dependent and sometimes worse.",
            "uuid": "e4986.2",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Falcon-40B",
            "name_full": "Falcon-40B",
            "brief_description": "A 40B-parameter open LLM used as an additional architecture to test LogicLLM's effectiveness at larger scale (trained with QLoRA in this work).",
            "citation_title": "Falcon-40B",
            "mention_or_use": "use",
            "model_name": "Falcon-40B",
            "model_description": "Large decoder-only Transformer (40B parameters); in this paper it was adapted via QLoRA for LogicLLM continual training due to hardware limits.",
            "model_size": "40B",
            "logical_reasoning_task": "ReClor, LogiQA-v2",
            "task_description": "Multiple-choice logical reading comprehension requiring relational and multi-hop reasoning.",
            "method_or_approach": "Applied LogicLLM meta-training (autoregressive + counterfactual augmentation) using QLoRA low-rank adaptation for the large model.",
            "performance": "Baseline Falcon-40B zero-shot: ReClor dev/test 38.4/37.1, LogiQA-v2 dev/test 35.9/36.1. After LogicLLM (via QLoRA): ReClor dev/test 41.4/43.0 (+~3.0), LogiQA-v2 dev/test 38.6/37.2 (+~2.7/+1.1).",
            "limitations_or_failure_cases": "Large models were trained via QLoRA (low-rank adaptation) which may affect degree of generalization and limit exact comparability to full fine-tuning. Authors note scaling limited by resources.",
            "comparison": "LogicLLM yields improvements comparable in magnitude to those seen with LLaMA family, indicating method generalizes to different LLM architectures.",
            "uuid": "e4986.3",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT (GPT-3.5-turbo)",
            "name_full": "ChatGPT / GPT-3.5-turbo",
            "brief_description": "An instruction-tuned conversational model (GPT-3.5-turbo) used in the paper as a competitive baseline for logical reasoning benchmarks and for chain-of-thought experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5-turbo)",
            "model_description": "Instruction-tuned OpenAI model (GPT-3.5 family); model size not disclosed by authors in this paper.",
            "model_size": null,
            "logical_reasoning_task": "ReClor, LogiQA-v2 (primary); used also for generating CoT rationales and data evaluation",
            "task_description": "Multiple-choice reading comprehension logic tasks; used both as evaluation baseline and as a generator of reasoning chains.",
            "method_or_approach": "Zero-shot and Chain-of-Thought prompting (including 'Let's think step by step' variants) used to produce answers and rationales; also used as judge in data auto-verification.",
            "performance": "Reported zero-shot accuracies: ReClor dev/test 56.6/61.2, LogiQA-v2 dev/test 54.5/52.7. CoT variants sometimes did not improve and occasionally worsened performance on LogiQA-v2 (e.g., ChatGPT w/ CoT performed worse on some splits). Few-shot CoT gave limited gains depending on exemplar selection.",
            "limitations_or_failure_cases": "Chain-of-Thought prompting does not reliably improve performance and can sometimes degrade it; exemplar selection for few-shot CoT yields limited and inconsistent gains; ChatGPT evaluations of data consistency show variance and sensitivity to anonymization.",
            "comparison": "Serves as a strong baseline—FLAN-T5-11B with LogicLLM reaches or surpasses ChatGPT on some ReClor/LogiQA splits; many vanilla LLaMA models underperform compared to ChatGPT prior to LogicLLM training.",
            "uuid": "e4986.4",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "State-of-the-art OpenAI instruction-following model used in this paper as an automatic annotator to evaluate the logical consistency of constructed data pairs.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large instruction-tuned OpenAI model (details per OpenAI technical report); used as an external evaluator to estimate the fraction of constructed examples that are logically consistent.",
            "model_size": null,
            "logical_reasoning_task": "Data auto-verification for logic-consistency (not direct benchmark evaluation)",
            "task_description": "Binary judgments on whether direct and indirect relation sentences for an entity pair are logically consistent (Yes/No), including anonymized and counterfactual variants.",
            "method_or_approach": "Prompted GPT-4 to judge logical consistency of sampled pretraining pairs (normal, counterfactual, anonymized).",
            "performance": "GPT-4 judged a high proportion of the 'normal' automatically extracted pairs as consistent and still judged &gt;70% of counterfactual pairs consistent (according to authors' reported statistics), showing the data construction heuristic is largely reasonable per GPT-4.",
            "limitations_or_failure_cases": "Automatic judgments vary with anonymization and counterfactual replacement; ChatGPT and GPT-4 disagree in some settings; automatic evaluation does not guarantee strict formal logical equivalence—only a heuristic consistency measure.",
            "comparison": "Used to validate data construction; results indicate GPT-4 is more stable under anonymization than GPT-3.5 (ChatGPT).",
            "uuid": "e4986.5",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "MERIt",
            "name_full": "MERIt: Meta-path guided contrastive learning for logical reasoning",
            "brief_description": "A contrastive, meta-path based self-supervised method previously proposed to improve relational/logical reasoning using contrastive objectives and knowledge-graph guided negative sampling; used as a baseline comparison.",
            "citation_title": "Merit: Meta-path guided contrastive learning for logical reasoning",
            "mention_or_use": "mention",
            "model_name": "MERIt (applied to RoBERTa-L, DeBERTa-XXL in prior work)",
            "model_description": "Contrastive learning approach that constructs positive/negative relation pairs informed by meta-paths; demonstrated strong supervised/fine-tuning results in prior work (RoBERTa/DeBERTa backbones).",
            "model_size": null,
            "logical_reasoning_task": "ReClor, LogiQA-v2 (in cited prior evaluations); used as a comparative pretraining strategy in this paper",
            "task_description": "Benchmarks requiring logical and multi-hop relational reasoning in natural language.",
            "method_or_approach": "Contrastive pretraining that discriminates logically consistent pairs from constructed negatives (MERIt uses negative candidate construction and contrastive loss). In this paper, authors compare MERIt-style contrastive training (ctr) vs their autoregressive reconstruction (ar) when applied to LLaMA.",
            "performance": "Prior MERIt results (from cited table): MERIt (RoBERTa-L) reported ReClor dev/test ~69.4/61.6 and LogiQA-v2 dev/test ~62.6/59.3; MERIt (DeBERTa-XXL) reported up to ~80.6/78.1 on some splits. In this paper when applied to LLaMA (ctr) it produced only modest or inconsistent improvements compared to the autoregressive LogicLLM.",
            "limitations_or_failure_cases": "Contrastive negative mining heuristics can introduce noisy negatives that are not truly contradictory, reducing effectiveness; objective mismatch (global pair discrimination) is less aligned with token-level autoregressive in-context learning and thus underperforms when applied to LLMs.",
            "comparison": "Autoregressive LogicLLM (ar) outperforms MERIt-style contrastive (ctr) when adapted for LLM in-context learning; MERIt remains strong in supervised fine-tuning contexts with discriminative models.",
            "uuid": "e4986.6",
            "source_info": {
                "paper_title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Merit: Meta-path guided contrastive learning for logical reasoning",
            "rating": 2,
            "sanitized_title": "merit_metapath_guided_contrastive_learning_for_logical_reasoning"
        },
        {
            "paper_title": "Llama: Open and efficient foundation language models",
            "rating": 2,
            "sanitized_title": "llama_open_and_efficient_foundation_language_models"
        },
        {
            "paper_title": "The flan collection: Designing data and methods for effective instruction tuning.",
            "rating": 2,
            "sanitized_title": "the_flan_collection_designing_data_and_methods_for_effective_instruction_tuning"
        },
        {
            "paper_title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "LogiQA: A challenge dataset for machine reading comprehension with logical reasoning",
            "rating": 2,
            "sanitized_title": "logiqa_a_challenge_dataset_for_machine_reading_comprehension_with_logical_reasoning"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "QLoRA: Efficient finetuning of quantized LLMs",
            "rating": 1,
            "sanitized_title": "qlora_efficient_finetuning_of_quantized_llms"
        },
        {
            "paper_title": "Falcon-40B",
            "rating": 1,
            "sanitized_title": "falcon40b"
        }
    ],
    "cost": 0.01954225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring Self-supervised Logic-enhanced Training for Large Language Models
17 Jun 2024</p>
<p>Fangkai Jiao jiaofangkai@hotmail.com 
Nanyang Technological University
Singapore</p>
<p>Institute for Infocomm Research (I</p>
<p>Zhiyang Teng zhiyang.teng@ntu.edu.sg 
Nanyang Technological University
Singapore</p>
<p>Bosheng Ding bosheng001@e.ntu.edu.sg 
Nanyang Technological University
Singapore</p>
<p>Zhengyuan Liu liu_zhengyuan@i2r.a-star.edu.sg 
Institute for Infocomm Research (I</p>
<p>Nancy F Chen nfychen@i2r.a-star.edu.sg 
Nanyang Technological University
Singapore</p>
<p>Institute for Infocomm Research (I</p>
<p>Shafiq Joty sjoty@salesforce.com 
Nanyang Technological University
Singapore</p>
<p>Institute for Infocomm Research (I</p>
<p>A * Star 
Singapore 
Salesforce Research 
Exploring Self-supervised Logic-enhanced Training for Large Language Models
17 Jun 20247D83B3DA1754CBD33A7A9B2D40A54E77arXiv:2305.13718v7[cs.CL]
Traditional attempts to enhance the logical reasoning abilities of language models often rely on supervised fine-tuning, limiting their generalization to new tasks or domains.Large Language Models (LLMs), with their capacity to condense vast knowledge, can effectively tackle many tasks.Yet, our experiments reveal a gap in their performance on logical reasoning benchmarks when compared to state-of-theart fine-tuning based models.To bridge this gap, we present LogicLLM, a first-of-its-kind, fully self-supervised framework for integrating logical reasoning capabilities into LLMs, and activating them via in-context learning.We apply this to two LLM series, FLAN-T5 and LLaMA, with parameter sizes from 3 billion to 33 billion.LogicLLM demonstrates its effectiveness through successful improvements on two logical reasoning benchmarks (ReClor and LogiQA-v2).Additionally, Log-icLLM based on FLAN-T5-11B attains comparable results to ChatGPT, and evaluations with LLaMA-based models on three language understanding benchmarks (RACE, MMLU and Big-Bench-Hard) confirm that the improvements come without compromising the model's general language understanding capabilities. 1</p>
<p>Introduction</p>
<p>Logical reasoning serves as a bedrock for negotiation, debate and writing, underpinning our ability to engage with complex cognitive tasks (Yu et al., 2020).An example of logic reasoning in natural language is shown in Figure 1.As the complexity of relations and expressions presented in this task defy straightforward conversion into symbolic or formal languages, perfecting logical reasoning within language models has proven to be a significant challenge (Zhong et al., 2021).Figure 1: An example logical reasoning task from LogiQA-v2 dataset (Liu et al., 2020).The relations between different constituents, e.g., agriculture and development of Andean society, include various predicates, and it is hard to be converted into logical form through either first-order logic or formal language.</p>
<p>Past attempts to incorporate logical reasoning into language models primarily focused on integrating knowledge about logic.For instance, Huang et al. (2021) employed graph neural networks to capture relational semantics, while Wang et al. (2022) used data augmentation to implement first-order logic.These techniques, however, are constrained by their need for extensive annotated training data, which hinders the model's ability to generalize across different tasks due to disparities in data distribution and optimization objectives.</p>
<p>Conversely, recent breakthroughs in Large Language Models (LLMs) like PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), Chat-GPT 2 , GPT-4 (OpenAI, 2023), and Bard 3 offer a promising alternative.These LLMs effectively encapsulate a vast array of knowledge and tackle diverse tasks with minimal specialization, guided by human instruction.Despite their potential, our experiments on logical reasoning benchmarks revealed deficiencies in their logical reasoning capabilities as shown later in our experiments.</p>
<p>Contemporary efforts to fortify LLMs' specific capabilities fall broadly into two categories.The first employs external tools or APIs (Schick et al., 2023;Mialon et al., 2023;Cheng et al., 2022;Gao et al., 2022;Chen et al., 2022), aiding LLMs in argument parsing and semantic understanding.Yet, these tools' utility for logical reasoning remains limited due to the absence of a symbolic language for problem descriptions.The second category, instruction tuning, relies on data augmentation or enriched human feedback but struggles due to the scarcity of task-specific data and high annotation costs (Ouyang et al., 2022;Xu et al., 2023).In this work, we pivot away from these traditional methods and introduce LogicLLM, which performs self-supervised logic-enhanced meta-training for LLMs.It tackles two primary challenges: 1) synthesising logic-consistent data from raw texts ensuring fully self-supervised training, and 2) effectively incorporating logic prior into LLMs while preventing learning problems, such as memorization, forgetting and generalization.</p>
<p>To tackle the first challenge, LogicLLM emphasizes the necessity of understanding and exploiting fuzzy logical consistency.As mentioned previously, strict formal logic is often absent in natural language, we instead treat the relational consistency between different perspectives of relational expressions as an approximation to fuzzy logic consistency 4 .In fact, ensuring logical consistency in a discourse is a key requirement for text coherence and effective information conveyance (Jurafsky and Martin, 2009).We devise a method that inspects the implicit intra-sentence relation of entity pairs at the discourse level to extract logically consistent examples from Wikipedia articles (Figure 2).Specifically, we posit that direct and indirect relations of an anchor entity pair should be logically consistent, as they are derived from the "same" context.For the second challenge, LogicLLM adopts an auto-regressive objective optimizing on the logically consistent relation instances directly to make it seamlessly adapt to its pretraining objective.It tasks the model with gen-erating the alternative perspective (indirect or direct) given a direct or indirect description of the anchor entity pair.We further employ counterfactual data augmentation through entity replacement to enforce relation-centric reasoning, which not only avoids the model's tendency to merely recall results from memory but also ensures the preservation of the logic-enhanced aspect of the learning process.</p>
<p>LogicLLM is task-agnostic and does not require any annotations, making it adaptable to various logical reasoning tasks.We have conducted experiments across two distinct LLM series, FLAN-T5 (Longpre et al., 2023) and LLaMA (Touvron et al., 2023), encompassing a variety of parameter sizes.These experiments are designed to investigate two main questions: (1) Can the logical reasoning capabilities be exclusively improved through self-supervised meta-training for LLMs, thereby circumventing the need for task-specific supervised fine-tuning?(2) How does the logicenhanced meta training affect the LLM's language understanding capabilities, i.e., does it suffer from forgetting or generalization issues?</p>
<p>In response to the first question, our findings suggest that LLMs trained with the LogicLLM objective demonstrate superior performance on logical reasoning benchmarks, eliminating the need for further fine-tuning.Our LogicLLM based on FLAN-T5-11B attain comparable results to Chat-GPT on two logic reasoning benchmarks, Re-Clor (Yu et al., 2020) and LogiQA-v2 (Liu et al., 2022a), highlighting the feasibility of enhancing logical reasoning abilities through self-supervised training alone.</p>
<p>Regarding the second question, our evaluations with LLaMA-based models on three general language understanding benchmarks -RACE (Lai et al., 2017), MMLU (Hendrycks et al., 2021) and BIG-Bench-Hard (BBH) (Suzgun et al., 2022), confirm that the enhanced logical reasoning capabilities do not compromise the model's overall language understanding on MMLU and BBH.In fact, the learned logic ability appears to boost the model's performance in RACE.</p>
<p>Related Work</p>
<p>Large Language Models</p>
<p>In recent years, Large Language Models with incontext learning have emerged as a groundbreaking paradigm in the field of NLP.Unlike the traditional fine-tuning approach, in-context learning leverages natural language instructions or a small number of annotated examples as demonstrations to predict responses for new instances.This unique approach empowers LLMs to serve as a versatile tool for handling multiple tasks without requiring task-specific training.However, recent evaluations of LLMs (Qin et al., 2023;Bang et al., 2023;Jiao et al., 2023;Laskar et al., 2023;Wang et al., 2023a) have revealed a limitation in their ability to learn complex skills like logic and planning through language modeling alone.To address this, even the training of GPT-4 has incorporated labeled matching datasets to enhance its performance in solving math word problems (OpenAI, 2023).Nevertheless, due to the vast amount of data used in pre-training LLMs, annotated data for specific capabilities may be severely undersampled, and the cost of obtaining annotations should not be overlooked.Therefore, it remains crucial to develop various self-supervised or weaklysupervised training methods that do not rely on human annotation.These approaches are essential for constructing more robust and versatile LLMs that can perform a wider range of tasks with higher proficiency and lower resource.</p>
<p>Reasoning in Natural Language</p>
<p>Previous research aimed at natural language reasoning tasks can be broadly classified into three categories.The first category involves explicit prior knowledge, such as discourse structure or linguistic knowledge, to model implicit reasoning processes (Gao et al., 2020;Huang et al., 2021).The second category is neural-symbolic reasoning, where variables are first parsed, and then predefined programs are executed to obtain final results (Wang et al., 2022;Zhong et al., 2021).However, a significant challenge with these methods is the requirement of a robust semantic parser and a self-contained symbolic system for extracting variables or arguments, which is impractical for logic reasoning based on natural language.The third category encompasses methods that focus on general domain pre-training for reasoning via denoising auto-encoding (Jiao et al., 2021;Deng et al., 2021;Liu et al., 2022b).Nevertheless, restricted by the poor task generalization of discriminative models with few parameters, these methods are still in demand of task-specific fine-tuning to activate learned knowledge.</p>
<p>Our approach in this paper falls within the third category, which improves the efforts of MERIt (Jiao et al., 2022) by transforming it into auto-regressive framework to better align the nature of LLMs as generative model.We also drop the usage of knowledge graph enabling enhancing the logic of LLMs through purely self-supervised learning.</p>
<p>LogicLLM</p>
<p>Figure 2 shows the framework of LogicLLM.It involves three main steps: 1) Logic-consistent Data Construction (Section 3.1), which synthesises the logic-consistent data using relation discrimination between entity pairs; 2) Counterfactual Data Augmentation (Section 3.2), which augments the logic-consistent training data by entity sampling and replacement; 3) LLM Training (Section 3.3), which performs continual training of LLMs using the training data generated by the previous two steps.</p>
<p>Logically consistent Data Construction</p>
<p>Ensuring logical consistency in discourse and pragmatics is a fundamental prerequisite for natural language to effectively convey information and maintain coherence.Consequently, logically consistent data is prevalent in text documents and various techniques can be applied to extract them.In this study, we implement this by inspecting intrasentence relation of entity pairs at the discourse level to extract logically consistent examples from Wikipedia.</p>
<p>Direct relation Given an arbitrary paragraph and an anchor entity pair ⟨ e i , e j ⟩, we assume there exists an implicit relation s k between ⟨ e i , e j ⟩ if one sentence directly mentioning them can be found.This comes from the distant supervision (Mintz et al., 2009) and has been employed and extended in self-supervised training by previous work (Deng et al., 2021).For example, the instance ① in Figure 2 is a direct relation.To this end, we simply treat ⟨ e i , s k , e j ⟩ as the direct relation triplet for further data construction.</p>
<p>Indirect relation Entities e i and e j can be indirectly connected through multiple sentences within the input paragraph.</p>
<p>In such situations, we identify a chain of triplets, such as ⟨e i , s i+1 , e i+1 , • • • , s j , e j ⟩, which represents an indirect relation between the entity pair ⟨ e i , e j ⟩ through the relation composition of serial relation triplets ⟨ e i , s i+1 , e i+1 ⟩, ⟨ e i+1 , s i+2 , e i+2 ⟩, • • • , Logical consistency Intuitively, the direct and indirect relations between ⟨ e i , e j ⟩ should be logically consistent since they are derived from same context and describing the same entity pairs.Instances ① and ② in Figure 2 exemplify logically consistent relations.By establishing implicit connections between single-step and multihop reasoning, LLMs gain the ability to understand relation composition process between s k and ⟨s i+1 , s i+2 , • • • , s j−1 ⟩.This capability consequently enhances the LLMs' logical reasoning abilities.</p>
<p>To retrieve logically consistent relation pairs, we follow a two-step process.First, we recognize all entities within each paragraph via distant annotation from WikiData (Wang et al., 2021).And secondly, we enumerate every possible entity pair and search for a series of sentences and check if both direct and indirect relations can be extracted.</p>
<p>Counterfactual Data Augmentation</p>
<p>The work we have described in Section 3.1 produces logically consistent data that correlates entities and relations within reasoning paths.To enhance entity-irrelevant reasoning and ensure LLM focuses more on the process of relational composition rather than the entities themselves, we have additionally introduced counterfactual data augmentation.This approach, similar to the method suggested by Jiao et al. (2022), includes the random replacement of entities.</p>
<p>To create counterfactual examples of ⟨ e i , e j ⟩ within paragraph P , we initially select a random paragraph, denoted as Q, from a separate document.Subsequently, we sample a new set of entities, such as e a , e a+1 , • • • , e b from Q.The head and tail entities in the original relation instances of ⟨ e i , e j ⟩ are then substituted by these randomly sampled entities, maintaining the relationships unchanged.For instance, after substituting e i and e j with e a and e b , ③ and ④ become the counterfactual augmentations of ① and ②, respectively.In our research, we postulate that the logic-consistency between s k and s i+1 , e i+1 , s i+2 , • • • , s j−1 remains undisturbed in the counterfactual examples.This assertion is based on the idea that logical relationships within a paragraph's context are primarily driven by shared entities and their interconnections rather than the specific entities themselves.</p>
<p>Training Objective</p>
<p>During the training phase, we apply continual training to LLMs using logic-consistent data.Drawing inspiration from the success of in-context learning, we treat one relation from a logicconsistent relation pair as the in-context example and task the LLM with generating the other relation.As depicted in Figure 2, using the logicconsistent pair ⟨①, ②⟩ as an example, when ① is given as the conditional input, the LLM is expected to produce ② as the output, and vice versa.This process intuitively forces the LLM to reason the logic-consistent connections between the input and output relations since they are from the same context and the entity pairs of ① and ② are both e i and e j .</p>
<p>Formally, we denote the data extracted from Section 3.1 and Section 3.2 as
D = {⟨R 1 i , R 2 i ⟩} N i=1
, where N represents the number of training examples, and ⟨R 1 i , R 2 i ⟩ is the i-th logic-consistent record.Here, R 1 i refers to the direct relation-related instance, while R 2 i represents the instance with an indirect relation.The goal of LLM training is to minimize the negative log-likelihood function as follows:
L logic = − N i=1 [log P (R 1 i |R 2 i ) + log P (R 2 i |R 1 i )] = − N i=1 [ |R 1 i | j=1 log P (R 1 i,j |R 1 i,&lt;j , R 2 i ) + |R 2 i | j=1 log P (R 2 i,j |R 2 i,&lt;j , R 1 i )],(1)
where R 1 i,j , R 2 i,j denotes the j-th token of R 1 i and R 2 i , respectively.Furthermore, we incorporate the another causal language modeling loss L lm to mitigate the catastrophic forgetting problem.Both L lm and L logic are implemented as auto-regressive decoding.The only difference is that they sample from different data source.L lm continuously samples data from the subset of training corpus used during the laststage pre-training, i.e., Wikipedia paragraphs for LLaMA series models, and FLAN-collection-v2 for FLAN-T5 series models.Therefore, the over-all training objective is defined as:
L = L logic + L lm .
(2)</p>
<p>During training, for each forward-backward, we randomly sample two mini-batches with the same size from the datasets for logic-enhanced training and language modeling, respectively, and merge them into a single one.</p>
<p>Experiment</p>
<p>We integrate our pre-training approach into two prominent LLMs: LLaMA (Touvron et al., 2023) and FLAN-T5 (Wei et al., 2022a).These models boast parameter sizes ranging from 3 billion to 30 billion.To thoroughly evaluate the capability of LLMs from various angles, we have carefully selected five datasets representing three distinct categories.ReClor (Yu et al., 2020) and LogiQA-V2 (Liu et al., 2020) are two logical reasoning benchmarks sourced respectively from standardized graduate admission examinations and logical examination papers intended for reading comprehension.RACE (Lai et al., 2017) is a reading comprehension task that assesses general reasoning abilities.MMLU (Hendrycks et al., 2021) is used for measuring the learned knowledge and massive multitask language understanding, and BIG-Bench-Hard (BBH) (Suzgun et al., 2022) is a collection of multiple challenging tasks where LLMs fall behind human being.By employing MMLU and BBH, we aim to verify whether the logic-oriented meta-training negatively impacts the models' ability to generalize across a wide range of tasks.Due to space limitation, more implementation details can be found in Appendix A.</p>
<p>5 Results and Analysis</p>
<p>Logical Reasoning</p>
<p>Table 1 shows the results on ReClor and LogiQA-v2 under zero-shot setting.From the table we can find that the performance of LLaMA-based models is notably lower compared to ChatGPT.By training LLaMA models with LogicLLM, we observe significant enhancement in their zero-shot logical reasoning capabilities.For instance, on LLaMA-13B and LLaMA-33B, the average improvements across the four dataset splits are 3.2 and 3.7 points, respectively.The benefits are more substantial than those observed in the 7B models (0.9 points), which aligns with the findings  (Dettmers et al., 2023).on emergent abilities (Wei et al., 2022b).This could be attributed to the fact that larger models possess stronger generalization abilities and better apply their learned capabilities to different tasks.We also conducted experiments on Falcon-40B (Penedo et al., 2023), and found that LogicLLM brings an average improvement of 3.2 points.</p>
<p>Consistent with LLaMA-based models, we can draw similar conclusions for those based on FLAN-T5, where logic-oriented meta-training also yields improvements for both FLAN-T5-3B and FLAN-T5-11B.For FLAN-T5-11B, our model achieves accuracies of 61.2 and 61.1 on the development and test sets of ReClor, respectively.On the development and test sets of LogiQA-v2, our logic-oriented FLAN-T5-11B model achieves accuracies of 56.0 and 54.0, respectively.Notably, on the development set of ReClor, our logic-oriented FLAN-T5-11B model outperforms ChatGPT by a significant margin of 4.8 accuracy points.Similarly, on the development and test sets of LogiQA-v2, our logic-oriented FLAN-T5-11B model surpasses ChatGPT by 1.5 and 1.3 accuracy points, respectively.These overall results indicate that instruction tuning on multiple supervised datasets, such as the FLAN collection, can still be improved for learning logic.We hypothesize that this may be attributed to the sparsity of reasoningrelevant data in the entire collection and the conflicts between different tasks.</p>
<p>Hybrid Reasoning and Application</p>
<p>In addition to logical reasoning in text, we are also curious about whether logic-enhanced training contributes to general language understanding (RACE), and maintain the general capabilities on massive knowledge based tasks (MMLU).To investigate this, we evaluate the performance of the enhanced LLaMA models on these two datasets.</p>
<p>As shown in Table 2, from 7B to 33B, Logi-cLLM can consistently improve the performance on RACE, except the one of LLaMA-33B w/ Log-icLLMon the test set.Specifically, LLaMA-7B w/ LogicLLM obtain around 4.2 absolute improvements, and LLaMA-13B w/ LogicLLM achieves 1.5 improvements, which has verified that the logic-enhanced training is also beneficial to general reasoning and reading comprehension.Additionally, we find that LogicLLM can also benefits the massive multitask language understanding (MMLU) on LLaMA-7B and 13B.We find that the improvements of both RACE and MMLU on LLaMA-33B are marginal, probably because lowrank adaptation have restricted the generalization.</p>
<p>Pre-training Strategy</p>
<p>LogicLLM draws inspiration from the contrastive learning framework for logical reasoning, i.e., MERIt, which has demonstrated its efficacy in fine-tuning based approaches.As mentioned earlier, we hypothesize that contrastive learning may be inadequate for LLM with in-context learning.To validate this assumption, we examine the effects of contrastive learning (ctr) and auto-regressive generation (ar).In the case of contrastive learning, we adopt the methodology of MERIt to construct logically inconsistent instances and optimize the model by maximizing the distance between logically consistent instances and the inconsistent counterparts.Referring to the table, it can be observed that LogicLLM (ctr) fails to yield significant improvements compared to LLaMA-13B, except for the dev set of Re-Clor.Conversely, the auto-regressive models consistently outperform both the baseline models and the contrastive methods by considerable margins across all dataset splits.We propose two primary reasons to explain the superiority of autoregressive models over the contrastive approach.</p>
<p>First, the heuristic construction process for negative candidates used in contrastive learning fails to identify true contradictory relations, resulting in randomly chosen negative samples that lack logically opposite relationships with the positive instances.To this end, the contrastive learning process can degrade into a positive-only optimization process, which is similar to auto-regressive learning but receives less token-level supervision.</p>
<p>Second, the divergence between the training objectives of contrastive learning and auto-regressive generation undermines the model's ability to effectively do in-context reasoning.Contrastive learning primarily focuses on discriminating positive pairs from negative pairs based on a global semantic perspective.Auto-regressive models, on the other hand, accumulate their ability through local token prediction.During inference, LLMs are expected to understand instruction, and jointly consider the logical relations between different hypothesises within single input.By placing emphasis on fine-grained relations, the auto-regressive objective can better support in-context learning, enabling the model to grasp the nuanced connections and reasoning processes required for logical understanding.</p>
<p>Moreover, the auto-regressive objective signifi- Table 4: Ablation study to explore if LogicLLM can be combined with instruction tuning.For FLAN-T5 , we use the subset of FLAN collection.For LLaMA, we introduce GPT4All (Anand et al., 2023).cantly reduces computation costs during training by eliminating the need for negative candidates encoding.The streamlining of training process leads to more efficient and resource-friendly training without sacrificing performance.We also add another experiment by adjusting the ratio between counterfactual data and the normal ones as 1:1, and the comparison reveal that mixing more counterfactual data can also benefit the performance, which could be especially useful for low-resource domain, like finance and multi-lingual LLMs.</p>
<p>In summary, considering the advantages in both performance and training cost, the auto-regressive variant proves to be a superior choice for incorporating logic reasoning into LLMs.</p>
<p>Factors Relevant to Logic Prior</p>
<p>In Table 3, we also present the ablation results on LLaMA-33B when the counterfactual data augmentation strategy is omitted.Without the inclusion of counterfactual data, LogicLLM degrades into a conditional generative task that can be solved through memorization, as each sample has its own prototypes within Wikipedia.</p>
<p>As indicated in the table, even without the augmentation (no aug.), LogicLLM still contributes to the enhancement of logical reasoning abilities, albeit with more limited improvements.However, the introduction of counterfactual data augmentation to eliminate memorization effects can further amplify the benefits.The overall experimental results point out that relation construction serves as effective supervision signal for introducing logic prior.We leave the work about developing novel techniques to prevent memorization but less involve factual noise as future work.</p>
<p>Compatibility with Instruction Tuning</p>
<p>Instruction tuning has served as a critical step to make LLMs better in following human instruction, and/or generating with less toxic.In this section, we hope to study if LogicLLM can be well integrated with supervised instruction tuning so that LogicLLM has the potential to serve as a basic approach to train logic-enhanced foundation model before building applications.For FLAN-T5, we directly use the same subset of FLAN collection with our approach as the instruction tuning data.For LLaMA models, we introduce GPT4All (Anand et al., 2023) data for extra supervision.During training, we simply sum the loss of instruction tuning and LogicLLM in multitask training manner to keep the same data ratio.</p>
<p>As shown in Table 4, on most dataset splits, LogicLLM can achieve additional improvements compared with the instruction tuning-only baselines.Specifically, we find that the improvements are more significant on ReClor that those on LogiQA-v2.One possible reason is that the language style in LogiQA-v2 is more close to formal language, leaving a gap with the natural user questions.</p>
<p>Data Assumption Auto-Verification</p>
<p>In order to verify the rationality of our assumption that the direct and indirect relations are logically consistent, we employ ChatGPT and GPT-4 for automatic evaluations.Specifically, we randomly sample 1,000 examples from the development set for our pre-training with the ratio of normal data and counterfactual ones as 1:1.For each data pair, we ask ChatGPT/GPT-4 to determine if the relation between the target entities are logically consistent.The prompt we used is shown in Appendix E. We have involved four different settings.Beside the normal data and the counterfactual ones, we have also applied anonymization (Qiu et al., 2020) to them to decouple the background knowledge from entity.Specifically, the target entities are replaced with [X] and [Y], and for counterfactual data, the other replaced entities during data augmentation are not further anonymized.Some cases can also be found in Appendix E for clearer understanding.</p>
<p>Our results are shown in Tabel 5, from which we can observe that: (1) for normal data, Chat-GPT and GPT-4 deem that the logically consistent data occupie high ratios, which has initially verified the rationality of our data construction assumption.</p>
<p>(2) For counterfactual data, the ratios significantly decrease.Yet, in the view of GPT-4, there is still more than 70% of logically consistent data in the whole corpus.(3) When combined with entity anonymization, the ratios become much higher for counterfactual data, i.e., nearly 15% absolute improvements for ChatGPT and 23% for GPT-4.Besides, the ratio of normal data decreases significantly for ChatGPT, but is less perturbed for GPT-4.The observation further demonstrates that most counterfactual data should also hold the assumption since the anonymization only remove the backgrounds of entities, yet leaving the context as original.And the great variation brought by counterfactual data augmentation also reveals the potential weakness of current LLMs on identifying the true causal relations.</p>
<p>Robustness</p>
<p>By training LLMs on logic-consistent data and counterfactual augmentations, they are exposed to a wide range of input variations.This exposure helps them become less sensitive to minor perturbations such as shuffling of input options.To determine the robustness of LogicLLM , we conducted experiments on LogiQA-v2 using models of varying sizes.We shuffled the input order of different options and reperformed the inference process.</p>
<p>Figure 3 illustrates the findings of our experiments.We observed that LLaMA exhibited higher variance across different input option orders, as indicated by the greater spread in results.The circular outlier values that indicate specific input orders causing significant variations, leading to substantially higher or lower performance results.Our observation is consistent with the recent findings of Wang et al. (2023b), suggesting that the normal LLMs heavily suffer from position bias.In contrast, when LLaMA is enhanced with Logi-cLLM, it achieves more stable performance across different parameter sizes.Moreover, the averaged performance of LLaMA w/ LogicLLM is significantly superior to that of LLaMA alone.These results show that LogicLLM produces consistent and improved results compared to traditional LLMs, demonstrating the value of incorporating logic-enhanced training techniques into LLMs.</p>
<p>Training Quality Analysis</p>
<p>In order to analyze the quality of our metatraining, we have constructed a test set using the framework of MERIt (Jiao et al., 2022), which contains both logically consistent and inconsistent data.We have measured the log-likelihood on each sample as illustrated by Equation 1, and report the averaged results in Figure 4.</p>
<p>As shown in the figure, for logically consistent data, LogicLLM significantly reduced the negative log-likelihood.Moreover, the 7B-based model with LogicLLM surpasses the performance of LLaMA-13B.Notably, the disparity between the negative log-likelihood of logically consistent and inconsistent instances is further amplified, highlighting the effectiveness of LogicLLM in logical relation reconstruction.Furthermore, our experiments suggest a decrease in the negative log-likelihood for logically inconsistent data.This observation exposes a weakness in the contrastive learning-based method, i.e., MERIt, wherein the heuristic process for generating negative candidates introduces considerable noise.Consequently, some negative instances may not genuinely present contradictory logical relations.</p>
<p>Conclusion</p>
<p>In this paper, we have explored the feasibility and effectiveness of enhancing logical reasoning of LLMs via purely self-supervised training.We evaluate the performance based on two LLM series, i.e., FLAN-T5 and LLaMA.The experimental results on two logical reasoning benchmarks, LogiQA-v2 and ReClor, demonstrate the effectiveness of our method.And the performance on RACE, MMLU and Big-Bench-Hard have also verified that the framework do not hurt the generalization of LLMs.Finally, we have analyzed the factors relevant to logic during training, and the compability with supervised instruction tuning.We hope the analysis could bring new insights to future research.</p>
<p>A Implementation Details</p>
<p>A.1 LLM Prompting</p>
<p>In order to evaluate the generalization capabilities of LLMs across different tasks after post-training, we adopt a prompting-based approach.Here, the input to the LLMs is structured as Instruction [Exemplars] Task input.The instruction is tailored to the specific task at hand, while exemplars are utilized only in a few-shot setting.Each exemplar comprises both the task input and its corresponding output.For tasks such as multiplechoice question answering, the task input is a concatenation of the context, the question, and all potential options.The correct option index is used as the output.Besides, in a Chain-of-Thought (CoT) setting, we include a reasoning process formulated in natural language between the task input and output.</p>
<p>A.2 Data</p>
<p>We have constructed our self-supervised logicenhanced training data from Wikipedia, where we directly used the paragraph corpus pre-processed by Qin et al. (2021).We have constructed around 200K logically consistent sample pairs.After that, we further performed counterfactual data augmentation with the ratio of 1:3, and finally induced 800K training sample pairs in total.The data construction process mainly follows the original setting of Jiao et al. (2022)  we have dropped the negative candidates since we employed auto-regressive training.</p>
<p>For language modeling, we employed different dataset with respect to the data used in their last stage training.For FLAN-T5 series models, we used the subset of FLAN-collection-v2 (Longpre et al., 2023); while for LLaMA series models, we used the same Wikipedia paragraphs from the corpus of Qin et al. (2021).</p>
<p>A.3 Hyper-parameters of Training</p>
<p>During the pre-training process, we set the batch size to 4,096, which is implemented using gradient accumulation.The maximum sequence length is truncated at 1,024 for the FLAN collection and 512 for the MERIt corpus.For the FLAN-T5 series models, we conduct training steps for 200 iterations, while for the LLaMA series models, we perform training steps for 500 iterations.The learning rates are set as follows: 1e-4 for FLAN-T5-3B, 5e-5 for FLAN-T5-11B, 1e-5 for LLaMA-7B, and 5e-6 for LLaMA-13B.To carry out the training process, we utilize 8 NVIDIA A100 80G GPUs.However, due to hardware limitations, models larger than 13B are trained using QLoRA (Dettmers et al., 2023), a low-rank adaptation approach specifically designed for quantized LLMs.We follow the setting used in QLoRA with α as 16 and r as 64.All linear layers are used for adaptation and the LoRA dropout is 0.05.The learning rate for LLaMA-33B and Falcon-40B is set as 5e-4.</p>
<p>A.4 Evaluation</p>
<p>To ensure a fair comparison, we maintain consistency across different models for each dataset.This involves using identical instructions and fewshot samples.We use accuracy as the evaluation metric across all experiments.The prompts for different dataset can be found in Appendix D.</p>
<p>B Interpretation for Different Results on RACE</p>
<p>In this section, we will discuss the different results on RACE between ours and those reported by the original paper of LLaMA.Specifically, Touvron et al. (2023) do not report the weighted results, so we convert them by ourselves.The results are shown in Table 7. From the table we can find that only LLaMA-7B cannot match the performance reported by the authors.On LLaMA-13B and LLaMA-33B, our reproduced accuracies are much higher than the reported ones, which can help address the concern of unfair comparison, and demonstrate the effectiveness of our proposed LogicLLM.</p>
<p>C Logic-enhanced Meta-training for Complex Task Understanding</p>
<p>We evaluated the performance of logic-enhanced pre-trained models on BIG-Bench-Hard, a benchmark comprising challenging tasks where human performance surpasses that of LLMs.Table 8 presents the results achieved by the LLaMA and FLAN-T5 models under three evaluation settings: zero-shot, direct few-shot, and CoT.</p>
<p>In the zero-shot setting, our logic-enhanced meta-training significantly improves all four investigated models.For instance, the zero-shot accuracies of LLaMA-13B and FLAN-T5-T5-11B are 25.0% and 38.0%, respectively.When combined with the LogicLLM model, the accuracy scores of LLaMA-13B and FLAN-T5-11B improve to 26.3% and 44.1%, respectively.Some tasks included in BBH require free-form answers thus we cannot evaluate the models by selecting the candidate with lowest perplexity or log likelihood.Instead, we need to follow the evaluation of API-based models, which employs regularization expression to capture the answer from the response.However, smaller language models, especially those without being instruction tuned, fail to accept diverse instruction, and generate structured response.As a result, the absolute performance under zero-setting setting of LLaMA-based models are relatively limited.</p>
<p>On the other hand, the direct few-shot results outperform the zero-shot results in three out of four models, with the exception of FLAN-T5-11B.Similarly, logic-enhanced meta-training boosts the performance of models, except for FLAN-T5-11B.In the CoT setting, our method further enhances the performances of LLaMA-13B and FLAN-T5-3B.However, the best direct few-shot and CoT results (42.6% and 40.9%, respectively) are both inferior to the best zeroshot result (44.1%).Notably, the CoT results on FLAN-T5-3B are significantly worse than the zero-shot and direct few-shot results.These observations suggest the potential drawback that learning CoT from annotated training data, i.e., FLAN collection, has difficulty in generalizing to different task categories, for example, learning CoT from math word problem solving and solving logical puzzles.We provide further discussion on these findings in Appendix G.</p>
<p>D Prompt Template</p>
<p>E.2 Normal Version</p>
<p>[User]:</p>
<p>Determine whether the relation between "Everdingen" and "Sweden" in the given two sentences are logically consistent.The output should either be Yes or No.</p>
<p>[ChatGPT]:</p>
<p>Yes.The output should either be Yes or No.</p>
<p>E.3 Counterfactual Version</p>
<p>[ChatGPT]:</p>
<p>No.</p>
<p>Entity replacement:</p>
<p>• Everdingen → Nicholas Roerich;</p>
<p>• Sweden → Master;</p>
<p>• Norwegian (connecting entity) → Canal del Dique;   The output should either be Yes or No.</p>
<p>E.4 Anonymized Version</p>
<p>[ChatGPT]:</p>
<p>Yes.</p>
<p>F Discussion about Different Perspectives of Logical Reasoning</p>
<p>In our opinion, logic can be reflected through multiple aspects.Here, we use a simple logic rule to discuss the different perspectives:
(α → β) ∧ (β → γ) ↔ α → γ.(3)
The above equation shows the simplest case of first-order logic reasoning, where α, β and γ are different variables, and ∧ is logical and.We can also introduce the necessary logical connectives in natural language to make it easier for understanding:
IF α → β AND β → γ, THEN α → γ. (4)
It should be noted that, in symbolic logic, we often ignore the actual meaning of relations.However, we can always find a path, i.e., a series of relation triplets from knowledge graph to transform the above symbolic form into natural language based logical reasoning process:
IF α r 1 −→ β AND β r 2 −→ γ, THEN α r 3 −→ γ.
(5) One example here can be: r 1 refers to is the father of, r 2 refers to is the mother of, and r 3 refers to is the grandpa of.</p>
<p>From the above discussion, we can conclude that (1) logical connectives focus on discourselevel connections, (2) symbolic logic can be viewed as the simplified version of logical reasoning in natural language, where we focus more on the formal rules of atomic logic operations, and (3) relational reasoning concentrates on the actual logic operations built on world knowledge.Both of what we have discussed in the paper and the reviewers have mentioned in comments, i.e., logical connectives, are indeed different perspectives of logical reasoning.They do not contradict to each other, and discussing them separately is beneficial to make the problem easier.Besides, there are also several studies also discuss logical reasoning from the relational reasoning perspective (Wong et al., 2023;Xu et al., 2021;Zeng et al., 2021;Wang et al., 2022).And Figure 1 also shows the case emphasizing relational reasoning.</p>
<p>G Weakness of LLMs on Logical Reasoning</p>
<p>Table 9 showcases the evaluation results of LLMs' performance in both few-shot and CoT settings.</p>
<p>The intermediate reasoning process is automatically generated by ChatGPT using the prompt "Let's think step by step."In the case of zeroshot CoT, we include the suffix prompt "So the answer is" to guide the models in summarizing and concluding the answer.For few-shot CoT, the reasoning process is initially generated for each sample in the training set.Subsequently, we retain the samples where the final prediction is correct, following the steps outlined in zero-shot CoT.During testing, we randomly select samples from the retained candidates, as well as the automatically generated CoT, to serve as exemplars.However, our observations indicate that both few-shot learning and the use of CoT do not significantly improve the models' performance.For example, ChatGPT w/ CoT performs much worse than that without CoT on the development set of LogiQA-v2.One potential reason for this is that the selected samples differ substantially from the target example.To investigate further, we incorporate reasoning category information during exemplar selection.In LogiQA-V2, each question is annotated with a reasoning category, such as categorical reasoning, sufficient conditional reasoning, or necessary conditional reasoning.For few-shot CoT prompting, we only consider candidates that share at least two common reasoning categories.This particular variant is denoted as "ChatGPT w/ CoT + Cate." in the table.</p>
<p>Despite these efforts, we find that carefully selecting prompting exemplars only provides limited improvement.The results indicate that LLMs struggle to comprehend the reasoning structure from a limited number of observed examples.Consequently, they face challenges in effectively learning the mapping between input-label and input-rationale-label.Additionally, as shown in Table 1, we observe that LogicLLM also contributes minimally to addressing this issue.We recognize the need for further investigation in this area and leave it as a potential avenue for future research.</p>
<p>In 1644 ,Figure 2 :
16442
Figure 2: The LogicLLM framework.P and Q are two arbitrary paragraphs from Wikipedia.In Step 1, we extract intra-sentence relations ①: ⟨ e i , s k , e j ⟩, and the compositions of them ②: ⟨e i , s i+1 , e i+1 , • • • , s j , e j ⟩ from P for an entity pair ⟨ e i , e j ⟩; ① and ② are direct and indirection relations, respectively.Here s k is a relation, represented by the sentence that mentions ⟨ e i , e j ⟩. ① and ② are viewed as logically consistent since both of them describe the "same" relation between ⟨ e i , e j ⟩ from different view.In Part I of the figure, e i refers to Everdigen and e j represents Sweden.The intermediate entity is Norwegian here.The direct relation on the left says that Everdigen has traveled to Sweden, and the indirect relation implies the fact that Everdigen has probably visited Sweden as well as its nearby area, otherwise he could not complete the sketches of Norwegian, demonstrating the fuzzy logic consistency with high probability.Step 2 is the process of counterfactual data augmentation, where counterfactual relation composition is generated by random entity replacement.③ and ④ are the counterfactual augmentations of ① and ②, respectively.Finally, in Step 3, the LLM is optimized to generate direct/indirect relations with their logically consistent indirect/direct counterparts as inputs.Here, ①→ ②, ②→ ①, ③→ ④, and ④→ ③ are considered.</p>
<p>Figure 3 :
3
Figure 3: Results of 5 experiments with different option input orders across different model sizes on the test set of LogiQA-v2.Brown circular marker: outlier, green triangle: arithmetic mean value.</p>
<p>Figure 4 :
4
Figure 4: The averaged log-likelihood value of different models on the self-constructed logically consistent and inconsistent instances, respectively.w/ L. refers to the models augmented with LogicLLM.</p>
<p>of Frans Post, Everdingen took advantage of this mishap by making sketches of the Norwegian landscape, which would have seemed very exotic to his Dutch countrymen.His annotated drawings document visits to the south -east Norwegian coast and to Bohusland and the Göteborg area in western Sweden.Sentence 2: In 1644 Everdingen travelled to Norway and Sweden, a trip that was to have profound consequences on his art.</p>
<p>Roerich travelled toNorway and Master , a trip that was to have profound consequences on his art .</p>
<p>[</p>
<p>User]: Determine whether the relation between "[X]" and "[Y]" in the given two sentences are logically consistent.</p>
<p>of Frans Post, [X] took advantage of this mishap by making sketches of the Canal del Dique landscape , which would have seemed very exotic to his Dutch countrymen.His annotated drawings document visits to the south -east Canal del Dique coast and to Bohusland and the Göteborg area in western [Y].Sentence 2: In 1644 [X] travelled to Norway and [Y], a trip that was to have profound consequences on his art .</p>
<p>Table 1 :
1
The results on logical reasoning benchmarks.Better results are annotated in bold.† refers that the corresponding model is trained through QLoRA
ReClorLogiQA-v2Model / DatasetDevTestDevTestAcc. Acc. Acc. Acc.ChatGPT56.6 61.2 54.5 52.7LLaMA-7B30.2 30.3 27.4 28.1w/ LogicLLM32.4 31.0 27.7 28.6LLaMA-13B30.4 33.5 33.0 32.1w/ LogicLLM37.4 36.3 34.1 34.0LLaMA-33B45.2 50.3 41.2 41.6w/ LogicLLM  †50.2 54.4 45.9 42.6Falcon-40B38.4 37.1 35.9 36.1w/ LogicLLM  †41.4 43.0 38.6 37.2FLAN-T5-3B54.6 52.5 48.7 48.7w/ LogicLLM &amp; FLAN 55.8 54.1 50.8 50.1FLAN-T5-11B57.4 59.9 55.3 53.1w/ LogicLLM &amp; FLAN 61.2 61.1 56.0 54.0</p>
<p>Table 2 :
2
The results of LLaMA models on RACE and MMLU.† means training through QLoRA.
RACEMMLUModel / DatasetDevTest 0-shot 5-shotAcc. Acc.Acc.Acc.LLaMA-7B31.3 32.333.336.2w/ LogicLLM37.3 37.934.636.6LLaMA-13B55.8 54.541.146.7w/ LogicLLM57.7 55.643.347.3LLaMA-33B68.4 68.154.358.3w/ LogicLLM  † 68.8 68.154.458.3ReClorLogiQA-v2Model / DatasetDevTestDevTestAcc. Acc. Acc. Acc.LLaMA-13B30.4 33.5 33.0 32.1w/ LogicLLM (ctr)33.4 33.3 33.1 32.7w/ LogicLLM (ar)37.4 36.3 34.1 34.0LLaMA-33B45.2 50.3 41.2 41.6w/ LogicLLM  † (no aug.) 49.4 53.0 44.2 40.8w/ LogicLLM  † (1 aug.)50.8 52.7 45.6 41.5w/ LogicLLM  †50.2 54.4 45.9 42.6</p>
<p>Table 3 :
3
The effect of different training objectives.Ctr refers contrastive learning and ar means the autoregressive variant.no aug.means the counterfactual data augmentation is removed from the Logi-cLLM framework.</p>
<p>† means that the model is trained with QLoRA.</p>
<p>Table 5 :
5
The ratio of consistent data deemed by Chat-GPT and GPT-4.Anony.refers to anonymization and C.F. is the simplification of Counterfactual.</p>
<p>Table 6 :
6
(Jiao et al., 2022)y of LLMs, i.e., Chat-GPT (GPT-3.5-turbo)andLLaMA,and existing stateof-the-art methods(Jiao et al., 2022)on logical reasoning benchmarks.The evaluation of LLMs follows zeroshot in-context learning setting, where the models are expected to decode the answer based on the given instruction, context, and question.
ReClorLogiQA-v2Model / DatasetDevTestDevTestAcc. Acc. Acc. Acc.RoBERTa-L.62.655.659.857.0MERIt (RoBERTa-L)69.461.662.659.3MERIt (DeBERTa-XXL) 80.678.1--LLaMA-7B28.828.324.423.7LLaMA-13B31.634.431.631.1LLaMA-33B45.250.341.241.6GPT-3.5-turbo56.661.254.552.7w/ CoT58.857.7-53.1</p>
<p>Table 7 :
7
except two differences.First, we remove the usage of knowledge graph for relation annotation to enable fully self-supervision and simplify the construction workflow.Secondly, The comparison on RACE dataset between our reproduced results and those reported by the opriginal paper of LLaMA.
High Middle WeightedLLaMA-7B46.961.151.0LLaMA-7B (Ours)--32.3LLaMA-13B47.261.651.4LLaMA-13B (Ours)--54.5LLaMA-33B48.364.152.9LLaMA-33B (Ours)--68.1</p>
<p>Table 8 :
8
The accuracy of LLaMA and FLAN-T5 based models on BIG-Bench-Hard.Direct refer to few-shot setting through direct prompting, where only the final answer is given.Instead, in CoT setting, the reasoning process is also concatenated.The exemplars used for direct few-shot prompting and CoT prompting are consistent in each task, which are officially provided.
Model / DatasetZero-shot Direct CoTLLaMA-7B24.930.4 27.0w/ LogicLLM25.230.8 25.9LLaMA-13B25.034.7 32.3w/ LogicLLM26.335.0 33.9FLAN-T5-3B38.040.2 35.1w/ LogicLLM &amp; FLAN40.541.2 36.7FLAN-T5-11B43.042.6 40.9w/ LogicLLM &amp; FLAN44.136.2 40.2</p>
<p>Table 9 :
9
The results on logical reasoning benchmarks with enhanced Chain-of-Thought prompting.
ReClorLogiQA-v2Model / DatasetDevTestDevTestAcc. Acc. Acc. Acc.zero-shotChatGPT56.6 61.2 54.5 52.7w/ CoT58.8 57.7 54.5 53.15-shotChatGPT61.0 63.0 55.1 54.5w/ CoT62.0 62.5 47.6 55.6w/ CoT + Cate. N/A N/A 55.8 55.0
In this paper, we will use the term logical consistency to represent consistency in fuzzy logic for simplification, which is further described by relational consistency. This means that the relationship between a logically consistent data pair has a higher degree of logical consistency but cannot be strictly proved considering the diverse expressions of relations.
In practice, we find 87% indirect relations are composed of two relation triplets, 12% contain three triplets, and less than 1% have more than 4 triplets. This prevents the logical consistency be weakened by long context.
AcknowledgementsThis research is supported by the Ministry of Education, Singapore, under its Science of Learning Grant (award ID: MOE-MOESOL2021-0006). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of the Ministry of Education, Singapore.Besides, we sincerely appreciate the valuable comments from all the reviewers to help us make the paper polished.We also greatly thank to Chengwei Qin and Professor Aixin Sun for their kind suggestions.code and models are released at https://github.com/SparkJiao/LogicLLM.LimitationsIn this paper, we have explored the feasibility to introduce logical reasoning capability into LLMs via purely self-supervised meta-training.Though the results have demonstrated significant improvements on logical reasoning benchmarks, there are also some limitations: Randomness from Diverse Prompt/Instruction.In our experiments, we find that the performance of LLMs, especially those never optimized by instruction tuning, is varying to different prompts.We try to reduce the variance by (1) using simpler prompt (as shown in Section D or (2) using the released prompt by commonly accepted benchmark or leaderboard, e.g., MMLU, Big-Bench-Hard and Chain-of-Thought Hub(Fu et al., 2023).Nevertheless, this still cannot entirely keep the certainty of the experimental results.Non-uniform Evaluation Strategy.Currently, there is no de facto technical standard for LLMs evaluation.Some work just let language models generate the response and match the content.However, this can be unfair for non-instructiontuned models since they often cannot generate meaningful and complete sentences, especially those under 13 billion parameters.Scaling.Due to the resource limitation, we can only scale the method into models with 40 billion parameters under the help of low-rank adaptation.
Gpt4all: Training an assistant-style chatbot with large scale data distillation. Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, Andriy Mulyar, 2023from gpt-3.5-turbo</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, 10.48550/arXiv.2302.04023CoRR, abs/2302.040232023</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 10.48550/arXiv.2211.12588CoRR, abs/2211.125882022</p>
<p>Binding language models in symbolic languages. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu, 10.48550/arXiv.2210.02875CoRR, abs/2210.028752022</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, Sepassi, 10.48550/arXiv.2204.02311David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas EckJeff Dean, Slav Petrovand Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311</p>
<p>Reasonbert: Pre-trained to reason with distant supervision. Xiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu, Huan Sun, 10.18653/v1/2021.emnlp-main.494EMNLP. ACL2021</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, CoRR, abs/2305.143142023</p>
<p>Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, Tushar Khot, 10.48550/arXiv.2305.17306CoRR, abs/2305.173062023</p>
<p>PAL: program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, 10.48550/arXiv.2211.10435CoRR, abs/2211.104352022</p>
<p>Discern: Discourseaware entailment reasoning network for conversational machine reading. Yifan Gao, Chien-Sheng Wu, Jingjing Li, Shafiq R Joty, C H Steven, Caiming Hoi, Irwin Xiong, Michael R King, Lyu, 10.18653/v1/2020.emnlp-main.191EMNLP. ACL2020</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, ICLR. OpenReview2021</p>
<p>DAGN: discourse-aware graph network for logical reasoning. Yinya Huang, Meng Fang, Yu Cao, Liwei Wang, Xiaodan Liang, 10.18653/v1/2021.naacl-main.467NAACL-HLT. ACL2021</p>
<p>REPT: bridging language models and machine reading comprehension via retrieval-based pre-training. Fangkai Jiao, Yangyang Guo, Yilin Niu, Feng Ji, Feng-Lin Li, Liqiang Nie, 10.18653/v1/2021.findings-acl.13Findings of ACL/IJCNLP. ACL2021</p>
<p>Merit: Meta-path guided contrastive learning for logical reasoning. Fangkai Jiao, Yangyang Guo, Xuemeng Song, Liqiang Nie, 10.18653/v1/2022.findings-acl.276Findings of ACL. ACL2022</p>
<p>Is chatgpt A good translator? A preliminary study. Wenxiang Jiao, Wenxuan Wang, Jen-Tse Huang, Xing Wang, Zhaopeng Tu, 10.48550/arXiv.2301.08745CoRR, abs/2301.087452023</p>
<p>series in artificial intelligence. Daniel Jurafsky, James H Martin, Speech and language processing. Pearson Education International20092pearson international edition] edition</p>
<p>RACE: large-scale reading comprehension dataset from examinations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard H Hovy, 10.18653/v1/d17-1082EMNLP. ACL2017</p>
<p>A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, Jimmy Xiangji Huang, ACL. ACL. 2023</p>
<p>Hanmeng Liu, Jian Liu, Leyang Cui, Nan Duan, Ming Zhou, Yue Zhang, Logiqa2.0 datasetlogical reasoning in mrc and nli tasks. TASLP2022a</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, 10.24963/ijcai.2020/501IJCAI. 2020</p>
<p>Knowledge based multilingual language model. Linlin Liu, Xin Li, Ruidan He, Lidong Bing, R Shafiq, Luo Joty, Si, EMNLP. ACL2022b</p>
<p>The flan collection: Designing data and methods for effective instruction tuning. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, Adam Roberts, 10.48550/arXiv.2301.13688CoRR, abs/2301.136882023</p>
<p>Augmented language models: a survey. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Edouard Celikyilmaz, Yann Grave, Thomas Lecun, Scialom, 10.48550/arXiv.2302.07842CoRR, abs/2302.078422023</p>
<p>Distant supervision for relation extraction without labeled data. Mike Mintz, Steven Bills, Rion Snow, Daniel Jurafsky, Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLPACL2009</p>
<p>OpenAI. 2023. Gpt-4 technical report. Preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, 10.48550/arXiv.2306.01116CoRR, abs/2306.01116Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for falcon LLM: outperforming curated corpora with web data, and web data only. Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro CappelliGuilherme Penedo2022</p>
<p>Is chatgpt a general-purpose natural language processing task solver?. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang, 10.48550/arXiv.2302.06476CoRR, abs/2302.064762023</p>
<p>ERICA: Improving entity and relation understanding for pre-trained language models via contrastive learning. Yujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu, Peng Li, Heng Ji, Minlie Huang, Maosong Sun, Jie Zhou, 10.18653/v1/2021.acl-long.260ACL/IJCNLP. ACL2021</p>
<p>GCC: graph contrastive coding for graph neural network pre-training. Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, Jie Tang, 10.1145/3394486.3403168KDD. ACM2020</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 10.48550/arXiv.2302.04761CoRR, abs/2302.047612023</p>
<p>Challenging bigbench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, Jason Wei, 10.48550/arXiv.2210.09261CoRR, abs/2210.092612022</p>
<p>Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, CoRR, abs/2302.139712023</p>
<p>Seaeval for multilingual foundation models: From cross-lingual alignment to cultural reasoning. Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao, Yang Ding, Ai Ti Aw, Nancy F Chen, CoRR, abs/2309.047662023a</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, 10.48550/arXiv.2305.17926CoRR, abs/2305.179262023b</p>
<p>Logic-driven context extension and data augmentation for logical reasoning of text. Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, Nan Duan, 10.18653/v1/2022.findings-acl.127ACL. ACL2022</p>
<p>KEPLER: A unified model for knowledge embedding and pre-trained language representation. Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, Jian Tang, 10.1162/tacl_a_00360TACL. 92021</p>
<p>2022a. Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, ICLR. OpenReviewM. Dai, and Quoc V. Le.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, 10.48550/arXiv.2206.07682CoRR, abs/2206.07682Emergent abilities of large language models. 2022b</p>
<p>From word models to world models: Translating from natural language to the probabilistic language of thought. Lionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, K Vikash, Jacob Mansinghka, Joshua B Andreas, Tenenbaum, 10.48550/arXiv.2306.12672CoRR, abs/2306.126722023</p>
<p>Wizardlm: Empowering large language models to follow complex instructions. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, CoRR, abs/2304.122442023</p>
<p>Discriminative reasoning for document-level relation extraction. Wang Xu, Kehai Chen, Tiejun Zhao, 10.18653/v1/2021.findings-acl.144Findings of ACL. ACL2021</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, In ICLR. Open-Review. 2020</p>
<p>SIRE: separate intra-and inter-sentential reasoning for document-level relation extraction. Shuang Zeng, Yuting Wu, Baobao Chang, Findings of ACL. ACL2021</p>
<p>Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, Nan Duan, CoRR, abs/2104.06598AR-LSAT: investigating analytical reasoning of text. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1505 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1505</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1505</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-263909305</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.08549v1.pdf" target="_blank">Cross-Episodic Curriculum for Transformer Agents</a></p>
                <p><strong>Paper Abstract:</strong> We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the learning efficiency and generalization of Transformer agents. Central to CEC is the placement of cross-episodic experiences into a Transformer’s context, which forms the basis of a curriculum. By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes. Such synergy combined with the potent pattern recognition capabilities of Transformer models delivers a powerful cross-episodic attention mechanism. The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task reinforcement learning with discrete control, such as in DeepMind Lab, where the curriculum captures the learning progression in both individual and progressively complex settings, and the other involving imitation learning with mixed-quality data for continuous control, as seen in RoboMimic, where the curriculum captures the improvement in demonstrators’ expertise. In all instances, policies resulting from CEC exhibit superior performance and strong generalization. Code is open-sourced on the project website cec-agent.github.io to facilitate research on Transformer agent learning.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1505.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1505.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Learning-Progress Curriculum (CEC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning-Progress-Based Cross-Episodic Curriculum (within CEC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum that orders recorded trajectories by the temporal learning progress of a (multi-task) RL agent, concatenating episodes from earlier to later checkpoints so the Transformer can attend across episodes and distill policy improvements via cross-episodic attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CEC Transformer policy (Transformer-XL backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Causal Transformer policy (Transformer-XL) that takes concatenated cross-episodic observation-action sequences as context and is trained by supervised negative log-likelihood on actions; uses self-attention across episodes (cross-episodic attention) to internalize improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>DeepMind Lab (DMLab) — Goal Maze / Watermaze / Irreversible Path</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>3D partially-observable ego-centric pixel-input mazes with joystick/discrete actions; episodes spawn agents and goals randomly and require long-horizon exploration and planning to reach sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>embodied navigation / goal-reaching (not commonsense or science procedures)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Goal Maze (multi-room navigation), Watermaze (spatial navigation), Irreversible Path (planning with irreversible choices)</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Tasks require hierarchical planning and multi-step exploration (explore, remember spatial layout, plan path); complex navigation decomposes into discovering subregions and chain of navigation actions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Learning-progress-based curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Collect online interactions (trajectories) from a multi-task PPO agent across training checkpoints; order episodes by training time / checkpoint so earlier, suboptimal behaviours precede later, improved behaviours; sample sequences by concatenating episodes from increasing learning stages to form cross-episodic context for the Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>learning progress (temporal improvement of a source RL agent)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Same task configuration across curriculum but trajectories reflect policy improvement over training; for DMLab experiments, curricula include checkpoints corresponding to agent performance across training epochs (no explicit step-count range provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Averaged across three DMLab tasks, CEC agents trained with curricula (including learning-progress) achieve competitive success rates comparable to RL oracles and outperform many baselines; per-task CEC (all curricula combined) results: Goal Maze ~65.2% ± 6.7, Watermaze ~50.9% ± 6.6, Irreversible Path ~38.2% ± 7.0 (Table 3 'Ours' row summarizes CEC agent performance across tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>When trained on the same curricular data but with cross-episodic attention ablated (i.e., effectively no cross-episodic aggregation), performance degrades substantially: Goal Maze 35.0% ± 7.1, Watermaze 20.0% ± 2.5, Irreversible Path 3.8% ± 4.9 (Table 3 'Ours w/o Cross-Episodic Attention').</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>The paper compares learning-progress curricula to task-difficulty and expertise curricula. Overall, task-difficulty-based CEC often performs best on average (Table A.5); learning-progress CEC yields non-trivial performance and outperforms offline RL baselines like CQL when only machine-generated data exist (Appendix C.5). Exact per-curriculum averaged numbers are reported in Table A.5 (e.g., averaged across tasks: Ours (Task Difficulty), Auto: ~51.4; Ours (Task Difficulty), Fixed: ~54.4; Ours (Learning Progress): ~32.4 — see table for full context and per-experiment selection of best checkpoints).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>CEC trained with learning-progress curricula shows strong zero-shot generalization to varied task instances within DMLab test distributions and to some out-of-distribution difficulty/configuration changes; e.g., on Irreversible Path CEC outperforms the curriculum RL oracle by ~50% in zero-shot evaluation and shows robustness to dynamics perturbations (up to 1.6× better than RL oracles under some probes).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cross-episodic attention plus curricula that encode learning progress enables Transformers to distill improved behaviors from mixed-quality trajectories; ablation shows cross-episodic attention is indispensable (large drop without it); fine-grained curricula (higher granularity of learning stages) yields better performance; sufficiently long but not excessively long Transformer context is required to capture cross-episodic signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cross-Episodic Curriculum for Transformer Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1505.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1505.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Task-Difficulty Curriculum (CEC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-Difficulty-Based Cross-Episodic Curriculum (within CEC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum that sequences episodes from easier to progressively harder task instances (parameterized environment difficulty levels), concatenating trajectories across difficulty levels so the Transformer learns gradual adaptation and complexity handling across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CEC Transformer policy (Transformer-XL backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Causal Transformer policy (Transformer-XL) trained by supervised action prediction over concatenated multi-episode sequences; uses cross-episodic self-attention to access earlier episodes of varying difficulty while predicting current actions.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>DeepMind Lab (DMLab) — difficulty-parameterized mazes</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>3D mazes where difficulty is parameterized (examples in experiments: number of rooms {5,10,15,20} for Goal Maze; spawn radius 150→300→450→580 for Watermaze; built-in difficulty increments 0.1→0.3→0.5→0.7→0.9 for Irreversible Path).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>embodied navigation / goal-reaching (not commonsense or science procedures)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Navigation tasks with increasing number of rooms, larger spawn radii, or higher built-in difficulty requiring longer planning horizons and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Complex tasks are formed by increasing layout complexity and exploration demands (e.g., more rooms means more subregions and multi-step route planning), effectively composing longer planning sequences from simpler navigation subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Task-difficulty-based curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Construct episodes in sequences across parametrically easier tasks to progressively harder ones (e.g., mazes with 5 → 10 → 15 → 20 rooms); two sequencing mechanisms evaluated: 'Fixed' (fixed time per difficulty) and 'Auto' (dynamic promotion to next difficulty after 3 consecutive successes). Curricular sequences are concatenated to form cross-episodic context for training.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>task difficulty (parameterized difficulty levels ordered from easy to hard)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Explicit parameter ranges used: number of rooms 5→10→15→20; spawn radius 150→300→450→580; built-in difficulty 0.1→0.3→0.5→0.7→0.9 — thus complexity spans easy to very hard maze layouts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Task-difficulty CEC variants perform best on average across DMLab tasks. Averaged results (Table A.5) show Ours (Task Difficulty), Fixed and Auto around ~51–54% (average across tasks), and per-task results include outperforming BC baselines by up to 2.8×; on Irreversible Path (hardest) CEC exceeds curriculum RL oracle by ~50% zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Baselines trained without such curricular ordering (e.g., BC w/ Expert Data, Decision Transformer, Agentic Transformer) achieve substantially lower performance; ablation removing cross-episodic attention from the same curricular data yields large drops (e.g., Irreversible Path from 38.2% ±7.0 to 3.8% ±4.9).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Compared Fixed vs Auto sequencing: both evaluated; 'Auto' dynamic promotion performed well in zero-shot tests. Across curricula, task-difficulty-based CEC generally performed best on average (Table A.5). The paper also compares to AT and DT trained on 'Mixed Difficulty' data (same data as Task-Difficulty CEC) where CEC outperforms those baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Task-difficulty CEC is explicitly zero-shot evaluated on unseen test task distributions (e.g., mazes with 20 rooms not seen during training) and demonstrates strong generalization, surpassing curriculum RL oracles and other baselines in several settings; also robust to environment dynamics changes (up to 1.6× over oracles in some probes).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Ordering by task difficulty and using cross-episodic attention yields highly sample-efficient policies that generalize zero-shot to harder/unseen task configurations; fine-grained difficulty granularity improves performance (performance degrades monotonically as curricula become coarser); dynamic sequencing (Auto promotion) is effective for progressive curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cross-Episodic Curriculum for Transformer Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1505.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1505.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expertise Curriculum (CEC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expertise-Based Cross-Episodic Curriculum (within CEC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For imitation learning from mixed-quality demonstrations, trajectories are ordered by demonstrator expertise (from novice to expert) to form a curriculum so the Transformer can attend across demonstrations of increasing proficiency and learn robust visuomotor policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CEC Transformer policy (Transformer-XL backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Visuomotor Transformer-XL agent that encodes wrist and frontal camera images and proprioceptive inputs, and is trained by behavior cloning (negative log-likelihood) on concatenated multi-demo cross-episodic sequences; uses cross-episodic attention to distill patterns across mixed-quality demonstrators.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>RoboMimic (Multi-Human dataset) — robotic manipulation 'Lift' and 'Can' tasks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated robot manipulation tasks with image and proprioceptive inputs; 'Lift' requires picking up a cube, 'Can' requires picking up a soda can from a bin and placing into a target bin; demonstrations are human-collected with varying expertise levels.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>robotic manipulation procedures (not commonsense or science procedures)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Lift (pick up cube), Can (pick up can and place into target bin)</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Manipulation tasks require sequences of perceptual-motor primitives (approach, grasp, lift, place) composed into higher-level manipulation procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Expertise-based curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Order offline demonstration trajectories by demonstrator proficiency (worse operators → okay operators → better operators) to construct sequences; sample a small number of trajectories per expertise level to concatenate cross-episodically for training the Transformer via behavior cloning.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>demonstrator expertise (quality of demonstrations ranked and ordered from low to high)</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>From simpler manipulation episodes (novice trajectories) to more expert demonstrations exhibiting fluent task execution; exact numeric ranges: 3 expertise levels with 90 trajectories per level in experiments and sampled episodes per level uniformly from [1,5] when composing curricular sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>RoboMimic results (Table 2): Ours (Expertise-based CEC) achieves 100.0% ± 0.0 success on Lift and 100.0% ± 0.0 on Can (averages over multiple runs), outperforming or matching strong baselines; specifically, BC-RNN achieves 100.0% ± 0.0 on Lift and 96.0% ± 1.6 on Can; offline RL baselines (BCQ, CQL) perform worse (e.g., CQL 11.3% ± 9.3 on Lift, 0.0% on Can).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Training the same Transformer on the same demonstration data but without cross-episodic attention (ablated) reduces performance on Lift from 100% to 75.9% ± 12.3 and on Can from 100% to 99.3% ± 0.9 (Table 3 'Ours w/o Cross-Episodic Attention'), indicating cross-episodic aggregation over expertise-ordered demos is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Paper compares expertise-based curriculum to learning-progress-based curriculum on RoboMimic: expertise-based ordering is preferred when heterogeneous human demonstrations are available (expertise-based outperforms learning-progress in those settings); learning-progress still performs well when only machine-generated data are available (Appendix C.5).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Visuomotor policies trained with expertise-based CEC match or exceed baselines and generalize to test variations in RoboMimic tasks; specific zero-shot or compositional generalization to novel manipulation compositions is not the paper's focus beyond reporting high test success rates on the standard task splits.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Ordering mixed-quality demonstrations by demonstrator expertise and using cross-episodic attention lets the Transformer extract high-quality manipulation behaviors even when much data are sub-optimal; ablation shows cross-episodic attention is critical for extracting improvement signals from mixed-quality data; when human expertise labels are available, expertise-based curricula outperform learning-progress-based alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cross-Episodic Curriculum for Transformer Agents', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>In-context reinforcement learning with algorithm distillation <em>(Rating: 2)</em></li>
                <li>Curriculum learning <em>(Rating: 2)</em></li>
                <li>Multi-task curriculum learning in a complex, visual, hard-exploration domain <em>(Rating: 2)</em></li>
                <li>Teacher-student curriculum learning <em>(Rating: 2)</em></li>
                <li>Mix&match -agent curricula for reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1505",
    "paper_id": "paper-263909305",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "Learning-Progress Curriculum (CEC)",
            "name_full": "Learning-Progress-Based Cross-Episodic Curriculum (within CEC)",
            "brief_description": "A curriculum that orders recorded trajectories by the temporal learning progress of a (multi-task) RL agent, concatenating episodes from earlier to later checkpoints so the Transformer can attend across episodes and distill policy improvements via cross-episodic attention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CEC Transformer policy (Transformer-XL backbone)",
            "agent_description": "Causal Transformer policy (Transformer-XL) that takes concatenated cross-episodic observation-action sequences as context and is trained by supervised negative log-likelihood on actions; uses self-attention across episodes (cross-episodic attention) to internalize improvements.",
            "agent_size": null,
            "environment_name": "DeepMind Lab (DMLab) — Goal Maze / Watermaze / Irreversible Path",
            "environment_description": "3D partially-observable ego-centric pixel-input mazes with joystick/discrete actions; episodes spawn agents and goals randomly and require long-horizon exploration and planning to reach sparse rewards.",
            "procedure_type": "embodied navigation / goal-reaching (not commonsense or science procedures)",
            "procedure_examples": "Goal Maze (multi-room navigation), Watermaze (spatial navigation), Irreversible Path (planning with irreversible choices)",
            "compositional_structure": "Tasks require hierarchical planning and multi-step exploration (explore, remember spatial layout, plan path); complex navigation decomposes into discovering subregions and chain of navigation actions.",
            "uses_curriculum": true,
            "curriculum_name": "Learning-progress-based curriculum",
            "curriculum_description": "Collect online interactions (trajectories) from a multi-task PPO agent across training checkpoints; order episodes by training time / checkpoint so earlier, suboptimal behaviours precede later, improved behaviours; sample sequences by concatenating episodes from increasing learning stages to form cross-episodic context for the Transformer.",
            "curriculum_ordering_principle": "learning progress (temporal improvement of a source RL agent)",
            "task_complexity_range": "Same task configuration across curriculum but trajectories reflect policy improvement over training; for DMLab experiments, curricula include checkpoints corresponding to agent performance across training epochs (no explicit step-count range provided).",
            "performance_with_curriculum": "Averaged across three DMLab tasks, CEC agents trained with curricula (including learning-progress) achieve competitive success rates comparable to RL oracles and outperform many baselines; per-task CEC (all curricula combined) results: Goal Maze ~65.2% ± 6.7, Watermaze ~50.9% ± 6.6, Irreversible Path ~38.2% ± 7.0 (Table 3 'Ours' row summarizes CEC agent performance across tasks).",
            "performance_without_curriculum": "When trained on the same curricular data but with cross-episodic attention ablated (i.e., effectively no cross-episodic aggregation), performance degrades substantially: Goal Maze 35.0% ± 7.1, Watermaze 20.0% ± 2.5, Irreversible Path 3.8% ± 4.9 (Table 3 'Ours w/o Cross-Episodic Attention').",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "The paper compares learning-progress curricula to task-difficulty and expertise curricula. Overall, task-difficulty-based CEC often performs best on average (Table A.5); learning-progress CEC yields non-trivial performance and outperforms offline RL baselines like CQL when only machine-generated data exist (Appendix C.5). Exact per-curriculum averaged numbers are reported in Table A.5 (e.g., averaged across tasks: Ours (Task Difficulty), Auto: ~51.4; Ours (Task Difficulty), Fixed: ~54.4; Ours (Learning Progress): ~32.4 — see table for full context and per-experiment selection of best checkpoints).",
            "transfer_generalization": "CEC trained with learning-progress curricula shows strong zero-shot generalization to varied task instances within DMLab test distributions and to some out-of-distribution difficulty/configuration changes; e.g., on Irreversible Path CEC outperforms the curriculum RL oracle by ~50% in zero-shot evaluation and shows robustness to dynamics perturbations (up to 1.6× better than RL oracles under some probes).",
            "key_findings": "Cross-episodic attention plus curricula that encode learning progress enables Transformers to distill improved behaviors from mixed-quality trajectories; ablation shows cross-episodic attention is indispensable (large drop without it); fine-grained curricula (higher granularity of learning stages) yields better performance; sufficiently long but not excessively long Transformer context is required to capture cross-episodic signals.",
            "uuid": "e1505.0",
            "source_info": {
                "paper_title": "Cross-Episodic Curriculum for Transformer Agents",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Task-Difficulty Curriculum (CEC)",
            "name_full": "Task-Difficulty-Based Cross-Episodic Curriculum (within CEC)",
            "brief_description": "A curriculum that sequences episodes from easier to progressively harder task instances (parameterized environment difficulty levels), concatenating trajectories across difficulty levels so the Transformer learns gradual adaptation and complexity handling across episodes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CEC Transformer policy (Transformer-XL backbone)",
            "agent_description": "Causal Transformer policy (Transformer-XL) trained by supervised action prediction over concatenated multi-episode sequences; uses cross-episodic self-attention to access earlier episodes of varying difficulty while predicting current actions.",
            "agent_size": null,
            "environment_name": "DeepMind Lab (DMLab) — difficulty-parameterized mazes",
            "environment_description": "3D mazes where difficulty is parameterized (examples in experiments: number of rooms {5,10,15,20} for Goal Maze; spawn radius 150→300→450→580 for Watermaze; built-in difficulty increments 0.1→0.3→0.5→0.7→0.9 for Irreversible Path).",
            "procedure_type": "embodied navigation / goal-reaching (not commonsense or science procedures)",
            "procedure_examples": "Navigation tasks with increasing number of rooms, larger spawn radii, or higher built-in difficulty requiring longer planning horizons and exploration.",
            "compositional_structure": "Complex tasks are formed by increasing layout complexity and exploration demands (e.g., more rooms means more subregions and multi-step route planning), effectively composing longer planning sequences from simpler navigation subtasks.",
            "uses_curriculum": true,
            "curriculum_name": "Task-difficulty-based curriculum",
            "curriculum_description": "Construct episodes in sequences across parametrically easier tasks to progressively harder ones (e.g., mazes with 5 → 10 → 15 → 20 rooms); two sequencing mechanisms evaluated: 'Fixed' (fixed time per difficulty) and 'Auto' (dynamic promotion to next difficulty after 3 consecutive successes). Curricular sequences are concatenated to form cross-episodic context for training.",
            "curriculum_ordering_principle": "task difficulty (parameterized difficulty levels ordered from easy to hard)",
            "task_complexity_range": "Explicit parameter ranges used: number of rooms 5→10→15→20; spawn radius 150→300→450→580; built-in difficulty 0.1→0.3→0.5→0.7→0.9 — thus complexity spans easy to very hard maze layouts.",
            "performance_with_curriculum": "Task-difficulty CEC variants perform best on average across DMLab tasks. Averaged results (Table A.5) show Ours (Task Difficulty), Fixed and Auto around ~51–54% (average across tasks), and per-task results include outperforming BC baselines by up to 2.8×; on Irreversible Path (hardest) CEC exceeds curriculum RL oracle by ~50% zero-shot.",
            "performance_without_curriculum": "Baselines trained without such curricular ordering (e.g., BC w/ Expert Data, Decision Transformer, Agentic Transformer) achieve substantially lower performance; ablation removing cross-episodic attention from the same curricular data yields large drops (e.g., Irreversible Path from 38.2% ±7.0 to 3.8% ±4.9).",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Compared Fixed vs Auto sequencing: both evaluated; 'Auto' dynamic promotion performed well in zero-shot tests. Across curricula, task-difficulty-based CEC generally performed best on average (Table A.5). The paper also compares to AT and DT trained on 'Mixed Difficulty' data (same data as Task-Difficulty CEC) where CEC outperforms those baselines.",
            "transfer_generalization": "Task-difficulty CEC is explicitly zero-shot evaluated on unseen test task distributions (e.g., mazes with 20 rooms not seen during training) and demonstrates strong generalization, surpassing curriculum RL oracles and other baselines in several settings; also robust to environment dynamics changes (up to 1.6× over oracles in some probes).",
            "key_findings": "Ordering by task difficulty and using cross-episodic attention yields highly sample-efficient policies that generalize zero-shot to harder/unseen task configurations; fine-grained difficulty granularity improves performance (performance degrades monotonically as curricula become coarser); dynamic sequencing (Auto promotion) is effective for progressive curricula.",
            "uuid": "e1505.1",
            "source_info": {
                "paper_title": "Cross-Episodic Curriculum for Transformer Agents",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Expertise Curriculum (CEC)",
            "name_full": "Expertise-Based Cross-Episodic Curriculum (within CEC)",
            "brief_description": "For imitation learning from mixed-quality demonstrations, trajectories are ordered by demonstrator expertise (from novice to expert) to form a curriculum so the Transformer can attend across demonstrations of increasing proficiency and learn robust visuomotor policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CEC Transformer policy (Transformer-XL backbone)",
            "agent_description": "Visuomotor Transformer-XL agent that encodes wrist and frontal camera images and proprioceptive inputs, and is trained by behavior cloning (negative log-likelihood) on concatenated multi-demo cross-episodic sequences; uses cross-episodic attention to distill patterns across mixed-quality demonstrators.",
            "agent_size": null,
            "environment_name": "RoboMimic (Multi-Human dataset) — robotic manipulation 'Lift' and 'Can' tasks",
            "environment_description": "Simulated robot manipulation tasks with image and proprioceptive inputs; 'Lift' requires picking up a cube, 'Can' requires picking up a soda can from a bin and placing into a target bin; demonstrations are human-collected with varying expertise levels.",
            "procedure_type": "robotic manipulation procedures (not commonsense or science procedures)",
            "procedure_examples": "Lift (pick up cube), Can (pick up can and place into target bin)",
            "compositional_structure": "Manipulation tasks require sequences of perceptual-motor primitives (approach, grasp, lift, place) composed into higher-level manipulation procedures.",
            "uses_curriculum": true,
            "curriculum_name": "Expertise-based curriculum",
            "curriculum_description": "Order offline demonstration trajectories by demonstrator proficiency (worse operators → okay operators → better operators) to construct sequences; sample a small number of trajectories per expertise level to concatenate cross-episodically for training the Transformer via behavior cloning.",
            "curriculum_ordering_principle": "demonstrator expertise (quality of demonstrations ranked and ordered from low to high)",
            "task_complexity_range": "From simpler manipulation episodes (novice trajectories) to more expert demonstrations exhibiting fluent task execution; exact numeric ranges: 3 expertise levels with 90 trajectories per level in experiments and sampled episodes per level uniformly from [1,5] when composing curricular sequences.",
            "performance_with_curriculum": "RoboMimic results (Table 2): Ours (Expertise-based CEC) achieves 100.0% ± 0.0 success on Lift and 100.0% ± 0.0 on Can (averages over multiple runs), outperforming or matching strong baselines; specifically, BC-RNN achieves 100.0% ± 0.0 on Lift and 96.0% ± 1.6 on Can; offline RL baselines (BCQ, CQL) perform worse (e.g., CQL 11.3% ± 9.3 on Lift, 0.0% on Can).",
            "performance_without_curriculum": "Training the same Transformer on the same demonstration data but without cross-episodic attention (ablated) reduces performance on Lift from 100% to 75.9% ± 12.3 and on Can from 100% to 99.3% ± 0.9 (Table 3 'Ours w/o Cross-Episodic Attention'), indicating cross-episodic aggregation over expertise-ordered demos is beneficial.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Paper compares expertise-based curriculum to learning-progress-based curriculum on RoboMimic: expertise-based ordering is preferred when heterogeneous human demonstrations are available (expertise-based outperforms learning-progress in those settings); learning-progress still performs well when only machine-generated data are available (Appendix C.5).",
            "transfer_generalization": "Visuomotor policies trained with expertise-based CEC match or exceed baselines and generalize to test variations in RoboMimic tasks; specific zero-shot or compositional generalization to novel manipulation compositions is not the paper's focus beyond reporting high test success rates on the standard task splits.",
            "key_findings": "Ordering mixed-quality demonstrations by demonstrator expertise and using cross-episodic attention lets the Transformer extract high-quality manipulation behaviors even when much data are sub-optimal; ablation shows cross-episodic attention is critical for extracting improvement signals from mixed-quality data; when human expertise labels are available, expertise-based curricula outperform learning-progress-based alternatives.",
            "uuid": "e1505.2",
            "source_info": {
                "paper_title": "Cross-Episodic Curriculum for Transformer Agents",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "In-context reinforcement learning with algorithm distillation",
            "rating": 2,
            "sanitized_title": "incontext_reinforcement_learning_with_algorithm_distillation"
        },
        {
            "paper_title": "Curriculum learning",
            "rating": 2,
            "sanitized_title": "curriculum_learning"
        },
        {
            "paper_title": "Multi-task curriculum learning in a complex, visual, hard-exploration domain",
            "rating": 2,
            "sanitized_title": "multitask_curriculum_learning_in_a_complex_visual_hardexploration_domain"
        },
        {
            "paper_title": "Teacher-student curriculum learning",
            "rating": 2,
            "sanitized_title": "teacherstudent_curriculum_learning"
        },
        {
            "paper_title": "Mix&match -agent curricula for reinforcement learning",
            "rating": 1,
            "sanitized_title": "mixmatch_agent_curricula_for_reinforcement_learning"
        }
    ],
    "cost": 0.014111499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Cross-Episodic Curriculum for Transformer Agents
12 Oct 2023</p>
<p>Lucy Xiaoyang Shi 
Stanford University</p>
<p>Yunfan Jiang 
Stanford University</p>
<p>Jake Grigsby 
The University of Texas at Austin</p>
<p>Linxi Fan 
Yuke Zhu 
The University of Texas at Austin</p>
<p>Cross-Episodic Curriculum for Transformer Agents
12 Oct 2023CAFB3A99D59E08CB7D4649180E299A52arXiv:2310.08549v1[cs.LG]
We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the learning efficiency and generalization of Transformer agents.Central to CEC is the placement of cross-episodic experiences into a Transformer's context, which forms the basis of a curriculum.By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes.Such synergy combined with the potent pattern recognition capabilities of Transformer models delivers a powerful cross-episodic attention mechanism.The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task reinforcement learning with discrete control, such as in DeepMind Lab, where the curriculum captures the learning progression in both individual and progressively complex settings, and the other involving imitation learning with mixed-quality data for continuous control, as seen in RoboMimic, where the curriculum captures the improvement in demonstrators' expertise.In all instances, policies resulting from CEC exhibit superior performance and strong generalization.Code is opensourced on the project website cec-agent.github.io to facilitate research on Transformer agent learning.1 Following the canonical definition in Sutton and Barto[73], we refer to the sequences of agent-environment interaction with clearly identified initial and terminal states as "episodes".We interchangeably use "episode", "trial", and "trajectory" in this work.37th Conference on Neural Information Processing Systems (NeurIPS 2023).</p>
<p>Introduction</p>
<p>The paradigm shift driven by foundation models [8] is revolutionizing the communities who study sequential decision-making problems [80], with innovations focusing on control [2,45,38,9], planning [76,32,33,78,17], pre-trained visual representation [57,50,67,51], among others.Despite the progress, the data-hungry nature makes the application of Transformer [75] agents extremely challenging in data-scarce domains like robotics [52,53,19,38,9].This leads us to the question: Can we maximize the utilization of limited data, regardless of their optimality and construction, to foster more efficient learning?</p>
<p>To this end, this paper introduces a novel algorithm named Cross-Episodic Curriculum (CEC), a method that explicitly harnesses the shifting distributions of multiple experiences when organized into a curriculum.The key insight is that sequential cross-episodic data manifest useful learning signals that do not easily appear in any separated training episodes. 1As illustrated in Figure 1, CEC realizes this through two stages: 1) formulating curricular sequences to capture (a) the policy improvement on single environments, (b) the learning progress on a series of progressively harder environments, or (c) the increase of demonstrators' proficiency; and 2) causally distilling policy improvements into the model weights of Transformer agents through cross-episodic attention.When a policy is trained to predict actions at current time steps, it can trace back beyond ongoing trials and internalize improved behaviors encoded in curricular data, thereby achieving efficient learning  1) Preparation of curricular data.We order multiple experiences such that they explicitly capture curricular patterns.For instance, they can be policy improvement in single environments, learning progress in a series of progressively harder environments, or the increase of the demonstrator's expertise.2) Model training with cross-episodic attention.When training the model to predict actions, it can trace back beyond the current episode and internalize the policy refinement for more efficient learning.Here each τ represents an episode (trajectory).â refers to actions predicted by the model.Colored triangles denote causal Transformer models.and robust deployment when probed with visual or dynamics perturbations.Contrary to prior works like Algorithm Distillation (AD, Laskin et al. [42]) which, at test time, samples and retains a single task configuration across episodes for in-context refinement, our method, CEC, prioritizes zero-shot generalization across a distribution of test configurations.With CEC, agents are evaluated on a new task configuration in each episode, emphasizing adaptability to diverse tasks.</p>
<p>We investigate the effectiveness of CEC in enhancing sample efficiency and generalization with two representative case studies.They are: 1) Reinforcement Learning (RL) on DeepMind Lab (DM-Lab) [5], a 3D simulation encompassing visually diverse worlds, complicated environment dynamics, ego-centric pixel inputs, and joystick control; and 2) Imitation Learning (IL) from mixed-quality human demonstrations on RoboMimic [53], a framework designed to study robotic manipulation with proprioceptive and external camera observations and continuous control.Despite RL episodes being characterized by state-action-reward tuples and IL trajectories by state-action pairs, our method exclusively employs state-action pairs in its approach.</p>
<p>In challenging embodied navigation tasks, despite significant generalization gaps (Table 1), our method surpasses concurrent and competitive method Agentic Transformer (AT, Liu and Abbeel [47]).It also significantly outperforms popular offline RL methods such as Decision Transformer (DT, Chen et al. [13]) and baselines trained on expert data, with the same amount of parameters, architecture, and data size.It even exceeds RL oracles directly trained on test task distributions by 50% in a zero-shot manner.CEC also yields robust embodied policies that are up to 1.6× better than RL oracles when zero-shot probed with unseen environment dynamics.When learning continuous robotic control, CEC successfully solves two simulated manipulation tasks, matching and outperforming previous well-established baselines [53,25,41].Further ablation reveals that CEC with cross-episodic attention is a generally effective recipe for learning Transformer agents, especially in applications where sequential data exhibit moderate and smooth progression.</p>
<p>Cross-Episodic Curriculum: Formalism and Implementations</p>
<p>In this section, we establish the foundation for our cross-episodic curriculum method by first reviewing the preliminaries underlying our case studies, which encompass two representative scenarios in sequential decision-making.Subsequently, we formally introduce the assembly of curricular data and the specifics of model optimization utilizing cross-episodic attention.Lastly, we delve into the practical implementation of CEC in the context of these two scenarios.</p>
<p>Preliminaries</p>
<p>Reinforcement learning.We consider the setting where source agents learn through trial and error in partially observable environments.Denoting states s ∈ S and actions a ∈ A, an agent interacts in a Partially Observable Markov Decision Process (POMDP) with the transition function p(s t+1 |s t , a t ) : S × A → S. It observes o ∈ O emitted from observation function Ω(o t |s t , a t−1 ) : S × A → O and receives scalar reward r from R(s, a) : S × A → R.Under the episodic task setting, RL seeks to learn a parameterized policy π θ (•|s) that maximizes the return over a fixed length T of interaction steps: π θ = arg max θ∈Θ T −1 t=0 γ t r t , where γ ∈ [0, 1) is a discount factor.Here we follow the canonical definition of an episode τ as a series of environment-agent interactions with length T , τ := (s 0 , a 0 , r 0 , . . ., s T −1 , a T −1 , r T −1 , s T ), where initial states s 0 are sampled from initial state distribution s 0 ∼ ρ 0 (s) and terminal states s T are reached once the elapsed timestep exceeds T .Additionally, we view all RL tasks considered in this work as goal-reaching problems [39,26] and constrain all episodes to terminate upon task completion.It is worth noting that similar to previous work [42], training data are collected by source RL agents during their online learning.Nevertheless, once the dataset is obtained, our method is trained offline in a purely supervised manner.</p>
<p>Imitation learning.We consider IL settings with existing trajectories composed only of state-action pairs.Furthermore, we relax the assumption on demonstration optimality and allow them to be crowdsourced [10,12,11].Data collected by operators with varying expertise are therefore unavoidable.Formally, we assume the access to a dataset D N := {τ 1 , . . ., τ N } consisting of N demonstrations, with each demonstrated trajectory τ i := (s 0 , a 0 , . . ., s T −1 , a T −1 ) naturally identified as an episode.The goal of IL, specifically of behavior cloning (BC), is to learn a policy π θ that accurately models the distribution of behaviors.When viewed as goal-reaching problems, BC policies can be evaluated by measuring the success ratio in completing tasks [26].</p>
<p>Curricular Data Assembly and Model Optimization</p>
<p>Meaningful learning signals emerge when multiple trajectories are organized and examined crossepisodically along a curriculum axis.This valuable information, which is not easily discernible in individual training episodes, may encompass aspects such as the improvement of an RL agent's navigation policy or the generally effective manipulation skills exhibited by operators with diverse proficiency levels.With a powerful model architecture such as Transformer [75,16], such emergent and valuable learning signals can be baked into policy weights, thereby boosting performance in embodied tasks.</p>
<p>For a given embodied task M, we define its curriculum C M as a collection of trajectories τ consisting of state-action pairs.A series of ordered levels [L 1 , . . ., L L ] partitions this collection such that l∈{1,...,L} L l = C M and ∀i,j∈{1,...,L},i̸ =j L {i,j} = ∅.More importantly, these ordered levels characterize a curriculum by encoding, for example, learning progress in single environments, learning progress in a series of progressively harder environments, or the increase of the demonstrator's expertise.</p>
<p>With a curriculum C M := {τ i } N i=1 and its characteristics [L 1 , . . ., L L ], we construct a curricular sequence T that spans multiple episodes and captures the essence of gradual improvement in the following way:
T := l∈{1,...,L}
τ (1) , . . ., τ (C) , where C ∼ U ( |L l | ) and τ (c) ∼ L l .</p>
<p>(
)1
The symbol ⊕ denotes the concatenation operation.U ( K ) denotes a uniform distribution over the discrete set {k ∈ N, k ≤ K}.In practice, we use values smaller than |L l | considering the memory consumption.We subsequently learn a causal policy that only depends on cross-episodic historical observations π θ (•|o
(≤n)
≤t ).Note that this modeling strategy differs from previous work that views sequential decision-making as a big sequence-modeling problem [13,37,42,38].It instead resembles the causal policy in Baker et al. [4].Nevertheless, we still follow the best practice [36,60,22] to provide previous action as an extra modality of observations in POMDP RL tasks.</p>
<p>We leverage the powerful attention mechanism of Transformer [75] to enable cross-episodic attention.Given observation series O
= f Q (O), key K = f K (O)
, and value V = f V (O) matrices, with each row being a D-dim vector.Attention operation is performed to aggregate information:
Attention(Q, K, V ) = softmax( QK ⊺ √ D )V.(2)
Depending on whether the input arguments for f Q and f {K,V } are the same, attention operation can be further divided into self-attention and cross-attention.Since tasks considered in this work do not require additional conditioning for task specification, we follow previous work [4,82] to utilize self-attention to process observation series.Nevertheless, ours can be naturally extended to handle, for example, natural language or multi-modal task prompts, following the cross-attention introduced in Jiang et al. [38].</p>
<p>Finally, this Transformer policy is trained by simply minimizing the negative log-likelihood objective J NLL of labeled actions, conditioned on cross-episodic context:
J NLL = − log π θ (•|T ) = 1 |T | × T |T | n=1 T t=1 − log π θ a (n) t |o (≤n) ≤t .(3)
Regarding the specific memory architecture, we follow Baker et al. [4], Adaptive Agent Team et al. [1] to use Transformer-XL [16] as our model backbone.Thus, during deployment, we keep its hidden states propagating across test episodes to mimic the training settings.</p>
<p>Practical Implementations</p>
<p>We now discuss concrete instantiations of CEC for 1) RL with DMLab and 2) IL with RoboMimic.Detailed introductions to the benchmark and task selection are deferred to Sec. 3. We investigate the following three curricula, where the initial two pertain to RL, while the final one applies to IL:</p>
<p>Learning-progress-based curriculum.In the first instantiation, inspired by the literature on learning progress [54,27,65,40], we view the progression of learning agents as a curriculum.</p>
<p>Concretely, we train multi-task PPO agents [70,63] on tasks drawn from test distributions.We record their online interactions during training, which faithfully reflect the learning progress.Finally, we form the learning-progress-based curriculum by sequentially concatenating episodes collected at different learning stages.Note that this procedure is different from Laskin et al. [42], where for each environment, the learning dynamics of multiple single-task RL agents has to be logged.In contrast, we only track a single multi-task agent per environment.</p>
<p>Task-difficulty-based curriculum.In the second instantiation, instead of taking snapshots of RL agents directly trained on test configurations, we collect learning progress on a series of easier but progressively harder tasks.For instance, in an embodied navigation task, the test configuration includes 20 rooms.Rather than logging source agents' learning progression in the 20-room maze, we record in a series of mazes with 5, 10, and 15 rooms.We then structure stored episodes first following learning progress and then the increase of layout complexity.This practice naturally creates a task-difficulty-based curriculum, which resembles curriculum RL that is based on task difficulty [54,58].We find it especially helpful for hard-exploration problems where the source RL agent does not make meaningful progress.</p>
<p>Expertise-based curriculum.For the setting of IL from mixed-quality demonstrations, we instantiate a curriculum based on demonstrators' expertise.This design choice is motivated by literature on learning from heterogeneous demonstrators [6,81], with the intuition that there is little to learn from novices but a lot from experts.To realize this idea, we leverage the Multi-Human dataset from RoboMimic [53].Since it contains demonstrations collected by human demonstrators with varying proficiency, we organize offline demonstration trajectories following the increase of expertise to construct the expertise-based curriculum.</p>
<p>Experimental Setup</p>
<p>In this section, we elaborate on the experimental setup of our case studies.Our investigation spans two representative and distinct settings: 1) online reinforcement learning with 3D maze environments of DMLab [5], and 2) imitation learning from mixed-quality human demonstrations of RoboMimic [53].</p>
<p>For each of them, we discuss task selection, baselines, and training and evaluation protocols.Teasers of these tasks are shown in Figure 2.</p>
<p>Task Settings and Environments</p>
<p>DeepMind Lab [5] is a 3D learning environment with diverse tasks.Agents spawn in visually complex worlds, receive ego-centric (thus partially observable) RGB pixel inputs, and execute joystick actions.We consider three levels from this benchmark: Goal Maze, Watermaze [56], and Sky Maze with Irreversible Path.They challenge agents to explore, memorize, and plan over a long horizon.Their goals are similar -to navigate in complicated mazes and find a randomly spawned goal, upon which sparse rewards will be released.Episodes start with randomly spawned agents and goals and terminate once goals are reached or elapsed steps have exceeded pre-defined horizons.</p>
<p>RoboMimic [53] is a framework designed for studying robot manipulation and learning from demonstrations.Agents control robot arms with fixed bases, receive proprioceptive measurements and image observations from mounted cameras, and operate with continuous control.We evaluate two simulated tasks: "Lift" and "Can".In the "Lift" task, robots are tasked with picking up a small cube.In the "Can" task, robots are required to pick up a soda can from a large bin and place it into a smaller target bin.Episodes start with randomly initialized object configuration and terminate upon successfully completing the task or exceeding pre-defined horizons.</p>
<p>Baselines</p>
<p>The primary goal of these case studies is to assess the effectiveness of our proposed cross-episodic curriculum in increasing the sample efficiency and boosting the generalization capability of Transformer agents.Therefore, in online RL settings, we compare against source RL agents which generate training data for our method and refer to them as oracles.These include a) PPO agents directly trained on test task distributions, denoted as "RL (Oracle)" hereafter, and b) curriculum PPO agents that are gradually adapted from easier tasks to the test difficulty, which is referred to as "Curriculum RL (Oracle)".Furthermore, we compare against one concurrent and competitive method Agentic Transformer [47], denoted as "AT".It is closely related to our method, training Transformers on sequences of trajectory ascending sorted according to their rewards.We also compare against popular offline RL method Decision Transformer [13], denoted as "DT".Additionally, we include another behavior cloning agent that has the same model architecture as ours but is trained on optimal data without cross-episodic attention.This baseline is denoted as "BC w/ Expert Data".For the case study on IL from mixed-quality demonstrations, we adopt the most competing approach, BC-RNN, from Mandlekar et al. [53] as the main baseline.We also include comparisons against other offline RL methods [44] such as Batch-Constrained Q-learning (BCQ) [25] and Conservative Q-Learning (CQL) [41].</p>
<p>Training and Evaluation</p>
<p>We follow the best practice to train Transformer agents, including adopting AdamW optimizer [49], learning rate warm-up and cosine annealing [48], etc. Training is performed on NVIDIA V100 GPUs.During evaluation, for agents resulting from our method, each run involves several test rollouts to fill the context.We keep hidden states of Transformer-XL [16] propagating across episodes.We run other baselines and oracles for 100 episodes to estimate their performances.For our methods on RL settings, we compute the maximum success rate averaged across a sliding window over all test episodes to account for in-context improvement.The size of the sliding window equals one-quarter of the total test episodes.These values are averaged over 20 runs to constitute the final reporting metric.For our methods on the IL setting, since all training data are successful trajectories, we follow Mandlekar et al. [53] to report the maximum success rate achieved over the course of training, directly averaged over test episodes.</p>
<p>Experiments</p>
<p>We aim to answer the following four research questions through comprehensive experiments.</p>
<ol>
<li>To what extent can our cross-episodic curriculum increase the sample efficiency of Transformer agents and boost their generalization capability? 2. Is CEC consistently effective and generally applicable across distinct learning settings?3. What are the major components that contribute to the effectiveness of our method?</li>
</ol>
<p>Main Evaluations</p>
<p>We answer the first two questions above by comparing learned agents from our method against 1) Reinforcement Learning (RL) oracles in online RL settings and 2) well-established baselines on learning from mixed-quality demonstrations in the Imitation Learning (IL) setting.</p>
<p>We first examine agents learned from learning-progress-based and task-difficulty-based curricula in challenging 3D maze environments.The first type of agent is denoted as "Ours (Learning Progress)".For the second type, to ensure that the evaluation also contains a series of tasks with increasing difficulty, we adopt two mechanisms that control the task sequencing [58]: 1) fixed sequencing where agents try each level of difficulty for a fixed amount of times regardless of their performance and 2) dynamic sequencing where agents are automatically promoted to the next difficulty level if they consecutively succeed in the previous level for three times.We denote these two variants as "Ours (Task Difficulty), Fixed" and "Ours (Task Difficulty), Auto", respectively.Note that because the task-difficulty-based curriculum does not contain any training data on test configurations, these two settings are zero-shot evaluated on test task distributions.We summarize these differences in Table 1.We denote AT and DT trained on data consisting of a mixture of task difficulties as "AT (Mixed Difficulty)" and "DT (Mixed Difficulty)".Note that these data are the same used to train "Ours (Task Difficulty)".Similarly, we denote AT and DT directly trained on test difficulty as "AT (Single Difficulty)" and "DT (Single Difficulty)".These data are the same used to train "Ours (Learning Progress)".</p>
<p>Cross-episodic curriculum results in sample-efficient agents.As shown in Figure 3, on two out of three examined DMLab levels, CEC agents perform comparable to RL oracles and outperform the BC baselines trained on expert data by at most 2.8×.On the hardest level Irreversible Path where agents have to plan the route ahead and cannot backtrack, both the BC baseline and RL oracle fail.However, our agents succeed in proposing correct paths that lead to goals and significantly outperform the curriculum RL oracle by 50% even in a zero-shot manner.Because CEC only requires environment interactions generated during the course of training of online source agents (the task-difficulty-based curriculum even contains fewer samples compared to the curriculum RL, as illustrated in Table 1), the comparable and even better performance demonstrates that our method yields highly sample-efficient embodied policies.On average, our method with task-difficulty-based curriculum performs the best during evaluation (Table A.5), confirming the benefit over the Table 2: Evaluation results on RoboMimic.Visuomotor policies trained with our expertise-based curriculum outperform the most competing history-dependent behavior cloning baseline, as well as other offline RL algorithms.For our method on the Lift task, we conduct 5 independent runs each with 10 rollout episodes.On the Can task, we conduct 10 independent runs each with 5 rollout episodes due to the longer horizon required to complete the task.Standard deviations are included.</p>
<p>Task</p>
<p>Ours BC-RNN [53] BCQ [25] CQL [41] Lift 100.0 ± 0.0 100.0 ± 0.0 93.3 ± 0.9 11.3 ± 9.3 Can 100.0 ± 0.0 96.0 ± 1.6 77.3 ± 6.8 0.0 ± 0.0  4, CEC generally improves Transformer agents in learning robust policies that can generalize to perturbations across various axes.On three settings where the BC w/ Expert Data baseline still manages to make progress, CEC agents are up to 2× better.Compared to oracle curriculum RL agents, our policies significantly outperform them under three out of five examined scenarios.It is notable that on Irreversible Path with out-of-distribution difficulty, CEC agent is 1.6× better than the curriculum RL oracle trained on the same data.These results highlight the benefit of learning with curricular contexts.On average, our method surpasses the concurrent AT baseline and achieves significantly better performance than other baselines (Table A .6).This empirically suggests that CEC helps to learn policies that are robust to environmental perturbations and can quickly generalize to new changes.</p>
<p>Cross-episodic curriculum is effective across a wide variety of learning scenarios.We now move beyond RL settings and study the effectiveness of the expertise-based curriculum in the IL setting with mixed-quality demonstrations.This is a common scenario, especially in robotics, where demonstrations are collected by human operators with varying proficiency [52].As presented in Table 2, visuomotor policies trained with the expertise-based curriculum are able to match and outperform the well-established baseline [53] on two simulated robotic manipulation tasks and achieve significantly better performance than agents learned from prevalent offline RL algorithms [25,41].These results suggest that our cross-episodic curriculum is effective and broadly applicable across various problem settings.More importantly, it provides a promising approach to utilizing limited but sub-optimal data in data-scarce regimes such as robot learning.</p>
<p>Ablation Studies</p>
<p>In this section, we seek to answer the third research question to identify the components critical to the effectiveness of our approach.We focus on three parts: the importance of cross-episodic attention, the influence of curriculum granularity, and the effect of varying context length.Finally, we delve into the fourth question, identifying scenarios where CEC is expected to be helpful.</p>
<p>Importance of cross-episodic attention.The underlying hypothesis behind our method is that cross-episodic attention enables Transformer agents to distill policy improvement when mixed-optimality trajectories are viewed collectively.To test this, on DMLab levels and RoboMimic tasks, we train the same Transformer agents with the same curricular data and training epochs but without cross-episodic attention.We denote such agents as "Ours w/o Cross-Episodic Attention" in Table 3. Results demonstrate that the ablated variants experience dramatic performance degradation on four out of five examined tasks, which suggests that naively behaviorally cloning sub-optimal data can be problematic and detrimental.Cross-episodic attention views curricular data collectively, facilitating the extraction of knowledge and patterns crucial for refining decision-making, thereby optimizing the use of sub-optimal data.</p>
<p>Fine Medium Coarse</p>
<p>Curriculum Granularity Curriculum granularity.We perform this ablation with the task-difficulty-based curriculum on DMLab levels, due to the ease of adjusting granularity.We treat the curricula listed in the column "Ours (Task Difficulty)" in Table 1 as "Fine", and gradually make them coarser to study the impact.Note that we ensure the same amount of training data.See the Appendix, Sec.C.4 for how we define granularity levels "Medium" and "Coarse".We visualize the performance relative to the most fine-grained in Figure 5.The monotonic degradation of policy performance with respect to curriculum coarseness suggests that fine-grained curricula are critical for Transformer agents to mostly benefit from cross-episodic training.</p>
<p>Varying context length.Lastly, we study the effect of varying context length on DMLab and visualize it in Figure 6.We normalize all performance values relative to those of "Ours (Task Difficulty), Auto" reported in Figure 3.It turns out that both too short and unnecessarily long context windows are harmful.On two out of three levels, using a shorter context decreases the performance even more.This finding coincides with Laskin et al. [42] that a sufficiently long Transformer context is necessary to retain cross-episodic information.Furthermore, we also discover that an unnecessarily long context is also harmful.We hypothesize that this is due to the consequent training and optimization instability.Curriculum selection based on task complexities and data sources.For RL tasks, we recommend starting with the learning-progress-based curriculum.However, if the task itself is too challenging, such that source algorithms barely make progress, we recommend the task-difficultybased curriculum.In IL settings, we further investigate the performance of the learning-progress-based curriculum on RoboMimic tasks considered in this work.Detailed setup and results are included in Appendix, Sec C.5.To summarize, if human demonstrations are available, even if they are generated to be heterogeneous in quality, we recommend using the expertise-based curriculum.However, in the absence of human demonstrations and only with access to machine-generated data (e.g., generated by RL agents), our learning-progress-based curriculum is recommended because it achieves non-trivial performance and significantly outperforms offline RL methods such as CQL [41].</p>
<p>Related Work</p>
<p>Sequential decision-making with Transformer agents.There are many ongoing efforts to replicate the strong emergent properties demonstrated by Transformer models for sequential decisionmaking problems [80].Decision Transformer [13] and Trajectory Transformer [37] pioneered this thread by casting offline RL [44] as sequence modeling problems.Gato [68] learns a massively multitask agent that can be prompted to complete embodied tasks.MineDojo [22] and VPT [4] utilize numerous YouTube videos for large-scale pre-training in the video game Minecraft.VIMA [38] and RT-1 [9] build Transformer agents trained at scale for robotic manipulation tasks.BeT [71] and C-BeT [14] design novel techniques to learn from demonstrations with multiple modes with Trans-formers.Our causal policy most resembles to VPT [4].But we focus on designing learning techniques that are generally effective across a wide spectrum of learning scenarios and application domains.</p>
<p>Cross-episodic learning.Cross-episodic learning is a less-explored terrain despite that it has been discussed together with meta-RL [77] for a long time.RL 2 [18] uses recurrent neural networks for online meta-RL by optimizing multi-episodic value functions.Meta-Q-learning [21] instead learns multi-episodic value functions in an offline manner.Algorithm Distillation (AD) [42] and Adaptive Agent (AdA) [1] are two recent, inspiring methods in cross-episodic learning.Though at first glance our learning-progress-based curriculum appears similar to AD, significant differences emerge.Unlike AD, which focuses on in-context improvements at test time and requires numerous single-task source agents for data generation, our approach improves data efficiency for Transformer agents by structuring data in curricula, requiring only a single multi-task agent and allowing for diverse task instances during evaluations.Meanwhile, AdA, although using cross-episodic attention with a Transformer backbone, is rooted in online RL within a proprietary environment.In contrast, we focus on offline behavior cloning in accessible, open-source environments, also extending to IL scenarios unexplored by other meta-learning techniques.Complementary to this, another recent study [43] provides theoretical insight into cross-episodic learning.</p>
<p>Curriculum learning.Curriculum learning represents training strategies that organize learning samples in meaningful orders to facilitate learning [7].It has been proven effective in numerous works that adaptively select simpler task [58,74,69,62,15,55,59,46] or auxiliary rewards [35,72].Tasks are also parameterized to form curricula by manipulating goals [24,30,66], environment layouts [79,3,64], and reward functions [28,34].Inspired by this paradigm, our work harnesses the improving nature of sequential experiences to boost learning efficiency and generalization for embodied tasks.</p>
<p>Conclusion</p>
<p>In this work, we introduce a new learning algorithm named Cross-Episodic Curriculum to enhance the sample efficiency of policy learning and generalization capability of Transformer agents.It leverages the shifting distributions of past learning experiences or human demonstrations when they are viewed as curricula.Combined with cross-episodic attention, CEC yields embodied policies that attain high performance and robust generalization across distinct and representative RL and IL settings.CEC represents a solid step toward sample-efficient policy learning and is promising for data-scarce problems and real-world domains.</p>
<p>Limitations and future work.The CEC algorithm relies on the accurate formulation of curricular sequences that capture the improving nature of multiple experiences.However, defining these sequences accurately can be challenging, especially when dealing with complex environments or tasks.Incorrect or suboptimal formulations of these sequences could negatively impact the algorithm's effectiveness and the overall learning efficiency of the agents.A thorough exploration regarding the attainability of curricular data is elaborated upon in Appendix, Sec D.</p>
<p>In subsequent research, the applicability of CEC to real-world tasks, especially where task difficulty remains ambiguous, merits investigation.A deeper assessment of a demonstrator's proficiency trajectory -from initial unfamiliarity to the establishment of muscle memory -could offer a valuable learning signal.Moreover, integrating real-time human feedback to dynamically adjust the curriculum poses an intriguing challenge, potentially enabling CEC to efficiently operate in extended contexts, multi-agent environments, and tangible real-world tasks.Our DMLab main experiment is conducted on three levels with task IDs</p>
<p>• explore goal locations large,</p>
<p>• rooms watermaze,</p>
<p>• and skymaze irreversible path hard.</p>
<p>We use no action repeats during training and evaluation.For experiments with varying task difficulty, we select difficulty parameters "room numbers", "spawn radius", and "built-in difficulty" for these three levels, respectively.We adopt environment wrappers and helper functions from Petrenko et al. [63] to flexibly and precisely maneuver task difficulties.</p>
<p>Due to different task horizons, we tune the context length of Transformer-XL models and vary curricular trajectories accordingly.These differences are summarized in Table A.4.</p>
<p>RL oracles serve as source agents used to generate training data for our methods and the "BC w/ Expert Data" baseline.They are trained with the PPO [70] implementation from Petrenko et al. [63].</p>
<p>The "BC w/ Expert Data" baselines have the same model architecture, training hyperparameters, and amount of training data as our method, but are trained solely on trajectories generated by the best performing RL oracles without cross-episodic attention.</p>
<p>C.2 DMLab Generalization</p>
<p>This series of experiments probe the zero-shot generalization capabilities of embodied agents in unseen maze configurations, out-of-distribution difficulty levels, and varying environment dynamics.For the task "Goal Maze w/ Unseen Mechanism", we use the level with task ID</p>
<p>C.5 Comparison of Curricula in RoboMimic</p>
<p>In IL settings, we further explored the efficacy of various curricula.For the RoboMimic tasks examined, we employed a learning-progress-based curriculum, ensuring the total training trajectories matched those of the expertise-based curriculum (i.e., 270 trajectories per task).All other parameters remained consistent, with the training data derived from RoboMimic's machine-generated dataset.</p>
<p>Table A.8 indicates that when heterogeneous-quality human demonstrations are accessible, the expertise-based curriculum is preferable due to its superior performance over the learning-progressbased approach.Conversely, without expert demonstrations and relying solely on machine-generated data, the learning-progress-based curriculum is still commendable.It offers noteworthy results and surpasses offline RL methods like CQL [41], even though CQL is trained on the full RoboMimic dataset, encompassing 1500 trajectories for the Lift task and 3900 for the Can task.</p>
<p>D Feasibility of Obtaining Curricular Data</p>
<p>The challenge of accurately orchestrating a curriculum is non-trivial and hinges on various factors.</p>
<p>In the present work, three curriculum designs are introduced and validated, each with its practical considerations and underlying assumptions, discussed herein.</p>
<p>Learning-Progress-Based Curriculum.RL agents typically exhibit monotonic improvement over training epochs, thereby naturally producing incrementally better data.The curriculum here is devised through a series of checkpoints throughout the training duration, necessitating no supplementary assumptions for its formulation.</p>
<p>Task-Difficulty-Based Curriculum.In contexts where environmental difficulty is parameterizable, curricula can be structured through a schedule, determined by the relevant difficulty parameter, as demonstrated within this work.In scenarios lacking parameterized difficulty, alternatives such as methods proposed by Kanitscheider et al. [40] may be employed.The application of our method to tasks where difficulty is not explicitly characterized presents an intriguing avenue for future research.</p>
<p>Expertise-Based Curriculum.A notable limitation resides in the requisite to estimate demonstrators' proficiency.While some IL benchmarks, e.g., RoboMimic [53], come pre-equipped with proficiency labels, a broader application of our method necessitates an approximation of proficiency.One plausible approach entails ranking trajectories via completion time.Furthermore, a demonstrator's proficiency is likely to organically improve-from initial unfamiliarity with teleoperation systems or tasks, to a stage of executing data collection with muscle memory [52].This progression potentially provides a rich learning signal conducive for CEC application.</p>
<p>E Broader Impact</p>
<p>Our Cross-Episodic Curriculum can significantly enhance Transformer agent learning but carries potential societal impacts.The efficiency of our method depends on the curriculum's design.If the curriculum unintentionally reflects biases, it could lead to the amplification of these biases in learned policies, potentially perpetuating unfair or discriminatory outcomes in AI-driven decisions.Furthermore, the computational intensity of our approach at evaluation could contribute to increased energy usage, which has implications for the environmental footprint of AI applications.</p>
<p>Figure 1 :
1
Figure 1: Cross-episodic curriculum for Transformer agents.CEC involves two major steps: 1) Preparation of curricular data.We order multiple experiences such that they explicitly capture curricular patterns.For instance, they can be policy improvement in single environments, learning progress in a series of progressively harder environments, or the increase of the demonstrator's expertise.2) Model training with cross-episodic attention.When training the model to predict actions, it can trace back beyond the current episode and internalize the policy refinement for more efficient learning.Here each τ represents an episode (trajectory).â refers to actions predicted by the model.Colored triangles denote causal Transformer models.</p>
<p>Figure 2 :
2
Figure 2: We evaluate our method on five tasks that cover challenges such as exploration and planning over long horizons in RL settings, as well as object manipulation and continuous control in IL settings.Figures are from Beattie et al. [5] and Mandlekar et al. [53].</p>
<p>≤t } (shorthanded as O hereafter for brevity), Transformer projects it into query Q</p>
<p>Figure 4 :
4
Figure 4: Generalization results on DMLab.Top row: Evaluation results on Goal Maze with unseen maze mechanism and Irreversible Path with out-of-distribution difficulty levels.Bottom row: Evaluation results on three levels with environment dynamics differing from training ones.CEC agents display robustness and generalization across various dimensions, outperforming curriculum RL oracles by up to 1.6×.We follow the same evaluation protocol as in Figure 3.The error bars represent the standard deviations over 20 runs.</p>
<p>Figure 5 :
5
Figure 5: We compare the performance relative to agents trained with the finegrained curricula.Performance monotonically degrades as task-difficultybased curricula become coarser.</p>
<p>Figure 6 :
6
Figure 6: Both short and unnecessarily long context windows decrease the performance.Numbers in the legend denote context lengths.Performance values are relative to those of "Ours (Task Difficulty), Auto" reported in Figure 3. "Irrevers.Path" stands for the task "Irreversible Path".</p>
<p>Figure A. 1 :
1
Figure A.1: A visualization of the task "Goal Maze (Unseen Mechanism)".It includes doors that are randomly opened or closed.</p>
<p>Table 1 :
1
Generalization gaps between training and testing for DMLab levels.Note that agents resulting from task-difficulty-based curricula are not trained on test configurations.Therefore, their performance should be considered as zero-shot.
LevelDifficultyTestTraining DifficultyNameParameterDifficultyOursOursBCRLCurriculum RL(Learning Progress)(Task Difficulty)w/ Expert Data(Oracle)(Oracle)Goal MazeRoom Numbers20205→10→1520205→10→15→20WatermazeSpawn Radius580580150→300→450580580150→300→450→580Irreversible Path Built-In Difficulty.9.9.1→.3→.5→.7.9.9.1→.3→.5→.7→.9</p>
<p>Evaluation results on DMLab.Our CEC agents perform comparable to RL oracles and on average outperform other baseline methods.On the hardest task Irreversible Path where the RL oracle and BC baseline completely fail, our agents outperform the curriculum RL oracle by 50% even in a zero-shot manner.For our methods, DT, AT, and the BC w/ expert data baselines, we conduct 20 independent evaluation runs, each consisting of 100 episodes for Goal Maze and Watermaze and 50 episodes for Irreversible Path due to longer episode length.We test RL oracles for 100 episodes.The error bars represent the standard deviations over 20 runs.
Ours (Task Difficulty), AutoOurs (Task Difficulty), FixedOurs (Learning Progress)BC w/ Expert DataDT (Mixed Difficulty)DT (Single Difficulty)AT (Mixed Difficulty)AT (Single Difficulty)RL (Oracle)Curriculum RL (Oracle)Figure 3:</p>
<p>Table 3 :
3
Ablation on the importance of cross-episodic attention.Transformer agents trained with the same curricular data but without cross-episodic attention degrade significantly during evaluation, suggesting its indispensable role in learning highly performant policies.
DMLabRoboMimicGoal Maze Watermaze Irreversible PathLiftCanOurs 65.2 ± 6.7 50.9 ± 6.638.2 ± 7.0100.0 ± 0.0 100.0 ± 0.0Ours w/o Cross-Episodic Attention 35.0 ± 7.120.0 ± 2.53.8 ± 4.975.9 ± 12.399.3 ± 0.9
concurrent AT approach that leverages chain-of-hindsight experiences.When compared to DT, it outperforms by a significant margin, which suggests that our cross-episodic curriculum helps to squeeze learning signals that are useful for downstream decision-making.Cross-episodic curriculum boosts the generalization capability.To further investigate whether CEC can improve generalization at test time, we construct settings with unseen maze mechanisms (randomly open/closed doors), out-of-distribution difficulty, and different environment dynamics.See the Appendix, Sec.C.2 for the exact setups.As demonstrated in Figure</p>
<p>Table A .
A
2: Model hyperparameters for Transformer-XL.All experiments are conducted on cluster nodes with NVIDIA V100 GPUs.We utilize DDP (distributed data parallel) to accelerate the training if necessary.Training hyperparameters are listed in Table A.3.
Hyperparameter Value (DMLab) Value (RoboMimic)Hidden Size256400Number of Layers 42Number of Heads 88Pointwise Ratio44has a hidden dimension of 400 with ReLU activations. During deployment, we employ the "low-noiseevaluation" trick [31].B Training Details and HyperparametersTable A.3: Hyperparameters used during training.HyperparameterValue (DMLab) Value (RoboMimic)Learning Rate0.00050.0001Warmup Steps10000LR Cosine Annealing Steps 100000N/AWeight Decay0.00.0C Experiment DetailsC.1 DMLab Main Experiment</p>
<p>Table A .
A
4: Experiment details on DMLab tasks.Columns "Epoch" denote the exact training epochs with best validation performance.We select these checkpoints for evaluation.For task-difficultybased curriculum, the column "Training Trajectories" with n × m entries means n trajectories per difficulty level (m levels in total).The column "Sampled Episodes" with [i, j] entries means we first determine the number of episodes per difficulty level by uniformly sampling an integer from [i, j] (inclusively).
LevelContextTask-Difficulty-Based CurriculumLearning-Progress-Based CurriculumNameLengthEpoch Training Trajectories Sampled Episodes Epoch Training Trajectories Sampled EpisodesGoal Maze50084100 x 3[1, 5]883009Watermaze40089100 x 3[1, 5]803009Irreversible Path160090100 x 4[1, 3]974008</p>
<p>Table A .
A
5: Evaluation results on DMLab, averaged over three tasks (Figure3).
Ours (TaskOurs (TaskOurs (LearningDT (MixedDT (SingleAT (MixedAT (SingleBC w/ ExpertRLCurriculum RLDifficulty), AutoDifficulty), FixedProgress)Difficulty)Difficulty)Difficulty)Difficulty)Data(Oracle)(Oracle)51.454.432.435.311.742.733.414.240.650.6</p>
<p>Table A .
A
6: Generalization results on DMLab, averaged over five settings (Figure4).
Ours (TaskOurs (LearningDT (MixedDT (SingleAT (MixedAT (SingleBC w/ ExpertDifficulty)Progress)Difficulty)Difficulty)Difficulty)Difficulty)
Acknowledgments and Disclosure of FundingWe thank Guanzhi Wang and Annie Xie for helpful discussions.We are grateful to Yifeng Zhu, Zhenyu Jiang, Soroush Nasiriany, Huihan Liu, and Rutav Shah for constructive feedback on an early draft of this paper.We also thank the anonymous reviewers for offering us insightful suggestions and kind encouragement during the review period.This work was partially supported by research funds from Salesforce and JP Morgan.DataA Model ArchitectureIn this section, we provide comprehensive details about the Transformer model architectures considered in this work.We implement all models in PyTorch[61]and adapt the implementation of Transformer-XL from VPT[4].A.1 Observation EncodingExperiments conducted on both DMLab and RoboMimic include RGB image observations.For models trained on DMLab, we use a ConvNet[29]similar to the one used in Espeholt et al.[20].For models trained on RoboMimic, we follow Mandlekar et al.[53]to use a ResNet-18 network[29]followed by a spatial-softmax layer[23].We use independent and separate encoders for images taken from the wrist camera and frontal camera.Detailed model parameters are listed in Since DMLab is highly partially observable, we follow previous work[20,22,4]to supply the model with previous action input.We learn 16-dim embedding vectors for all discrete actions.To encode proprioceptive measurement in RoboMimic, we follow Mandlekar et al.[53]to not apply any learned encoding.Instead, these types of observation are concatenated with image features and passed altogether to the following layers.Note that we do not provide previous action inputs in RoboMimic, since we find doing so would incur significant overfitting.A.2 Transformer BackboneWe use Transformer-XL[16]as our model backbone, adapted from Baker et al.[4].Transformer-XL splits long sequences into shorter sub-sequences that reduce the computational cost of attention while allowing the hidden states to be carried across the entire input by attending to previous keys and values.This feature is critical for the long sequence inputs necessary for cross-episodic attention.Detailed model parameters are listed in Table A.2.A.3 Action DecodingTo decode joystick actions in DMLab tasks, we learn a 3-layer MLP whose output directly parameterizes a categorical distribution.This action head has a hidden dimension of 128 with ReLU activations.The "Goal Maze" and "Irreversible Path" tasks have an action dimension of 7, while "Watermaze" has 15 actions.To decode continuous actions in RoboMimic, we learn a 2-layer MLP that parameterizes a Gaussian Mixture Model (GMM) with 5 modes that generates a 7-dimensional action.This network The task "Irreversible Path (OOD.Difficulty)" corresponds to configurations with the built-in difficulty of 1 (agents are only trained on difficulty up to 0.9, as noted in Table1).For tasks with varying environment dynamics, we directly test agents with an action repeat of 2. This is different from the training setting with no action repeat.C.3 RoboMimic Main ExperimentWe leverage the Multi-Human (MH) dataset from Mandlekar et al.[53].It consists of demonstrations collected by operators with varying proficiency.We construct the expertise-based curriculum by following the order of "worse operators, okay operators, then better operators".We use a context length of 200 for both tasks.There are 90 trajectories per expertise level.To determine the number of trajectories per expertise level when constructing curricular data, we uniformly sample an integer from [1, 5] (inclusively).The "Lift" and "Can" tasks are solved after training for 33 epochs and 179 epochs, respectively.We control for the same number of training epochs in subsequent ablation studies.C.4 Ablation Study on Curriculum GranularityWe perform this ablation with the task-difficulty-based curriculum on DMLab levels due to the ease of adjusting granularity.The definition of varying levels of curriculum coarseness is listed in
Human-timescale adaptation in an open-ended task space. Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson, Hannah Openshaw, Jack Parker-Holder, Shreya Pathak, Nicolas Perez-Nieves, Nemanja Rakicevic, Tim Rocktäschel, Yannick Schroecker, Jakub Sygnowski, Karl Tuyls, Sarah York, Alexander Zacherl, Lei Zhang, arXiv: Arxiv-2301.076082023arXiv preprint</p>
<p>Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Keerthana Finn, Karol Gopalakrishnan, Alex Hausman, Daniel Herzog, Jasmine Ho, Julian Hsu, Brian Ibarz, Alex Ichter, Eric Irpan, Rosario Jang, Kyle Jauregui Ruano, Sally Jeffrey, Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Yuheng Kalashnikov, Kuang-Huei Kuang, Sergey Lee, Yao Levine, Linda Lu, Carolina Luu, Peter Parada, Jornell Pastor, Kanishka Quiambao, Jarek Rao, Diego Rettinghouse, Pierre Reyes, Sermanet, arXiv: Arxiv-2204.01691Do as i can, not as i say: Grounding language in robotic affordances. Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, 2022arXiv preprint</p>
<p>Emergent tool use from multi-agent autocurricula. Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob Mcgrew, Igor Mordatch, International Conference on Learning Representations. 2020</p>
<p>Video pretraining (vpt): Learning to act by watching unlabeled online videos. Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, Jeff Clune, arXiv: Arxiv-2206.117952022arXiv preprint</p>
<p>Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, Julian Schrittwieser, arXiv: Arxiv-1612.03801Demis Hassabis, Shane Legg, and Stig Petersen. Deepmind lab. Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King2016arXiv preprint</p>
<p>Imitation learning by estimating expertise of demonstrators. M Beliaev, Andy Shih, S Ermon, Dorsa Sadigh, Ramtin Pedarsani, International Conference On Machine Learning. 2022</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, International Conference on Machine Learning. 2009</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Michael S Sydney Von Arx, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Erik Brunskill, Shyamal Brynjolfsson, Dallas Buch, Rodrigo Card, Niladri Castellon, Annie Chatterji, Kathleen Chen, Jared Quincy Creel, Dora Davis, Chris Demszky, Moussa Donahue, Esin Doumbouya, Stefano Durmus, John Ermon, Kawin Etchemendy, Li Ethayarajh, Chelsea Fei-Fei, Trevor Finn, Lauren Gale, Karan Gillespie, Noah Goel, Shelby Goodman, Neel Grossman, Tatsunori Guha, Peter Hashimoto, John Henderson, Daniel E Hewitt, Jenny Ho, Kyle Hong, Jing Hsu, Thomas Huang, Saahil Icard, Dan Jain, Pratyusha Jurafsky, Siddharth Kalluri, Geoff Karamcheti, Fereshte Keeling, Omar Khani, Pang Wei Khattab, Mark Koh, Ranjay Krass, Rohith Krishna, Ananya Kuditipudi, Faisal Kumar, Mina Ladhak, Tony Lee, Jure Lee, Isabelle Leskovec, Levent, Lisa Xiang, Xuechen Li, Tengyu Li, Ali Ma, Christopher D Malik, Suvir Manning, Eric Mirchandani, Zanele Mitchell, Suraj Munyikwa, Avanika Nair, Deepak Narayan, Ben Narayanan, Allen Newman, Juan Carlos Nie, Hamed Niebles, Julian Nilforoshan, Giray Nyarko, Andy Ogut, Krishnan Shih, Alex Srinivasan, Rohan Tamkin, Armin W Taori, Florian Thomas, Rose E Tramèr, William Wang, Bohan Wang, Jiajun Wu, Yuhuai Wu, Sang Wu, Michihiro Michael Xie, Jiaxuan Yasunaga, Matei You, Michael Zaharia, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zhang, Kaitlyn Zheng, Percy Zhou, Liang, arXiv: Arxiv-2108.07258On the opportunities and risks of foundation models. Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Laurel Orr, Isabel Papadimitriou2021arXiv preprintJoon Sung Park</p>
<p>. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Yuheng Kalashnikov, Isabel Kuang, Kuang-Huei Leal, Sergey Lee, Yao Levine, Utsav Lu, Deeksha Malla, Igor Manjunath, Ofir Mordatch, Carolina Nachum, Jodilyn Parada, Emily Peralta, Karl Perez, Jornell Pertsch, Kanishka Quiambao, Michael Rao, Grecia Ryoo, Pannag Salazar, Kevin Sanketi, Jaspiar Sayed, Sumedh Singh, Austin Sontakke, Clayton Stone, Huong Tan, Vincent Tran, Steve Vanhoucke, Quan Vega, Fei Vuong, Ted Xia, Peng Xiao, Sichun Xu, Tianhe Xu, Brianna Yu, Zitkovich, Arxiv-2212.068172022Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv</p>
<p>Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations. S Daniel, Wonjoon Brown, Prabhat Goo, Scott Nagarajan, Niekum, arXiv: Arxiv-1904.063872019arXiv preprint</p>
<p>Learning from imperfect demonstrations from agents with varying dynamics. Zhangjie Cao, Dorsa Sadigh, arXiv: Arxiv-2103.059102021arXiv preprint</p>
<p>Learning from suboptimal demonstration via self-supervised reward regression. Letian Chen, Rohan Paleja, Matthew Gombolay, arXiv: Arxiv-2010.117232020arXiv preprint</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021. Aurelio Marc, Alina Ranzato, Yann N Beygelzimer, Percy Dauphin, Jennifer Wortman Liang, Vaughan, December 6-14, 2021. 2021</p>
<p>Jeff Zichen, Yibin Cui, Nur Wang, Muhammad Mahi, Lerrel Shafiullah, Pinto, arXiv: Arxiv-2210.10047From play to policy: Conditional behavior generation from uncurated robot data. 2022arXiv preprint</p>
<p>Mix&amp;match -agent curricula for reinforcement learning. Wojciech Czarnecki, M Siddhant, Max Jayakumar, Leonard Jaderberg, Yee Whye Hasenclever, Nicolas Teh, Otto Manfred, Simon Heess, Razvan Osindero, Pascanu, International Conference on Machine Learning. 2018</p>
<p>Transformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, Ruslan Salakhutdinov, arXiv: Arxiv-1901.028602019arXiv preprint</p>
<p>Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Yevgen Huang, Pierre Chebotar, Daniel Sermanet, Sergey Duckworth, Vincent Levine, Karol Vanhoucke, Marc Hausman, Klaus Toussaint, Andy Greff, Igor Zeng, Pete Mordatch, Florence, arXiv: Arxiv-2303.03378Palm-e: An embodied multimodal language model. 2023arXiv preprint</p>
<p>Rl 2 : Fast reinforcement learning via slow reinforcement learning. Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, Pieter Abbeel, arXiv: Arxiv- 1611.027792016arXiv preprint</p>
<p>Bridge data: Boosting generalization of robotic skills with cross-domain datasets. Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, Sergey Levine, arXiv: Arxiv-2109.133962021arXiv preprint</p>
<p>Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu, arXiv: Arxiv-1802.01561Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. 2018arXiv preprint</p>
<p>. Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, Alexander J Smola, arXiv: Arxiv-1910.001252019Meta-q-learning. arXiv preprint</p>
<p>Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, arXiv: Arxiv-2206.08853Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. 2022arXiv preprint</p>
<p>Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, Pieter Abbeel, arXiv: Arxiv-1509.06113Deep spatial autoencoders for visuomotor learning. 2015arXiv preprint</p>
<p>Intrinsically motivated goal exploration processes with automatic curriculum learning. Sébastien Forestier, Yoan Mollard, Pierre-Yves Oudeyer, ArXiv, abs/1708.021902017</p>
<p>Off-policy deep reinforcement learning without exploration. Scott Fujimoto, David Meger, Doina Precup, arXiv: Arxiv-1812.029002018arXiv preprint</p>
<p>Learning to reach goals via iterated supervised learning. Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, Sergey Levine, arXiv: Arxiv-1912.060882019arXiv preprint</p>
<p>Automated curriculum learning for neural networks. Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, Koray Kavukcuoglu, arXiv: Arxiv-1704.030032017arXiv preprint</p>
<p>Unsupervised meta-learning for reinforcement learning. Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, Sergey Levine, ArXiv, abs/1806.046402018</p>
<p>Deep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, arXiv:1512.03385December 2015</p>
<p>Automatic goal generation for reinforcement learning agents. David Held, Xinyang Geng, Carlos Florensa, Pieter Abbeel, International Conference on Machine Learning. 2018</p>
<p>Matthew W Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Nikola Momchev, Danila Sinopalnikov, Piotr Stańczyk, Sabela Ramos, Anton Raichuk, Damien Vincent, Léonard Hussenot, Robert Dadashi, Gabriel Dulac-Arnold, Manu Orsini, Alexis Jacq, Johan Ferret, Nino Vieillard, Seyed Kamyar, Seyed Ghasemipour, Sertan Girgin, Olivier Pietquin, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Abe Friesen, Ruba Haroun, Alex Novikov, Sergio Gómez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Andrew Cowie, Ziyu Wang, arXiv:2006.00979Bilal Piot, and Nando de Freitas. Acme: A research framework for distributed reinforcement learning. 2020arXiv preprint</p>
<p>Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning, ICML 2022. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, Sivan Sabato, Baltimore, Maryland, USAPMLRJuly 2022. 2022162of Proceedings of Machine Learning Research</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter, arXiv: Arxiv- 2207.05608Inner monologue: Embodied reasoning through planning with language models. 2022arXiv preprint</p>
<p>Unsupervised curricula for visual meta-reinforcement learning. Allan Jabri, Kyle Hsu, Ben Eysenbach, Abhishek Gupta, Alexei A Efros, Sergey Levine, Chelsea Finn, Advances in Neural Information Processing Systems. 2019</p>
<p>Reinforcement learning with unsupervised auxiliary tasks. Max Jaderberg, Volodymyr Mnih, Wojciech Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, Koray Kavukcuoglu, International Conference on Learning Representations. 2017</p>
<p>Human-level performance in 3d multiplayer games with population-based reinforcement learning. Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castañeda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel, 10.1126/science.aau6249Science. 36464432019</p>
<p>Offline reinforcement learning as one big sequence modeling problem. Michael Janner, Qiyang Li, Sergey Levine, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021. Aurelio Marc, Alina Ranzato, Yann N Beygelzimer, Percy Dauphin, Jennifer Wortman Liang, Vaughan, December 6-14, 2021. 2021</p>
<p>Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan, Vima, arXiv: Arxiv-2210.03094General robot manipulation with multimodal prompts. 2022arXiv preprint</p>
<p>Learning to achieve goals. Leslie Pack, Kaelbling , International Joint Conference on Artificial Intelligence. 1993</p>
<p>Multi-task curriculum learning in a complex, visual, hard-exploration domain. Ingmar Kanitscheider, Joost Huizinga, David Farhi, William Hebgen Guss, Brandon Houghton, Raul Sampedro, Peter Zhokhov, Bowen Baker, Adrien Ecoffet, Jie Tang, Oleg Klimov, Jeff Clune, arXiv: Arxiv-2106.148762021Minecraft. arXiv preprint</p>
<p>Conservative q-learning for offline reinforcement learning. Aviral Kumar, Aurick Zhou, George Tucker, Sergey Levine, arXiv: Arxiv-2006.047792020arXiv preprint</p>
<p>In-context reinforcement learning with algorithm distillation. Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, Steven Strouse, Angelos Stenberg Hansen, Ethan Filos, Himanshu Brooks, Satinder Sahni, Volodymyr Singh, Mnih, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Supervised pretraining can learn in-context reinforcement learning. Jonathan N Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, Emma Brunskill, arXiv: Arxiv-2306.148922023arXiv preprint</p>
<p>Offline reinforcement learning: Tutorial, review, and perspectives on open problems. Sergey Levine, Aviral Kumar, George Tucker, Justin Fu, arXiv: Arxiv-2005.016432020arXiv preprint</p>
<p>Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, arXiv: Arxiv-2209.07753Code as policies: Language model programs for embodied control. 2022arXiv preprint</p>
<p>Adaptive auxiliary task weighting for reinforcement learning. Xingyu Lin, Harjatin Singh Baweja, George Kantor, David Held, Advances in Neural Information Processing Systems. 2019</p>
<p>Emergent agentic transformer from chain of hindsight experience. Hao Liu, Pieter Abbeel, arXiv: Arxiv-2305.165542023arXiv preprint</p>
<p>SGDR: stochastic gradient descent with warm restarts. Ilya Loshchilov, Frank Hutter, 5th International Conference on Learning Representations. Toulon, France2017. April 24-26, 2017. 2017Conference Track Proceedings. OpenReview.net</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USAMay 6-9, 2019. 2019OpenReview.net</p>
<p>Vip: Towards universal visual reward and representation via value-implicit pre-training. Jason Yecheng, Shagun Ma, Dinesh Sodhani, Osbert Jayaraman, Vikash Bastani, Amy Kumar, Zhang, arXiv: Arxiv-2210.000302022arXiv preprint</p>
<p>Where are we in the search for. Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Jason Yecheng, Claire Ma, Sneha Chen, Aryan Silwal, Vincent-Pierre Jain, Pieter Berges, Jitendra Abbeel, Dhruv Malik, Yixin Batra, Oleksandr Lin, Aravind Maksymets, Franziska Rajeswaran, Meier, arXiv: Arxiv-2303.182402023arXiv preprint</p>
<p>Roboturk: A crowdsourcing platform for robotic skill learning through imitation. Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Booher, Max Spero, Albert Tung, Julian Gao, John Emmons, Anchit Gupta, Emre Orbay, Silvio Savarese, Li Fei-Fei, arXiv: Arxiv-1811.027902018arXiv preprint</p>
<p>Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, Roberto Martín-Martín, arXiv: Arxiv- 2108.03298What matters in learning from offline human demonstrations for robot manipulation. 2021arXiv preprint</p>
<p>Teacher-student curriculum learning. Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman, arXiv: Arxiv-1707.001832017arXiv preprint</p>
<p>Teacher-student curriculum learning. Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman, IEEE transactions on neural networks and learning systems. 2019</p>
<p>Spatial localization does not require the presence of local cues. G M Richard, Morris, 10.1016/0023-9690(81)90020-5Learning and Motivation. 1221981</p>
<p>R3m: A universal visual representation for robot manipulation. Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta, arXiv: Arxiv-2203.126012022arXiv preprint</p>
<p>Autonomous task sequencing for customized curriculum design in reinforcement learning. S Narvekar, J Sinapov, P Stone, International Joint Conference on Artificial Intelligence. 2017</p>
<p>Learning curriculum policies for reinforcement learning. Sanmit Narvekar, Peter Stone, Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. International Foundation for Autonomous Agents and Multiagent Systems. the 18th International Conference on Autonomous Agents and MultiAgent Systems. International Foundation for Autonomous Agents and Multiagent Systems2019</p>
<p>Christopher Openai, Greg Berner, Brooke Brockman, Vicki Chan, Przemyslaw Cheung, Christy Debiak, David Dennison, Quirin Farhi, Shariq Fischer, Chris Hashme, Rafal Hesse, Scott Józefowicz, Catherine Gray, Jakub Olsson, Michael Pachocki, Henrique P D O Petrov, Jonathan Pinto, Tim Raiman, Jeremy Salimans, Jonas Schlatter, Schneider, arXiv: Arxiv-1912.06680Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning. 2019arXiv preprint</p>
<p>Pytorch: An imperative style, highperformance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>Curriculum design for machine learners in sequential decision tasks. B Peng, J Macglashan, R Loftin, M Littman, D Roberts, Matthew E Taylor, IEEE Transactions on Emerging Topics in Computational Intelligence. 22018</p>
<p>Sample factory. Aleksei Petrenko, Zhehui Huang, Tushar Kumar, Gaurav Sukhatme, Vladlen Koltun, arXiv: Arxiv-2006.11751Egocentric 3d control from pixels at 100000 fps with asynchronous reinforcement learning. 2020arXiv preprint</p>
<p>Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. Rémy Portelas, Cédric Colas, Katja Hofmann, Pierre-Yves Oudeyer, Conference on Robot Learning. 2019</p>
<p>Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments. Rémy Portelas, Cédric Colas, Katja Hofmann, Pierre-Yves Oudeyer, arXiv: Arxiv-1910.072242019arXiv preprint</p>
<p>Automated curriculum generation through setter-solver interactions. Sébastien Racanière, Andrew Kyle Lampinen, Adam Santoro, David P Reichert, Vlad Firoiu, Timothy P Lillicrap, International Conference on Learning Representations. 2020</p>
<p>Real-world robot learning with masked visual pre-training. Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, Trevor Darrell, arXiv: Arxiv- 2210.031092022arXiv preprint</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, arXiv: Arxiv-2205.06175Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. 2022arXiv preprint</p>
<p>Learning by playing-solving sparse reward tasks from scratch. Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van De Wiele, Volodymyr Mnih, Nicolas Heess, Jost Tobias, Springenberg , International Conference on Machine Learning. 2018</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv: Arxiv-1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Behavior transformers: Cloning k modes with one stone. Nur Muhammad, Mahi Shafiullah, Jeff Zichen, Ariuntuya Cui, Lerrel Altanzaya, Pinto, arXiv: Arxiv-2206.112512022arXiv preprint</p>
<p>Situational fusion of visual representation for visual navigation. Danfei William B Shen, Yuke Xu, Leonidas J Zhu, Li Guibas, Silvio Fei-Fei, Savarese, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2019</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, 2018MIT press</p>
<p>Automatic curriculum graph generation for reinforcement learning agents. M Svetlik, Matteo Leonetti, J Sinapov, Rishi Shah, Nick Walker, P Stone, AAAI Conference on Artificial Intelligence. 2017</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, arXiv: Arxiv- 1706.037622017arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv: Arxiv-2305.162912023arXiv preprint</p>
<p>Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, Matt Botvinick, arXiv: Arxiv-1611.05763Learning to reinforcement learn. 2016arXiv preprint</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang, arXiv: Arxiv-2302.015602023arXiv preprint</p>
<p>A performance-based start state curriculum framework for reinforcement learning. Jan Wöhlke, Felix Schmitt, Herke Van Hoof, Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems. the 19th International Conference on Autonomous Agents and MultiAgent Systems2020</p>
<p>Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, arXiv: Arxiv-2303.04129Pieter Abbeel, and Dale Schuurmans. Foundation models for decision making: Problems, methods, and opportunities. 2023arXiv preprint</p>
<p>Confidence-aware imitation learning from demonstrations with varying optimality. Songyuan Zhang, Zhangjie Cao, Dorsa Sadigh, Yanan Sui, Advances in Neural Information Processing Systems. M Ranzato, A Beygelzimer, Y Dauphin, P S Liang, J Wortman Vaughan, Curran Associates, Inc202134</p>
<p>Viola: Imitation learning for visionbased manipulation with object proposal priors. Yifeng Zhu, Abhishek Joshi, Peter Stone, Yuke Zhu, arXiv: Arxiv-2210.113392022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
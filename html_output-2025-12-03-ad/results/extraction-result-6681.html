<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6681 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6681</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6681</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-128.html">extraction-schema-128</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-247451124</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2022.emnlp-main.775.pdf" target="_blank">ScienceWorld: Is your Agent Smarter than a 5th Grader?</a></p>
                <p><strong>Paper Abstract:</strong> We present ScienceWorld, a benchmark to test agents’ scientific reasoning abilities in a new interactive text environment at the level of a standard elementary school science curriculum. Despite the transformer-based progress seen in question-answering and scientific text processing, we find that current models cannot reason about or explain learned science concepts in novel contexts. For instance, models can easily answer what the conductivity of a known material is but struggle when asked how they would conduct an experiment in a grounded environment to find the conductivity of an unknown material. This begs the question of whether current models are simply retrieving answers by way of seeing a large number of similar examples or if they have learned to reason about concepts in a reusable manner. We hypothesize that agents need to be grounded in interactive environments to achieve such reasoning capabilities. Our experiments provide empirical evidence supporting this hypothesis – showing that a 1.5 million parameter agent trained interactively for 100k steps outperforms a 11 billion parameter model statically trained for scientific question-answering and reasoning from millions of expert demonstrations.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6681.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6681.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CALM-GPT2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CALM (GPT-2 fine-tuned action generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-2 based action-generation agent fine-tuned on oracle action-sequence transcripts to propose candidate text commands; its outputs are re-ranked by an RL policy (DRRN-like) to choose executable actions in the SCIENCEWORLD text environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>SCIENCEWORLD: Is your Agent Smarter than a 5 th Grader?</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CALM-GPT2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A 12-layer GPT-2 model fine-tuned on oracle demonstration transcripts to generate candidate next actions (beam search), provided with current observation, previous observation, and previous action; beam outputs are filtered to valid actions and then re-ranked/selected by an RL policy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (12-layer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>131M (full GPT-2; 6.9M policy params updated during RL)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SCIENCEWORLD (text-based science environment)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short context window (current + last observation + last action in prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw observation text and previous-action tokens included in the model context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>no explicit external memory — context is overwritten each step by providing current/previous observations and last action (prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>prompt-based retrieval (information is available only if present in the current context passed to the model)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised fine-tuning on oracle action sequences (imitation), used with RL re-ranking during execution</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>normalized task score (per-task score normalized 0–1), averaged across tasks; episode rewards reported during training</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>0.05 average normalized score across tasks (reported CALM overall average)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>No ablations on memory (context window) reported for CALM; key failure mode is generation of invalid or non-useful actions despite large demonstration data.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>High computation cost; frequently generates invalid actions leading to poor effective performance; no explicit persistent memory beyond short prompt context, limiting use of longer-horizon information.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Use of a valid-action detection aid at test time is important (authors used filtering of generated beams to valid actions); combining LM generation with an RL re-ranker can mitigate invalid generation but did not fully solve it.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6681.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6681.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Behavior Cloning (T5/Macaw)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavior Cloning via T5 (initialized with Macaw weights)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-to-text transformer (T5) fine-tuned with behavior cloning on oracle action trajectories to directly map observations (and previous context) to the next action in SCIENCEWORLD.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>SCIENCEWORLD: Is your Agent Smarter than a 5 th Grader?</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Behavior Cloning (T5 / Macaw)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>T5-based sequence-to-sequence policy trained on (task description, previous obs, previous action, current obs) -> next action pairs extracted from oracle transcripts; at inference uses beam search to generate candidate actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (initialized with Macaw weights)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M (Macaw-Large / T5-Large) and also evaluated at 11B (Macaw-11B)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SCIENCEWORLD</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>transformer context window / short prompt context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw observation text and previous action tokens provided in the input sequence</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>no explicit persistent memory; the model ingests the last and current observations (sliding context) at each step</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>prompt-based attention over tokens in the input context</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised imitation learning (behavioral cloning) on oracle demonstration transcripts</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>average normalized task score (0–1) on unseen test variations; per-task averages reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Behavior Cloned Macaw-Large: 0.17 average normalized score; T5-Large: 0.15; Macaw-11B: 0.08</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Authors report inverse scaling with model size for BC/TDT variants (larger 11B models performed worse than 770M models) but provide no ablation specifically on context-length or memory mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Tendency to generate few valid actions in candidate lists (necessitating valid-action detection aid); no explicit long-term memory beyond immediate context, limiting multi-step procedural reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Pre-training on domain-specific scientific QA (Macaw) helps more than generic pretraining; use of valid-action filtering and beam diversity penalties are practical aids.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6681.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6681.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text Decision Transformer (TDT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text Decision Transformer (trajectory modeling with returns-to-go)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer agent that models POMDP trajectories as sequences including returns-to-go, observations, and actions, trained from offline expert trajectories to predict actions that maximize future expected reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>SCIENCEWORLD: Is your Agent Smarter than a 5 th Grader?</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Text Decision Transformer (TDT-T5/Macaw)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A transformer model trained on sequences of (task description, previous obs, returns-to-go, previous action, current obs, returns) to predict the next action, enabling the model to consider long-horizon return signals when selecting actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (initialized with Macaw weights)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M (Macaw-Large / T5-Large) and 11B (Macaw-11B) variants evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SCIENCEWORLD</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-context window / trajectory context used as implicit memory (returns-to-go appended)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>action-observation-return tuples (sequence of past timesteps and returns-to-go tokens) represented as transformer input tokens</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>no explicit external memory updates — agent conditions on the trajectory provided in the transformer context (sliding/finite context window)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>self-attention over the transformer context (attention to past tokens and returns-to-go)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>offline imitation learning using trajectories with returns-to-go (Decision Transformer paradigm)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>average normalized task score (0–1) across tasks; per-model averages reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>TDT Macaw-Large: 0.15 average normalized score; T5-Large: 0.13; Macaw-11B: 0.08</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>No direct ablations on context length or memory retrieval strategy reported; authors note TDT follows Decision Transformer setup but do not evaluate explicit memory variants.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Performance limited by generation of invalid actions; larger model sizes did not necessarily improve performance (inverse scaling observed), and context-window-only approach may not capture longer episodic information beyond the transformer history.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Modeling returns-to-go and trajectories can help action selection for long-horizon tasks, but practical gains depend on model size/pretraining and valid-action handling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6681.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6681.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Graph A2C (KG-A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An actor-critic RL agent that represents the observed environment state as a dynamically-built symbolic knowledge graph (triples) extracted from text observations, and selects actions by grounding action templates to objects from this graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>SCIENCEWORLD: Is your Agent Smarter than a 5 th Grader?</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A policy-gradient (A2C) agent whose state representation is a dynamically maintained knowledge graph constructed from parsed observation text; action templates are filled with object types and grounded against this graph to produce concrete actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom RL (graph-based state encoder) — not an LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>5.5M parameters (policy network)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SCIENCEWORLD</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dynamic symbolic knowledge graph (episodic/working state memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>subject-relation-object triples (heuristically extracted from look and inventory text) representing objects, relations, and environment state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>rule-based / heuristic extractor run each step to parse text observations into graph triples (replaces OpenIE extractor with regex-based heuristic to obtain ground-truth-like graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>graph lookup and grounding: agent queries the knowledge graph to select object types and then grounds to visible object referents (random choice if multiple referents)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>on-policy reinforcement learning (A2C) with the graph-based state representation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>average normalized task score (0–1); per-task scores reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>0.11 average normalized score across tasks (KG-A2C overall)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Authors replaced OpenIE extractor with a heuristic extractor (functionally extracting near-ground-truth graphs); no direct ablation varying graph size/horizon reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Graph extractor relies on heuristics tuned to SCIENCEWORLD text (less general); grounding from object types to referents includes randomness when multiple referents exist; agent still benefits from valid-action detection aid.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Using a structured symbolic memory (knowledge graph) derived from observations improves sample efficiency and performance on long action-sequence tasks compared to purely generative LMs; constructing accurate extractors for the target environment is critical.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6681.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6681.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Relevance Network (DRRN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement-learning agent that learns separate representations for state (text observation) and candidate actions, selecting the action most relevant to the observation; uses a prioritized replay buffer during training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>SCIENCEWORLD: Is your Agent Smarter than a 5 th Grader?</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A relevance-network RL agent embedding observations and candidate actions separately and learning a Q-value for each action; at training time it uses an experience replay buffer with prioritized sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom DRRN architecture (not an LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SCIENCEWORLD</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>experience replay buffer (off-policy memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>stored experience tuples (observation, action, reward, next observation, done)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>FIFO / capacity-limited buffer with prioritized fraction — explicit memory size set to 100k and priority fraction 0.50</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>prioritized sampling from replay buffer during training (priority fraction indicates fraction of prioritized samples)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning with replay (off-policy), trained online interactively in environment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>average normalized task score (0–1), episode reward curves across steps</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>0.17 average normalized score across tasks (best-performing agent in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Authors report DRRN's hyperparameters include memory size=100k and priority fraction=0.50, but no direct ablation varying memory size or priority fraction is reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Relies on valid-action detection aid at test time like most agents evaluated; memory is an experience buffer for learning, not a structured episodic memory for planning at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Interactive online training with replay (as used by DRRN) can be more sample- and parameter-efficient than large offline LLMs for long-horizon procedural tasks; prioritized replay with adequate capacity is used in practice.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CALM <em>(Rating: 2)</em></li>
                <li>Decision Transformer <em>(Rating: 2)</em></li>
                <li>Deep Reinforcement Relevance Network <em>(Rating: 2)</em></li>
                <li>KG-A2C <em>(Rating: 2)</em></li>
                <li>Behavior Cloning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6681",
    "paper_id": "paper-247451124",
    "extraction_schema_id": "extraction-schema-128",
    "extracted_data": [
        {
            "name_short": "CALM-GPT2",
            "name_full": "CALM (GPT-2 fine-tuned action generator)",
            "brief_description": "A GPT-2 based action-generation agent fine-tuned on oracle action-sequence transcripts to propose candidate text commands; its outputs are re-ranked by an RL policy (DRRN-like) to choose executable actions in the SCIENCEWORLD text environment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "SCIENCEWORLD: Is your Agent Smarter than a 5 th Grader?",
            "agent_name": "CALM-GPT2",
            "agent_description": "A 12-layer GPT-2 model fine-tuned on oracle demonstration transcripts to generate candidate next actions (beam search), provided with current observation, previous observation, and previous action; beam outputs are filtered to valid actions and then re-ranked/selected by an RL policy.",
            "model_name": "GPT-2 (12-layer)",
            "model_size": "131M (full GPT-2; 6.9M policy params updated during RL)",
            "benchmark_name": "SCIENCEWORLD (text-based science environment)",
            "memory_used": false,
            "memory_type": "short context window (current + last observation + last action in prompt)",
            "memory_representation": "raw observation text and previous-action tokens included in the model context",
            "memory_update_mechanism": "no explicit external memory — context is overwritten each step by providing current/previous observations and last action (prompting)",
            "memory_retrieval_method": "prompt-based retrieval (information is available only if present in the current context passed to the model)",
            "training_method": "supervised fine-tuning on oracle action sequences (imitation), used with RL re-ranking during execution",
            "evaluation_metric": "normalized task score (per-task score normalized 0–1), averaged across tasks; episode rewards reported during training",
            "performance_with_memory": "0.05 average normalized score across tasks (reported CALM overall average)",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": "No ablations on memory (context window) reported for CALM; key failure mode is generation of invalid or non-useful actions despite large demonstration data.",
            "reported_limitations": "High computation cost; frequently generates invalid actions leading to poor effective performance; no explicit persistent memory beyond short prompt context, limiting use of longer-horizon information.",
            "best_practices_recommendations": "Use of a valid-action detection aid at test time is important (authors used filtering of generated beams to valid actions); combining LM generation with an RL re-ranker can mitigate invalid generation but did not fully solve it.",
            "uuid": "e6681.0"
        },
        {
            "name_short": "Behavior Cloning (T5/Macaw)",
            "name_full": "Behavior Cloning via T5 (initialized with Macaw weights)",
            "brief_description": "A text-to-text transformer (T5) fine-tuned with behavior cloning on oracle action trajectories to directly map observations (and previous context) to the next action in SCIENCEWORLD.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "SCIENCEWORLD: Is your Agent Smarter than a 5 th Grader?",
            "agent_name": "Behavior Cloning (T5 / Macaw)",
            "agent_description": "T5-based sequence-to-sequence policy trained on (task description, previous obs, previous action, current obs) -&gt; next action pairs extracted from oracle transcripts; at inference uses beam search to generate candidate actions.",
            "model_name": "T5 (initialized with Macaw weights)",
            "model_size": "770M (Macaw-Large / T5-Large) and also evaluated at 11B (Macaw-11B)",
            "benchmark_name": "SCIENCEWORLD",
            "memory_used": false,
            "memory_type": "transformer context window / short prompt context",
            "memory_representation": "raw observation text and previous action tokens provided in the input sequence",
            "memory_update_mechanism": "no explicit persistent memory; the model ingests the last and current observations (sliding context) at each step",
            "memory_retrieval_method": "prompt-based attention over tokens in the input context",
            "training_method": "supervised imitation learning (behavioral cloning) on oracle demonstration transcripts",
            "evaluation_metric": "average normalized task score (0–1) on unseen test variations; per-task averages reported",
            "performance_with_memory": "Behavior Cloned Macaw-Large: 0.17 average normalized score; T5-Large: 0.15; Macaw-11B: 0.08",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": "Authors report inverse scaling with model size for BC/TDT variants (larger 11B models performed worse than 770M models) but provide no ablation specifically on context-length or memory mechanisms.",
            "reported_limitations": "Tendency to generate few valid actions in candidate lists (necessitating valid-action detection aid); no explicit long-term memory beyond immediate context, limiting multi-step procedural reasoning.",
            "best_practices_recommendations": "Pre-training on domain-specific scientific QA (Macaw) helps more than generic pretraining; use of valid-action filtering and beam diversity penalties are practical aids.",
            "uuid": "e6681.1"
        },
        {
            "name_short": "Text Decision Transformer (TDT)",
            "name_full": "Text Decision Transformer (trajectory modeling with returns-to-go)",
            "brief_description": "A transformer agent that models POMDP trajectories as sequences including returns-to-go, observations, and actions, trained from offline expert trajectories to predict actions that maximize future expected reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "SCIENCEWORLD: Is your Agent Smarter than a 5 th Grader?",
            "agent_name": "Text Decision Transformer (TDT-T5/Macaw)",
            "agent_description": "A transformer model trained on sequences of (task description, previous obs, returns-to-go, previous action, current obs, returns) to predict the next action, enabling the model to consider long-horizon return signals when selecting actions.",
            "model_name": "T5 (initialized with Macaw weights)",
            "model_size": "770M (Macaw-Large / T5-Large) and 11B (Macaw-11B) variants evaluated",
            "benchmark_name": "SCIENCEWORLD",
            "memory_used": false,
            "memory_type": "long-context window / trajectory context used as implicit memory (returns-to-go appended)",
            "memory_representation": "action-observation-return tuples (sequence of past timesteps and returns-to-go tokens) represented as transformer input tokens",
            "memory_update_mechanism": "no explicit external memory updates — agent conditions on the trajectory provided in the transformer context (sliding/finite context window)",
            "memory_retrieval_method": "self-attention over the transformer context (attention to past tokens and returns-to-go)",
            "training_method": "offline imitation learning using trajectories with returns-to-go (Decision Transformer paradigm)",
            "evaluation_metric": "average normalized task score (0–1) across tasks; per-model averages reported",
            "performance_with_memory": "TDT Macaw-Large: 0.15 average normalized score; T5-Large: 0.13; Macaw-11B: 0.08",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": "No direct ablations on context length or memory retrieval strategy reported; authors note TDT follows Decision Transformer setup but do not evaluate explicit memory variants.",
            "reported_limitations": "Performance limited by generation of invalid actions; larger model sizes did not necessarily improve performance (inverse scaling observed), and context-window-only approach may not capture longer episodic information beyond the transformer history.",
            "best_practices_recommendations": "Modeling returns-to-go and trajectories can help action selection for long-horizon tasks, but practical gains depend on model size/pretraining and valid-action handling.",
            "uuid": "e6681.2"
        },
        {
            "name_short": "KG-A2C",
            "name_full": "Knowledge-Graph A2C (KG-A2C)",
            "brief_description": "An actor-critic RL agent that represents the observed environment state as a dynamically-built symbolic knowledge graph (triples) extracted from text observations, and selects actions by grounding action templates to objects from this graph.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "SCIENCEWORLD: Is your Agent Smarter than a 5 th Grader?",
            "agent_name": "KG-A2C",
            "agent_description": "A policy-gradient (A2C) agent whose state representation is a dynamically maintained knowledge graph constructed from parsed observation text; action templates are filled with object types and grounded against this graph to produce concrete actions.",
            "model_name": "Custom RL (graph-based state encoder) — not an LLM",
            "model_size": "5.5M parameters (policy network)",
            "benchmark_name": "SCIENCEWORLD",
            "memory_used": true,
            "memory_type": "dynamic symbolic knowledge graph (episodic/working state memory)",
            "memory_representation": "subject-relation-object triples (heuristically extracted from look and inventory text) representing objects, relations, and environment state",
            "memory_update_mechanism": "rule-based / heuristic extractor run each step to parse text observations into graph triples (replaces OpenIE extractor with regex-based heuristic to obtain ground-truth-like graphs)",
            "memory_retrieval_method": "graph lookup and grounding: agent queries the knowledge graph to select object types and then grounds to visible object referents (random choice if multiple referents)",
            "training_method": "on-policy reinforcement learning (A2C) with the graph-based state representation",
            "evaluation_metric": "average normalized task score (0–1); per-task scores reported",
            "performance_with_memory": "0.11 average normalized score across tasks (KG-A2C overall)",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": "Authors replaced OpenIE extractor with a heuristic extractor (functionally extracting near-ground-truth graphs); no direct ablation varying graph size/horizon reported.",
            "reported_limitations": "Graph extractor relies on heuristics tuned to SCIENCEWORLD text (less general); grounding from object types to referents includes randomness when multiple referents exist; agent still benefits from valid-action detection aid.",
            "best_practices_recommendations": "Using a structured symbolic memory (knowledge graph) derived from observations improves sample efficiency and performance on long action-sequence tasks compared to purely generative LMs; constructing accurate extractors for the target environment is critical.",
            "uuid": "e6681.3"
        },
        {
            "name_short": "DRRN",
            "name_full": "Deep Reinforcement Relevance Network (DRRN)",
            "brief_description": "A reinforcement-learning agent that learns separate representations for state (text observation) and candidate actions, selecting the action most relevant to the observation; uses a prioritized replay buffer during training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "SCIENCEWORLD: Is your Agent Smarter than a 5 th Grader?",
            "agent_name": "DRRN",
            "agent_description": "A relevance-network RL agent embedding observations and candidate actions separately and learning a Q-value for each action; at training time it uses an experience replay buffer with prioritized sampling.",
            "model_name": "Custom DRRN architecture (not an LLM)",
            "model_size": "1.5M parameters",
            "benchmark_name": "SCIENCEWORLD",
            "memory_used": true,
            "memory_type": "experience replay buffer (off-policy memory)",
            "memory_representation": "stored experience tuples (observation, action, reward, next observation, done)",
            "memory_update_mechanism": "FIFO / capacity-limited buffer with prioritized fraction — explicit memory size set to 100k and priority fraction 0.50",
            "memory_retrieval_method": "prioritized sampling from replay buffer during training (priority fraction indicates fraction of prioritized samples)",
            "training_method": "reinforcement learning with replay (off-policy), trained online interactively in environment",
            "evaluation_metric": "average normalized task score (0–1), episode reward curves across steps",
            "performance_with_memory": "0.17 average normalized score across tasks (best-performing agent in this work)",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": "Authors report DRRN's hyperparameters include memory size=100k and priority fraction=0.50, but no direct ablation varying memory size or priority fraction is reported in the paper.",
            "reported_limitations": "Relies on valid-action detection aid at test time like most agents evaluated; memory is an experience buffer for learning, not a structured episodic memory for planning at inference.",
            "best_practices_recommendations": "Interactive online training with replay (as used by DRRN) can be more sample- and parameter-efficient than large offline LLMs for long-horizon procedural tasks; prioritized replay with adequate capacity is used in practice.",
            "uuid": "e6681.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CALM",
            "rating": 2
        },
        {
            "paper_title": "Decision Transformer",
            "rating": 2
        },
        {
            "paper_title": "Deep Reinforcement Relevance Network",
            "rating": 2
        },
        {
            "paper_title": "KG-A2C",
            "rating": 2
        },
        {
            "paper_title": "Behavior Cloning",
            "rating": 1
        }
    ],
    "cost": 0.015393249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SCIENCEWORLD: Is your Agent Smarter than a 5 th Grader?
11298 December 7-11, 2022</p>
<p>Ruoyao Wang ruoyaowang@arizona.edu 
Peter Jansen pajansen@arizona.edu 
Marc-Alexandre Côté 
Microsoft Research Montréal ♢ Allen Institute for AI
SeattleWA</p>
<p>Prithviraj Ammanabrolu </p>
<p>University of Arizona
TucsonAZ</p>
<p>SCIENCEWORLD: Is your Agent Smarter than a 5 th Grader?</p>
<p>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing
the 2022 Conference on Empirical Methods in Natural Language Processing1127911298 December 7-11, 2022
We present SCIENCEWORLD, a benchmark to test agents' scientific reasoning abilities in a new interactive text environment at the level of a standard elementary school science curriculum. Despite the transformer-based progress seen in question-answering and scientific text processing, we find that current models cannot reason about or explain learned science concepts in novel contexts. For instance, models can easily answer what the conductivity of a known material is but struggle when asked how they would conduct an experiment in a grounded environment to find the conductivity of an unknown material. This begs the question of whether current models are simply retrieving answers by way of seeing a large number of similar examples or if they have learned to reason about concepts in a reusable manner. We hypothesize that agents need to be grounded in interactive environments to achieve such reasoning capabilities. Our experiments provide empirical evidence supporting this hypothesisshowing that a 1.5 million parameter agent trained interactively for 100k steps outperforms a 11 billion parameter model statically trained for scientific question-answering and reasoning from millions of expert demonstrations. 12</p>
<p>Introduction</p>
<p>Question answering (QA) has seen rapid progress recently. Standardized elementary and middle school science exams have served as a challenge task for QA (Clark et al., 2018), as these questions require combining science-domain knowledge with world knowledge in complex reasoning procedures to solve. As large language models have toppled these benchmarks (Clark et al., 2020;Khashabi et al., 2020;Xu et al., 2021a), the focus has shifted away from simply answering questions toward producing human-readable explanations for a model's 1 Website: https://sciworld.apps.allenai.org 2 Code: https://github.com/allenai/ScienceWorld SCIENCEWORLD Task 3 (Test Electrical Conductivity Subtask)</p>
<p>Task Description</p>
<p>Your task is to determine if a metal fork is electrically conductive. The metal fork is located around the kitchen. First, focus on the metal fork. If conductive, place it in the blue box. If nonconductive, place it in the green box.</p>
<p>Agent starts in a random location, navigates to kitchen</p>
<p>Kitchen</p>
<p>This room is called the kitchen. In it, you see: a fridge. The freezer door is closed. a cupboard. The cupboard door is closed. a table. On the table is: a glass cup (containing nothing). a stove, which is turned o . On the stove is: nothing. a sink, which is turned o . In the sink is: nothing. a thermometer, currently reading a temperature of 10 degrees celsius a metal fork a counter. On the counter is: a drawer, a bowl (containing an orange, a banana, a potato, a red apple).</p>
<p>You also see: A door to the outside (that is open) A door to the bathroom (that is open) A door to the hallway (that is open)</p>
<p>Action: focus on metal fork You focus on the metal fork.</p>
<p>Action: pick up metal fork</p>
<p>You move the metal fork to the inventory.</p>
<p>Agent navigates to room with electrical components</p>
<p>Workshop</p>
<p>This room is called the workshop. In it, you see: a electric buzzer, which is o a electric motor, which is o a solar panel, which is o a table. On the table is: a blue wire, a battery, a red light bulb, which is o , a black wire, a switch, which is o , a orange wire. a blue answer box a green answer box You also see: A door to the hallway (that is open) Action: connect battery cathode to orange wire terminal 1 cathode on battery is now connected to terminal 1 on orange wire.</p>
<p>Action: connect metal fork terminal 1 to cathode in red light bulb</p>
<p>terminal 1 on metal fork is now connected to cathode on red light bulb.</p>
<p>Agent continues wiring a circuit containing the metal fork Action: examine red light bulb a red light bulb, which is on.</p>
<p>Action: move metal fork to blue answer box (disconnecting metal fork) You move the metal fork to the blue box.</p>
<p>Task Completed.</p>
<p>In room with task object</p>
<p>In room with components The agent finds the metal fork, use it to build an electrical circuit with a red light bulb, and observe if the light turns on. Note the transcript has been simplified to fit in available space, and large sections have been omitted (greyed sections).</p>
<p>answers (Jansen et al., 2018;Yang et al., 2018;Xie et al., 2020;Valentino et al., 2021;Xu et al., 2021b;Lamm et al., 2021;Aggarwal et al., 2021). While language models are able to produce compelling answers (Zoph et al., 2022) or explanations (Jansen et al., 2021) to science questions, are they simply retrieving (or shallowly assembling) these answers, or can they understand and use the knowledge they output in a meaningful way? Also, how can we evaluate if a model's explanation is correct?</p>
<p>In this work we explore these two questions by reframing science exam question answering into an interactive task where agents must complete elementary science experiments in a simulated textbased environment called SCIENCEWORLD (see Table 1). Instead of simply answering a question (e.g., Q: "What will happen to an ice cube when placed on a stove?", A: "it will melt"), the agent must demonstrate its capacity to combine declarative scientific knowledge with the procedural knowledge required to correctly complete the experiment in the virtual environment. Similarly, the sequence of virtual actions an agent performs can serve as a form of procedural ("how") explanation to the question, that can be directly evaluated in the virtual environment for correctness (e.g., whether the actions led to the ice cube melting).</p>
<p>The contributions of this work are:</p>
<ol>
<li>
<p>We construct SCIENCEWORLD, a complex interactive text environment, with simulation engines for thermodynamics, electrical circuits, chemistry reactions, and biological processes.</p>
</li>
<li>
<p>We implement 30 benchmark tasks across 10 topics spanning the elementary science curriculum, including changes of state of matter and the role of pollinators when growing fruit.</p>
</li>
<li>
<p>We evaluate 5 state-of-the-art reinforcement learning and language model agents on this benchmark, empirically showing that they perform poorly on tasks (e.g., melting ice) that 5 th grade students can perform with ease.</p>
</li>
</ol>
<p>Related Work</p>
<p>Science-domain Inference: Standardized science exams are a challenging task for question answering due to their diverse knowledge and inference requirements (Clark et al., 2013;Jansen et al., 2016;Boratko et al., 2018;Clark et al., 2018). Top performing models can answer more than 90% of multiple-choice questions correctly (Clark et al., 2020), typically through the use of large language models (Khashabi et al., 2020;Zoph et al., 2022). A number of corpora of structured and semistructured science exam explanations exist for training the explanation-generation task (Jansen et al., 2018;Xie et al., 2020;Khot et al., 2020;Dalvi et al., 2021). Evaluating explanations is challenging, and typically done by comparing generated explanations to a single gold explanation. This has been shown to substantially underestimate explanation generation performance by up to 40% (Jansen et al., 2021). While others have worked to mitigate this by generating multiple alternate explanations for each question, this is expensive, and has only been demonstrated for adding one or two additional explanations per question (Inoue et al., 2020;Jhamtani and Clark, 2020). Here, we propose a partial solution to this evaluation problem by treating the action sequences of agents as structured manner explanations for how to solve a task. These action sequences can be directly run in the SCIENCEWORLD simulator to automatically determine their correctness (i.e., whether they accomplish the task), independent of any variations in their solution methods. For example, whether an agent melts ice by using a stove, building a campfire, or leaving the ice out on a kitchen counter, the end result is the same and directly measurable through the formal semantics of the simulator.</p>
<p>Environments: Interactive text environments are becoming a vehicle for research in natural language processing (see Jansen (2021) for a review), primarily because of their reduced development costs relative to 3D environments, combined with their ability to easily implement high-level tasks with large action spaces. In spite of these benefits, implementation costs can still be substantial for large complex environments, and text agents are frequently evaluated on a small set of exist-ing interactive fiction games such as Zork (Lebling et al., 1979) using an unified interface like Jericho (Hausknecht et al., 2020). A few purpose-built environments provide simple tasks for studying text agents, typically on procedurally generated pick-and-place or object-combination tasks (e.g., cooking, Yin and May, 2019). Kitchen Cleanup (Murugesan et al., 2020b) and TextWorld Common Sense (Murugesan et al., 2020a) require agents to tidy up one or more rooms in a house environment by putting objects in their typical locations (e.g., a hat should be placed on the hat rack), testing an agent's declarative knowledge of common object locations with the procedural knowledge required for this pick-and-place task. The closest existing interactive text environment to SCIENCEWORLD is TextLabs (Tamari et al., 2021) which simulates chemistry wet-lab protocols with actions such as pipetting and centrifuging. Compared to these existing environments, SCIENCEWORLD is generally a larger and more dynamic environment, populated with more complex objects with greater depth of physical simulation. This simulation depth enables more complex tasks associated with elementary science (e.g., thermodynamics, electrical circuits, etc.) to be tested, and a greater variety of solutions.</p>
<p>Simulators: Nearly all text-based world simulations are currently implemented as Z-machine games (Infocom, 1989;Nelson, 2014), frequently through higher-level application-specific languages (such as Inform7, Nelson, 2006) that compile to Z-machine code. TextWorld (Côté et al., 2018) creates environments using linear-logic statements that specify action preconditions and postconditions (Martens, 2015) and generate Inform7 code. Existing tooling is designed for simpler simulations than SCIENCEWORLD, with primarily agent-centered state changes that make modeling autonomous physical processes (e.g., thermodynamics) difficult. As such, in this work we build a novel simulator to model physical processes in text environments.</p>
<p>Agents: A variety of agent models have been proposed for reasoning in interactive text environments. Most approaches frame reasoning as a partially-observable Markov decision process (POMDP), and model inference using reinforcement learning (RL). This includes RL-based models that learn a policies to pick relevant actions from lists of candidate actions (He et al., 2016), or models that mix RL with knowledge graphs (Ammanabrolu and Hausknecht, 2020) or language models (Yao et al., 2020) to aid in next-action selection. Action selection has also been modelled using case-based reasoning (Atzeni et al., 2021), or directly as a sequence-prediction imitation learning task, using language models trained on gold action sequences to predict the next action given the current state (Torabi et al., 2018;Ammanabrolu et al., 2021Ammanabrolu et al., , 2022. In general, agent performance on solving interactive fictions is still modest, with only easier games close to completion (Jansen, 2021).</p>
<p>In this work, we benchmark state-of-the-art agents on SCIENCEWORLD as well as introduce novel agents. We empirically show that these elementary science tasks are difficult for current agents, and also that smaller and simpler agents can outperform billion-scale parameter language models trained on gold sequences, highlighting the difficulty of this task for transformer-based models.</p>
<p>SCIENCEWORLD</p>
<p>SCIENCEWORLD is a simulation of the world abstracted through a complex interactive text environment in English with many objects, actions, and simulation engines. The framework consists of 40k lines of SCALA (speed) with a PYTHON interface.</p>
<p>The SCIENCEWORLD environment contains 10 interconnected locations (see Figure 1), populated with up to 200 types of objects, including devices, instruments, plants/animals, electrical components, substances, containers, and common environment objects such as furniture, books, and paintings. The SCIENCEWORLD action space contains 25 high-level actions, including both science-domain actions (e.g., using thermometer) and common actions (e.g., moving, opening containers, picking up items), with approximately 200k possible action-object combinations per step (though only a limited subset of these will be meaningful). See Appendix A for details about SCIENCEWORLD, including the object model, actions, and input parser.</p>
<p>Simulation Engines</p>
<p>SCIENCEWORLD supports actions commonly found in interactive text environments -for example, objects can be moved or examined, foods can be eaten, and books can be read. In addition, the environment contains a number of elementary science-domain specific processes that either occur automatically (e.g., thermodynamics) or are coupled to actions (e.g., devices, mixing chemicals). Those simulation engines 3 are:</p>
<p>Thermodynamics: All objects have temperatures and other thermal properties based on their materials. All objects within a container are considered in thermal contact with each other, and transfer heat energy using a simplified conductive heat model. The proportion of heat transferred between objects at each step is mediated by the object's thermal conduction coefficient, allowing thermal conductors (like metal pots) and insulators (like ceramics) to be modelled. Every material has phase transition points (i.e., melting point, boiling point) and combustion points populated based on the best-known or approximate physical values for those materials.</p>
<p>Objects that move past these thresholds will change state of matter (e.g., from a solid to a liquid), or begin a combustion process that ultimately ends in the object turning to ash unless its fire is put out. Convective heat transfer is also modelled in the form of heat sources (e.g., oven, stove) and heat sinks (e.g., fridge, freezer) that transfer heat energy to or from objects. Rooms also transfer ambient heat energy to/from the objects they contain.</p>
<p>Electricity: The simulator models simple series electrical circuits, where electrically-powered devices (e.g., light bulb, motor) can be powered by being connected to electrical energy sources (e.g., battery, solar panel) through electrical conductors (nominally, wires). Polarized and unpolarized components are modelled, with each object having exactly two terminals (anode and cathode for polarized; terminals 1 and 2 for unpolarized). Connection happens through explicit terminal-to-terminal connection actions (e.g., connect battery anode to blue wire terminal 1). Every non-electrical object in SCIENCEWORLD has virtual unpolarized terminals, allowing circuits to be build with valid electrical conductors (e.g., using a metal fork in place of a wire), and for the agent to build circuits that test conductivity by (for example) observing if a light bulb illuminates when a plastic versus metal fork is used in the circuit.</p>
<p>Devices: Many objects are also considered devices, that can be activated or deactivated by the agent (e.g., stove, electrical switch), or may have environment-specific conditions to being activated (e.g., a light bulb will only activate if it is properly electrically connected; a solar panel will only produce power if it is outside). Objects can also be used with other objects in specific contexts (e.g., a thermometer, to measure an object's temperature; a shovel, to dig soil from the ground).</p>
<p>Chemistry: A subset of specific chemical reactions are modelled, where mixing a set of substances together in a container will produce a resultant substance (e.g., salt and water mix to produce salt water). Friction (Inclined Plane): Forces are a significant part of an elementary science curriculum, but difficult to incorporate without 2D or 3D simulation. SCIENCEWORLD models the forces of gravity and friction in the specific context of 1-dimensional inclined plane experiments. Objects placed at the top of an inclined plane will slide down the plane at a speed proportional to the plane's angle, and the friction coefficient of its surface material. The position is described to the agent (e.g., "an inclined plant with a block 60% of the way down the plane"), allowing experiments to determine either the relative angle or friction coefficients of different inclined planes based on the speed the object moves down a given plane.</p>
<p>Containers: Containers can be always open (e.g., a metal pot) or closeable (e.g., a cupboard). Objects contained inside containers are not visible until the container is open. Some effects spread beyond a container -for example, a wooden cupboard with a hot object inside may combust, causing other objects in the kitchen to also increase temperature.</p>
<p>Experiments</p>
<p>To understand how contemporary approaches to text agents perform at SCIENCEWORLD tasks, we benchmark a selection of recent architectures.</p>
<p>Tasks. To support our goal of generating a diverse set of tasks, we identified a candidate set of 10 broad science exam topics from the list of 400 fine-grained science curriculum topics of Xu et al. (2020). Topics were chosen that would be amenable to text-based simulation, and that did not have critical fine-grained spatial reasoning requirements, and include: changes of state, temperature measurement, electrical circuits, friction, object classification, chemical mixtures, plants and pollinators, life spans, life stages, and Mendelian genetics. Each topic was further divided into between 2 and 4 specific tasks for agents to perform, producing a total of 30 science-domain tasks. These topics and tasks are described in Appendix B.2.</p>
<p>To prevent overfitting and encourage generalization, each subtask contains between 10 and 1400 parametric variations (with 7200 total variations across all 30 subtasks). Variations change critical task objects (e.g., the specific substance to be melted), the agent's starting location in the environment, as well as randomly vary the contents of the environment itself (e.g., whether the living room contains a bookshelf, or a painting, or both).</p>
<p>Train, Development, Test sets: For a given subtask, variations are split into 50% training, 25% development, and 25% test sets. Variations are sorted such that critical unseen variations (e.g., substances, animals, or plants unseen during training) are found in development and test sets.</p>
<p>Goals and Rewards. To reduce reward sparsity, each task includes between 2 and 15 optional subgoals (such as turning on the stove, or the substance increasing in temperature by 10C) that help nudge agents in the direction of canonical solutions, if desired. Meeting required and optional subgoals increases the agent's score on a given subtask. Scores for all tasks are normalized to between 0 and 1.</p>
<p>Oracle Agents To support imitation learning, we provide gold trajectories from 30 hand-coded oracles on all subtasks and variations. For tractability these solutions represent canonical solution methods (e.g., using a stove to boil water), rather than all possible solution methods that lead to the goal state (e.g., building a campfire to boil water). Learning Agents. An interactive text environment can be cast as a partially observable Markov decision process (POMDP) defined by ⟨S, T, A, R, O, Ω, γ⟩ representing the set of possible states (S), conditional transition probabilities between states (T ), available text commands (A), reward function (R ∶ S × A → R), set of possible text observations (O), observation conditional probabilities (Ω ∶ S → O), and discount factor (γ ∈ [0, 1]). The goal for a learning agent is then to learn a policy π θ (o) → a that chooses or generates a text command a ∈ A given the text observation o ∈ Ω(s) of state s ∈ S that maximizes the expected discounted sum of rewards E[∑ t γ t R(s t , a t )]. In SCIENCEWORLD, the agent is also provided with a task description d.</p>
<p>To provide a fair comparison, all models were run using identical experiment configurations when possible. Reinforcement learning models were run with identical training regiments (8 environment threads at 100k steps per thread). Training episodes reset after meeting an end state (success or failure), or after reaching a maximum number of steps (we used 100 in all experiments). Additional model details are provided in Appendix C. Random Baseline: At each time step t, this baseline randomly chooses an action a from the set of valid actions A t obtained from the simulator. DRRN (He et al., 2016): The Deep Reinforcement Relevance Network (DRRN) learns separate representations of the observation space and action space of an environment, then trains a policy that selects from A t the action that is the most relevant given the current text observation o t (which also includes the description of the current room o look t and the current agent inventory o inv t ) and that would lead to an increased reward. The DRRN is a strong baseline with near state-of-the-art performance on many medium-to-hard interactive text environments (Hausknecht et al., 2020). KG-A2C (Ammanabrolu and Hausknecht, 2020): This model represents the state space with a knowledge graph built dynamically from the text observations o t using OpenIE triples (Angeli et al., 2015) such as (glass bottle, contains, water), while the action space is represented using action templates with placeholders (e.g., open OBJ) obtained from SCIENCEWORLD. The model learns a policy that selects relevant action templates then populates them with objects from the knowledge graph.</p>
<p>CALM (GPT2) (Yao et al., 2020): We collect transcripts of expert demonstrations for the train variations of the tasks using the oracle agents, then use them to fine-tune a pre-trained language model (GPT-2, Radford et al. (2019)). At runtime, the language model is provided with the current observation o t , last observation o t−1 , and last action a t−1 , then generates a shortlist of 30 possible actions to take. This shortlist serves as input to an RL model similar to the DRRN, which re-ranks the actions and chooses the next action to perform. At test time, the agent performs zero-shot inference online in the simulator by generating a fixed number of actions with beam search on the unseen test variations for each task. Despite training on a large number of demonstrations, the generated actions are often invalid or not useful-resulting in zero scores. Thus, we treat the beam search's output as a ranked-list and run the highest-ranked action appearing in A t , similar to the language-modelto-valid-action aligner of Huang et al. (2022).</p>
<p>Text Decision Transformer: Inspired by the Decision Transformer (Chen et al., 2021), we create a novel text game agent that models the entire POMDP trajectory as a sequence and has the ability to potentially predict actions that maximize future long term expected reward. We again used the same transcripts of demonstrations as the two previous agents to extract 224,902 training examples with (d, o t−1 ,R t−1 , a t−1 , o t ,R t ) as inputs and a t as targets. HereR is the returns-to-go (i.e., sum of future rewards)R = ∑ T t ′ =t r t ′ where r t ′ is the future reward obtained by the expert at step t ′enabling models to predict actions that maximize future expected rewards. The architecture, pretraining, parameter sizes, and test inference are otherwise similar to the behavior cloning agent. Both the Behavior Cloning and Text Decision Transformer agents learn to perform SCIENCE-WORLD tasks offline from demonstrations once pre-trained for scientific QA. They use the prevailing paradigm for achieving state-of-the-art on many language benchmarks (e.g., QA (Khashabi et al., 2020;Tafjord and Clark, 2021), language understanding (Raffel et al., 2020;Brown et al., 2020)).</p>
<p>Results. Performance for all agents across each SCIENCEWORLD task is shown in Table 2. Overall, these tasks are challenging for current models, with the best model (DRRN) achieving an average score of 0.17 across all 30 subtasks. Models that rely on the valid action detection aid generally perform better than those that learn to generate valid actions in addition to learning what actions to pick to increase task performance. All models relying on large language model for action selection (CALM, BC-T5, TDT-T5) generally achieve low performance as they tend to generate few valid actions in their candidate action lists. Figure 2 shows episode reward curves for four selected tasks (with reward curves for all tasks included in Appendix C). These four tasks include the current best-performing task (Task 4-2: Find a non-living thing), which requires an agent to focus on any non-living thing in the environment, pick it up, and place it in a specific container (typically in a different location than the agent's starting location). Most RL models quickly solve the majority of this task, but struggle with picking up and moving the object to the final container. In contrast, other more open-ended tasks (such as Task 1-4, which requires agents to perform any state-of-matter change to a specific substance) are performed poorly by all models. Finally, SCIENCEWORLD includes pairs of identical tasks where one can be solved by retrieving some critical component of the answer, while the other requires conducting the experimental procedure successfully. For example, in Task 3-3, an agent could look up that a metal fork is an electrical conductor and solve the task with comparatively fewer steps then in its paired Task 3-4, where the substance name is randomly generated (e.g., unknown substance B) and the experiment must be completed to get the answer. We do not yet  averaged over 5 independent random seeds. Results across seeds tend to have low variance, with 80% of standard deviations below 0.05, and 95% of standard deviations below 0.10. Performance for RL agents is averaged over the last 10% of evaluation episodes, while T5 performance represents average task score across all test variations of a task. * signifies that the value of 131M parameters includes the number of the parameters of the pre-trained GPT-2 action generator model. Only 6.9 million policy parameters are updated in RL training. (right) Performance on two variations of a task where the agent must determine whether a specific substance is electrically conductive or not. In one variation (center-right), the substance is named (e.g. metal fork), while in the other variation (right) the substance is randomly generated (e.g. unknown substance B).</p>
<p>observe this behavior with the agents under examination. They generally struggle with commonsense level tasks (e.g., navigation) and are unable to reach a point where the language models (either GPT-2 in CALM, or T5 initialized with Macaw in BC and TDT) are able to leverage their internal knowledge to solve these tasks through retrieval.</p>
<p>Discussion</p>
<p>Elementary science tasks are challenging for text agents. With top-performing agents reaching normalized average scores of 0.17 across tasks, performance on SCIENCEWORLD is comparable to the current best-performing agents on mediumdifficulty interactive fiction games such as Zork (Ammanabrolu et al., 2020;Yao et al., 2021). Much as in interactive fiction games, examining agent trajectories reveals that while agents appear to struggle with science-domain inference procedures such as how to heat a substance or how to grow a seed, they also currently lack a fluency with commonsense skills such as navigating the environment or storing liquids in containers. This underscores the need for models that can incorporate commonsense and science-domain knowledge, and integrate that declarative knowledge into actionable procedures to progress towards goals in the environment.</p>
<p>Larger models are not necessarily better. While larger models generally perform better in question answering tasks (e.g., Raffel et al., 2020), here we observe that larger models do not always increase performance. Our best-performing model, the DRRN, has only 1.5 million parameters -four orders of magnitude less than the T5 models. Both models also receive the same number of gradient updates (10 6 ) with respect to SCIENCEWORLD training tasks-though the T5 models have the added benefit of pre-training both from science exam QA and a large number of expert demonstrations. 4 This underscores that how a model approaches modeling state spaces and action sequences may be more important than the scope of its pre-training. Online, interactive training enables models such as the DRRN and KG-A2C to perform tasks requiring long action sequences more efficiently in terms of both samples and parameters.</p>
<p>Limitations of agents and environments. While agents still find text environments challenging, it is important to recognize that even this modest performance is achieved through a number of simplifying properties. For example, because agents frequently generate plausible but invalid actions, all but two agents benchmarked here depend on SCI-ENCEWORLD's valid action detection aid at test time, substantially simplifying their search problem in the action space. Similarly, while SCIENCE-WORLD achieves a high environment fidelity for a text simulation, this is still tempered by pragmatic concerns, such as generating comparatively short descriptions of environments that can fit into the sequence lengths of most transformer models. As such, even environments with complex physical, chemical, and biological processes underlying their simulations (such as SCIENCEWORLD) still ultimately must limit the vividness of their descriptions, until these technical limitations in modelling can be surpassed. Hybrid environments (e.g., Shridhar et al., 2020) that concurrently model the same environment as both a high-fidelity 3D world and comparatively low-fidelity text-based simulation have shown that text environments can be used to provide useful task pre-training that can transfer back to the 3D environment with relatively low simulation compute cost.</p>
<p>Explanations as action sequences. Explanations take on a variety of roles (Lombrozo, 2006;Gilpin et al., 2018), from detailed human-readable descriptions of classification processes that describe how a decision was made (e.g., Ribeiro et al., 2016), to higher-level appeals to scientific processes to explain why an answer is correct (e.g., Jansen et al., 2018;Dalvi et al., 2021). Here, the action procedures generated by agents act as manner explanations for how to solve a particular task -but while they describe how to accomplish something, they don't explain at a high-level why those actions accomplish the task. For example, action sequences lack high-level goal information such as "melting a substance requires heating it, so first the agent needs to heat the substance with a heating device, like a stove.". Similar to how cuing agents to answer contextual questions can help improve their task performance (Peng et al., 2021), cuing agents to generate these explanatory scaffolds may help future agents increase task performance, while structuring their action sequence explanations for better human interpretability.</p>
<p>Conclusion</p>
<p>Despite recent progress in both interactive text agents and scientific text processing via transformers, current models are unable to reason about fundamental science concepts in a grounded and reusable manner-calling into question how much they are actually understanding the tasks at hand.</p>
<p>To better measure such scientific reasoning abilities, we introduce SCIENCEWORLD, an interactive text environment derived from an elementary school science curriculum-with tasks ranging from electrical conductivity to Mendelian genetics. We evaluate three state-of-the-art reinforcement learning text game agents: DRRN, KG-A2C, and CALM; and further introduce two large-scale transformer-based agents inspired by recent ad-vances such as Behavior Cloning and the Decision Transformer and trained for scientific reasoning in SCIENCEWORLD. While we find that overall performance on unseen tasks that require using science-domain knowledge is low across all agents, our results also suggest that agents that learn interactively in a grounded environment are more sample and parameter efficient than large language models that learn offline by reading text from static sources. The best agent performance is still modest -and on-par with medium-difficulty interactive fiction environments such as Zork -highlighting the need for agents that can integrate declarative scientific and world knowledge with procedural action sequences in virtual environments.</p>
<p>Broader Impacts</p>
<p>Interactive text environments can provide a faster and cheaper alternative to 3D environments to teach agents how to plan via sequential decision making. They allow better control over the level of abstraction desired to approach a task (i.e., go to kitchen, vs. put hand on door's knob, turn knob clockwise, pull door, let go of the knob, walk through the door). We believe making a plan in this abstract language space is simpler and more interpretable.</p>
<p>With respect to risks, we consider this current work as exploratory only. ScienceWorld is primarily intended for training agents to learn reasoning capabilities in the science domain, with limited immediate utility to human science students. Agents trained on ScienceWorld should not be used to provide advice for the real world. The environment in ScienceWorld has been made safer compared to the real world. For instance, the agent can't accidentally burn itself while boiling a substance on a campfire, and its actions should not be taken as demonstrations of safe procedures for students. </p>
<p>A.1 Object Model</p>
<p>Objects in SCIENCEWORLD are represented using an object-oriented model and are implemented as classes. SCIENCEWORLD objects can be thought of as collections of sets of properties (e.g., material properties, life properties, device properties, etc.). All objects implement common functions, such as those that produce textual descriptions of the object, or that provide one or more possible referents for the object based on its current state (e.g., the water substance in the solid state could generate the referents ice, solid water, and substance, each of which could be used by the agent to refer to that object in an action). Similar to Z-machine games (Infocom, 1989), objects are stored in an object tree representing the object's current container (its immediate parent object in the tree), and any objects it contains (child nodes in the tree).</p>
<p>A.2 Environment and Objects</p>
<p>SCIENCEWORLD is composed of a map of 10 locations centered around a house theme (kitchen, bathroom, workshop, art studio, greenhouse, outside, etc.), as shown in Figure 1. While the rooms and how they interconnect is static, the environment is randomly populated with different combinations of relevant contextual items each time it is initialized -for example, in one run, the living room may have a bookcase with three books. In other runs, the bookcase may have different books, no books, or not be present in the environment. This parametric variation discourages agents from memorizing the specific environment, and encourages robustness in task performance. The environment is populated with up to 195 specific types of objects (excluding variations of those objects that change names or task properties, i.e., red wires and black wires belong to the same object type). This includes 23 animals, 11  plants, 25 substances, 10 canonical liquid containers (like tin cups or glass jars), 13 electrical components (such as light bulbs, motors, wires, and generators), 16 devices (including a stove, thermometer, and stopwatch), and 15 common pieces of furniture. To support these objects, the simulator includes a variety of other properties, including (for example) plant/animal life cycles, and 80 material properties (including water, glass, iron, and wood) that pure substances or physical objects (e.g., tables) can be made from.</p>
<p>A.3 Action Space</p>
<p>The simulator implements 25 actions, shown in Table 3, including generic actions common in interactive text environments (e.g., opening a door, moving to a location), as well as science-domain specific actions (e.g., connecting electrical components, chemically mixing items, pouring liquids). Five actions take two arguments, 16 take one argument, and four actions take zero arguments. Given the approximately 200 possible objects (excluding parametric variations) in SCIENCEWORLD, the action space can naively be estimated to be approximately 200,000 possible unique action possibilities at each step, though only a small subset of these would be meaningful. Similar to the Jericho framework, the simulator can provide valid action detection as an aid to agents (such as the DRRN) that require selecting their next action from a list of possible known-valid actions at a given step.</p>
<p>Input Parser At each step, an input parser attempts to parse user or agent input into a single unique action. Actions are specified as templates that can take on a variety of surface forms (e.g., move to LOCATION or go to LOCATION), and that include placeholders for object referents. At runtime, the parser examines all valid referents for visible objects from the agent's point of view, and if a given input string can produce more than one valid action, the parser will ask for clarification 5 .</p>
<p>A.4 Simulation Engines</p>
<p>SCIENCEWORLD supports actions commonly found in interactive text environments -for example, objects can be moved or examined, foods can be eaten, and books can be read. In addition, the environment contains a number of elementary science-domain specific processes that either occur automatically (e.g., thermodynamics) or are coupled to actions (e.g., using devices, mixing chemicals). Those simulation engines are: 6 Thermodynamics: All objects have temperatures and other thermal properties based on their materials. All objects within a container are considered in thermal contact with each other, and transfer heat energy using a simplified conductive heat model. The proportion of heat transferred between objects at each step is mediated by the object's thermal conduction coefficient, allowing thermal conductors (like metal pots) and insulators (like ceramics) to be modelled. Every material has phase transition points (i.e., melting point, boiling point) and combustion points populated based on the best-known or approximate physical values for those materials.</p>
<p>Objects that move past these thresholds will change state of matter (e.g., from a solid to a liquid), or be-5 For example, if the agent is in a room with two apples, one on a table and one in a bowl, the command take apple will cause the parser to ask the agent to clarify which apple they mean by selecting possible alternatives from a numbered list. 6 To maintain tractability in implementation, simulation engines are implemented with a fidelity at the level of an elementary science curriculum. Thermal transfer uses a simplified equation, biological changes happen in stages rather than gradually, only series instead of arbitrary electrical circuits are simulated (and without the concepts of resistance, inductance or other advanced topics), etc. gin a combustion process that ultimately ends in the object turning to ash unless its fire is put out. Convective heat transfer is also modelled in the form of heat sources (e.g., oven, stove) and heat sinks (e.g., fridge, freezer) that transfer heat energy to or from objects. Rooms also transfer ambient heat energy to/from the objects they contain.</p>
<p>Electricity: The simulator models simple series electrical circuits, where electrically-powered devices (e.g., light bulb, motor) can be powered by being connected to electrical energy sources (e.g., battery, solar panel) through electrical conductors (nominally, wires). Polarized and unpolarized components are modelled, with each object having exactly two terminals (either an anode and cathode for polarized components, or terminals 1 and 2 for unpolarized components). Connection happens through explicit terminal-to-terminal connection actions (e.g., connect battery anode to blue wire terminal 1). Every non-electrical object in SCIENCEWORLD has virtual unpolarized terminals, allowing circuits to be build with valid electrical conductors (e.g., using a metal fork in place of a wire), and for the agent to build circuits that test conductivity by (for example) observing if a light bulb illuminates when a plastic versus metal fork is used in the circuit.</p>
<p>Devices: Many objects are also considered devices, that can be activated or deactivated by the agent (e.g., stove, electrical switch), or may have environment-specific conditions to being activated (e.g., a light bulb will only activate if it is properly electrically connected; a solar panel will only produce power if it is outside). Objects can also be used with other objects in specific contexts (e.g., a thermometer, to measure an object's temperature; a shovel, to dig soil from the ground).</p>
<p>Chemistry: A subset of specific chemical reactions are modelled, where mixing a set of substances together in a container will produce a resultant substance (e.g., salt and water mix to produce salt water). Common chemical reactions described in elementary science questions (e.g., water reactions, rust, food reactions, paint mixing) are modelled.</p>
<p>Life Stages: Living things (plants and animals) progress through life stages (e.g., seed, seedling, juvenile plant, adult plant, reproducing plant, dead plant). Progression through life stages happens over time by continuing to meet the needs of that living thing (e.g., water, soil). If the needs are not met (e.g., a plant is not watered, is removed from soil, or becomes too hot), then it dies.</p>
<p>Reproduction and Genetics: Living things can have genes that express traits (e.g., flower colour, seed shape, leaf size). Genes are inherited from the alleles of both parents, and genotype is determined at the time of reproduction using a Punnett square. Phenotype (expressed, visible traits) are determined based on which genes are dominant versus recessive. Currently, traits are only populated for selected plants to reproduce Mendeliangenetics experiments. Plants reproduce by exchanging pollen (containing their genes) between flowers, typically by a pollinator (such as a bee). Pollinated flowers eventually wilt and turn into fruits containing seeds of genetic descendants.</p>
<p>Friction (Inclined Plane): Forces are a significant part of an elementary science curriculum, but difficult to incorporate without 2D or 3D simulation. SCIENCEWORLD models the forces of gravity and friction in the specific context of 1-dimensional inclined plane experiments. Objects placed at the top of an inclined plane will slide down the plane at a speed proportional to the plane's angle, and the friction coefficient of its surface material. The position is described to the agent (e.g., "an inclined plant with a block 60% of the way down the plane"), allowing experiments to determine either the relative angle or friction coefficients of different inclined planes based on the speed the object moves down a given plane.</p>
<p>Containers: Containers can be always open (e.g., a metal pot) or closeable (e.g., a cupboard). Objects contained inside containers are not visible until the container is open. Some effects spread beyond a container -for example, a wooden cupboard with a hot object inside may combust, causing other objects in the kitchen to also increase temperature.</p>
<p>B Tasks and Competencies</p>
<p>To support our goal of generating a diverse set of tasks, we identified a candidate set of 10 broad science exam topics from the list of 400 fine-grained science curriculum topics of Xu et al. (2020). Topics were chosen that would be amenable to textbased simulation, and that did not have critical fine-grained spatial reasoning requirements. These topics and tasks are described in Section B.2.</p>
<p>Subtasks and Masked Objects: Each of the 10 broad curriculum topics is further subdivided into between 2 and 4 specific subtasks that test specific reasoning capacities (e.g., melting, boiling, and freezing subtasks for the change-of-state task), or ask the agent to perform the same task but with names of critical task objects masked. Some tasks are possible to partially solve by looking up critical task information (e.g., knowing that white flowers are a dominant trait of pea plants for the Mendelian genetics task). We include two versions of tasks, one with using masked names (e.g., growing Unknown Plant B instead of a Pea Plant) while simultaneously randomly generating the properties of those objects to provide an instrument to measure when agents are solving tasks by performing the experimental procedure, and when they are directly looking up answers.</p>
<p>Task Formats: Task goals are structured with the broad goal of both (a) accomplishing a task, and (b) doing so intentionally. Tasks typically include preliminary subgoals where the agent must signal their intent to perform the task on a specific object by first "focusing" on the object they intend to perform the task with (e.g., for a boiling task, focusing on water they intend to boil) before they perform the task.</p>
<p>Tasks take on two main forms: Perform a task: the agent must directly perform a task, that produces some measurable change in the environment (e.g., growing a fruit through pollination) that can be directly measured as an end-state. Forcedchoice: The agent must perform a task that requires making an inference (e.g., whether an object is an electrical conductor or insulator), and provide their answer by placing the task object in a specific container (i.e., an "answer box") if the object is conductive, and a different container if it is an insulator.</p>
<p>Task Variations: To prevent overfitting and encourage generalization, each subtask contains between 10 and 1400 parametric variations of that subtask (with 7200 total variations across all 30 subtasks). Variations change critical task objects (e.g., the specific substance to be melted), the agent's starting location in the environment, as well as randomly vary the contents of the environment itself (e.g., whether the living room contains a bookshelf, or a painting, or both).</p>
<p>Task Simplifications: Agents find different competencies that SCIENCEWORLD tests to be chal-lenging. Tasks can be made easier by enabling any of 5 environment simplifications (or choosing "easy" mode, which enables all the simplifications). Examples of simplifications include a teleport action that lets agents instantly move to any location, and having all containers open by default.</p>
<p>B.1 Scoring and Evaluation Protocol</p>
<p>Goals and Reward Shaping: Each subtask contains a small number of method-agnostic required goals to be met (such as focusing on the substance to melt, and causing that substance's state of matter to change from a solid to a liquid for the melting task). In addition, to make rewards less sparse for agents learning these tasks, each task includes between 2 and 15 optional subgoals (such as turning on the stove, or the substance increasing in temperature by 10C) that help nudge agents in the direction of canonical solutions, if desired. Meeting required and optional subgoals increases the agent's score on a given subtask. Scores for all tasks are normalized to between 0 and 1.</p>
<p>B.2 Specific Tasks</p>
<p>Changes of State: The agent must find a named substance (e.g., ice), and change the state of matter of that substance (solid, liquid, gas) using the heating and cooling devices (e.g., stove, freezer) available in the environment. Subtasks require specific phase changes (melting, boiling, freezing, or the agent's choice). Variations change the substance, and ablate common devices (e.g., the stove becomes disabled) so that the agent must find alternate methods of heating or cooling.</p>
<p>Measurement Instrument: The agent must find a thermometer and use it to measure the temperature of a named object. In two additional subtasks, the agent must use the thermometer to measure the melting point of a named substance by heating it and continually monitoring the temperature. Answers are modelled as a forced-choice task, where the agent is given a predetermined temperature threshold at the start of the task (e.g., 50</p>
<p>• C), and must focus on one answer box if the melting point is above the threshold, and the other answer box if the melting point is below the threshold. Variations change the substance to be measured, and the preset temperature threshold.</p>
<p>Electrical Circuits: The agent must build a working series electrical circuit by connecting various electrical components including power sources (e.g., battery, wind mill, gas generator), different coloured wires, and active components (e.g., lights, motors). Subtasks include (a) powering a named component, (b) powering using renewable versus nonrenewable energy, and (c) measuring whether named or unknown substances are electrically conductive. Variations change available components, colours of wire, and specific task objects required to be used in the circuit.</p>
<p>Classification: In four subtasks, the agent must find an object in the environment belonging to a specific category (living things, non-living things, plants, or animals), and place it in an answer box. Variations change the location and description of the answer box.</p>
<p>Growing plants:</p>
<p>The agent must grow a named plant (e.g., a peach tree) from seed. To do this, they must place the seed and soil in a flower pot, and provide regular water as the plant progresses through life stages into adulthood. Failure to water appropriately causes the plant to perish. In a subtask, the agent must grow a fruit by growing several plants, then releasing pollinators (i.e., bees) that cross-pollinate flowers on the plants, which will eventually produce fruits. Variations change seed type, and soil location (either provided in the pot, provided in the room, or must be gathered outside using a shovel).</p>
<p>Chemistry: The agent must create a specific substance by mixing two or more input substances in a container. In the generic subtask, a recipe document that can be read by the agent is provided somewhere in the environment. In two paint-themed subtasks, the agent is given primary colours of paint (red, green, yellow), and must mix secondary (e.g., orange) or tertiary (e.g., orange-yellow) colours through several steps. Variations change the required output substance.</p>
<p>Life Spans: In three task variations, the agent must find 3 animals in the environment, then select either the shortest-lived (e.g., bee), longest-lived (e.g., turtle), or shortest-then-longest lived (beethen-turtle). Variations change which animals are populated in the environment from a list of long, medium, and short-lived animals.</p>
<p>Life Stages: The agent must find a named plant or animal, and focus on its life stages, from earliest (e.g., seed) to latest (e.g., reproducing adult plant). Variations change the plant or animal involved.</p>
<p>Forces: The agent must use inclined planes and masses (e.g., a block) for experiments about forces. In one subtask the agent is given two planes, and must determine which has the steepest or shallowest angle based on the time the block takes to move down the plane. In two other subtasks, the agent must find which of two planes has a surface of highest or least friction, from either named (e.g., plastic, steel) or unknown surfaces. The agent can measure time internally (in terms of number of steps for a block to fall), or measure this explicitly with a provided stopwatch. Variations change inclined plane angles (first task) or surface material types (remaining tasks).</p>
<p>Mendelian Genetics: The agent must determine whether a named trait (e.g., white flower colour) is a dominant or recessive trait in a plant. Two seeds are provided (one with the trait as dominant, one recessive), and the agent must grow two generations of plants and count how often it observes a given trait in successive generations to determine whether it is dominant or recessive. Subtasks change whether the plant is known (the pea plant, as in Mendel's experiments) or a randomly generated plant, while variations change the trait under investigation.</p>
<p>B.3 Commonsense Competencies</p>
<p>In addition to science-domain competencies, the agent must demonstrate fluency with commonsense knowledge and procedures to complete tasks successfully. Agents must know the locations of common objects (e.g., water comes from a sink, orange juice is typically found in a fridge), the affordances of common objects (a sink can be turned on to create water, a cup can be used to carry a liquid), navigation (a world is made of discrete rooms that can be traversed through doors), containers need to be opened to observe or use their contents, and so forth.</p>
<p>B.4 Scoring and Evaluation Protocol</p>
<p>Goals and Reward Shaping: Each subtask contains a small number of method-agnostic required goals to be met (such as focusing on the substance to melt, and causing that substance's state of matter to change from a solid to a liquid for the melting task). In addition, to make rewards less sparse for agents learning these tasks, each task includes between 2 and 15 optional subgoals (such as turning on the stove, or the substance increasing in temperature by 10C) that help nudge agents in the direction of canonical solutions, if desired. Meeting required and optional subgoals increases the agent's score on a given subtask. Scores for all tasks are normalized to between 0 and 1.</p>
<p>Train, Development, Test sets: For a given subtask, variations are split into 50% training, 25% development, and 25% test sets. Variations are sorted such that critical unseen variations (e.g., substances, animals, or plants unseen during training) are found in development and test sets.</p>
<p>B.5 Task Simplifications</p>
<p>Agents find different competencies that SCIENCE-WORLD tests to be challenging. Tasks can be made easier by enabling any of 5 environment simplifications (or choosing "easy" mode, which enables all the simplifications). Examples of simplifications include a teleport action that lets agents instantly move to any location; self-watering flowering flower pots that mean plants do not have to be frequently watered; and having all containers open by default.</p>
<p>C Experiment Details</p>
<p>C.1 Reinforcement Learning Models</p>
<p>For each reinforcement learning model, we ran 8 environment threads at 100k steps per thread. Training episodes reset after meeting an end state (success or failure), or after reaching 100 steps. For KG-A2C and CALM, the training episode will also reset if stuck by invalid actions for 100 steps (invalid actions are not counted by the environment). We did evaluation on the test variations every 1000 steps per environment thread. We randomly chose 10 test variations and run 1 episode of testing for each chosen variation during each evaluation period and reported the average score of the 10% test steps scores.</p>
<p>DRRN:</p>
<p>We use the DRRN architecture from https://github.com/microsoft/tdqn, with embedding size and hidden size set as 128. The learning rate we use to train DRRN is 0.0001. The memory size is 100k, and priority fraction is 0.50. The tokenizer for the input text is a uni-gram subword tokenizer model adapted from Kudo (2018).</p>
<p>KG-A2C: We make two major changes to the original KG-A2C model to function with SCIENCE-WORLD. (1) We replace the OpenIE knowledge graph extractor with a heuristic extractor. The heuristic extractor uses regular expressions to parse the text of the "look around" and "agent inventory" information into (subject, relation, object) triples. This heuristic functionally extracts the ground truth knowledge graph representation of the observable environment for the agent. (2) We change the KG-A2C agent to generate object types instead of references to specific objects. After selecting the action template and object type fillers that the agent has the highest confidence in, we ground those object types (e.g. apple) with specific object referents in the agent's current visible environment. If more than one referent meets that type, one is chosen at random. The learning rate we use to train the KG-A2C agent is 0.003 and the tokenizer used is the same as the DRRN agent.</p>
<p>CALM-GPT2: Following the original CALM paper (Yao et al., 2020), we use a 12-layer, 768hidden, 12-head GPT-2 model. We use the default pre-trained weight offered by the Huggingface Transformers library (Wolf et al., 2020). We fine-tune this GPT-2 model on complete action sequences generated by the oracle agents. The GPT-2 input prompt is formed as "
[CLS] d [SEP] o t [SEP] o look t [SEP] o inv t [SEP] o t−1 [SEP] a t−1 [SEP]
" and targets to predict a t , where d stands for the task description and o t , o look t , o inv t , and a t stand for the observation (excluding the look around and inventory information), the output of a "look around" action at the agent's current location, the agent inventory, and the action at time step t. We use beam search for generation, generating 16 beams representing candidate actions for the agent to select from. We set the diversity penalty to 50.0 to encourage the GPT-2 model to generate different actions. For the GPT-2 training we use a batch size of 12 and train for 20 epochs with a learning rate of 0.00002. The learning rate we use to train the CALM agent is 0.0001 and the input tokenizer is the same as that used in the original GPT-2 paper.</p>
<p>Due to the high computation cost of the CALM model, and modest overall performance, performance for each task is the average of only 3 random seeds instead of the 5 used for training the DRRN and KGA2C models. During development, a small error was found in the prompt. Pilot experiments suggested this resulted in a negligible (±0.01) change in performance, so the full experiments (requiring up to 6000 GPU hours) were not regenerated.</p>
<p>Episode reward curves: Episode reward curves for each model across all 30 subtasks in SCIENCE-WORLD are shown in Figure 3.</p>
<p>C.2 Behavior Cloning and Text Decision Transformer</p>
<p>The T5 models used to train both of these models are initialized with weights and tokenizers from the trained Macaw-11b model released at https: //github.com/allenai/macaw. They are trained on a v3-32 TPU pod with a batch size of 16 and 32-way model parallelism for 100k gradient update steps.</p>
<p>At inference time, we use the model to generate actions given the observation of current and previous step. We use beam search with a beam size 16 to get the top 16 generations. We set the diversity penalty to 50.0 to encourage diversity in generation. For each subtask, we run one episode on each of its test variations and report the average score. We do not update weights of the T5 model during evaluation.  Online RL and Offline Transformer model reflect runtimes for one full run of one subtask at one seed. Runtime estimates should be multiplied by the number of tasks (30) and number of random seeds (5) to obtain full runtime estimates. All GPUbased runs were completed on P100, V100. or A6000 GPUs.   size across all tasks and random seeds. For SCIENCEWORLD tasks, larger models do not necessarily perform better, and in some cases appear to show inverse scaling. * signifies that the value of 131M parameters includes the number of the parameters of the pre-trained GPT-2 action generator model. Only 6.9 million policy parameters are updated in RL training.</p>
<p>C.3 Resources</p>
<p>To examine the effect of model size on behavior cloned and decision transformer model performance, we ran two model sizes for the T5 models, shown in Table 5. We first observe that pre-training specifically for scientific question answering on a curated dataset (Macaw) outperforms T5 general language model pre-training. Further, we note that T5-Large and Macaw-Large, with 14 times fewer parameters (770m each), out-perform the larger 11 billion parameter models by approximately a factor of two. These results suggest that while SCI-ENCEWORLD can benefit from external scientific knowledge, it may also present an inverse scaling problem 7 , where increasing model size can sometimes decrease overall task performance. However, these results are only suggestive of an inverse scaling problem rather than a concrete demonstration. Due to the high cost of training and inference for these models, we can't currently rule out that these differences may be due to hyperparameter differences. We leave this verification as future work.</p>
<p>C.5 Attribution</p>
<p>Graphical visualization makes use of RPG game assets developed by @Noiracide.</p>
<p>Figure 1 :
1A graphical representation of an agent performing the Electrical Conductivity Subtask in SCIENCEWORLD.</p>
<p>Behavior
Cloning (Torabi et al., 2018): We follow the methodology ofAmmanabrolu et al. (2021)  in adapting the popular imitation learning method of behavior cloning from observations to text agents. We used the same transcripts of demonstrations as the CALM (GPT2) agent to extract 211,092 training examples with (d, o t−1 , a t−1 , o t ) as inputs and a t as targets. We fine-tune a transformer-based text-to-text model with a T5 architecture(Raffel et al., 2020)  initialized with the weights of a Macaw (Tafjord and Clark, 2021) model designed to answer science questions.</p>
<p>Figure 2 :
2Episode reward curves for the DRRN, KGA2C, and CALM models on the unseen test set as a function of the number of training environment interactions. (left) An example of an easier task, where the agent must pick up any non-living thing, and place it in a specific box in the environment. (center-left) An example challenge task, where the agent must perform any change of state (melt, boil, freeze) on a specific substance.</p>
<p>Figure 3 :
3Episode reward curves for the DRRN, KGA2C, and CALM models on the unseen test set as a function of the number of training environment interactions (steps).</p>
<p>Table 2 :
2Zero-shot performance of the agents on test variations of across all tasks. All online RL-trained agent performances are</p>
<p>Table 3 :
3The 25 actions in the action space of SCIENCE-WORLD. Actions can take up to two parameters, referencing objects the action should interact with.* signifies that the teleport action is only available to agents in a simplified mode.</p>
<p>Table 4 :
4Approximate computational resources per model.</p>
<p>C . 4
.Impact of Model Size and Pre-training Methodology on PerformanceAverage 
Model 
Model 
Performance Parameters </p>
<p>DRRN 
0.17 
1.5M 
KGA2C 
0.11 
5.5M 
CALM 
0.05 
131M </p>
<ul>
<li></li>
</ul>
<p>Behavior Cloned 
T5-Large 
0.15 
770M 
Macaw-Large 
0.17 
770M 
Macaw-11B 
0.08 
11B </p>
<p>Decision Transformer 
T5-Large 
0.13 
770M 
Macaw-Large 
0.15 
770M 
Macaw-11B 
0.08 
11B </p>
<p>Table 5 :
5Average model performance versus model parameter
For tractability, simulation engines are implemented with fidelity at the level of elementary science. Thermal transfer uses a simplified equation, biological changes happen in stages rather than gradually, only simple series circuits are simulated (no resistance, inductance, or any advanced topics), etc.
See the APPENDIX for additional experiments evaluating performance versus model size.
https://github.com/inverse-scaling/ prize
AcknowledgementsThis work supported in part by National Science Foundation (NSF) award #1815948 to PJ, Google Cloud Compute, and the Allen Institute for AI. The authors would also like to thank Liwei Jiang, Jack Hessel, and Oyvind Tafjord for their very timely technical assistance and advice, giving us the ability to train our larger, transformer-based agents. We'd also like to thank Michal Guerquin for technical assistance with the project website.
Explanations for Common-senseQA: New Dataset and Models. Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, Dinesh Garg, 10.18653/v1/2021.acl-long.238Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics1Shourya Aggarwal, Divyanshu Mandowara, Vishwa- jeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. 2021. Explanations for Common- senseQA: New Dataset and Models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3050-3065, Online. Association for Computational Linguistics.</p>            </div>
        </div>

    </div>
</body>
</html>
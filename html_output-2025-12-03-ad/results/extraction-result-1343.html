<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1343 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1343</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1343</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-368132f8dfcbd6e857dfc1b7dce2ab91bd9648ad</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/368132f8dfcbd6e857dfc1b7dce2ab91bd9648ad" target="_blank">Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on robotics</p>
                <p><strong>Paper TL;DR:</strong> What is now the de-facto standard formulation for SLAM is presented, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers.</p>
                <p><strong>Paper Abstract:</strong> Simultaneous localization and mapping (SLAM) consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications and witnessing a steady transition of this technology to industry. We survey the current state of SLAM and consider future directions. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1343",
    "paper_id": "paper-368132f8dfcbd6e857dfc1b7dce2ab91bd9648ad",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.008862749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>This paper has been accepted for publication in IEEE Transactions on Robotics.
DOI: 10.1109/TRO.2016.2624754
IEEE Explore: http://ieeexplore.ieee.org/document/7747236/</p>
<p>Please cite the paper as:
C. Cadena and L. Carlone and H. Carrillo and Y. Latif and D. Scaramuzza and J. Neira and I. Reid and J.J. Leonard, "Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age", in IEEE Transactions on Robotics 32 (6) pp 1309-1332, 2016
bibtex:
@article{Cadena16tro-SLAMfuture,
title $={$ Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age $}$,
author $={$ C. Cadena and L. Carlone and H. Carrillo and Y. Latif and D. Scaramuzza and J. Neira and I. Reid and J.J. Leonard $}$,
journal $={{$ IEEE Transactions on Robotics $}}$,
year $={2016}$,
number $={6}$,
pages $={1309-1332}$,
volume $={32}$
}</p>
<h1>Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age</h1>
<p>Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, José Neira, Ian Reid, John J. Leonard</p>
<h4>Abstract</h4>
<p>Simultaneous Localization And Mapping (SLAM) consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications, and witnessing a steady transition of this technology to industry. We survey the current state of SLAM and consider future directions. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?</p>
<p>Index Terms—Robots, SLAM, Localization, Mapping, Factor graphs, Maximum a posteriori estimation, sensing, perception.</p>
<h2>Multimedia Material</h2>
<p>Additional material for this paper, including an extended list of references (bibtex) and a table of pointers to online datasets for SLAM, can be found at https://slam-future.github.io/.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>I. INTRODUCTION</h2>
<p>SLAM comprises the simultaneous estimation of the state of a robot equipped with on-board sensors, and the construction of a model (the map) of the environment that the sensors are perceiving. In simple instances, the robot state is described by its pose (position and orientation), although other quantities may be included in the state, such as robot velocity, sensor biases, and calibration parameters. The map, on the other hand, is a representation of aspects of interest (e.g., position of landmarks, obstacles) describing the environment in which the robot operates.</p>
<p>The need to use a map of the environment is twofold. First, the map is often required to support other tasks; for instance, a map can inform path planning or provide an intuitive visualization for a human operator. Second, the map allows limiting the error committed in estimating the state of the robot. In the absence of a map, dead-reckoning would quickly drift over time; on the other hand, using a map, e.g., a set of distinguishable landmarks, the robot can "reset" its localization error by re-visiting known areas (so-called loop closure). Therefore, SLAM finds applications in all scenarios in which a prior map is not available and needs to be built.</p>
<p>In some robotics applications the location of a set of landmarks is known a priori. For instance, a robot operating on a factory floor can be provided with a manually-built map of artificial beacons in the environment. Another example is the case in which the robot has access to GPS (the GPS satellites can be considered as moving beacons at known locations). In such scenarios, SLAM may not be required if localization can be done reliably with respect to the known landmarks.</p>
<p>The popularity of the SLAM problem is connected with the emergence of indoor applications of mobile robotics. Indoor operation rules out the use of GPS to bound the localization error; furthermore, SLAM provides an appealing alternative to user-built maps, showing that robot operation is possible in the absence of an ad hoc localization infrastructure.</p>
<p>A thorough historical review of the first 20 years of the SLAM problem is given by Durrant-Whyte and Bailey in two surveys [7, 69]. These mainly cover what we call the classical age (1986-2004); the classical age saw the introduction of the main probabilistic formulations for SLAM, including approaches based on Extended Kalman Filters, RaoBlackwellised Particle Filters, and maximum likelihood estimation; moreover, it delineated the basic challenges connected</p>
<p>to efficiency and robust data association. Two other excellent references describing the three main SLAM formulations of the classical age are the book of Thrun, Burgard, and Fox [240] and the chapter of Stachniss et al. [234, Ch. 46]. The subsequent period is what we call the algorithmic-analysis age (2004-2015), and is partially covered by Dissanayake et al. in [64]. The algorithmic analysis period saw the study of fundamental properties of SLAM, including observability, convergence, and consistency. In this period, the key role of sparsity towards efficient SLAM solvers was also understood, and the main open-source SLAM libraries were developed.</p>
<p>We review the main SLAM surveys to date in Table I, observing that most recent surveys only cover specific aspects or sub-fields of SLAM. The popularity of SLAM in the last 30 years is not surprising if one thinks about the manifold aspects that SLAM involves. At the lower level (called the front-end in Section II) SLAM naturally intersects other research fields such as computer vision and signal processing; at the higher level (that we later call the back-end), SLAM is an appealing mix of geometry, graph theory, optimization, and probabilistic estimation. Finally, a SLAM expert has to deal with practical aspects ranging from sensor calibration to system integration.</p>
<p>The present paper gives a broad overview of the current state of SLAM, and offers the perspective of part of the community on the open problems and future directions for the SLAM research. Our main focus is on metric and semantic SLAM, and we refer the reader to the recent survey by Lowry et al. [160], which provides a comprehensive review of visionbased place recognition and topological SLAM.</p>
<p>Before delving into the paper, we first discuss two questions that often animate discussions during robotics conferences: (1) do autonomous robots need SLAM? and (2) is SLAM solved as an academic research endeavor? We will revisit these questions at the end of the manuscript.</p>
<p>Answering the question "Do autonomous robots really need SLAM?" requires understanding what makes SLAM unique. SLAM aims at building a globally consistent representation of the environment, leveraging both ego-motion measurements and loop closures. The keyword here is "loop closure": if we sacrifice loop closures, SLAM reduces to odometry. In early applications, odometry was obtained by integrating wheel encoders. The pose estimate obtained from wheel odometry quickly drifts, making the estimate unusable after few meters [128, Ch. 6]; this was one of the main thrusts behind the development of SLAM: the observation of external landmarks is useful to reduce the trajectory drift and possibly correct it [185]. However, more recent odometry algorithms are based on visual and inertial information, and have very small drift ( $&lt;0.5 \%$ of the trajectory length [82]). Hence the question becomes legitimate: do we really need SLAM? Our answer is three-fold.</p>
<p>First of all, we observe that the SLAM research done over the last decade has itself produced the visual-inertial odometry algorithms that currently represent the state of the art, e.g., [163, 175]; in this sense Visual-Inertial Navigation (VIN) is SLAM: VIN can be considered a reduced SLAM system, in which the loop closure (or place recognition) module is disabled. More generally, SLAM has directly led to the</p>
<p>TABLE I: Surveying the surveys and tutorials</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Year</th>
<th style="text-align: left;">Topic</th>
<th style="text-align: left;">Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2006</td>
<td style="text-align: left;">Probabilistic approaches <br> and data association</td>
<td style="text-align: left;">Durrant-Whyte and Bailey [7, 69]</td>
</tr>
<tr>
<td style="text-align: center;">2008</td>
<td style="text-align: left;">Filtering approaches</td>
<td style="text-align: left;">Aulinas et al. [6]</td>
</tr>
<tr>
<td style="text-align: center;">2011</td>
<td style="text-align: left;">SLAM back-end</td>
<td style="text-align: left;">Grisetti et al. [97]</td>
</tr>
<tr>
<td style="text-align: center;">2011</td>
<td style="text-align: left;">Observability, consistency <br> and convergence</td>
<td style="text-align: left;">Dissanayake et al. [64]</td>
</tr>
<tr>
<td style="text-align: center;">2012</td>
<td style="text-align: left;">Visual odometry</td>
<td style="text-align: left;">Scaramuzza and Fraundofer [85, 218]</td>
</tr>
<tr>
<td style="text-align: center;">2016</td>
<td style="text-align: left;">Multi robot SLAM</td>
<td style="text-align: left;">Saeedi et al. [216]</td>
</tr>
<tr>
<td style="text-align: center;">2016</td>
<td style="text-align: left;">Visual place recognition</td>
<td style="text-align: left;">Lowry et al. [160]</td>
</tr>
<tr>
<td style="text-align: center;">2016</td>
<td style="text-align: left;">SLAM in the Handbook <br> of Robotics</td>
<td style="text-align: left;">Stachniss et al. [234, Ch. 46]</td>
</tr>
<tr>
<td style="text-align: center;">2016</td>
<td style="text-align: left;">Theoretical aspects</td>
<td style="text-align: left;">Huang and Dissanayake [109]</td>
</tr>
</tbody>
</table>
<p>study of sensor fusion under more challenging setups (i.e., no GPS, low quality sensors) than previously considered in other literature (e.g., inertial navigation in aerospace engineering).</p>
<p>The second answer regards the true topology of the environment. A robot performing odometry and neglecting loop closures interprets the world as an "infinite corridor" (Fig. 1left) in which the robot keeps exploring new areas indefinitely. A loop closure event informs the robot that this "corridor" keeps intersecting itself (Fig. 1-right). The advantage of loop closure now becomes clear: by finding loop closures, the robot understands the real topology of the environment, and is able to find shortcuts between locations (e.g., point B and C in the map). Therefore, if getting the right topology of the environment is one of the merits of SLAM, why not simply drop the metric information and just do place recognition? The answer is simple: the metric information makes place recognition much simpler and more robust; the metric reconstruction informs the robot about loop closure opportunities and allows discarding spurious loop closures [150]. Therefore, while SLAM might be redundant in principle (an oracle place recognition module would suffice for topological mapping), SLAM offers a natural defense against wrong data association and perceptual aliasing, where similarly looking scenes, corresponding to distinct locations in the environment, would deceive place recognition. In this sense, the SLAM map provides a way to predict and validate future measurements: we believe that this mechanism is key to robust operation.</p>
<p>The third answer is that SLAM is needed for many applications that, either implicitly or explicitly, do require a globally consistent map. For instance, in many military and civilian applications, the goal of the robot is to explore an environment and report a map to the human operator, ensuring that full coverage of the environment has been obtained. Another example is the case in which the robot has to perform structural inspection (of a building, bridge, etc.); also in this case a globally consistent 3D reconstruction is a requirement for successful operation.</p>
<p>This question of "is SLAM solved?" is often asked within the robotics community, c.f. [87]. This question is difficult to answer because SLAM has become such a broad topic that the question is well posed only for a given robot/environment/performance combination. In particular, one can evaluate the maturity of the SLAM problem once the following aspects are specified:</p>
<ul>
<li>robot: type of motion (e.g., dynamics, maximum speed), available sensors (e.g., resolution, sampling rate), avail-</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Left: map built from odometry. The map is homotopic to a long corridor that goes from the starting position A to the final position B. Points that are close in reality (e.g., B and C) may be arbitrarily far in the odometric map. Right: map built from SLAM. By leveraging loop closures, SLAM estimates the actual topology of the environment, and "discovers" shortcuts in the map.</p>
<p>able computational resources;</p>
<ul>
<li><strong>environment</strong>: planar or three-dimensional, presence of natural or artificial landmarks, amount of dynamic elements, amount of symmetry and risk of perceptual aliasing. Note that many of these aspects actually depend on the sensor-environment pair: for instance, two rooms may look identical for a 2D laser scanner (perceptual aliasing), while a camera may discern them from appearance cues;</li>
<li><strong>performance requirements</strong>: desired accuracy in the estimation of the state of the robot, accuracy and type of representation of the environment (e.g., landmark-based or dense), success rate (percentage of tests in which the accuracy bounds are met), estimation latency, maximum operation time, maximum size of the mapped area.</li>
</ul>
<p>For instance, mapping a 2D indoor environment with a robot equipped with wheel encoders and a laser scanner, with sufficient accuracy (&lt; 10cm) and sufficient robustness (say, low failure rate), can be considered largely solved (an example of industrial system performing SLAM is the <em>Kuka Navigation Solution</em> [145]). Similarly, vision-based SLAM with slowly-moving robots (e.g., Mars rovers [166], domestic robots [114]), and visual-inertial odometry [94] can be considered mature research fields.</p>
<p>On the other hand, other robot/environment/performance combinations still deserve a large amount of fundamental research. Current SLAM algorithms can be easily induced to fail when either the motion of the robot or the environment are too challenging (e.g., fast robot dynamics, highly dynamic environments); similarly, SLAM algorithms are often unable to face strict performance requirements, e.g., high rate estimation for fast closed-loop control. This survey will provide a comprehensive overview of these open problems, among others.</p>
<p>In this paper, we argue that we are entering in a third era for SLAM, the <em>robust-perception age</em>, which is characterized by the following key requirements:</p>
<ol>
<li><strong>robust performance</strong>: the SLAM system operates with low failure rate for an extended period of time in a broad set of environments; the system includes fail-safe mechanisms and has self-tuning capabilities<sup>1</sup> in that it can adapt the selection of the system parameters to the scenario.</li>
<li><strong>high-level understanding</strong>: the SLAM system goes beyond basic geometry reconstruction to obtain a high-level understanding of the environment (e.g., high-level geometry, semantics, physics, affordances);</li>
<li><strong>resource awareness</strong>: the SLAM system is tailored to the available sensing and computational resources, and provides means to adjust the computation load depending on the available resources;</li>
<li><strong>task-driven perception</strong>: the SLAM system is able to select relevant perceptual information and filter out irrelevant sensor data, in order to support the task the robot has to perform; moreover, the SLAM system produces adaptive map representations, whose complexity may vary depending on the task at hand.</li>
</ol>
<p><strong>Paper organization.</strong> The paper starts by presenting a standard formulation and architecture for SLAM (Section II). Section III tackles robustness in life-long SLAM. Section IV deals with scalability. Section V discusses how to represent the geometry of the environment. Section VI extends the question of the environment representation to the modeling of semantic information. Section VII provides an overview of the current accomplishments on the theoretical aspects of SLAM. Section VIII broadens the discussion and reviews the active SLAM problem in which decision making is used to improve the quality of the SLAM results. Section IX provides an overview of recent trends in SLAM, including the use of unconventional sensors and deep learning. Section X provides final remarks. Throughout the paper, we provide many pointers to related work outside the robotics community. Despite its unique traits, SLAM is related to problems addressed in computer vision, computer graphics, and control theory, and cross-fertilization among these fields is a necessary condition to enable fast progress.</p>
<p>For the non-expert reader, we recommend to read Durrant-Whyte and Bailey's SLAM tutorials [7, 69] before delving in this position paper. The more experienced researchers can jump directly to the section of interest, where they will find a self-contained overview of the state of the art and open problems.</p>
<h2>II. Anatomy of a Modern SLAM System</h2>
<p>The architecture of a SLAM system includes two main components: the <em>front-end</em> and the <em>back-end</em>. The front-end abstracts sensor data into models that are amenable for estimation, while the back-end performs inference on the abstracted data produced by the front-end. This architecture is summarized in Fig. 2. We review both components, starting from the back-end.</p>
<p><sup>1</sup>The SLAM community has been largely affected by the "curse of manual tuning", in that satisfactory operation is enabled by expert tuning of the system parameters (e.g., stopping conditions, thresholds for outlier rejection).</p>
<p>Maximum a posteriori (MAP) estimation and the SLAM back-end. The current de-facto standard formulation of SLAM has its origins in the seminal paper of Lu and Milios [161], followed by the work of Gutmann and Konolige [101]. Since then, numerous approaches have improved the efficiency and robustness of the optimization underlying the problem [63, 81, 100, 125, 192, 241]. All these approaches formulate SLAM as a maximum a posteriori estimation problem, and often use the formalism of factor graphs [143] to reason about the interdependence among variables.</p>
<p>Assume that we want to estimate an unknown variable $\mathcal{X}$; as mentioned before, in SLAM the variable $\mathcal{X}$ typically includes the trajectory of the robot (as a discrete set of poses) and the position of landmarks in the environment. We are given a set of measurements $Z=\left{z_{k}: k=1, \ldots, m\right}$ such that each measurement can be expressed as a function of $\mathcal{X}$, i.e., $z_{k}=h_{k}\left(\mathcal{X}<em k="k">{k}\right)+\epsilon</em>}$, where $\mathcal{X<em k="k">{k} \subseteq \mathcal{X}$ is a subset of the variables, $h</em>$ is random measurement noise.}(\cdot)$ is a known function (the measurement or observation model), and $\epsilon_{k</p>
<p>In MAP estimation, we estimate $\mathcal{X}$ by computing the assignment of variables $\mathcal{X}^{\star}$ that attains the maximum of the posterior $\mathrm{p}(\mathcal{X} \mid Z)$ (the belief over $\mathcal{X}$ given the measurements):</p>
<p>$$
\mathcal{X}^{\star} \doteq \underset{\mathcal{X}}{\operatorname{argmax}} \mathrm{p}(\mathcal{X} \mid Z)=\underset{\mathcal{X}}{\operatorname{argmax}} \mathrm{p}(Z \mid \mathcal{X}) \mathrm{p}(\mathcal{X})
$$</p>
<p>where the equality follows from the Bayes theorem. In (1), $\mathrm{p}(Z \mid \mathcal{X})$ is the likelihood of the measurements $Z$ given the assignment $\mathcal{X}$, and $\mathrm{p}(\mathcal{X})$ is a prior probability over $\mathcal{X}$. The prior probability includes any prior knowledge about $\mathcal{X}$; in case no prior knowledge is available, $\mathrm{p}(\mathcal{X})$ becomes a constant (uniform distribution) which is inconsequential and can be dropped from the optimization. In that case MAP estimation reduces to maximum likelihood estimation. Note that, unlike Kalman filtering, MAP estimation does not require an explicit distinction between motion and observation model: both models are treated as factors and are seamlessly incorporated in the estimation process. Moreover, it is worth noting that Kalman filtering and MAP estimation return the same estimate in the linear Gaussian case, while this is not the case in general.</p>
<p>Assuming that the measurements $Z$ are independent (i.e., the corresponding noises are uncorrelated), problem (1) factorizes into:</p>
<p>$$
\begin{gathered}
\mathcal{X}^{\star}=\underset{\mathcal{X}}{\operatorname{argmax}} \mathrm{p}(\mathcal{X}) \prod_{k=1}^{m} \mathrm{p}\left(z_{k} \mid \mathcal{X}\right)= \
\underset{\mathcal{X}}{\operatorname{argmax}} \mathrm{p}(\mathcal{X}) \prod_{k=1}^{m} \mathrm{p}\left(z_{k} \mid \mathcal{X}_{k}\right)
\end{gathered}
$$</p>
<p>where, on the right-hand-side, we noticed that $z_{k}$ only depends on the subset of variables in $\mathcal{X}_{k}$.</p>
<p>Problem (2) can be interpreted in terms of inference over a factors graph [143]. The variables correspond to nodes in the factor graph. The terms $\mathrm{p}\left(z_{k} \mid \mathcal{X}<em k="k">{k}\right)$ and the prior $\mathrm{p}(\mathcal{X})$ are called factors, and they encode probabilistic constraints over a subset of nodes. A factor graph is a graphical model that encodes the dependence between the $k$-th factor (and its measurement $z</em>$. A first advantage of the factor graph interpretation is that it enables an insightful
}$ ) and the corresponding variables $\mathcal{X}_{k<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3: SLAM as a factor graph: Blue circles denote robot poses at consecutive time steps $\left(x_{1}, x_{2}, \ldots\right)$, green circles denote landmark positions $\left(l_{1}, l_{2}, \ldots\right)$, red circle denotes the variable associated with the intrinsic calibration parameters $(K)$. Factors are shown as black squares: the label "u" marks factors corresponding to odometry constraints, "v" marks factors corresponding to camera observations, "c" denotes loop closures, and "p" denotes prior factors.
visualization of the problem. Fig. 3 shows an example of a factor graph underlying a simple SLAM problem. The figure shows the variables, namely, the robot poses, the landmark positions, and the camera calibration parameters, and the factors imposing constraints among these variables. A second advantage is generality: a factor graph can model complex inference problems with heterogeneous variables and factors, and arbitrary interconnections. Furthermore, the connectivity of the factor graph in turn influences the sparsity of the resulting SLAM problem as discussed below.</p>
<p>In order to write (2) in a more explicit form, assume that the measurement noise $\epsilon_{k}$ is a zero-mean Gaussian noise with information matrix $\Omega_{k}$ (inverse of the covariance matrix). Then, the measurement likelihood in (2) becomes:</p>
<p>$$
\mathrm{p}\left(z_{k} \mid \mathcal{X}<em k="k">{k}\right) \propto \exp \left(-\frac{1}{2}\left|h</em>}\left(\mathcal{X<em k="k">{k}\right)-z</em>\right|<em k="k">{\Omega</em>\right)
$$}}^{2</p>
<p>where we use the notation $|e|<em 0="0">{\Omega}^{2}=e^{\mathrm{T}} \Omega e$. Similarly, assume that the prior can be written as: $\mathrm{p}(\mathcal{X}) \propto \exp \left(-\frac{1}{2}\left|h</em>\right|}(\mathcal{X})-\right.\right.$ $\left.\left.z_{0<em 0="0">{\Omega</em>$. Since maximizing the posterior is the same as minimizing the negative log-posterior, the MAP estimate in (2) becomes:}}^{2}\right)$, for some given function $h_{0}(\cdot)$, prior mean $z_{0}$, and information matrix $\Omega_{0</p>
<p>$$
\begin{gathered}
\mathcal{X}^{\star}=\underset{\mathcal{X}}{\operatorname{argmin}}-\log \left(\mathrm{p}(\mathcal{X}) \prod_{k=1}^{m} \mathrm{p}\left(z_{k} \mid \mathcal{X}<em k="0">{k}\right)\right)= \
\underset{\mathcal{X}}{\operatorname{argmin}} \sum</em>}^{m}\left|h_{k}\left(\mathcal{X<em k="k">{k}\right)-z</em>\right|<em k="k">{\Omega</em>
\end{gathered}
$$}}^{2</p>
<p>which is a nonlinear least squares problem, as in most problems of interest in robotics, $h_{k}(\cdot)$ is a nonlinear function. Note that the formulation (4) follows from the assumption of Normally distributed noise. Other assumptions for the noise distribution lead to different cost functions; for instance, if the noise follows a Laplace distribution, the squared $\ell_{2}$-norm in (4) is replaced by the $\ell_{1}$-norm. To increase resilience to outliers, it is also common to substitute the squared $\ell_{2}$-norm in (4) with robust loss functions (e.g., Huber or Tukey loss) [112].</p>
<p>The computer vision expert may notice a resemblance between problem (4) and bundle adjustment (BA) in Structure from Motion [244]; both (4) and BA indeed stem from a maximum a posteriori formulation. However, two key features</p>
<p>make SLAM unique. First, the factors in (4) are not constrained to model projective geometry as in BA, but include a broad variety of sensor models, e.g., inertial sensors, wheel encoders, GPS, to mention a few. For instance, in laserbased mapping, the factors usually constrain relative poses corresponding to different viewpoints, while in direct methods for visual SLAM, the factors penalize differences in pixel intensities across different views of the same portion of the scene. The second difference with respect to BA is that, in a SLAM scenario, problem (4) typically needs to be solved incrementally: new measurements are made available at each time step as the robot moves.</p>
<p>The minimization problem (4) is commonly solved via successive linearizations, e.g., the Gauss-Newton or the Levenberg-Marquardt methods (alternative approaches, based on convex relaxations and Lagrangian duality are reviewed in Section VII). Successive linearization methods proceed iteratively, starting from a given initial guess $\mathcal{X}$, and approximate the cost function at $\mathcal{X}$ with a quadratic cost, which can be optimized in closed form by solving a set of linear equations (the so called normal equations). These approaches can be seamlessly generalized to variables belonging to smooth manifolds (e.g., rotations), which are of interest in robotics [1, 82].</p>
<p>The key insight behind modern SLAM solvers is that the matrix appearing in the normal equations is sparse and its sparsity structure is dictated by the topology of the underlying factor graph. This enables the use of fast linear solvers [125, 126, 146, 204]. Moreover, it allows designing incremental (or online) solvers, which update the estimate of $\mathcal{X}$ as new observations are acquired [125, 126, 204]. Current SLAM libraries (e.g., GTSAM [61], g2o [146], Ceres [214], iSAM [126], and SLAM++ [204]) are able to solve problems with tens of thousands of variables in few seconds. The handson tutorials [61, 97] provide excellent introductions to two of the most popular SLAM libraries; each library also includes a set of examples showcasing real SLAM problems.</p>
<p>The SLAM formulation described so far is commonly referred to as maximum a posteriori estimation, factor graph optimization, graph-SLAM, full smoothing, or smoothing and mapping (SAM). A popular variation of this framework is pose graph optimization, in which the variables to be estimated are poses sampled along the trajectory of the robot, and each factor imposes a constraint on a pair of poses.</p>
<p>MAP estimation has been proven to be more accurate and efficient than original approaches for SLAM based on nonlinear filtering. We refer the reader to the surveys [7, 69] for an overview on filtering approaches, and to [236] for a comparison between filtering and smoothing. We remark that some SLAM systems based on EKF have also been demonstrated to attain state-of-the-art performance. Excellent examples of EKF-based SLAM systems include the MultiState Constraint Kalman Filter of Mourikis and Roumeliotis [175], and the VIN systems of Kottas et al. [139] and Hesch et al. [105]. Not surprisingly, the performance mismatch between filtering and MAP estimation gets smaller when the linearization point for the EKF is accurate (as it happens in visual-inertial navigation problems), when using slidingwindow filters, and when potential sources of inconsistency in
the EKF are taken care of $[105,108,139]$.
As discussed in the next section, MAP estimation is usually performed on a pre-processed version of the sensor data. In this regard, it is often referred to as the SLAM back-end.</p>
<p>Sensor-dependent SLAM front-end. In practical robotics applications, it might be hard to write directly the sensor measurements as an analytic function of the state, as required in MAP estimation. For instance, if the raw sensor data is an image, it might be hard to express the intensity of each pixel as a function of the SLAM state; the same difficulty arises with simpler sensors (e.g., a laser with a single beam). In both cases the issue is connected with the fact that we are not able to design a sufficiently general, yet tractable representation of the environment; even in the presence of such a general representation, it would be hard to write an analytic function that connects the measurements to the parameters of such a representation.</p>
<p>For this reason, before the SLAM back-end, it is common to have a module, the front-end, that extracts relevant features from the sensor data. For instance, in vision-based SLAM, the front-end extracts the pixel location of few distinguishable points in the environment; pixel observations of these points are now easy to model within the back-end. The front-end is also in charge of associating each measurement to a specific landmark (say, 3D point) in the environment: this is the so called data association. More abstractly, the data association module associates each measurement $z_{k}$ with a subset of unknown variables $\mathcal{X}<em k="k">{k}$ such that $z</em>}=h_{k}\left(\mathcal{X<em k="k">{k}\right)+\epsilon</em>$. Finally, the front-end might also provide an initial guess for the variables in the nonlinear optimization (4). For instance, in featurebased monocular SLAM the front-end usually takes care of the landmark initialization, by triangulating the position of the landmark from multiple views.</p>
<p>A pictorial representation of a typical SLAM system is given in Fig. 2. The data association module in the front-end includes a short-term data association block and a long-term one. Shortterm data association is responsible for associating corresponding features in consecutive sensor measurements; for instance, short-term data association would track the fact that 2 pixel measurements in consecutive frames are picturing the same 3D point. On the other hand, long-term data association (or loop closure) is in charge of associating new measurements to older landmarks. We remark that the back-end usually feeds back information to the front-end, e.g., to support loop closure detection and validation.</p>
<p>The pre-processing that happens in the front-end is sensor dependent, since the notion of feature changes depending on the input data stream we consider.</p>
<h2>III. LONG-TERM AUTONOMY I: ROBUSTNESS</h2>
<p>A SLAM system might be fragile in many aspects: failure can be algorithmic ${ }^{2}$ or hardware-related. The former class includes failure modes induced by limitation of the existing SLAM algorithms (e.g., difficulty to handle extremely dynamic or harsh environments). The latter includes failures due</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>to sensor or actuator degradation. Explicitly addressing these failure modes is crucial for long-term operation, where one can no longer make simplifying assumptions about the structure of the environment (e.g., mostly static) or fully rely on onboard sensors. In this section we review the main challenges to algorithmic robustness. We then discuss open problems, including robustness against hardware-related failures.</p>
<p>One of the main sources of algorithmic failures is data association. As mentioned in Section II data association matches each measurement to the portion of the state the measurement refers to. For instance, in feature-based visual SLAM, it associates each visual feature to a specific landmark. Perceptual aliasing, the phenomenon in which different sensory inputs lead to the same sensor signature, makes this problem particularly hard. In the presence of perceptual aliasing, data association establishes erroneous measurement-state matches (outliers, or false positives), which in turn result in wrong estimates from the back-end. On the other hand, when data association decides to incorrectly reject a sensor measurement as spurious (false negatives), fewer measurements are used for estimation, at the expense of estimation accuracy.</p>
<p>The situation is made worse by the presence of unmodeled dynamics in the environment including both short-term and seasonal changes, which might deceive short-term and longterm data association. A fairly common assumption in current SLAM approaches is that the world remains unchanged as the robot moves through it (in other words, landmarks are static). This static world assumption holds true in a single mapping run in small scale scenarios, as long as there are no short term dynamics (e.g., people and objects moving around). When mapping over longer time scales and in large environments, change is inevitable.</p>
<p>Another aspect of robustness is that of doing SLAM in harsh environments such as underwater [73, 131]. The challenges in this case are the limited visibility, the constantly changing conditions, and the impossibility of using conventional sensors (e.g., laser range finder).</p>
<p>Brief Survey. Robustness issues connected to incorrect data association can be addressed in the front-end and/or in the back-end of a SLAM system. Traditionally, the front-end has been entrusted with establishing correct data association. Short-term data association is the easier one to tackle: if the sampling rate of the sensor is relatively fast, compared to the dynamics of the robot, tracking features that correspond to the same 3D landmark is easy. For instance, if we want to track a 3D point across consecutive images and assuming that the framerate is sufficiently high, standard approaches based on descriptor matching or optical flow [218] ensure reliable tracking. Intuitively, at high framerate, the viewpoint of the sensor (camera, laser) does not change significantly, hence the features at time $t+1$ (and its appearance) remain close to the ones observed at time $t .{ }^{3}$ Long-term data association in the front-end is more challenging and involves loop closure detection and validation. For loop closure detection at the front-end, a brute-force approach which detects features in</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the current measurement (e.g., image) and tries to match them against all previously detected features quickly becomes impractical. Bag-of-words models [226] avoid this intractability by quantizing the feature space and allowing more efficient search. Bag-of-words can be arranged into hierarchical vocabulary trees [189] that enable efficient lookup in largescale datasets. Bag-of-words-based techniques such as [53] have shown reliable performance on the task of single session loop closure detection. However, these approaches are not capable of handling severe illumination variations as visual words can no longer be matched. This has led to develop new methods that explicitly account for such variations by matching sequences [173], gathering different visual appearances into a unified representation [48], or using spatial as well as appearance information [106]. A detailed survey on visual place recognition can be found in Lowry et al. [160]. Featurebased methods have also been used to detect loop closures in laser-based SLAM front-ends; for instance, Tipaldi et al. [242] propose FLIRT features for 2D laser scans.</p>
<p>Loop closure validation, instead, consists of additional geometric verification steps to ascertain the quality of the loop closure. In vision-based applications, RANSAC is commonly used for geometric verification and outlier rejection, see [218] and the references therein. In laser-based approaches, one can validate a loop closure by checking how well the current laser scan matches the existing map (i.e., how small is the residual error resulting from scan matching).</p>
<p>Despite the progress made to robustify loop closure detection at the front-end, in presence of perceptual aliasing, it is unavoidable that wrong loop closures are fed to the backend. Wrong loop closures can severely corrupt the quality of the MAP estimate [238]. In order to deal with this issue, a recent line of research [33, 150, 191, 238] proposes techniques to make the SLAM back-end resilient against spurious measurements. These methods reason on the validity of loop closure constraints by looking at the residual error induced by the constraints during optimization. Other methods, instead, attempt to detect outliers a priori, that is, before any optimization takes place, by identifying incorrect loop closures that are not supported by the odometry [215].</p>
<p>In dynamic environments, the challenge is twofold. First, the SLAM system has to detect, discard, or track changes. While mainstream approaches attempt to discard the dynamic portion of the scene [180], some works include dynamic elements as part of the model [11, 253]. The second challenge regards the fact that the SLAM system has to model permanent or semi-permanent changes, and understand how and when to update the map accordingly. Current SLAM systems that deal with dynamics either maintain multiple (time-dependent) maps of the same location [60], or have a single representation parameterized by some time-varying parameter [140].</p>
<p>Open Problems. In this section we review open problems and novel research questions arising in long-term SLAM.</p>
<p>Failsafe SLAM and recovery: Despite the progress made on the SLAM back-end, current SLAM solvers are still vulnerable in the presence of outliers. This is mainly due to the fact that virtually all robust SLAM techniques are based on iterative optimization of nonconvex costs. This has two consequences:</p>
<p>first, the outlier rejection outcome depends on the quality of the initial guess fed to the optimization; second, the system is inherently fragile: the inclusion of a single outlier degrades the quality of the estimate, which in turn degrades the capability of discerning outliers later on. These types of failures lead to an incorrect linearization point from which recovery is not trivial, especially in an incremental setup. An ideal SLAM solution should be fail-safe and failure-aware, i.e., the system needs to be aware of imminent failure (e.g., due to outliers or degeneracies) and provide recovery mechanisms that can re-establish proper operation. None of the existing SLAM approaches provides these capabilities. A possible way to achieve this is a tighter integration between the front-end and the back-end, but how to achieve that is still an open question.</p>
<p>Robustness to HW failure: While addressing hardware failures might appear outside the scope of SLAM, these failures impact the SLAM system, and the latter can play a key role in detecting and mitigating sensor and locomotion failures. If the accuracy of a sensor degrades due to malfunctioning, off-nominal conditions, or aging, the quality of the sensor measurements (e.g., noise, bias) does not match the noise model used in the back-end (c.f. eq. (3)), leading to poor estimates. This naturally poses different research questions: how can we detect degraded sensor operation? how can we adjust sensor noise statistics (covariances, biases) accordingly? more generally, how do we resolve conflicting information from different sensors? This seems crucial in safety-critical applications (e.g., self-driving cars) in which misinterpretation of sensor data may put human life at risk.</p>
<p>Metric Relocalization: While appearance-based, as opposed to feature-based, methods are able to close loops between day and night sequences or between different seasons, the resulting loop closure is topological in nature. For metric relocalization (i.e., estimating the relative pose with respect to the previously built map), feature-based approaches are still the norm; however, current feature descriptors lack sufficient invariance to work reliably under such circumstances. Spatial information, inherent to the SLAM problem, such as trajectory matching, might be exploited to overcome these limitations. Additionally, mapping with one sensor modality (e.g., 3D lidar) and localizing in the same map with a different sensor modality (e.g., camera) can be a useful addition. The work of Wolcott et al. [260] is an initial step in this direction.</p>
<p>Time varying and deformable maps: Mainstream SLAM methods have been developed with the rigid and static world assumption in mind; however, the real world is non-rigid both due to dynamics as well as the inherent deformability of objects. An ideal SLAM solution should be able to reason about dynamics in the environment including non-rigidity, work over long time periods generating "all terrain" maps and be able to do so in real time. In the computer vision community, there have been several attempts since the 80s to recover shape from non-rigid objects but with restrictive applicability. Recent results in non-rigid SfM such as [91, 96] are less restrictive but only work in small scenarios. In the SLAM community, Newcombe et al. [182] have address the non-rigid case for small-scale reconstruction. However, addressing the problem of non-rigid maps at a large scale is still largely unexplored.</p>
<p>Automatic parameter tuning: SLAM systems (in particular, the data association modules) require extensive parameter tuning in order to work correctly for a given scenario. These parameters include thresholds that control feature matching, RANSAC parameters, and criteria to decide when to add new factors to the graph or when to trigger a loop closing algorithm to search for matches. If SLAM has to work "out of the box" in arbitrary scenarios, methods for automatic tuning of the involved parameters need to be considered.</p>
<h2>IV. LONG-TERM AUTONOMY II: SCALABILITY</h2>
<p>While modern SLAM algorithms have been successfully demonstrated mostly in indoor building-scale environments, in many application endeavors, robots must operate for an extended period of time over larger areas. These applications include ocean exploration for environmental monitoring, nonstop cleaning robots in our ever changing cities, or large-scale precision agriculture. For such applications the size of the factor graph underlying SLAM can grow unbounded, due to the continuous exploration of new places and the increasing time of operation. In practice, the computational time and memory footprint are bounded by the resources of the robot. Therefore, it is important to design SLAM methods whose computational and memory complexity remains bounded.</p>
<p>In the worst-case, successive linearization methods based on direct linear solvers imply a memory consumption which grows quadratically in the number of variables. When using iterative linear solvers (e.g., the conjugate gradient [62]) the memory consumption grows linearly in the number of variables. The situation is further complicated by the fact that, when re-visiting a place multiple times, factor graph optimization becomes less efficient as nodes and edges are continuously added to the same spatial region, compromising the sparsity structure of the graph.</p>
<p>In this section we review some of the current approaches to control, or at least reduce, the growth of the size of the problem and discuss open challenges.</p>
<p>Brief Survey. We focus on two ways to reduce the complexity of factor graph optimization: (i) sparsification methods, which trade off information loss for memory and computational efficiency, and (ii) out-of-core and multi-robot methods, which split the computation among many robots/processors.</p>
<p>Node and edge sparsification: This family of methods addresses scalability by reducing the number of nodes added to the graph, or by pruning less "informative" nodes and factors. Ila et al. [115] use an information-theoretic approach to add only non-redundant nodes and highly-informative measurements to the graph. Johannsson et al. [120], when possible, avoid adding new nodes to the graph by inducing new constraints between existing nodes, such that the number of variables grows only with size of the explored space and not with the mapping duration. Kretzschmar et al. [141] propose an information-based criterion for determining which nodes to marginalize in pose graph optimization. Carlevaris-Bianco and Eustice [28], and Mazuran et al. [170] introduce the Generic Linear Constraint (GLC) factors and the Nonlinear Graph Sparsification (NGS) method, respectively. These methods</p>
<p>operate on the Markov blanket of a marginalized node and compute a sparse approximation of the blanket. Huang et al. [107] sparsify the Hessian matrix (arising in the normal equations) by solving an $\ell_{1}$-regularized minimization problem.</p>
<p>Another line of work that allows reducing the number of parameters to be estimated over time is the continuous-time trajectory estimation. The first SLAM approach of this class was proposed by Bibby and Reid using cubic-splines to represent the continuous trajectory of the robot [12]. In their approach the nodes in the factor graph represented the control-points (knots) of the spline which were optimized in a sliding window fashion. Later, Furgale et al. [88] proposed the use of basis functions, particularly B-splines, to approximate the robot trajectory, within a batch-optimization formulation. Slidingwindow B-spline formulations were also used in SLAM with rolling shutter cameras, with a landmark-based representation by Patron-Perez et al. [196] and with a semi-dense direct representation by Kim et al. [133]. More recently, Mueggler et al. [177] applied the continuous-time SLAM formulation to event-based cameras. Bosse et al. [21] extended the continuous 3D scan-matching formulation from [19] to a large-scale SLAM application. Later, Anderson et al. [4] and Dubé et al. [67] proposed more efficient implementations by using wavelets or sampling non-uniform knots over the trajectory, respectively. Tong et al. [243] changed the parametrization of the trajectory from basis curves to a Gaussian process representation, where nodes in the factor graph are actual robot poses and any other pose can be interpolated by computing the posterior mean at the given time. An expensive batch GaussNewton optimization is needed to solve for the states in this first proposal. Barfoot et al. [3] then proposed a Gaussian process with an exactly sparse inverse kernel that drastically reduces the computational time of the batch solution.</p>
<p>Out-of-core (parallel) SLAM: Parallel out-of-core algorithms for SLAM split the computation (and memory) load of factor graph optimization among multiple processors. The key idea is to divide the factor graph into different subgraphs and optimize the overall graph by alternating local optimization of each subgraph, with a global refinement. The corresponding approaches are often referred to as submapping algorithms, an idea that dates back to the initial attempts to tackle largescale maps [18]. Ni et al. [187] and Zhao et al. [267] present submapping approaches for factor graph optimization, organizing the submaps in a binary tree structure. Grisetti et al. [98] propose a hierarchy of submaps: whenever an observation is acquired, the highest level of the hierarchy is modified and only the areas which are substantially affected are changed at lower levels. Some methods approximately decouple localization and mapping in two threads that run in parallel like Klein and Murray [135]. Other methods resort to solving different stages in parallel: inspired by [223], Strasdat et al. [235] take a two-stage approach and optimize first a local pose-features graph and then a pose-pose graph; Williams et al. [259] split factor graph optimization in a high-frequency filter and lowfrequency smoother, which are periodically synchronized.</p>
<p>Distributed multi robot SLAM: One way of mapping a large-scale environment is to deploy multiple robots doing SLAM, and divide the scenario in smaller areas, each one
mapped by a different robot. This approach has two main variants: the centralized one, where robots build submaps and transfer the local information to a central station that performs inference [66, 210], and the decentralized one, where there is no central data fusion and the agents leverage local communication to reach consensus on a common map. Nerurkar et al. [181] propose an algorithm for cooperative localization based on distributed conjugate gradient. Araguez et al. [5] investigate consensus-based approaches for map merging. Knuth and Barooah [137] estimate 3D poses using distributed gradient descent. In Lazaro et al. [151], robots exchange portions of their factor graphs, which are approximated in the form of condensed measurements to minimize communication. Cunnigham et al. [54] use Gaussian elimination, and develop an approach, called DDF-SAM, in which each robot exchanges a Gaussian marginal over the separators (i.e., the variables shared by multiple robots). A recent survey on multi-robot SLAM approaches can be found in [216].</p>
<p>While Gaussian elimination has become a popular approach it has two major shortcomings. First, the marginals to be exchanged among the robots are dense, and the communication cost is quadratic in the number of separators. This motivated the use of sparsification techniques to reduce the communication cost [197]. The second reason is that Gaussian elimination is performed on a linearized version of the problem, hence approaches such as DDF-SAM [54] require good linearization points and complex bookkeeping to ensure consistency of the linearization points across the robots. An alternative approach to Gaussian elimination is the Gauss-Seidel approach of Choudhary et al. [47], which implies a communication burden which is linear in the number of separators.</p>
<p>Open Problems. Despite the amount of work to reduce complexity of factor graph optimization, the literature has large gaps on other aspects related to long-term operation.</p>
<p>Map representation: A fairly unexplored question is how to store the map during long-term operation. Even when memory is not a tight constraint, e.g. data is stored on the cloud, raw representations as point clouds or volumetric maps (see also Section V) are wasteful in terms of memory; similarly, storing feature descriptors for vision-based SLAM quickly becomes cumbersome. Some initial solutions have been recently proposed for localization against a compressed known map [163], and for memory-efficient dense reconstruction [136].</p>
<p>Learning, forgetting, remembering: A related open question for long-term mapping is how often to update the information contained in the map and how to decide when this information becomes outdated and can be discarded. When is it fine, if ever, to forget? In which case, what can be forgotten and what is essential to maintain? Can parts of the map be "offloaded" and recalled when needed? While this is clearly taskdependent, no grounded answer to these questions has been proposed in the literature.</p>
<p>Robust distributed mapping: While approaches for outlier rejection have been proposed in the single robot case, the literature on multi robot SLAM barely deals with the problem of outliers. Dealing with spurious measurements is particularly challenging for two reasons. First, the robots might not share a common reference frame, making it harder to detect and</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: Left: feature-based map of a room produced by ORB-SLAM [179]. Right: dense map of a desktop produced by DTAM [184].</p>
<p>rejects wrong loop closures. Second, in the distributed setup, the robots have to detect outliers from very partial and local information. An early attempt to tackle this issue is [84], in which robots actively verify location hypotheses using a rendezvous strategy before fusing information. Indelman <em>et al.</em> [117] propose a probabilistic approach to establish a common reference frame in the face of spurious measurements.</p>
<p><em>Resource-constrained platforms:</em> Another relatively unexplored issue is how to adapt existing SLAM algorithms to the case in which the robotic platforms have <em>severe</em> computational constraints. This problem is of great importance when the size of the platform is scaled down, e.g., mobile phones, micro aerial vehicles, or robotic insects [261]. Many SLAM algorithms are too expensive to run on these platforms, and it would be desirable to have algorithms in which one can tune a "knob" that allows to gently trade off accuracy for computational cost. Similar issues arise in the multi-robot setting: how can we guarantee reliable operation for multi-robot teams when facing tight bandwidth constraints and communication dropout? The "version control" approach of Cieslewski <em>et al.</em> [49] is a first study in this direction.</p>
<h2>V. Representation I: Metric Map Models</h2>
<p>This section discusses how to model geometry in SLAM. More formally, a <em>metric representation</em> (or metric map) is a symbolic structure that encodes the geometry of the environment. We claim that understanding how to choose a suitable metric representation for SLAM (and extending the set of representations currently used in robotics) will impact many research areas, including long-term navigation, physical interaction with the environment, and human-robot interaction.</p>
<p>Geometric modeling appears much simpler in the 2D case, with only two predominant paradigms: <em>landmark-based maps</em> and <em>occupancy grid maps</em>. The former models the environment as a sparse set of landmarks, the latter discretizes the environment in cells and assigns a probability of occupation to each cell. The problem of standardization of these representations in the 2D case has been tackled by the <em>IEEE RAS Map Data Representation Working Group</em>, which recently released a standard for 2D maps in robotics [113]; the standard defines the two main metric representations for planar environments (plus topological maps) in order to facilitate data exchange, benchmarking, and technology transfer.</p>
<p>The question of 3D geometry modeling is more delicate, and the understanding of how to efficiently model 3D geometry during mapping is in its infancy. In this section we review metric representations, taking a broad perspective across robotics, computer vision, computer-aided design (CAD), and computer graphics. Our taxonomy draws inspiration from [80, 209, 221], and includes pointers to more recent work.</p>
<p><strong>Landmark-based sparse representations.</strong> Most SLAM methods represent the scene as a set of <em>sparse</em> 3D landmarks corresponding to discriminative features in the environment (e.g., lines, corners) [179]; one example is shown in Fig. 4(left). These are commonly referred to as <em>landmark-based</em> or <em>feature-based</em> representations, and have been widespread in mobile robotics since early work on localization and mapping, and in computer vision in the context of <em>Structure from Motion</em> [2, 244]. A common assumption underlying these representations is that the landmarks are distinguishable, i.e., sensor data measure some geometric aspect of the landmark, but also provide a <em>descriptor</em> which establishes a (possibly uncertain) data association between each measurement and the corresponding landmark. Previous work also investigates different 3D landmark parameterizations, including global and local Cartesian models, and inverse depth parametrization [174]. While a large body of work focuses on the estimation of point features, the robotics literature includes extensions to more complex geometric landmarks, including lines, segments, or arcs [162].</p>
<p><strong>Low-level raw dense representations.</strong> Contrary to landmark-based representations, dense representations attempt to provide high-resolution models of the 3D geometry; these models are more suitable for obstacle avoidance, or for visualization and rendering, see Fig. 4(right). Among dense models, <em>raw representations</em> describe the 3D geometry by means of a large unstructured set of points (i.e., <em>point clouds</em>) or polygons (i.e., <em>polygon soup</em> [222]). Point clouds have been widely used in robotics, in conjunction with stereo and RGB-D cameras, as well as 3D laser scanners [190]. These representations have recently gained popularity in monocular SLAM, in conjunction with the use of <em>direct methods</em> [118, 184, 203], which estimate the trajectory of the robot and a 3D model directly from the intensity values of all the image pixels. Slightly more complex representations are <em>surfel maps</em>, which encode the geometry as a set of disks [104, 257]. While these representations are visually pleasant, they are usually cumbersome as they require storing a large amount of data. Moreover, they give a low-level description of the geometry, neglecting, for instance, the topology of the obstacles.</p>
<p><strong>Boundary and spatial-partitioning dense representations.</strong> These representations go beyond unstructured sets of low-level primitives (e.g., points) and attempt to explicitly represent surfaces (or <em>boundaries</em>) and volumes. These representations lend themselves better to tasks such as motion or footstep planning, obstacle avoidance, manipulation, and other physics-based reasoning, such as contact reasoning. Boundary representations (b-reps) define 3D objects in terms of their surface boundary. Particularly simple boundary representations are plane-based models, which have been used for mapping by Castle <em>et al.</em> [44] and Kaess [124, 162]. More general b-reps include <em>curve-based representations</em> (e.g., tensor product of NURBS or B-splines), <em>surface mesh models</em> (connected sets</p>
<p>of polygons), and implicit surface representations. The latter specify the surface of a solid as the zero crossing of a function defined on $\mathbb{R}^{3}$ [16]; examples of functions include radial-basis functions [38], signed-distance function [55], and truncated signed-distance function (TSDF) [264]. TSDF are currently a popular representation for vision-based SLAM in robotics, attracting increasing attention after the seminal work [183]. Mesh models have been also used in [257, 258].</p>
<p>Spatial-partitioning representations define 3D objects as a collection of contiguous non-intersecting primitives. The most popular spatial-partitioning representation is the so called spatial-occupancy enumeration, which decomposes the 3D space into identical cubes (voxels), arranged in a regular 3D grid. More efficient partitioning schemes include octree, Polygonal Map octree, and Binary Space-Partitioning tree [80, §12.6]. In robotics, octree representations have been used for 3D mapping [75], while commonly used occupancy grid maps [71] can be considered as probabilistic variants of spatial-partitioning representations. In 3D environments without hanging obstacles, 2.5D elevation maps have been also used [23]. Before moving to higher-level representations, let us better understand how sparse (feature-based) representations (and algorithms) compare to dense ones in visual SLAM.</p>
<p>Which one is best: feature-based or direct methods? Feature-based approaches are quite mature, with a long history of success [59]. They allow to build accurate and robust SLAM systems with automatic relocation and loop closing [179]. However, such systems depend on the availability of features in the environment, the reliance on detection and matching thresholds, and on the fact that most feature detectors are optimized for speed rather than precision. On the other hand, direct methods work with the raw pixel information and dense-direct methods exploit all the information in the image, even from areas where gradients are small; thus, they can outperform feature-based methods in scenes with poor texture, defocus, and motion blur [184, 203]. However, they require high computing power (GPUs) for real-time performance. Furthermore, how to jointly estimate dense structure and motion is still an open problem (currently they can be only be estimated subsequently to one another). To avoid the caveats of feature-based methods there are two alternatives. Semidense methods overcome the high-computation requirement of dense method by exploiting only pixels with strong gradients (i.e., edges) [72, 83]; semi-direct methods instead leverage both sparse features (such as corners or edges) and direct methods [83] and are proven to be the most efficient [83]; additionally, because they rely on sparse features, they allow joint estimation of structure and motion.</p>
<p>High-level object-based representations. While point clouds and boundary representations are currently dominating the landscape of dense mapping, we envision that higher-level representations, including objects and solid shapes, will play a key role in the future of SLAM. Early techniques to include object-based reasoning in SLAM are "SLAM++" from SalasMoreno et al. [217], the work from Civera et al. [50], and Dame et al. [56]. Solid representations explicitly encode the fact that real objects are three-dimensional rather than 1D (i.e., points), or 2D (surfaces). Modeling objects as solid shapes
allows associating physical notions, such as volume and mass, to each object, which is definitely important for robots which have to interact the world. Luckily, existing literature from CAD and computer graphics paved the way towards these developments. In the following, we list few examples of solid representations that have not yet been used in a SLAM context:</p>
<ul>
<li>Parameterized Primitive Instancing: relies on the definition of families of objects (e.g., cylinder, sphere). For each family, one defines a set of parameters (e.g., radius, height), that uniquely identifies a member (or instance) of the family. This representation may be of interest for SLAM since it enables the use of extremely compact models, while still capturing many elements in man-made environments.</li>
<li>Sweep representations: define a solid as the sweep of a 2D or 3D object along a trajectory through space. Typical sweeps representations include translation sweep (or extrusion) and rotation sweep. For instance, a cylinder can be represented as a translation sweep of a circle along an axis that is orthogonal to the plane of the circle. Sweeps of 2D cross-sections are known as generalized cylinders in computer vision [13], and they have been used in robotic grasping [200]. This representation seems particularly suitable to reason on the occluded portions of the scene, by leveraging symmetries.</li>
<li>Constructive solid geometry: defines complex solids by means of boolean operations between primitives [209]. An object is stored as a tree in which the leaves are the primitives and the edges represent operations. This representation can model fairly complicated geometry and is extensively used in computer graphics.</li>
</ul>
<p>We conclude this review by mentioning that other types of representations exist, including feature-based models in CAD [220], dictionary-based representations [266], affordance-based models [134], generative and procedural models [172], and scene graphs [121]. In particular, dictionarybased representations, which define a solid as a combination of atoms in a dictionary, have been considered in robotics and computer vision, with dictionary learned from data [266] or based on existing repositories of object models [149, 157].</p>
<p>Open Problems. The following problems regarding metric representation for SLAM deserve a large amount of fundamental research, and are still vastly unexplored.</p>
<p>High-level, expressive representations in SLAM: While most of the robotics community is currently focusing on point clouds or TSDF to model 3D geometry, these representations have two main drawbacks. First, they are wasteful of memory. For instance, both representations use many parameters (i.e., points, voxels) to encode even a simple environment, such as an empty room (this issue can be partially mitigated by the so-called voxel hashing [188]). Second, these representations do not provide any high-level understanding of the 3D geometry. For instance, consider the case in which the robot has to figure out if it is moving in a room or in a corridor. A point cloud does not provide readily usable information about the type of environment (i.e., room vs. corridor). On the other hand, more sophisticated models (e.g., parameterized primitive instancing) would provide easy ways to discern the two scenes (e.g., by looking at the parameters</p>
<p>defining the primitive). Therefore, the use of higher-level representations in SLAM carries three promises. First, using more compact representations would provide a natural tool for map compression in large-scale mapping. Second, highlevel representations would provide a higher-level description of objects geometry which is a desirable feature to facilitate data association, place recognition, semantic understanding, and human-robot interaction; these representations would also provide a powerful support for SLAM, enabling to reason about occlusions, leverage shape priors, and inform the inference/mapping process of the physical properties of the objects (e.g., weight, dynamics). Finally, using rich 3D representations would enable interactions with existing standards for construction and management of modern buildings, including CityGML [193] and IndoorGML [194]. No SLAM techniques can currently build higher-level representations, beyond point clouds, mesh models, surfels models, and TSDFs. Recent efforts in this direction include [17, 51, 231].</p>
<p>Optimal Representations: While there is a relatively large body of literature on different representations for 3D geometry, few works have focused on understanding which criteria should guide the choice of a specific representation. Intuitively, in simple indoor environments one should prefer parametrized primitives since few parameters can sufficiently describe the 3D geometry; on the other hand, in complex outdoor environments, one might prefer mesh models. Therefore, how should we compare different representations and how should we choose the "optimal" representation? Requicha [209] identifies few basic properties of solid representations that allow comparing different representation. Among these properties we find: domain (the set of real objects that can be represented), conciseness (the "size" of a representation for storage and transmission), ease of creation (in robotics this is the "inference" time required for the construction of the representation), and efficacy in the context of the application (this depends on the tasks for which the representation is used). Therefore, the "optimal" representation is the one that enables preforming a given task, while being concise and easy to create. Soatto and Chiuso [229] define the optimal representation as a minimal sufficient statistics to perform a given task, and its maximal invariance to nuisance factors. Finding a general yet tractable framework to choose the best representation for a task remains an open problem.</p>
<p>Automatic, Adaptive Representations: Traditionally, the choice of a representation has been entrusted to the roboticist designing the system, but this has two main drawbacks. First, the design of a suitable representation is a time-consuming task that requires an expert. Second, it does not allow any flexibility: once the system is designed, the representation of choice cannot be changed; ideally, we would like a robot to use more or less complex representations depending on the task and the complexity of the environment. The automatic design of optimal representations will have a large impact on long-term navigation.</p>
<h2>VI. Representation II: Semantic Map Models</h2>
<p>Semantic mapping consists in associating semantic concepts to geometric entities in a robot's surroundings. Recently, the
limitations of purely geometric maps have been recognized and this has spawned a significant and ongoing body of work in semantic mapping of environments, in order to enhance robot's autonomy and robustness, facilitate more complex tasks (e.g. avoid muddy-road while driving), move from path-planning to task-planning, and enable advanced human-robot interaction [9, 26, 217]. These observations have led to different approaches for semantic mapping which vary in the numbers and types of semantic concepts and means of associating them with different parts of the environments. As an example, Pronobis and Jensfelt [206] label different rooms, while Pillai and Leonard [201] segment several known objects in the map. With the exception of few approaches, semantic parsing at the basic level was formulated as a classification problem, where simple mapping between the sensory data and semantic concepts has been considered.</p>
<p>Semantic vs. topological SLAM. As mentioned in Section I, topological mapping drops the metric information and only leverages place recognition to build a graph in which the nodes represent distinguishable "places", while edges denote reachability among places. We note that topological mapping is radically different from semantic mapping. While the former requires recognizing a previously seen place (disregarding whether that place is a kitchen, a corridor, etc.), the latter is interested in classifying the place according to semantic labels. A comprehensive survey on vision-based topological SLAM is presented in Lowry et al. [160], and some of its challenges are discussed in Section III. In the rest of this section we focus on semantic mapping.</p>
<p>Semantic SLAM: Structure and detail of concepts. The unlimited number of, and relationships among, concepts for humans opens a more philosophical and task-driven decision about the level and organization of the semantic concepts. The detail and organization depend on the context of what, and where, the robot is supposed to perform a task, and they impact the complexity of the problem at different stages. A semantic representation is built by defining the following aspects:</p>
<ul>
<li>Level/Detail of semantic concepts: For a given robotic task, e.g. "going from room A to room B", coarse categories (rooms, corridor, doors) would suffice for a successful performance, while for other tasks, e.g. "pick up a tea cup", finer categories (table, tea cup, glass) are needed.</li>
<li>Organization of semantic concepts: The semantic concepts are not exclusive. Even more, a single entity can have an unlimited number of properties or concepts. A chair can be "movable" and "sittable"; a dinner table can be "movable" and "unsittable". While the chair and the table are pieces of furniture, they share the movable property but with different usability. Flat or hierarchical organizations, sharing or not some properties, have to be designed to handle this multiplicity of concepts.
Brief Survey. There are three main ways to attack semantic mapping, and assign semantic concepts to data.</li>
</ul>
<p>SLAM helps Semantics: The first robotic researchers working on semantic mapping started by the straightforward approach of segmenting the metric map built by a classical SLAM system into semantic concepts. An early work was that of Mozos et al. [176], which builds a geometric map using a</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5: Semantic understanding allows humans to predict changes in the environment at different time scales. For instance, in the construction site shown in the figure, humans account for the motion of the crane and expect the crane-truck not to move in the immediate future, while at the same time we can predict the semblance of the site which will allow us to localize even after the construction finishes. This is possible because we reason on the functional properties and interrelationships of the entities in the environment. Enhancing our robots with similar capabilities is an open problem for semantic SLAM.</p>
<p>2D laser scan and then fuses the classified semantic places from each robot pose through an associative Markov network in an offline manner. Similarly, Lai et al. [148] build a 3D map from RGB-D sequences to then carry out an offline object classification. An online semantic mapping system was later proposed by Pronobis et al. [206], who combine three layers of reasoning (sensory, categorical, and place) to build a semantic map of the environment using laser and camera sensors. More recently, Cadena et al. [26] use motion estimation, and interconnect a coarse semantic segmentation with different object detectors to outperform the individual systems. Pillai and Leonard [201] use a monocular SLAM system to boost the performance in the task of object recognition in videos.</p>
<p>Semantics helps SLAM: Soon after the first semantic maps came out, another trend started by taking advantage of known semantic classes or objects. The idea is that if we can recognize objects or other elements in a map then we can use our prior knowledge about their geometry to improve the estimation of that map. First attempts were done in small scale by Castle et al. [44] and by Civera et al. [50] with a monocular SLAM with sparse features, and by Dame et al. [56] with a dense map representation. Taking advantage of RGB-D sensors, Salas-Moreno et al. [217] propose a SLAM system based on the detection of known objects in the environment.</p>
<p>Joint SLAM and Semantics inference: Researchers with expertise in both computer vision and robotics realized that they could perform monocular SLAM and map segmentation within a joint formulation. The online system of Flint et al. [79] presents a model that leverages the Manhattan world assumption to segment the map in the main planes in indoor scenes. Bao et al. [9] propose one of the first approaches to jointly estimate camera parameters, scene points and object labels using both geometric and semantic attributes in the scene. In their work, the authors demonstrate the improved object recognition performance and robustness, at the cost of a run-time of 20 minutes per image-pair, and the limited number of object categories makes the approach impractical for on-
line robot operation. In the same line, Häne et al. [102] solve a more specialized class-dependent optimization problem in outdoors scenarios. Although still offline, Kundu et al. [147] reduce the complexity of the problem by a late fusion of the semantic segmentation and the metric map, a similar idea was proposed earlier by Sengupta et al. [219] using stereo cameras. It should be noted that [147] and [219] focus only on the mapping part and they do not refine the early computed poses in this late stage. Recently, a promising online system was proposed by Vineet et al. [251] using stereo cameras and a dense map representation.</p>
<p>Open Problems. The problem of including semantic information in SLAM is in its infancy, and, contrary to metric SLAM, it still lacks a cohesive formulation. Fig. 5 shows a construction site as a simple example where we can find the challenges discussed below.</p>
<p>Consistent semantic-metric fusion: Although some progress has been done in terms of temporal fusion of, for instance, per frame semantic evidence [219, 251], the problem of consistently fusing several sources of semantic information with metric information coming at different points in time is still open. Incorporating the confidence or uncertainty of the semantic categorization in the already well known factor graph formulation for the metric representation is a possible way to go for a joint semantic-metric inference framework.</p>
<p>Semantic mapping is much more than a categorization problem: The semantic concepts are evolving to more specialized information such as affordances and actionability ${ }^{4}$ of the entities in the map and the possible interactions among different active agents in the environment. How to represent these properties, and interrelationships, are questions to answer for high level human-robot interaction.</p>
<p>Ignorance, awareness, and adaptation: Given some prior knowledge, the robot should be able to reason about new concepts and their semantic representations, that is, it should be able to discover new objects or classes in the environment, learning new properties as result of active interaction with other robots and humans, and adapting the representations to slow and abrupt changes in the environment over time. For example, suppose that a wheeled-robot needs to classify whether a terrain is drivable or not, to inform its navigation system. If the robot finds some mud on a road, that was previously classified as drivable, the robot should learn a new class depending on the grade of difficulty of crossing the muddy region, or adjust its classifier if another vehicle stuck in the mud is perceived.</p>
<p>Semantic-based reasoning ${ }^{5}$ : As humans, the semantic representations allow us to compress and speed-up reasoning about the environment, while assessing accurate metric representations takes us some effort. Currently, this is not the case for robots. Robots can handle (colored) metric representation but they do not truly exploit the semantic concepts. Our robots</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>are currently unable to effectively, and efficiently localize and continuously map using the semantic concepts (categories, relationships and properties) in the environment. For instance, when detecting a car, a robot should infer the presence of a planar ground under the car (even if occluded) and when the car moves the map update should only refine the hallucinated ground with the new sensor readings. Even more, the same update should change the global pose of the car as a whole in a single and efficient operation as opposed to update, for instance, every single voxel.</p>
<h2>VII. New theoretical tools for SLAM</h2>
<p>This section discusses recent progress towards establishing performance guarantees for SLAM algorithms, and elucidates open problems. The theoretical analysis is important for three main reasons. First, SLAM algorithms and implementations are often tested in few problem instances and it is hard to understand how the corresponding results generalize to new instances. Second, theoretical results shed light on the intrinsic properties of the problem, revealing aspects that may be counter-intuitive during empirical evaluation. Third, a true understanding of the structure of the problem allows pushing the algorithmic boundaries, enabling to extend the set of realworld SLAM instances that can be solved.</p>
<p>Early theoretical analysis of SLAM algorithms were based on the use of EKF; we refer the reader to [64, 255] for a comprehensive discussion, on consistency and observability of EKF SLAM. ${ }^{6}$ Here we focus on factor graph optimization approaches. Besides the practical advantages (accuracy, efficiency), factor graph optimization provides an elegant framework which is more amenable to analysis.</p>
<p>In the absence of priors, MAP estimation reduces to maximum likelihood estimation. Consequently, without priors, SLAM inherits all the properties of maximum likelihood estimators: the estimator in (4) is consistent, asymptotically Gaussian, asymptotically efficient, and invariant to transformations in the Euclidean space [171, Theorems 11-1,2]. Some of these properties are lost in presence of priors (e.g., the estimator is no longer invariant [171, page 193]).</p>
<p>In this context we are more interested in algorithmic properties: does a given algorithm converge to the MAP estimate? How can we improve or check convergence? What is the breakdown point in presence of spurious measurements?</p>
<p>Brief Survey. Most SLAM algorithms are based on iterative nonlinear optimization [63, 99, 125, 126, 192, 204]. SLAM is a nonconvex problem and iterative optimization can only guarantee local convergence. When an algorithm converges to a local minimum ${ }^{7}$ it usually returns an estimate that is completely wrong and unsuitable for navigation (Fig. 6). State-of-the-art iterative solvers fail to converge to a global minimum of the cost for relatively small noise levels [32, 37].</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Failure to converge in iterative methods has triggered efforts towards a deeper understanding of the SLAM problem. Huang and collaborators [110] pioneered this effort, with initial works discussing the nature of the nonconvexity in SLAM. Huang et al. [111] discuss the number of minima in small pose graph optimization problems. Knuth and Barooah [138] investigate the growth of the error in the absence of loop closures. Carlone [29] provides estimates of the basin of convergence for the Gauss-Newton method. Carlone and Censi [32] show that rotation estimation can be solved in closed form in 2D and show that the corresponding estimate is unique. The recent use of alternative maximum likelihood formulations (e.g., assuming Von Mises noise on rotations [34, 211]) has enabled even stronger results. Carlone and Dellaert [31, 36] show that under certain conditions (strong duality) that are often encountered in practice, the maximum likelihood estimate is unique and pose graph optimization can be solved globally, via (convex) semidefinite programming (SDP). A very recent overview on theoretical aspects of SLAM is given in [109].</p>
<p>As mentioned earlier, the theoretical analysis is sometimes the first step towards the design of better algorithms. Besides the dual SDP approach of [31, 36], other authors proposed convex relaxation to avoid convergence to local minima. These contributions include the work of Liu et al. [159] and Rosen et al. [211]. Another successful strategy to improve convergence consists in computing a suitable initialization for iterative nonlinear optimization. In this regard, the idea of solving for the rotations first and to use the resulting estimate to bootstrap nonlinear iteration has been demonstrated to be very effective in practice [20, 30, 32, 37]. Khosoussi et al. [130] leverage the (approximate) separability between translation and rotation to speed up optimization.</p>
<p>Recent theoretical results on the use of Lagrangian duality in SLAM also enabled the design of verification techniques: given a SLAM estimate these techniques are able to judge whether such estimate is optimal or not. Being able to ascertain the quality of a given SLAM solution is crucial to design failure detection and recovery strategies for safetycritical applications. The literature on verification techniques for SLAM is very recent: current approaches [31, 36] are able to perform verification by solving a sparse linear system and are guaranteed to provide a correct answer as long as strong duality holds (more on this point later).</p>
<p>We note that these results, proposed in a robotics context, provide a useful complement to related work in other communities, including localization in multi agent systems [46, 199, 202, 245, 254], structure from motion in computer vision [86, $95,103,168]$, and cryo-electron microscopy [224, 225].</p>
<p>Open Problems. Despite the unprecedented progress of the last years, several theoretical questions remain open.</p>
<p>Generality, guarantees, verification: The first question regards the generality of the available results. Most results on guaranteed global solutions and verification techniques have been proposed in the context of pose graph optimization. Can these results be generalized to arbitrary factor graphs? Moreover, most theoretical results assume the measurement noise to be isotropic or at least to be structured. Can we generalize existing results to arbitrary noise models?</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6: The back-bone of most SLAM algorithms is the MAP estimation of the robot trajectory, which is computed via non-convex optimization. The figure shows trajectory estimates for two simulated benchmarking problems, namely sphere-a and torus, in which the robot travels on the surface of a sphere and a torus. The top row reports the correct trajectory estimate, corresponding to the global optimum of the optimization problem. The bottom row shows incorrect trajectory estimates resulting from convergence to local minima. Recent theoretical tools are enabling detection of wrong convergence episodes, and are opening avenues for failure detection and recovery techniques.</p>
<p>Weak or Strong duality? The works [31, 36] show that when strong duality holds SLAM can be solved globally; moreover, they provide empirical evidence that strong duality holds in most problem instances encountered in practical applications. The outstanding problem consists in establishing a priori conditions under which strong duality holds. We would like to answer the question "given a set of sensors (and the corresponding measurement noise statistics) and a factor graph structure, does strong duality hold?". The capability to answer this question would define the domain of applications in which we can compute (or verify) global solutions to SLAM. This theoretical investigation would also provide fundamental insights in sensor design and active SLAM (Section VIII).</p>
<p>Resilience to outliers: The third question regards estimation in the presence of spurious measurements. While recent results provide strong guarantees for pose graph optimization, no result of this kind applies in the presence of outliers. Despite the work on robust SLAM (Section III) and new modeling tools for the non-Gaussian noise case [212], the design of global techniques that are resilient to outliers and the design of verification techniques that can certify the correctness of a given estimate in presence of outliers remain open.</p>
<h2>VIII. ACTIVE SLAM</h2>
<p>So far we described SLAM as an estimation problem that is carried out passively by the robot, i.e. the robot performs SLAM given the sensor data, but without acting deliberately to collect it. In this section we discuss how to leverage a robot's motion to improve the mapping and localization results.</p>
<p>The problem of controlling robot's motion in order to minimize the uncertainty of its map representation and localization is usually named active SLAM. This definition stems from the well-known Bajcsy's active perception [8] and Thrun's robotic exploration [240, Ch. 17] paradigms.</p>
<p>Brief Survey. The first proposal and implementation of an active SLAM algorithm can be traced back to Feder [77] while the name was coined in [152]. However, active SLAM has its roots in ideas from artificial intelligence and robotic exploration that can be traced back to the early eighties (c.f. [10]). Thrun in [239] concluded that solving the exploration-exploitation dilemma, i.e., finding a balance between visiting new places (exploration) and reducing the uncertainty by re-visiting known areas (exploitation), provides a more efficient alternative with respect to random exploration or pure exploitation.</p>
<p>Active SLAM is a decision making problem and there are several general frameworks for decision making that can be used as backbone for exploration-exploitation decisions. One of these frameworks is the Theory of Optimal Experimental Design (TOED) [198] which, applied to active SLAM [41, 43], allows selecting future robot action based on the predicted map uncertainty. Information theoretic [164, 208] approaches have been also applied to active SLAM [40, 232]; in this case decision making is usually guided by the notion of information gain. Control theoretic approaches for active SLAM include the use of Model Predictive Control [152, 153]. A different body of works formulates active SLAM under the formalism of Partially Observably Markov Decision Process (POMDP) [123], which in general is known to be computationally intractable; approximate but tractable solutions for active SLAM include Bayesian Optimization [169] or efficient Gaussian beliefs propagation [195], among others.</p>
<p>A popular framework for active SLAM consists of selecting the best future action among a finite set of alternatives. This family of active SLAM algorithms proceeds in three main steps [15, 35]: 1) The robot identifies possible locations to explore or exploit, i.e. vantage locations, in its current estimate of the map; 2) The robot computes the utility of visiting each vantage point and selects the action with the highest utility; and 3) The robot carries out the selected action and decides if it is necessary to continue or to terminate the task. In the following, we discuss each point in details.</p>
<p>Selecting vantage points: Ideally, a robot executing an active SLAM algorithm should evaluate every possible action in the robot and map space, but the computational complexity of the evaluation grows exponentially with the search space which proves to be computationally intractable in real applications [24, 169]. In practice, a small subset of locations in the map is selected, using techniques such as frontier-based exploration [127, 262]. Recent works [250] and [116] have proposed approaches for continuous-space planning under uncertainty that can be used for active SLAM; currently these approaches can only guarantee convergence to locally optimal policies. Another recent continuous-domain avenue for active SLAM algorithms is the use of potential fields. Some examples are [249], which uses convolution techniques to compute entropy and select the robot's actions, and [122], which resorts to the solution of a boundary value problem.</p>
<p>Computing the utility of an action: Ideally, to compute the utility of a given action the robot should reason about the evolution of the posterior over the robot pose and the map, taking into account future (controllable) actions and future</p>
<p>(unknown) measurements. If such posterior were known, an information-theoretic function, as the information gain, could be used to rank the different actions [22, 233]. However, computing this joint probability analytically is, in general, computationally intractable [35, 76, 233]. In practice, one resorts to approximations. Initial work considered the uncertainty of the map and the robot to be independent [246] or conditionally independent [233]. Most of these approaches define the utility as a linear combination of metrics that quantify robot and map uncertainties [22, 35]. One drawback of this approach is that the scale of the numerical values of the two uncertainties is not comparable, i.e. the map uncertainty is often orders of magnitude larger than the robot one, so manual tuning is required to correct it. Approaches to tackle this issue have been proposed for particle-filter-based SLAM [35], and for pose graph optimization [40].</p>
<p>The Theory of Optimal Experimental Design (TOED) [198] can also be used to account for the utility of performing an action. In the TOED, every action is considered as a stochastic design, and the comparison among designs is done using their associated covariance matrices via the so-called optimality criteria, e.g. A-opt, D-opt and E-opt. A study about the usage of optimality criteria in active SLAM can be found in [42, 43].</p>
<p>Executing actions or terminating exploration: While executing an action is usually an easy task, using well-established techniques from motion planning, the decision on whether or not the exploration task is complete, is currently an open challenge that we discuss in the following paragraph.</p>
<p>Open Problems. Several problems still need to be addressed, for active SLAM to have impact in real applications.</p>
<p>Fast and accurate predictions of future states: In active SLAM each action of the robot should contribute to reduce the uncertainty in the map and improve the localization accuracy; for this purpose, the robot must be able to forecast the effect of future actions on the map and robots localization. The forecast has to be fast to meet latency constraints and precise to effectively support the decision process. In the SLAM community it is well known that loop closings are important to reduce uncertainty and to improve localization and mapping accuracy. Nonetheless, efficient methods for forecasting the occurrence and the effect of a loop closing are yet to be devised. Moreover, predicting the effects of future actions is still a computational expensive task [116]. Recent approaches to forecasting future robot states can be found in the machine learning literature, and involve the use of spectral techniques [230] and deep learning [252].</p>
<p>Enough is enough: When do you stop doing active SLAM? Active SLAM is a computationally expensive task: therefore a natural question is when we can stop doing active SLAM and switch to classical (passive) SLAM in order to focus resources on other tasks. Balancing active SLAM decisions and exogenous tasks is critical, since in most real-world tasks, active SLAM is only a means to achieve an intended goal. Additionally, having a stopping criteria is a necessity because at some point it is provable that more information would lead not only to a diminishing return effect but also, in case of contradictory information, to an unrecoverable state (e.g. several wrong loop closures). Uncertainty metrics from</p>
<p>TOED, which are task oriented, seem promising as stopping criteria, compared to information-theoretic metrics which are difficult to compare across systems [39].</p>
<p>Performance guarantees: Another important avenue is to look for mathematical guarantees for active SLAM and for near-optimal policies. Since solving the problem exactly is intractable, it is desirable to have approximation algorithms with clear performance bounds. Examples of this kind of effort is the use of submodularity [93] in the related field of active sensors placement.</p>
<h2>IX. New Frontiers: Sensors and Learning</h2>
<p>The development of new sensors and the use of new computational tools have often been key drivers for SLAM. Section IX-A reviews unconventional and new sensors, as well as the challenges and opportunities they pose in the context of SLAM. Section IX-B discusses the role of (deep) learning as an important frontier for SLAM, analyzing the possible ways in which this tool is going to improve, affect, or even restate, the SLAM problem.</p>
<h2>A. New and Unconventional Sensors for SLAM</h2>
<p>Besides the development of new algorithms, progress in SLAM (and mobile robotics in general) has often been triggered by the availability of novel sensors. For instance, the introduction of 2D laser range finders enabled the creation of very robust SLAM systems, while 3D lidars have been a main thrust behind recent applications, such as autonomous cars. In the last ten years, a large amount of research has been devoted to vision sensors, with successful applications in augmented reality and vision-based navigation.</p>
<p>Sensing in robotics has been mostly dominated by lidars and conventional vision sensors. However, there are many alternative sensors that can be leveraged for SLAM, such as depth, light-field, and event-based cameras, which are now becoming a commodity hardware, as well as magnetic, olfaction, and thermal sensors.</p>
<p>Brief Survey. We review the most relevant new sensors and their applications for SLAM, postponing a discussion on open problems to the end of this section.</p>
<p>Range cameras: Light-emitting depth cameras are not new sensors, but they became commodity hardware in 2010 with the advent the Microsoft Kinect game console. They operate according to different principles, such as structured light, time of flight, interferometry, or coded aperture. Structurelight cameras work by triangulation; thus, their accuracy is limited by the distance between the cameras and the pattern projector (structured light). By contrast, the accuracy of Time-of-Flight (ToF) cameras only depends on the time-of-flight measurement device; thus, they provide the highest range accuracy (sub millimeter at several meters). ToF cameras became commercially available for civil applications around the year 2000 but only began to be used in mobile robotics in 2004 [256]. While the first generation of ToF and structuredlight cameras was characterized by low signal-to-noise ratio and high price, they soon became popular for video-game applications, which contributed to making them affordable</p>
<p>and improving their accuracy. Since range cameras carry their own light source, they also work in dark and untextured scenes, which enabled the achievement of remarkable SLAM results [183].</p>
<p>Light-field cameras: Contrary to standard cameras, which only record the light intensity hitting each pixel, a light-field camera (also known as plenoptic camera), records both the intensity and the direction of light rays [186]. One popular type of light-field camera uses an array of micro lenses placed in front of a conventional image sensor to sense intensity, color, and directional information. Because of the manufacturing cost, commercially available light-field cameras still have relatively low resolution ( $&lt;1 \mathrm{MP}$ ), which is being overcome by current technological effort. Light-field cameras offer several advantages over standard cameras, such as depth estimation, noise reduction [57], video stabilization [227], isolation of distractors [58], and specularity removal [119]. Their optics also offers wide aperture and wide depth of field compared with conventional cameras [14].</p>
<p>Event-based cameras: Contrarily to standard frame-based cameras, which send entire images at fixed frame rates, event-based cameras, such as the Dynamic Vision Sensor (DVS) [156] or the Asynchronous Time-based Image Sensor (ATIS) [205], only send the local pixel-level changes caused by movement in a scene at the time they occur.</p>
<p>They have five key advantages compared to conventional frame-based cameras: a temporal latency of 1 ms , an update rate of up to 1 MHz , a dynamic range of up to 140 dB (vs 60 70 dB of standard cameras), a power consumption of 20 mW (vs 1.5 W of standard cameras), and very low bandwidth and storage requirements (because only intensity changes are transmitted). These properties enable the design of a new class of SLAM algorithms that can operate in scenes characterized by high-speed motion [89] and high-dynamic range [132, 207], where standard cameras fail. However, since the output is composed of a sequence of asynchronous events, traditional frame-based computer-vision algorithms are not applicable. This requires a paradigm shift from the traditional computer vision approaches developed over the last fifty years. Eventbased real-time localization and mapping algorithms have recently been proposed [132, 207]. The design goal of such algorithms is that each incoming event can asynchronously change the estimated state of the system, thus, preserving the event-based nature of the sensor and allowing the design of microsecond-latency control algorithms [178].</p>
<p>Open Problems. The main bottleneck of active range cameras is the maximum range and interference with other external light sources (such as sun light); however, these weaknesses can be improved by emitting more light power.</p>
<p>Light-field cameras have been rarely used in SLAM because they are usually thought to increase the amount of data produced and require more computational power. However, recent studies have shown that they are particularly suitable for SLAM applications because they allow formulating the motion estimation problem as a linear optimization and can provide more accurate motion estimates if designed properly [65].</p>
<p>Event-based cameras are revolutionary image sensors that overcome the limitations of standard cameras in scenes char-
acterized by high dynamic range and high speed motion. Open problems concern a full characterization of the sensor noise and sensor non idealities: event-based cameras have a complicated analog circuitry, with nonlinearities and biases that can change the sensitivity of the pixels, and other dynamic properties, which make the events susceptible to noise. Since a single event does not carry enough information for state estimation and because an event camera generate on average 100,000 events a second, it can become intractable to do SLAM at the discrete times of the single events due to the rapidly growing size of the state space. Using a continuoustime framework [12], the estimated trajectory can be approximated by a smooth curve in the space of rigid-body motions using basis functions (e.g., cubic splines), and optimized according to the observed events [177]. While the temporal resolution is very high, the spatial resolution of event-based cameras is relatively low (QVGA), which is being overcome by current technological effort [155]. Newly developed event sensors overcome some of the original limitations: an ATIS sensor sends the magnitude of the pixel-level brightness; a DAVIS sensor [155] can output both frames and events (this is made possible by embedding a standard frame-based sensor and a DVS into the same pixel array). This will allow tracking features and motion in the blind time between frames [144].</p>
<p>We conclude this section with some general observations on the use of novel sensing modalities for SLAM.</p>
<p>Other sensors: Most SLAM research has been devoted to range and vision sensors. However, humans or animals are able to improve their sensing capabilities by using tactile, olfaction, sound, magnetic, and thermal stimuli. For instance, tactile cues are used by blind people or rodents for haptic exploration of objects, olfaction is used by bees to find their way home, magnetic fields are used by homing pigeons for navigation, sound is used by bats for obstacle detection and navigation, while some snakes can see infrared radiation emitted by hot objects. Unfortunately, these alternative sensors have not been considered in the same depth as range and vision sensors to perform SLAM. Haptic SLAM can be used for tactile exploration of an object or of a scene [237, 263]. Olfaction sensors can be used to localize gas or other odor sources [167]. Although ultrasound-based localization was predominant in early mobile robots, their use has rapidly declined with the advent of cheap optical range sensors. Nevertheless, animals, such as bats, can navigate at very high speeds using only echo localization. Thermal sensors offer important cues at night and in adverse weather conditions [165]. Local anomalies of the ambient magnetic field, present in many indoor environments, offer an excellent cue for localization [248]. Finally, preexisting wireless networks, such as WiFi , can be used to improve robot navigation without any prior knowledge of the location of the antennas [78].</p>
<p>Which sensor is best for SLAM? A question that naturally arises is: what will be the next sensor technology to drive future long-term SLAM research? Clearly, the performance of a given algorithm-sensor pair for SLAM depends on the sensor and algorithm parameters, and on the environment [228]. A complete treatment of how to choose algorithms and sensors to achieve the best performance has not been found</p>
<p>yet. A preliminary study by Censi et al. [45], has shown that the performance for a given task also depends on the power available for sensing. It also suggests that the optimal sensing architecture may have multiple sensors that might be instantaneously switched on and off according to the required performance level or measure the same phenomenon through different physical principles for robustness [68].</p>
<h2>B. Deep Learning</h2>
<p>It would be remiss of a paper that purports to consider future directions in SLAM not to make mention of deep learning. Its impact in computer vision has been transformational, and, at the time of writing this article, it is already making significant inroads into traditional robotics, including SLAM.</p>
<p>Researchers have already shown that it is possible to learn a deep neural network to regress the inter-frame pose between two images acquired from a moving robot directly from the original image pair [52], effectively replacing the standard geometry of visual odometry. Likewise it is possible to localize the 6 DoF of a camera with regression forest [247] and with deep convolutional neural network [129], and to estimate the depth of a scene (in effect, the map) from a single view solely as a function of the input image [27, 70, 158].
This does not, in our view, mean that traditional SLAM is dead, and it is too soon to say whether these methods are simply curiosities that show what can be done in principle, but which will not replace traditional, well-understood methods, or if they will completely take over.</p>
<p>Open Problems. We highlight here a set of future directions for SLAM where we believe machine learning and more specifically deep learning will be influential, or where the SLAM application will throw up challenges for deep learning.</p>
<p>Perceptual tool: It is clear that some perceptual problems that have been beyond the reach of off-the-shelf computer vision algorithms can now be addressed. For example, object recognition for the imagenet classes [213] can now, to an extent, be treated as a black box that works well from the perspective of the roboticist or SLAM researcher. Likewise semantic labeling of pixels in a variety of scene types reaches performance levels of around $80 \%$ accuracy or more [74]. We have already commented extensively on a move towards more semantically meaningful maps for SLAM systems, and these black-box tools will hasten that. But there is more at stake: deep networks show more promise for connecting raw sensor data to understanding, or connecting raw sensor data to actions, than anything that has preceded them.</p>
<p>Practical deployment: Successes in deep learning have mostly revolved around lengthy training times on supercomputers and inference on special-purpose GPU hardware for a one-off result. A challenge for SLAM researchers (or indeed anyone who wants to embed the impressive results in their system) is how to provide sufficient computing power in an embedded system. Do we simply wait for the technology to catch up, or do we investigate smaller, cheaper networks that can produce "good enough" results, and consider the impact of sensing over an extended period?</p>
<p>Online and life-long learning: An even greater and important challenge is that of online learning and adaptation, that will be essential to any future long-term SLAM system. SLAM systems typically operate in an open-world with continuous observation, where new objects and scenes can be encountered. But to date deep networks are usually trained on closedworld scenarios with, say, a fixed number of object classes. A significant challenge is to harness the power of deep networks in a one-shot or zero-shot scenario (i.e. one or even zero training examples of a new class) to enable life-long learning for a continuously moving, continuously observing SLAM system.</p>
<p>Similarly, existing networks tend to be trained on a vast corpus of labelled data, yet it cannot always be guaranteed that a suitable dataset exists or is practical to label for the supervised training. One area where some progress has recently been made is that of single-view depth estimation: Garg et al. [90] have recently shown how a deep network for single-view depth estimation can be trained simply by observing a large corpus of stereo pairs, without the need to observe or calculate depth explicitly. It remains to be seen if similar methods can be developed for tasks such as semantic scene labelling.</p>
<p>Bootstrapping: Prior information about a scene has increasingly been shown to provide a significant boost to SLAM systems. Examples in the literature to date include known objects [56, 217] or prior knowledge about the expected structure in the scene, like smoothness as in DTAM [184], Manhattan constraints as in [79], or even the expected relationships between objects [9]. It is clear that deep learning is capable of distilling such prior knowledge for specific tasks such as estimating scene labels or scene depths. How best to extract and use this information is a significant open problem. It is more pertinent in SLAM than in some other fields because in SLAM we have solid grasp of the mathematics of the scene geometry - the question then is how to fuse this wellunderstood geometry with the outputs of a deep network. One particular challenge that must be solved is to characterize the uncertainty of estimates derived from a deep network.</p>
<p>SLAM offers a challenging context for exploring potential connections between deep learning architectures and recursive state estimation in large-scale graphical models. For example, Krishan et al. [142] have recently proposed Deep Kalman Filters; perhaps it might one day be possible to create an end-to-end SLAM system using a deep architecture, without explicit feature modeling, data association, etc.</p>
<h2>X. CONCLUSION</h2>
<p>The problem of simultaneous localization and mapping has seen great progress over the last 30 years. Along the way, several important questions have been answered, while many new and interesting questions have been raised, with the development of new applications, new sensors, and new computational tools.</p>
<p>Revisiting the question "is SLAM necessary?", we believe the answer depends on the application, but quite often the answer is a resounding yes. SLAM and related techniques, such</p>
<p>as visual-inertial odometry, are being increasingly deployed in a variety of real-world settings, from self-driving cars to mobile devices. SLAM techniques will be increasingly relied upon to provide reliable metric positioning in situations where infrastructure-based solutions such as GPS are unavailable or do not provide sufficient accuracy. One can envision cloudbased location-as-a-service capabilities coming online, and maps becoming commoditized, due to the value of positioning information for mobile devices and agents.</p>
<p>In some applications, such as self-driving cars, precision localization is often performed by matching current sensor data to a high definition map of the environment that is created in advance [154]. If the a priori map is accurate, then online SLAM is not required. Operations in highly dynamic environments, however, will require dynamic online map updates to deal with construction or major changes to road infrastructure. The distributed updating and maintenance of visual maps created by large fleets of autonomous vehicles is a compelling area for future work.</p>
<p>One can identify tasks for which different flavors of SLAM formulations are more suitable than others. For instance, a topological map can be used to analyze reachability of a given place, but it is not suitable for motion planning and low-level control; a locally-consistent metric map is well-suited for obstacle avoidance and local interactions with the environment, but it may sacrifice accuracy; a globally-consistent metric map allows the robot to perform global path planning, but it may be computationally demanding to compute and maintain.</p>
<p>One may even devise examples in which SLAM is unnecessary altogether and can be replaced by other techniques, e.g., visual servoing for local control and stabilization, or "teach and repeat" to perform repetitive navigation tasks. A more general way to choose the most appropriate SLAM system is to think about SLAM as a mechanism to compute a sufficient statistic that summarizes all past observations of the robot, and in this sense, which information to retain in this compressed representation is deeply task-dependent.</p>
<p>As to the familiar question "is SLAM solved?", in this position paper we argue that, as we enter the robust-perception age, the question cannot be answered without specifying a robot/environment/performance combination. For many applications and environments, numerous major challenges and important questions remain open. To achieve truly robust perception and navigation for long-lived autonomous robots, more research in SLAM is needed. As an academic endeavor with important real-world implications, SLAM is not solved.</p>
<p>The unsolved questions involve four main aspects: robust performance, high-level understanding, resource awareness, and task-driven inference. From the perspective of robustness, the design of fail-safe, self-tuning SLAM systems is a formidable challenge with many aspects being largely unexplored. For long-term autonomy, techniques to construct and maintain large-scale time-varying maps, as well as policies that define when to remember, update, or forget information, still need a large amount of fundamental research; similar problems arise, at a different scale, in severely resource-constrained robotic systems.</p>
<p>Another fundamental question regards the design of metric
and semantic representations for the environment. Despite the fact that the interaction with the environment is paramount for most applications of robotics, modern SLAM systems are not able to provide a tightly-coupled high-level understanding of the geometry and the semantic of the surrounding world; the design of such representations must be task-driven and currently a tractable framework to link task to optimal representations is lacking. Developing such a framework will bring together the robotics and computer vision communities.</p>
<p>Besides discussing many accomplishments and future challenges for the SLAM community, we also examined opportunities connected to the use of novel sensors, new tools (e.g., convex relaxations and duality theory, or deep learning), and the role of active sensing. SLAM still constitutes an indispensable backbone for most robotics applications and despite the amazing progress over the past decades, existing SLAM systems are far from providing insightful, actionable, and compact models of the environment, comparable to the ones effortlessly created and used by humans.</p>
<h2>ACKNOWLEDGMENT</h2>
<p>We would like to thank all the contributors and members of the Google Plus community [25], especially Liam Paul, Ankur Handa, Shane Griffith, Hilario Tomé, Andrea Censi, and Raul Mur Artal, for their inputs towards the discussion of the open problems in SLAM. We are also thankful to all the speakers of the RSS workshop [25]: Mingo Tardos, Shoudong Huang, Ryan Eustice, Patrick Pfaff, Jason Meltzer, Joel Hesch, Esha Nerurkar, Richard Newcombe, Zeeshan Zia, Andreas Geiger. We are also indebted to Guoquan (Paul) Huang and Udo Frese for the discussions during the preparation of this document, and Guillermo Gallego and Jeff Delmerico for their early comments on this document.</p>
<h2>REFERENCES</h2>
<p>[1] P. A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton University Press, 2007.
[2] S. Agarwal, N. Snavely, I. Simon, S. M. Seitz, and R. Szeliski. Bundle adjustment in the large. In European Conference on Computer Vision (ECCV), pages 29-42. Springer, 2010.
[3] S. Anderson, T. D. Barfoot, C. H. Tong, and S. Särkkä. Batch Nonlinear Continuous-time Trajectory Estimation As Exactly Sparse Gaussian Process Regression. Autonomous Robots (AR), 39(3):221-238, Oct. 2015.
[4] S. Anderson, F. Dellaert, and T. D. Barfoot. A hierarchical wavelet decomposition for continuous-time SLAM. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 373-380. IEEE, 2014.
[5] R. Aragues, J. Cortes, and C. Sagues. Distributed Consensus on Robot Networks for Dynamically Merging Feature-Based Maps. IEEE Transactions on Robotics (TRO), 28(4):840-854, 2012.
[6] J. Aulinas, Y. Petillot, J. Salvi, and X. Lladó. The SLAM Problem: A Survey. In Proceedings of the International Conference of the Catalan Association for Artificial Intelligence, pages 363-371. IOS Press, 2008.
[7] T. Bailey and H. F. Durrant-Whyte. Simultaneous Localisation and Mapping (SLAM): Part II. Robotics and Autonomous Systems (RAS), 13(3):108-117, 2006.
[8] R. Bajcsy. Active Perception. Proceedings of the IEEE, 76(8):9661005, 1988.
[9] S. Y. Bao, M. Bagra, Y. W. Chao, and S. Savarese. Semantic structure from motion with points, regions, and objects. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2703-2710. IEEE, 2012.
[10] A. G. Barto and R. S. Sutton. Goal seeking components for adaptive intelligence: An initial assessment. Technical Report Technical Report</p>
<p>AFWAL-TR-81-1070, Air Force Wright Aeronautical Laboratories. Wright-Patterson Air Force Base, Jan. 1981.
[11] C. Bibby and I. Reid. Simultaneous Localisation and Mapping in Dynamic Environments (SLAMIDE) with Reversible Data Association. In Proceedings of Robotics: Science and Systems Conference (RSS), pages 105-112, Atlanta, GA, USA, June 2007.
[12] C. Bibby and I. D. Reid. A hybrid SLAM representation for dynamic marine environments. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 1050-4729. IEEE, 2010.
[13] T. O. Binford. Visual perception by computer. In IEEE Conference on Systems and Controls. IEEE, 1971.
[14] T. E. Bishop and P. Favaro. The light field camera: Extended depth of field, aliasing, and superresolution. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(5):972-986, 2012.
[15] J. L. Blanco, J. A. Fernández-Madrigal, and J. Gonzalez. A Novel Measure of Uncertainty for Mobile Robot SLAM with RaoBlackwellized Particle Filters. The International Journal of Robotics Research (IJRR), 27(1):73-89, 2008.
[16] J. Bloomerthal. Introduction to Implicit Surfaces. Morgan Kaufmann, 1997.
[17] A. Bodis-Szomoru, H. Riemenschneider, and L. Van-Gool. Efficient edge-aware surface mesh reconstruction for urban scenes. Journal of Computer Vision and Image Understanding, 66:91-106, 2015.
[18] M. Bosse, P. Newman, J. J. Leonard, and S. Teller. Simultaneous Localization and Map Building in Large-Scale Cyclic Environments Using the Atlas Framework. The International Journal of Robotics Research (IJRR), 23(12):1113-1139, 2004.
[19] M. Bosse and R. Zlot. Continuous 3D Scan-Matching with a Spinning 2D Laser. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 4312-4319. IEEE, 2009.
[20] M. Bosse and R. Zlot. Keypoint design and evaluation for place recognition in 2D LIDAR maps. Robotics and Autonomous Systems (RAS), 57(12):1211-1224, 2009.
[21] M. Bosse, R. Zlot, and P. Flick. Zebedee: Design of a SpringMounted 3D Range Sensor with Application to Mobile Mapping. IEEE Transactions on Robotics (TRO), 28(5):1104-1119, 2012.
[22] F. Bourgault, A. A. Makarenko, S. B. Williams, B. Grocholsky, and H. F. Durrant-Whyte. Information Based Adaptive Robotic Exploration. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 540-545. IEEE, 2002.
[23] C. Brand, M. J. Schuster, H. Hirschmuller, and M. Suppa. Stereo-Vision Based Obstacle Mapping for Indoor/Outdoor SLAM. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2153-0858. IEEE, 2014.
[24] W. Burgard, M. Moors, C. Stachniss, and F. E. Schneider. Coordinated Multi-Robot Exploration. IEEE Transactions on Robotics (TRO), 21(3):376-386, 2005.
[25] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, J. Neira, I. D. Reid, and J. J. Leonard. Robotics: Science and Systems (RSS), Workshop "The problem of mobile sensors: Setting future goals and indicators of progress for SLAM". http://ylatif.github.io/ movingsensors/, Google Plus Community: https://plus.google.com/ communities/102832228492942322585, June 2015.
[26] C. Cadena, A. Dick, and I. D. Reid. A Fast, Modular Scene Understanding System using Context-Aware Object Detection. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 4859-4866. IEEE, 2015.
[27] C. Cadena, A. Dick, and I. D. Reid. Multi-modal Auto-Encoders as Joint Estimators for Robotics Scene Understanding. In Proceedings of Robotics: Science and Systems Conference (RSS), pages 377-386, 2016.
[28] N. Carlevaris-Bianco and R. M. Eustice. Generic factor-based node marginalization and edge sparsification for pose-graph SLAM. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 5748-5755. IEEE, 2013.
[29] L. Carlone. Convergence Analysis of Pose Graph Optimization via Gauss-Newton methods. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 965-972. IEEE, 2013.
[30] L. Carlone, R. Aragues, J. A. Castellanos, and B. Bona. A fast and accurate approximation for planar pose graph optimization. The International Journal of Robotics Research (IJRR), 33(7):965-987, 2014.
[31] L. Carlone, G. Calafiore, C. Tommolillo, and F. Dellaert. Planar Pose Graph Optimization: Duality, Optimal Solutions, and Verification. IEEE Transactions on Robotics (TRO), 32(3):545-565, 2016.
[32] L. Carlone and A. Censi. From Angular Manifolds to the Integer Lattice: Guaranteed Orientation Estimation With Application to Pose Graph Optimization. IEEE Transactions on Robotics (TRO), 30(2):475492, 2014.
[33] L. Carlone, A. Censi, and F. Dellaert. Selecting good measurements via $\ell_{1}$ relaxation: a convex approach for robust estimation over graphs. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2667 -2674. IEEE, 2014.
[34] L. Carlone and F. Dellaert. Duality-based Verification Techniques for 2D SLAM. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 4589-4596. IEEE, 2015.
[35] L. Carlone, J. Du, M. Kaouk, B. Bona, and M. Indri. Active SLAM and Exploration with Particle Filters Using Kullback-Leibler Divergence. Journal of Intelligent \&amp; Robotic Systems, 75(2):291-311, 2014.
[36] L. Carlone, D. Rosen, G. Calafiore, J. J. Leonard, and F. Dellaert. Lagrangian Duality in 3D SLAM: Verification Techniques and Optimal Solutions. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 125-132. IEEE, 2015.
[37] L. Carlone, R. Tron, K. Daniilidis, and F. Dellaert. Initialization Techniques for 3D SLAM: a Survey on Rotation Estimation and its Use in Pose Graph Optimization. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 4597-4604. IEEE, 2015.
[38] J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell, W. R. Fright, B. C. McCallum, and T. R. Evans. Reconstruction and Representation of 3D Objects with Radial Basis Functions. In SIGGRAPH, pages 67-76. ACM, 2001.
[39] H. Carrillo, O. Birbach, H. Taubig, B. Bauml, U. Frese, and J. A. Castellanos. On Task-oriented Criteria for Configurations Selection in Robot Calibration. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 3653-3659. IEEE, 2013.
[40] H. Carrillo, P. Dames, K. Kumar, and J. A. Castellanos. Autonomous Robotic Exploration Using Occupancy Grid Maps and Graph SLAM Based on Shannon and Rényi Entropy. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 487-494. IEEE, 2015.
[41] H. Carrillo, Y. Latif, J. Neira, and J. A. Castellanos. Fast Minimum Uncertainty Search on a Graph Map Representation. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2504-2511. IEEE, 2012.
[42] H. Carrillo, Y. Latif, M. L. Rodríguez, J. Neira, and J. A. Castellanos. On the Monotonicity of Optimality Criteria during Exploration in Active SLAM. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 1476-1483. IEEE, 2015.
[43] H. Carrillo, I. Reid, and J. A. Castellanos. On the Comparison of Uncertainty Criteria for Active SLAM. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 2080-2087. IEEE, 2012.
[44] R. O. Castle, D. J. Gawley, G. Klein, and D. W. Murray. Towards simultaneous recognition, localization and mapping for hand-held and wearable cameras. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 4102-4107. IEEE, 2007.
[45] A. Censi, E. Mueller, E. Frazzoli, and S. Soatto. A Power-Performance Approach to Comparing Sensor Families, with application to comparing neuromorphic to traditional vision sensors. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 3319-3326. IEEE, 2015.
[46] A. Chiuso, G. Picci, and S. Soatto. Wide-sense Estimation on the Special Orthogonal Group. Communications in Information and Systems, 8:185-200, 2008.
[47] S. Choudhary, L. Carlone, C. Nieto, J. Rogers, H. I. Christensen, and F. Dellaert. Distributed Trajectory Estimation with Privacy and Communication Constraints: a Two-Stage Distributed Gauss-Seidel Approach. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 5261-5268. IEEE, 2015.
[48] W. Churchill and P. Newman. Experience-based navigation for longterm localisation. The International Journal of Robotics Research (IJRR), 32(14):1645-1661, 2013.
[49] T. Cieslewski, L. Simon, M. Dymczyk, S. Magnenat, and R. Siegwart. Map API - Scalable Decentralized Map Building for Robots. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 6241-6247. IEEE, 2015.
[50] J. Civera, D. Gálvez-López, L. Riazuelo, J. D. Tardós, and J. M. M. Montiel. Towards Semantic SLAM Using a Monocular Camera. In Proceedings of the IEEE/RSJ International Conference on Intelligent</p>
<p>Robots and Systems (IROS), pages 1277-1284. IEEE, 2011.
[51] A. Cohen, C. Zach, S. Sinha, and M. Pollefeys. Discovering and Exploiting 3D Symmetries in Structure from Motion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.
[52] G. Costante, M. Mancini, P. Valigi, and T. A. Ciarfuglia. Exploring Representation Learning With CNNs for Frame-to-Frame Ego-Motion Estimation. IEEE Robotics and Automation Letters, 1(1):18-25, Jan 2016.
[53] M. Cummins and P. Newman. FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance. The International Journal of Robotics Research (IJRR), 27(6):647-665, 2008.
[54] A. Cunningham, V. Indelman, and F. Dellaert. DDF-SAM 2.0: Consistent distributed smoothing and mapping. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 5220-5227. IEEE, 2013.
[55] B. Curless and M. Levoy. A volumetric method for building complex models from range images. In SIGGRAPH, pages 303-312. ACM, 1996.
[56] A. Dame, V. A. Prisacariu, C. Y. Ren, and I. D. Reid. Dense Reconstruction using 3D Object Shape Priors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1288-1295. IEEE, 2013.
[57] D. G. Dansereau, D. L. Bongiorno, O. Pizarro, and S. B. Williams. Light Field Image Denoising Using a Linear 4D Frequency-Hyperfan all-in-focus Filter. In Proceedings of the SPIE Conference on Computational Imaging, volume 8657, pages 86570P1-86570P14, 2013.
[58] D. G. Dansereau, S. B. Williams, and P. Corke. Simple Change Detection from Mobile Light Field Cameras. Computer Vision and Image Understanding, 2016.
[59] A. Davison, I. Reid, N. Molton, and O. Stasse. MonoSLAM: RealTime Single Camera SLAM. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 29(6):1052-1067, 2007.
[60] F. Dayoub, G. Cielniak, and T. Duckett. Long-term experiments with an adaptive spherical view representation for navigation in changing environments. Robotics and Autonomous Systems (RAS), 59(5):285295, 2011.
[61] F. Dellaert. Factor Graphs and GTSAM: A Hands-on Introduction. Technical Report GT-RIM-CP\&amp;R-2012-002, Georgia Institute of Technology, Sept. 2012.
[62] F. Dellaert, J. Carlson, V. Ila, K. Ni, and C. E. Thorpe. Subgraphpreconditioned Conjugate Gradient for Large Scale SLAM. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2566-2571. IEEE, 2010.
[63] F. Dellaert and M. Kaess. Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing. The International Journal of Robotics Research (IJRR), 25(12):1181-1203, 2006.
[64] G. Dissanayake, S. Huang, Z. Wang, and R. Ranasinghe. A review of recent developments in Simultaneous Localization and Mapping. In International Conference on Industrial and Information Systems, pages 477-482. IEEE, 2011.
[65] F. Dong, S.-H. Ieng, X. Savatier, R. Etienne-Cummings, and R. Benosman. Plenoptic cameras in real-time robotics. The International Journal of Robotics Research (IJRR), 32(2), 2013.
[66] J. Dong, E. Nelson, V. Indelman, N. Michael, and F. Dellaert. Distributed real-time cooperative localization and mapping using an uncertainty-aware expectation maximization approach. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 5807-5814. IEEE, 2015.
[67] R. Dubé, H. Sommer, A. Gawel, M. Bosse, and R. Siegwart. Nonuniform sampling strategies for continuous correction based trajectory estimation. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 4792-4798. IEEE, 2016.
[68] H. F. Durrant-Whyte. An Autonomous Guided Vehicle for Cargo Handling Applications. The International Journal of Robotics Research (IJRR), 15(5):407-440, 1996.
[69] H. F. Durrant-Whyte and T. Bailey. Simultaneous Localisation and Mapping (SLAM): Part I. IEEE Robotics and Automation Magazine, 13(2):99-110, 2006.
[70] D. Eigen and R. Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015.
[71] A. Elfes. Occupancy Grids: A Probabilistic Framework for Robot Perception and Navigation. Journal on Robotics and Automation, RA$3(3): 249-265,1987$.
[72] J. Engel, J. Schöps, and D. Cremers. LSD-SLAM: Large-scale direct monocular SLAM. In European Conference on Computer Vision (ECCV), pages 834-849. Springer, 2014.
[73] R. M. Eustice, H. Singh, J. J. Leonard, and M. R. Walter. Visually Mapping the RMS Titanic: Conservative Covariance Estimates for SLAM Information Filters. The International Journal of Robotics Research (IJRR), 25(12):1223-1242, 2006.
[74] M. Everingham, L. Van-Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes (VOC) Challenge. International Journal of Computer Vision, 88(2):303-338, 2010.
[75] N. Fairfield, G. Kantor, and D. Wettergreen. Real-Time SLAM with Octree Evidence Grids for Exploration in Underwater Tunnels. Journal of Field Robotics (IFR), 24(1-2):03-21, 2007.
[76] N. Fairfield and D. Wettergreen. Active SLAM and Loop Prediction with the Segmented Map Using Simplified Models. In Proceedings of International Conference on Field and Service Robotics (FSR), pages 173-182. Springer, 2010.
[77] H. J. S. Feder. Simultaneous Stochastic Mapping and Localization. PhD thesis, Massachusetts Institute of Technology, 1999.
[78] B. Ferris, D. Fox, and N. D. Lawrence. WiFi-SLAM using gaussian process latent variable models. In International Joint Conference on Artificial Intelligence, 2007.
[79] A. Flint, D. Murray, and I. D. Reid. Manhattan Scene Understanding Using Monocular, Stereo, and 3D Features. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 22282235. IEEE, 2011.
[80] J. Foley, A. van Dam, S. Feiner, and J. Hughes. Computer Graphics: Principles and Practice. Addison-Wesley, 1992.
[81] J. Folkesson and H. Christensen. Graphical SLAM for outdoor applications. Journal of Field Robotics (IFR), 23(1):51-70, 2006.
[82] C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza. On-Manifold Preintegration for Real-Time Visual-Inertial Odometry. IEEE Transactions on Robotics (TRO), PP(99):1-21, 2016.
[83] C. Forster, Z. Zhang, M. Gassner, M. Werlberger, and D. Scaramuzza. SVO: Semi-Direct Visual Odometry for Monocular and Multi-Camera Systems. IEEE Transactions on Robotics (TRO), PP(99):1-17, 2016.
[84] D. Fox, J. Ko, K. Konolige, B. Limketkai, D. Schulz, and B. Stewart. Distributed Multirobot Exploration and Mapping. Proceedings of the IEEE, 94(7):1325-1339, July 2006.
[85] F. Fraundorfer and D. Scaramuzza. Visual Odometry. Part II: Matching, Robustness, Optimization, and Applications. IEEE Robotics and Automation Magazine, 19(2):78-90, 2012.
[86] J. Fredriksson and C. Olsson. Simultaneous Multiple Rotation Averaging using Lagrangian Duality. In Asian Conference on Computer Vision (ACCV), pages 245-258. Springer, 2012.
[87] U. Frese. Interview: Is SLAM Solved? KI - Künstliche Intelligenz, 24(3):255-257, 2010.
[88] P. Furgale, T. D. Barfoot, and G. Sibley. Continuous-Time Batch Estimation using Temporal Basis Functions. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 2088-2095. IEEE, 2012.
[89] G. Gallego, J. Lund, E. Mueggler, H. Rebecq, T. Delbruck, and D. Scaramuzza. Event-based, 6-DOF Camera Tracking for High-Speed Applications. CoRR, abs/1607.03468:1-8, 2016.
[90] R. Garg, V. Kumar, G. Carneiro, and I. Reid. Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue. In European Conference on Computer Vision (ECCV), pages 740-756, 2016.
[91] R. Garg, A. Roussos, and L. Agapito. Dense Variational Reconstruction of Non-Rigid Surfaces from Monocular Video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1272-1279, 2013.
[92] J. J. Gibson. The ecological approach to visual perception: classic edition. Psychology Press, 2014.
[93] D. Golovin and A. Krause. Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization. Journal of Artificial Intelligence Research, 42(1):427-486, 2011.
[94] Google. Project tango. https://www.google.com/atap/projecttango/, 2016.
[95] V. M. Govindu. Combining Two-view Constraints For Motion Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 218-225. IEEE, 2001.
[96] O. Grasa, E. Bernal, S. Casado, I. Gil, and J. M. M. Montiel. Visual SLAM for Handheld Monocular Endoscope. IEEE Transactions on Medical Imaging, 33(1):135-146, 2014.
[97] G. Grisetti, R. Kümmerle, C. Stachniss, and W. Burgard. A Tutorial on Graph-based SLAM. IEEE Intelligent Transportation Systems Magazine, 2(4):31-43, 2010.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ Interestingly, the lack of observability manifests itself very clearly in factor graph optimization, since the linear system to be solved in iterative methods becomes rank-deficient; this enables the design of techniques that can explicitly deal with problems that are not fully observable [265].
${ }^{7}$ We use the term "local minimum" to denote a minimum of the cost which does not attain the globally optimal objective.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8277 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8277</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8277</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-276617556</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.18873v1.pdf" target="_blank">Multi-LLM Collaborative Search for Complex Problem Solving</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) often struggle with complex reasoning tasks due to their limitations in addressing the vast reasoning space and inherent ambiguities of natural language. We propose the Mixture-of-Search-Agents (MoSA) paradigm, a novel approach leveraging the collective expertise of multiple LLMs to enhance search-based reasoning. MoSA integrates diverse reasoning pathways by combining independent exploration with iterative refinement among LLMs, mitigating the limitations of single-model approaches. Using Monte Carlo Tree Search (MCTS) as a backbone, MoSA enables multiple agents to propose and aggregate reasoning steps, resulting in improved accuracy. Our comprehensive evaluation across four reasoning benchmarks demonstrates MoSA's consistent performance improvements over single-agent and other multi-agent baselines, particularly in complex mathematical and commonsense reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8277.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8277.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOSA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture-of-Search-Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent search-based reasoning paradigm that combines multiple distinct LLMs as search agents within an MCTS backbone, using both independent proposal of sub-questions/answers and an LLM-based aggregator to refine candidate sub-answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MOSA (Mixture-of-Search-Agents) with LLM pool (Llama-3.1-8B-Instruct, Qwen-2-7B-Instruct, Mistral-8B-Instruct, GLM-4-9B-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A method rather than a single pretrained LM: MOSA runs Monte Carlo Tree Search (MCTS) where each Expansion samples sub-questions and sub-answers from a pool of distinct instruction-tuned LLMs (the paper's default pool: Llama-3.1-8B-Instruct, Qwen-2-7B-Instruct, Ministral/Mistral-8B-Instruct, GLM-4-9B-Chat). It also uses a learned/prompted neural 'aggregator' (an LLM-based consolidation step) to merge candidate sub-answers prior to voting/selection.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Search-based reasoning (MCTS)', 'Multi-agent proposal (diverse sub-question/sub-answer generation)', 'Neural aggregation (LLM-based critic/merging of candidate answers)', 'Majority voting / self-consistency for final-answer scoring', 'Chain-of-Thought style few-shot prompts used inside agents (where applicable)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>MOSA implements MCTS as the outer search framework. At Expansion, multiple distinct LLMs independently propose sub-questions (diversifying search directions). Each proposed sub-question is answered by multiple (possibly different) LLMs producing candidate sub-answers. Candidate sub-answers are aggregated either by heuristic majority voting or by a dedicated LLM 'aggregator' prompt that critiques/merges answers. Rewards are approximated by self-consistency majority confidence (used as rollout reward proxy). The method therefore fuses independent exploration (different models/sample draws) with collaborative iterative refinement (aggregators reading and improving other agents' outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (explicitly multi-agent) with collaborative refinement — 'both' in the sense it combines diverse independent proposals and an aggregative/similarity step</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared MOSA to single-LLM MCTS baselines by: (1) keeping total LLM forward calls comparable, (2) varying single-LLM sampling temperature to control diversity (T in {0.25,0.5,0.75,1.0,1.25}) and measuring Vendi n-gram diversity, (3) ablating number of distinct LLMs (1 to 4), (4) ablating role counts (proposers vs aggregators single vs multi), and (5) combining MOSA with expanded action sets (rStar's A{1..5}).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Evaluated on four benchmarks: GSM8K (math word problems), SVAMP (math), MATH-500 (harder math), and StrategyQA (commonsense multi-hop QA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>RAP baseline (single-LLM MCTS with A{2,3}) reported avg 77.63%; RAP + MOSA (proposers & aggregators) achieves avg 79.97% (per-dataset: GSM8K 91.96, SVAMP 94.90, MATH-500 56.60, StrategyQA 76.42). When combined with an expanded action set (rStar A{1..5}), rStar baseline avg 79.66% vs rStar + MOSA (proposers & aggregators) avg 81.54% (per-dataset rStar+MOSA: GSM8K 92.04, SVAMP 95.10, MATH-500 63.60, StrategyQA 75.40). The paper also reports an overall average improvement of ~1.71% over single-LLM counterparts (text claim) and shows MOSA's consistent gains across benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>MOSA reduces the need for careful tuning of single-model sampling parameters because model-level heterogeneity yields intrinsic diversity; single-LLM diversity (via temperature) improves accuracy up to a point then degrades, with the optimal temperature differing across benchmarks. Aggregators (LLM-based consolidation) can turn some 'bad' candidate answers into better ones by learning from good candidates, increasing the chance majority voting produces correct outcomes. Ablations indicate MOSA's proposer role contributes more to gains than aggregation (ablating to single proposer yields a larger drop than ablating aggregators). Increasing the number of distinct LLMs generally correlates with higher accuracy (positive correlation), though small exceptions appear (e.g., slight drop 3→4 LLMs on MATH-500).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Combining multiple distinct LLM search agents (diverse proposals) with aggregator-based iterative refinement consistently outperforms single-LLM MCTS and other baselines, improving reasoning accuracy across math and commonsense benchmarks and alleviating the difficulty of tuning diversity-quality trade-offs for single models; proposer diversity is the primary source of benefit, while aggregators provide additional but smaller improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LLM Collaborative Search for Complex Problem Solving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8277.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8277.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAP (single-LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reasoning-via-Planning (single-LLM MCTS baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An MCTS-based search reasoning baseline (referenced as RAP) where a single LLM acts as the search agent generating sub-questions and sub-answers; used in this paper as the principal single-agent comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RAP (single-LLM MCTS using Llama-3.1-8B-Instruct in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A search-based reasoning system that applies MCTS for step-wise decomposition: at each expansion the single LLM proposes sub-questions and sub-answers; uses heuristic aggregation (majority voting) to finalize candidate sub-answers and self-consistency scoring as a reward proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Search-based reasoning (MCTS)', 'Single-agent proposal', 'Stochastic decoding (temperature/top-k/top-p) to introduce diversity', 'Majority-voting/self-consistency']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>A single LLM generates candidate sub-questions and sub-answers during MCTS Expansion. Diversity is controlled via decoding hyperparameters (temperature, top-k, top-p). Final sub-answers are decided via majority voting; reward values approximate correctness by self-consistency of rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar / single-model (diversity introduced only by stochastic decoding parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>The paper varied sampling temperature (T ∈ {0.25, 0.5, 0.75, 1.0, 1.25}) for the single-LLM RAP baseline and measured trajectory diversity using Vendi n-gram scores; they compared accuracy-vs-diversity curves to MOSA's multi-LLM results.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, SVAMP, MATH-500, StrategyQA (same set used for MOSA evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>RAP (single-LLM, reported in paper for A{2,3}) per Table 2: GSM8K 90.52, SVAMP 91.60, MATH-500 53.00, StrategyQA 75.40, average 77.63%. Single-LLM accuracy improves with temperature up to a point then declines; best temperature differs across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Single-LLM search faces a trade-off between diversity and generation quality that requires careful hyperparameter tuning; even with tuning a single model can get trapped in local optima due to its inherent biases and training constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Single-LLM MCTS benefits from increased sampling diversity up to a dataset-dependent optimal level, but tuning is brittle and single-agent approaches often underperform multi-LLM MOSA which attains better diversity without careful sampling tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LLM Collaborative Search for Complex Problem Solving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8277.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8277.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (sampling + majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding/aggregation technique where multiple independent reasoning chains are sampled and the final answer is chosen by majority voting; evaluated both in single-LLM and multi-LLM variants in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-Consistency@n (applied with few-shot CoT prompts; single-LLM and multi-LLM variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Few-shot Chain-of-Thought prompting is used to elicit stepwise reasoning; n independent chains are sampled (via stochastic decoding or across different LLMs) and the final answer is selected by majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought few-shot prompting', 'Sampling multiple reasoning traces (n)', 'Majority-vote aggregation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Self-consistency obtains multiple independent CoT outputs and selects the final answer by majority vote. The paper also experiments with a multi-LLM variant where independent answers are collected from different models and aggregated similarly.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (diversity is achieved by repeated sampling or by using different LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Evaluated conventional single-LLM self-consistency and a multi-LLM self-consistency variant; also used self-consistency majority confidence as a reward proxy in MCTS experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Used as a baseline on GSM8K, SVAMP, MATH-500, StrategyQA (few-shot CoT results reported for each of the four LLMs individually).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>The paper reports CoT / self-consistency baselines for each model (all four LLMs for few-shot CoT). Exact per-method numbers for Self-Consistency alone are not central in the tables shown, but self-consistency is used as a reward proxy and a baseline aggregation technique; MOSA outperforms both single-LLM self-consistency and multi-LLM self-consistency baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Self-consistency benefits from independent samples, but diversity introduced purely via temperature tuning of a single model can be suboptimal; using distinct LLMs for self-consistency can be seen as a simplified multi-agent ensemble and offers improvements over purely single-model sampling in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Multi-LLM variants of self-consistency (collecting answers from different agents) can be regarded as a simplified ensemble and were compared; MOSA's structured multi-agent search and aggregator-based refinement yield further gains beyond simple multi-LLM majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LLM Collaborative Search for Complex Problem Solving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8277.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8277.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>rStar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>rStar (expanded MCTS action set)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent MCTS-based reasoning system that expands the action set used in MCTS from a couple of actions to a richer set (A1..A5); used in the paper both as a baseline and as an action-set to combine with MOSA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>rStar (MCTS with expanded action set A{1,2,3,4,5})</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>rStar extends MCTS-based LLM reasoning by defining a comprehensive action space (A1: one-step thought, A2: remaining thought steps, A3: next sub-question + answer (primary action), A4: answer sub-question again, A5: rephrase question) and (in original rStar) employs a secondary LLM to verify trajectories; in this paper rStar is implemented/used as a baseline and as a complementary action set to MOSA.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Search-based reasoning (MCTS)', 'Expanded action set (A1..A5)', 'Secondary-model verification (in original rStar concept)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>rStar introduces a richer set of atomic actions for MCTS expansions and (originally) a secondary LLM verifier; in this paper, MOSA is evaluated both with the smaller action set A{2,3} and with rStar's expanded A{1..5} to test robustness to action set size.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (single-agent rStar baseline) and both when combined with MOSA (multi-agent proposals over rStar actions)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Paper runs rStar-style experiments and compares rStar baseline to rStar + MOSA (proposers and proposers+aggregators) to measure complementarity of MOSA with an enriched action set.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, SVAMP, MATH-500, StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>rStar baseline (A{1..5}) reported avg 79.66%; rStar + MOSA as proposers & aggregators yields avg 81.54% (per-dataset: rStar+MOSA A{1..5}: GSM8K 92.04, SVAMP 95.10, MATH-500 63.60, StrategyQA 75.40).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Enriching the action set is not uniformly beneficial across tasks (e.g., extended action set hurt performance on StrategyQA), but MOSA still provides improvements when combined with rStar's expanded actions, showing complementarity between multi-agent diversity and a richer action vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>MOSA complements rStar's expanded action set and yields additional improvements, indicating that diverse agent proposals help explore the larger action space more effectively; however, larger action sets can be task-dependent in benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-LLM Collaborative Search for Complex Problem Solving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reasoning-via-Planning (RAP) <em>(Rating: 2)</em></li>
                <li>rStar <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Monte Carlo Tree Search (original MCTS references) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8277",
    "paper_id": "paper-276617556",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "MOSA",
            "name_full": "Mixture-of-Search-Agents",
            "brief_description": "A multi-agent search-based reasoning paradigm that combines multiple distinct LLMs as search agents within an MCTS backbone, using both independent proposal of sub-questions/answers and an LLM-based aggregator to refine candidate sub-answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MOSA (Mixture-of-Search-Agents) with LLM pool (Llama-3.1-8B-Instruct, Qwen-2-7B-Instruct, Mistral-8B-Instruct, GLM-4-9B-Chat)",
            "model_description": "A method rather than a single pretrained LM: MOSA runs Monte Carlo Tree Search (MCTS) where each Expansion samples sub-questions and sub-answers from a pool of distinct instruction-tuned LLMs (the paper's default pool: Llama-3.1-8B-Instruct, Qwen-2-7B-Instruct, Ministral/Mistral-8B-Instruct, GLM-4-9B-Chat). It also uses a learned/prompted neural 'aggregator' (an LLM-based consolidation step) to merge candidate sub-answers prior to voting/selection.",
            "reasoning_methods": [
                "Search-based reasoning (MCTS)",
                "Multi-agent proposal (diverse sub-question/sub-answer generation)",
                "Neural aggregation (LLM-based critic/merging of candidate answers)",
                "Majority voting / self-consistency for final-answer scoring",
                "Chain-of-Thought style few-shot prompts used inside agents (where applicable)"
            ],
            "reasoning_methods_description": "MOSA implements MCTS as the outer search framework. At Expansion, multiple distinct LLMs independently propose sub-questions (diversifying search directions). Each proposed sub-question is answered by multiple (possibly different) LLMs producing candidate sub-answers. Candidate sub-answers are aggregated either by heuristic majority voting or by a dedicated LLM 'aggregator' prompt that critiques/merges answers. Rewards are approximated by self-consistency majority confidence (used as rollout reward proxy). The method therefore fuses independent exploration (different models/sample draws) with collaborative iterative refinement (aggregators reading and improving other agents' outputs).",
            "reasoning_diversity": "diverse (explicitly multi-agent) with collaborative refinement — 'both' in the sense it combines diverse independent proposals and an aggregative/similarity step",
            "reasoning_diversity_experimental_setup": "Compared MOSA to single-LLM MCTS baselines by: (1) keeping total LLM forward calls comparable, (2) varying single-LLM sampling temperature to control diversity (T in {0.25,0.5,0.75,1.0,1.25}) and measuring Vendi n-gram diversity, (3) ablating number of distinct LLMs (1 to 4), (4) ablating role counts (proposers vs aggregators single vs multi), and (5) combining MOSA with expanded action sets (rStar's A{1..5}).",
            "task_or_benchmark": "Evaluated on four benchmarks: GSM8K (math word problems), SVAMP (math), MATH-500 (harder math), and StrategyQA (commonsense multi-hop QA).",
            "performance_results": "RAP baseline (single-LLM MCTS with A{2,3}) reported avg 77.63%; RAP + MOSA (proposers & aggregators) achieves avg 79.97% (per-dataset: GSM8K 91.96, SVAMP 94.90, MATH-500 56.60, StrategyQA 76.42). When combined with an expanded action set (rStar A{1..5}), rStar baseline avg 79.66% vs rStar + MOSA (proposers & aggregators) avg 81.54% (per-dataset rStar+MOSA: GSM8K 92.04, SVAMP 95.10, MATH-500 63.60, StrategyQA 75.40). The paper also reports an overall average improvement of ~1.71% over single-LLM counterparts (text claim) and shows MOSA's consistent gains across benchmarks.",
            "qualitative_findings": "MOSA reduces the need for careful tuning of single-model sampling parameters because model-level heterogeneity yields intrinsic diversity; single-LLM diversity (via temperature) improves accuracy up to a point then degrades, with the optimal temperature differing across benchmarks. Aggregators (LLM-based consolidation) can turn some 'bad' candidate answers into better ones by learning from good candidates, increasing the chance majority voting produces correct outcomes. Ablations indicate MOSA's proposer role contributes more to gains than aggregation (ablating to single proposer yields a larger drop than ablating aggregators). Increasing the number of distinct LLMs generally correlates with higher accuracy (positive correlation), though small exceptions appear (e.g., slight drop 3→4 LLMs on MATH-500).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Combining multiple distinct LLM search agents (diverse proposals) with aggregator-based iterative refinement consistently outperforms single-LLM MCTS and other baselines, improving reasoning accuracy across math and commonsense benchmarks and alleviating the difficulty of tuning diversity-quality trade-offs for single models; proposer diversity is the primary source of benefit, while aggregators provide additional but smaller improvements.",
            "uuid": "e8277.0",
            "source_info": {
                "paper_title": "Multi-LLM Collaborative Search for Complex Problem Solving",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "RAP (single-LLM)",
            "name_full": "Reasoning-via-Planning (single-LLM MCTS baseline)",
            "brief_description": "An MCTS-based search reasoning baseline (referenced as RAP) where a single LLM acts as the search agent generating sub-questions and sub-answers; used in this paper as the principal single-agent comparator.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RAP (single-LLM MCTS using Llama-3.1-8B-Instruct in experiments)",
            "model_description": "A search-based reasoning system that applies MCTS for step-wise decomposition: at each expansion the single LLM proposes sub-questions and sub-answers; uses heuristic aggregation (majority voting) to finalize candidate sub-answers and self-consistency scoring as a reward proxy.",
            "reasoning_methods": [
                "Search-based reasoning (MCTS)",
                "Single-agent proposal",
                "Stochastic decoding (temperature/top-k/top-p) to introduce diversity",
                "Majority-voting/self-consistency"
            ],
            "reasoning_methods_description": "A single LLM generates candidate sub-questions and sub-answers during MCTS Expansion. Diversity is controlled via decoding hyperparameters (temperature, top-k, top-p). Final sub-answers are decided via majority voting; reward values approximate correctness by self-consistency of rollouts.",
            "reasoning_diversity": "similar / single-model (diversity introduced only by stochastic decoding parameters)",
            "reasoning_diversity_experimental_setup": "The paper varied sampling temperature (T ∈ {0.25, 0.5, 0.75, 1.0, 1.25}) for the single-LLM RAP baseline and measured trajectory diversity using Vendi n-gram scores; they compared accuracy-vs-diversity curves to MOSA's multi-LLM results.",
            "task_or_benchmark": "GSM8K, SVAMP, MATH-500, StrategyQA (same set used for MOSA evaluation).",
            "performance_results": "RAP (single-LLM, reported in paper for A{2,3}) per Table 2: GSM8K 90.52, SVAMP 91.60, MATH-500 53.00, StrategyQA 75.40, average 77.63%. Single-LLM accuracy improves with temperature up to a point then declines; best temperature differs across datasets.",
            "qualitative_findings": "Single-LLM search faces a trade-off between diversity and generation quality that requires careful hyperparameter tuning; even with tuning a single model can get trapped in local optima due to its inherent biases and training constraints.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Single-LLM MCTS benefits from increased sampling diversity up to a dataset-dependent optimal level, but tuning is brittle and single-agent approaches often underperform multi-LLM MOSA which attains better diversity without careful sampling tuning.",
            "uuid": "e8277.1",
            "source_info": {
                "paper_title": "Multi-LLM Collaborative Search for Complex Problem Solving",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (sampling + majority voting)",
            "brief_description": "A decoding/aggregation technique where multiple independent reasoning chains are sampled and the final answer is chosen by majority voting; evaluated both in single-LLM and multi-LLM variants in the paper.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "Self-Consistency@n (applied with few-shot CoT prompts; single-LLM and multi-LLM variants)",
            "model_description": "Few-shot Chain-of-Thought prompting is used to elicit stepwise reasoning; n independent chains are sampled (via stochastic decoding or across different LLMs) and the final answer is selected by majority voting.",
            "reasoning_methods": [
                "Chain-of-Thought few-shot prompting",
                "Sampling multiple reasoning traces (n)",
                "Majority-vote aggregation"
            ],
            "reasoning_methods_description": "Self-consistency obtains multiple independent CoT outputs and selects the final answer by majority vote. The paper also experiments with a multi-LLM variant where independent answers are collected from different models and aggregated similarly.",
            "reasoning_diversity": "both (diversity is achieved by repeated sampling or by using different LLMs)",
            "reasoning_diversity_experimental_setup": "Evaluated conventional single-LLM self-consistency and a multi-LLM self-consistency variant; also used self-consistency majority confidence as a reward proxy in MCTS experiments.",
            "task_or_benchmark": "Used as a baseline on GSM8K, SVAMP, MATH-500, StrategyQA (few-shot CoT results reported for each of the four LLMs individually).",
            "performance_results": "The paper reports CoT / self-consistency baselines for each model (all four LLMs for few-shot CoT). Exact per-method numbers for Self-Consistency alone are not central in the tables shown, but self-consistency is used as a reward proxy and a baseline aggregation technique; MOSA outperforms both single-LLM self-consistency and multi-LLM self-consistency baselines.",
            "qualitative_findings": "Self-consistency benefits from independent samples, but diversity introduced purely via temperature tuning of a single model can be suboptimal; using distinct LLMs for self-consistency can be seen as a simplified multi-agent ensemble and offers improvements over purely single-model sampling in some settings.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Multi-LLM variants of self-consistency (collecting answers from different agents) can be regarded as a simplified ensemble and were compared; MOSA's structured multi-agent search and aggregator-based refinement yield further gains beyond simple multi-LLM majority voting.",
            "uuid": "e8277.2",
            "source_info": {
                "paper_title": "Multi-LLM Collaborative Search for Complex Problem Solving",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "rStar",
            "name_full": "rStar (expanded MCTS action set)",
            "brief_description": "A recent MCTS-based reasoning system that expands the action set used in MCTS from a couple of actions to a richer set (A1..A5); used in the paper both as a baseline and as an action-set to combine with MOSA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "rStar (MCTS with expanded action set A{1,2,3,4,5})",
            "model_description": "rStar extends MCTS-based LLM reasoning by defining a comprehensive action space (A1: one-step thought, A2: remaining thought steps, A3: next sub-question + answer (primary action), A4: answer sub-question again, A5: rephrase question) and (in original rStar) employs a secondary LLM to verify trajectories; in this paper rStar is implemented/used as a baseline and as a complementary action set to MOSA.",
            "reasoning_methods": [
                "Search-based reasoning (MCTS)",
                "Expanded action set (A1..A5)",
                "Secondary-model verification (in original rStar concept)"
            ],
            "reasoning_methods_description": "rStar introduces a richer set of atomic actions for MCTS expansions and (originally) a secondary LLM verifier; in this paper, MOSA is evaluated both with the smaller action set A{2,3} and with rStar's expanded A{1..5} to test robustness to action set size.",
            "reasoning_diversity": "similar (single-agent rStar baseline) and both when combined with MOSA (multi-agent proposals over rStar actions)",
            "reasoning_diversity_experimental_setup": "Paper runs rStar-style experiments and compares rStar baseline to rStar + MOSA (proposers and proposers+aggregators) to measure complementarity of MOSA with an enriched action set.",
            "task_or_benchmark": "GSM8K, SVAMP, MATH-500, StrategyQA",
            "performance_results": "rStar baseline (A{1..5}) reported avg 79.66%; rStar + MOSA as proposers & aggregators yields avg 81.54% (per-dataset: rStar+MOSA A{1..5}: GSM8K 92.04, SVAMP 95.10, MATH-500 63.60, StrategyQA 75.40).",
            "qualitative_findings": "Enriching the action set is not uniformly beneficial across tasks (e.g., extended action set hurt performance on StrategyQA), but MOSA still provides improvements when combined with rStar's expanded actions, showing complementarity between multi-agent diversity and a richer action vocabulary.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "MOSA complements rStar's expanded action set and yields additional improvements, indicating that diverse agent proposals help explore the larger action space more effectively; however, larger action sets can be task-dependent in benefit.",
            "uuid": "e8277.3",
            "source_info": {
                "paper_title": "Multi-LLM Collaborative Search for Complex Problem Solving",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reasoning-via-Planning (RAP)",
            "rating": 2,
            "sanitized_title": "reasoningviaplanning_rap"
        },
        {
            "paper_title": "rStar",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Monte Carlo Tree Search (original MCTS references)",
            "rating": 1,
            "sanitized_title": "monte_carlo_tree_search_original_mcts_references"
        }
    ],
    "cost": 0.01584025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multi-LLM Collaborative Search for Complex Problem Solving
26 Feb 2025</p>
<p>Sen Yang 
Yafu Li 
Wai Lam 
Yu Cheng 
Multi-LLM Collaborative Search for Complex Problem Solving
26 Feb 2025932B1E7698A759B3D2E8729E67DA6DABarXiv:2502.18873v1[cs.AI]
Large language models (LLMs) often struggle with complex reasoning tasks due to their limitations in addressing the vast reasoning space and inherent ambiguities of natural language.We propose the Mixture-of-Search-Agents (MOSA) paradigm, a novel approach leveraging the collective expertise of multiple LLMs to enhance search-based reasoning.MOSA integrates diverse reasoning pathways by combining independent exploration with iterative refinement among LLMs, mitigating the limitations of single-model approaches.Using Monte Carlo Tree Search (MCTS) as a backbone, MOSA enables multiple agents to propose and aggregate reasoning steps, resulting in improved accuracy.Our comprehensive evaluation across four reasoning benchmarks demonstrates MOSA's consistent performance improvements over single-agent and other multiagent baselines, particularly in complex mathematical and commonsense reasoning tasks.</p>
<p>Introduction</p>
<p>Large language models (LLMs) face challenges with complex reasoning, even when augmented with linearized reasoning chains (e.g., Chain-of-Thought), due to the vast reasoning space inherent in the complexity and ambiguity of natural languages.A promising approach is step-wise search-based reasoning, which decomposes the reasoning problem into a traversal over a directed graph, where nodes and edges represent individual reasoning sub-steps distributed across the expansive reasoning space.Related methods have applied various search algorithms to LLMs, such as breadth-first search (BFS), depth-first search (DFS) (Yao et al., 2024;Besta et al., 2024), and best-first search (Hao et al., 2023;Zhang et al., 2024a;Qi et al., 2024).</p>
<p>A successful search trial is featured with diverse yet effective explorations (Hao et al., 2023;Yao et al., 2024).A straightforward method to enhance diversity involves increasing Preliminary work.Under review.</p>
<p>Single-LLM Search</p>
<p>Multi-LLM Search the temperature, thereby making the probability distribution more uniform.This is typically combined with top-k and top-p sampling to balance diversity and quality.However, as shown in Figure 1, despite these sampling techniques, achieving a balance between diversity and quality remains challenging and necessitates careful tuning.Besides, even with near-optimal sampling parameters, a single LLM might still get trapped in local optima due to constraints inherent in its training data and architectural design.</p>
<p>To mitigate this limitation, an alternative solution is to aggregate the specialized strengths of multiple LLMs.Recent work (Wang et al., 2024b) has demonstrated that multiple LLMs can collaboratively enhance their instructionfollowing capabilities by post-editing each other's responses to the same instruction.Motivated by this progress, we explore leveraging the collective expertise of multiple LLMs for search-based reasoning, which, to the best of our knowledge, has not been previously tested.Figure 1 illustrates the reasoning accuracy on the MATH-500 dataset as a function of search diversity.The performance of search using a single LLM initially improves with increased temperature but subsequently degrades, remaining consistently lower than that of multiple-LLM search.</p>
<p>In this work, we propose Mixture-of-Search-Agents (MOSA), an advanced paradigm for step-wise search-based reasoning that aggregates the complementary strengths of multiple LLMs, leveraging both independent and collaborative contributions to search for reasoning sub-steps more effectively.As illustrated in Figure 3, multiple LLMs propose diverse potential search directions at each reasoning step, either independently or through iterative refinement of each other's outputs.This hybrid approach ensures that the reasoning process is not constrained by the limitations or biases of any single model.For instance, one model may excel at identifying a promising initial direction, while another might build on it to refine or extend the reasoning path.By combining independence and collaboration, the framework avoids local optima while enhancing reasoning accuracy in the search process.</p>
<p>We performed a comprehensive evaluation of MOSA across four reasoning benchmarks.The findings indicate that MOSA consistently outperforms its single-LLM counterpart in reasoning accuracy with an average improvement of 1.71%.Additionally, our results indicate a synergistic interaction between multi-agent collaboration and search-based reasoning.Further analysis and ablation studies reveal a key challenge for single-agent search-based reasoning: balancing diversity and quality varies across different benchmarks.We also confirm a positive correlation between reasoning performance and the number of distinct search agents, validating the efficacy of multi-agent search.Finally, experiments with an extended action set demonstrate the robustness of MOSA across diverse types of search actions.</p>
<p>Method</p>
<p>Search-based methods have been extensively used to tackle complex reasoning tasks, such as coding and mathematics, by breaking these problems into multiple search steps (Zhou et al., 2023b;Yao et al., 2024;Hao et al., 2023).Our proposed paradigm is readily applicable to various search algorithms, with the Monte Carlo Tree Search (MCTS) algorithm (Kocsis &amp; Szepesvári, 2006;Coulom, 2007) adopted as the search backbone in this work.This section first introduces the baseline MCTS-based reasoning method with a single search agent (Hao et al., 2023;Qi et al., 2024) in § 2.1, followed by our method, which leverages the expertise of multiple LLMs as search agents in § 2.2.</p>
<p>Baseline Framework</p>
<p>Overview Given a problem x and a generator π * , MCTS involves iteratively building a search tree starting from the root node x.We first define the state space S and the action space A. In our case, each state s j ∈ S captures the actions (i.e., reasoning steps) generated so far alongside a specific trajectory in the search tree, while each action a j ∈ A represents the next reasoning step based on the current state and the type of action chosen.As shown in the upper part of Figure 2, given the selected node s i (i.e., the reasoning steps generated so far), a step of Expansion essentially creates a set of child nodes.A child node is created by concatenat- ing s i with the new action, with that action being the next reasoning step generated by a search agent (e.g., an LLM) given s i .</p>
<h1>1 #2 #3 s i LLM LLM LLM 1-1 1-2 1-3 2-1 2-2 2-3 3-1 3-2 3-3</h1>
<p>Candidate</p>
<p>Action Space We follow rStar (Qi et al., 2024) to define a comprehensive set of actions into MCTS-based LLM reasoning.The set of actions, A = {A1, A2, A3, A4, A5}, includes:</p>
<p>• A1: Propose a one-step thought;</p>
<p>• A2: Propose the remaining thought steps;</p>
<p>• A3: Propose the next sub-question along with its answer;</p>
<p>• A4: Answer the sub-question again;</p>
<p>• A5: Rephrase the question.Among these actions, we designate A3 as the primary action, comprising a sub-question and its corresponding subanswer, i.e., action i ≡ concat(sub question i , sub answer i ).</p>
<p>For instance, an action can be "### Sub-question 3: Does the sum of the previous two digits equal 8? ### Sub-answer 3: The two digits are 3 and 5.We have 3 + 5 = 8, so the answer is yes.".We consider the other actions along with their effects in an ablation analysis ( § 4.3).We present a detailed illustration of generating new actions, i.e., combinations of sub-question &amp; sub-answer in Algorithm 1.For a given state s i , the algorithm traverses all possible actions, where the final sub-answer for each sub-question is determined by a heuristic function, e.g., majority voting.</p>
<p>Algorithm 1 GenerateActions: A function for generating actions, i.e., a sub-question along with a sub-answer, given the current state.The implementation of this function using conventional MCTS and MoSA mainly differs in two aspects: (1) In conventional MCTS methods, the number of search agents m = 1, while for MOSA, m &gt; 1.</p>
<p>(2) The FinalizeSubAnswer function employs heuristic majority voting for single-model search, while employing an additional neural aggregation function for MOSA (see the right section of Figure 3).Reward Function Following Hao et al. (2023); Qi et al. (2024), we consider a simple yet effective reward function: actions that frequently lead to correct final answers are assigned higher rewards.Specifically, Q(s, a), the reward value for node s created by action a, receives a positive reward if a trajectory containing node s reaches a correct final answer, and no reward otherwise.Since the gold answer is not available during testing, the confidence given by majority voting is regarded as an approximation of the reward value.</p>
<p>Require</p>
<p>MCTS Iterations</p>
<p>Typically, each MCTS iteration involves four steps: Selection, Expansion, Simulation, and Back-propagation.To balance exploration and exploitation, we adopt the widely-used Upper Confidence Bounds for Trees (UCT) algorithm (Kocsis &amp; Szepesvári, 2006) for Selection.Formally, a node s is selected to maximize:
UCT(s, a) = Q(s, a) N (s, a) + c ln Nparent(s) N (s, a)(1)
where N parent (s) is the number of times the parent node of s has been visited, N (s, a) is the number of times node s has been visited, and c is a constant.Once the node s is selected, an Expansion step is performed to add child nodes to s.After that, starting from a random child node, a Simulation is performed using the default rollout policy until a terminal node is obtained or a predefined maximum depth is reached.The outcome of the simulation determines the reward, which is then propagated back up the tree during the Back-propagation step.Upon multiple iterations, we consider each leaf node as a solution.In this work, we focus on Expansion, which aims to effectively expand the search space.</p>
<p>Sampling Diversity Applying stochastic sampling techniques in LLM generation is essential for introducing diversity to MCTS.As presented in the lower part of Figure 2, given the selected state s 0 , the sub-questions and the sub-answer candidates are all stochastically sampled using temperature scaling, top-k sampling and nucleus sampling (Holtzman et al., 2020).In § 4.1, we empirically alter search diversity by manipulating generation temperature for single-LLM search.</p>
<p>Mixture-of-Search-Agents</p>
<p>Conventional Monte Carlo Tree Search (MCTS) methods utilizing a single model face two significant limitations: (1) Encouraging search diversity while maintaining generation quality is challenging (Zhang et al., 2020), necessitating meticulous tuning of sampling parameters to balance the trade-off between these aspects; (2) using heuristic metrics like majority voting to determine the final sub-answer can be less accurate when the model favors incorrect search directions.To this end, we explore a simple yet effective alternative, Mixture-of-Search-Agents (MOSA), which employs multiple agents to perform search algorithms like MCTS and utilizes a neural function to refine the candidate stepwise outputs.Firstly, leveraging the distinct distributions from different models intrinsically yields better generation diversity, alleviating the necessity for sampling parameters optimization.Additionally, incorporating a neural function 1 1-2 1-3 2-1 2-2 2-3 3-1 3-2 3-3 Candidate Sub-answers 1-1 1-2  3-2  1-3 2-1 2-2 2-3 3-1  3 enhances the robustness of answer aggregation.</p>
<h1>1 #2 #3 s i 1-</h1>
<p>Sub-questions</p>
<p>Sub-answers</p>
<p>Majarity voting
-3 #1 #2 #3 3-1 3-2 3-3 3-2 3-1 3-3 #1 #2 #3 #3 3-1 3-2 3-3 #3 #3</p>
<p>Aggregated</p>
<p>Figure 3 illustrates how our method generates three new actions starting from the current node s i .Unlike vanilla single-model search, MOSA employs multiple agents (denoted by distinct colors) to explore diverse actions, such as sub-questions and sub-answers.In the remainder of this section, we illustrate two roles performed by MOSA when generating new actions in MCTS.Specifically, we will start with the straightforward improvement, MOSA as Proposers, where multiple agents are involved for sampling actions; then we will introduce the more intricate MOSA as Aggregators, which extends the heuristic majority voting method to an aggregating phase where multiple LLMs read and refine the answers given by all.</p>
<p>MOSA as Proposers to Diversify Actions The left side of Figure 3 shows MOSA as Proposers, where the single search agent adopted by the baseline MCTS method (as in Figure 2) is replaced by MOSA.MOSA leverages multiple LLMs to enhance action diversity by fulfilling two sub-roles: multi-agent proposers that generate sub-questions and subanswers.</p>
<p>Generating a new search action begins with sampling a sub-question from the current state s i .The sub-question proposing phase essentially controls the directions of the current search step because whatever follows within this step is constrained by the scope of that sub-question.Because of this, we consider maintaining the independence among subquestions, ensuring that the initial search direction indicated by each sub-question is independent of others.As shown in the upper-left part of Figure 3, this effectively diversifies the sampled sub-questions as the same s i is colored with distinct characteristics after going through different LLMs.</p>
<p>After the initial search directions are created, the target is to comprehensively explore each search direction.To achieve this, each sub-question is answered by various LLMs, generating a diverse set of candidate sub-answers.These candidates are then aggregated to reach a finalized sub-answer.A simple yet effective aggregating method is majority voting, leveraging the principle of self-consistency (Wang et al., 2023).</p>
<p>MOSA as Aggregators for Collaborative Refinement</p>
<p>We introduce a neural function, termed "aggregator", to mitigate the limitation of majority voting for selecting the final answer.An aggregator leverages the innate capability of the LLM to critique, compare and aggregate multiple answers into a final answer.Specifically, we prompt each LLM to consolidate all responses into an aggregated answer (see Appendix B for detailed prompts), resulting in a new set of aggregated answers as illustrated in the right section of Figure 3.The underlying intuition is that this aggregation process enhances the likelihood of producing correct answers by facilitating comparisons among different responses, thereby increasing the overall success rates for correct answers under majority voting.We present an example below to illustrate this intuition.</p>
<p>In the previous section, we consider majority-voting after obtaining candidate sub-answers from diverse proposers.</p>
<p>Let us consider a sub-question that requires 3 sub-answers to be generated and an MOSA component consisting of 3 distinct LLMs.We simply assume that each LLM proposes one sub-answer.If there are k LLMs that are proficient at this sub-question and the other 3 − k are not, then it is likely that we would have k good sub-answer candidates and 3 − k bad candidates 1 .With majority voting, a bad finalized answer is likely if k ≥ 2.</p>
<p>Now we turn to use MOSA to aggregate the candidate subanswers and then include the aggregated sub-answers into majority-voting.The inputs for all three aggregator LLMs are the same, which concatenates the sub-question and all the three candidate sub-answers.We hypothesize that a bad aggregator that receives at least a good sub-answer could yield a sub-answer that is at least better than its original bad sub-answer.Such a hypothesis has been empirically verified in the case of instruction following by Wang et al. (2024b), who showed that many LLMs can generate higher-quality responses by building upon outputs from other LLMs.Thus, if the two bad aggregator LLMs can learn from the good sub-answer and generate good aggregated sub-answers, then we will have 4 good sub-answers and 2 bad ones, which lead to a good finalized sub-answer.</p>
<p>Experiments</p>
<p>Baselines</p>
<p>Few-shot Chain-of-Thought (CoT) (Wei et al., 2023) feeds the LLM with a few demonstrations followed by the input question.Since we are using instruction-tuned LLMs, we format the demonstrations as multi-turn dialogues.In each turn, the human asks a question and then the assistant answers it.</p>
<p>Self-Consistency@n (Wang et al., 2023) also adopts the few-shot CoT prompting scheme, but it samples n independent answers per instance.The final answer is then given by majority voting over the n candidate answers.Except for the conventional single-LLM self-consistency experiments, we also evaluate self-consistency with multiple different LLMs.Such a multi-LLM self-consistency setting can be regarded as a simplified version of Wang et al. (2024b), which collects direct answers from various agents and aggregates them with majority voting.</p>
<p>Reasoning-via-Planning (RAP) (Hao et al., 2023) is a representative LLM-based reasoning method using MCTS.We use it as the foundation to apply MOSA.In each search step, RAP generates one or more sub-questions along with their sub-answers.The original RAP paper adopted different reward functions for different types of tasks.In this work, we use the simple self-consistency score as the reward value, which has been shown to be competitive with those manually designed ones in Appendix A.1 of Qi et al. (2024).Note that we ensure the total number of LLM forward calls of a single-LLM method are approximately the same as its multi-LLM counterpart, e.g., RAP versus RAP + MOSA as Proposers in Table 1.</p>
<p>rStar (Qi et al., 2024) is one of the recent SoTA MCTSbased LLM reasoning methods.The authors proposed a comprehensive set of search actions, which we have introduced in § 2.1.We adopt their innovative set of actions to evaluate the effects brought by the scope of action set on MOSA in § 4.3.</p>
<p>Experimental Settings</p>
<p>Benchmarks We perform evaluation on four reasoning benchmarks covering different scopes, including three mathematical reasoning datasets (GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), MATH-500 (Hendrycks et al., 2021;Lightman et al., 2023)) and one commonsense reasoning dataset (StrategyQA (Geva et al., 2021)).</p>
<p>Models</p>
<p>We adopt four open-sourced instruction-following LLMs to formulate the LLM pool of MOSA: Llama-3.1-8B-Instruct(Grattafiori et al., 2024), Qwen-2-7B-Instruct (Yang et al., 2024), Ministral-8B-Instruct-2410 (Mistral, 2024), and GLM-4-9B-Chat (GLM et al., 2024).The number of LLMs could also be made larger or smaller, depending on customized choices.Our later experiments will show that benchmark performances are positively correlated with the number of distinct LLMs.</p>
<p>Implementation Details For few-shot CoT baselines, we report the results of all four LLMs.For other single-LLM baselines, like Self-Consistency@n and RAP, we adopt Llama-3.1-8B-Instructdue to its competitiveness and robustness across various benchmarks.For all experiments regarding sampling from multiple LLMs, we try to maintain a pseudo uniform distribution for the SelectLLM function in Algorithm 1.That is, if 7 completions need to be sampled and there are 4 distinct LLMs, we manually assign each LLM to sample one completion and then uniformly sample 3 LLMs out of 4 without replacement to finish the remaining 3 completions.Hyper-parameter settings are listed in Appendix A.</p>
<p>Main Results</p>
<p>We report the main results on the four benchmarks in Table 1.</p>
<p>Below we highlight our key findings.</p>
<p>MOSA Leads in</p>
<p>Analysis</p>
<p>We perform a comprehensive analysis on MOSA in this section.Specifically, we scale the diversity of the single-LLM search baseline in § 4.1 and compare it with MOSA.In § 4.2, we vary the number of distinct LLMs in MOSA.In § 4.3, we combine MOSA with the rich set of actions proposed by Qi et al. (2024).Finally, we evaluate variations of MOSA by ablating the numbers of proposers and aggregators in § 4.4.</p>
<p>Diversity versus Performance</p>
<p>For single-LLM search, a common technique to increase generation diversity is to manipulate with decoding hyperparameters, e.g., the sampling temperature.We modify the temperature of the RAP + Single-LLM as Aggregator base- line on two datasets, with T = {0.25,0.5, 0.75, 1.0, 1.25}.</p>
<p>Diversity is assessed by calculating the {1, 2, 3, 4}-gram Vendi Score (Friedman &amp; Dieng, 2023) across search trajectories.Figure 4 illustrates that while the reasoning accuracy of RAP initially improves with increasing diversity, it subsequently declines.More importantly, the two benchmarks favor different temperature values.This suggests that attaining an optimal balance between diversity and reasoning performance requires careful tuning, as balancing diversity and quality can be challenging (Zhang et al., 2020).In contrast, RAP + MOSA with the default sampling parameters consistently holds an advantageous position.</p>
<p>Ablation of LLM Collaboration</p>
<p>To evaluate the impact of varying the number of different LLMs in MOSA, we conduct an analysis using 1 to 4 LLMs across four benchmarks, prioritizing them in the following order: Llama, GLM, Qwen, Ministral.All four variants utilize approximately the same number of LLM forward calls, ensuring that the only variable is the number of distinct LLMs involved.Figure 5 shows that increasing the number of different LLMs generally correlates with higher reasoning accuracy, except for a slight decrease in performance when the number of LLMs increases from 3 to 4 on MATH-500.This trend indicates that the diverse expertise contributed by different LLMs significantly enhances search-based reasoning performance.</p>
<p>Support for Extended Action Set</p>
<p>rStar (Qi et al., 2024)   We also found that enriching the action set is not always beneficial.On StrategyQA, the expanded action set yielded inferior performance; however, we note that MOSA still demonstrates improvements.</p>
<p>Ablation of Proposers &amp; Aggregators</p>
<p>We consider to isolate the effects of MOSA as Proposers and MOSA as Aggregators by ablating the number of distinct LLMs for those two roles.As shown in Table 3, changing the number of distinct proposers to be single yields a larger decrease comparing with ablating the number of aggregators (-1.23% versus -0.47%), suggesting that MOSA brings more benefits as proposers.The recent focus on large language models is partly due to their exceptional performance in solving complex reasoning tasks.A prominent example is Chain-of-Thought (CoT) reasoning (Wei et al., 2023).Recent advancements include self-consistency (Wang et al., 2023), problem decomposition (Zhou et al., 2023b), the use of tools (Gao et al., 2023;Chen et al., 2023), and search-based methods (Hao et al., 2023;Yao et al., 2024;Qi et al., 2024).Among these approaches, MOSA is most closely aligned with search-based reasoning methods.</p>
<p>Search-based Reasoning Search-based reasoning has demonstrated effectiveness, particularly for solving complex, multi-step problems (Hao et al., 2023;Yao et al., 2024;Chen et al., 2024c;Zhang et al., 2024a;Chen et al., 2024a;Qi et al., 2024;Zhang et al., 2024b;Zhou et al., 2023a;Koh et al., 2024).One of the recent state-of-the-art systems in this domain is rStar (Qi et al., 2024).rStar introduces two key innovations: (1) expanding the Monte Carlo Tree Search (MCTS) action space from one or two actions to five; and (2) employing a secondary LLM to verify the reasoning trajectories generated by the primary LLM through MCTS.In § 4.3, we empirically demonstrate that our method is complementary to the enriched action set of rStar.</p>
<p>LLM Ensemble</p>
<p>Ensembling, a widely used technique for leveraging the strengths of multiple models, remains highly effective in the era of LLMs.Jiang et al. (2023) proposed pairwise reranking of LLM outputs and fusing multiple responses using a trained generative model.Several studies have proposed training routing functions to match queries with appropriate LLMs (Lu et al., 2023;Shnitzer et al., 2023;Wang et al., 2024a).Others have proposed averaging the output distributions of multiple LLMs (Huang et al., 2024).</p>
<p>Another line of research focuses on multi-agent collaboration, where multiple LLMs interact to discuss or debate specific topics (Du et al., 2023;Liang et al., 2024;Chan et al., 2023;Xu et al., 2023;Liu et al., 2024;He et al., 2023;Chen et al., 2024b;Zhang et al., 2024c).Common design variations in this paradigm include role assignments for LLMs (e.g., debaters and judges) and discussion mechanisms (e.g., symmetric versus asymmetric interactions).</p>
<p>Conclusion</p>
<p>In this work, we investigated a novel paradigm called MOSA.MOSA combines independent exploration and iterative refinement among multiple LLMs to enhance reasoning diversity and accuracy.Experiments across benchmarks demonstrate its consistent advantages over single-LLM and multi-agent baselines, especially in complex tasks.This work underscores the potential of multi-agent collaboration in advancing search-based reasoning.1. Critically assess the information in the responses, as some may be biased, incorrect, or nonsensical.</p>
<ol>
<li>Your synthesized response should not merely echo the provided answers but should present a refined, accurate, and thorough response to the prompt.3.Ensure that your response is well-organized, coherent, and meets the highest standards of accuracy and reliability.</li>
</ol>
<p>User:</p>
<p>[Here is the user query that was fed into those AI assistants]: Given a question, please decompose it into sub-questions.For each sub-question, please answer it in a complete sentence, ending with "The answer is <a numeric answer >".When the original question is answerable, please start the subquestion with "Now we can answer the question: <original question>".Question 5: Find the product CD of the integers C and D for which</p>
<p>Figure 1 :
1
Figure 1: Reasoning performance on MATH-500 against search trajectory diversity.While the diversity of single-LLM search varies with different sampling temperatures, the multi-LLM search consistently achieves superior performance.More details are provided in § 4.1.</p>
<p>Figure 2 :
2
Figure 2: Top: An overview of the root node s 0 and its expanded child nodes.Bottom: The detailed framework for generating new actions (i.e., sampling sub-questions and sub-answers).</p>
<p>Figure 3 :
3
Figure 3: Generate three new actions using MOSA.Left: Use MOSA to propose sub-questions and sub-answers.Right: Use MOSA to aggregate candidate sub-answers.</p>
<p>Figure 4 :
4
Figure 4: Diversity versus accuracy.T = Temperature.</p>
<p>Figure 5 :
5
Figure 5: Reasoning accuracy with different number of distinct LLMs as search agents.</p>
<p>−</p>
<p>x 2 + 5x − 24 for all real values of x except −8 and 3. Question 5.1: What is the equation obtained by multiplying both sides of the given equation by (x − 3)(x + 8)?Answer 5.1:(x − 3)(x + 8) • C x−3 + (x − 3)(x + 8) • D x+8 = (x − 3)(x + 8) • 4x−23 x 2 +5x−24This simplifies toC(x + 8) + D(x − 3) = 4x − 23.The answer is C(x + 8) + D(x − 3) = 4x− 23.Question 5.2: What are the values of C and D that satisfy the equation C(x + 8) + D(x − 3) = 4x − 23 for all x except −8 and 3? Answer 5.2: [Here are the responses from various AI assistants]: Response 1: To find the values of C and D, we can substitute specific values of x into the equation C(x + 8) + D(x − 3) = 4x − 23 to obtain a system of equations.Question 5.2.1:What happens if we substitute x = 3 into the equation C(x + 8) + D(x − 3) = 4x − 23? Answer 5.2.1:Substituting x = 3 gives C(3 + 8) + D(3 − 3) = 4 • 3 − 23, which simplifies to 11C = −11.The answer is 11C = −11.Question 5.2.2:What happens if we substitute x = −8 into the equation C(x + 8) + D(x − 3) = 4x − 23? Answer 5.2.2:Substituting x = −8 gives C(−8 + 8) + D(−8 − 3) = 4 • (−8) − 23, which simplifies to −11D = −59.The answer is −11D = −59.Question 5.2.3:Now we can answer the question: What are the values of C and D? Answer 5.2.3:Solving the system of equations 11C = −11 and −11D = −59 gives C = −1 and D = 7/11 • 11 = 7.The answer is -1 and 7 respectively.Response 2: To find the values of C and D, we can compare the coefficients of the corresponding terms on both sides of the equation.For the x 2 terms, we have C + D = 0.For the x terms, we have 8C − 3D = 4. Solving these equations simultaneously, we find that C = 3 and D = −3.Therefore, the product CD = 3 • (−3) = −9 .Response 3: To find C and D, we can substitute x = −8 and x = 3 into the equation C(x + 8) + D(x − 3) = 4x − 23.For x = −8: C(−8 + 8) + D(−8 − 3) = 4(−8) − 23 answer the question: What is the product CD of the integers C and D? To find C and D, we can set x = 3 and x = −8 to get two equations: 1. C(3 + 8) + D(3 − 3) = 4(3) − 23 ⇒ 11C = −11 ⇒ C = −1 2. C(−8 + 8) + D(−8 − 3) = 4(−8) − 23 ⇒ −11D = −55 ⇒ D = 5 The answer is C = −1 and D = 5.</p>
<p>:</p>
<p>Selected node si; Number of sub-questions nq; Number of candidate sub-answers per sub-question na; A set of LLMs π mix = {π1, π2, ..., πm} Ensure: A set of new actions new actions = {action1, action2, ..., actionn q }
Initialize new actions ← ∅ % Prepare the set of new actionsfor i = 1 to nq doπ sub q ← SelectLLM(π mix ) % Select an LLM for generating sub-questionsub question i ← GenerateSubQuestion(π sub q , si) % Generate the i-th sub-questionInitialize candidate sub answers ← ∅ % Store candidate sub-answers for sub-question ifor j = 1 to na doπ sub a ← SelectLLM(π mix ) % Select an LLM for generating a sub-answercandidate sub answerj←GenerateSubAnswer(π sub a , si, sub question i ) % Generate the j-thcandidate sub-answercandidate sub answers.add(candidate sub answerj) % Store the candidate sub-answerend forsub answeri ← FinalizeSubAnswer(candidate sub answers) % Aggregate or vote on candidatesub-answersactioni ← concat(sub question i , sub answeri) % Form the final action by concatenationnew actions.add(actioni) % Add the action to the set of new actionsend forreturn new actions</p>
<p>Table 1 :
1
(Qi et al., 2024)e rows marked by ‡ were reported by the rStar paper(Qi et al., 2024)using Llama-3-8B-Instruct.All other results are reported by our experiments.MULTI refers to multi-LLM while STG represents StrategyQA.The highest number on each dataset is marked in bold while the secondary high is underlined.
Reasoning Tasks RAP + MOSA as Pro-posers &amp; Aggregators consistently yields superior perfor-mances across all datasets (GSM8K, SVAMP, MATH-500,StrategyQA), reaching an average performance (Avg.) of79.97%. Specifically, it obtains exceptional improvements(+1.8%) over the best baseline on the challenging MATH-500 benchmark, suggesting it is effective at handling com-plex reasoning problems.Synergistic Effect between Multi-Agent Collaborationand Search-based Reasoning MOSA integrates two re-search paradigms: multi-agent collaboration and search-based reasoning. When applied independently, eachachieves moderate improvements, but their combination</p>
<p>Table 2 :
2
(Qi et al., 2024)SA combined with rStar(Qi et al., 2024), a recent SoTA MCTS-based reasoning method that extends the set of actions.Those results marked with ‡ were reported by the rStar paper using Llama-3-8B-Instruct.All other results are reported by our experiments.The definitions of A{1, 2, 3, 4, 5} are in § 2.1.The highest number on each dataset is marked in bold while the secondary high is underlined.
METHODACTION SETGSM8K SVAMP MATHSTGAVG.RAP  ‡A{2, 3}80.5985.7018.8068.71 63.45RAPA{2, 3}90.5291.6053.0075.40 77.63+ MOSA AS PROPOSERSA{2, 3}91.1394.5054.6075.69 78.98+ MOSA AS PROPOSERS &amp; AGGREGATORSA{2, 3}91.9694.9056.6076.42 79.97RSTAR  ‡A{1, 2, 3, 4, 5}88.7091.8938.3071.47 72.59RSTARA{1, 2, 3, 4, 5}91.3693.3059.0074.96 79.66+ MOSA AS PROPOSERSA{1, 2, 3, 4, 5}91.9695.6063.2075.11 81.47+ MOSA AS PROPOSERS &amp; AGGREGATORS A{1, 2, 3, 4, 5}92.0495.1063.6075.40 81.54</p>
<p>Table 3 :
3
Ablation analysis to isolate the effects of MOSA as proposers and as aggregators, respectively, for search-based reasoning.By multi, we are referring to the default setting in our experiments, i.e., 4 distinct LLMs.The highest number on each dataset is marked in bold while the secondary high is underlined.</p>
<h1>PROPOSER(S) #AGGREGATOR(S) GSM8K SVAMP MATH STRATEGYQAAVG.SINGLENONE90.5291.6053.0075.4077.63SINGLESINGLE90.0592.5054.8075.6978.26SINGLEMULTI91.0591.9055.6076.4278.74MULTINONE91.1394.5054.6075.6978.98MULTISINGLE91.6694.2056.0076.1379.50MULTIMULTI91.9694.9056.6076.4279.975. Related Work5.1. Reasoning with LLMs</h1>
<p>You are an intelligent and supportive AI assistant.You will receive a collection of responses from various AI assistants regarding a query.Your goal is to synthesize these responses into a single, high-quality response.You should first write down your thoughts as "### Thoughts: <THOUGHTS>.", and then write down the synthesized response in the format of "### Synthesized Response: <SYNTHESIZED RESPONSE>.".If "The answer is: <ANSWER>" appears in the set of responses, you should also write down "The answer is: <ANSWER>"; otherwise, do not write it.
Basic Instruction for Aggregators<strong>TASK</strong>:
The Chinese University of Hong Kong
Shanghai AI Laboratory.
For clarity in illustrating our motivation, we simplify the correctness of candidate sub-answers into two groups: good and bad. This abstraction helps explain the role of aggregators in improving answer correctness, though actual correctness exists on a spectrum depending on task complexity and evaluation criteria.
https://github.com/zhentingqi/rStar/
Impact StatementThis work aims to contribute to the advancement of reasoning with LLMs.While our research could have various societal implications, none are deemed significant enough to warrant specific mention at this stage.A. Additional Experimental SettingsA.1. Hyper-parametersThe default sampling parameters for LLM generation are {temperature=0.75,top k=40, top p=0.95}.Across all MCTS experiments, we set the number of rollouts to 8, the number of sub-questions per node to 4, the number of candidate sub-answers per sub-question to 4, the maximum depth allowed to 5.A.2. Dataset StatisticsSince we make use of the rStar code base 2 to implement MOSA, we directly adopt the data files released in their git repository.There are 1,319 instances in GSM8K, 1,000 instances in SVAMP, 500 instances in MATH-500, and 687 instances in StrategyQA.B. Additional Implementation Details for AggregatorsIn this section, we will show the basic instruction and several in-context learning demonstrations for aggregators.1.The responses contain valuable information about the composition of a water polo team.2. There is a consensus that a water polo team consists of 13 players in total.However, there is some variation in how these 13 players are categorized.3. The key point to clarify is how many players are in the water at one time and how many are substitutes.4. It appears that in the water during play, there are 7 players (including the goalkeeper), and the remaining players are substitutes.### Synthesized Response:The synthesized response is: To form a water polo team, a total of 13 players are required.This team includes 7 players who are in the water during play, comprising one goalkeeper and six field players.The remaining 6 players serve as substitutes.
Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, R Gerstenberger, M Podstawski, L Gianinazzi, J Gajda, T Lehmann, H Niewiadomski, P Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Chateval: Towards better llmbased evaluators through multi-agent debate. C.-M Chan, W Chen, Y Su, J Yu, W Xue, S Zhang, J Fu, Z Liu, 2023</p>
<p>Alphamath almost zero: Process supervision without process. G Chen, M Liao, C Li, Fan , K , 2024a</p>
<p>Round-table conference improves reasoning via consensus among diverse LLMs. J Chen, S Saha, M Bansal, Reconcile, 10.18653/v1/2024.acl-long.381Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAugust 2024b1Association for Computational Linguistics</p>
<p>Understanding when tree of thoughts succeeds: Larger models excel in generation, not discrimination. Q Chen, X Wang, P Mondorf, M A Hedderich, B Plank, 2024c</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. W Chen, X Ma, X Wang, W W Cohen, Transactions on Machine Learning Research. 2023</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, 2021</p>
<p>Efficient selectivity and backup operators in monte-carlo tree search. R Coulom, Computers and Games. H J Van Den Herik, P Ciancarini, H H L M J Donkers, Berlin, Heidelberg; Berlin HeidelbergSpringer2007</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Y Du, S Li, A Torralba, J B Tenenbaum, I Mordatch, 2023</p>
<p>The vendi score: A diversity evaluation metric for machine learning. D Friedman, A B Dieng, Transactions on Machine Learning Research. 2835-88562023</p>
<p>Pal: program-aided language models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, ICML'23. JMLR.orgProceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. M Geva, D Khashabi, E Segal, T Khot, D Roth, J Berant, 10.1162/tacla00370Transactions of the Association for Computational Linguistics. 92021</p>
<p>. T Glm, A Zeng, B Xu, B Wang, C Zhang, D Yin, D Rojas, G Feng, H Zhao, H Lai, H Yu, H Wang, J Sun, J Zhang, J Cheng, J Gui, J Tang, J Zhang, J Li, L Zhao, L Wu, L Zhong, M Liu, M Huang, P Zhang, Q Zheng, R Lu, S Duan, S Zhang, S Cao, S Yang, W L Tam, W Zhao, X Liu, X Xia, X Zhang, X Gu, X Lv, X Liu, X Liu, X Yang, X Song, X Zhang, Y An, Y Xu, Y Niu, Y Yang, Y Li, Y Bai, Y Dong, Z Qi, Z Wang, Z Yang, Z Du, Z Hou, Wang , 2024Z. Chatglm: A family of large language models from glm-130b to glm-4 all tools</p>
<p>. A Grattafiori, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Vaughan, A Yang, A Fan, A Goyal, A Hartshorn, A Yang, A Mitra, A Sravankumar, A Korenev, A Hinsvark, A Rao, A Zhang, A Rodriguez, A Gregerson, A Spataru, B Roziere, B Biron, B Tang, B Chern, C Caucheteux, C Nayak, C Bi, C Marra, C Mcconnell, C Keller, C Touret, C Wu, C Wong, C C Ferrer, C Nikolaidis, D Allonsius, D Song, D Pintz, D Livshits, D Wyatt, D Esiobu, D Choudhary, D Mahajan, D Garcia-Olano, D Perino, D Hupkes, E Lakomkin, E Albadawy, E Lobanova, E Dinan, E M Smith, F Radenovic, F Guzmán, F Zhang, G Synnaeve, G Lee, G L Anderson, G Thattai, G Nail, G Mialon, G Pang, G Cucurell, H Nguyen, H Korevaar, H Xu, H Touvron, I Zarov, I A Ibarra, I Kloumann, I Misra, I Evtimov, J Zhang, J Copet, J Lee, J Geffert, J Vranes, J Park, J Mahadeokar, </p>
<p>. J Shah, J Van Der Linde, J Billock, J Hong, J Lee, J Fu, J Chi, J Huang, J Liu, J Wang, J Yu, J Bitton, J Spisak, J Park, J Rocca, J Johnstun, J Saxe, J Jia, K V Alwala, K Prasad, K Upasani, K Plawiak, K Li, K Heafield, K Stone, K El-Arini, K Iyer, K Malik, K Chiu, K Bhalla, K Lakhotia, L Rantala-Yeary, L Van Der Maaten, L Chen, L Tan, L Jenkins, L Martin, L Madaan, L Malo, L Blecher, L Landzaat, L De Oliveira, M Muzzi, M Pasupuleti, M Singh, M Paluri, M Kardas, M Tsimpoukelli, M Oldham, M Rita, M Pavlova, M Kambadur, M Lewis, M Si, M K Singh, M Hassan, N Goyal, N Torabi, N Bashlykov, N Bogoychev, N Chatterji, N Zhang, O Duchenne, O ¸elebi, P Alrassy, P Zhang, P Li, P Vasic, P Weng, P Bhargava, P Dubal, P Krishnan, P S Koura, P Xu, Q He, Q Dong, R Srinivasan, R Ganapathy, R Calderer, R S Cabral, R Stojnic, R Raileanu, R Maheswari, R Girdhar, R Patel, R Sauvestre, R Polidoro, R Sumbaly, R Taylor, R Silva, R Hou, R Wang, S Hosseini, S Chennabasappa, S Singh, S Bell, S S Kim, S Edunov, S Nie, S Narang, S Raparthy, S Shen, S Wan, S Bhosale, S Zhang, S Vandenhende, S Batra, S Whitman, S Sootla, S Collot, S Gururangan, S Borodinsky, T Herman, T Fowler, T Sheasha, T Georgiou, T Scialom, T Speckbacher, T Mihaylov, T Xiao, U Karn, V Goswami, V Gupta, V Ramanathan, V Kerkez, V Gonguet, V Do, V Vogeti, V Albiero, V Petrovic, W Chu, W Xiong, W Fu, W Meers, X Martinet, X Wang, X Wang, X E Tan, X Xia, X Xie, X Jia, X Wang, Y Goldschlag, Y Gaur, Y Babaei, Y Wen, Y Song, Y Zhang, Y Li, Y Mao, Z D Coudert, Z Yan, Z Chen, Z Papakipos, A Singh, A Srivastava, A Jain, A Kelsey, A Shajnfeld, A Gangidi, A Victoria, A Goldstand, A Menon, A Sharma, A Boesenberg, A Baevski, A Feinstein, A Kallet, A Sangani, A Teo, A Yunus, A Lupu, A Alvarado, A Caples, A Gu, A Ho, A Poulton, A Ryan, A Ramchandani, A Dong, A Franco, A Goyal, A Saraf, A Chowdhury, A Gabriel, A Bharambe, A Eisenman, A Yazdan, B James, B Maurer, B Leonhardi, B Huang, B Loyd, B D Paola, B Paranjape, B Liu, B Wu, B Ni, B Hancock, B Wasti, B Spence, B Stojkovic, B Gamido, B Montalvo, C Parker, C Burton, C Mejia, C Liu, C Wang, C Kim, C Zhou, C Hu, C.-H Chu, C Cai, C Tindal, C Feichtenhofer, C Gao, D Civin, D Beaty, D Kreymer, D Li, D Adkins, D Xu, D Testuggine, D David, D Parikh, D Liskovich, D Foss, D Wang, D Le, D Holland, E Dowling, E Jamil, E Montgomery, E Presani, E Hahn, E Wood, E.-T Le, E Brinkman, E Arcaute, E Dunbar, E Smothers, F Sun, F Kreuk, F Tian, F Kokkinos, F Ozgenel, F Caggioni, F Kanayet, F Seide, G M Florez, G Schwarz, G Badeer, G Swee, G Halpern, G Herman, G Sizov, Guangyi, Zhang, G Lakshminarayanan, H Inan, H Shojanazeri, H Zou, H Wang, H Zha, H Habeeb, H Rudolph, H Suk, H Aspegren, H Goldman, H Zhan, I Damlaj, I Molybog, I Tufanov, I Leontiadis, I.-E Veliche, I Gat, J Weissman, J Geboski, J Kohli, J Lam, J Asher, J.-B Gaya, J Marcus, J Tang, J Chan, J Zhen, J Reizenstein, J Teboul, J Zhong, J Jin, J Yang, J Cummings, J Carvill, J Shepard, J Mcphie, J Torres, J Ginsburg, J Wang, K Wu, U , K H Saxena, K Khandelwal, K Zand, K Matosich, K Veeraraghavan, K Michelena, K Li, K Jagadeesh, K Huang, K Chawla, K Huang, K Chen, L Garg, L , A , L Silva, L Bell, L Zhang, L Guo, L Yu, L Moshkovich, L Wehrstedt, L Khabsa, M Avalani, M Bhatt, M Mankus, M Hasson, M Lennie, M Reso, M Groshev, M Naumov, M Lathi, M Keneally, M Liu, M Seltzer, M L Valko, M Restrepo, M Patel, M Vyatskov, M Samvelyan, M Clark, M Macey, M Wang, M Hermoso, M J Metanat, M Rastegari, M Bansal, M Santhanam, N Parks, N White, N Bawa, N Singhal, N Egebo, N Usunier, N Mehta, N Laptev, N P Dong, N Cheng, N Chernoguz, O Hart, O Salpekar, O Kalinli, O Kent, P Parekh, P Saab, P Balaji, P Rittner, P Bontrager, P Roux, P Dollar, P Zvyagina, P Ratanchandani, P Yuvraj, P Liang, Q Alao, R Rodriguez, R Ayub, R Murthy, R Nayani, R Mitra, R Parthasarathy, R Li, R Hogan, R Battey, R Wang, R Howes, R Rinott, R Mehta, S Siby, S Bondu, S J Datta, S Chugh, S Hunt, S Dhillon, S Sidorov, S Pan, S Mahajan, S Verma, S Yamamoto, S Ramaswamy, S Lindsay, S Lindsay, S Feng, S Lin, S Zha, S C Patil, S Shankar, S Zhang, S Zhang, S Wang, S Agarwal, S Sajuyigbe, S Chintala, S Max, S Chen, S Kehoe, S Satterfield, S Govindaprasad, S Gupta, S Deng, S Cho, S Virk, S Subramanian, S Choudhury, S Goldman, S Remez, T Glaser, T Best, T Koehler, T Robinson, T Li, T Zhang, T Matthews, T Chou, T Shaked, T Vontimitta, V Ajayi, V Montanez, V Mohan, V Kumar, V S Mangla, V Ionescu, V Poenaru, V Mihailescu, V T Ivanov, V Li, W Wang, W Jiang, W Bouaziz, W Constable, W Tang, X Wu, X Wang, X Wu, X Gao, X Kleinman, Y Chen, Y Hu, Y Jia, Y Qi, Y Li, Y Zhang, Y Zhang, Y Adi, Y Nam, Y Yu, Wang, Y Zhao, Y Hao, Y Qian, Y Li, Y He, Z Rait, Z Devito, Z Rosnbrick, Z Wen, Z Yang, Z Zhao, Ma , 2024The llama 3 herd of models</p>
<p>Reasoning with language model is planning with world model. S Hao, Y Gu, H Ma, J Hong, Z Wang, D Wang, Z Hu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language Processing</p>
<p>Association for Computational Linguistics. Singapore, 10.18653/v1/2023December 2023emnlp-main</p>
<p>URL. </p>
<p>A multi-agent collaborative framework with role-playing and iterative feedback for causality explanation generation. Z He, P Cao, Y Chen, K Liu, R Li, M Sun, J Zhao, Lego, doi: 10.18653Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>URL. </p>
<p>Measuring mathematical problem solving with the math dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, 2021NeurIPS</p>
<p>The curious case of neural text degeneration. A Holtzman, J Buys, L Du, M Forbes, Y Choi, International Conference on Learning Representations. 2020</p>
<p>Ensemble learning for heterogeneous large language models with deep parallel collaboration. Y Huang, X Feng, B Li, Y Xiang, H Wang, B Qin, T Liu, 2024</p>
<p>LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. D Jiang, X Ren, B Y Lin, 10.18653/v1/2023.acl-long.792Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Bandit based monte-carlo planning. L Kocsis, C Szepesvári, Machine Learning: ECML 2006. J Fürnkranz, T Scheffer, M Spiliopoulou, Berlin, Heidelberg; Berlin HeidelbergSpringer2006</p>
<p>Tree search for language model agents. J Y Koh, S Mcaleer, D Fried, R Salakhutdinov, 2024</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. T Liang, Z He, W Jiao, X Wang, Y Wang, R Wang, Y Yang, S Shi, Z Tu, 10.18653/v1/2024.emnlp-main.992Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y.-N Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNovember 2024</p>
<p>Let's verify step by step. H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, arXiv:2305.200502023arXiv preprint</p>
<p>A dynamic LLM-powered agent network for task-oriented agent collaboration. Z Liu, Y Zhang, P Li, Y Liu, D Yang, First Conference on Language Modeling. 2024</p>
<p>Routing to the expert: Efficient reward-guided ensemble of large language models. K Lu, H Yuan, R Lin, J Lin, Z Yuan, C Zhou, J Zhou, 2023</p>
<p>Un ministral, des ministraux -mistral ai -frontier ai in your hands. A T Mistral, </p>
<p>Are NLP models really able to solve simple math word problems?. A Patel, S Bhattamishra, N Goyal, K Rumshisky, A Zettlemoyer, L Hakkani-Tur, D Beltagy, I Bethard, S , doi: 10.18653Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. R Chakraborty, T Zhou, Y , the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJune 2021Toutanova,</p>
<p>URL. </p>
<p>Mutual reasoning makes smaller llms stronger problemsolvers. Z Qi, M Ma, J Xu, L L Zhang, F Yang, M Yang, arXiv:2408.061952024arXiv preprint</p>
<p>Large language model routing with benchmark datasets. T Shnitzer, A Ou, M Silva, K Soule, Y Sun, J Solomon, N Thompson, M Yurochkin, 2023</p>
<p>Fusing models with complementary expertise. H Wang, F M Polo, Y Sun, S Kundu, E Xing, M Yurochkin, The Twelfth International Conference on Learning Representations. 2024a</p>
<p>J Wang, J Wang, B Athiwaratkun, C Zhang, J Zou, Mixture-of-agents enhances large language model capabilities. 2024b</p>
<p>Selfconsistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, S Narang, A Chowdhery, D Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 2023</p>
<p>Towards reasoning in large language models via multi-agent peer review collaboration. Z Xu, S Shi, B Hu, J Yu, D Li, M Zhang, Y Wu, 2023</p>
<p>A Yang, B Yang, B Hui, B Zheng, B Yu, C Zhou, C Li, C Li, D Liu, F Huang, G Dong, H Wei, H Lin, J Tang, J Wang, J Yang, J Tu, J Zhang, J Ma, J Yang, J Xu, J Zhou, J Bai, J He, J Lin, K Dang, K Lu, K Chen, K Yang, M Li, M Xue, N Ni, P Zhang, P Wang, R Peng, R Men, R Gao, R Lin, S Wang, S Bai, S Tan, T Zhu, T Li, T Liu, W Ge, X Deng, X Zhou, X Ren, X Zhang, X Wei, X Ren, X Liu, Y Fan, Y Yao, Y Zhang, Y Wan, Y Chu, Y Liu, Z Cui, Z Zhang, Z Guo, Fan , Qwen2 technical report. 2024</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. D Zhang, X Huang, D Zhou, Y Li, W Ouyang, 2024a</p>
<p>Rest-mcts*: Llm self-training via process reward guided tree search. D Zhang, S Zhoubian, Z Hu, Y Yue, Y Dong, J Tang, 2024b</p>
<p>Trading off diversity and quality in natural language generation. H Zhang, D Duckworth, D Ippolito, A Neelakantan, CoRR, abs/2004.104502020</p>
<p>Exploring collaboration mechanisms for LLM agents: A social psychology view. J Zhang, X Xu, N Zhang, R Liu, B Hooi, S Deng, doi: 10.18653Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 2024c1</p>
<p>URL. </p>
<p>Language agent tree search unifies reasoning acting and planning in language models. A Zhou, K Yan, M Shlapentokh-Rothman, H Wang, Y.-X Wang, arXiv:2310.044062023aarXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q V Le, E H Chi, The Eleventh International Conference on Learning Representations. 2023b</p>            </div>
        </div>

    </div>
</body>
</html>
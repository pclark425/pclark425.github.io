<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9293 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9293</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9293</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-265213255</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.08803v3.pdf" target="_blank">StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving</a></p>
                <p><strong>Paper Abstract:</strong> Most existing prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other instances and lack task-level consistency across the selected few-shot examples. To address these limitations, we propose a comprehensive framework, StrategyLLM, allowing LLMs to perform inductive reasoning, deriving general strategies from specific task instances, and deductive reasoning, applying these general strategies to particular task examples, for constructing generalizable and consistent few-shot prompts. It employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task. Experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (34.2\% $\rightarrow$ 38.8\%), commonsense reasoning (70.3\% $\rightarrow$ 72.5\%), algorithmic reasoning (73.7\% $\rightarrow$ 85.0\%), and symbolic reasoning (30.0\% $\rightarrow$ 79.2\%). Further analysis reveals that StrategyLLM is applicable to various LLMs and demonstrates advantages across numerous scenarios.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9293.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9293.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StrategyLLM vs CoT-SC (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StrategyLLM (strategy-based few-shot prompting) compared to Chain-of-Thought with Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper introduces StrategyLLM, an LLM-based multi-agent framework that automatically generates task-level strategies to build few-shot prompts; it reports consistent gains over chain-of-thought (CoT) with self-consistency (CoT-SC) across math, commonsense, algorithmic, and symbolic reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-16k-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math reasoning, Commonsense reasoning, Algorithmic reasoning, Symbolic reasoning (multiple datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks including the MATH suite (AL, PA, IA, CP, NT, GE, PC) for math; StrategyQA and Date Understanding (DU) for commonsense; Word Sorting (WS) and Multi-step Arithmetic (MA) for algorithmic; Last Letter Concatenation (LLC) with OOD splits for symbolic.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Strategy-based few-shot prompting: prompts consist of a general task-level strategy (sequence of subtasks) paired with consistent instance executions following that strategy; strategy generation, execution, optimization, and evaluation performed by internal LLM agents. Few-shot examples used during strategy generation (typically 4 for MATH).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Chain-of-Thought prompting with self-consistency (CoT-SC) and standard few-shot prompting (SP); also SolutionLLM (few-shot examples with instance-specific solutions generated by LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>StrategyLLM reported improved aggregate performance vs CoT-SC: Math (CoT-SC 34.2% → StrategyLLM 38.8%); Commonsense (70.3% → 72.5%); Algorithmic (73.7% → 85.0%); Symbolic (30.0% → 79.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT-SC baselines as above (e.g., CoT-SC math 34.2%); SolutionLLM and SP lower than CoT and StrategyLLM in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Absolute improvements reported ranging from a few points (commonsense +2.2%) to very large gains for some tasks (symbolic +49.2% absolute on LLC OOD sets); algorithmic +11.3% absolute.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors attribute gains to (1) task-level generalizable strategies improving out-of-distribution and complex reasoning, and (2) prompt consistency (all few-shot examples following the same strategy) making it easier for the model to learn and apply the approach. Strategy prompts encode high-level task knowledge rather than instance-specific heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Main experiments use GPT-3.5-turbo-16k-0613 as backend for all StrategyLLM agents. Strategy generator creates n=30 strategies (temperature=1); executor/optimizer/evaluator use greedy decoding (temperature=0). Execution threshold set to 0.75 (0.5 for CP and GE). Validation set size 100; for large test sets, up to 200 test examples sampled to reduce API cost. Typically 4 few-shot examples used for MATH tasks; top-k (k=1 or 3) strategies selected for inference. Cost per strategy prompt generation reported ($0.08–$1.12 depending on GPT-3.5 version and task).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9293.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9293.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StrategyLLM-SC vs CoT-SC (task-level multiple-strategy voting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StrategyLLM with Self-Consistency (majority voting over multiple strategy-based solutions) versus CoT with Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using multiple distinct strategy-based prompts to generate multiple solutions and taking a majority vote (StrategyLLM-SC) yields higher accuracy than applying self-consistency to CoT reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-16k-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CP (Counting & Probability), StrategyQA, MA (Multi-step Arithmetic) and other complex tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Commonsense and math/algorithmic tasks where multiple complementary strategies can cover different instance types.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>StrategyLLM-SC: select top-k strategies (k up to 9 explored) created by StrategyLLM, run each strategy on examples to produce multiple solutions, aggregate answers by majority vote (self-consistency style).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>CoT-SC: chain-of-thought with temperature sampling to generate multiple reasoning paths and majority-vote aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On CP, StrategyQA, and MA datasets, StrategyLLM-SC consistently outperforms CoT-SC when multiple solutions are employed (see figures/tables: StrategyLLM-SC > CoT-SC across k values). Example aggregated numbers: StrategyLLM-SC reported 72.5% avg on commonsense (relative +3.1% vs CoT-SC).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT-SC baseline values varied by dataset but were consistently below StrategyLLM-SC in reported experiments; StrategyLLM-SC also outperforms single-strategy StrategyLLM in many complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Relative improvements reported in the paper (e.g., +3.1% relative on commonsense average) and larger absolute improvements on some tasks when multiple strategies are used.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Multiple complementary strategies cover different subsets of examples; majority voting over strategy-based solutions leverages complementary strengths and increases coverage. Also, explicit task-level strategies produce higher-quality diverse solutions than temperature-sampled CoT chains.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Authors experimented with using top 1,3,5,7,9 strategies; found increasing k often helps but not guaranteed. Coverage (percent of examples solvable by at least one strategy) exceeds majority-vote accuracy, indicating room for selection mechanisms. Execution used greedy decoding; sampling only in the generator; evaluation used validation set of size 100. StrategyLLM-ZS is an alternative aggregation that asks an LLM to choose among solution candidates (zero-shot), and sometimes outperforms majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9293.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9293.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StrategyLLM-ZS vs StrategyLLM-SC (aggregator style)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StrategyLLM with Zero-Shot adjudication (StrategyLLM-ZS) versus majority-vote aggregation (StrategyLLM-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two methods to combine multiple strategy-derived solutions: majority-vote (SC) and asking an LLM to decide the final answer given multiple solutions (ZS); ZS can outperform SC on intricate tasks like math.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-16k-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH benchmark (various subjects)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Challenging math competition problems requiring multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>StrategyLLM-ZS: generate multiple strategy-consistent solutions, then use a zero-shot prompt that provides the candidate solutions and asks the model to determine the final answer; StrategyLLM-SC uses majority vote over answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>StrategyLLM-SC (majority vote).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported that StrategyLLM-ZS outperforms StrategyLLM-SC on the math reasoning task (example averages: StrategyLLM-SC ~37.2% vs StrategyLLM-ZS ~38.8% on MATH reported in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Absolute improvement modest but consistent on math; on other tasks SC and ZS relative performance varies by dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Small absolute gains (≈1.6 percentage points in reported MATH average example), but authors note ZS is more appropriate for intricate tasks where examing multiple detailed solutions helps.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Allowing the LLM to adjudicate among full solutions enables exploitation of the richer information in the step-by-step reasoning (rather than simple plurality), which helps for complex reasoning where different strategies may produce conflicting answers that need nuanced evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>ZS prompt provided in Appendix D; greedy decoding used for inference. Top-k strategies (k=3 in many experiments) were provided as candidate solutions to the adjudicator LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9293.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9293.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plan-and-Solve / CoT+Strategy vs StrategyLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of Plan-and-Solve and CoT+Strategy (instance-specific plans) against StrategyLLM (task-level strategies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper compares instance-specific planning at inference (Plan-and-Solve) and a CoT variant that asks for a strategy at inference time (CoT+Strategy) with StrategyLLM and finds StrategyLLM substantially better.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-16k-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CP, StrategyQA, MA (representative tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Representative complex tasks where plan or strategy could be employed; CP = Counting & Probability; MA = Multi-step Arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Plan-and-Solve: request the model to generate a plan for each test instance at inference and then execute it; CoT+Strategy: prompt to generate a general strategy and apply it to a specific example at inference time. StrategyLLM: offline produced task-level strategy-based few-shot prompt used for all test instances.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>StrategyLLM (optimal and SC variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported averages (GPT-3.5): Plan-and-Solve: ~49.8; Plan-and-Solve-SC: ~54.0; CoT+Strategy: ~52.1; CoT+Strategy-SC: ~58.8; StrategyLLM: 63.1; StrategyLLM-SC: 66.7 (table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>StrategyLLM (and StrategyLLM-SC) substantially outperform Plan-and-Solve and CoT+Strategy across the three tested datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>StrategyLLM-SC outperforms CoT+Strategy-SC by ~7.9 percentage points on average in the reported table (66.7 vs 58.8).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Instance-specific plans are often low-quality or encode limited task-general knowledge; generating high-quality per-example plans at inference is hard. Task-level strategies that are induced and validated offline give higher-quality, consistent guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Experiments run with GPT-3.5; Plan-and-Solve and CoT+Strategy prompts described; CoT+Strategy prompt included in Appendix F. Validation/test selection and evaluation protocols same as main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9293.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9293.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inconsistent few-shot prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of inconsistent few-shot prompts (mixing strategies across examples) on LLM performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper constructs inconsistent few-shot prompts by mixing examples generated from different strategy-based prompts, showing that prompt inconsistency substantially degrades performance compared to strategy-consistent prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-16k-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CP, StrategyQA, MA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Representative tasks used to test robustness to prompt consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Inconsistent prompt: for each test example, create a few-shot prompt by sampling examples uniformly from multiple different strategy-based prompts (top-3 strategies applied randomly to few-shot examples), generating a prompt whose examples do not all follow the same strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Consistent strategy-based prompt (StrategyLLM optimal strategy used consistently across few-shot examples).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On CP/StrategyQA/MA: Inconsistent Prompt average = 54.3 vs StrategyLLM optimal strategy = 63.1 (Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Consistently applying a single effective strategy yields substantially higher performance (~+8.8 absolute on the three-task average in the reported example).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Substantial negative effect: inconsistent prompts underperform consistent strategy prompts by multiple percentage points (≈8.8 absolute in reported three-dataset average).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Prompt inconsistency confuses the model because few-shot examples fail to present a stable task-level procedure; consistent examples that all follow a strategy provide clearer inductive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Inconsistent prompts constructed by randomly and uniformly applying top-3 strategies to examples in the prompt. Other settings as in main experiments (validation size 100, greedy decoding).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9293.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9293.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-8 (cost-matched) comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of StrategyLLM to a cost-matched CoT baseline with 8 few-shot examples (CoT-8)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>To control for increased token cost of strategy-based prompts, the authors compare StrategyLLM to a CoT baseline containing 8 few-shot CoT examples (CoT-8) and find StrategyLLM still outperforms CoT-8.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-16k-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH benchmark (various subjects)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MATH datasets where CoT prompts have human-annotated solutions; StrategyLLM uses 4 few-shot examples for MATH in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>CoT-8: chain-of-thought few-shot prompting with 8 examples (first 4 same as baseline CoT) to match average input/output token cost of StrategyLLM prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>StrategyLLM (optimal strategy-based prompt, typically using 4 examples for MATH generation but containing general strategy text and example executions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports StrategyLLM achieves marked improvement over CoT-8 on MATH datasets (detailed table in Appendix/Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>StrategyLLM outperforms cost-matched CoT (CoT-8), indicating gains are not solely due to larger prompt size.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Improvements persist after controlling for prompt length/cost because strategy content (task-level structure) matters beyond token count.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>CoT-8 constructed by adding 4 more human CoT examples (average tokens ~2697 input, ~288 output for MATH datasets); StrategyLLM prompt includes a general strategy plus example executions, leading to higher token use but still outperforms when CoT is given more human examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9293.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9293.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Universal gains across LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Application of strategy-based prompting (StrategyLLM) across multiple base LLMs (closed- and open-source)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>StrategyLLM's strategy-based prompts improve performance across a variety of LLMs, including closed-source models (GPT-4, Claude-3-Sonnet) and open-source models (Meta-Llama-3 variants, Mixtral), with particularly notable gains for smaller open-source models on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Claude-3-Sonnet, Meta-Llama-3-8B-Instruct, Meta-Llama-3-70B-Instruct, Mixtral-8x7B-Instruct-v0.1, Mixtral-8x22B-Instruct-v0.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varies (8B, 70B, etc.; null when unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CP, StrategyQA, MA (representative tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Selected tasks representing different reasoning types to test universality.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Strategy-based few-shot prompts produced by StrategyLLM (same procedure applied across different base LLMs), with selection of top strategies and either single-strategy or multi-strategy inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>CoT and CoT-SC baselines run on the same LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported improvements across models; for open-source models (e.g., Meta-Llama-3-8B-Instruct, Mixtral-8x7B), StrategyLLM and StrategyLLM-SC gave substantial gains on CP and MA relative to CoT-SC (Table 5), e.g., StrategyLLM-SC improved averages by several relative percentage points (exact values in Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Relative improvements vary by model and task; smaller open-source models showed larger relative gains on complex tasks compared to their CoT-SC baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Task-level strategies supply structured guidance that compensates for weaker inherent reasoning capabilities of smaller models, enabling larger relative improvements. StrategyLLM is robust across model capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Experiments reported in Tables 4 and 5. When applying StrategyLLM to closed-source models, the same multi-agent pipeline was used to generate and evaluate strategies; validation and execution settings followed the main experimental protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9293.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9293.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SolutionLLM (instance solutions in prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SolutionLLM: few-shot prompts where LLMs directly write instance-specific solutions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline where LLMs are prompted to generate full solutions for each few-shot example (greedy decoding) without a task-level strategy; used to isolate effects of strategy vs. LLM-written solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-16k-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various tasks (MATH, StrategyQA, WS, MA, LLC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks as StrategyLLM experiments, but few-shot examples' solutions are produced by LLM rather than human or strategy-based generation.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompt containing question-answer pairs where the solution steps are generated by an LLM (SolutionLLM) using greedy decoding; no explicit general strategy text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>StrategyLLM (task-level strategy with consistent executions), CoT (human CoT examples), SP (standard QA pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>SolutionLLM typically outperforms plain SP but underperforms StrategyLLM and often underperforms CoT depending on dataset (see Table 2 and related results). Example: SolutionLLM numbers vary by dataset but were consistently below StrategyLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>StrategyLLM > SolutionLLM; CoT sometimes > SolutionLLM depending on human-annotated solutions availability.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Instance-specific LLM-written solutions can be inconsistent and lack task-level generalizations, reducing generalizability vs strategy-based prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>SolutionLLM prompt shown in Appendix E; greedy decoding used to create few-shot solutions. Comparisons control for human involvement by generating both SolutionLLM and StrategyLLM prompts using LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. <em>(Rating: 2)</em></li>
                <li>Plan-and-Solve prompting <em>(Rating: 1)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 1)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9293",
    "paper_id": "paper-265213255",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "StrategyLLM vs CoT-SC (aggregate)",
            "name_full": "StrategyLLM (strategy-based few-shot prompting) compared to Chain-of-Thought with Self-Consistency",
            "brief_description": "This paper introduces StrategyLLM, an LLM-based multi-agent framework that automatically generates task-level strategies to build few-shot prompts; it reports consistent gains over chain-of-thought (CoT) with self-consistency (CoT-SC) across math, commonsense, algorithmic, and symbolic reasoning benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-16k-0613)",
            "model_size": null,
            "task_name": "Math reasoning, Commonsense reasoning, Algorithmic reasoning, Symbolic reasoning (multiple datasets)",
            "task_description": "Benchmarks including the MATH suite (AL, PA, IA, CP, NT, GE, PC) for math; StrategyQA and Date Understanding (DU) for commonsense; Word Sorting (WS) and Multi-step Arithmetic (MA) for algorithmic; Last Letter Concatenation (LLC) with OOD splits for symbolic.",
            "presentation_format": "Strategy-based few-shot prompting: prompts consist of a general task-level strategy (sequence of subtasks) paired with consistent instance executions following that strategy; strategy generation, execution, optimization, and evaluation performed by internal LLM agents. Few-shot examples used during strategy generation (typically 4 for MATH).",
            "comparison_format": "Chain-of-Thought prompting with self-consistency (CoT-SC) and standard few-shot prompting (SP); also SolutionLLM (few-shot examples with instance-specific solutions generated by LLM).",
            "performance": "StrategyLLM reported improved aggregate performance vs CoT-SC: Math (CoT-SC 34.2% → StrategyLLM 38.8%); Commonsense (70.3% → 72.5%); Algorithmic (73.7% → 85.0%); Symbolic (30.0% → 79.2%).",
            "performance_comparison": "CoT-SC baselines as above (e.g., CoT-SC math 34.2%); SolutionLLM and SP lower than CoT and StrategyLLM in reported experiments.",
            "format_effect_size": "Absolute improvements reported ranging from a few points (commonsense +2.2%) to very large gains for some tasks (symbolic +49.2% absolute on LLC OOD sets); algorithmic +11.3% absolute.",
            "explanation_or_hypothesis": "Authors attribute gains to (1) task-level generalizable strategies improving out-of-distribution and complex reasoning, and (2) prompt consistency (all few-shot examples following the same strategy) making it easier for the model to learn and apply the approach. Strategy prompts encode high-level task knowledge rather than instance-specific heuristics.",
            "null_or_negative_result": false,
            "experimental_details": "Main experiments use GPT-3.5-turbo-16k-0613 as backend for all StrategyLLM agents. Strategy generator creates n=30 strategies (temperature=1); executor/optimizer/evaluator use greedy decoding (temperature=0). Execution threshold set to 0.75 (0.5 for CP and GE). Validation set size 100; for large test sets, up to 200 test examples sampled to reduce API cost. Typically 4 few-shot examples used for MATH tasks; top-k (k=1 or 3) strategies selected for inference. Cost per strategy prompt generation reported ($0.08–$1.12 depending on GPT-3.5 version and task).",
            "uuid": "e9293.0",
            "source_info": {
                "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "StrategyLLM-SC vs CoT-SC (task-level multiple-strategy voting)",
            "name_full": "StrategyLLM with Self-Consistency (majority voting over multiple strategy-based solutions) versus CoT with Self-Consistency",
            "brief_description": "Using multiple distinct strategy-based prompts to generate multiple solutions and taking a majority vote (StrategyLLM-SC) yields higher accuracy than applying self-consistency to CoT reasoning paths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-16k-0613)",
            "model_size": null,
            "task_name": "CP (Counting & Probability), StrategyQA, MA (Multi-step Arithmetic) and other complex tasks",
            "task_description": "Commonsense and math/algorithmic tasks where multiple complementary strategies can cover different instance types.",
            "presentation_format": "StrategyLLM-SC: select top-k strategies (k up to 9 explored) created by StrategyLLM, run each strategy on examples to produce multiple solutions, aggregate answers by majority vote (self-consistency style).",
            "comparison_format": "CoT-SC: chain-of-thought with temperature sampling to generate multiple reasoning paths and majority-vote aggregation.",
            "performance": "On CP, StrategyQA, and MA datasets, StrategyLLM-SC consistently outperforms CoT-SC when multiple solutions are employed (see figures/tables: StrategyLLM-SC &gt; CoT-SC across k values). Example aggregated numbers: StrategyLLM-SC reported 72.5% avg on commonsense (relative +3.1% vs CoT-SC).",
            "performance_comparison": "CoT-SC baseline values varied by dataset but were consistently below StrategyLLM-SC in reported experiments; StrategyLLM-SC also outperforms single-strategy StrategyLLM in many complex tasks.",
            "format_effect_size": "Relative improvements reported in the paper (e.g., +3.1% relative on commonsense average) and larger absolute improvements on some tasks when multiple strategies are used.",
            "explanation_or_hypothesis": "Multiple complementary strategies cover different subsets of examples; majority voting over strategy-based solutions leverages complementary strengths and increases coverage. Also, explicit task-level strategies produce higher-quality diverse solutions than temperature-sampled CoT chains.",
            "null_or_negative_result": false,
            "experimental_details": "Authors experimented with using top 1,3,5,7,9 strategies; found increasing k often helps but not guaranteed. Coverage (percent of examples solvable by at least one strategy) exceeds majority-vote accuracy, indicating room for selection mechanisms. Execution used greedy decoding; sampling only in the generator; evaluation used validation set of size 100. StrategyLLM-ZS is an alternative aggregation that asks an LLM to choose among solution candidates (zero-shot), and sometimes outperforms majority voting.",
            "uuid": "e9293.1",
            "source_info": {
                "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "StrategyLLM-ZS vs StrategyLLM-SC (aggregator style)",
            "name_full": "StrategyLLM with Zero-Shot adjudication (StrategyLLM-ZS) versus majority-vote aggregation (StrategyLLM-SC)",
            "brief_description": "Two methods to combine multiple strategy-derived solutions: majority-vote (SC) and asking an LLM to decide the final answer given multiple solutions (ZS); ZS can outperform SC on intricate tasks like math.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-16k-0613)",
            "model_size": null,
            "task_name": "MATH benchmark (various subjects)",
            "task_description": "Challenging math competition problems requiring multi-step reasoning.",
            "presentation_format": "StrategyLLM-ZS: generate multiple strategy-consistent solutions, then use a zero-shot prompt that provides the candidate solutions and asks the model to determine the final answer; StrategyLLM-SC uses majority vote over answers.",
            "comparison_format": "StrategyLLM-SC (majority vote).",
            "performance": "Reported that StrategyLLM-ZS outperforms StrategyLLM-SC on the math reasoning task (example averages: StrategyLLM-SC ~37.2% vs StrategyLLM-ZS ~38.8% on MATH reported in figures).",
            "performance_comparison": "Absolute improvement modest but consistent on math; on other tasks SC and ZS relative performance varies by dataset.",
            "format_effect_size": "Small absolute gains (≈1.6 percentage points in reported MATH average example), but authors note ZS is more appropriate for intricate tasks where examing multiple detailed solutions helps.",
            "explanation_or_hypothesis": "Allowing the LLM to adjudicate among full solutions enables exploitation of the richer information in the step-by-step reasoning (rather than simple plurality), which helps for complex reasoning where different strategies may produce conflicting answers that need nuanced evaluation.",
            "null_or_negative_result": false,
            "experimental_details": "ZS prompt provided in Appendix D; greedy decoding used for inference. Top-k strategies (k=3 in many experiments) were provided as candidate solutions to the adjudicator LLM.",
            "uuid": "e9293.2",
            "source_info": {
                "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Plan-and-Solve / CoT+Strategy vs StrategyLLM",
            "name_full": "Comparison of Plan-and-Solve and CoT+Strategy (instance-specific plans) against StrategyLLM (task-level strategies)",
            "brief_description": "The paper compares instance-specific planning at inference (Plan-and-Solve) and a CoT variant that asks for a strategy at inference time (CoT+Strategy) with StrategyLLM and finds StrategyLLM substantially better.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-16k-0613)",
            "model_size": null,
            "task_name": "CP, StrategyQA, MA (representative tasks)",
            "task_description": "Representative complex tasks where plan or strategy could be employed; CP = Counting & Probability; MA = Multi-step Arithmetic.",
            "presentation_format": "Plan-and-Solve: request the model to generate a plan for each test instance at inference and then execute it; CoT+Strategy: prompt to generate a general strategy and apply it to a specific example at inference time. StrategyLLM: offline produced task-level strategy-based few-shot prompt used for all test instances.",
            "comparison_format": "StrategyLLM (optimal and SC variants).",
            "performance": "Reported averages (GPT-3.5): Plan-and-Solve: ~49.8; Plan-and-Solve-SC: ~54.0; CoT+Strategy: ~52.1; CoT+Strategy-SC: ~58.8; StrategyLLM: 63.1; StrategyLLM-SC: 66.7 (table 6).",
            "performance_comparison": "StrategyLLM (and StrategyLLM-SC) substantially outperform Plan-and-Solve and CoT+Strategy across the three tested datasets.",
            "format_effect_size": "StrategyLLM-SC outperforms CoT+Strategy-SC by ~7.9 percentage points on average in the reported table (66.7 vs 58.8).",
            "explanation_or_hypothesis": "Instance-specific plans are often low-quality or encode limited task-general knowledge; generating high-quality per-example plans at inference is hard. Task-level strategies that are induced and validated offline give higher-quality, consistent guidance.",
            "null_or_negative_result": false,
            "experimental_details": "Experiments run with GPT-3.5; Plan-and-Solve and CoT+Strategy prompts described; CoT+Strategy prompt included in Appendix F. Validation/test selection and evaluation protocols same as main experiments.",
            "uuid": "e9293.3",
            "source_info": {
                "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Inconsistent few-shot prompts",
            "name_full": "Effect of inconsistent few-shot prompts (mixing strategies across examples) on LLM performance",
            "brief_description": "The paper constructs inconsistent few-shot prompts by mixing examples generated from different strategy-based prompts, showing that prompt inconsistency substantially degrades performance compared to strategy-consistent prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-16k-0613)",
            "model_size": null,
            "task_name": "CP, StrategyQA, MA",
            "task_description": "Representative tasks used to test robustness to prompt consistency.",
            "presentation_format": "Inconsistent prompt: for each test example, create a few-shot prompt by sampling examples uniformly from multiple different strategy-based prompts (top-3 strategies applied randomly to few-shot examples), generating a prompt whose examples do not all follow the same strategy.",
            "comparison_format": "Consistent strategy-based prompt (StrategyLLM optimal strategy used consistently across few-shot examples).",
            "performance": "On CP/StrategyQA/MA: Inconsistent Prompt average = 54.3 vs StrategyLLM optimal strategy = 63.1 (Table 11).",
            "performance_comparison": "Consistently applying a single effective strategy yields substantially higher performance (~+8.8 absolute on the three-task average in the reported example).",
            "format_effect_size": "Substantial negative effect: inconsistent prompts underperform consistent strategy prompts by multiple percentage points (≈8.8 absolute in reported three-dataset average).",
            "explanation_or_hypothesis": "Prompt inconsistency confuses the model because few-shot examples fail to present a stable task-level procedure; consistent examples that all follow a strategy provide clearer inductive bias.",
            "null_or_negative_result": true,
            "experimental_details": "Inconsistent prompts constructed by randomly and uniformly applying top-3 strategies to examples in the prompt. Other settings as in main experiments (validation size 100, greedy decoding).",
            "uuid": "e9293.4",
            "source_info": {
                "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "CoT-8 (cost-matched) comparison",
            "name_full": "Comparison of StrategyLLM to a cost-matched CoT baseline with 8 few-shot examples (CoT-8)",
            "brief_description": "To control for increased token cost of strategy-based prompts, the authors compare StrategyLLM to a CoT baseline containing 8 few-shot CoT examples (CoT-8) and find StrategyLLM still outperforms CoT-8.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-16k-0613)",
            "model_size": null,
            "task_name": "MATH benchmark (various subjects)",
            "task_description": "MATH datasets where CoT prompts have human-annotated solutions; StrategyLLM uses 4 few-shot examples for MATH in main experiments.",
            "presentation_format": "CoT-8: chain-of-thought few-shot prompting with 8 examples (first 4 same as baseline CoT) to match average input/output token cost of StrategyLLM prompt.",
            "comparison_format": "StrategyLLM (optimal strategy-based prompt, typically using 4 examples for MATH generation but containing general strategy text and example executions).",
            "performance": "Paper reports StrategyLLM achieves marked improvement over CoT-8 on MATH datasets (detailed table in Appendix/Table 10).",
            "performance_comparison": "StrategyLLM outperforms cost-matched CoT (CoT-8), indicating gains are not solely due to larger prompt size.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Improvements persist after controlling for prompt length/cost because strategy content (task-level structure) matters beyond token count.",
            "null_or_negative_result": false,
            "experimental_details": "CoT-8 constructed by adding 4 more human CoT examples (average tokens ~2697 input, ~288 output for MATH datasets); StrategyLLM prompt includes a general strategy plus example executions, leading to higher token use but still outperforms when CoT is given more human examples.",
            "uuid": "e9293.5",
            "source_info": {
                "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Universal gains across LLMs",
            "name_full": "Application of strategy-based prompting (StrategyLLM) across multiple base LLMs (closed- and open-source)",
            "brief_description": "StrategyLLM's strategy-based prompts improve performance across a variety of LLMs, including closed-source models (GPT-4, Claude-3-Sonnet) and open-source models (Meta-Llama-3 variants, Mixtral), with particularly notable gains for smaller open-source models on complex tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, Claude-3-Sonnet, Meta-Llama-3-8B-Instruct, Meta-Llama-3-70B-Instruct, Mixtral-8x7B-Instruct-v0.1, Mixtral-8x22B-Instruct-v0.1",
            "model_size": "varies (8B, 70B, etc.; null when unspecified)",
            "task_name": "CP, StrategyQA, MA (representative tasks)",
            "task_description": "Selected tasks representing different reasoning types to test universality.",
            "presentation_format": "Strategy-based few-shot prompts produced by StrategyLLM (same procedure applied across different base LLMs), with selection of top strategies and either single-strategy or multi-strategy inference.",
            "comparison_format": "CoT and CoT-SC baselines run on the same LLMs.",
            "performance": "Reported improvements across models; for open-source models (e.g., Meta-Llama-3-8B-Instruct, Mixtral-8x7B), StrategyLLM and StrategyLLM-SC gave substantial gains on CP and MA relative to CoT-SC (Table 5), e.g., StrategyLLM-SC improved averages by several relative percentage points (exact values in Table 5).",
            "performance_comparison": "Relative improvements vary by model and task; smaller open-source models showed larger relative gains on complex tasks compared to their CoT-SC baselines.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Task-level strategies supply structured guidance that compensates for weaker inherent reasoning capabilities of smaller models, enabling larger relative improvements. StrategyLLM is robust across model capabilities.",
            "null_or_negative_result": false,
            "experimental_details": "Experiments reported in Tables 4 and 5. When applying StrategyLLM to closed-source models, the same multi-agent pipeline was used to generate and evaluate strategies; validation and execution settings followed the main experimental protocol.",
            "uuid": "e9293.6",
            "source_info": {
                "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "SolutionLLM (instance solutions in prompt)",
            "name_full": "SolutionLLM: few-shot prompts where LLMs directly write instance-specific solutions",
            "brief_description": "A baseline where LLMs are prompted to generate full solutions for each few-shot example (greedy decoding) without a task-level strategy; used to isolate effects of strategy vs. LLM-written solutions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-16k-0613)",
            "model_size": null,
            "task_name": "Various tasks (MATH, StrategyQA, WS, MA, LLC)",
            "task_description": "Same tasks as StrategyLLM experiments, but few-shot examples' solutions are produced by LLM rather than human or strategy-based generation.",
            "presentation_format": "Few-shot prompt containing question-answer pairs where the solution steps are generated by an LLM (SolutionLLM) using greedy decoding; no explicit general strategy text.",
            "comparison_format": "StrategyLLM (task-level strategy with consistent executions), CoT (human CoT examples), SP (standard QA pairs).",
            "performance": "SolutionLLM typically outperforms plain SP but underperforms StrategyLLM and often underperforms CoT depending on dataset (see Table 2 and related results). Example: SolutionLLM numbers vary by dataset but were consistently below StrategyLLM.",
            "performance_comparison": "StrategyLLM &gt; SolutionLLM; CoT sometimes &gt; SolutionLLM depending on human-annotated solutions availability.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Instance-specific LLM-written solutions can be inconsistent and lack task-level generalizations, reducing generalizability vs strategy-based prompts.",
            "null_or_negative_result": false,
            "experimental_details": "SolutionLLM prompt shown in Appendix E; greedy decoding used to create few-shot solutions. Comparisons control for human involvement by generating both SolutionLLM and StrategyLLM prompts using LLMs.",
            "uuid": "e9293.7",
            "source_info": {
                "paper_title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.",
            "rating": 2,
            "sanitized_title": "planandsolve_prompting_improving_zeroshot_chainofthought_reasoning_by_large_language_models"
        },
        {
            "paper_title": "Plan-and-Solve prompting",
            "rating": 1,
            "sanitized_title": "planandsolve_prompting"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        }
    ],
    "cost": 0.02100875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving
24 May 2024</p>
<p>Chang Gao gaochang@se.cuhk.edu 
The Chinese University of Hong Kong ♢ Sun Yat-sen University ♡ Tencent AI Lab</p>
<p>Haiyun Jiang haiyunjiangnlp@gmail.com 
The Chinese University of Hong Kong ♢ Sun Yat-sen University ♡ Tencent AI Lab</p>
<p>The Chinese University of Hong Kong ♢ Sun Yat-sen University ♡ Tencent AI Lab</p>
<p>Deng Cai 
The Chinese University of Hong Kong ♢ Sun Yat-sen University ♡ Tencent AI Lab</p>
<p>Shuming Shi shumingshi@tencent.com 
The Chinese University of Hong Kong ♢ Sun Yat-sen University ♡ Tencent AI Lab</p>
<p>Wai Lam wlam@se.cuhk.edu 
The Chinese University of Hong Kong ♢ Sun Yat-sen University ♡ Tencent AI Lab</p>
<p>StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving
24 May 2024A3D40D90082904AB187C78E37938E3A3arXiv:2311.08803v3[cs.CL]
Most existing prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other instances and lack task-level consistency across the selected few-shot examples.To address these limitations, we propose a comprehensive framework, StrategyLLM, allowing LLMs to perform inductive reasoning, deriving general strategies from specific task instances, and deductive reasoning, applying these general strategies to particular task examples, for constructing generalizable and consistent few-shot prompts.It employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task.Experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (34.2% → 38.8%), commonsense reasoning (70.3% → 72.5%), algorithmic reasoning (73.7% → 85.0%), and symbolic reasoning (30.0%→ 79.2%).Further analysis reveals that StrategyLLM is applicable to various LLMs and demonstrates advantages across numerous scenarios.</p>
<p>Introduction</p>
<p>Recent advances in large language models (LLMs) have facilitated the development of prompting techniques [26,43,20,8].In particular, chain-of-thought (CoT) prompting methods [43,6,12,41], which condition LLMs on a few task examples with step-by-step solutions, guide LLMs to break down complex reasoning processes into simpler steps.These approaches have markedly improved performance compared to standard few-shot prompting across a variety of tasks.</p>
<p>Despite their potential, current CoT approaches employing few-shot prompts with instance-specific solutions may face challenges in terms of generalizability and consistency.Concerning generalizability, the solution can be highly specific to the question in each instance, limiting its applicability to other instances.For example, as illustrated in the left part of Figure 1, a solution for a particular system of linear equations with two variables may not provide valuable insights for addressing another system with three variables.Furthermore, the solutions in different instances within the few-shot prompt may exhibit a lack of task-level consistency, which complicates the process for LLMs to develop effective solutions for tackling new instances.As demonstrated in the left part of Figure 1, the two specific solutions are based on different approaches: Solution 1 employs expression substitution, while Solution 2 utilizes equation subtraction, which may not provide consistent guidance for LLMs to solve new instances.To address these limitations, it is crucial to incorporate effective</p>
<p>Specific Solutions</p>
<ol>
<li>Write the augmented matrix of the system of linear equations.2. Perform row operations to transform the matrix into an upper triangular matrix.The row operations include: … 3. Perform back-substitution to find the solution.problem-solving strategies and develop consistent strategy-based solutions within few-shot prompts.The right part of Figure 1 presents an effective strategy, i.e., Gaussian Elimination Method, offering generalizable steps applicable to any system of linear equations.By providing this strategy and consistently applying it across various instances in the few-shot prompt, LLMs can be better equipped to generate effective solutions for new task instances.</li>
</ol>
<p>Strategy (Gaussian Elimination Method)</p>
<p>This paper aims to construct generalizable and consistent strategy-based few-shot prompts for various tasks automatically, while being highly cost-efficient.Our proposed framework, StrategyLLM, draws inspiration from human cognitive processes to derive general problem-solving strategies.This approach enables LLMs to reason inductively, i.e., deriving general strategies from specific task instances, and deductively, i.e., applying general strategies to particular task examples, to formulate prompts.An example of strategy-based prompts can be seen in Figure 3.The inductive reasoning process enhances generalizability by formulating general problem-solving strategies, while the deductive reasoning process improves consistency by producing consistent solutions using a given strategy.Developing effective problem-solving strategies is crucial to the success of our framework.To achieve this, we design StrategyLLM as a multi-agent collaboration framework comprising four LLM-based agents-strategy generator, executor, optimizer, and evaluator, as shown in Figure 2. The strategy generator initially creates a pool of strategies that are executed on task examples to assess accuracy, with qualified strategies cached based on a threshold and further evaluated.Unqualified ones may be optimized and re-evaluated iteratively.Through the collaboration of these intelligent agents, our framework is capable of autonomously generating, evaluating, and selecting effective strategies for various tasks and eliminating the need for human involvement.</p>
<p>Crucially, the strategy-based few-shot prompt generation phase is applied once for a given task, after which the learned prompt can be employed for inference on the entire test set.This inference process does not require any additional input beyond the standard few-shot prompting settings.The prompt generation process is highly cost-effective as it necessitates only a few task examples.In particular, StrategyLLM expends less than $0.24 to develop a strategy-based prompt for a variety of tasks using the latest version of GPT-3.5-Turbo.</p>
<p>We conduct comprehensive evaluations of StrategyLLM on 13 datasets across 4 challenging tasks: math reasoning, commonsense reasoning, algorithmic reasoning, and symbolic reasoning.The experimental results reveal the following key findings: (1) StrategyLLM outperforms competitive baselines on all tasks without using any human-annotated reasoning processes; (2) StrategyLLM can be applied to various LLMs and is robust to different groups of task examples; (3) StrategyLLM can generate generalizable and consistent prompts in a cost-effective manner.These findings demonstrate the potential of StrategyLLM as an effective, efficient, and reliable problem-solving framework.</p>
<p>StrategyLLM</p>
<p>Our StrategyLLM framework is designed to efficiently create strategy-based few-shot prompts for a wide range of tasks.Subsequently, these prompts can be utilized for inference.In this section, we will introduce our framework in detail.The inference procedure will be discussed in Section 3. Qualified strategies meeting a pre-defined threshold are cached, and if necessary, unqualified strategies are optimized and re-evaluated in iterative cycles.Once a sufficient number of qualified strategies are obtained or the maximum iteration number is reached, the top k strategies are ranked by execution accuracy and evaluated using a validation set.</p>
<p>Overview of StrategyLLM</p>
<p>As presented in Figure 2, our framework consists of four key agents: strategy generator, executor, optimizer, and evaluator.The prompts of the generator, executor, and optimizer are in Appendix C. Typically, only a few task examples are used in the collaboration process, making our framework highly efficient.</p>
<p>The collaboration process begins with the strategy generator formulating a pool of strategies based on its understanding of the target task.Subsequently, the strategy executor applies each strategy to a set of task examples to yield its execution result and compute its execution accuracy.Strategies that meet or exceed a pre-set threshold of execution accuracy are deemed qualified and are cached with their corresponding execution results and accuracy.If the number of qualified strategies is less than a pre-defined number k, the optimizer refines the unqualified strategies using their execution results.These enhanced strategies are then sent back to the strategy executor for the next iteration.This cycle may repeat until a sufficient number of qualified strategies are achieved or the maximum iteration limit is reached.Following this, all cached strategies are ranked based on their execution accuracy, and the top k strategies are selected.Lastly, the strategy evaluator constructs strategy-based few-shot prompts for each candidate strategy using itself and its execution result and assesses all candidate strategies using their corresponding prompts for inference on a validation set.</p>
<p>Notations We use p, q, st, so, and a to denote the prompt, question, strategy, solution, and answer, respectively.During inference, given a question q, the language model M : (p, q) → (so, a) generates a solution so and an answer a for it conditioned on the prompt p.We denote the target task as t, its definition as d, and the set of task examples as E. Each example in E is a (q, a) pair.</p>
<p>Strategy Definition</p>
<p>In this paper, a task-solving strategy st is defined as a systematic approach designed for universal application across task examples, comprising a series of subtasks that encode task knowledge to address the target task t.It is characterized by the following properties: (1) Task-Level Applicability: The strategy is formulated in a manner that allows for its application across all task instances, ensuring universality and consistency in its implementation.(2) Structured Organization: The strategy comprises a sequence of subtasks that are organized in a logical order to collectively tackle the target task.These subtasks are interconnected and contribute to the overall achievement of the task objective.(3) Task Knowledge Encoding: The strategy encapsulates general task knowledge and principles, avoiding any specific details unique to individual task examples.These properties collectively contribute to the effectiveness and efficiency of a strategy by promoting consistency, clarity, and informed decision-making in addressing task-level challenges.By embodying these properties, a strategy can serve as a valuable tool to navigate complex tasks and achieve optimal outcomes.An example of the strategy is presented in Figure 3.</p>
<p>Strategy Generator G The strategy generator, represented as G : (E, d, n) → {st j } n j=1 , aims to generate n diverse strategies for the target task t based on a set of task examples E and the task definition d using temperature sampling.</p>
<p>Figure 3: Comparison of the strategy-based, standard, and chain-of-thought (CoT) [43] prompt.</p>
<p>Strategy Executor X The strategy executor, denoted as X : (E, d, st) → (R st , eacc st ), writes solutions to a set of task examples E following the strategy st to obtain the execution result R st = {(q, so, a)} |E| i=1 of st.The execution accuracy eacc st is calculated as the proportion of examples whose solutions yield correct answers, reflecting the degree of alignment between the strategy and task.Therefore, we select strategies with high execution accuracy as qualified strategies.</p>
<p>Strategy Optimizer O The strategy optimizer, represented as O : (E, d, st, R st ) → st o , optimize the strategy st according to its execution result R st to obtain the updated strategy st o .Firstly, the strategy optimizer O analyzes why some solutions in R st are not correct and provides suggestions for improving st.Secondly, it modifies st to obtain st o based on the analysis and suggestions.</p>
<p>Strategy Evaluator E We select top k candidate strategies according to the execution accuracy.However, to ensure efficiency, we use a limited number of task examples for execution, making the execution accuracy not a very informative metric for choosing strategies.Therefore, we introduce a strategy evaluator to further evaluate the candidate strategies on a validation set V. This process only requires to perform inference once for each candidate strategy and is efficient.The strategy evaluator, denoted as E : (st, R st , V) → vacc st , computes the validation accuracy vacc st of the strategy st on V. To achieve this, it constructs the strategy-based few-shot prompt p st = (st, R st ) and conducts inference on V using p st .An example of strategy-based prompts is presented in Figure 3.The validation accuracy vacc st is calculated as the percentage of validation examples whose answers are correct, reflecting the effectiveness of st in real-world scenarios.Strategies with high validation accuracy can be used for inference.</p>
<p>Inference</p>
<p>Through collaborative efforts among multiple agents, we have obtained multiple candidate strategies, each with its few-shot prompt and validation accuracy.Depending on the task at hand, we can select one or more strategies with high validation accuracy for inference.For simple or specific tasks, a single optimal strategy that solves all task examples effectively may exist, making it sufficient to use only one strategy.However, for complex or diverse tasks, it is unlikely to find a strategy with absolute superiority.In such cases, adopting multiple strategies for inference is more appropriate, as they may excel in different task examples.To harness the strengths of multiple strategies, we employ two methods.The first method involves taking a majority vote on all answers obtained by multiple strategies, akin to the self-consistency (SC) method [41].The second method requires LLMs to determine the final answer by considering the solutions derived from multiple strategies in a zero-shot (ZS) manner, making it more proper for complex and diverse tasks.We denote the first and second methods as StrategyLLM-SC and StrategyLLM-ZS, respectively.The prompt for the second approach is provided in Appendix D.   • Math Reasoning: We use the challenging MATH benchmark [16] which comprises problems from mathematics competitions that require more than standard K-12 mathematics tools.It consists of seven datasets of different subjects, namely, Algebra (AL), Prealgebra (PA), Intermediate Algebra (IA), Counting and Probability (CP), Number Theory (NT), Geometry (GE), and Precalculus (PC).</p>
<p>Experiments</p>
<p>• Commonsense Reasoning: We employ StrategyQA [14] and the Date Understanding (DU) task from Big-Bench Hard [38,9].StrategyQA necessitates inferring a multi-hop strategy to answer questions, while the DU task involves deducing a date from a given context.</p>
<p>• Algorithmic Reasoning: We adopt the Word Sorting (WS) task and the Multi-step Arithmetic (MA) task from Big-Bench Hard [38,9].The WS task involves sorting a list of words lexicographically, and the MA task requires solving multi-step equations with basic arithmetic operations.</p>
<p>• Symbolic Reasoning: We utilize the Last Letter Concatenation (LLC) task from [43], which requires concatenating the last letters of words in a sequence.In the few-shot prompt, the model only sees examples with two words.To evaluate the generalization abilities of different methods, we construct three out-of-distribution test sets (LLC-4, LLC-8, and LLC-16) with 4, 8, and 16 words in a sequence, respectively.</p>
<p>Baselines We conduct experiments in the few-shot setting and compare StrategyLLM with the following baselines:</p>
<p>• Standard Prompting (SP): SP is the most direct approach for problem-solving.In SP, the prompt p contains a set of question-answer pairs without intermediate reasoning steps.</p>
<p>• Chain-of-Thought (CoT) Prompting [43]: CoT incorporates step-by-step solutions for questions in the prompt p to elicit the multi-step reasoning capabilities of LLMs.We use few-shot CoT prompts from [43] for GSM8K, StrategyQA, DU, and LLC, and prompts from [38] for WS and MA.</p>
<p>For MATH datasets, we create few-shot CoT prompts by randomly sampling 4 examples from each dataset's training set since these datasets contain human-annotated solutions.The CoT prompts for these datasets are in Appendix H.</p>
<p>• Self-Consistency with CoT (CoT-SC) [41]: CoT-SC generates a set of solutions using CoT via temperature sampling to obtain multiple answers.Subsequently, it takes a majority vote over these answers to determine the final answer.For experiments, we sample 3 reasoning paths using temperature sampling with a temperature of 0.7.• SolutionLLM: We construct this baseline to leverage LLMs to directly write the solution for each example in the few-shot prompts using greedy decoding, without using any strategies.The prompt of SolutionLLM is in Appendix E. Since both SolutionLLM and StrategyLLM generate prompts using LLMs, we can eliminate the potential effect of human expertise in the comparison, isolating the impact of incorporating effective strategies.</p>
<p>Implementation Details We employ GPT-3.5 (gpt-3.5-turbo-16k-0613)[32] as the language model for our main experiments, serving as the backend for the strategy generator, executor, optimizer, and evaluator.For a fair comparison with baselines such as CoT, we use the same examples in their few-shot prompts for strategy generation, execution, and optimization.We select the top 1 or 3 strategies with the highest validation accuracy for inference.This allows us to demonstrate the performance of the optimal strategy and the benefits of using multiple strategies.We adopt greedy decoding for inference.Details of the strategies for each dataset can be found in Appendix G.The validation set size is 100 for all the datasets.For datasets with over 200 test examples, we randomly sample 200 examples for testing to reduce API costs.More details can be found in Appendix B.</p>
<p>Main Results</p>
<p>Tables 1 and 2 present the experimental results of StrategyLLM and several baselines across four reasoning tasks.We have the following observations:</p>
<p>• StrategyLLM is an effective and efficient framework for problem-solving.StrategyLLM using multiple strategies, i.e., StrategyLLM-SC and StrategyLLM-ZS, outperforms all baselines across the four reasoning tasks.Furthermore, StrategyLLM employing the optimal strategy consistently outperforms CoT and SolutionLLM.Notably, StrategyLLM automatically constructs generalizable and consistent few-shot prompts for tackling various tasks without human expertise, while CoT relies on human-annotated examples for each task.• Explicitly incorporating effective strategies significantly enhance the complex reasoning and out-ofdistribution (OOD) generalization abilities of LLMs.For example, our framework demonstrates more considerable improvements on the MATH benchmark compared to the simpler commonsense reasoning datasets.Furthermore, StrategyLLM substantially surpasses CoT and SolutionLLM on the three OOD test sets of the LLC task, showcasing the generalizability of effective strategies.• Adopting multiple strategies brings obvious benefits on complex or diverse tasks.The performance of StrategyLLM is significantly improved by using multiple strategies on the math and commonsense reasoning tasks.The benefits of leveraging multiple strategies on simpler or more specific tasks, i.e., symbolic and algorithmic reasoning, is less significant.These observations indicate that our framework is capable of creating multiple complementary strategies for diverse or complex tasks.Furthermore, StrategyLLM-ZS outperforms StrategyLLM-SC on the math reasoning task, showing that allowing LLMs to determine the answer is more appropriate for intricate tasks.</p>
<p>Analysis</p>
<p>Evaluating the robustness of StrategyLLM We conduct an investigation to assess the robustness of our StrategyLLM framework with respect to varying groups of examples.For this purpose, we</p>
<p>Exploring the universality of StrategyLLM</p>
<p>To investigate the universality of our StrategyLLM framework, we apply it to a variety of LLMs to evaluate its effectiveness.For closed-source models, we utilize GPT-4 (gpt-4-0613) [31] and Claude-3-Sonnet (claude-3-sonnet-20240229) [2].For opensource models, we employ Meta-Llama-3-8B-Instruct, Meta-Llama-3-70B-Instruct [1], Mixtral-8x7B-Instruct-v0.1, and Mixtral-8x22B-Instruct-v0.1 [18].We conduct experiments on the CP, StrategyQA, and MA datasets, which represent three distinct reasoning tasks.The results, summarized in Tables 4  and 5, reveal that integrating effective strategies for constructing generalizable and consistent fewshot prompts yields significant benefits across a range of model capabilities and task complexities, underscoring the framework's universality.StrategyLLM notably enhances performance in opensource models such as Meta-Llama-3-8B-Instruct and Mixtral-8x7B-Instruct-v0.1, particularly on the CP and MA datasets which demand complex reasoning, indicating the effectiveness of our framework in scenarios requiring sophisticated problem-solving.These findings further corroborate StrategyLLM's robustness and reliability as a problem-solving framework.</p>
<p>Comparing reasoning via task-level strategy and instance-specific planning Our framework facilitates generalizable and consistent reasoning by developing task-level strategies.To evaluate  2) Even when explicitly encouraged to devise a general task-solving strategy in the CoT+Strategy method, the LLM tends to produce strategies that are highly specific to the test example and encode limited task-level knowledge.This underscores the necessity of creating generalizable strategy-based few-shot prompts.</p>
<p>Analyzing the cost of strategy-based prompt generation In this analysis, we evaluate the cost of the strategy-based prompt generation process.The process includes the strategy generator, executor, optimizer, and evaluator, each contributing to the overall cost for each reasoning task.Table 7 details the average cost incurred by our StrategyLLM framework in generating a candidate strategy-based prompt, calculated by dividing the total cost of the process by the number of candidate strategies k.</p>
<p>The costs are presented in terms of input and output tokens and the money associated with using GPT-3.5-Turbo.The results indicate that our framework is economically efficient.The average cost for gpt-3.5-turbo-16k-0613ranges from $0.33 to $1.12 across the four reasoning tasks.For the latest version of GPT-3.5-Turbo,specifically gpt-3.5-turbo-0125, the cost is considerably lower, ranging from $0.08 to $0.24.Generally, tasks of higher complexity consume more tokens due to their inherently longer solutions.Examining results across various difficulty levels The problems in the MATH benchmark are classified by difficulty on a scale of 1 to 5. The easiest problems are assigned a difficulty level of 1, while the most challenging problems are given a difficulty level of 5. Figure 4 illustrates the performance of CoT-SC and StrategyLLM-SC on the seven datasets within the MATH benchmark, considering different difficulty levels.It is evident that the enhanced performance of StrategyLLM-SC over CoT-SC stems from its ability to tackle more complex problems, underscoring the significance of generalizable strategies in augmenting intricate reasoning.</p>
<p>More analysis can be found in Appendix A. Prompting LLMs for Problem Solving The prominent chain-of-thought (CoT) prompting approach [43] has inspired a variety of prompting methods aimed at enhancing the problem-solving abilities of LLMs.These methods include using programming languages to describe the reasoning process [6,13,28], representing the reasoning process with complex structures such as trees or graphs [46,3,36,47], applying task decomposition [49,19,34,4], implementing self-correction with automatic feedback [22,29,30,5,7], and combining different prompting techniques [27,50].However, most of these approaches require manual annotation of reasoning processes, limiting their generalizability and flexibility.By comparison, our StrategyLLM framework can automatically construct strategy-based few-shot prompts for any task, ensuring generalizable and consistent solutions following effective strategies.This approach sets it apart from existing automatic prompt construction methods [48,37,45], which may generate inconsistent solutions within the prompt.The plan-and-solve prompting method [40] aims to address missing-step errors by requesting LLMs to generate a plan before solving a specific example in a zero-shot manner.The plan is instance-specific and significantly different from the task-solving strategy which can be applied to all task examples.The learning-to-plan approach [15] learns a text plan for each task to assist LLMs in problem-solving.</p>
<p>The plan, which is not necessarily a strategy, can be any instruction helpful for solving the task.Moreover, it demands a large training and validation set during the learning process, resulting in high costs.In contrast, our framework is efficient and cost-effective.</p>
<p>LLM-based Autonomous Agents</p>
<p>The adoption of autonomous agents driven by LLMs across various disciplines is revolutionizing our methodologies for tackling problems, making decisions, and fostering innovation [39,44].These agents have been utilized to enhance the reasoning capabilities of LLMs [42,24,11], contribute to social simulation [33,23,25,21], and advance software development [35,17,10].In this paper, we employ multiple LLM-based agents to collaborate in the generation, execution, optimization, and evaluation of problem-solving strategies.</p>
<p>Discussion</p>
<p>Limitation and Impact The key idea behind StrategyLLM is to harness the knowledge and reasoning capabilities of LLMs to develop and refine task-solving strategies tailored to specific tasks.By utilizing the extensive knowledge embedded in these LLMs, which are trained on diverse data sources spanning multiple domains, StrategyLLM is able to generate generalizable strategies that incorporate domain-specific expertise.However, if the model possesses limited knowledge in a particular domain, it is unlikely to create effective strategies for that domain.In such cases, merely optimizing the prompt may not significantly improve performance, and domain-specific continual training may be necessary.As LLMs continue to expand their knowledge bases and enhance their reasoning capabilities, their ability to generate generalizable strategies for diverse tasks is expected to improve, implying the potential of our StrategyLLM framework.</p>
<p>Conclusion</p>
<p>This paper proposes StrategyLLM, harnessing the power of LLMs to construct generalizable and consistent few-shot prompts for various tasks efficiently.Our framework's effectiveness and reliability are substantiated through extensive evaluations on four challenging tasks: mathematical reasoning, commonsense reasoning, algorithmic reasoning, and symbolic reasoning.Further analysis reveals that our framework exhibits robustness across different task example groups, application to various LLMs, cost-efficiency in prompt generation, and effectiveness in complex reasoning.</p>
<p>A Additional Analysis</p>
<p>Examining the optimization process In our primary experiments, the strategy generator initially produces 30 strategies, and we aim to obtain 10 qualified strategies for validation.For complex datasets, it is difficult to directly acquire 10 qualified strategies without optimization.Table 8 presents the optimization process for three challenging datasets: CP from the mathematical reasoning task, StrategyQA from the commonsense reasoning task, and MA from the algorithmic reasoning task, along with the iteration in which the optimal strategy (i.e., the strategy with the highest validation accuracy) is obtained.It is evident that the strategy optimizer plays a vital role in obtaining more qualified strategies and superior strategies, indicating its significance in our framework.</p>
<p>Table 8: Total count of qualified strategies achieved in each iteration of the optimization process and the specific iteration when the optimal strategy is attained.
Iteration CP StrategyQA MA 1 7 9 6 2 12 10 9 3 - -10Optimal 2 1 2
Analyzing the inference cost In this section, we assess the inference cost associated with the optimal strategy-based prompt generated by our StrategyLLM framework, as well as the inference costs of the SolutionLLM and CoT baselines.The costs are represented by the average input and output tokens required for each test example, which are displayed in Table 9.Generally, StrategyLLM consumes more tokens during inference compared to CoT and SolutionLLM, as its prompt encompasses both general strategies and step-by-step solutions adhering to these strategies.In this paper, our primary objective is to develop a framework capable of generating generalizable and consistent prompts for various tasks without human intervention.To reduce inference costs, we may encourage LLMs to create more succinct strategies by imposing additional constraints during the prompt generation process, which will be explored in our future work.</p>
<p>In   guidance for LLMs, requiring them to choose the most appropriate strategy for each test example based on their understanding of the strategies and the test example itself.We evaluate the effect of prompt consistency on the CP, StrategyQA, and MA datasets.Table 11 presents the comparison between this method and StrategyLLM using the optimal strategy.It is evident that the approach employing inconsistent prompts performs considerably worse than StrategyLLM, indicating that automatically identifying the most suitable strategy for each test example is quite difficult.Therefore, consistently applying an effective strategy to various examples within the prompt is advantageous.</p>
<p>Assessing the complementarity of strategies To this end, we employ multiple strategies to derive various solutions and ascertain the answer by majority voting.Specifically, we employ the top 1, 3, 5, 7, and 9 strategies for the CP, StrategyQA, and MA datasets.Figure 5 unveils the following observations: (1) StrategyLLM-SC consistently surpasses CoT-SC on all three datasets when employing multiple solutions, suggesting that explicitly introducing effective strategies to obtain solutions is beneficial; (2) Leveraging multiple strategies outperforms the utilization of a single strategy across all three datasets, implying that multiple complementary strategies exist in most scenarios;</p>
<p>(3) Incorporating additional strategies generally demonstrates advantageous.However, this does not guarantee enhancement in performance, as it relies on the effectiveness of the newly introduced strategies and their complementarity with pre-existing strategies.</p>
<p>Upper limit of accuracy with multiple strategies The upper limit of utilizing multiple strategies can be determined by calculating the coverage, which is defined as the percentage of examples that can be accurately solved by at least one strategy.The coverage represents the maximum potential accuracy achievable with multiple strategies.Figure 6 illustrates the coverage and accuracy (i.e., through a majority vote) of the StrategyLLM using multiple strategies on the CP, StrategyQA, and  MA datasets.We can observe that there is a significant gap between accuracy and coverage, indicating that substantial performance improvements could be realized if the optimal strategy for each test example is selected rather than relying on a simple majority vote.</p>
<p>B Implementation Details</p>
<p>The strategy generator creates n = 30 strategies using temperature sampling with a temperature of 1.The strategy executor, optimizer, and evaluator employ greedy decoding with a temperature of 0. The threshold of the execution accuracy for all datasets is set to 0.75, except for the CP and GE datasets.Their thresholds are set to 0.5 due to the challenging examples in their few-shot prompts.</p>
<p>The maximum iteration number for strategy optimization is 3.We retain k = 10 candidate strategies for validation, selecting the top 1 or 3 strategies with the highest validation accuracy for inference.</p>
<p>C Prompts of Strategy Generator, Executor, and Optimizer</p>
<p>The prompts for the strategy generator and executor are presented in Figure 7 and Figure 8, respectively.For optimizing a strategy, the strategy optimizer conducts the following tasks: (1) constructing the examination result for each task example by comparing the answer extracted from its execution result and the gold answer, which is demonstrated in Figure 9; (2) analyzing the examination results of all provided task examples and providing suggestions for improving the strategy using the prompt in Figure 10; (3) modifying the strategy to obtain an updated strategy based on the feedback, i.e., the analysis and suggestions, leveraging the prompt in Figure 11.</p>
<p>D Prompt of StrategyLLM-ZS</p>
<p>The prompt is shown in Figure 12.</p>
<p>E Prompt of SolutionLLM</p>
<p>The prompt is presented in Figure 13.</p>
<p>F Prompt of CoT+Strategy</p>
<p>The prompt is demonstrated in Figure 14.</p>
<p>Strategy Generator</p>
<p>Task: {task definition} Some examples of the task are as follows: {task examples} Let's understand the task and write a strategy that consists of a sequence of subtasks to solve the task.For writing, you must satisfy the following requirements:</p>
<p>-Include all necessary subtasks.</p>
<p>-All subtasks are easy to solve.</p>
<p>-Subtasks are in an appropriate order.</p>
<p>-Do not include specific information in the examples.</p>
<p>-Make sure the strategy is general and concise.The result must be a numbered list in the following format: 1.First subtask 2. Second subtask</p>
<p>Strategy: {strategy}</p>
<p>The strategy consists of a sequence of subtasks for solving the task.Please execute the strategy on the provided example.For executing, you need to write a step-by-step solution to the example based on the subtasks.The solution must satisfy the following requirements:</p>
<p>-Adjust and execute these subtasks for this example.</p>
<p>-Compute as many intermediate results as possible.</p>
<p>-The answer obtained from the solution must be the same as the original answer.The result must be in the following format: Question: Question in the provided example Solution: Solution obtained based on the subtasks in the strategy Answer: Answer in the provided example, which must include the string 'The answer is '   The answer extracted from the execution result is {predicted answer}, and the correct answer is {gold answer}.They are {different or the same}.</p>
<p>G Strategies and Execution Results</p>
<p>H Chain-of-Thought Prompts for MATH Datasets</p>
<p>MATH-Algebra</p>
<p>Strategy:</p>
<ol>
<li>
<p>Simplify expressions involving radicals by multiplying and combining under a single radical if possible.</p>
</li>
<li>
<p>Rationalize the denominators of fractions with radicals by multiplying the numerator and denominator by the conjugate of the denominator.</p>
</li>
<li>
<p>Use the quadratic formula or factoring to find the roots of quadratic equations.4. Find the coordinates of points of intersection between two curves by solving the system of equations formed by setting the two equations equal to each other.5. Find the product or sum of the roots of a quadratic equation by applying Vieta's formulas.</p>
</li>
</ol>
<p>Executing the strategy on a task example: Question: The quadratic 8x 2 + 12x − 14 has two real roots.What is the sum of the squares of these roots?Express your answer as a common fraction in lowest terms.Solution:</p>
<ol>
<li>Simplify expressions involving radicals by multiplying and combining under a single radical if possible.</li>
</ol>
<p>-There are no radicals in the given quadratic equation, so no simplification is needed.2. Rationalize the denominators of fractions with radicals by multiplying the numerator and denominator by the conjugate of the denominator.</p>
<p>-There are no fractions with radicals in the given quadratic equation, so no rationalization is needed.</p>
<ol>
<li>Use the quadratic formula or factoring to find the roots of quadratic equations.</li>
</ol>
<p>-The quadratic formula is given by:
x = −b± √ b 2 −4ac 2a
, where ax 2 + bx + c = 0. -For the given quadratic equation 8x 2 + 12x − 14, we have a = 8, b = 12, and c = −14.</p>
<p>-Plugging these values into the quadratic formula, we get:
x = −12± √ 12 2 −4(8)(−14) 2(8) x = −12± √ 144+448 16 x = −12± √ 592 16 x = −12±4 √ 37 16 x = −3± √ 37 4
-Therefore, the two real roots of the quadratic equation are −3+ -Let r 1 and r 2 be the roots of the quadratic equation.</p>
<p>-The sum of the squares of the roots is given by r 2 1 + r 2 2 .-Plugging in the values of the roots, we get:
r 2 1 + r 2 2 = −3+ √ 37 4 2 + −3− √ 37 4 2 r 2 1 + r 2 2 = (−3+ √ 37) 2 4 2 + (−3− √ 37) 2 4 2 r 2 1 + r 2 2 = 9−6 √ 37+3716+ 9+6 √ 37+37 16 r 2 1 + r 2 2 = 2(9+37) 16 r 2 1 + r 2 2 = 2(46) 16 r 2 1 + r 2 2 = 92 16 r 2 1 + r 2 2 = 234
Answer: The answer is 23  4 .</p>
<p>Figure 15: Strategy generated by GPT-3.5 for MATH-Algebra and its execution on a task example.</p>
<p>MATH-Intermediate Algebra</p>
<p>Strategy:</p>
<ol>
<li>
<p>Begin by identifying the type of problem you are given: quadratic polynomial, algebraic equation, geometric problem, etc. 2. If the problem involves a polynomial, start by determining the degree of the polynomial and then proceed to solve for the unknown coefficients using the given conditions or given equations.</p>
</li>
<li>
<p>If the problem involves algebraic equations, simplify the expressions and solve for the unknowns by isolating the variable.Make use of algebraic rules such as distributive property, solving equations with fractions, and solving systems of equations if necessary.4. If the problem involves geometric concepts, analyze the given information and understand the geometric relationships involved.Apply the appropriate formulas or theorems and solve for the unknown quantities. 5.If the problem involves inequalities, solve for the range of possible values by analyzing the conditions or constraints given.Make use of algebraic manipulation and reasoning skills to determine the range or the maximum/minimum values.6.If the problem involves functions, analyze the properties of the function and use any given information to determine the unknown variables, such as finding the maximum/minimum values, solving for x-intercepts or y-intercepts, or finding asymptotes.7. Check your solutions by substituting them back into the original equation or problem statement to ensure they satisfy the given conditions.</p>
</li>
</ol>
<p>Executing the strategy on a task example: Question: Find the quadratic polynomial p(x) such that p(−3) = 10, p(0) = 1, and p(2) = 5.. Solution:</p>
<ol>
<li>
<p>Identify the type of problem: quadratic polynomial.</p>
</li>
<li>
<p>Determine the degree of the polynomial and solve for the unknown coefficients using the given conditions.</p>
</li>
</ol>
<p>Let the quadratic polynomial be p(x) = ax 2 + bx + c.Using the given conditions: Answer: The answer is x 2 + 1.
p(−3) = 10 ⇒ a(−3) 2 + b(−3) + c = 10 p(0) = 1 ⇒ a(0) 2 + b(0) + c = 1 p(2) = 5 ⇒ a(2) 2 + b(2) + c =
Figure 17: Strategy generated by GPT-3.5 for MATH-Intermediate Algebra and its execution on a task example.</p>
<p>Strategy:</p>
<ol>
<li>
<p>Read and understand the question fully by carefully analyzing the given information and requirements.</p>
</li>
<li>
<p>Identify the type of question being asked, whether it is a simple "yes or no" question or if there is a need to provide a specific answer.</p>
</li>
<li>
<p>Determine the key elements and concepts mentioned in the question that need to be addressed.4. Break down the question into smaller parts, if necessary, to identify any hidden information or assumptions.5. Look for any clues or hints within the question that can help in finding the answer.6. Decide on the approach to be used, whether it requires logical reasoning, knowledge of specific facts, comparison, or analysis.7. Use prior knowledge and general understanding to answer the question, if applicable.8.If additional information is required, either use external sources or gather relevant data from memory.9. Organize the information and answer the question clearly and concisely.10.Review the answer to ensure it accurately addresses the question and provides a correct response.11.If necessary, double-check the answer to ensure it aligns with the requirements and request.12. Finally, present the answer in a clear and understandable manner, either as a "yes or no" response or as a specific answer as required by the question.</p>
</li>
</ol>
<p>Executing the strategy on a task example: Question: Could Brooke Shields succeed at University of Pennsylvania?Solution:</p>
<ol>
<li>Read and understand the question fully by carefully analyzing the given information and requirements.</li>
</ol>
<p>-The question is asking if Brooke Shields could succeed at the University of Pennsylvania.</p>
<ol>
<li>Identify the type of question being asked, whether it is a simple "yes or no" question or if there is a need to provide a specific answer.</li>
</ol>
<p>-The question requires a simple "yes or no" answer.3. Determine the key elements and concepts mentioned in the question that need to be addressed.</p>
<p>-Key elements: Brooke Shields, University of Pennsylvania, succeed.4. Break down the question into smaller parts, if necessary, to identify any hidden information or assumptions.</p>
<p>-No need to break down the question further. 5. Look for any clues or hints within the question that can help in finding the answer.</p>
<p>-The question does not provide any specific clues or hints.6. Decide on the approach to be used, whether it requires logical reasoning, knowledge of specific facts, comparison, or analysis.</p>
<p>-The approach requires logical reasoning and knowledge of the University of Pennsylvania.7. Use prior knowledge and general understanding to answer the question, if applicable.</p>
<p>-The University of Pennsylvania is a prestigious university known for its rigorous academic programs.Brooke Shields, as a successful actress and model, may have the potential to succeed at the university.8.If additional information is required, either use external sources or gather relevant data from memory.</p>
<p>-No additional information is required.9. Organize the information and answer the question clearly and concisely.</p>
<p>-The answer is yes, Brooke Shields could succeed at the University of Pennsylvania.10.Review the answer to ensure it accurately addresses the question and provides a correct response.</p>
<p>-The answer accurately addresses the question and provides a correct response.11.If necessary, double-check the answer to ensure it aligns with the requirements and request.</p>
<p>-The answer aligns with the requirements and request.12. Finally, present the answer in a clear and understandable manner, either as a "yes or no" response or as a specific answer as required by the question.</p>
<p>Answer: The answer is yes.</p>
<p>Figure 22: Strategy generated by GPT-3.5 for StrategyQA and its execution on a task example.</p>
<p>Date Understanding</p>
<p>Strategy:</p>
<ol>
<li>
<p>Determine the current date based on the given information.Executing the strategy on a task example: Question: Sort the following words alphabetically: List: oven costume counterpart Solution:</p>
</li>
<li>
<p>Identify the list of words that needs to be sorted.</p>
</li>
</ol>
<p>-The list of words is "oven costume counterpart".2. Split the list of words into individual words.</p>
<p>-The individual words are "oven", "costume", and "counterpart".</p>
<ol>
<li>Sort the individual words alphabetically.</li>
</ol>
<p>-The sorted words are "costume", "counterpart", and "oven".4. Combine the sorted words back into a single list.</p>
<p>-The combined sorted list is "costume counterpart oven". 5. Present the sorted list of words as the answer.</p>
<p>Answer: The answer is costume counterpart oven.</p>
<p>Figure 24: Strategy generated by GPT-3.5 for Word Sorting and its execution on a task example.</p>
<p>StrategyQA</p>
<p>Strategy:</p>
<ol>
<li>
<p>Identify the main subject(s) or concept(s) in the question.</p>
</li>
<li>
<p>Identify the relationship(s) or action(s) associated with the main subject(s) or concept(s).</p>
</li>
<li>
<p>Break down the question into sub-questions based on the identified subjects or concepts and their associated relationships or actions, ensuring each sub-question leads to additional information required to answer the main question.8.The answer 'yes' accurately addresses the main question, given the information available.</p>
</li>
<li>
<p>The answer is recorded.</p>
</li>
</ol>
<p>Answer: The answer is yes.Answer: The answer is 10</p>
<p>Question: A standard deck of 52 cards has 13 ranks (Ace, 2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King) and 4 suits (♠, ♡, ♢, and ♣), such that there is exactly one card for any given rank and suit.Two of the suits (♠ and ♣) are black and the other two suits (♡ and ♢) are red.The deck is randomly arranged.What is the probability that the top card is a ♡? Solution: There are 13 ♡'s and 52 cards total, so the probability that the top card is a ♡ is 13 52 = 1 4 .</p>
<p>Answer: The answer is Answer: The answer is 1   19   Question: A pentagon is drawn by placing an isosceles right triangle on top of a square as pictured.What percent of the area of the pentagon is the area of the right triangle?[asy] size (50); draw((0,0)-(0,-1)-(1,-1)-(1,0)-(0,0)-(.5,.5)-(1,0)); [/asy] Solution: Let the leg length of the isosceles right triangle be x, so the hypotenuse of the triangle has length x √ 2. The hypotenuse of the triangle is a side of the square, so the area of the square is (x √ 2) 2 = 2x 2 .The area of the triangle is (x)(x)/2 = x 2 /2.So, the area of the pentagon is
x 2 2 + 2x 2 = 5x 2 2 .
Therefore, the fraction of the pentagon's area that is inside the triangle is
x 2 /2 5x 2 /2 = x 2 2 • 2 5x 2 = 1 5 = 20%.
Answer: The answer is 20 Answer: The answer is x 2 + 1 Question: Calculate:  Answer: The answer is 1</p>
<p>Question: Find 2 −1 (mod 185), as a residue modulo 185.</p>
<p>(Give an answer between 0 and 184, inclusive.)Solution: Since 2 • 93 ≡ 186 ≡ 1 (mod 185), 2 −1 ≡ 93 (mod 185).</p>
<p>Answer: The answer is 93</p>
<p>Question: What is the remainder when 2007 is divided by 81? Solution: Dividing using long division, we find that 2007 = 81 • 24 + 63, so the remainder is 63.</p>
<p>Answer: The answer is 63 CoT Prompt for MATH-Geometry (Part 1)</p>
<p>Question: Two sectors of a circle of radius 12 overlap as shown, with P and R as the centers of the respective circles.Determine the area of the shaded region.</p>
<p>[asy] draw((0,0)-( 10.3923,-6)-(20.7846,0)-(10.3923,6)-cycle,black+linewidth(1)); filldraw ((10.3923,6).. (12,0)..(10.3923,-6)-cycle,gray,black+linewidth(1));filldraw ((10.3923,6).. (8.7846,0)..(10.3923,-6)-cycle,gray,black+linewidth(1));label("P ",(0,0),W); label("Q", (10.3923,6),N); label("R",(20.7846,0),E);label("S",(10.3923,-6),S);label("60 • ",(0,0),2E); label("60 • ",(20.7846,0),2W);[/asy] Solution: By symmetry, the areas of the two parts of the shaded region are equal.Consider the right part of the shaded region and the left triangle.</p>
<p>[asy] draw((0,0)-( 10.3923,-6)-( 10.3923,6)-cycle,black+linewidth(1)); filldraw ((10.3923,6).. (12,0)..( 10.3923,-6)-cycle,gray,black+linewidth(1)); draw((0,0)-(10.3923,0),black+linewidth(1));draw((10.3923,0)-(9.3923,0)-( 9.3923,1)-(10.3923,1),black+linewidth(1));label("P ",(0,0),W); label("Q", (10.3923,6),N); label("S",(10.3923,-6),S);label("Z",(10.3923,0),SW);[/asy] The shaded area is equal to the area of sector P QS minus the area of triangle P QS.Since ∠P QS = 60 • and P Q = 12, the area of sector P QS is
1 6 • 12 2 • π = 24π.
Also, triangle P QS is equilateral with side length 12, so its area is
√ 3 4 • 12 2 = 36 √ 3.
Thus, the area of the right part of the shaded region is 24π − 36 √ 3, so the area of the entire shaded region is 2(24π − 36 √ 3) = 48π − 72 √ 3.</p>
<p>Answer The line y = x 2 will intersect the two vertical sides of the square, as shown below: [asy] real f(real x) { return x/2; } import graph; size(6cm); real a = 8; pair A=(a,a), B=(a,a), C=(a,-a), D=(-a,-a); draw(A-B-C-D-cycle); draw(graph(f,-11,11),Arrows); axes(Arrows(4)); dot("(−a, a)",A,N); dot("(a, a)",B,N); dot("(a, −a)",C,S); dot("(−a, −a)",D,S); real eps=0.2;dot((8,4)h-8,-4)); draw(shift((10,0))<em>"2a",(-a+eps,-a/2-.5)-(a-eps,-a/2-.5),Arrows);draw(shift((0,10))</em>"a",(a+2<em>eps,-a/2)-(a+2</em>eps,a/2),Arrows);[/asy] The equation of the right side of the square is x = a, so we have y = x 2 = a 2 , which means that the intersection point with the right side of the square is a, a 2 .Similarly, the equation of the left side of the square is x = −a, so we have y = x 2 = − a 2 , which means that the intersection point with the left side of the square is −a, − a 2 .It follows that the sides of each quadrilateral have lengths a 2 , 2a, 3a 2 , and a 2 + (2a) 2 = a √ 5, by the Pythagorean theorem.Hence, the perimeter of the quadrilateral is</p>
<p>Solution 1 :Solution 2 :
12
First, solve for  from the first equation:  = (5 − )/2.Then substitute this expression into the second equation: 3 − (5 − )/2 = 8.Solve for  :  = 3.Now substitute  back into the first equation:  = 5 − 3 /2 = 1.Subtracting the second equation from the first equation:  = (8 − 2)/3 = 2. Now substitute  into the second equation:  = 2 + 2 = 4.</p>
<p>7 . 3 Solution 2 : 1 . 2 .. 4 Strategy
732124
Now we have an upper triangular matrix.3. Back-substitution:  = −7/−7 = 1  = (5 − 2 * 1)/1 = Augmented matrix: Row operations: subtracting the first row from the second row 1 Now we have an upper triangular matrix.3. Back-substitution:  = −6/−3 = 2  = (8 − 2 * 2)/1 =</p>
<p>Figure 1 :
1
Figure 1: Comparison of specific solutions and strategy-based solutions.</p>
<p>Figure 2 :
2
Figure 2: Overview of StrategyLLM.Initially, the strategy generator creates a pool of strategies, which are then applied by the strategy executor to task examples to calculate execution accuracy.Qualified strategies meeting a pre-defined threshold are cached, and if necessary, unqualified strategies are optimized and re-evaluated in iterative cycles.Once a sufficient number of qualified strategies are obtained or the maximum iteration number is reached, the top k strategies are ranked by execution accuracy and evaluated using a validation set.</p>
<p>60.0 61.5 18.0 38.5 30.5 28.0 24.0 37.2 (+8.8%)StrategyLLM-ZS 64.5 65.5 19.0 39.0 32.5 28.5 22.5 38.8 (+13.4%)</p>
<p>Figure 4 :
4
Figure 4: Comparison of CoT-SC and StrategyLLM-SC performance on the MATH benchmark across various difficulty levels.</p>
<p>60.0 61.5 18.0 38.5 30.5 28.0 24.0 37.2 (+6.5%)StrategyLLM-ZS 64.5 65.5 19.0 39.0 32.5 28.5 22.5 38.8 (+11.1%)</p>
<p>Figure 5 :
5
Figure 5: Performance of StrategyLLM-SC and CoT-SC on the CP, StrategyQA, and MA datasets.</p>
<p>Figure 6 :
6
Figure 6: Coverage and accuracy of StrategyLLM using multiple strategies on the CP, StrategyQA, and MA datasets.</p>
<p>Figure 7 :
7
Figure 7: Prompt of the strategy generator.</p>
<p>Figure 8 :
8
Figure 8: Prompt of the strategy executor.</p>
<p>Figures 15 -
15
Figures 15-26 provide the optimal strategies for various datasets and their execution results on task examples obtained by GPT-3.5.Figures 27-29 present the optimal strategies for various datasets and their execution results on task examples obtained by GPT-4.</p>
<p>Figures 30 -
30
demonstrate the chain-of-thought prompts for the seven datasets from the MATH benchmark.</p>
<p>Figure 9 :
9
Figure 9: Construction of the examination result for a task example.</p>
<p>Figure 10 :Figure 11 :
1011
Figure 10: Analysis &amp; Suggestion Prompt of the strategy optimizer.</p>
<p>Figure 12 :Figure 13 :Figure 14 :
121314
Figure 12: Prompt of StrategyLLM-ZS.</p>
<p>. 4 .
4
Find the sum of the squares of the roots.</p>
<p>5
5
Simplifying the equations: 9a − 3b + c = 10 c = 1 4a + 2b + c = 5 Substituting c = 1 into the equations: 9a − 3b + 1 = 10 4a + 2b + 1 = 5 Simplifying further: 9a − 3b = 9 4a + 2b = 4 3. Solve the system of equations using algebraic manipulation.Multiplying the second equation by 3: 9a − 3b = 9 12a + 6b = 12 Adding the equations together: 21a = 21 Dividing both sides by 21: a = 1 Substituting a = 1 into the second equation: Dividing both sides by 2: b = 0 Substituting a = 1 and b = 0 into the first equation: 9(1) − 3(0) = 9 9 = 9 Therefore, a = 1, b = 0, and c = 1. 4. Write the quadratic polynomial p(x).p(x) = 1x 2 + 0x + 1 Simplifying: p(x) = x 2 + 1</p>
<p>Figure 23 : 1 .
231
Figure 23: Strategy generated by GPT-3.5 for Date Understanding and its execution on a task example.</p>
<p>Figure 28 :
28
Figure 28: Strategy generated by GPT-4 for StrategyQA and its execution on a task example.</p>
<p>Figure 31 :
31
Figure 31: Chain-of-thought (CoT) prompt for MATH-Prealgebra.</p>
<p>, 24 40 reduce to 3 5 . 25 − x ≤ 10 . 10 Question 3 √ 2 √ 3 √ 2 √ 8 Figure 32 : 1 ≡≡ ( 5 • 7 • 11 ) 2 ≡ ( 35 • 11 ) 2 ≡ ( 11 • 11 ) 2 ≡ (121) 2 ≡ 1 2 ≡ 1 (
24525101032328321571123511211112221
Figure 32: Chain-of-thought (CoT) prompt for MATH-Intermediate Algebra.</p>
<dl>
<dt>Figure 34 :</dt>
<dt>34</dt>
<dt>Figure 34: Chain-of-thought (CoT) prompt for MATH-Number Theory.</dt>
<dd>
<p>3 Question:
3
The answer is 48π − 72 √ The square with vertices (−a, −a), (a, −a), (−a, a), (a, a) is cut by the line y = x/2 into congruent quadrilaterals.The perimeter of one of these congruent quadrilaterals divided by a equals what?Express your answer in simplified radical form.Solution:</p>
</dd>
</dl>
<p>5 . 5 Figure 35 :
5535
Figure 35: Chain-of-thought (CoT) prompt for MATH-Geometry (Part 1).</p>
<p>Table 1 :
1
Experimental results on the math reasoning task.The numbers in parentheses represent the relative improvement compared to CoT-SC.
4.1 Experimental SetupEvaluation Tasks and Datasets We evaluate StrategyLLM on a variety of tasks:</p>
<p>Table 2 :
2
Experimental results on the commonsense, algorithmic, and symbolic reasoning tasks.The numbers in parentheses represent the relative improvement compared to CoT-SC.
MethodsCommonsenseAlgorithmicSymbolicStrategyQA DU AvgWS MA AvgLLC-4 LLC-8 LLC-16 AvgSP56.548.5 52.573.3 2.0 37.70000SolutionLLM59.552.0 55.874.7 55.3 65.081.525.5035.7CoT64.070.5 67.367.2 84.0 75.668.522.0030.2CoT-SC70.070.5 70.361.3 86.0 73.768.022.0030.0StrategyLLM67.568.5 68.080.0 86.7 83.498.086.551.5 78.7StrategyLLM-SC71.074.0 72.5 (+3.1%) 79.3 90.7 85.0 (+15.4%) 98.087.052.5 79.2 (+164.0%)StrategyLLM-ZS70.072.5 71.3 (+1.4%) 78.7 89.3 84.0 (+14.1%)98.086.044.0 76.0 (+153.3%)</p>
<p>Table 3 :
3
Experimental results on two math reasoning datasets, namely AL and CP, with different groups of examples.
MethodsAL-dev AL-random CP-dev CP-randomSP36.029.1±3.925.526.8±2.5SolutionLLM58.056.5±2.231.032.2±2.8CoT57.555.1±1.534.033.4±1.2CoT-SC59.558.3±1.231.533.0±1.2StrategyLLM57.054.7±2.534.535.6±2.3StrategyLLM-SC64.058.9±1.136.538.4±1.3StrategyLLM-ZS62.560.8±2.638.538.8±1.7</p>
<p>Table 4 :
4
Experimental results of closed-source models on the CP, StrategyQA, and MA datasets.The numbers in parentheses represent the relative improvement compared to CoT-SC.
MethodsGPT-4Claude-3-SonnetCP StrategyQA MA AvgCP StrategyQA MA AvgSolutionLLM52.075.596.7 74.721.073.569.3 54.6CoT49.580.592.7 74.226.069.072.7 55.9CoT-SC54.583.594.7 77.626.075.076.7 59.2StrategyLLM52.581.598.7 77.228.075.083.3 62.1StrategyLLM-SC 56.083.598.7 79.4 (+2.4%) 28.077.088.0 64.3 (+8.6%)</p>
<p>Table 5 :
5
Experimental results of open-source models on the CP, StrategyQA, and MA datasets.The numbers in parentheses represent the relative improvement compared to CoT-SC.
MethodsMeta-Llama-3-8B-InstructMeta-Llama-3-70B-InstructCP StrategyQA MA AvgCP StrategyQA MA AvgSolutionLLM20.564.043.3 42.651.579.072.0 67.5CoT16.061.044.7 40.648.580.581.3 70.1CoT-SC19.571.045.3 45.347.081.582.0 70.2StrategyLLM24.574.064.7 54.451.582.088.0 73.8StrategyLLM-SC 25.074.066.0 55.0 (+21.5%) 54.083.591.3 76.3 (+8.7%)MethodsMixtral-8x7B-Instruct-v0.1Mixtral-8x22B-Instruct-v0.1CP StrategyQA MA AvgCP StrategyQA MA AvgSolutionLLM22.561.034.7 39.444.572.060.7 59.1CoT24.563.059.3 48.941.072.080.0 64.3CoT-SC26.573.562.7 54.240.575.080.7 65.4StrategyLLM28.573.576.0 59.344.576.584.0 68.3StrategyLLM-SC 32.075.078.0 61.7 (+13.7%) 47.577.089.3 71.3 (+9.0%)
select two math reasoning datasets with diverse examples, namely AL and CP, and randomly sample 5 distinct groups of examples from their respective training sets.We then report the mean and standard deviation of the results.Additionally, we employ the validation set to identify a group of 4 examples from the training set.Specifically, we use the OpenAI embedding model API (the text-embedding-3-large model) to map training and validation questions to embeddings and subsequently select the 4 training examples with the highest cosine similarities to all validation examples.We designate these groups of examples as AL-dev and CP-dev, respectively.The results, as presented in Table3, demonstrate that StrategyLLM consistently delivers satisfactory performance on both datasets, suggesting that StrategyLLM is a robust and reliable framework for problem-solving.</p>
<p>Table 6 :
6
[40]arison of Plan-and-Solve, CoT+Strategy, and StrategyLLM.Solve Prompting[40], which directs LLMs to formulate specific plans for each test instance at inference time and execute these plans to solve the instances; (2) CoT+Strategy, which combines the CoT prompt with instructions that guide LLMs to devise a task-solving strategy and apply it to a specific test example at inference time.The prompt for CoT+Strategy is detailed in Appendix F.The performance of GPT-3.5 on the CP, StrategyQA, and MA datasets, representing three distinct reasoning tasks, is presented in Table6.Our observations are as follows: (1) StrategyLLM significantly outperforms both Plan-and-Solve Prompting and CoT+Strategy across all three datasets.
MethodsCP StrategyQA MA AvgPlan-and-Solve26.054.069.3 49.8Plan-and-Solve-SC 27.564.570.0 54.0CoT+Strategy30.563.062.7 52.1CoT+Strategy-SC36.570.070.0 58.8StrategyLLM35.067.586.7 63.1StrategyLLM-SC38.571.090.7 66.7the necessity of effective task-level strategies, we compare our framework against two baselines: (1)Plan-and-
This highlights the superiority of generalizable task-level strategies over instance-specific plans in enhancing performance across various problem-solving contexts.This improvement can be attributed to two key factors: (a) our task-level strategies encapsulate essential task-level knowledge, thereby providing professional and high-level guidance; (b) generating high-quality, specific plans for each test example at inference time is inherently challenging, making it difficult to ensure the quality of these plans.(</p>
<p>Table 7 :
7
Average cost of prompt generation across four reasoning tasks.
Math Commonsense Algorithmic Symbolic</p>
<p>Table 9 :
9
Examining the impact of prompt consistency In our primary experiments, we have demonstrated that StrategyLLM outperforms inconsistent CoT prompts.To further examine the impact of prompt consistency, we intentionally create inconsistent prompts by employing multiple strategies.For each test example within a specific dataset, we generate an inconsistent few-shot prompt for it by randomly selecting examples from different strategy-based prompts.Specifically, we apply the top 3 strategies randomly and uniformly to the examples in the prompt.As a result, the prompt fails to offer consistent Average inference cost of each test example across four reasoning tasks.# I and # O denote the number of input and output tokens, respectively.
Math Commonsense Algorithmic SymbolicSolutionLLM (# I)1693740373301SolutionLLM (# O)3739010348CoT (# I)1332487830261CoT (# O)3045033165StrategyLLM (# I)26492139888842StrategyLLM (# O)511279227244
our main experiments, we utilize 4 examples in the few-shot prompts of datasets within the MATH benchmark.To compare StrategyLLM with baselines of comparable costs, we construct CoT baselines comprising 8 few-shot examples, with average input and output tokens on the datasets of the MATH benchmark amounting to 2697 and 288, respectively.This baseline is referred as CoT-8, and its first 4 examples are the same as CoT in Table1.The comparison results between CoT-8 and StrategyLLM are presented in Table10.Our framework achieves a marked improvement over CoT-8, showcasing its effectiveness.</p>
<p>Table 10 :
10
Experimental results on the math reasoning task.The numbers in parentheses represent the relative improvement compared to CoT-8-SC.
MethodsALPAIACPNTGEPC Avg</p>
<p>Table 11 :
11
Results of StrategyLLM using the optimal strategy and the method employing inconsistent prompts.
MethodsCP StrategyQA MA AvgInconsistent Prompt 29.056.577.3 54.3StrategyLLM35.067.586.7 63.1</p>
<ol>
<li>
<p>Calculate the number of hours or days given in the question.3.Determine if the time period given is in the past or future.4.Adjust the current date based on the time period and direction (past or future). 5. Convert the adjusted date to the required format (MM/DD/YYYY) as specified in the question.6.Provide the final date as the answer to the question.The concert was scheduled to be on 06/01/1943, but was delayed by one day to today.What is the date 10 days ago in MM/DD/YYYY?Solution:1.Determine the current date based on the given information.-Theconcert was delayed by one day, so the current date is 06/02/1943.2. Calculate the number of days given in the -The question asks for the date 10 days ago.3. Determine if the time period given is in the past or future.-The time period is in the past.4. Adjust the current date based on the time period and direction (past or future).-Subtract 10 days from the current date: 06/02/1943 -10 days = 05/23/1943.5. Convert the adjusted date to the required format (MM/DD/YYYY) as specified in the question.-The adjusted date is already in the required format: 05/23/1943.6. Provide the final date as the answer to the question.Answer: The answer is 05/23/1943.
Executing the strategy on a task example:Question:</p>
</li>
<li>
<p>For each sub-question, infer or retrieve relevant information.This can come from provided information, general knowledge, or external resources.5.Compare and synthesize the information gathered from each sub-question.6.Based on the synthesis, determine if the information satisfies or contradicts the main question.7.Formulate a 'yes' or 'no' answer to the main question based on the evaluation.8.Confirm that the answer accurately addresses the main question.If not, revisit the sub-questions and retrieval process.9.Record the answer.The University of Pennsylvania is an Ivy League research university in Philadelphia.It is one of the most prestigious universities in the world.-Tosucceed at the University of Pennsylvania would likely mean to graduate with a degree, possibly with honors or high academic achievement.5. Comparing and synthesizing the information gathered from each sub-question, we know that Brooke Shields is a successful actress and model, and the University of Pennsylvania is a prestigious university.Success at the university would likely require dedication, intelligence, and hard work.6.Based on this synthesis, it is possible that Brooke Shields could succeed at the University of Pennsylvania, given her demonstrated success in her career.However, without specific information about her academic abilities or aspirations, we cannot definitively say.7. Given the information available, we can infer a 'yes' answer to the main question.
Executing the strategy on a task example:Question: Could Brooke Shields succeed at University of Pennsylvania?Solution:1. The main subject in the question is Brooke Shields and the main concept is University ofPennsylvania.2. The relationship associated with the main subject and concept is Brooke Shields' potentialsuccess at the University of Pennsylvania.3. Sub-questions based on the identified subjects or concepts and their associated relationshipsor actions could be:-Who is Brooke Shields?-What is the University of Pennsylvania?-What does it mean to succeed at the University of Pennsylvania?4. For each sub-Question:-Brooke Shields is a well-known American actress and model.-</p>
</li>
</ol>
<p>To count the number of numbers in this set, we subtract 49 from all of the numbers, giving the set {1, 2, 3, . . ., 950}, making it obvious that there are 950 numbers total.Furthermore, the set {50, 51, 52, . . ., 98, 99} corresponds to the more easily counted {1, 2, 3, . . ., 49, 50} by subtracting 49.So, the probability of selecting a two-digit number is
50 950 = 1 19 .
1   4Question: A number in the set {50, 51, 52, 53, ..., 999} is randomly selected.What is the probability that it is a two-digit number?Express your answer as a common fraction.Solution:</p>
<p>MATH-PrealgebraStrategy:1. Understand the problem: Read and carefully analyze the given problem to understand the requirements and constraints.2. Identify the known and unknown information: Determine what information is provided and what needs to be calculated or determined.3. Translate the problem into mathematical equations or statements: Use appropriate mathematical symbols, equations, or statements to represent the problem.4. Solve the equations or statements: Use algebraic techniques to solve the mathematical equations or statements. 5. Verify the Solution: Substitute the found values back into the original problem to check if they satisfy the given conditions.6. Express the solution in the required format: Answer the question or provide the solution in the appropriate form, such as a fraction, decimal, or percentage.Executing the strategy on a task example: Question: Pete thinks of a number.He doubles it, adds 10, multiplies by 4, and ends up with 120.What was his original number?Solution:1. Understand the problem: Pete starts with a number, performs a series of operations on it, and ends up with a final result.We need to determine the original number.The final result is indeed 120, which matches the given information.6. Express the solution in the required format:Answer: The answer is 10.Figure16: Strategy generated by GPT-3.5 for MATH-Prealgebra and its execution on a task example.MATH-Counting and ProbabilityStrategy: 1. Understand the problem statement: -Read and understand the given problem.-Identify what needs to be calculated or determined.-Make note of any special conditions or restrictions mentioned.2. Identify the counting or probability concept: -Determine which counting or probability concept is needed to solve the problem.-Common concepts include permutations, combinations, probability rules, etc. 3. Break down the problem: -Break down the problem into smaller subproblems or cases if needed.-Identify any patterns or symmetries that can simplify the calculations.-Consider any specific conditions or restrictions mentioned in the problem.4. Calculate the required counting or probability: -Apply the appropriate counting or probability formula or rule to calculate the desired quantity.-Use combinations or permutations for counting problems.-Apply probability rules like the multiplication rule, addition rule, conditional probability, etc., for probability problems.5. Simplify or reduce the problem: -Look for ways to simplify or reduce the problem.-Utilize symmetry or equivalence to eliminate redundant cases.-Consider any combinatorial identities or properties that can simplify calculations.6. Verify and check the Solution: -Double-check the calculations to ensure accuracy.-Check if the solution satisfies any specific conditions or restrictions mentioned in the problem.-Compare the solution with any provided answers or results.Communicate the solution clearly:-Present the final solution in a clear and concise manner.-Include any necessary explanations or steps taken to arrive at the solution.-Express the final answer in the required units or as a fraction, decimal, or whole number, as specified by the problem.8. Practice and review: -Solve additional problems on counting and probability to reinforce the concepts.-Review the solved problems to identify any common patterns or strategies that can be applied to similar problems.Executing the strategy on a task example: Question: Given thatto break down the problem.-Using this identity, we can express 19  9 in terms of the given values:MATH-GeometryStrategy: 1.To find the area of a shaded region in a circle sector overlap problem: a. Calculate the area of each sector using the formula Asector = 1 2 r 2 θ, where r is the radius and θ is the central angle.b.Determine the area of the overlapping region by subtracting the area of the triangle formed by the two radii from the sum of the areas of the two sectors.c.Double-check the calculations and verify the obtained answer against the correct answer.2. To calculate the perimeter of a quadrilateral formed by cutting a square with a line: a. Find the coordinates of the intersection points between the line and the square.b.Calculate the lengths of the sides of the quadrilateral using the distance formula.c.Add up the lengths of the sides to find the perimeter.d.Double-check the calculations and verify the obtained answer against the correct answer.3. To determine the height of a cylinder inscribed in a hemisphere: a. Use the Pythagorean theorem to find the radius of the cylinder's base by subtracting the height of the cylinder from the radius of the hemisphere.b.The height of the cylinder is equal to the radius of the hemisphere minus the radius of the cylinder's base.c.Double-check the calculations and verify the obtained answer against the correct answer.Executing the strategy on a task example: Question: Parallelogram ABCD with A(2, 5), B(4, 9), C(6, 5), and D(4, 1) is reflected across the x-axis to  Answer: The answer is (−2, 5).MATH-PrecalculusStrategy:1. Identify the problem or concept being tested in the given question.2. Define any necessary terms or formulas related to the problem.3. Simplify or manipulate any given expressions or equations to solve the problem.4. Solve the problem step by step with the help of appropriate formulas or techniques. 5. Verify the solution or answer to ensure its correctness.6. Repeat steps 1-5 for each problem in the precalculus subject.Executing the strategy on a task example: Question: The functionrepresents a rotation around some complex number c. Find c. Solution:1. Identify the problem or concept being tested in the given question.-The problem is asking us to find the complex number c that represents the center of rotation for the given function. .-To find θ, we can take the argument of e iθ , which is θ = arg −1+i √ 3 2.-Simplifying the expression inside the argument, we get −1+i2 .-The argument of this complex number is θ = arctanStrategy:1. Identify the innermost operations enclosed in parentheses.2. Evaluate the innermost operations first.3. Replace the evaluated expression with the result.4. Repeat steps 1-3 until all parentheses are removed.5. Evaluate the remaining arithmetic operations according to the order of operations (multiplication and division before addition and subtraction).6. Evaluate all multiplication and division operations from left to right.7. Replace the evaluated expression with the result.8. Repeat steps 5-7 until there are no more multiplication and division operations to be performed.9. Evaluate all addition and subtraction operations from left to right.10.Replace the evaluated expression with the result.11.Repeat steps 9-10 until there are no more addition and subtraction operations to be performed.12. The final result is the solution to the multi-step arithmetic problem.Executing the strategy on a task example: Question: ((-9 * 7 * 7 * -9) + (4 * -9 -8 --4)) = Solution:Step 1: Identify the innermost operations enclosed in parentheses.The innermost operation is (4 * -9 -8 --4).Step 2: Evaluate the innermost operations first.(3969 + -40)Step 7: Repeat steps 5-6 until there are no more multiplication and division operations to be performed.There are no more multiplication and division operations to be performed.Step 8: Evaluate all addition and subtraction operations from left to right.3969 + -40 = 3929Step 9: Replace the evaluated expression with the result.Answer: The answer is 3929.Figure25: Strategy generated by GPT-3.5 for Multi-step Arithmetic and its execution on a task example.Last Letter ConcatenationStrategy:1. Identify the given input, which will be a string containing multiple words separated by spaces.2. Split the input string into individual words.3. Initialize an empty string to store the last letters of the words.4. Iterate through each word in the list of words. 5. Get the last letter of each word.6. Concatenate the last letter to the string from step 3. 7. Continue steps 4-6 until all words have been processed.8. Print or return the final concatenated string.Executing the strategy on a task example: Question: Take the last letters of the words in "Sergey Brin" and concatenate them.Solution:1. Identify the given input: "Sergey Brin" 2. Split the input string into individual words: ["Sergey", "Brin"] 3. Initialize an empty string: last_letters = "" 4. Iterate through each word in the list of words:-For the word "Sergey": -Get the last letter: last_letter = "y" -Concatenate the last letter to the string: last_letters = "y" -For the word "Brin": -Get the last letter: last_letter = "n" -Concatenate the last letter to the string: last_letters = "yn" 5. Print or return the final concatenated string.Answer: The answer is yn.Figure26: Strategy generated by GPT-3.5 for Last Letter Concatenation and its execution on a task example.MATH-Counting and ProbabilityStrategy:1. Identify the type of the counting or probability problem presented.2. If the problem is regarding permutations or combinations, identify all possible outcomes and arrange or select the required number of objects accordingly.3. If the problem is about probability, assess the sample space, the number of possible outcomes and the number of favorable outcomes.4. For probability questions, use the formula of probability, which is the number of favorable outcomes divided by the number of total outcomes.5.If the problem involves conditions or restrictions, ensure to factor in these when calculating probability or counting the possible arrangements or selections.6.If the problem involves complex calculations, break them down into smaller manageable calculations.7. Apply the suitable rule or formula according to the type of counting or probability problem, such as multiplication rule, addition rule, arrangement rules, rules of permutations and combinations, or probability rules.8. Compute the result using arithmetic calculation or mathematical formulas, and write down the answer.9. Review the process and the answer to ensure accuracy.Executing the strategy on a task example: Question: Given thatSolution:1.The problem is about combinations, which is a type of counting problem.2. The problem is asking for the number of ways to choose 9 items from a set of 19, which is represented by 19  9 .3. We are given the values of 17  7 , 17 8 , and 17  9 , which can be used to calculate 19  9 .The formula for combinations is, where n is the total number of items, r is the number of items to choose, and '!' denotes factorial.5.However, we can use the property of combinations that n r = n n−r , and the relationship between n+1 r and n r−1 and n r , which is n+1 r = n r−1 + n r .6. Using these properties, we can express 19  9 as 18 8 + 18 9 .7. Similarly, 18  8 can be expressed asStrategy:1. Begin by identifying all the numbers and operations in the given problem.2. Apply the mathematical rules of order operations.The correct order of operations is Parentheses, Exponents, Multiplication and Division (from left to right), Addition and Subtraction (from left to right).This rule is often abbreviated as PEMDAS.3. Start solving the problem from the inside of the parentheses first, keeping PEMDAS rule in mind.4. While doing multiplication, division, addition, and subtraction (in the order), make sure to change the sign of the number if there is a negative sign before it.5.After solving the operations inside the parentheses, proceed solving the operations outside the parentheses.Remember to still follow the rules and order of operations.6. Continue to calculate step by step until you reach the final answer.Executing the strategy on a task example: Question: ((-9 * 7 * 7 * -9) + (4 * -9 -8 --4)) = Solution:1. Identify all the numbers and operations in the given problem.The numbers are -9, 7, -9, 4, -9, 8, -4.The operations are multiplication (<em>), addition (+), and subtraction (-). 2. Apply the mathematical rules of order operations (PEMDAS).CoT Prompt for MATH-Algebra. Express your answer in simplest radical form in terms of x.Solution: Writing everything in terms of prime factorizations, the given expression isAnswer: The answer is 10x √ 21xQuestion: Rationalize the denominator ofThe answer can be written as, where A, B, C, and D are integers, D is positive, and B is not divisible by the square of any prime.Find the minimum possible value of A + B + C + D. Solution: We can write the numerator as 4 √ 2, the denominator as 4 − √ 2.Then, we multiply the numerator and denominator by the conjugate of the denominator.Answer: The answer is 21 Question: The quadratic 8x 2 + 12x − 14 has two real roots.What is the sum of the squares of these roots?Express your answer as a common fraction in lowest terms.Solution: Let x1 and x2 be the roots of the equation 8x 2 + 12x − 14.We want to find x 2 1 + x 2 2 .Note that x 2 1 + x 2 2 = (x1 + x2) 2 − 2x1x2.We know that x1 + x2, the sum of the roots, is equal to −b a , which for this equation is −12 8 = −3 2 .Likewise, we know that x1x2, the product of the roots, is equal to c a , which for this equation is −14 8 = −7 4 .Thus,Answer: The answer is 23   4   Question: What is the product of all the coordinates of all the points of intersection of the two circles defined by x 2 − 2x + y 2 − 10y + 25 = 0 and x 2 − 8x + y 2 − 10y + 37 = 0? Solution: Add (−2/2) 2 and (−10/2) 2 to the first equation and (−8/2) 2 and (−10/2) 2 to the second equation to find that the given equations are equivalent to (x 2 − 2x + 1) + (y 2 − 10y + 25) = 1, andwhich are equivalent torespectively.Hence, the two circles have centers (1, 5) and (4, 5) respectively and radii 1 and 2 respectively.Since the centers of the circles are 3 units apart and the sum of their radii is 3, the two circles intersect at only one point.We can see that(2,5)is the desired intersection point, so our product is 2 • 5 = 10.Answer: The answer is 10CoT Prompt for MATH-Counting and ProbabilityQuestion: The digits 2, 3, 5 and 7 are arranged randomly to form a four-digit number.What is the probability that the number is odd?Express your answer as a common fraction.Solution: The number formed is odd if and only if its units digit is not 2. Since the digits 2, 3, 5, and 7 are arranged randomly, the probability that 2 is the units digit is 1/4.Therefore, the probability that the number is odd is 1 − 1/4 = 3 4 .Answer: The answer is 3   4   Question: Eight congruent equilateral triangles, each of a different color, are used to construct a regular octahedron.How many distinguishable ways are there to construct the octahedron?(Two colored octahedrons are distinguishable if neither can be rotated to look just like the other.)[asy] import three; import math; unitsize(1.5cm);currentprojection=orthographic(2,0.2,1);triple A=(0,0,1); triple B=(sqrt(2[asy] size(8cm); defaultpen(0.5);import three; import math; currentprojection=orthographic(2,0.2,1);triple A=(0,0,1); triple B=(sqrt(2)/2,sqrt(2)/2,0); triple C=(sqrt(2)/2,-sqrt(2)/2,0); triple D=(-sqrt(2)/2,sqrt(2)/2,0); triple E=(-sqrt(2)/2,sqrt(2)/2,0); triple F=(0,0,-1); draw(A-B-E-cycle); draw(A-C-D-cycle); draw(F-C-B-cycle); draw(F-D-E-cycle,dotted+linewidth(0.7));draw(surface(A-B-Ccycle),rgb(1,.6,.6),nolight);[/asy]There are 7! ways to arrange the remaining seven colors, but there still are three possible rotations about the fixed face, so the answer is 7!/3 = 1680.[asy] size(8cm); defaultpen(0.5);import three; import math; currentprojection=orthographic(2,0,1); triple A=(0,0,1); triple B=(sqrt(2Question: The Gnollish language consists of 3 words, "splargh," "glumph," and "amr."In a sentence, "splargh" cannot come directly before "glumph"; all other sentences are grammatically correct (including sentences with repeated words).How many valid 3-word sentences are there in Gnollish?Solution: We proceed by counting the complement, or the number of invalid 3-word sentences.A sentence is invalid precisely when it is of the form "(word) splargh glumph" or "splargh glumph (word)."There are 3 choices for the missing word in each sentence, and since each case is exclusive, we have a total of 6 invalid sentences.Since there are 3 • 3 • 3 = 27 possible 3-word sentences with no restrictions, there are 27 − 6 = 21 that satisfy the restrictions of the problem.Answer: The answer is 21 Question: Given that  Question: A right circular cylinder with radius 2 is inscribed in a hemisphere with radius 5 so that its bases are parallel to the base of the hemisphere.What is the height of this cylinder?Solution: We draw and label a diagram as follows:[asy] size(110); pair O = (0,0); pair A = (.3,.94); pair B = (.3,.075); draw(O-A-Bcycle,heavycyan); label("O",O,W); label("A",A,N); label("B",B,S); import solids; import three; defaultpen(linewidth(0.8)); currentprojection = orthographic(5,0,1.3);revolution c = cylinder((0,0,0), .4,.91);draw(c,black); draw(scale(1,.25)</em>arc((0,0),1,0,180),dashed);draw(scale(1,.25)*arc((0,0),1,180,360));draw(Arc((0,0),1,0,180)); [/asy] Let the center of the hemisphere be O, and let A be a point on the circumference of the top circle of the cylinder.Since the cylinder is inscribed in the hemisphere, A lies on the hemisphere as well, so OA = 5.We drop a perpendicular from A to the base of the hemisphere and let it intersect the base of the hemisphere at B. Since the cylinder is right and AB is a height of the cylinder, ∠OBA is a right angle, and B lies on the circumference of the bottom circle of the cylinder.Thus, OB is a radius of the cylinder, so OB = 2.We have that △OBA is right, so by the Pythagorean theorem, we haveThus, the height of the cylinder is Answer: The answer is (−4 √ 3, −4) Answer: The answer is 16 −4 37 −13 Question: A line is parameterized by a parameter t, so that the vector on the line at t = 2 is 1 4, and the vector on the line at t = 3 is 3 −4 .Find the vector on the line at t = −7.Solution: Let the line be x y = a + td.Then from the given information, Answer: The answer is −17 76
Introducing meta llama 3: The most capable openly available llm to date. A I Meta, 2024</p>
<p>Introducing the next generation of claude. Anthropic, 2024</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Jiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song, Xiaoyang Wang, Dong Yu, Jianshu Chen, arXiv:2308.00304Skills-in-context prompting: Unlocking compositionality in large language models. 2023arXiv preprint</p>
<p>Iterative translation refinement with large language models. Pinzhen Chen, Zhicheng Guo, Barry Haddow, Kenneth Heafield, arXiv:2306.038562023arXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, Transactions on Machine Learning Research. 2023</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu, arXiv:2309.15402A survey of chain of thought reasoning: Advances, frontiers and future. 2023arXiv preprint</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research. 2023BIG-Bench collaboration</p>
<p>Self-collaboration code generation via chatgpt. Yihong Dong, Xue Jiang, Zhi Jin, Ge Li, arXiv:2304.075902023arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, The Eleventh International Conference on Learning Representations. 2023</p>
<p>PAL: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, Proceedings of the 40th International Conference on Machine Learning. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, the 40th International Conference on Machine LearningPMLRJul 2023202</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Yiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu, Dongyan Zhao, Nan Duan, arXiv:2304.10464Learning to plan with natural language. 2023arXiv preprint</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>MetaGPT: Meta programming for a multi-agent collaborative framework. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, Jürgen Schmidhuber, The Twelfth International Conference on Learning Representations. 2024</p>
<p>. Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Grgur Kovač, Rémy Portelas, Peter Ford Dominey, Pierre-Yves Oudeyer, arXiv:2307.07871The socialai school: Insights from developmental psychology towards artificial socio-cultural agents. 2023arXiv preprint</p>
<p>Self-checker: Plug-and-play modules for factchecking with large language models. Miaoran Li, Baolin Peng, Zhu Zhang, arXiv:2305.146232023arXiv preprint</p>
<p>Are you in a masquerade? exploring the behavior and impact of large language model driven social bots in online social networks. Siyu Li, Jin Yang, Kui Zhao, arXiv:2307.103372023arXiv preprint</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, arXiv:2305.191182023arXiv preprint</p>
<p>Agentsims: An open-source sandbox for large language model evaluation. Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, Qin Chen, arXiv:2308.040262023arXiv preprint</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Comput. Surv. 559jan 2023</p>
<p>Plan, verify and switch: Integrated reasoning with diverse X-of-thoughts. Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Faithful chain-of-thought reasoning. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. Long Papers. Jong C Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti, Adila Alfa Krisnadhi, the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterBaliAssociation for Computational LinguisticsNovember 20231</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Selfcheck: Using LLMs to zero-shot check their own step-by-step reasoning. Ning Miao, Yee Whye Teh, Tom Rainforth, The Twelfth International Conference on Learning Representations. 2024</p>
<p>arXiv:2303.08774OpenAI. Gpt-4 technical report. 2023arXiv preprint</p>
<p>. OpenAI. Introducing chatgpt. 2023</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST '23. the 36th Annual ACM Symposium on User Interface Software and Technology, UIST '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, Mike Lewis, Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, arXiv:2307.07924Communicative agents for software development. 2023arXiv preprint</p>
<p>Algorithm of thoughts: Enhancing exploration of ideas in large language models. Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Lu Wang, Ruoxi Jia, Ming Jin, arXiv:2308.103792023arXiv preprint</p>
<p>Automatic prompt augmentation and selection with chain-of-thought from labeled data. Kashun Shum, Shizhe Diao, Tong Zhang, Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Challenging BIG-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, Jason Wei, Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023arXiv preprint</p>
<p>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona selfcollaboration. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji, arXiv:2307.053002023arXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>Reprompting: Automated chain-ofthought prompt inference through gibbs sampling. Weijia Xu, Andrzej Banburski-Fahey, Nebojsa Jojic, arXiv:2305.099932023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao, arXiv:2308.04371Cumulative reasoning with large language models. 2023arXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Ed H Quoc V Le, Chi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Adaptive-solver framework for dynamic strategy selection in large language model reasoning. Jianpeng Zhou, Wanjun Zhong, Yanlin Wang, Jiahai Wang, arXiv:2310.014462023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
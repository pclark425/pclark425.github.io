<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8402 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8402</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8402</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-fc1ffc7df07cc9b665deca4a94b871732e1f0b4d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fc1ffc7df07cc9b665deca4a94b871732e1f0b4d" target="_blank">Length Generalization in Arithmetic Transformers</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is shown that priming allows models trained on $5-digit $\times$ $3-digit multiplications to generalize to $35\times 3$ examples, and that the priming sample size scales as the logarithm of the training set size.</p>
                <p><strong>Paper Abstract:</strong> We examine how transformers cope with two challenges: learning basic integer arithmetic, and generalizing to longer sequences than seen during training. We find that relative position embeddings enable length generalization for simple tasks, such as addition: models trained on $5$-digit numbers can perform $15$-digit sums. However, this method fails for multiplication, and we propose train set priming: adding a few ($10$ to $50$) long sequences to the training set. We show that priming allows models trained on $5$-digit $\times$ $3$-digit multiplications to generalize to $35\times 3$ examples. We also show that models can be primed for different generalization lengths, and that the priming sample size scales as the logarithm of the training set size. Finally, we discuss potential applications of priming beyond arithmetic.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8402.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8402.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RPE_k</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relative Position Embeddings (relative over keys)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A positional encoding that represents relative distances between tokens (applied to keys only); when used in encoder-only transformers it enables strong length generalization on multi-digit addition and many modular tasks by allowing the model to learn position-invariant digit-level algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer / UTransformer (encoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformers and Universal Transformers (shared layers) trained from scratch (not pre-trained) in Base/Standard/Large sizes (examples: Base D=6,d_model=512,h=8; Standard D=6,d_model=1024,h=16; Large D=10,d_model=1024,h=16). Supervised classification training over digit tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit addition, modular addition, modular multiplication; limited effect on multi-digit multiplication (5x3) without additional interventions</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Encodes relative token distances so the same local algorithm (e.g., digit-wise addition + carry handling) can be applied at any digit position; representations allow the model to learn a single algorithm that applies across positions (digits are learned concurrently rather than per-position memorization).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Ablations comparing APE vs RPE variants (RPE_k and RPE_{k,q}); digit-level learning curves (per-output-digit accuracy over training); element-wise addition probe (no-carry addition) to isolate local operations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Addition: RPE models reach near-100% on 6- and 10-digit tests and high performance on 15-digit tests (examples reported: ~99.9% on 10 digits, ~98% on 15 digits for best configurations); performance drops at very long lengths (e.g., ~21% on 20-digit in one Transformer config). Element-wise (no-carry) addition: RPE_k: 97.5% on 6-digit, 90.5% on 10-digit, 86.2% on 15-digit, 78.13% on 20-digit (base UTransformer). Modular arithmetic: RPE improves extrapolation for moduli that are powers of 10 (see modular table results); for multiplication (5x3) RPE alone generally fails to extrapolate to >=6 digits without priming.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Performance degrades for very long sequences (e.g., 20+ digits for some configs). RPE does not solve all hard modular cases (some moduli like 101 produce failures) and fails to produce length-generalization for 5x3 multiplication without priming interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Digit-level learning curves show RPE models learn most output digit positions simultaneously (concurrent rise in per-digit accuracy), indicating a single position-agnostic algorithm; element-wise addition experiments show RPE generalizes when the operation is local; RPE-based models substantially outperform APE in many extrapolation settings (tables and ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>RPE does not enable length generalization for 5x3 multiplication in the standard training regime; some modular cases (non-power-of-10 moduli like 101) remain hard even with RPE; extreme extrapolation lengths still show degraded accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length Generalization in Arithmetic Transformers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8402.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8402.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>APE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Absolute Position Embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classical positional encodings added to token embeddings (position-specific), which cause learned behaviors to be position-dependent and therefore fail to generalize to longer sequences except in special cases (e.g., operations with fixed output length).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer / UTransformer (encoder-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same encoder-only architectures as above (Base/Standard/Large), trained from scratch on digit-token classification of arithmetic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit addition, modular arithmetic; multiplication with priming can be enabled but requires larger priming rates</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Injects absolute position into token representations so the model effectively learns position-specific rules (separate mapping per output position) rather than a position-agnostic algorithm; tends to memorize mappings for trained output positions.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Comparative ablations with RPE; digit-wise learning curves showing per-position learning order; element-wise addition probe; priming experiments applied to APE models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In-domain accuracy ~100% on 5-digit tasks. Out-of-domain: very poor on longer addition (examples: ~3.1% on 6-digit and 0% for longer lengths in some Transformer/UTransformer configs). In special modular cases with fixed output length (modulus a power of 10), APE can generalize (e.g., modular multiplication with c=100: high accuracy up to 35 digits in some configs; modular addition c=100: base APE 73.3% on 10-digit). With priming, APE can be made to generalize on multiplication but needs larger priming rates (reported ~10% priming rate vs ~1% for RPE).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Fails to generalize to digits beyond those seen in training — predictions tend to reproduce leftmost (in-domain) digits and fail on higher-order positions; cannot cope with changing output lengths; learns positions independently and thus cannot apply a learned rule to unseen positions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Digit-by-digit learning curves show APE learns each output position largely independently (staggered learning), and element-wise addition shows APE models predict trained-left digits but fail on new rightmost digits; modular experiments show APE generalizes when the output length remains constant (consistent with position-specific strategy).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Priming can enable APE to extrapolate but at a much higher data cost; APE sometimes generalizes for modular cases (power-of-10 moduli), so failure is not universal—it's tied to variable output-length problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length Generalization in Arithmetic Transformers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8402.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8402.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TrainSetPriming</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Train Set Priming (adding a tiny fraction of long examples to training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intervention where a very small number of long-target-distribution examples are permanently added to the training set (epsilon fraction), which enables transformer models to extrapolate to much longer operands (notably enabling 5x3 multiplication models to generalize to 35x3).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UTransformer (standard: D=6, d_model=1024, h=16) primarily reported</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Universal Transformer trained on a fixed training pool (N_train typically 5000 first operands sampled, second operand online) with an epsilon fraction of long examples included as 'priming' throughout training.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit multiplication (5-digit × 3-digit → extrapolate to up to 35-digit × 3-digit), and extended to enable extrapolation across ranges of lengths when priming set includes multiple lengths; also effective for APE configurations with higher priming rates.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Hypothesized to allow the model to discover and reinforce a position-agnostic algorithm over digit positions by exposing the network to a few long-position examples; results in simultaneous learning across many digit positions (concurrent per-digit accuracy gains).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Intervention experiments varying priming set size (number of long examples), priming rate ε, priming length distribution (single-length priming vs curriculum priming vs mixture of lengths), ablations on priming thresholds, and comparisons to fine-tuning; digit-level learning curves examined.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Key observed numbers: adding 50 examples (≈1% priming rate for N_train=5000) of 35-digit × 3-digit yields close to 100% accuracy on both 5×3 (ID) and 35×3 (OOD). Minimal effective priming threshold ~25 samples; fine-tuning required ~1000 examples to reach similar OOD performance (thus priming can be ~20× more sample-efficient). Priming sample needed to reach 90% 35-digit accuracy scales approximately logarithmically with N_train: examples reported: ~30 priming samples for N_train=1e3, ~70 for N_train=1e4, ~100 for N_train=1e5. Priming sample scales (roughly) linearly with extrapolation length (e.g., 6-digit extrapolation needs ~10 priming samples).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>There is a minimal priming threshold; below ~25 examples (in the studied configuration) priming fails to enable generalization. Priming only generalizes to lengths included (or covered) by the priming distribution — e.g., priming even lengths only yields extrapolation to even lengths only. If priming distribution is too narrow (only 35-digit examples), extrapolation to intermediate lengths may not occur unless those lengths are present or near those primed.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Learning curves show ID accuracy rises quickly while OOD follows under priming; digit-level curves indicate that priming leads to concurrent learning across multiple digit positions (contrast to per-position staggered learning without priming); ablations show threshold behavior and scaling laws (logarithmic with training set size, linear with extrapolation length).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Priming must include target lengths (or a distribution covering them) to generalize across a range of lengths; naive curriculum priming (mixing many lengths) usually fails unless carefully chosen (successful curriculum found: mixture of 34- and 35-digit examples). Mechanistic reason remains empirical — theoretical understanding is open.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length Generalization in Arithmetic Transformers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8402.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8402.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FineTuning_OOD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuning on target long sequences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard fine-tuning of a model trained on short sequences by retraining on many long target-distribution examples; can enable length generalization but is sample-inefficient and causes catastrophic forgetting of the original (short-length) task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UTransformer (standard configurations reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Universal Transformer trained on 5×3 multiplication then fine-tuned on varying numbers of 35×3 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit multiplication (extrapolating to 35×3)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Directly trains network parameters on long examples so the model fits target-distribution patterns (may encourage memorization or adaptation to long-position features rather than discovering a position-agnostic algorithm).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Fine-tuning experiments varying number of target long examples; measure accuracy on both original (5×3) and target (35×3) tasks to detect forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported: ~1000 fine-tuning examples were sufficient to learn 35×3 multiplication; however, after fine-tuning the model loses ability on 5×3 (catastrophic forgetting).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Catastrophic forgetting of original (short-length) task; requires orders-of-magnitude more target examples than priming to achieve similar extrapolation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Figure 3a shows trade-off: increasing fine-tuning examples raises 35-digit accuracy but lowers 5-digit accuracy; direct comparison with priming quantifies sample inefficiency and forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Priming demonstrates that similar OOD performance can be achieved with far fewer long examples and without forgetting, so fine-tuning is not the most efficient intervention for length extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length Generalization in Arithmetic Transformers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8402.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8402.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SharedLayer_UTransformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Universal Transformer (shared layers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer variant with shared (recurrent) layers that implicitly encode iterative/recursive computation; found to help on algorithmic tasks and modular arithmetic and shows different depth/width scaling behavior for length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Universal Transformer (UTransformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformer with weight sharing across layers (Dehghani et al. design instantiated via HuggingFace ALBERT-like implementation); evaluated in Base/Standard/Large sizes in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition, modular arithmetic, multiplication (with priming), element-wise addition probes</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Shared-layer recurrence is hypothesized to better capture recursive/iterative algorithms (e.g., digit-by-digit carry propagation) enabling repeated application of the same transformation across positions.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Ablations on depth and hidden dimensionality comparing Transformers vs UTransformers (Figure 2); performance comparisons across tasks and position embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>UTransformers require ~6 layers to extrapolate; minimal hidden size for extrapolation: 256 for UTransformers (vs 512 for Transformers); with sufficient width/depth, UTransformers achieved high extrapolation (examples: UTransformer RPE_k,Large achieved ~98.3% on 15-digit addition in some reported configs). Shallow Transformers (2 layers) can sometimes extrapolate to ~10 digits.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Even with shared layers, tasks requiring more global operations (e.g., 5×3 multiplication without priming) still fail unless additional interventions applied.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Scaling experiments (varying depth and dimension) show different thresholds for extrapolation between Transformers and UTransformers; UTransformer architectures better handle some modular problems and benefit from shared-layer recurrence for iterative digit processing.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Shared layers are not alone sufficient for multiplication extrapolation (priming still required for strong OOD multiplication performance).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length Generalization in Arithmetic Transformers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8402.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8402.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CarryFailureAnalysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Carry-based error modes and digit-wise failure analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical analysis showing that most addition extrapolation failures are concentrated on cases with multiple carries (≥3 total carries and ≥2 consecutive carries), errors typically affect a small number of digits biased toward the most-significant positions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UTransformer (standard: D=6,d_model=1024,h=16) with RPE in main reported analysis</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Universal Transformer trained on 5-digit addition, evaluated on 20-digit addition error distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition extrapolation analysis (20-digit test)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Failures are associated with difficulty in correctly propagating carries across multiple positions (especially multiple consecutive carries), indicating the model's learned algorithm struggles with complex carry chains in OOD positions.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Error partitioning by number of carries (NC) and maximum consecutive carries (MC); distribution of number of incorrect digits in wrong predictions; positional distribution of single-digit errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Example reported: a reported RPE UTransformer achieves 57% accuracy on 20-digit additions in one reported configuration; analyses show almost all failures occur when NC≥3 and MC≥2; when wrong, the model typically has only a few incorrect digits, with errors concentrated on the two highest-order digit positions.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Fail when there are several carries and especially when there are two or more consecutive carries; errors tend to be few digits wrong (not wholesale hallucinations) and biased to the most significant digit positions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Figures 6a–6d: empirical bins showing accuracy vs NC and MC and per-digit error counts/positions; element-wise experiments (no carry) show different generalization patterns reinforcing role of carry handling.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Some errors still occur even for relatively small carry chains; improving carry handling remains a challenge for reliable long-digit extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length Generalization in Arithmetic Transformers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8402.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8402.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ModularArithmeticFindings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular addition/multiplication behavior vs modulus type</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical finding that modular operations where the modulus is a power of 10 (c=10^k) are easier to learn and generalize (because only last k digits matter), while non-power-of-10 moduli are harder and may block extrapolation; position embedding choice (APE vs RPE) interacts with modulus properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UTransformer / Transformer (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformers trained on modular addition/multiplication for moduli c in {100,101,128,1000} and evaluated for length generalization up to 35 digits.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Modular addition and modular multiplication across specified moduli</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>When modulus is 10^k, output depends only on k last digits so the model learns a constant-length mapping; this reduces dependence on position extrapolation. For other moduli, integer division effects and non-decoupled digit influence make the problem harder.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Systematic evaluation across moduli and position embeddings; performance tables (Tables 2,4,5) report accuracies across lengths and configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples: For c=100 (power of 10) both APE and RPE reach high accuracy up to 35 digits (100% in some Large configs). For c=1000, RPE yields substantially better extrapolation than APE (e.g., RPE base: 100% at 5 digits, 84.8% at 10, small numbers at larger lengths in base but large models recover); for c=101 (prime) and c=128 results vary, often failing to generalize; detailed tables in paper list per-modulus per-length accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Non-power-of-10 moduli introduce hard integer-division-like behavior and can prevent both in-domain learning and OOD extrapolation; modulus arithmetic can require more global reasoning and is less amenable to simple local digit algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Systematic tables (Tables 2,4,5) show clear performance differences across moduli and PE choices; interpretation in text notes that constant-length outputs explain why APE sometimes generalizes in modular-power-of-10 cases.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Some non-power-of-10 moduli (e.g., 128) show partial length generalization with RPE and larger models, so modulus-type alone does not completely determine outcome; the exact reasons for differences among non-power-of-10 moduli are not fully explained (hypotheses such as primality vs power-of-two are noted but not proven).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Length Generalization in Arithmetic Transformers', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-attention with relative position representations <em>(Rating: 2)</em></li>
                <li>An improved relative self-attention mechanism for transformer with application to music generation <em>(Rating: 2)</em></li>
                <li>Investigating the limitations of transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 1)</em></li>
                <li>Neural gpus learn algorithms <em>(Rating: 1)</em></li>
                <li>Grokking: Generalization beyond overfitting on small algorithmic datasets <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8402",
    "paper_id": "paper-fc1ffc7df07cc9b665deca4a94b871732e1f0b4d",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "RPE_k",
            "name_full": "Relative Position Embeddings (relative over keys)",
            "brief_description": "A positional encoding that represents relative distances between tokens (applied to keys only); when used in encoder-only transformers it enables strong length generalization on multi-digit addition and many modular tasks by allowing the model to learn position-invariant digit-level algorithms.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Transformer / UTransformer (encoder-only)",
            "model_description": "Encoder-only transformers and Universal Transformers (shared layers) trained from scratch (not pre-trained) in Base/Standard/Large sizes (examples: Base D=6,d_model=512,h=8; Standard D=6,d_model=1024,h=16; Large D=10,d_model=1024,h=16). Supervised classification training over digit tokens.",
            "arithmetic_task_type": "Multi-digit addition, modular addition, modular multiplication; limited effect on multi-digit multiplication (5x3) without additional interventions",
            "mechanism_or_representation": "Encodes relative token distances so the same local algorithm (e.g., digit-wise addition + carry handling) can be applied at any digit position; representations allow the model to learn a single algorithm that applies across positions (digits are learned concurrently rather than per-position memorization).",
            "probing_or_intervention_method": "Ablations comparing APE vs RPE variants (RPE_k and RPE_{k,q}); digit-level learning curves (per-output-digit accuracy over training); element-wise addition probe (no-carry addition) to isolate local operations.",
            "performance_metrics": "Addition: RPE models reach near-100% on 6- and 10-digit tests and high performance on 15-digit tests (examples reported: ~99.9% on 10 digits, ~98% on 15 digits for best configurations); performance drops at very long lengths (e.g., ~21% on 20-digit in one Transformer config). Element-wise (no-carry) addition: RPE_k: 97.5% on 6-digit, 90.5% on 10-digit, 86.2% on 15-digit, 78.13% on 20-digit (base UTransformer). Modular arithmetic: RPE improves extrapolation for moduli that are powers of 10 (see modular table results); for multiplication (5x3) RPE alone generally fails to extrapolate to &gt;=6 digits without priming.",
            "error_types_or_failure_modes": "Performance degrades for very long sequences (e.g., 20+ digits for some configs). RPE does not solve all hard modular cases (some moduli like 101 produce failures) and fails to produce length-generalization for 5x3 multiplication without priming interventions.",
            "evidence_for_mechanism": "Digit-level learning curves show RPE models learn most output digit positions simultaneously (concurrent rise in per-digit accuracy), indicating a single position-agnostic algorithm; element-wise addition experiments show RPE generalizes when the operation is local; RPE-based models substantially outperform APE in many extrapolation settings (tables and ablations).",
            "counterexamples_or_challenges": "RPE does not enable length generalization for 5x3 multiplication in the standard training regime; some modular cases (non-power-of-10 moduli like 101) remain hard even with RPE; extreme extrapolation lengths still show degraded accuracy.",
            "uuid": "e8402.0",
            "source_info": {
                "paper_title": "Length Generalization in Arithmetic Transformers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "APE",
            "name_full": "Absolute Position Embeddings",
            "brief_description": "Classical positional encodings added to token embeddings (position-specific), which cause learned behaviors to be position-dependent and therefore fail to generalize to longer sequences except in special cases (e.g., operations with fixed output length).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Transformer / UTransformer (encoder-only)",
            "model_description": "Same encoder-only architectures as above (Base/Standard/Large), trained from scratch on digit-token classification of arithmetic outputs.",
            "arithmetic_task_type": "Multi-digit addition, modular arithmetic; multiplication with priming can be enabled but requires larger priming rates",
            "mechanism_or_representation": "Injects absolute position into token representations so the model effectively learns position-specific rules (separate mapping per output position) rather than a position-agnostic algorithm; tends to memorize mappings for trained output positions.",
            "probing_or_intervention_method": "Comparative ablations with RPE; digit-wise learning curves showing per-position learning order; element-wise addition probe; priming experiments applied to APE models.",
            "performance_metrics": "In-domain accuracy ~100% on 5-digit tasks. Out-of-domain: very poor on longer addition (examples: ~3.1% on 6-digit and 0% for longer lengths in some Transformer/UTransformer configs). In special modular cases with fixed output length (modulus a power of 10), APE can generalize (e.g., modular multiplication with c=100: high accuracy up to 35 digits in some configs; modular addition c=100: base APE 73.3% on 10-digit). With priming, APE can be made to generalize on multiplication but needs larger priming rates (reported ~10% priming rate vs ~1% for RPE).",
            "error_types_or_failure_modes": "Fails to generalize to digits beyond those seen in training — predictions tend to reproduce leftmost (in-domain) digits and fail on higher-order positions; cannot cope with changing output lengths; learns positions independently and thus cannot apply a learned rule to unseen positions.",
            "evidence_for_mechanism": "Digit-by-digit learning curves show APE learns each output position largely independently (staggered learning), and element-wise addition shows APE models predict trained-left digits but fail on new rightmost digits; modular experiments show APE generalizes when the output length remains constant (consistent with position-specific strategy).",
            "counterexamples_or_challenges": "Priming can enable APE to extrapolate but at a much higher data cost; APE sometimes generalizes for modular cases (power-of-10 moduli), so failure is not universal—it's tied to variable output-length problems.",
            "uuid": "e8402.1",
            "source_info": {
                "paper_title": "Length Generalization in Arithmetic Transformers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "TrainSetPriming",
            "name_full": "Train Set Priming (adding a tiny fraction of long examples to training)",
            "brief_description": "An intervention where a very small number of long-target-distribution examples are permanently added to the training set (epsilon fraction), which enables transformer models to extrapolate to much longer operands (notably enabling 5x3 multiplication models to generalize to 35x3).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "UTransformer (standard: D=6, d_model=1024, h=16) primarily reported",
            "model_description": "Encoder-only Universal Transformer trained on a fixed training pool (N_train typically 5000 first operands sampled, second operand online) with an epsilon fraction of long examples included as 'priming' throughout training.",
            "arithmetic_task_type": "Multi-digit multiplication (5-digit × 3-digit → extrapolate to up to 35-digit × 3-digit), and extended to enable extrapolation across ranges of lengths when priming set includes multiple lengths; also effective for APE configurations with higher priming rates.",
            "mechanism_or_representation": "Hypothesized to allow the model to discover and reinforce a position-agnostic algorithm over digit positions by exposing the network to a few long-position examples; results in simultaneous learning across many digit positions (concurrent per-digit accuracy gains).",
            "probing_or_intervention_method": "Intervention experiments varying priming set size (number of long examples), priming rate ε, priming length distribution (single-length priming vs curriculum priming vs mixture of lengths), ablations on priming thresholds, and comparisons to fine-tuning; digit-level learning curves examined.",
            "performance_metrics": "Key observed numbers: adding 50 examples (≈1% priming rate for N_train=5000) of 35-digit × 3-digit yields close to 100% accuracy on both 5×3 (ID) and 35×3 (OOD). Minimal effective priming threshold ~25 samples; fine-tuning required ~1000 examples to reach similar OOD performance (thus priming can be ~20× more sample-efficient). Priming sample needed to reach 90% 35-digit accuracy scales approximately logarithmically with N_train: examples reported: ~30 priming samples for N_train=1e3, ~70 for N_train=1e4, ~100 for N_train=1e5. Priming sample scales (roughly) linearly with extrapolation length (e.g., 6-digit extrapolation needs ~10 priming samples).",
            "error_types_or_failure_modes": "There is a minimal priming threshold; below ~25 examples (in the studied configuration) priming fails to enable generalization. Priming only generalizes to lengths included (or covered) by the priming distribution — e.g., priming even lengths only yields extrapolation to even lengths only. If priming distribution is too narrow (only 35-digit examples), extrapolation to intermediate lengths may not occur unless those lengths are present or near those primed.",
            "evidence_for_mechanism": "Learning curves show ID accuracy rises quickly while OOD follows under priming; digit-level curves indicate that priming leads to concurrent learning across multiple digit positions (contrast to per-position staggered learning without priming); ablations show threshold behavior and scaling laws (logarithmic with training set size, linear with extrapolation length).",
            "counterexamples_or_challenges": "Priming must include target lengths (or a distribution covering them) to generalize across a range of lengths; naive curriculum priming (mixing many lengths) usually fails unless carefully chosen (successful curriculum found: mixture of 34- and 35-digit examples). Mechanistic reason remains empirical — theoretical understanding is open.",
            "uuid": "e8402.2",
            "source_info": {
                "paper_title": "Length Generalization in Arithmetic Transformers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "FineTuning_OOD",
            "name_full": "Fine-tuning on target long sequences",
            "brief_description": "Standard fine-tuning of a model trained on short sequences by retraining on many long target-distribution examples; can enable length generalization but is sample-inefficient and causes catastrophic forgetting of the original (short-length) task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "UTransformer (standard configurations reported)",
            "model_description": "Encoder-only Universal Transformer trained on 5×3 multiplication then fine-tuned on varying numbers of 35×3 examples.",
            "arithmetic_task_type": "Multi-digit multiplication (extrapolating to 35×3)",
            "mechanism_or_representation": "Directly trains network parameters on long examples so the model fits target-distribution patterns (may encourage memorization or adaptation to long-position features rather than discovering a position-agnostic algorithm).",
            "probing_or_intervention_method": "Fine-tuning experiments varying number of target long examples; measure accuracy on both original (5×3) and target (35×3) tasks to detect forgetting.",
            "performance_metrics": "Reported: ~1000 fine-tuning examples were sufficient to learn 35×3 multiplication; however, after fine-tuning the model loses ability on 5×3 (catastrophic forgetting).",
            "error_types_or_failure_modes": "Catastrophic forgetting of original (short-length) task; requires orders-of-magnitude more target examples than priming to achieve similar extrapolation performance.",
            "evidence_for_mechanism": "Figure 3a shows trade-off: increasing fine-tuning examples raises 35-digit accuracy but lowers 5-digit accuracy; direct comparison with priming quantifies sample inefficiency and forgetting.",
            "counterexamples_or_challenges": "Priming demonstrates that similar OOD performance can be achieved with far fewer long examples and without forgetting, so fine-tuning is not the most efficient intervention for length extrapolation.",
            "uuid": "e8402.3",
            "source_info": {
                "paper_title": "Length Generalization in Arithmetic Transformers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "SharedLayer_UTransformer",
            "name_full": "Universal Transformer (shared layers)",
            "brief_description": "Transformer variant with shared (recurrent) layers that implicitly encode iterative/recursive computation; found to help on algorithmic tasks and modular arithmetic and shows different depth/width scaling behavior for length generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Universal Transformer (UTransformer)",
            "model_description": "Encoder-only transformer with weight sharing across layers (Dehghani et al. design instantiated via HuggingFace ALBERT-like implementation); evaluated in Base/Standard/Large sizes in the paper.",
            "arithmetic_task_type": "Addition, modular arithmetic, multiplication (with priming), element-wise addition probes",
            "mechanism_or_representation": "Shared-layer recurrence is hypothesized to better capture recursive/iterative algorithms (e.g., digit-by-digit carry propagation) enabling repeated application of the same transformation across positions.",
            "probing_or_intervention_method": "Ablations on depth and hidden dimensionality comparing Transformers vs UTransformers (Figure 2); performance comparisons across tasks and position embeddings.",
            "performance_metrics": "UTransformers require ~6 layers to extrapolate; minimal hidden size for extrapolation: 256 for UTransformers (vs 512 for Transformers); with sufficient width/depth, UTransformers achieved high extrapolation (examples: UTransformer RPE_k,Large achieved ~98.3% on 15-digit addition in some reported configs). Shallow Transformers (2 layers) can sometimes extrapolate to ~10 digits.",
            "error_types_or_failure_modes": "Even with shared layers, tasks requiring more global operations (e.g., 5×3 multiplication without priming) still fail unless additional interventions applied.",
            "evidence_for_mechanism": "Scaling experiments (varying depth and dimension) show different thresholds for extrapolation between Transformers and UTransformers; UTransformer architectures better handle some modular problems and benefit from shared-layer recurrence for iterative digit processing.",
            "counterexamples_or_challenges": "Shared layers are not alone sufficient for multiplication extrapolation (priming still required for strong OOD multiplication performance).",
            "uuid": "e8402.4",
            "source_info": {
                "paper_title": "Length Generalization in Arithmetic Transformers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "CarryFailureAnalysis",
            "name_full": "Carry-based error modes and digit-wise failure analysis",
            "brief_description": "Empirical analysis showing that most addition extrapolation failures are concentrated on cases with multiple carries (≥3 total carries and ≥2 consecutive carries), errors typically affect a small number of digits biased toward the most-significant positions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "UTransformer (standard: D=6,d_model=1024,h=16) with RPE in main reported analysis",
            "model_description": "Encoder-only Universal Transformer trained on 5-digit addition, evaluated on 20-digit addition error distributions.",
            "arithmetic_task_type": "Addition extrapolation analysis (20-digit test)",
            "mechanism_or_representation": "Failures are associated with difficulty in correctly propagating carries across multiple positions (especially multiple consecutive carries), indicating the model's learned algorithm struggles with complex carry chains in OOD positions.",
            "probing_or_intervention_method": "Error partitioning by number of carries (NC) and maximum consecutive carries (MC); distribution of number of incorrect digits in wrong predictions; positional distribution of single-digit errors.",
            "performance_metrics": "Example reported: a reported RPE UTransformer achieves 57% accuracy on 20-digit additions in one reported configuration; analyses show almost all failures occur when NC≥3 and MC≥2; when wrong, the model typically has only a few incorrect digits, with errors concentrated on the two highest-order digit positions.",
            "error_types_or_failure_modes": "Fail when there are several carries and especially when there are two or more consecutive carries; errors tend to be few digits wrong (not wholesale hallucinations) and biased to the most significant digit positions.",
            "evidence_for_mechanism": "Figures 6a–6d: empirical bins showing accuracy vs NC and MC and per-digit error counts/positions; element-wise experiments (no carry) show different generalization patterns reinforcing role of carry handling.",
            "counterexamples_or_challenges": "Some errors still occur even for relatively small carry chains; improving carry handling remains a challenge for reliable long-digit extrapolation.",
            "uuid": "e8402.5",
            "source_info": {
                "paper_title": "Length Generalization in Arithmetic Transformers",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "ModularArithmeticFindings",
            "name_full": "Modular addition/multiplication behavior vs modulus type",
            "brief_description": "Empirical finding that modular operations where the modulus is a power of 10 (c=10^k) are easier to learn and generalize (because only last k digits matter), while non-power-of-10 moduli are harder and may block extrapolation; position embedding choice (APE vs RPE) interacts with modulus properties.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "UTransformer / Transformer (various sizes)",
            "model_description": "Encoder-only transformers trained on modular addition/multiplication for moduli c in {100,101,128,1000} and evaluated for length generalization up to 35 digits.",
            "arithmetic_task_type": "Modular addition and modular multiplication across specified moduli",
            "mechanism_or_representation": "When modulus is 10^k, output depends only on k last digits so the model learns a constant-length mapping; this reduces dependence on position extrapolation. For other moduli, integer division effects and non-decoupled digit influence make the problem harder.",
            "probing_or_intervention_method": "Systematic evaluation across moduli and position embeddings; performance tables (Tables 2,4,5) report accuracies across lengths and configurations.",
            "performance_metrics": "Examples: For c=100 (power of 10) both APE and RPE reach high accuracy up to 35 digits (100% in some Large configs). For c=1000, RPE yields substantially better extrapolation than APE (e.g., RPE base: 100% at 5 digits, 84.8% at 10, small numbers at larger lengths in base but large models recover); for c=101 (prime) and c=128 results vary, often failing to generalize; detailed tables in paper list per-modulus per-length accuracies.",
            "error_types_or_failure_modes": "Non-power-of-10 moduli introduce hard integer-division-like behavior and can prevent both in-domain learning and OOD extrapolation; modulus arithmetic can require more global reasoning and is less amenable to simple local digit algorithms.",
            "evidence_for_mechanism": "Systematic tables (Tables 2,4,5) show clear performance differences across moduli and PE choices; interpretation in text notes that constant-length outputs explain why APE sometimes generalizes in modular-power-of-10 cases.",
            "counterexamples_or_challenges": "Some non-power-of-10 moduli (e.g., 128) show partial length generalization with RPE and larger models, so modulus-type alone does not completely determine outcome; the exact reasons for differences among non-power-of-10 moduli are not fully explained (hypotheses such as primality vs power-of-two are noted but not proven).",
            "uuid": "e8402.6",
            "source_info": {
                "paper_title": "Length Generalization in Arithmetic Transformers",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-attention with relative position representations",
            "rating": 2
        },
        {
            "paper_title": "An improved relative self-attention mechanism for transformer with application to music generation",
            "rating": 2
        },
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 1
        },
        {
            "paper_title": "Neural gpus learn algorithms",
            "rating": 1
        },
        {
            "paper_title": "Grokking: Generalization beyond overfitting on small algorithmic datasets",
            "rating": 1
        }
    ],
    "cost": 0.018242,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Length Generalization in Arithmetic Transformers</h1>
<p>Samy Jelassi<br>Princeton University<br>Yuhuai Wu<br>Stanford University<br>Google Research</p>
<p>Stéphane d'Ascoli<br>EPFL<br>Yuanzhi Li<br>Carnegie Mellon University<br>Microsoft Research</p>
<h2>Carles Domingo-Enrich</h2>
<p>New York University</p>
<h2>François Charton</h2>
<p>Meta AI</p>
<p>June 28, 2023</p>
<h4>Abstract</h4>
<p>We examine how transformers cope with two challenges: learning basic integer arithmetic, and generalizing to longer sequences than seen during training. We find that relative position embeddings enable length generalization for simple tasks, such as addition: models trained on 5-digit numbers can perform 15-digit sums. However, this method fails for multiplication, and we propose train set priming: adding a few ( 10 to 50 ) long sequences to the training set. We show that priming allows models trained on 5-digit $\times 3$-digit multiplications to generalize to $35 \times 3$ examples. We also show that models can be primed for different generalization lengths, and that the priming sample size scales as the logarithm of the training set size. Finally, we discuss potential applications of priming beyond arithmetic.</p>
<h2>1 Introduction</h2>
<p>Transformers (Vaswani et al., 2017) achieve remarkable results in domains ranging from Natural Language Processing (NLP) (Vaswani et al., 2017; Devlin et al., 2018), to computer vision (Dosovitskiy et al., 2020), reinforcement learning (Chen et al., 2021; Janner et al., 2021), and program synthesis (Austin et al., 2021). Yet, they struggle on simple tasks, such as integer arithmetic (Nogueira et al., 2021). Recent, transformer-based, large language models, such as ChatGPT (Schulman et al., 2022), can perform arithmetic on small integers, but their performance drops steeply as operands become large. The text corpora used to train language models is partly responsible for this situation. Most of the problems of mathematics featured in these data sets involve small numbers. In fact, large integers, with 15 digits or more, almost never appear in print. The absence of large numbers in the training data limits the mathematical ability of large language models. To mitigate this, language models must be able to extrapolate the small number arithmetic they have learned, to larger integers.</p>
<p>Most prior works on learning arithmetic with transformers (Nogueira et al., 2021; Power et al., 2022) consider the in-distribution setting, where numbers in the training and test sets are drawn from the same distribution. Out-of-distribution experiments, and in particular extrapolation to larger numbers, have so far proven disappointing.</p>
<p>On the other hand, length generalization in transformers has been widely studied. The seminal paper by Shaw et al. (2018) identified the position embedding (PEs) as the likely culprit for</p>
<p>their inability to generalize. Indeed, the absolute position embeddings (APEs), used in many implementations, mix the representation of a token with the embedding of its position in the sequence, making trained models very susceptible to changes in sequence lengths. Since then, several papers have proposed to use relative position embeddings (RPEs), that encode the relative distance between tokens Shaw et al. 2018, Huang et al. 2018, Dai et al. 2019, Huang et al. 2020, or to replace position embeddings by weighted attention schemes Raffel et al. 2020, Su et al. 2021, Press et al. 2021). While these changes improved extrapolation in natural language processing (NLP), their impact on arithmetic tasks has been little studied.</p>
<p>Recent work suggests that large language models can generalize to longer sequences for the addition task, thanks to specialized prompt engineering techniques Zhou et al. 2022. However, results for multiplication are limited to short extrapolation lengths (7 digits).</p>
<p>In this paper, we study length generalization in transformers for four basic arithmetic tasks: addition, modular addition, multiplication and modular multiplication. We train models on 5-digit operations, and investigate their ability to generalize to numbers with up to 20 digits for addition, and 35 digits for multiplication. We show that the use of relative position embeddings allows for length generalization in the case of addition and some modular operations. For 5-digit $\times$ 3-digit multiplication, we show that train set priming: adding a tiny amount of examples (50 out of 5000) from the target distribution, surprisingly allows the model to length generalize to very long operands (i.e. 35-digit $\times$ 3-digit multiplications). The paper is organized as follows.</p>
<ul>
<li>Section 2 presents our experimental setup: problems, data generation, encoding, models, training and evaluation.</li>
<li>Section 3 demonstrates that, on the addition task, encoder-only transformers using relative position embeddings, can length generalize.</li>
<li>Section 4 presents our results for modular arithmetic. In some cases, absolute position embedding allow for length generalization.</li>
<li>Section 5 introduces train set priming and shows that it achieves extrapolation to very long multiplications.</li>
<li>Section 6 discusses the results, highlights a few additional results and proposes some future directions.</li>
</ul>
<p>Contributions. This paper delivers five key messages.</p>
<ul>
<li>Relative position embeddings ensure length generation in addition. Models trained to add 5-digit numbers can generalize to 20-digit operands.</li>
<li>Simple techniques fail for multiplication. RPE do not allow length generalization. Fine-tuning on long sequences helps generalize, but requires a lot of samples from the target distribution. Also, it causes catastrophic forgetting.</li>
<li>Train set priming enables length generalization. For multiplication, adding a tiny amount of long sequences to the training set ( 50 out of the $9 \times 10^{34}$ possible 35 -digit numbers) allows generalization to 35-digit operands. Remarkably, the number of long sequences is much smaller than the one needed for fine-tuning.</li>
<li>
<p>Priming sample size scales as the logarithm of the train set size.</p>
</li>
<li>
<p>Primed model can extrapolate to several lengths. A model trained to multiply 5-digit numbers can be primed, with 500 priming examples, to generalize to numbers with 6 to 35 -digits. On the other hand, 500 examples along would be far from sufficient to train a model to multiply 6 to 35 digits.</p>
</li>
</ul>
<p>Remark: In our multiplication experiments, we arbitrarily fix the second operand to have 3 digits. This is to ensure that the task is challenging enough. Regarding the first operand, we arbitrarily set the extrapolation to 35 in order to hightlight that our models are really able to do length generalization when using priming. However, we believe that our empirical results would still hold when extrapolating to any reasonable length.</p>
<h1>Related work</h1>
<p>Transformers for mathematics. Early applications of transformers to mathematics focus on symbolic computations. Lample and Charton (2019) trained them to perform symbolic integration and solve differential equations. Polu and Sutskever (2020) applied them to theorem proving, Hahn et al. (2020) to temporal logic, and Dersy et al. (2022) trained them to simplify formulas involving polylogarithms. Nogueira et al. (2021) investigates their limitations on basic arithmetic operations. Palamas (2017) experiments with modular arithmetic, and Wenger et al. (2022) demonstrates that universal transformers can be trained to perform modular inversion. Despite their limitations in arithmetic, Charton (2021) shows that transformers can perform numerical calculations, like computing eigenvalues or inverting matrices.</p>
<p>With the advent of large language models (Bommasani et al., 2021), a new line of research focuses solving problems of mathematics written in natural language (Griffith and Kalita, 2021; Meng and Rumshisky, 2019; Cobbe et al., 2021). Lewkowycz et al. (2022) show that a large pre-trained transformer can be retrained on a large math corpus to solve grade and high school problems of mathematics.</p>
<p>Length generalization with transformers. Multiple works observe the difficulty of transformers to length generalize especially in NLP (Shaw et al., 2018; Murray and Chiang, 2018; Rosendahl et al., 2019; Press et al., 2021). Several techniques have then been introduced to address this problem: new position embeddings Shaw et al. (2018); Dai et al. (2019); Raffel et al. (2020); Huang et al. (2020); Kiyono et al. (2021); Su et al. (2021); Press et al. (2021), introducing new tokens Newman et al. (2020), new attention mechanisms Dubois et al. (2019). In this paper, we leverage one of these techniques (RPE) for addition and introduce a new one, train set priming, for multiplication.</p>
<p>Length generalization in mathematics. Generalization to long sequences, in arithmetic operations, is a longstanding problem. Using recurrent architectures, Joulin and Mikolov (2015) and Kaiser and Sutskever (2015) achieve length generalization in the case of binary addition and multiplication. Later, Trask et al. (2018) introduces NALU, an architecture that learns addition and multiplication, and that generalizes to any length. However, their network has hand-crafted modules that are specifically designed to encode addition and multiplication. Several recent works use auto-regressive models to length generalize in math tasks. Anil et al. (2022) and Zhou et al. (2022) show that fine-tuning or scratchpad (Nye et al., 2021; Wei et al., 2022) on autoregressive decoder models is insufficient to length generalize. They tackle this by changing the scratchpad</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Model overview. We linearly embed each symbol token, add position embeddings, and feed the resulting sequence of vectors to a transformer or universal transformer encoder. In order to predict the result of the operation, we select the first nout tokens and apply a linear classifier to each of them.</p>
<p>procedure and designing new prompt engineering techniques. Closer to our work, Zhang et al. (2022) train encoder-only models to length generalize on variable assignment tasks.</p>
<h2>2 Experimental setup</h2>
<h3>2.1 Problems and encodings</h3>
<p>We consider four arithmetic tasks:</p>
<ul>
<li>Addition: y = x1 + x2.</li>
<li>Modular addition: y ≡ x1 + x2 [c].</li>
<li>Multiplication: y = x1 × x2.</li>
<li>Modular multiplication: y ≡ x1 × x2 [c],</li>
</ul>
<p>with x1 and x2, two positive integers, and c &gt; 1, a fixed modulus. Our models are trained to predict y from (x1, x2).</p>
<p>For the addition tasks, the train set is composed of pairs of positive integers with up to 5 digits, i.e. (x1, x2) ∈ ℕ105. x1 is randomly sampled from a fixed set of N_{train} values (we usually set N_{train} = 5000). x2 is uniformly sampled in ℕ105. Since N_{train} ≪ 100,000, the training set only covers a small portion of the problem space. This guarantees that the model will not overfit. Trained models are tested on random pairs of positive integers with n_{test} digits: (x1, x2) ∈ ℕ2p, p = 10^{ntest}. We set n_{test} = 5 for in-domain accuracy, and n_{test} ∈ {6, ...20} for length generalization.</p>
<p>For multiplication, we train from pairs of positive integers with up to 5-digits and 3-digits, i.e. x1 &lt; 105 and x2 &lt; 103. We henceforth refer to this setting as “5 × 3 multiplication”. As before, x1 is randomly sampled from a fixed set of N_{train} examples, and x2 is uniformly sampled in ℕ1000.</p>
<p>Trained models are tested on $n_{\text {test }} \times 3$ products, with $n_{\text {test }}=5$ in-domain, and $n_{\text {test }} \in{6, \ldots 35}$ for length generalization.</p>
<p>Data formatting. The arithmetic operations (e.g. $535 \times 257$ ) and the integers (137495) that correspond to model input and output are encoded as sequences of discrete symbols. Integers are represented as sequences of digits, in base 10, and padded (using the special token <PAD>) to lengths $n_{\text {test }}$ for input operands, and $n_{\text {out }}$ for output. We have $n_{\text {out }}=n_{\text {test }}+1$ for addition, and $n_{\text {out }}=2 n_{\text {test }}$ for multiplication. The four operations are encoded with the dedicated tokens,$+ \mathbf{\%}$, $\times$ and $<em>$. Overall, we use a vocabulary of 15 tokens: ${0, \ldots, 9,+, \mathbf{\%}, \times, </em>,&lt;$ PAD $&gt;}$. For example, for addition with $n_{\text {train }}=2$ and $n_{\text {test }}=3$, the train and test examples $12+39=51$ and $999+345=1344$ would be encoded as:</p>
<p>$$
\begin{aligned}
&amp; x^{\text {train }}=12&lt;\text { PAD }&gt;+39&lt;\text { PAD }&gt; \
&amp; y^{\text {train }}=51&lt;\text { PAD }&gt; \
&amp; x^{\text {test }}=999+345 \
&amp; y^{\text {test }}=1344
\end{aligned}
$$</p>
<p>We use the padding symbol in order to ensure that all the input sequences and output sequences have the same length. This is crucial for the model in order to deal with carries.</p>
<p>Training procedures. We use the following three procedures. Standard training is used in Sections 3 and 4. Fine-tuning and priming are introduced in Section 5. In all training procedures, the first operands and randomly sampled from a fixed set of $N_{\text {train }}$ examples, and the second operands are generated online (i.e. uniformly sampled between 1 and $10^{5}$ for addition, and between 1 and $10^{3}$ for multiplication).</p>
<ul>
<li>Standard training: the model is trained on $N_{\text {train }}$ examples of $n_{\text {train }}$-digit integers.</li>
<li>Fine-tuning: the model is trained on $N_{\text {train }}$ examples of $n_{\text {train }}$-digit integers and then fine-tuned on $N_{\text {fine }}$ examples of $n_{\text {test }}$-digit integers.</li>
<li>Train set priming: the model is trained on $(1-\varepsilon) N_{\text {train }}$ examples of $n_{\text {train }}$-digit integers and $\varepsilon N_{\text {train }}$ priming examples of $n_{\text {test }}$-digit integers, with $\varepsilon \ll 1$. The priming examples are fixed throughout the training.</li>
</ul>
<p>Evaluation sets. During and after training, model performance is evaluated on randomly generated test sets, of $N_{\text {test }}$ integers with $n$ digits. The resulting accuracy is said to be in-distribution (ID) when $n=n_{\text {train }}$, and out-of-distribution (OOD) when $n&gt;n_{\text {train }}$. New test sets are generated online for each evaluation step. If not specified otherwise, we use $n_{\text {train }}=5, N_{\text {train }}=5000$, and $N_{\text {test }}=10000$. We set $n_{\text {test }}=20$ for addition, and $n_{\text {test }}=35$ for multiplication.</p>
<h1>2.2 Model and training</h1>
<p>Model. We experiment with two encoder-only architectures: a regular transformer (Vaswani et al., 2017), and a universal transformer (UTransformer) (Dehghani et al., 2018), in the HuggingFace implementation (Wolf et al., 2020) of BERT (Devlin et al., 2018) and ALBERT (Lan et al., 2019). Our model is a stack of three components (see Figure 1):</p>
<ol>
<li>
<p>Embedding: a ( $s_{\text {vocab }} \times d_{\text {model }}$ )-trainable embedding layer and a position embedding.</p>
</li>
<li>
<p>Encoder: an encoder-only transformer or UTransformer.</p>
</li>
<li>Classifier: encoder output is truncated (to its first $n_{\text {out }}$ elements, forming a $n_{\text {out }} \times d_{\text {model }}$ matrix), which is processed by a linear layer that outputs $n_{\text {out }} \times s_{\text {vocab }}$ predictions, and encodes each symbol as a one-hot vector.</li>
</ol>
<p>Important note: Although we use the HuggingFace implementation, our encoders are not pre-trained, and we do not use masked language modelling. We train non-causal encoders in a supervised way, using cross-entropy loss.</p>
<p>Notes on design. We chose to use universal transformers, i.e. transformers with shared layers (Dehghani et al., 2018), because recurrent models are used in prior work on length generalization (Bansal et al., 2022; Kaiser and Sutskever, 2015), and universal transformers proved essential on tasks involving modular arithmetic (Wenger et al., 2022). We believe shared-layer architectures are central to solving arithmetic problems, because they embed the recursive nature of many algorithms. They also seem fit for extrapolation tasks where a long operand is processed by successive applications of a simple technique (e.g. one-digit add and carry).</p>
<p>The choice of an encoder-only model contrasts with concurrent works that consider decoder-only (Power et al., 2022; Bueno et al., 2022; Zhou et al., 2022) or sequence to sequence (seq2seq) models (Nogueira et al., 2021). We believe that autoregressive models, such as the decoder-only architecture, are not optimal for problems of arithmetic, because they are trained to learn the correlations between successive tokens in the input sequence. In natural language, these correlations are meaningful: they represent the syntactic and grammatical relations between words in a sentence. In arithmetic, these correlations are tiny: knowing that the first three digits of number 1234 are 1,2 and 3 , offers no clue about the value of the fourth digit. As for seq2seq models, in problems where output are guaranteed to be shorter than input, we consider an auto-regressive decoder as an unnecessary complication. Overall, we choose encoder-only models because they are the simplest architecture that can address our problems.</p>
<p>Learning problem. We frame our arithmetic tasks as the following supervised multi-classification problem:</p>
<p>$$
\min <em i="1">{\theta \in \Theta} \sum</em>
$$}^{N_{\text {train }}} \sum_{j=1}^{n_{\text {out }}} \sum_{k=1}^{s_{\text {vocab }}} \mathbf{1}\left[y_{i}[j]=k-1\right] \frac{e^{f_{\theta}\left(x_{i}\right)[j, k]}}{\sum_{k^{\prime}=1}^{s_{\text {vocab }}} e^{f_{\theta}\left(x_{i}\right)\left[j, k^{\prime}\right]}</p>
<p>where $f_{\theta}\left(x_{i}\right) \in \mathbb{R}^{n_{\text {out }} \times s_{\text {vocab }}}$ are the model logits evaluated at $x_{i}$ and $\theta \in \Theta$ are the model parameters. To solve (1), we minimize the cross entropy between model predictions and the ground truth symbols for each position in the sequence. An alternative approach, perhaps more natural, would consider these problems as regressions. However, prior works report that reformulating regression as classification leads to state-of-the-art performance (Rothe et al., 2015; Rogez et al., 2017; Akkaya et al., 2019; Schrittwieser et al., 2020).</p>
<p>We consider three model sizes. Base (B) models have $D=6$ layers, $d_{\text {model }}=512$ dimensions, and $h=8$ attention heads, Standard (S) models have $D=6, d_{\text {model }}=1024$ and $h=16$, and Large (L) models,</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Number of digits</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Encoder</td>
<td style="text-align: center;">PE</td>
<td style="text-align: center;">Size</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">Transformer</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">$\mathbf{9 7 . 2}$</td>
<td style="text-align: center;">$\mathbf{2 1 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">1.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">19.2</td>
</tr>
<tr>
<td style="text-align: left;">UTransformer</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">$\mathbf{9 8 . 3}$</td>
<td style="text-align: center;">$\mathbf{1 8 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">1.4</td>
</tr>
</tbody>
</table>
<p>Table 1: Addition: Impact of encoder type, size and position embeddings on length generalization. We consider transformers and UTransformers in their Base (B) and Large (L) format, using three position embeddings methods $\left(\mathrm{APE}, \mathrm{RPE}<em k_="k," q="q">{k}, \mathrm{RPE}</em>\right)$. We evaluate different degrees of extrapolation: easy ( 6 digits), medium ( 10 digits) and hard ( 15 and 20 digits). The models are trained on 5000 examples with 1 to 5 digits and we report the accuracy reached by the models on 100,000 example test sets. Results are averaged over 3 seeds.
we have $D=10, d_{\text {model }}=1024$ and $h=16$. We investigate three kinds of position embeddings: absolute (APE) Vaswani et al. (2017), relative over keys $\left(\mathrm{RPE}<em k_="k," q="q">{k}\right)$ Shaw et al. (2018), and relative over keys and queries $\left(\mathrm{RPE}</em>$ is our default option. All other parameters are set to the default HuggingFace values, and are initialized with random Gaussian values.}\right)$ Huang et al. (2018). $\mathrm{RPE}_{k</p>
<p>Optimization. We train our models using AdamW (Loshchilov and Hutter, 2017), with a batch size to 32 , a learning rate between $10^{-5}$ and $10^{-4}$ and weight decays in ${1 \mathrm{e}-5,1 \mathrm{e}-4,1 \mathrm{e}-3,1 \mathrm{e}-2}$. We apply a cosine scheduler Loshchilov and Hutter (2016) to update the learning rate and train the model for 15000 epochs of $N_{\text {train }}$ examples.</p>
<h1>3 Addition: relative position embeddings enable length generalization</h1>
<p>In these experiments, we train transformers to add two numbers with up to five digits, and test trained models on sums of numbers with 6 to 20 digits. We compare the Transformer and UTransformer encoders, in their Base ( 6 layers, 512 dimensions, 8 attentions heads) and Large ( 10 layers, 1024 dimensions, 16 heads) configurations, using three position embeddings: absolute, relative on keys, and relative on keys and queries. All models achieve $100 \%$ in-domain accuracy. We make the following observations (Table 1):</p>
<ul>
<li>Models using the absolute position embedding fail to generalize. Our best models achieve $3.1 \%$ accuracy on 6 -digit test examples, and $0 \%$ for all longer lengths. This was</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Scaling laws for integer addition. We train Transformers and UTransformers, with standard model size $\left(d_{\text {model }}=16, D=6, h=16\right)$ to add numbers with up to 5 digits. We set $N_{\text {train }}=50000$. We vary their hidden size (a) and depth (b). The $y$-axis indicates the largest extrapolation length where the model achieves $75 \%$ accuracy. Results are averaged over 3 seeds.
observed in previous works Shaw et al. (2018); Dai et al. (2019); Huang et al. (2020); Kiyono et al. (2021).</p>
<ul>
<li>Models using relative position embedding generalize to longer sequences. Our best models achieve $99.9 \%$ accuracy on 10 -digits test sets, and $98.3 \%$ on 15 -digit sets. Performance drops for longer sequences: we achieve $21.3 \%$ for 20 -digits numbers. We remark that the RPE key variant is crucial for achieving extrapolation.</li>
</ul>
<p>In APE models, because the position embedding is added to the embedding of every token, the rules of addition must be learned separately for every position. At test time, a model trained on operands with 5 digits only will not know how to handle digits in position 6 , or 7 , even though it has learned to add digits in position 1 to 5 . Further discussion of the role of position embeddings, and additional experiments on model failures, can be found in Section 6.</p>
<p>Depth and dimension for longer extrapolation. Figures 2a and 2b provide ablation results on model dimension and depth. For models with 64 to 1024 dimensions and 2 to 8 layers, trained on 5 digit examples, they indicate the largest extrapolation length that the model can achieve with $75 \%$ accuracy. A minimal hidden size of 512 for Transformers, and 256 for UTransformers, is needed for the model to extrapolate. Past this value, length extrapolation scales with dimension, and 1024-dimension models achieve 17-digit extrapolation. UTransformers need 6 layers to extrapolate, whereas shallow Transformers with 2 layers can extrapolate to 10-digit numbers. The efficiency of shallow transformer models for computational tasks was observed in previous works (Charton, 2021).</p>
<h1>4 Modular arithmetic</h1>
<p>In this section, we study modular addition $y \equiv\left(x_{1}+x_{2}\right)[c]$ and multiplication $y \equiv\left(x_{1} \times x_{2}\right)[c]$, for $c \in{100,101,128,1000}$. The difficulty of these operations depends on the modulus $c$. When $c$ is a power of 10 , i.e. $c=10^{k}$, modular operations only involve the $k$ last digits of their operands, and the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Modulo</th>
<th style="text-align: center;">PE</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
<th style="text-align: center;">Digits <br> 10</th>
<th style="text-align: center;">15</th>
<th style="text-align: center;">20</th>
<th style="text-align: center;">c</th>
<th style="text-align: center;">PE</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">10</th>
<th style="text-align: center;">Digits</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">88.1</td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">6.4</td>
</tr>
<tr>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">3.9</td>
</tr>
</tbody>
</table>
<p>Table 2: Modular addition and multiplication: (a) Extrapolation results for addition and (b) for multiplication. We train a UTransformer in its base version $\left(D=6, d_{\text {model }}=512, h=8\right)$ with three position embedding methods $\left(\mathrm{APE}, \mathrm{RPE}<em k_="k," q="q">{k}, \mathrm{RPE}</em>\right)$. We report the accuracy on 100,000 example test sets.
result has constant length $k$. This makes these operations easier to learn (because they only involve $k$ digits), and easier to generalize (because $k$ is independent of the length of the operands). When the modulus is not a power of 10 , the problem becomes harder than tbeir non-modular verison, because modularity adds an integer division on top of the operation (addition or multiplication).</p>
<p>Modular addition. In the "easy" cases $(c \in{100,1000})$, RPE-based models generalize to large numbers, achieving better extrapolation performance than for non-modular addition (Table 2a). This is expected, because this is an easier task than standard addition. Interestingly, APE-based models do generalize; they achieve $73.3 \%$ accuracy on 10-digit numbers. This confirms our intuition that the failure of APE on length generalization is a consequence of their inability to deal with change in output sequence lengths.</p>
<p>For the hard cases $(c \in{101,128})$, no model manages to learn 5-digit modular addition indomain. Scaling to larger architectures, with up to 14 layers and 1280 dimensions, brings no improvement. This matches previous observations by Palamas (2017), about the difficulty of learning modular arithmetic in the general case.</p>
<p>Modular multiplication. In the easy cases $(c \in{100,1000})$, both APE and RPE-based model generalize, achieving $100 \%$ on 35-digit numbers for $c=100$. For $c=1000$, APE achieve $43 \%$ on 20-digit numbers, but the use of RPE improves performance, to $83 \%$ on 20-digit numbers and $55 \%$ on 30-digit numbers (Table 2b). On hard instances (see Appendix A), for $c=128$, the model performance drops, both in and out of domain, but length generalization still happens, and is facilitated by RPE and larger models. Finally, for $c=101$, models can learn modular multiplication in-domain, but consistently fail on longer sequences. Modular multiplication turns out to be easier to learn than modular addition. A possible explanation is the fact that multiplication tables display more redundancy, that the model can exploit, than addition tables.</p>
<p>Our experiments with modular arithmetic help understand the role of position embeddings. APE-based models generalize when they learn an operation involving a fixed number of input tokens, and constant length output.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Second <br> operand</th>
<th style="text-align: center;">PE</th>
<th style="text-align: center;">Digits</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">1-digit</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">2-digits</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">3-digits</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 3: Multiplication by 1, 2 and 3-digit numbers: We train a UTransformer in its standard version $\left(D=6, d_{\text {model }}=1024, h=16\right)$ with three position embeddings $\left(\mathrm{APE}, \mathrm{RPE}<em k_="k," q="q">{k}, \mathrm{RPE}</em>\right)$. ID and OOD accuracy on 100,000 test examples.</p>
<h1>5 Multiplication: train set priming for length generalization</h1>
<p>We focus on the length generalization problem where we train a UTransformer to multiply 5-digit numbers by 3-digit numbers, from $N_{\text {train }}=5000$ examples and train it on a set of $N_{\text {train }}=5000$ examples that are $\left(n_{\text {train }} \times 3\right)$-multiplications with $n_{\text {train }} \leq 5$. We test its extrapolation ability to perform $35 \times 3$ multiplications.</p>
<h3>5.1 Relative position embeddings and fine-tuning</h3>
<p>Relative position embeddings are not sufficient. We first train UTransformers with the three position embedddings (Table 3). All models achieve close to $100 \%$ in-domain accuracy, but fail to generalize to numbers with 6 digits or more. For $5 \times 3$ multiplication, RPE do not generalize. On simpler versions of this task $(5 \times 2$ and $5 \times 1)$, RPE models achieve limited generalization to 6-digit numbers ( 12.2 and $16.9 \%$ for 1 and 2 -digits), but fail for longer sequences.</p>
<p>Fine-tuning requires a sizable sample set. Fine-tuning is a common solution for transfer learning (extrapolating from one distribution to another). Here, we first train a model on $5 \times 3$ multiplication, then re-train it on a fixed sample of $35 \times 3$ examples. We observe (Figure 3a) that 35-digit multiplication can indeed be learned by fine-tuning on a set of 1000 examples. This is a large number: as we shall see, train set priming allows for much smaller samples. Besides, the fine-tuned model is not longer able to perform $5 \times 3$ multiplication, a phenomenon known as catastrophic forgetting (McCloskey and Cohen, 1989).</p>
<h3>5.2 Priming for length generalization in multiplication.</h3>
<p>As an alternative, we introduce train set priming: adding a tiny amount ( $\varepsilon \%$ ) of long sequences to the training set. By adding 5035 -digit examples $(\varepsilon=1 \%)$, our model achieves close to $100 \%$ accuracy on $5 \times 3$ and $35 \times 3$ multiplication (Figure 3b). To reach equivalent performance, train sample priming needs 20 times less examples than fine-tuning. $5 \times 3$ multiplication is learned after a</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Fine-tuning (a) and train set priming (b). (a) fine-tuning, the model is trained on $5 \times 3$ multiplications, then fine-tuned on $35 \times 3$ multiplications. Final accuracy of $5 \times 3$ and $35 \times 3$ multiplications as a function of the number of fine-tuning examples. (b) priming, fifty $35 \times 3$ examples are added to the training set. Learning curves for 5-digit and 35-digit accuracy. All experiments use a standard UTransformer $\left(D=6, d_{\text {model }}=1024, h=16\right)$. Average over 3 seeds.
few hundred thousand examples, $35 \times 3$ multiplication (OOD generalization) after 1500 epochs, or 7.5 million examples ( 1500 passes over 5000 fixed examples), but only 75,00035 -digit example (i.e. 1,500 passes over 50 fixed examples, out of $9.10^{34}$ possible 35 -digit integers).</p>
<p>A minimal priming rate is required. Adding less than 25 samples ( 25 examples, $\varepsilon=0.5 \%$ ) prevents generalization. Over that threshold, accuracy increases with the priming rate (Figure 4a).</p>
<p>Priming sample scales logarithmically with train set size. As the number of training examples increases, so does the number of priming examples required to extrapolate to $35 \times 3$. However, it scales logarithmically: $30(\varepsilon=3 \%)$ priming examples are needed for $10^{3}$ training examples, $70(\varepsilon=0.7 \%)$ for $10^{4}$ and $100(\varepsilon=0.1 \%)$ for $10^{5}$ (Figure 4b).</p>
<p>Priming sample scales linearly with extrapolation length. Whereas 50 samples are needed for 35-digit generalization, 6-digit generalization only needs 10 (Figure 4c).</p>
<p>Curriculum priming fails. We consider curriculum priming as a possible improvement. Instead of priming on long sequences only (i.e. 35-digit numbers), we could split the priming examples between several lengths, from 6 to 35 . In most cases, curriculum priming fails to extrapolate to $35 \times 3$ multiplication, but one curriculum proves effective: priming the model on a mixture of 34 and 35 -digits numbers (Figure 4d). This causes the model to learn faster and achieve higher extrapolation accuracy.</p>
<h1>5.3 Priming for extrapolation at all lengths</h1>
<p>Priming the train set with 35-digit numbers only allows to extrapolate to 35-digit operands. No other extrapolation lengths are learned in the process (Figure 5a). However, by priming on numbers of all lengths from 6 to 35 , the model can extrapolate to all lengths up to 35 . This can be done at a</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Ablations on priming sample size. (a) Accuracy of $35 \times 3$-multiplications vs priming sample size. (b) Priming sample needed to achieve $90 \% 35$-digit accuracy for different train set sizes. (c) Priming sample needed to achieve $90 \%$ accuracy, for different extrapolation lengths. (d) Learning curves for 35-digit priming, and 34 and 35-digit curriculum. All experiments use a standard UTransformer $\left(D=6, d_{\text {model }}=1024, h=16\right)$. Results are averaged over 3 seeds.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Training set priming to all lengths. (a) Priming with 35-digit numbers only. (b) Priming with a mixture of all length. (c) Distribution of priming lengths for figure (b). (d) Priming on even lengths only. All experiments use a standard UTransformer $\left(D=6, d_{\text {model }}=1024, h=16\right)$. Average over 3 seeds.
moderate cost in additional data. Using the priming distribution from Figure 5c, our models learn to extrapolate with over $95 \%$ accuracy to all lengths (see Figure 5b). The priming set size is 500 , for a priming rate of $\varepsilon=10 \%$. More efficient priming distributions might exist: the point of this experiment is to show that priming to all lengths is possible within a reasonable data budget $\varepsilon$. On the other hand, we observe that all extrapolation length must be primed. For instance, if only even lengths are primed, the model only generalizes to even lengths. There is no overspill to odd lengths (Figure 5d).</p>
<h1>6 Discussion</h1>
<h3>6.1 Why do RPEs extrapolate better than APEs?</h3>
<p>In Section 3, we notice that replacing APE by RPE is the key for models to length generalize. Three experiments help understand the role of RPE.</p>
<p>Element-wise addition. A possible reason for generalization in RPE-based models, is that relative embeddings allow tokens to "know their neighbors". This could help models learn local operations, like carry propagation (an important factor in integer addition). To test this hypothesis,</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Success and failure cases in addition. (a) Accuracy of 20-digit sums, by number of carries in the sum. (b) Accuracy of 20-digit sums, by maximum number of consecutive carries. (c) Distribution of the number of incorrect digits in wrong predictions of 20-digit sums. (d) Positions of incorrect digits in sumes where only one digit is wrong. All experiments use a standard UTransformer $\left(D=6, d_{\text {model }}=1024, h=16\right)$, achieving $57 \%$ accuracy on 20-digit additions.
we train models on element-wise addition $\oplus$ (i.e. addition without carries: $99 \oplus 35=24$ ). If carry propagation is the reason why RPE succeed, APE-models should generalize on this task. Experimental results (in Appendix A) show that APE fail to generalize on element-wise addition, whereas RPE succeed, this disproving our hypothesis. It is striking to note (see Table 8) that when the generalize, APE models almost always predict the the 5 leftmost digits of the results, i.e. its "in-domain" positions, thus confirming our intuition that APE learn addition digit by digit.</p>
<p>Modular arithmetic. As we have seen, APE models length generalize on these tasks when the modulus is a power of 10 . (Tables 2a and 2b). In both cases, the model output have constant length. This, together with our element-wise results, suggest that varying output lengths are an important factor of APE extrapolation failures.</p>
<p>RPE-models learn all digits at once. Figures 7a and 7b present learning curves for each position in the output, when a model is trained on 5-digit addition (e.g. the 6 curve is the learning curve of the units of the sum, the 5 -curve is the tens). We note that whereas the first and last digits in the sums are learned first, all other digits are learned simultaneously by RPE models, whereas APE models seem to learn each position independently. This suggests that RPE models might learn a single algorithm for all positions, which greatly helps them to generalize.</p>
<h1>6.2 Failure cases in addition</h1>
<p>Figure 6 provides an analysis of model failures when extrapolating to 20-digit sums. First, we assess the role of carries, by introducing two metrics: the total number of carries (NC), and the maximum number of consecutive carries (MC). As Figures 6a and 6b indicate, almost all model failures happen on additions involving at least three carries, and two consecutive carries. Larger values of MC and NC have no further impact.</p>
<p>Figures 6 c and 6 d present the number of incorrect digits in wrong model predictions and their position. We note that, when wrong, the model usually does not hallucinate a irrelevant answer (with many wrong digits), but fails on just a few. Errors also concentrate on the first and second positions: the largest powers of ten in the sum.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Digit by digit learning curves. Training accuracy for each output digit (1 are the largest powers, 6 the units for a sum).(a) Addition APE models. (b) Addition RPE models. (c) Multiplication RPE models (no priming) (d). Multiplication RPE models (with priming). In all these experiments, 1 denotes the leftmost digit position while 6 (for addition) and 8 (for multiplication) All experiments use a standard UTransformer $\left(D=6, d_{\text {model }}=1024, h=16\right)$.</p>
<h1>6.3 More about priming</h1>
<p>Train set priming is our most striking result. In Section 5, we demonstrate that is allows length generalization in multiplication. We now present additional results. We first show that train set priming is also effective on APE models. Then, we investigate how the models learn multiplication.</p>
<p>Primed APE models generalize. In Appendix A, we show that priming on APE models also yields length generalization. We obtain a similar dynamics as in Figure 3b where the ID accuracy quickly increases and the OOD accuracy slowly follows (Figure 9a). However, as expected, this does not make APE models a viable proposition: the priming rate needed is 10 times larger i.e. $\varepsilon=10 \%$.</p>
<p>Primed models learn several digits simultaneously. In our addition experiments in Subsection 6.1, we noticed that whereas APE models learn to predict their output digit by digit as training proceeds (Figure 7a), RPE models seem to learn them all at once (Figure 7a). A similar pattern can be seen for multiplication with RPE models. Without priming (Figure 7c), models seem to learn $5 \times 3$ multiplication one digit at a time, over 1000 epochs. With priming, the model seems to learns several digits concurrently Figure 7d. A similar phenomenon holds for APE models: without priming, the model independently learns each digit (Figure 9b) while the digits are concurrently learnt with priming (Figure 9c). In summary, simultaneous learning of all the training digit positions seems a key determinant of length generalization.</p>
<h3>6.4 Priming beyond arithmetic</h3>
<p>Our work demonstrates that train set priming can improve the length generalization of transformers on arithmetic tasks. Compared to fine-tuning, it requires much fewer samples from the target distribution and allows for generalization without catastrophic forgetting. We conclude on a number of open questions, which constitute as many avenue for future research. All these directions may help shed light on the capabilities and limitations of transformers, and inspire new methods for improving their generalization and adaptation.</p>
<ul>
<li>Can priming be extended to other mathematical problems? For instance, numerical computations, matrix operations, or symbolic mathematics.</li>
<li>Can priming help with compositionality? Investigate the limits of length generalization in terms of the number and type of operations. For instance, if we train on adding $k$ numbers,</li>
</ul>
<p>can we generalize to adding $k+1$ numbers, or if we train on compositions of additions and multiplications separately, does it generalize to compose them together?</p>
<ul>
<li>Theoretical understanding of priming: why is train set priming more effective than fine-tuning for length generalization?</li>
<li>Can priming work for NLP? Can we use priming to adapt a pre-trained language model to a new language task, without losing its performance on the original data?</li>
</ul>
<h1>References</h1>
<p>Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik's cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019.</p>
<p>Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. arXiv preprint arXiv:2207.04901, 2022.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein. End-to-end algorithm synthesis with recurrent networks: Logical extrapolation without overthinking, 2022.</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Mirelle Bueno, Carlos Gemmel, Jeffrey Dalton, Roberto Lotufo, and Rodrigo Nogueira. Induced natural language rationales and interleaved markup tokens enable extrapolation in large language models. arXiv preprint arXiv:2208.11445, 2022.</p>
<p>François Charton. Linear algebra with transformers. arXiv preprint arXiv:2112.01898, 2021.
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 2021.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.</p>
<p>Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.</p>
<p>Aurélien Dersy, Matthew D. Schwartz, and Xiaoyuan Zhang. Simplifying polylogarithms with machine learning, 2022.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16 x 16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.</p>
<p>Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Location attention for extrapolation to longer sequences. arXiv preprint arXiv:1911.03872, 2019.</p>
<p>Kaden Griffith and Jugal Kalita. Solving arithmetic word problems with transformers and preprocessing of problem text. arXiv preprint arXiv:2106.00893, 2021.</p>
<p>Christopher Hahn, Frederik Schmitt, Jens U Kreber, Markus N Rabe, and Bernd Finkbeiner. Teaching temporal logics to neural networks. arXiv preprint arXiv:2003.04218, 2020.</p>
<p>Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis Hawthorne, Andrew M Dai, Matthew D Hoffman, and Douglas Eck. An improved relative self-attention mechanism for transformer with application to music generation. 2018.</p>
<p>Zhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang. Improve transformer models with better relative position embeddings. arXiv preprint arXiv:2009.13658, 2020.</p>
<p>Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34, 2021.</p>
<p>Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. Advances in neural information processing systems, 28, 2015.</p>
<p>Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015 .</p>
<p>Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui. Shape: Shifted absolute position embedding for transformers. arXiv preprint arXiv:2109.05644, 2021.</p>
<p>Guillaume Lample and François Charton. Deep learning for symbolic mathematics. arXiv preprint arXiv:1912.01412, 2019.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.</p>
<p>Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.</p>
<p>Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.</p>
<p>Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pages 109-165. Elsevier, 1989 .</p>
<p>Yuanliang Meng and Anna Rumshisky. Solving math word problems with double-decoder transformer. arXiv preprint arXiv:1908.10924, 2019.</p>
<p>Kenton Murray and David Chiang. Correcting length bias in neural machine translation. arXiv preprint arXiv:1808.10006, 2018.</p>
<p>Benjamin Newman, John Hewitt, Percy Liang, and Christopher D Manning. The eos decision and length extrapolation. arXiv preprint arXiv:2010.07174, 2020.</p>
<p>Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021 .</p>
<p>Theodoros Palamas. Investigating the ability of neural networks to learn simple modular arithmetic. 2017 .</p>
<p>Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020.</p>
<p>Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022 .</p>
<p>Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67, 2020.</p>
<p>Gregory Rogez, Philippe Weinzaepfel, and Cordelia Schmid. Lcr-net: Localization-classificationregression for human pose. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3433-3441, 2017.</p>
<p>Jan Rosendahl, Viet Anh Khoa Tran, Weiyue Wang, and Hermann Ney. Analysis of positional encodings for neural machine translation. In Proceedings of the 16th International Conference on Spoken Language Translation, 2019.</p>
<p>Rasmus Rothe, Radu Timofte, and Luc Van Gool. Dex: Deep expectation of apparent age from a single image. In Proceedings of the IEEE international conference on computer vision workshops, pages $10-15,2015$.</p>
<p>Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.</p>
<p>J Schulman, B Zoph, C Kim, J Hilton, J Menick, J Weng, JFC Uribe, L Fedus, L Metz, M Pokorny, et al. Chatgpt: Optimizing language models for dialogue, 2022.</p>
<p>Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018.</p>
<p>Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.</p>
<p>Andrew Trask, Felix Hill, Scott Reed, Jack Rae, Chris Dyer, and Phil Blunsom. Neural arithmetic logic units. arXiv preprint arXiv:1808.00508, 2018.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Emily Wenger, Mingjie Chen, François Charton, and Kristin Lauter. Salsa: Attacking lattice cryptography with transformers. arXiv preprint arXiv:2207:04785, 2022.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38-45, 2020.</p>
<p>Yi Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling transformers with lego: a synthetic reasoning task. arXiv preprint arXiv:2206.04301, 2022 .</p>
<p>Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022.</p>
<h1>A Additional experiments</h1>
<p>In this section, we present some additional experiments that were mentioned in the paper. We first provide in Subsection A. 1 the complete results for modular addition and multiplication that were mentioned in Section 4. We then present complementary results to our discussion in Section 6. We first report the results obtained by APE and RPE models on digitwise addition. Then, we show that APE models can also be primed to length generalize in multiplication at the expense of a much larger priming rate (Subsection A.3). Lastly, we present plots showing the digit order by which RPE and APE models make the correct predictions (Subsection A.5).</p>
<h2>A. 1 Additional experiments on modular arithmetic</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">C</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Digits</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PE</td>
<td style="text-align: center;">Size</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">35</td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">88.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">6.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">7.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">26.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">3.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">20.1</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">3.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">3.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">1.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">6.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">16.2</td>
</tr>
<tr>
<td style="text-align: center;">101</td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">1.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">1.8</td>
</tr>
</tbody>
</table>
<p>Table 4: Modular addition: Extrapolation results for modulo $c \in{100,1000,128,101}$. UTransformer model in their Base and Large format. We report the accuracy reached by the models on 100,000 example test sets.</p>
<p>Table 4 provides a more complete version of Table 2a where we do modular addition for modulus $c \in{128,101}$. As explained in Section 4, the model manages to extrapolate when the modulus is a power of 10 . When $c=128,101$, the model fails to extrapolate. This shows that what the model</p>
<p>struggles when the length of the digits that matter vary.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">c</th>
<th style="text-align: center;">PE</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">Digits</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">96.2</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">88.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">6.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">7.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">26.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">3.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">20.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">3.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">3.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">1.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">6.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">16.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">APE</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">1.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{RPE}_{k, q}$</td>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">1.8</td>
</tr>
</tbody>
</table>
<p>Table 5: Modular multiplication: Extrapolation results for modulo $c \in{100,1000,128,101}$. UTransformer model in their Base and Large format. We report the accuracy reached by the models on 100,000 example test sets.</p>
<p>Table 5 provides a more complete version of Table 2b where we do modular multiplication for modulus $c \in{128,101}$. As explained in Section 4, the model manages to extrapolate when the modulus is a power of 10 . When $c=128$, the model non-trivially length generalize while when $c=101$, the model fails to extrapolate. We do not fully know why this difference happens but one hypothesis is that 101 is a prime number while 128 a power of 2 .</p>
<h1>A. 2 Element-wise addition experiments</h1>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Digits</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PE</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">20</td>
</tr>
<tr>
<td style="text-align: left;">APE</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">5.3</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">RPE $_{k}$</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">97.5</td>
<td style="text-align: right;">90.5</td>
<td style="text-align: right;">86.2</td>
<td style="text-align: right;">78.13</td>
</tr>
</tbody>
</table>
<p>Table 6: Element-wise addition: Extrapolation results. We train a UTransformer in its base version $\left(D=6, d_{\text {model }}=\right.$ $512, h=8$ ) with two position embedding methods (APE, $\mathrm{RPE}_{k}$ ). We report the accuracy reached by the models on 10,000 example test sets.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Digitwise accuracy of the APE model on elementwise addition. We train a Base UTransformer with APEs and report the accuracy on 10,000 example test sets. Average over 3 seeds.</p>
<p>We consider here an element-wise addition operation. For example, $99 \oplus 45=34$ because $(9+5) \% 10=4$ and $(9+4) \% 10=3$. We train a UTransformer on 5-digit element-wise addition $\left(N_{\text {train }}=50,000\right)$ and evaluate its extrapolation on 20-digit $\left(N_{\text {test }}=10,000\right)$. Table 6 reports the final results obtained with APE and RPE models. We observe that the RPE models manage to length generalization while the APE models fail. In Figure 8, we plot the digitwise accuracy on the test samples. We observe that the model managed to well-predict the leftmost 5 digits (those seen during training) but fails in the right-most ones.</p>
<h2>A. 3 Multiplication experiments using APEs</h2>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Additional experiments on priming for multiplication. (a) shows the accuracy on $5 \times 3$ and $35 \times 3$ multiplications obtained by an APE model. (b) and (c) respectively display the learning process of an APE model without and with train set priming on multiplication. We train a standard UTransformer $\left(D=6, d_{\text {model }}=1024, h=16\right)$ on $5 \times 3$-multiplications and test on $35 \times 3$. Training set size is $N_{\text {train }}=5000$ and test set size is $N_{\text {test }}=10000$.</p>
<p>In this section, we consider the multiplication task with UTransformers using APEs. Similarly to the RPE case, we observe that training set priming lead to successful extrapolation to $(35 \times 3)$ -</p>            </div>
        </div>

    </div>
</body>
</html>
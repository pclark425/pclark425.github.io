<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8703 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8703</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8703</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-156.html">extraction-schema-156</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <p><strong>Paper ID:</strong> paper-269929882</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.12442v1.pdf" target="_blank">Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation</a></p>
                <p><strong>Paper Abstract:</strong> Concept recommendation aims to suggest the next concept for learners to study based on their knowledge states and the human knowledge system. While knowledge states can be predicted using knowledge tracing models, previous approaches have not effectively integrated the human knowledge system into the process of designing these educational models. In the era of rapidly evolving Large Language Models (LLMs), many fields have begun using LLMs to generate and encode text, introducing external knowledge. However, integrating LLMs into concept recommendation presents two urgent challenges: 1) How to construct text for concepts that effectively incorporate the human knowledge system? 2) How to adapt non-smooth, anisotropic text encodings effectively for concept recommendation? In this paper, we propose a novel Structure and Knowledge Aware Representation learning framework for concept Recommendation (SKarREC). We leverage factual knowledge from LLMs as well as the precedence and succession relationships between concepts obtained from the knowledge graph to construct textual representations of concepts. Furthermore, we propose a graph-based adapter to adapt anisotropic text embeddings to the concept recommendation task. This adapter is pre-trained through contrastive learning on the knowledge graph to get a smooth and structure-aware concept representation. Then, it's fine-tuned through the recommendation task, forming a text-to-knowledge-to-recommendation adaptation pipeline, which effectively constructs a structure and knowledge-aware concept representation. Our method does a better job than previous adapters in transforming text encodings for application in concept recommendation. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed approach.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8703.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8703.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Enhanced Textual Representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structure-and-Knowledge-Aware Enhanced Textual Representation (LLM-enhanced key-value)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional representational format that expresses each concept as a structured key-value text including: concept name, an LLM-generated context-resolved explanation conditioned on predecessor/successor concepts, a list of predecessor names, and a list of successor names; intended to capture both semantic meaning and immediate structural relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>LLM-enhanced structured textual representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Concepts are represented as short structured documents (key–value blocks) containing (1) a symbolic name token, (2) an explanatory natural-language description generated by an LLM (GPT-3.5) disambiguated using local graph context, and (3) explicit lists of predecessor and successor concept names from a knowledge graph; this format functionally encodes lexical/definitional content plus local relational context for downstream use.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>hybrid (symbolic + textual/distributional)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Next-concept recommendation (next-item prediction) and concept-disambiguation (ambiguity resolution when concept names are polysemous).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing enhanced text that includes LLM explanation plus predecessor/successor context improved recommendation metrics relative to name-only inputs. Ablation: SKarRec-No-LLM (drop LLM explanation) shows a performance drop (e.g., ASSIST09 HR@10 0.7898 vs full 0.7922), and SKarRec-Name (name-only) drops more (ASSIST09 HR@10 0.7775). The paper reports consistent small-to-moderate improvements across datasets when enhanced text is used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Enhanced textual representation outperforms bare concept-name (symbolic) representations and improves downstream performance relative to using only graph neighbor names (No-LLM). It is complementary to graph-based embedding adaptation — the enhanced text is the input that the adapter transforms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>LLM explanations can be ambiguous if not conditioned on graph context; their quality depends on the LLM (GPT-3.5 here) and prompt design. The format relies on access to a reasonably accurate predecessor/successor graph to disambiguate meanings. The evidence is task-based (recommendation metrics) rather than cognitive/psychological experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Combining natural-language, definitional content from LLMs with local graph structure yields richer concept representations that reduce ambiguity and better capture semantic relationships functionally relevant to learning-path recommendation and alignment with human knowledge structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8703.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8703.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Graph (Prerequisite Graph)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Directed Knowledge Graph of Concepts (prerequisite/succession relations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic relational format that encodes concepts as nodes in a directed graph where edges represent prerequisite (precedence) relationships; used to capture structural dependencies between concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Symbolic directed knowledge graph (prerequisite network)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Concepts are represented as discrete nodes connected by directed edges that indicate prerequisite or successor relations (i.e., ordering constraints or learning dependencies). Functionally this format encodes adjacency and path relations used to constrain and inform recommendations.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic / relational (graph-based)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Consistency with knowledge structure in concept recommendation (proportion of recommended concepts that are predecessors or successors of the previous concept); sequencing / curriculum consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using the knowledge graph to (1) provide predecessor/successor context to LLM prompts improved concept disambiguation; (2) serve as the substrate for graph-based contrastive pre-training improved downstream recommendation consistency with the graph (SKarRec more frequently suggests concepts that are predecessors/successors of the previous concept). The paper reports higher 'consistency ratios' for SKarRec versus baselines, especially for learners whose sequences align with the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Graph-based structure is contrasted with purely text-based and ID-only representations: text-only or ID-only methods fail to capture prerequisite relations. Graph-based representations (used directly or via adapter) lead to recommendations that better respect curriculum structure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Quality depends on availability and quality of the graph; when only transition graphs (estimated from sequences) are available they are only approximations and may be noisy. Graph-based methods relying on heterogeneous data decline in performance when such data are not present. Also, a pure symbolic graph lacks definitional semantic content (hence the hybrid approach).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Explicit structural encoding of prerequisite relations is essential for educational recommendation tasks; combining symbolic graph structure with semantic descriptions supports functionally better alignment to human pedagogical ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8703.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8703.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-adapted Knowledge-space Embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-based Adapter (GNN) with Contrastive Pre-training to produce knowledge-space embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional distributed representation format: initial text-derived vectors are transformed via a graph neural network trained with graph-contrastive objectives to produce smooth, structure-aware embeddings that align with the knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Graph-adapted distributed knowledge-space embedding</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Concepts are represented as continuous vectors produced by (1) encoding enhanced text with a language model to get initial vectors, then (2) applying a GNN-based adapter which aggregates neighbor information and is pre-trained with contrastive learning on graph edge-dropout views (InfoNCE) to produce smooth embeddings reflecting graph structure; functionally these embeddings capture both semantic content and relational proximity.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed (graph-regularized) / hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Next-concept recommendation accuracy, embedding clustering quality (assessed by t-SNE and Davies–Bouldin Index), and consistency of recommendations with prerequisite graph (graph-consistency ratio).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Graph-adapted embeddings (SKarRec) substantially improve recommendation metrics and embedding cluster separation relative to vanilla text embeddings and MoE-adapted embeddings. Example quantitative evidence: Davies–Bouldin Index (ASSIST09) — vanilla text 2.0257, MoE-adapted 1.6007, SKarRec 0.6611 (lower is better). Ablation: removing contrastive graph pre-training (SKarRec-No-GraphSSL) or replacing the adapter by MoE (SKarRec-MoE) led to drops in HR/NDCG/MRR (e.g., ASSIST09 HR@10: 0.7727 and 0.7754 vs 0.7922 full).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Graph-adapted embeddings outperform (a) vanilla LLM/BART text embeddings (which are anisotropic and non-smooth), and (b) MoE-based adapters used in previous recommendation work — both by downstream recommendation metrics and intrinsic clustering (DBI) analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Adapter performance depends on graph quality and the hyperparameters of contrastive augmentation (edge masking ratio γ affects performance). The representation is still learned via neural models — the paper provides functional/task-based evidence but no behavioral cognitive validation. Also, contrastive objectives require careful negative sampling and may be sensitive to graph sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Transforming textual semantic vectors into a knowledge-aligned continuous space via graph-aware contrastive learning produces smoother manifolds and embeddings whose proximities reflect pedagogical/prerequisite similarity, which functionally improves generalization for next-concept prediction and alignment with human curricular structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8703.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8703.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ID-based Symbolic Representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Concept ID Embedding (symbolic index-based representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minimal symbolic format where each concept is represented by a learned ID embedding vector (lookup) or by its discrete name token alone; captures only identity, not intrinsic semantic content or explicit relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Symbolic ID-based representation</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Each concept is represented by a unique identifier mapped to a learned embedding vector (an index in an embedding matrix) or simply by its name token; functionally this encodes only item identity and must rely on sequence co-occurrence to learn relations.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>symbolic (learned distributed lookup)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Next-item prediction / sequential recommendation (baseline comparisons), and shows limits in capturing semantic/prerequisite relations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ID-Only baselines (e.g., SASRec, BERT4Rec) perform worse than methods that incorporate textual or graph information; adding text/graph information yields better HR/NDCG than ID-only. This demonstrates that identity-only representations lack semantic/structural content necessary for curriculum-aware recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Performs worse than hybrid formats that include text or graph structure; combining with text and graph-adapted embeddings yields the best results. ID-only can be effective when patterns are heavily data-driven but fails on structure-aligned tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Cannot capture polysemy/semantic meaning or explicit prerequisite relations; relies on observed sequence statistics which may not reflect pedagogical dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Identity-only representations are insufficient for pedagogy-aligned recommendation; functional knowledge representation benefits from semantic and structural augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8703.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8703.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanilla Text Embeddings (BART)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla LLM Text Embeddings (BART [CLS] pooled representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Functional format that maps enhanced text into continuous vector space via a pre-trained language model (BART) using the [CLS] token hidden state as the concept embedding; provides semantic but anisotropic and non-smooth vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Vanilla LLM-based sentence/text embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Each concept's (enhanced) textual description is encoded with a pre-trained language model (BART) and the [CLS]-token final hidden vector is used as the concept's continuous representation; functionally intended to represent semantic content in a dense vector but suffers from anisotropy.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed (text-derived)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Used as input to adapters for next-concept recommendation; intrinsic embedding geometry analyzed by t-SNE and cluster separation (DBI).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vanilla text embeddings are anisotropic and poorly clustered for the downstream recommendation tasks, demonstrated by higher DBI (e.g., ASSIST09 DBI 2.0257) and weak t-SNE manifold structure. Without adaptation, they underperform compared to graph-adapted embeddings in recommendation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Inferior to graph-adapted embeddings and MoE-adapted embeddings for the concept recommendation task. The graph adapter substantially improves the embedding geometry and downstream performance relative to vanilla embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Anisotropy and non-smooth distribution make them suboptimal for direct use in recommendation tasks; require an adapter (GNN or MoE) to be effective. Evidence is empirical (embedding visualization, DBI, and recommendation metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Pre-trained LLM textual encodings provide rich semantics but must be transformed into a more isotropic and structure-aligned space to functionally support pedagogical recommendation and to reflect prerequisite relations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8703.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8703.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Learner Knowledge State (DKT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Knowledge Tracing (RNN/GRU-based knowledge-state representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional learner-side representation that encodes a student's evolving mastery over concepts as a time-varying vector produced by a recurrent sequence model (DKT with GRU here), used jointly with concept representations to recommend next concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep knowledge tracing.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>Sequence-derived learner knowledge state (DKT)</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Learner history (sequence of concept, correctness tuples) is encoded by a recurrent network (GRU) to produce a time-indexed vector representing predicted mastery levels for all concepts; functionally this serves as a personalized state used for conditional recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed (sequence-derived) / state-based</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Personalized next-concept recommendation that depends on individual mastery; ablation tests explore the impact of removing knowledge tracing on recommendation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Removing the knowledge tracing module significantly deteriorates recommendation performance (reported across datasets), indicating that representing the learner's dynamic knowledge state is crucial. The paper reports notable decreases when the KT module is ablated (plots in Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>Learner-state representations are complementary to concept-side representations (text, graph-adapted embeddings); both sides are necessary for best performance. Sequence-based pretraining also contributes meaningfully but cannot replace explicit KT state.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>DKT relies on quality and density of learner interaction data; it models mastery implicitly via prediction of correctness rather than explicit cognitive constructs. Evidence is empirical from recommendation performance; not tied to cognitive validation.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Functionally, an explicit, dynamically-updated learner knowledge state is indispensable for personalized concept recommendation and interacts synergistically with concept representations that encode semantic and structural information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8703.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8703.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of representational formats of conceptual knowledge at a functional (not neural) level, including descriptions of the format, supporting or challenging evidence, cognitive tasks or phenomena used as evidence, comparisons between formats, and theoretical claims or implications.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoE Adapter (Baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixture-of-Experts (MoE) Adapter for Text-to-Task Adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previous-adopted adapter design that transforms text embeddings for downstream tasks using an MoE of MLPs; used here as a baseline adaptation format to compare against the proposed graph-based adapter.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_name</strong></td>
                            <td>MoE-adapted distributed embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>representational_format_description</strong></td>
                            <td>Vanilla text vectors are passed through a mixture-of-experts (multiple MLP experts with gating) adapter to produce task-suitable embeddings; functionally intended to remap anisotropic text vectors into a space useful for recommendation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_type</strong></td>
                            <td>distributed (adapter-transformed)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_task_or_phenomenon</strong></td>
                            <td>Next-concept recommendation and embedding-space geometry (compared by DBI and t-SNE).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Replacing the graph-based adapter with MoE (SKarRec-MoE) causes measurable performance drops (e.g., ASSIST09 HR@10 0.7754 vs SKarRec 0.7922) and yields worse embedding clustering (DBI 1.6007 vs SKarRec 0.6611 for ASSIST09). This shows MoE adaptation is less effective than graph-contrastive GNN adaptation for capturing prerequisite structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_formats</strong></td>
                            <td>MoE is better than raw vanilla embeddings but worse than graph-adapted embeddings; lacks explicit structure-oriented pre-training objective so it fails to align embeddings with the knowledge graph as effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Structurally simple MoE adapters trained only end-to-end with the recommendation objective lack a tailored pre-training target and do not produce the same smoothing/structure alignment benefits as graph-contrastive pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_claims_or_implications</strong></td>
                            <td>Adapters that incorporate explicit structural objectives (e.g., graph contrastive learning) are functionally superior to generic MLP/MoE adapters for tasks that demand alignment with relational structures such as curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On the sentence embeddings from pre-trained language models <em>(Rating: 2)</em></li>
                <li>Semi-supervised classification with graph convolutional networks <em>(Rating: 2)</em></li>
                <li>Deep knowledge tracing <em>(Rating: 2)</em></li>
                <li>Towards universal sequence representation learning for recommender systems <em>(Rating: 2)</em></li>
                <li>Graph Contrastive Learning with Adaptive Augmentation for Knowledge Concept Recommendation <em>(Rating: 2)</em></li>
                <li>Text Is All You Need: Learning Language Representations for Sequential Recommendation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8703",
    "paper_id": "paper-269929882",
    "extraction_schema_id": "extraction-schema-156",
    "extracted_data": [
        {
            "name_short": "Enhanced Textual Representation",
            "name_full": "Structure-and-Knowledge-Aware Enhanced Textual Representation (LLM-enhanced key-value)",
            "brief_description": "A functional representational format that expresses each concept as a structured key-value text including: concept name, an LLM-generated context-resolved explanation conditioned on predecessor/successor concepts, a list of predecessor names, and a list of successor names; intended to capture both semantic meaning and immediate structural relations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representational_format_name": "LLM-enhanced structured textual representation",
            "representational_format_description": "Concepts are represented as short structured documents (key–value blocks) containing (1) a symbolic name token, (2) an explanatory natural-language description generated by an LLM (GPT-3.5) disambiguated using local graph context, and (3) explicit lists of predecessor and successor concept names from a knowledge graph; this format functionally encodes lexical/definitional content plus local relational context for downstream use.",
            "format_type": "hybrid (symbolic + textual/distributional)",
            "cognitive_task_or_phenomenon": "Next-concept recommendation (next-item prediction) and concept-disambiguation (ambiguity resolution when concept names are polysemous).",
            "key_findings": "Providing enhanced text that includes LLM explanation plus predecessor/successor context improved recommendation metrics relative to name-only inputs. Ablation: SKarRec-No-LLM (drop LLM explanation) shows a performance drop (e.g., ASSIST09 HR@10 0.7898 vs full 0.7922), and SKarRec-Name (name-only) drops more (ASSIST09 HR@10 0.7775). The paper reports consistent small-to-moderate improvements across datasets when enhanced text is used.",
            "comparison_with_other_formats": "Enhanced textual representation outperforms bare concept-name (symbolic) representations and improves downstream performance relative to using only graph neighbor names (No-LLM). It is complementary to graph-based embedding adaptation — the enhanced text is the input that the adapter transforms.",
            "limitations_or_counter_evidence": "LLM explanations can be ambiguous if not conditioned on graph context; their quality depends on the LLM (GPT-3.5 here) and prompt design. The format relies on access to a reasonably accurate predecessor/successor graph to disambiguate meanings. The evidence is task-based (recommendation metrics) rather than cognitive/psychological experiments.",
            "theoretical_claims_or_implications": "Combining natural-language, definitional content from LLMs with local graph structure yields richer concept representations that reduce ambiguity and better capture semantic relationships functionally relevant to learning-path recommendation and alignment with human knowledge structure.",
            "uuid": "e8703.0",
            "source_info": {
                "paper_title": "Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Knowledge Graph (Prerequisite Graph)",
            "name_full": "Directed Knowledge Graph of Concepts (prerequisite/succession relations)",
            "brief_description": "A symbolic relational format that encodes concepts as nodes in a directed graph where edges represent prerequisite (precedence) relationships; used to capture structural dependencies between concepts.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "Symbolic directed knowledge graph (prerequisite network)",
            "representational_format_description": "Concepts are represented as discrete nodes connected by directed edges that indicate prerequisite or successor relations (i.e., ordering constraints or learning dependencies). Functionally this format encodes adjacency and path relations used to constrain and inform recommendations.",
            "format_type": "symbolic / relational (graph-based)",
            "cognitive_task_or_phenomenon": "Consistency with knowledge structure in concept recommendation (proportion of recommended concepts that are predecessors or successors of the previous concept); sequencing / curriculum consistency.",
            "key_findings": "Using the knowledge graph to (1) provide predecessor/successor context to LLM prompts improved concept disambiguation; (2) serve as the substrate for graph-based contrastive pre-training improved downstream recommendation consistency with the graph (SKarRec more frequently suggests concepts that are predecessors/successors of the previous concept). The paper reports higher 'consistency ratios' for SKarRec versus baselines, especially for learners whose sequences align with the graph.",
            "comparison_with_other_formats": "Graph-based structure is contrasted with purely text-based and ID-only representations: text-only or ID-only methods fail to capture prerequisite relations. Graph-based representations (used directly or via adapter) lead to recommendations that better respect curriculum structure.",
            "limitations_or_counter_evidence": "Quality depends on availability and quality of the graph; when only transition graphs (estimated from sequences) are available they are only approximations and may be noisy. Graph-based methods relying on heterogeneous data decline in performance when such data are not present. Also, a pure symbolic graph lacks definitional semantic content (hence the hybrid approach).",
            "theoretical_claims_or_implications": "Explicit structural encoding of prerequisite relations is essential for educational recommendation tasks; combining symbolic graph structure with semantic descriptions supports functionally better alignment to human pedagogical ordering.",
            "uuid": "e8703.1",
            "source_info": {
                "paper_title": "Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Graph-adapted Knowledge-space Embeddings",
            "name_full": "Graph-based Adapter (GNN) with Contrastive Pre-training to produce knowledge-space embeddings",
            "brief_description": "A functional distributed representation format: initial text-derived vectors are transformed via a graph neural network trained with graph-contrastive objectives to produce smooth, structure-aware embeddings that align with the knowledge graph.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representational_format_name": "Graph-adapted distributed knowledge-space embedding",
            "representational_format_description": "Concepts are represented as continuous vectors produced by (1) encoding enhanced text with a language model to get initial vectors, then (2) applying a GNN-based adapter which aggregates neighbor information and is pre-trained with contrastive learning on graph edge-dropout views (InfoNCE) to produce smooth embeddings reflecting graph structure; functionally these embeddings capture both semantic content and relational proximity.",
            "format_type": "distributed (graph-regularized) / hybrid",
            "cognitive_task_or_phenomenon": "Next-concept recommendation accuracy, embedding clustering quality (assessed by t-SNE and Davies–Bouldin Index), and consistency of recommendations with prerequisite graph (graph-consistency ratio).",
            "key_findings": "Graph-adapted embeddings (SKarRec) substantially improve recommendation metrics and embedding cluster separation relative to vanilla text embeddings and MoE-adapted embeddings. Example quantitative evidence: Davies–Bouldin Index (ASSIST09) — vanilla text 2.0257, MoE-adapted 1.6007, SKarRec 0.6611 (lower is better). Ablation: removing contrastive graph pre-training (SKarRec-No-GraphSSL) or replacing the adapter by MoE (SKarRec-MoE) led to drops in HR/NDCG/MRR (e.g., ASSIST09 HR@10: 0.7727 and 0.7754 vs 0.7922 full).",
            "comparison_with_other_formats": "Graph-adapted embeddings outperform (a) vanilla LLM/BART text embeddings (which are anisotropic and non-smooth), and (b) MoE-based adapters used in previous recommendation work — both by downstream recommendation metrics and intrinsic clustering (DBI) analyses.",
            "limitations_or_counter_evidence": "Adapter performance depends on graph quality and the hyperparameters of contrastive augmentation (edge masking ratio γ affects performance). The representation is still learned via neural models — the paper provides functional/task-based evidence but no behavioral cognitive validation. Also, contrastive objectives require careful negative sampling and may be sensitive to graph sparsity.",
            "theoretical_claims_or_implications": "Transforming textual semantic vectors into a knowledge-aligned continuous space via graph-aware contrastive learning produces smoother manifolds and embeddings whose proximities reflect pedagogical/prerequisite similarity, which functionally improves generalization for next-concept prediction and alignment with human curricular structure.",
            "uuid": "e8703.2",
            "source_info": {
                "paper_title": "Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ID-based Symbolic Representation",
            "name_full": "Concept ID Embedding (symbolic index-based representation)",
            "brief_description": "A minimal symbolic format where each concept is represented by a learned ID embedding vector (lookup) or by its discrete name token alone; captures only identity, not intrinsic semantic content or explicit relations.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "Symbolic ID-based representation",
            "representational_format_description": "Each concept is represented by a unique identifier mapped to a learned embedding vector (an index in an embedding matrix) or simply by its name token; functionally this encodes only item identity and must rely on sequence co-occurrence to learn relations.",
            "format_type": "symbolic (learned distributed lookup)",
            "cognitive_task_or_phenomenon": "Next-item prediction / sequential recommendation (baseline comparisons), and shows limits in capturing semantic/prerequisite relations.",
            "key_findings": "ID-Only baselines (e.g., SASRec, BERT4Rec) perform worse than methods that incorporate textual or graph information; adding text/graph information yields better HR/NDCG than ID-only. This demonstrates that identity-only representations lack semantic/structural content necessary for curriculum-aware recommendation.",
            "comparison_with_other_formats": "Performs worse than hybrid formats that include text or graph structure; combining with text and graph-adapted embeddings yields the best results. ID-only can be effective when patterns are heavily data-driven but fails on structure-aligned tasks.",
            "limitations_or_counter_evidence": "Cannot capture polysemy/semantic meaning or explicit prerequisite relations; relies on observed sequence statistics which may not reflect pedagogical dependencies.",
            "theoretical_claims_or_implications": "Identity-only representations are insufficient for pedagogy-aligned recommendation; functional knowledge representation benefits from semantic and structural augmentation.",
            "uuid": "e8703.3",
            "source_info": {
                "paper_title": "Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Vanilla Text Embeddings (BART)",
            "name_full": "Vanilla LLM Text Embeddings (BART [CLS] pooled representation)",
            "brief_description": "Functional format that maps enhanced text into continuous vector space via a pre-trained language model (BART) using the [CLS] token hidden state as the concept embedding; provides semantic but anisotropic and non-smooth vectors.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "Vanilla LLM-based sentence/text embeddings",
            "representational_format_description": "Each concept's (enhanced) textual description is encoded with a pre-trained language model (BART) and the [CLS]-token final hidden vector is used as the concept's continuous representation; functionally intended to represent semantic content in a dense vector but suffers from anisotropy.",
            "format_type": "distributed (text-derived)",
            "cognitive_task_or_phenomenon": "Used as input to adapters for next-concept recommendation; intrinsic embedding geometry analyzed by t-SNE and cluster separation (DBI).",
            "key_findings": "Vanilla text embeddings are anisotropic and poorly clustered for the downstream recommendation tasks, demonstrated by higher DBI (e.g., ASSIST09 DBI 2.0257) and weak t-SNE manifold structure. Without adaptation, they underperform compared to graph-adapted embeddings in recommendation metrics.",
            "comparison_with_other_formats": "Inferior to graph-adapted embeddings and MoE-adapted embeddings for the concept recommendation task. The graph adapter substantially improves the embedding geometry and downstream performance relative to vanilla embeddings.",
            "limitations_or_counter_evidence": "Anisotropy and non-smooth distribution make them suboptimal for direct use in recommendation tasks; require an adapter (GNN or MoE) to be effective. Evidence is empirical (embedding visualization, DBI, and recommendation metrics).",
            "theoretical_claims_or_implications": "Pre-trained LLM textual encodings provide rich semantics but must be transformed into a more isotropic and structure-aligned space to functionally support pedagogical recommendation and to reflect prerequisite relations.",
            "uuid": "e8703.4",
            "source_info": {
                "paper_title": "Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Learner Knowledge State (DKT)",
            "name_full": "Deep Knowledge Tracing (RNN/GRU-based knowledge-state representation)",
            "brief_description": "A functional learner-side representation that encodes a student's evolving mastery over concepts as a time-varying vector produced by a recurrent sequence model (DKT with GRU here), used jointly with concept representations to recommend next concepts.",
            "citation_title": "Deep knowledge tracing.",
            "mention_or_use": "use",
            "representational_format_name": "Sequence-derived learner knowledge state (DKT)",
            "representational_format_description": "Learner history (sequence of concept, correctness tuples) is encoded by a recurrent network (GRU) to produce a time-indexed vector representing predicted mastery levels for all concepts; functionally this serves as a personalized state used for conditional recommendation.",
            "format_type": "distributed (sequence-derived) / state-based",
            "cognitive_task_or_phenomenon": "Personalized next-concept recommendation that depends on individual mastery; ablation tests explore the impact of removing knowledge tracing on recommendation performance.",
            "key_findings": "Removing the knowledge tracing module significantly deteriorates recommendation performance (reported across datasets), indicating that representing the learner's dynamic knowledge state is crucial. The paper reports notable decreases when the KT module is ablated (plots in Figure 3).",
            "comparison_with_other_formats": "Learner-state representations are complementary to concept-side representations (text, graph-adapted embeddings); both sides are necessary for best performance. Sequence-based pretraining also contributes meaningfully but cannot replace explicit KT state.",
            "limitations_or_counter_evidence": "DKT relies on quality and density of learner interaction data; it models mastery implicitly via prediction of correctness rather than explicit cognitive constructs. Evidence is empirical from recommendation performance; not tied to cognitive validation.",
            "theoretical_claims_or_implications": "Functionally, an explicit, dynamically-updated learner knowledge state is indispensable for personalized concept recommendation and interacts synergistically with concept representations that encode semantic and structural information.",
            "uuid": "e8703.5",
            "source_info": {
                "paper_title": "Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "MoE Adapter (Baseline)",
            "name_full": "Mixture-of-Experts (MoE) Adapter for Text-to-Task Adaptation",
            "brief_description": "A previous-adopted adapter design that transforms text embeddings for downstream tasks using an MoE of MLPs; used here as a baseline adaptation format to compare against the proposed graph-based adapter.",
            "citation_title": "",
            "mention_or_use": "use",
            "representational_format_name": "MoE-adapted distributed embeddings",
            "representational_format_description": "Vanilla text vectors are passed through a mixture-of-experts (multiple MLP experts with gating) adapter to produce task-suitable embeddings; functionally intended to remap anisotropic text vectors into a space useful for recommendation.",
            "format_type": "distributed (adapter-transformed)",
            "cognitive_task_or_phenomenon": "Next-concept recommendation and embedding-space geometry (compared by DBI and t-SNE).",
            "key_findings": "Replacing the graph-based adapter with MoE (SKarRec-MoE) causes measurable performance drops (e.g., ASSIST09 HR@10 0.7754 vs SKarRec 0.7922) and yields worse embedding clustering (DBI 1.6007 vs SKarRec 0.6611 for ASSIST09). This shows MoE adaptation is less effective than graph-contrastive GNN adaptation for capturing prerequisite structure.",
            "comparison_with_other_formats": "MoE is better than raw vanilla embeddings but worse than graph-adapted embeddings; lacks explicit structure-oriented pre-training objective so it fails to align embeddings with the knowledge graph as effectively.",
            "limitations_or_counter_evidence": "Structurally simple MoE adapters trained only end-to-end with the recommendation objective lack a tailored pre-training target and do not produce the same smoothing/structure alignment benefits as graph-contrastive pre-training.",
            "theoretical_claims_or_implications": "Adapters that incorporate explicit structural objectives (e.g., graph contrastive learning) are functionally superior to generic MLP/MoE adapters for tasks that demand alignment with relational structures such as curricula.",
            "uuid": "e8703.6",
            "source_info": {
                "paper_title": "Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On the sentence embeddings from pre-trained language models",
            "rating": 2,
            "sanitized_title": "on_the_sentence_embeddings_from_pretrained_language_models"
        },
        {
            "paper_title": "Semi-supervised classification with graph convolutional networks",
            "rating": 2,
            "sanitized_title": "semisupervised_classification_with_graph_convolutional_networks"
        },
        {
            "paper_title": "Deep knowledge tracing",
            "rating": 2,
            "sanitized_title": "deep_knowledge_tracing"
        },
        {
            "paper_title": "Towards universal sequence representation learning for recommender systems",
            "rating": 2,
            "sanitized_title": "towards_universal_sequence_representation_learning_for_recommender_systems"
        },
        {
            "paper_title": "Graph Contrastive Learning with Adaptive Augmentation for Knowledge Concept Recommendation",
            "rating": 2,
            "sanitized_title": "graph_contrastive_learning_with_adaptive_augmentation_for_knowledge_concept_recommendation"
        },
        {
            "paper_title": "Text Is All You Need: Learning Language Representations for Sequential Recommendation",
            "rating": 1,
            "sanitized_title": "text_is_all_you_need_learning_language_representations_for_sequential_recommendation"
        }
    ],
    "cost": 0.01491825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation
21 May 2024</p>
<p>Qingyao Li 
Yong Yu </p>
<p>Shanghai Jiao Tong University Shanghai
China</p>
<p>Wei Xia</p>
<p>Huawei Noah's Ark Lab Shenzhen
China Kounianhua Du</p>
<p>Shanghai Jiao Tong University Shanghai
China</p>
<p>Qiji Zhang</p>
<p>Shanghai Jiao Tong University Shanghai
China</p>
<p>Weinan Zhang</p>
<p>Shanghai Jiao Tong University Shanghai
China Ruiming Tang</p>
<p>Huawei Noah's Ark Lab Shenzhen
China</p>
<p>Shanghai Jiao Tong University Shanghai
China</p>
<p>Learning Structure and Knowledge Aware Representation with Large Language Models for Concept Recommendation
21 May 20240451E9D7E6645F33741283E00EEA72A0arXiv:2405.12442v1[cs.IR]
Concept recommendation aims to suggest the next concept for learners to study based on their knowledge states and the human knowledge system.While knowledge states can be predicted using knowledge tracing models, previous approaches have not effectively integrated the human knowledge system into the process of designing these educational models.In the era of rapidly evolving Large Language Models (LLMs), many fields have begun using LLMs to generate and encode text, introducing external knowledge.However, integrating LLMs into concept recommendation presents two urgent challenges: 1) How to construct text for concepts that effectively incorporate the human knowledge system? 2) How to adapt non-smooth, anisotropic text encodings effectively for concept recommendation?In this paper, we propose a novel Structure and Knowledge Aware Representation learning framework for concept Recommendation (SKarREC).We leverage factual knowledge from LLMs as well as the precedence and succession relationships between concepts obtained from the knowledge graph to construct textual representations of concepts.Furthermore, we propose a graph-based adapter to adapt anisotropic text embeddings to the concept recommendation task.This adapter is pre-trained through contrastive learning on the knowledge graph to get a smooth and structure-aware concept representation.Then, it's fine-tuned through the recommendation task, forming a text-toknowledge-to-recommendation adaptation pipeline, which effectively constructs a structure and knowledge-aware concept representation.Our method does a better job than previous adapters in transforming text encodings for application in concept recommendation.Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed approach.</p>
<p>INTRODUCTION</p>
<p>Online education is increasingly becoming a vital mean for individuals to acquire knowledge and skills.To serve learners' knowledge acquisition, online education platforms recommend appropriate learning content to them.This content can vary in scope, including both broad course recommendations and detailed exercise suggestions [18,19].This article focuses on knowledge concept recommendations, which align with the users' needs to acquire specific skills or understand certain concepts when they come to the platform.</p>
<p>To enhance learners' mastery levels, the strategy for planning their concept learning paths should be based on two key types of information: 1) The learner's knowledge state (learner side), which refers to their proficiency in the concepts -what they have mastered well and what they haven't.The prediction of a learner's knowledge state can be achieved through knowledge tracing models, and many related models have been proposed in this area [21,22,29].</p>
<p>2) The human knowledge system (concept side), which is the semantic meanings and structural relationships between concepts.Human teachers often make educational plans based on a good understanding of each concept.For instance, "multiplication" logically follows "addition" in learning progression due to its conceptual dependency on "addition".Many previous approaches have introduced relationships between concepts through knowledge graphs, yet these methods overlook the intrinsic semantics of the concepts.Consequently, while they capture how learners interact with concepts and the dependencies between them, they fail to integrate concrete, factual knowledge about the concepts, leaving potential deeper semantic connections between concepts unexplored.</p>
<p>Nowadays, the development of Large Language Models (LLMs) provides a way of introducing the semantic meanings of the concepts.In product recommendation, many studies have utilized LLMs to generate and encode textual information to assist in the representations of items for recommendation purposes.However, introducing the knowledge from LLMs to represent concepts faces two challenges: C1) How can concepts be represented in a way that includes information from the human knowledge system?A direct way is to use the text of the concept's name, but some concepts may have ambiguous definitions in different learning contexts.For instance, "Table" can mean a piece of furniture in everyday life but also refers to a tabular data representation in data science.Hence, constructing appropriate textual information to represent a concept LLM In the context of "Histogram as Table or Graph" and considering its predecessors and successors, it likely pertains to presenting data in tabular or graphical form for visualizing and analyzing information.</p>
<dl>
<dt>Human Knowledge System</dt>
<dd>
<p>What concept should i learn next?</p>
</dd>
</dl>
<p>Graph-based Adaptation Contrastive Learning</p>
<p>The next concept to learn is "multiplication".The concept recommendation strategy of combining the learner's knowledge state and the human knowledge system.The knowledge state is estimated by knowledge tracing, and the human knowledge system is modeled by graphadapted encoding of the enhanced text of each concept. is a key issue.C2) How to adapt the language model's encodings of the text to the concept recommendation problem?Research on language models indicates that text encodings produced by them have non-smooth and anisotropic distributions [13], meaning that sentences similar in the semantic way are not similar in the embedding space, which are not suitable for direct use in downstream tasks.Therefore, previous works often establish an adapter to transfer the text encodings to suit the recommendation task.However, these adapters typically have two drawbacks: i) They are structurally simple, often built with one or combining multiple MLPs in a mixture of experts (MoE) way, and ii) They lack a training target design.They are often trained end-to-end alongside the final recommendation task, lacking a tailed training objective that guarantees the effectiveness of the text adaptation.Therefore, using LLMs' knowledge to introduce the human knowledge system in concept recommendation requires constructing concept texts that include the semantic meanings as well as relationships between concepts and building a text adapter that transforms text representations into a space that better assists concept recommendation.</p>
<p>In light of the challenges, we propose Structure and Knowledge Aware Representation learning framework for concept Recommendation (SKarREC).As shown in Figure 1, we utilize the knowledge from the concept side (external factual knowledge provided by LLMs and relationships between concepts provided by the knowledge graph) and the learner side (the knowledge state predicted by knowledge tracing) to assist sequence models in recommending concepts.Specifically, for expressive concepts interpretation (C1), we develop a structure and knowledge-aware method that uses LLMs to generate concept explanation with ambiguity resolution.Combining the relationship between concepts, we construct the text description of each concept, which is fed into language models' encoders for text encoding.To effectively adapt text encodings tailed for concept recommendation (C2), we introduce a graph-based adapter.Specifically, the adapter is based on a graph neural network (GNN) and trained by contrastive learning in a self-supervised manner, transferring the representation from text space to the representation space that aligns with the knowledge structure (referred to as knowledge space).Thus, the final representations encompass the human knowledge system from LLMs as well as the precedence and succession relationships (structure information) of concepts in the knowledge graph.Finally, the recommendation is made based on the learner's knowledge state and the human knowledge system information contained in the concepts' representations.</p>
<p>Our contributions are summarized as follow:</p>
<p>•</p>
<p>RELATED WORK 2.1 Concept Recommendation</p>
<p>In concept recommendation, prior research has predominantly revolved around two distinct problem formulations.The first formulation is grounded in the construction of simulators, which primarily employ simulators to emulate the progression of a learner's knowledge and utilize the simulator to evaluate the recommendation performance.Owing to the interactive nature of the simulator, reinforcement learning(RL) techniques have demonstrated remarkable effectiveness in this context, like CSEAL [20], GEHRL [16], etc.Such methods usually take the learner's promotion from the simulator as the reward to train RL models, which naturally beat other non-RL methods.</p>
<p>Conversely, the focus of this paper diverges toward the second problem formulation, which concerns next-item prediction.In this setting, the primary objective of the model is to accurately align with the existing learning paths of learners as reflected in the dataset, which takes the learners' choice as the best choice instead of the simulator's prediction.In this setting, the graph-based methods mainly take advantage of using the structured information between learners and concepts.For example, ACKRec [8] leverages both content information and context information to learn the representation of entities via a graph convolution network.The drawback of this kind of method is that they are mostly built on heterogeneous graphs.They demand high-quality data.This is why such methods are typically tested only on comprehensive datasets like MOOCCube [30], where all necessary information is readily available.However, many platforms do not naturally possess such heterogeneous graphs, which significantly limits the applicability of these methods in various contexts.This paper explores the next concept prediction form, delving into the intricacies of aligning educational recommendations with individual student interests and choices, thereby contributing to a more personalized and effective learning experience.</p>
<p>LLM-enhanced Recommendation</p>
<p>With the development of LLMs, more and more works have been proposed by using LLMs for recommendation [17].LLMs have two main advantages in recommendation: 1) Interpret text information.That being said, the text information in the recommendation scenario that could not be fully used before could be utilized now.</p>
<p>2)Introducing open-world knowledge.LLMs are trained on a massive corpus that contains almost all the human knowledge, which makes it suitable to be used as an external knowledge base for recommendation tasks.</p>
<p>By taking the two advantages, LLMs are used in different ways.One way is to use the open-world knowledge and reasoning ability of the LLMs.That being said, this kind of method is mainly based on prompt engineering to make the LLMs generate the recommendation-related text information they want.For example, P5 [7] presents a text-to-text paradigm for recommendation.It converts all data into natural language sequences and trains the language model with a language modeling task.In this approach, the recommendation is taken as a natural language processing problem and depends more on the LLMs' inherent reasoning ability learned from the pre-training corpus.The other kind is to use it to generate dense vectors as some sort of representation or feature.For example, UniSRec [9] encodes each item using a language model and develops a mixture-of-experts (MoE) adaptor that transforms the text semantic into a universal form suited to the cross-domain recommendation tasks.RECFORMER [14], on the other hand, re-train Longformer [2] model on the recommendation datasets with maskitem-prediction and mask-token-prediction tasks, which develops a recommender system that is purely based on text information.Despite all the success these methods get, they paid little attention to the adapter that adapts the text representation to the recommendation task, which could cause the text information not to be fully utilized.</p>
<p>PRELIMINARIES</p>
<p>In this section, we formulate the task of concept recommendation and the knowledge graph.</p>
<p>Concept Recommendation Task.In the concept recommendation scenario, we have a set of learners, denoted by U, a set of concepts denoted by K.At time step , we recommend a concept  ∈ K based on the learner's learning history H  .The history is constructed as a sequence of tuples
H 𝑡 = ((𝑘 1 , 𝑐 1 ), (𝑘 2 , 𝑐 2 )...(𝑘 𝑡 , 𝑐 𝑡 )),
where   is the concept learned at time step ;   ∈ {0, 1} representing the feedback of the learner (like the answer correctness of the concept-related question).Our goal is to recommend   +1 based on H  .</p>
<p>Each concept  ∈ K corresponds to a text corpus   , like "addition" or "venn diagram" that could be used as auxiliary information for the recommendation.</p>
<p>Knowledge Graph.The concepts of a certain area could be formulated as a graph representing the prerequisite relationships between them.The knowledge graph is a directed graph  = {K, E}, where each node  is a concept, and each edge  from  1 to  2 represents that concept  1 is the prerequisite concept of  2 .However, since such graphs are often not readily available in many situations or datasets, we construct transition graphs [23] to approximate these knowledge graphs.In these transition graphs, an edge  from  1 to  2 is drawn if, typically,  2 is learned after  1 according to the sequences observed in learners' activities.If the graph is provided, then we use it directly.</p>
<p>METHODOLOGY 4.1 Overview</p>
<p>We have developed a concept recommendation framework that integrates the knowledge states of learners and the human knowledge system, as shown in Figure 2. To incorporate the human knowledge system using LLMs and knowledge graphs and to adapt this information into text for the recommendation process, we introduced three modules:</p>
<p>Structure and Knowledge-aware Concept Interpretation.The human knowledge system is based on the definitions and relationships of concepts.To incorporate them into the recommendation process, we enhance the textual descriptions of concepts through LLMs and the knowledge graph.By using the knowledge graph, we enable the LLM to generate unambiguous semantic interpretations of each concept.Subsequently, we present the name, explanation, and preceding and succeeding nodes of the concepts as the concept's text description for the subsequent recommendation.</p>
<p>Graph-based Text Adaptation.To improve how we transform text into recommendations, we develop a graph-based adapter.This adapter adopts contrastive learning on the graph to optimize the adaptation process, which emphasizes the structural connections between concepts, enhancing the effectiveness of the final recommendation task.</p>
<p>Learner Knowledge State Representation.In addition to modeling the concept side by incorporating the human knowledge system, we have introduced the results of knowledge tracing as auxiliary information on the learner side.This approach ensures that the concept recommendation not only considers the relevance between concepts but also bases recommendations on the current knowledge state of the learner.</p>
<p>Next, we go through the details of the framework.</p>
<p>Structure and Knowledge-aware Concept Interpretation</p>
<p>The key feature of the concept recommendation problem is that the learner's choice is largely based on the human knowledge system, which is constructed by the semantic meanings or definitions and the relationships between concepts.Human teachers often make educational plans based on a good understanding of each concept.Therefore, incorporating the semantics of each concept is crucial for developing a concept recommendation system.Directly using the concept name or letting a dialogue model, like GPT-3.5 [1], to generate explanations for each concept would cause the ambiguity problem.That is, a concept can have different meanings based on the field of study or the context.For instance, "Table" could refer to a data structure in data science or to a piece of furniture in everyday conversation.If the LLM is asked to explain  "Table ", it might incorrectly interpret it as a piece of furniture.To eliminate such ambiguities, we refine our prompts to the LLM by including the concept's predecessor and successor nodes of the concept from the knowledge graph.This method helps the LLM to deliver an explanation tailored to the specific educational context, greatly diminishing the risk of ambiguity.</p>
<p>To ensure that the text of a concept contains sufficient information, we have designed the textual representation of each concept to include the following four parts:</p>
<p>• Concept Name: The name of the concept   .• Concept Explanation: We utilize GPT-3.5 to infer the explanation of the concept based on its predecessors and successors on the knowledge graph.• Predecessors: The names of the prerequisite concepts of this concept in the knowledge graph.• Successors: The names of the successor concepts of this concept in the knowledge graph.</p>
<p>Each item is represented in the key-value text containing the four parts above.We denote the enhanced text as w .</p>
<p>Graph-based Text Adaptation</p>
<p>Due to the anisotropy nature of vanilla text embeddings [13], recommendation tasks often require an adapter to transfer the text embeddings to suit the task [9].Previously, this transfer process was trivial in two aspects: i) The construction of the adapter is often one or several Multi-Layer Perceptron (MLP) units.ii) The training objective for the adapter is not specifically tailored but rather is trained in conjunction with the recommendation task, which didn't show much adaptation effect.</p>
<p>In educational contexts, textual information is closely linked with the human knowledge system.Our idea is to adapt educational texts based on the structure of human knowledge rather than merely using a simple MLP and training it alongside recommendation tasks.In light of this, we propose a graph-based adapter that is built upon a GNN and trained by contrastive learning on the graph.</p>
<p>Concept Text Encoding.</p>
<p>After gaining the enhanced text of each concept w , we encode the text utilizing the pre-trained language model BART [12] due to its straightforward architecture and considerable text comprehension capabilities.Given a concept  and its corresponding enhanced text w = { w 1 , w 2 , ... w  }.We add a special token [CLS] at the beginning and use the language model to encode.
𝑇 𝑘 = 𝐿𝑀 ([[𝐶𝐿𝑆], w𝑘 1 , w𝑘 2 , ... w𝑘 𝑐 ])(1)
where   is the final hidden vector corresponding to token [CLS] as the encoding of the enhanced text of concept ;  is the length of the word sequence w .</p>
<p>Graph-based Adapter.</p>
<p>To get a smooth and structure-aware concept representation, we propose to learn from the knowledge graph  = {K, E}.Specifically, we first set the initial encoding of each node as the text encoding of the corresponding concept.
ℎ 0 𝑘 = 𝑇 𝑘(2)
Then we utilize Graph Convolutional Network (GCN) [11] to learn the embedding of each concept for its simplicity and effectiveness.
ℎ 𝑙 𝑘 = 𝐶𝑜𝑚𝑏𝑖𝑛𝑒 (𝐻 (ℎ 𝑙 −1 𝑘 ), 𝐴𝑔𝑔(𝐻 (ℎ 𝑙 −1 𝑖 )|𝑖 ∈ N 𝑘 ))(3)
where ℎ   denotes the node representation of concept  at the l-th layer; ℎ  −1  is that of previous layer;  (•) is a non-linear transformation function (usually implemented by MLP); (•) is the aggregation function of the neighbors' embeddings of node ;  (•) denotes the combination function of the previous layer's representations of node  and the aggregation result.</p>
<p>Using a GNN as the adapter for concept recommendation offers two key benefits: 1) In terms of the distribution of embeddings, GNN's learning mechanism can result in smoother embeddings [15], potentially resolving the problem of anisotropy in text embedding distributions.2) Regarding the concept recommendation task, GNN's ability to aggregate information allows each concept's embedding to incorporate relationships between concepts, leading to recommendations that are more aligned with the underlying prerequisite relationships between concepts.</p>
<p>Graph-based</p>
<p>Learning.The aim of utilizing a knowledge graph is to enhance the suitability of vanilla text embeddings for concept recommendation tasks.We achieve this by transforming the embedding of concepts from the text representation space to a representation space that aligns with the knowledge structure (referred to as knowledge space).This is done through learning on the graph.To accomplish this, we employ the contrastive learning method to train the graph adapter.It trains the adapter to learn stable node representations despite perturbations in the graph structure, thereby capturing the graph's deep structural information.</p>
<p>Specifically, we first gain different views of the knowledge graph by edge dropout [28]:
𝑣 1 (𝐺) = (K, M 1 ⊙ E), 𝑣 2 (𝐺) = (K, M 2 ⊙ E)(4)
where M 1 , M 2 ∈ {0, 1} | E | are two masking vectors on the edge set.</p>
<p>A hyper-parameter  is set to be the masking ratio.By randomly masking the graph edges by a certain ratio, we get two sets of edges corresponding to two views of the knowledge graph.We treat the identical nodes across different views as positive pairs and different nodes from these views as negative pairs.We then train the adapter through InfoNCE loss [24]:
L 𝐺 𝑠𝑠𝑙 = − ∑︁ 𝑘 ∈ K 𝑙𝑜𝑔 𝑒 𝑠𝑖𝑚 (ℎ (1) 𝑘 ,ℎ(2)𝑘 )/𝜏 𝑖 ∈ K,𝑖≠𝑘 𝑒 𝑠𝑖𝑚 (ℎ (1) 𝑘 ,ℎ(2)
 )/ (5) where (•) is the similarity measurement function, which is set as the cosine similarity; ℎ (1) and ℎ (2) are two representation of the nodes from two different views;  is a temperature parameter.The goal of introducing contrastive learning on the knowledge graph is to enhance the vanilla text embeddings and transfer them into a revamped embedding space.Within this space, the representation of each concept is enriched with relational characteristics from the knowledge graph, ensuring that nodes structurally akin to each other also exhibit similarity in the embedding space.</p>
<p>Learner Knowledge State Representation</p>
<p>Personalized concept recommendation should not only align with the human knowledge system but also be based on the individual learner's knowledge state.Therefore, we employ the widely used deep knowledge tracing (DKT) model [25] to track the learner's knowledge state.The learning history H  is encoded by a recurrent neural network (RNN) to predict the learner's mastery level of all concepts.We choose the gated recurrent unit (GRU) [3] network as the sequence encoder due to its simplicity and effectiveness.
𝑠 𝑡 = 𝐺𝑅𝑈 (H 𝑡 )(6)
The training objective of DKT is the prediction of the correctness   +1 of the next concept   +1 :
L 𝐾𝑇 = (𝑠 𝑡 (𝑘 𝑡 +1 ) − 𝑐 𝑡 +1 ) 2(7)
The knowledge state of the learner is consistently changing, so we predict the knowledge state representation at each time step and concatenate it to the input of the recommendation model.</p>
<p>Self-Attention Based Recommendation</p>
<p>Having the encoding of the graph-adapted textual representation of the concept ℎ   and the knowledge state   , we further add two embedding layers to map the IDs of concepts and the answers.Specifically, we have a concept embedding matrix   ∈ R | K | × and an answer embedding matrix   ∈ R 2× representing answering right or wrong.Each time step's record (  ,   ) is encoded by four parts of embeddings:
𝑥 𝑡 = 𝑖 𝑘 𝑡 ⊕ 𝑎 𝑐 𝑡 ⊕ ℎ 𝑘 𝑡 ⊕ 𝑠 𝑡(8)
where   is the final representation;    ∈   is the id embedding of   ;    ∈   is the answer embedding of   ; ℎ   is the graph-adapted encoding of the concept;   is the knowledge state representation.</p>
<p>Given a sequence of the encoding of the learning record ( 1 ,  2 ,  3 ...  ) , we further utilize the widely used Transformer structure [27] to encode the sequence and
[𝐹 1 , 𝐹 2 , ...𝐹 𝑡 ] = 𝐹 𝐹 𝑁 (𝐴𝑡𝑡𝑒𝑛([𝑥 1 , 𝑥 2 , ...𝑥 𝑡 ]))(9)
where    (•) is the feed forward network;   ∈ R  is the output encoding at time step ; (•) is the Multi-head attention algorithm.</p>
<p>We calculate the scores for all concepts:
𝑃 (•|H 𝑡 ) = 𝑊 𝑇 𝐹 𝑡(10)
where   ∈ R | K | × is a learnable parameter matrix.</p>
<p>Overall Training Procedure</p>
<p>Here we present our pre-training and fine-tuning procedure, the detailed algorithm could be found in the Appendix A Algorithm 1.For the sequence-based pre-training, various training strategies have been suggested, all revolving around the principle of partially hiding sequence information and then predicting it through sequence modeling.Here, we adopt the same scheme as S3Rec [33] since it covers most of the sequence-based self-supervised learning objectives: mask item prediction (L   ), mask segment prediction (L  ), mask attribute prediction (L  ), and associated attribute prediction (L  ).The final sequence-based pre-training objective is:  task.We adopt the cross-entropy loss:
L 𝑠𝑒𝑞 𝑠𝑠𝑙 = L 𝑀𝐼 𝑃 + L 𝑀𝑆𝑃 + L 𝑀𝐴𝑃 + L 𝐴𝐴𝑃(L 𝑟𝑒𝑐 = − 1 𝑁 𝑁 ∑︁ 𝑛=0 | K | ∑︁ 𝑗=0 𝑦 𝑛 𝑗 𝑙𝑜𝑔(𝑃 𝑛 𝑗 ) (12)
where  is the training sample number;    is a binary indicator of whether concept  is the actual next one of sample ;    is the predicted probability of the concept  of sample .</p>
<p>EXPERIMENTS</p>
<p>5.1 Experimental Settings.</p>
<p>5.1.1Datasets.We conducted experiments through three datasets: Junyi 1 , ASSIT12 2 and ASSIT09 3 .They all provide learning sequences of learners.These datasets are transformed from their original question-answer pair sequences into concept-answer pairs.Each concept has an ID and a name.We provide the statistics of these three datasets in Table 1.</p>
<p>For the knowledge graph, Junyi dataset offers a dedicated knowledge graph that outlines concept dependencies.While for ASSIST09 and ASSIST12, we utilize transition graphs [23] as an estimation of knowledge graphs.</p>
<p>Baselines.</p>
<p>We are comparing four different sets of baseline methods.These include methods that use only concept IDs, methods that utilize item IDs and treat the text description of items as additional information, methods that rely solely on the text of the concepts as input, and methods based on heterogeneous graphs proposed for concept recommendation.</p>
<p>(1) ID-Only methods:</p>
<p>• SASRec [10]: utilizes a self-attention model to predict the next item.• BERT4Rec [26]: utilizes the original Bert [5] scheme for next item prediction.</p>
<p>(2) ID-Text methods:</p>
<p>• FDSA [32]: utilizes self-attention to capture item and feature transition patterns.• S3-Rec [33]: pre-trains self-attention models with mutual information maximization objectives related to attributes, items, sub-sequences, etc. • UniSRec(Transductive) [9]: uses an MoE-based adapter to transit the information from text to recommendation area.We choose to compare with the model fine-tuned in transductive 1 https://www.kaggle.com/datasets/junyiacademy/learning-activity-public-datasetby-junyi-academy 2 https://sites.google.com/site/assistmentsdata/datasets/2012-13-school-data-withaffect 3 https://sites.google.com/site/assistmentsdata/home/2009-2010-assistment-datasetting using both ID and text considering that it gets better results than text-only setting in the original paper.</p>
<p>(3) Text-Only methods:</p>
<p>• ZESRec [6]: utilizes a pre-trained language model to encode the item texts as the input feature for next-item prediction.</p>
<p>• RECFORMER [14]: threats item key-value attributes as texts and formulates the interaction sequences as sentences sequences.</p>
<p>Then a pre-trained language model is fine-tuned for the next item prediction.</p>
<p>(4) Graph-based concept recommendation methods:</p>
<p>• ACKRec [8]: builds a heterogeneous graph an uses attention mechanism to learn the representation from different metapaths.</p>
<p>• GCARec [31]: develops topology-level and feature-level augmentations to generate different views of learner-concept graph to conduct contrastive learning.</p>
<p>Evaluation metrics.</p>
<p>To evaluate the performance of sequential recommendation, we adopt Hit Ratio (HR), Normalized Discounted Cumulative Gain (NDCG), and Mean Reciprocal Rank (MRR) as the evaluation metrics.We employ a leave-one-out strategy, where each sequence's last concept is reserved for testing, the second to last for validation, and all preceding ones for training.</p>
<p>Overall Performance</p>
<p>Table 2 reports the overall performance of all methods in three datasets.It demonstrates that:</p>
<p>• Our proposed SKarRec achieves the best overall performance.Different from baselines, our proposed enhanced text and graphbased adapter make the recommendation model understand the structure and semantic links between concepts, leading to improved performance in concept recommendation.• Adding textual information to concept recommendations proves beneficial, allowing methods that use both IDs and text (ID-Text methods) to outperform ID-Only methods.Nonetheless, the improvement is constrained by the absence of a dedicated adapter in earlier approaches.With the introduction of the graphbased adapter, the advantages of incorporating text into concept recommendations are more significantly realized.• The performance of graph-based methods designed for concept recommendation declines in the absence of heterogeneous information.Originally, these methods relied on heterogeneous graphs that included a variety of elements like courses, videos, and teachers.However, such comprehensive data is rarely available in most open-source educational datasets, leading to limitations in these approaches.</p>
<p>Ablation Study</p>
<p>We conduct ablation experiments to analyze how our proposed modules influence the final concept recommendation performance.</p>
<p>Graph-based</p>
<p>Adapter.We compare different versions of our model to assess the impact of our suggested graph-based adaptation strategy.The results are shown in Table 3.In "SKarRec-No-GraphSSL", we remove the contrastive learning (retaining the GNN structure, training only through the fine-tune recommendation task).In "SKarRec-MoE", we entirely replaced the graph-based adapter with the MoE adapter from previous work [9].Both variations exhibit a notable drop in performance compared to the original structure.This not only demonstrates the graph-based adapter's superior ability to adapt the text encodings but also indicates that this capability largely stems from the contrastive learning on the graph rather than merely changing the adapter's structure to a GNN.Graph-based contrastive learning has a clear training objective: to ensure that each concept's embedding captures the prerequisite relationships between concepts.This relationship is crucial for incorporating the semantics of concepts and effectively integrating the human knowledge system, thereby enhancing recommendation outcomes.</p>
<p>5.3.2</p>
<p>Structure and Knowledge-aware Concept Interpretation.We now delved into the effects of the proposed structure and knowledgeaware concept interpretation, referred to as enhanced text.We conduct comparison experiments, and the results are shown in Table 4.</p>
<p>In the variant "SKarRec-No-LLM", we drop the concept explanations provided by the LLM and only keep the information about the concept's predecessors and successors from the graph.We observe a decline in performance.It demonstrates that capturing the semantic meanings of concepts are important for introducing the</p>
<p>Learner sequence modeling.</p>
<p>In our framework, we mainly have two modules for learner sequence modeling: One is the knowledge tracing module to capture the learner's knowledge state from the learning history.The other is the sequence-based self-supervised pre-training (shortened as "S3Pre-training" since it's from the S3Rec method).We now explore the impact of them.The results are shown in Figure 3.It was observed that removing knowledge tracing significantly deteriorates the model's performance.The reason is that each learner's learning journey is highly individualized, making concept recommendations heavily reliant on their current knowledge state.Therefore, the modeling of learners' knowledge states is crucial.Additionally, eliminating the sequence-based pre-training results in a notable decrease in performance.This highlights the importance of learning patterns within learner sequences, which is essential for the model to predict the next concept accurately.</p>
<p>Consistency with Knowledge Structure</p>
<p>The primary distinction between concept recommendation and product recommendation lies in that educational recommendation of concepts should adhere to the structure of human knowledge [20].This is manifested in data as the prerequisite relationships between concepts.In this section, we conducted experiments to evaluate if the models' recommendations are consistent with the knowledge graph.We compare the proportion of recommended concepts,   +1 , having a prerequisite or successor relationship with the previous concept,   , on the knowledge graph.The results are displayed in Figure 5.In sub-figure (a), we measure the share of recommendations that match the knowledge graph's structure on all learners.A recommendation for a learner is deemed consistent if it is a predecessor or successor to the previous concept.We express this as a ratio:
𝑐𝑜𝑛𝑠𝑖𝑠𝑡𝑒𝑛𝑡𝑟𝑎𝑡𝑖𝑜 = #𝑐𝑜𝑛𝑠𝑖𝑠𝑡𝑒𝑛𝑡 𝑠𝑎𝑚𝑝𝑙𝑒 #𝑡𝑜𝑡𝑎𝑙 𝑠𝑎𝑚𝑝𝑙𝑒
In sub-figure (b), instead of evaluating the consistency on all learners, we evaluate on the learners whose learning style is adhered to the graph.Specifically, we evaluate whether a learner's learning sequence aligns with the knowledge graph before making recommendations.We take each concept a learner has studied with the next as a pair and consider a pair "consistent pair" if they follow the graph's order.The "consistency score" is the ratio of consistent pairs to total pairs:
𝑠𝑐𝑜𝑟𝑒 = #𝑐𝑜𝑛𝑠𝑖𝑠𝑡𝑒𝑛𝑡 𝑝𝑎𝑖𝑟𝑠 #𝑎𝑙𝑙 𝑝𝑎𝑖𝑟𝑠
. A score over 0.5 suggests the learner's style aligns with the graph.We focus on the rate of consistent recommendations for this subset of learners.</p>
<p>The experiments indicate that our approach more frequently suggests concepts consistent with the graph's structure for all learners.This advantage is even more pronounced for those learners whose styles adhere to the graph's structure.This improvement stems from the structure-aware concept interpretation and the graphbased adapter, which enriches each concept's representation with its prerequisite relationships to other concepts.</p>
<p>Embedding Space Transformation</p>
<p>To better understand the graph-based adapter's ability to adapt nonsmooth, anisotropic text embedding spaces, we employed t-SNE to perform visualization of the vanilla text embeddings, graph-adapted embeddings, and MoE-adapted embeddings.The visualization comparison on ASSIST09 is shown in Figure 4.The other visualizations of ASSIST12 and Junyi are provided in Appendix B.3.From the illustrations provided, it is evident that the vanilla text embeddings and those adapted through MoE lack clear distribution patterns or clustering traits.However, embeddings that have undergone graph-based adaptation display pronounced manifold and clustering properties, making them more conducive to subsequent recommendation tasks.</p>
<p>To further illustrate the transformation effect, we calculate the Davies-Bouldin Index (DBI) [4].It assesses the data separation state between clusters.A lower DBI indicates better-separated classes.We use K-Means to cluster the embeddings and calculate DBI as shown in Table 5.Our graph-based adapted embeddings achieve the lowest DBI scores, demonstrating a better clustering feature.</p>
<p>CONCLUSION</p>
<p>We present a structure and knowledge-aware representation framework for concept recommendation.Our approach involves generating textual descriptions for knowledge concepts using external information from LLMs and structural data from knowledge graphs.To address the challenge of converting non-smooth, anisotropic text encodings for concept recommendation, we have developed a graph-based adapter trained through contrastive learning.This marks the first effort to incorporate external knowledge from LLMs into the field of concept recommendation.Through thorough testing, our framework has proven effective in refining the textual information of concepts to enhance recommendation outcomes.</p>
<p>B ADDITIONAL EXPERIMENTS DETAILS B.1 Implementation Details.</p>
<p>For baselines purely based on heterogeneous graph-based concept recommendation, we constructed a bipartite graph of learnerconcept with two types of edges: correct-answer edges and wronganswer edges.In our method, SKarRec, we implemented three layers of attention blocks.The sizes for the ID embedding, answer embedding, and graph-adapted embedding were all set to 64.The maximum sequence length was configured to be 200, and the batch size was set at 256.The masking ratio  is tuned in {0.1, 0.2, 0.3, 0.4, 0.5}.We utilize GPT-3.5 to do the concept interpretation.</p>
<p>B.2 Impact of edge masking ratio 𝛾</p>
<p>A crucial component of our framework involves performing contrastive learning on the knowledge graph through edge dropout.</p>
<p>A key parameter in this approach is the masking ratio of edges  used to generate different views.We investigate the impact of this parameter, with the experimental results displayed in Figure 6.It can be observed that the impact of this parameter is relatively minor on ASSIST12, while it has a more significant effect on both Junyi and ASSIST09 datasets.A certain drop ratio must be maintained to allow the GNN to learn the structure of the graph effectively.Therefore, when the masking ratio is low, the performance tends to be poorer.</p>
<p>B.3 Embedding Space Transformation</p>
<p>In Figures 7 and Figure 8, we provide a comparison of the embedding distributions through different adaptations on the ASSIST-ments2012 and Junyi datasets, respectively.Additionally, we present their Davies-Bouldin Index (DBI) values, demonstrating that the distribution post-transformation by the graph adapter exhibits superior clustering properties compared to the MoE (Mixture of Experts) adapter.</p>
<p>Figure 1 :
1
Figure 1: The concept recommendation strategy of combining the learner's knowledge state and the human knowledge system.The knowledge state is estimated by knowledge tracing, and the human knowledge system is modeled by graphadapted encoding of the enhanced text of each concept.</p>
<p>We propose Structure and Knowledge Aware Representation learning framework for concept Recommendation (SKarREC).By bringing open-world factual knowledge from LLMs into concepts' representation, we integrate potential deeper semantic connections for recommendation.To the best of our knowledge, we are the first to introduce large language model's knowledge into concept recommendation scenario.• We utilize the structural relationships between concepts to help LLMs generate their explanation, which resolves the ambiguity problem in concept explanation.• We construct a novel graph-based adapter for text encoding adaptation.The adapter is pre-trained through contrastive learning on the knowledge graph, getting a smooth and structure-aware concept representation to help with concept recommendation.• We have conducted extensive experiments to show the effectiveness of our methods.Results demonstrate that the proposed approach outperforms previous baselines and improves the knowledge usage efficiency of text information.</p>
<p>Figure 2 :
2
Figure 2: The overall framework of SKarRec.The left part shows the construction of concept interpretations integrates structure and knowledge.The right part is the demonstration of knowledge tracing.The bottom center details the graph-based text adaptation process.The top middle outlines the recommendation mechanism using transformed embeddings.</p>
<ol>
<li>6 . 1
61
Pre-training.During the pre-training phase, our training focuses on three parts: 1) Graph-based pre-training.Train the graph adapter to transfer embeddings from text space into a graph or knowledge space.2) Knowledge-Tracing pre-training.Train the knowledge tracing module to represent a learner's knowledge state accurately.3)Sequence-based pre-training.Train the Transformer network to utilize the graph-adapted encodings and knowledge tracing encodings to understand the sequential order of learning sequences.The former two could be achieved by the contrastive learning loss on the knowledge graph L   and the knowledge tracing loss L  , respectively.</li>
</ol>
<p>11 )
11
In summary, in the pre-training stage, we pre-train graph-based adapter, knowledge tracing module and Transformer network by L   , L  and L   .4.6.2Fine-tuning.After pre-training, we fine-tune the whole model in an end-to-end manner based on the concept recommendation</p>
<p>Figure 3 :
3
Figure 3: Performance of comparison with/without sequence modeling tasks.</p>
<p>Figure 4 :
4
Figure 4: Comparing different distributions of embeddings in ASSIST09.The smaller the DBI value upper in figure, the better.</p>
<p>Figure 5 :
5
Figure 5: The matching ratios of the recommendation and graph structure.</p>
<p>Concept name: Table Concept Explanation: The concept of [Table] refers to a structured arrangement of data or information in rows and columns.Predecessors: Histogram as Table or Graph Successors: Venn Diagram, Mean Structure and Knowledge-aware Concept Interpretation Please tell me the meaning of the concept "Table".If this concept has ambiguity, please guess its meaning based on its predecessors: Histogram as Table or Graph and its successors: Venn Diagram, Mean.
RecommendationKnowledge TracingTransformer Blocks...ID Embedding Ans Embedding Text EncodingKT PredictionLLMGraph-based Text AdaptationContrastive LearningConcept name: TableConcept Explanation: ... Predecessors: Histogram as Table..EncoderSuccessors: Venn Diagram, Mean</p>
<p>Table 1 :
1
Dataset Statistics
DatasetJunyiASSIST12 ASSIT09#Concepts835265145#Learners341,19524,1553,322#Records21,460,249 1,853,338187,914Correct rate54.38%69.55%64.12%</p>
<p>Table 2 :
2
Performance comparison of different recommendation models.The best and the second-best performance is bold and underlined respectively."<em>"denotes that the improvement are significant at level of  &lt; 0.05 with paired t-test comparing with the second best baseline.
Graph-based Methods ID-Only MethodsText-Only MethodsID-Text MethodsDataset MetricACKRec GCARec SASRec BERT4Rec ZESRec RECFORMER FDSA S3-Rec UniSRec SKarRecHR@10.24080.25130.76490.19350.58090.60720.7627 0.76610.75970.7922  </em>ASSIST09NDCG@5 0.37870.39040.87360.22030.72690.65400.8722 0.87310.86470.8838  <em>MRR0.36270.37560.85030.23420.69980.67110.8484 0.85050.84390.8646  </em>HR@10.19760.21190.28380.25400.25960.19900.2861 0.29040.28880.2933  <em>ASSIST12NDCG@5 0.30780.31690.48950.42470.46490.20550.4886 0.4957 0.49340.4954MRR0.30640.31820.45740.40470.43430.25360.4579 0.46330.46170.4645  </em>HR@10.14470.12840.84690.35500.85460.82790.8492 0.86080.84070.8730  <em>JunyiNDCG@5 0.18720.16500.89070.48260.89300.76250.8917 0.90220.88900.9076  </em>MRR0.19180.17400.88250.46610.88600.84970.8840 0.89420.87990.9014  *</p>
<p>Table 3 :
3
Ablation study about the proposed graph-based adapter.The "-MoE" suffix means that we change the adapter to MoE adapter and the "-No-GraphSSL" means that we remove the graph-based contrasrive learning.
DatasetMetric SKarRec-No-GraphSSL SKarRec-MoE SKarRecHR@10.77270.77540.7922ASSIST09NDCG@50.87640.87600.8838MRR0.85430.85480.8646HR@10.29010.28550.2933ASSIST12NDCG@50.49460.49230.4954MRR0.46260.46010.4645HR@10.87110.87170.8730JunyiNDCG@50.90600.90710.9076MRR0.89980.90060.9014</p>
<p>Table 4 :
4
Ablation study about the structure and knowledgeaware concept interpretation.The "-No-LLM" suffix means that we drop the LLM's concept explanation and the "-Name" means that we drop all the enhanced text.
DatasetMetric SKarRec-No-LLM SKarRec-Name SKarRecHR@10.78980.77750.7922ASSIST09NDCG@50.88320.87560.8838MRR0.86330.85520.8646HR@10.28860.49260.2933ASSIST12NDCG@50.49360.46120.4954MRR0.46170.87210.4645HR@10.87080.87210.8730JunyiNDCG@50.90650.90690.9076MRR0.90010.90060.9014human knowledge system. In the variant "SKarRec-Name", we dropall the enhanced text (including LLM's Concept Explanation andthe predecessors and successors from the graph) and only providethe concept names. This substitution results in a significant drop inrecommendation performance, illustrating that the text names areinsufficient for incorporating concepts' semantic meanings. Ourproposed method successfully incorporates semantic and struc-tural information into the text, improving the learning for conceptrepresentation.0.75 0.80 0.85 0.900.770.76 w/o KT 0.79 w/o S3Pre-training 0.87 0.88 Ours0.880.850.850.860.83 0.86 0.89 0.920.860.87 w/o KT 0.87 w/o S3Pre-training 0.90 0.91 Ours0.910.900.900.900.70HR@1NDCG@5MRR0.80HR@1NDCG@5MRR(a) ASSIST09(b) Junyi</p>
<p>Table 5 :
5
The DBI value of different representation learned in three datasets.
EmbeddingsASSIST09 ASSIST12 JunyiVanilla Text Embeddings 2.02572.30553.0333MoE Adapted1.60071.91992.0392SKarRec0.66110.3255 1.7208</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Longformer: The longdocument transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, arXiv:2004.051502020. 2020arXiv preprint</p>
<p>Empirical evaluation of gated recurrent neural networks on sequence modeling. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, arXiv:1412.35552014. 2014arXiv preprint</p>
<p>A cluster separation measure. L David, Donald W Davies, Bouldin, 1979. 19792</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018. 2018arXiv preprint</p>
<p>Zeroshot recommender systems. Yifei Hao Ding, Anoop Ma, Yuyang Deoras, Hao Wang, Wang, arXiv:2105.083182021. 2021arXiv preprint</p>
<p>Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5). Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, Yongfeng Zhang, Proceedings of the 16th ACM Conference on Recommender Systems. the 16th ACM Conference on Recommender Systems2022</p>
<p>Attentional graph convolutional networks for knowledge concept recommendation in moocs in a heterogeneous view. Jibing Gong, Shen Wang, Jinlong Wang, Wenzheng Feng, Hao Peng, Jie Tang, Philip S Yu, Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval. the 43rd international ACM SIGIR conference on research and development in information retrieval2020</p>
<p>Towards universal sequence representation learning for recommender systems. Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, Ji-Rong Wen, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Self-attentive sequential recommendation. Wang-Cheng Kang, Julian Mcauley, 2018 IEEE international conference on data mining (ICDM). IEEE2018</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016. 2016arXiv preprint</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.134612019. 2019BartarXiv preprint</p>
<p>On the sentence embeddings from pre-trained language models. Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, Lei Li, arXiv:2011.058642020. 2020arXiv preprint</p>
<p>Text Is All You Need: Learning Language Representations for Sequential Recommendation. Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, Julian Mcauley, arXiv:2305.137312023. 2023arXiv preprint</p>
<p>Deeper insights into graph convolutional networks for semi-supervised learning. Qimai Li, Zhichao Han, Xiao-Ming Wu, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201832</p>
<p>Graph Enhanced Hierarchical Reinforcement Learning for Goal-oriented Learning Path Recommendation. Qingyao Li, Wei Xia, Jian Li'ang Yin, Renting Shen, Weinan Rui, Xianyu Zhang, Ruiming Chen, Yong Tang, Yu, Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. the 32nd ACM International Conference on Information and Knowledge Management2023</p>
<p>Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, arXiv:2306.05817How Can Recommender Systems Benefit from Large Language Models: A Survey. 2023. 2023arXiv preprint</p>
<p>Adaptive course recommendation in MOOCs. Yuanguo Lin, Shibo Feng, Fan Lin, Wenhua Zeng, Yong Liu, Pengcheng Wu, Knowledge-Based Systems. 2241070852021. 2021</p>
<p>Finding similar exercises in online education systems. Qi Liu, Zai Huang, Zhenya Huang, Chuanren Liu, Enhong Chen, Yu Su, Guoping Hu, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2018</p>
<p>Exploiting cognitive structure for adaptive learning. Qi Liu, Shiwei Tong, Chuanren Liu, Hongke Zhao, Enhong Chen, Haiping Ma, Shijin Wang, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2019</p>
<p>pyKT: a python library to benchmark deep learning based knowledge tracing models. Zitao Liu, Qiongqiong Liu, Jiahao Chen, Shuyan Huang, Jiliang Tang, Weiqi Luo, Advances in Neural Information Processing Systems. 352022. 2022</p>
<p>Improving knowledge tracing with collaborative information. Ting Long, Jiarui Qin, Jian Shen, Weinan Zhang, Wei Xia, Ruiming Tang, Xiuqiang He, Yong Yu, Proceedings of the fifteenth ACM international conference on web search and data mining. the fifteenth ACM international conference on web search and data mining2022</p>
<p>Graph-based knowledge tracing: modeling student proficiency using graph neural network. Hiromi Nakagawa, Yusuke Iwasawa, Yutaka Matsuo, IEEE/WIC/ACM International Conference on Web Intelligence. 2019</p>
<p>Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. 2018. 2018arXiv preprint</p>
<p>Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas J Guibas, Jascha Sohl-Dickstein, Deep knowledge tracing. 2015. 201528</p>
<p>BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, Peng Jiang, Proceedings of the 28th ACM international conference on information and knowledge management. the 28th ACM international conference on information and knowledge management2019</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 2017. 201730</p>
<p>Self-supervised graph learning for recommendation. Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, Xing Xie, Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. the 44th international ACM SIGIR conference on research and development in information retrieval2021</p>
<p>GIKT: a graph-based interaction model for knowledge tracing. Yang Yang, Jian Shen, Yanru Qu, Yunfei Liu, Kerong Wang, Yaoming Zhu, Weinan Zhang, Yong Yu, Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020. Ghent, BelgiumSpringer2021. September 14-18, 2020Proceedings, Part I</p>
<p>MOOCCube: a largescale data repository for NLP applications in MOOCs. Jifan Yu, Gan Luo, Tong Xiao, Qingyang Zhong, Yuquan Wang, Wenzheng Feng, Junyi Luo, Chenyu Wang, Lei Hou, Juanzi Li, Proceedings of the 58th annual meeting of the association for computational linguistics. the 58th annual meeting of the association for computational linguistics2020</p>
<p>Graph Contrastive Learning with Adaptive Augmentation for Knowledge Concept Recommendation. Mei Yu, Zhaoyuan Ding, Jian Yu, Wenbin Zhang, Ming Yang, Mankun Zhao, 2023 26th International Conference on Computer Supported Cooperative Work in Design (CSCWD). IEEE2023</p>
<p>Feature-level Deeper Self-Attention Network for Sequential Recommendation. Tingting Zhang, Pengpeng Zhao, Yanchi Liu, Jiajie Victor S Sheng, Deqing Xu, Guanfeng Wang, Xiaofang Liu, Zhou, IJCAI. 2019</p>
<p>S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, Ji-Rong Wen, Proceedings of the 29th ACM international conference on information &amp; knowledge management. the 29th ACM international conference on information &amp; knowledge management2020</p>
<p>A ALGORITHM We present the detailed pre-training and fine-tuning procedure in Algorithm 1. Algorithm 1: Pre-training and Fine-tuning Framework 1 Input: Knowledge graph 𝐺, learner sequences 𝐷 train , Random initialized graph-based adapter 𝐹 𝑔 , knowledge tracing network 𝐹 𝑘𝑡. Transformer blocks 𝐹 𝑡𝑟𝑎𝑛</p>
<p>7: end for 8: for 𝑛 in 𝑛 epoch do 9: L 𝐾𝑇 ← KnolwedgeTracingTask(𝐹 𝑘𝑡 , 𝐷 train ) 10: 𝐹 𝑘𝑡 ← Update(L 𝑘𝑡 , 𝐹 𝑘𝑡 ) 11: end for 12: for 𝑛 in 𝑛 epoch do 13: 𝐷 𝑀𝐼 𝑃 ← MaskItem(𝐷 train ) 14: 𝐷 𝑀𝑆𝑃 ← MaskSegment(𝐷 train ) 15: 𝐷 𝑀𝐴𝑃 ← MaskAttribute(𝐷 train ) 16: 𝐷 𝐴𝐴𝑃 ← MaskAssociatedAttribute(𝐷 train ) 17: L 𝑀𝐼 𝑃 ← MaskItemPredict. 𝐹 Trained, 𝐹 𝑔, 𝐹 𝑘𝑡, 𝑡𝑟𝑎𝑛𝑠 ; L 𝐺, 𝐹 𝑔 ; 𝐷 𝑀𝐼 𝑠𝑠𝑙, 𝐹 𝑃, 𝐹 𝑡𝑟𝑎𝑛𝑠, 𝐹 𝑘𝑡 ; 𝐷 𝑔, 𝐹 𝑀𝑆𝑃, 𝐹 𝑡𝑟𝑎𝑛𝑠, 𝐹 𝑘𝑡 ; 𝐷 𝑔, 𝐹 𝑀𝐴𝑃, 𝐹 𝑡𝑟𝑎𝑛𝑠, 𝐹 𝑘𝑡 ; 𝐷 𝑔, 𝐹 𝐴𝐴𝑃, 𝐹 𝑡𝑟𝑎𝑛𝑠, 𝐹 𝑘𝑡 ; L 𝑠𝑒𝑞 𝑠𝑠𝑙 ← L 𝑀𝐼 𝑃 + L 𝑀𝑆𝑃 + L 𝑀𝐴𝑃 + L 𝐴𝐴𝑃 ; 𝐹 𝑔, 𝐹 𝑡𝑟𝑎𝑛𝑠, 𝐹 𝑔, (l 𝑠𝑒𝑞 𝑘𝑡 ← Update, 𝐹 𝑠𝑠𝑙, 𝐹 𝑡𝑟𝑎𝑛𝑠, 𝐹 𝑘𝑡 ; 𝐹 𝑔, 𝐹 𝑡𝑟𝑎𝑛𝑠, 𝐹 𝑔, 𝑘𝑡, 𝐷 train ← LLMTextEnhance(𝐷 train ) 2: 𝐷 train ← LLMTextEncoding(𝐷 train ) Pre-training 3: for 𝑛 in 𝑛 epoch do 4: 𝑣 1 (𝐺), 𝑣 2 (𝐺) ← EdgeMask(𝐺, 𝛾) 5: L 𝐺 𝑠𝑠𝑙 ← ContrastiveLearning(𝑣 1 (𝐺), 𝑣 2 (𝐺), 𝐹 𝑔 ) 6: 𝐹 𝑔 ← Update. L 𝐴𝐴𝑃 ← Attributepredict, 18for 𝑛 in 𝑛 epoch do 25: L 𝑟𝑒𝑐 ← NextConceptPred. 𝐷 train ) 26: 𝐹 𝑡𝑟𝑎𝑛𝑠 , 𝐹 𝑔 , 𝐹 𝑘𝑡 ← Update(L 𝑟𝑒𝑐 , 𝐹 𝑡𝑟𝑎𝑛𝑠 , 𝐹 𝑔 , 𝐹 𝑘𝑡 ) 27: end for 28: return 𝐹 𝑡𝑟𝑎𝑛𝑠 , 𝐹 𝑔 , 𝐹 𝑘𝑡</p>            </div>
        </div>

    </div>
</body>
</html>
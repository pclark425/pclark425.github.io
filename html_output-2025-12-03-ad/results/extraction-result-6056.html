<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6056 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6056</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6056</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-267406182</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.01383v3.pdf" target="_blank">LLM-based NLG Evaluation: Current Status and Challenges</a></p>
                <p><strong>Paper Abstract:</strong> Evaluating natural language generation (NLG) is a vital but challenging problem in natural language processing. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM collaborative evaluation. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. Lastly, we discuss several open problems in this area and point out future research directions.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6056.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6056.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chiang&Lee-2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can large language models be an alternative to human evaluations? (Chiang & Lee, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical study showing that instruction-following LLMs (InstructGPT / ChatGPT) can produce evaluation ratings that align with expert human annotators on story generation and adversarial-attack evaluation, suggesting LLMs can in some settings match human evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can large language models be an alternative to human evaluations?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Story generation and adversarial-attack evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>InstructGPT, ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Expert human evaluators (described as experts; exact numbers not reported in the survey summary)</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Agreement/correlation between LLM ratings and expert human ratings (Likert scales / accuracy measures)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>LLM ratings were reported to be consistent with expert human evaluators in the studied settings; LLMs achieved comparable agreement with experts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Early models had limited instruction-following capabilities and sometimes produced unrecognizable outputs or refused to evaluate; results may depend on prompt quality and sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Occasional unrecognizable evaluation outputs or refusal to follow evaluation instructions with earlier LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use more advanced instruction-following models (ChatGPT/GPT-4), apply multiple sampling or randomness-control to avoid unrecognizable outputs, and design clearer human-like prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based NLG Evaluation: Current Status and Challenges', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6056.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6056.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kocmi&Federmann-2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models are state-of-the-art evaluators of translation quality (Kocmi & Federmann, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study reporting that GPT-3.5 and GPT-4 achieve state-of-the-art accuracy in evaluating translation quality compared to human labels, outperforming prior automatic metrics on WMT-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are state-of-the-art evaluators of translation quality</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Machine translation quality evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-3.5, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human labels (WMT-style human judgments used as gold), rating scales 1-5 or 0-100 referenced in survey</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Accuracy/correlation of LLM judgments vs. human labels; comparison against automatic metrics from WMT22</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>GPT-3.5 and GPT-4 matched or exceeded prior automatic metrics and attained state-of-the-art accuracy relative to human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Use of proprietary models introduces opacity and reproducibility concerns; potential dataset- and prompt-dependence of results.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Not explicitly enumerated in the survey for this study, but the broader survey notes reproducibility and bias concerns when using proprietary LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Fine-tune open-source LLMs for evaluation tasks to improve reproducibility; adopt human-LLM collaborative checks for high-stakes judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based NLG Evaluation: Current Status and Challenges', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6056.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6056.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wang-etal-2023a</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Is ChatGPT a good NLG evaluator? A preliminary study (Wang et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cross-task experiments (summarization, story generation, data-to-text) showing ChatGPT can achieve state-of-the-art or competitive correlations with human judgments in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is chatgpt a good NLG evaluator? A preliminary study.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Summarization, story generation, data-to-text</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human judgments used as reference (Likert-style scales typical); survey summarizes cross-dataset comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Correlation with human judgments across multiple datasets and aspects (e.g., fluency, faithfulness)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>ChatGPT often achieved state-of-the-art or comparative correlations with human judgments across many settings, outperforming prior automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Susceptible to position bias, verbosity bias, self-preference, and dataset-dependent variance; proprietary model opacity affects reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Lower correlation on high-quality summaries and robustness failures under adversarial perturbations (noted elsewhere in survey summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Careful prompt design, include necessary references/source documents, adopt pairwise/ranking methods where appropriate, and combine with human oversight or fine-tuned open models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based NLG Evaluation: Current Status and Challenges', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6056.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6056.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mendonca-2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simple LLM prompting is state-of-the-art for robust and multilingual dialogue evaluation (Mendonça et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstrates that ChatGPT with simple rating prompts is a strong evaluator for multilingual dialogue evaluation, outperforming encoder-based prior metrics in multilingual settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Simple LLM prompting is state-of-the-art for robust and multilingual dialogue evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multilingual dialogue response evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human judgments used for correlation evaluation (survey summary, multilingual dialogue datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Correlation with human judgments; robustness across languages</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>ChatGPT surpasses prior encoder-based metrics in correlation with human judgments for multilingual dialogue evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Survey notes potential weaker performance on non-Latin languages in other studies; may still be sensitive to prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Hada et al. (2024b) later reported biases (high-score bias) in non-Latin languages, indicating potential cross-lingual failures.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Further fine-tuning on low-resource languages, targeted prompt engineering, and human-LLM collaborative evaluation for non-Latin scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based NLG Evaluation: Current Status and Challenges', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6056.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6056.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shen-etal-2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models are not yet human-level evaluators for abstractive summarization (Shen et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study reporting that LLM-based evaluators do not yet reach human-level performance for abstractive summarization evaluation, particularly struggling with high-quality summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are not yet human-level evaluators for abstractive summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Abstractive summarization evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Various LLMs (survey cites GPT-family results in this context)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human summaries and human judgments as gold standard in SummEval-like settings</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Correlation and agreement with human judgments, especially on high-quality summaries</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>LLM evaluators show lower correlation with human assessments when judging high-quality summaries; performance gap remains versus human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Less sensitivity to fine-grained quality distinctions at the top end (high-quality outputs); robustness issues under adversarial perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Reduced correlation specifically on top-rated/higher-quality summaries where humans distinguish subtle differences that LLMs miss.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use human-LLM collaborative evaluations, fine-tune LLMs on high-quality human-annotated data, and adopt multi-stage chain-of-aspects prompting to improve granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based NLG Evaluation: Current Status and Challenges', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6056.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6056.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adversarial/dialogue-misrating</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial meta-evaluation findings showing LLMs without references mis-evaluate closed-ended dialogue (multiple authors cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adversarial meta-evaluation results across studies indicate LLM evaluators that lack access to references or dialogue history can give inflated scores to responses that conflict with facts or are otherwise incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Closed-ended dialogue response evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>ChatGPT / GPT-family evaluated in referenced adversarial studies</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human judgments with access to dialogue context used as gold standard; adversarial examples constructed to test evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Scoring consistency, robustness under adversarial perturbations, correlation with human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>LLMs without references tend to give high scores to responses that conflict with dialogue facts, diverging from human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Tendency to over-score hallucinated or context-conflicting responses when not provided the dialogue context/reference; brittle to adversarial perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>LLMs rated factually inconsistent replies highly in closed-ended dialogue tasks (as shown by Liu et al. (2023b) and Zhang et al. (2024) adversarial evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Provide references/source dialogue context to the LLM; use reference-aware prompting; adopt adversarial stress tests during evaluator validation; consider MQM/error-span approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based NLG Evaluation: Current Status and Challenges', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6056.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6056.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Biases-and-failure-modes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observed LLM-as-judge biases and failure modes (position, verbosity, self-preference, cross-lingual high-score bias, dataset variance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Surveyed studies report systematic evaluator biases: position/order bias, preference for longer/verbose outputs, self-generated output bias, tendency to prefer answers with factual errors over short/grammatically-bad ones in some studies, and high-score bias in non-Latin languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multiple NLG tasks (pairwise comparisons, summarization, dialogue, multilingual evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>ChatGPT, GPT-4 and other LLMs across cited studies</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human evaluations used for comparison; various datasets and criteria in cited works</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Agreement/consistency, bias measures, robustness checks under perturbation and adversarial attacks</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>LLM evaluators show systematic preferences (position bias, verbosity bias, self-preference), and performance varies significantly by dataset, criterion, and whether text is human-generated.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Position/order sensitivity, favoring longer outputs, preferring outputs generated by same model (self-bias), spurious ranking of factually flawed answers as better than short/grammatically poor ones (Wu & Aji 2025), and systematic high-score bias in some languages (Hada et al. 2024b).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Order effects changing pairwise comparison outputs; LLMs assigning higher scores to verbose or self-generated outputs; cross-lingual miscalibration (non-Latin scripts) producing inflated scores.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Randomize or split-align-merge input order (Li et al. 2023b); include references and strict evaluation criteria; fine-tune evaluators; use multi-LLM peer-review/aggregation; use human oversight and checklist-based evaluation; perform robustness/adversarial stress testing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based NLG Evaluation: Current Status and Challenges', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6056.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6056.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mitigations-and-collaboration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mitigation strategies and human-LLM collaborative evaluation (checklists, peer-examination, fine-tuning, auditing, multi-agent debates)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Surveyed mitigation approaches include careful prompt engineering, split-align-merge to remove position bias, multi-LLM peer-examination and debate frameworks, fine-tuning open LLMs on curated evaluation data, and human-LLM collaborative pipelines (checklists, human review) to improve reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General NLG evaluation (summarization, translation, dialogue, instruction-following outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Methods applied to ChatGPT/GPT-4 and open-source LLMs (LLaMA family, Vicuna, Mistral variants) as discussed across the survey</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human involvement ranges from designing checklists (COEVAL) to partial human review of LLM-produced scores (≈20% of LLM scores revised in one study) and sample-assignment frameworks (HMCEval) to reduce human effort while retaining accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Accuracy/consistency with human judgment, annotation cost/effort, robustness under adversarial tests, reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Hybrid human-LLM workflows improve accuracy and reduce human cost compared to pure human evaluation or LLM-only evaluation; specialized fine-tuned judges can approach GPT-4 performance with lower cost and better reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Some biases persist when training data uses GPT-4 annotations (self-bias); migrating fine-tuned models to newer foundation LLMs is costly; overambitious fine-tuning targets can fail (AUTO-J/CritiqueLLM issues).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Fine-tuned evaluation models trained on noisy LLM annotations inherit biases; collaborative pipelines still require human oversight for ~20% of cases (COEVAL).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use checklist-based human-LM pipelines (COEVAL), sample-assignment frameworks (HMCEval) to optimize human work, peer-examination across multiple LLMs (Bai et al.), decentralized multi-LLM aggregation, randomization during training to remove position bias (JudgeLM), targeted fine-tuning on high-quality human annotations (Themis), and LLM-assisted human auditing systems (AdaTest/AdaTest++).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-based NLG Evaluation: Current Status and Challenges', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 2)</em></li>
                <li>Is chatgpt a good NLG evaluator? A preliminary study. <em>(Rating: 2)</em></li>
                <li>Large language models are not yet human-level evaluators for abstractive summarization. <em>(Rating: 2)</em></li>
                <li>Simple LLM prompting is state-of-the-art for robust and multilingual dialogue evaluation <em>(Rating: 2)</em></li>
                <li>On the blind spots of model-based evaluation metrics for text generation <em>(Rating: 2)</em></li>
                <li>Split and merge: Aligning position biases in large language model based evaluators <em>(Rating: 2)</em></li>
                <li>HMCEval: human-machine collaborative evaluation framework (Zhang, Ren, and de Rijke 2021) <em>(Rating: 1)</em></li>
                <li>AdaTest: automated test-case generation with LLM assistance (Ribeiro & Lundberg 2022) <em>(Rating: 1)</em></li>
                <li>Peer-examination / PRD: peer rank and discussion improve LLM-based evaluations (Ruosen Li et al.) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6056",
    "paper_id": "paper-267406182",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "Chiang&Lee-2023",
            "name_full": "Can large language models be an alternative to human evaluations? (Chiang & Lee, 2023)",
            "brief_description": "Empirical study showing that instruction-following LLMs (InstructGPT / ChatGPT) can produce evaluation ratings that align with expert human annotators on story generation and adversarial-attack evaluation, suggesting LLMs can in some settings match human evaluators.",
            "citation_title": "Can large language models be an alternative to human evaluations?",
            "mention_or_use": "mention",
            "task_domain": "Story generation and adversarial-attack evaluation",
            "llm_judge_model": "InstructGPT, ChatGPT",
            "human_evaluation_setup": "Expert human evaluators (described as experts; exact numbers not reported in the survey summary)",
            "metrics_compared": "Agreement/correlation between LLM ratings and expert human ratings (Likert scales / accuracy measures)",
            "reported_differences": "LLM ratings were reported to be consistent with expert human evaluators in the studied settings; LLMs achieved comparable agreement with experts.",
            "llm_specific_limitations": "Early models had limited instruction-following capabilities and sometimes produced unrecognizable outputs or refused to evaluate; results may depend on prompt quality and sampling.",
            "notable_failure_cases": "Occasional unrecognizable evaluation outputs or refusal to follow evaluation instructions with earlier LLMs.",
            "mitigation_strategies": "Use more advanced instruction-following models (ChatGPT/GPT-4), apply multiple sampling or randomness-control to avoid unrecognizable outputs, and design clearer human-like prompts.",
            "uuid": "e6056.0",
            "source_info": {
                "paper_title": "LLM-based NLG Evaluation: Current Status and Challenges",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Kocmi&Federmann-2023",
            "name_full": "Large language models are state-of-the-art evaluators of translation quality (Kocmi & Federmann, 2023)",
            "brief_description": "Study reporting that GPT-3.5 and GPT-4 achieve state-of-the-art accuracy in evaluating translation quality compared to human labels, outperforming prior automatic metrics on WMT-style tasks.",
            "citation_title": "Large language models are state-of-the-art evaluators of translation quality",
            "mention_or_use": "mention",
            "task_domain": "Machine translation quality evaluation",
            "llm_judge_model": "GPT-3.5, GPT-4",
            "human_evaluation_setup": "Human labels (WMT-style human judgments used as gold), rating scales 1-5 or 0-100 referenced in survey",
            "metrics_compared": "Accuracy/correlation of LLM judgments vs. human labels; comparison against automatic metrics from WMT22",
            "reported_differences": "GPT-3.5 and GPT-4 matched or exceeded prior automatic metrics and attained state-of-the-art accuracy relative to human labels.",
            "llm_specific_limitations": "Use of proprietary models introduces opacity and reproducibility concerns; potential dataset- and prompt-dependence of results.",
            "notable_failure_cases": "Not explicitly enumerated in the survey for this study, but the broader survey notes reproducibility and bias concerns when using proprietary LLMs.",
            "mitigation_strategies": "Fine-tune open-source LLMs for evaluation tasks to improve reproducibility; adopt human-LLM collaborative checks for high-stakes judgments.",
            "uuid": "e6056.1",
            "source_info": {
                "paper_title": "LLM-based NLG Evaluation: Current Status and Challenges",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Wang-etal-2023a",
            "name_full": "Is ChatGPT a good NLG evaluator? A preliminary study (Wang et al., 2023)",
            "brief_description": "Cross-task experiments (summarization, story generation, data-to-text) showing ChatGPT can achieve state-of-the-art or competitive correlations with human judgments in many settings.",
            "citation_title": "Is chatgpt a good NLG evaluator? A preliminary study.",
            "mention_or_use": "mention",
            "task_domain": "Summarization, story generation, data-to-text",
            "llm_judge_model": "ChatGPT",
            "human_evaluation_setup": "Human judgments used as reference (Likert-style scales typical); survey summarizes cross-dataset comparisons",
            "metrics_compared": "Correlation with human judgments across multiple datasets and aspects (e.g., fluency, faithfulness)",
            "reported_differences": "ChatGPT often achieved state-of-the-art or comparative correlations with human judgments across many settings, outperforming prior automatic metrics.",
            "llm_specific_limitations": "Susceptible to position bias, verbosity bias, self-preference, and dataset-dependent variance; proprietary model opacity affects reproducibility.",
            "notable_failure_cases": "Lower correlation on high-quality summaries and robustness failures under adversarial perturbations (noted elsewhere in survey summaries).",
            "mitigation_strategies": "Careful prompt design, include necessary references/source documents, adopt pairwise/ranking methods where appropriate, and combine with human oversight or fine-tuned open models.",
            "uuid": "e6056.2",
            "source_info": {
                "paper_title": "LLM-based NLG Evaluation: Current Status and Challenges",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mendonca-2023",
            "name_full": "Simple LLM prompting is state-of-the-art for robust and multilingual dialogue evaluation (Mendonça et al., 2023)",
            "brief_description": "Demonstrates that ChatGPT with simple rating prompts is a strong evaluator for multilingual dialogue evaluation, outperforming encoder-based prior metrics in multilingual settings.",
            "citation_title": "Simple LLM prompting is state-of-the-art for robust and multilingual dialogue evaluation",
            "mention_or_use": "mention",
            "task_domain": "Multilingual dialogue response evaluation",
            "llm_judge_model": "ChatGPT",
            "human_evaluation_setup": "Human judgments used for correlation evaluation (survey summary, multilingual dialogue datasets)",
            "metrics_compared": "Correlation with human judgments; robustness across languages",
            "reported_differences": "ChatGPT surpasses prior encoder-based metrics in correlation with human judgments for multilingual dialogue evaluation.",
            "llm_specific_limitations": "Survey notes potential weaker performance on non-Latin languages in other studies; may still be sensitive to prompt templates.",
            "notable_failure_cases": "Hada et al. (2024b) later reported biases (high-score bias) in non-Latin languages, indicating potential cross-lingual failures.",
            "mitigation_strategies": "Further fine-tuning on low-resource languages, targeted prompt engineering, and human-LLM collaborative evaluation for non-Latin scripts.",
            "uuid": "e6056.3",
            "source_info": {
                "paper_title": "LLM-based NLG Evaluation: Current Status and Challenges",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Shen-etal-2023",
            "name_full": "Large language models are not yet human-level evaluators for abstractive summarization (Shen et al., 2023)",
            "brief_description": "Study reporting that LLM-based evaluators do not yet reach human-level performance for abstractive summarization evaluation, particularly struggling with high-quality summaries.",
            "citation_title": "Large language models are not yet human-level evaluators for abstractive summarization.",
            "mention_or_use": "mention",
            "task_domain": "Abstractive summarization evaluation",
            "llm_judge_model": "Various LLMs (survey cites GPT-family results in this context)",
            "human_evaluation_setup": "Human summaries and human judgments as gold standard in SummEval-like settings",
            "metrics_compared": "Correlation and agreement with human judgments, especially on high-quality summaries",
            "reported_differences": "LLM evaluators show lower correlation with human assessments when judging high-quality summaries; performance gap remains versus human annotators.",
            "llm_specific_limitations": "Less sensitivity to fine-grained quality distinctions at the top end (high-quality outputs); robustness issues under adversarial perturbations.",
            "notable_failure_cases": "Reduced correlation specifically on top-rated/higher-quality summaries where humans distinguish subtle differences that LLMs miss.",
            "mitigation_strategies": "Use human-LLM collaborative evaluations, fine-tune LLMs on high-quality human-annotated data, and adopt multi-stage chain-of-aspects prompting to improve granularity.",
            "uuid": "e6056.4",
            "source_info": {
                "paper_title": "LLM-based NLG Evaluation: Current Status and Challenges",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Adversarial/dialogue-misrating",
            "name_full": "Adversarial meta-evaluation findings showing LLMs without references mis-evaluate closed-ended dialogue (multiple authors cited)",
            "brief_description": "Adversarial meta-evaluation results across studies indicate LLM evaluators that lack access to references or dialogue history can give inflated scores to responses that conflict with facts or are otherwise incorrect.",
            "citation_title": "",
            "mention_or_use": "mention",
            "task_domain": "Closed-ended dialogue response evaluation",
            "llm_judge_model": "ChatGPT / GPT-family evaluated in referenced adversarial studies",
            "human_evaluation_setup": "Human judgments with access to dialogue context used as gold standard; adversarial examples constructed to test evaluators",
            "metrics_compared": "Scoring consistency, robustness under adversarial perturbations, correlation with human judgments",
            "reported_differences": "LLMs without references tend to give high scores to responses that conflict with dialogue facts, diverging from human judgments.",
            "llm_specific_limitations": "Tendency to over-score hallucinated or context-conflicting responses when not provided the dialogue context/reference; brittle to adversarial perturbations.",
            "notable_failure_cases": "LLMs rated factually inconsistent replies highly in closed-ended dialogue tasks (as shown by Liu et al. (2023b) and Zhang et al. (2024) adversarial evaluations).",
            "mitigation_strategies": "Provide references/source dialogue context to the LLM; use reference-aware prompting; adopt adversarial stress tests during evaluator validation; consider MQM/error-span approaches.",
            "uuid": "e6056.5",
            "source_info": {
                "paper_title": "LLM-based NLG Evaluation: Current Status and Challenges",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Biases-and-failure-modes",
            "name_full": "Observed LLM-as-judge biases and failure modes (position, verbosity, self-preference, cross-lingual high-score bias, dataset variance)",
            "brief_description": "Surveyed studies report systematic evaluator biases: position/order bias, preference for longer/verbose outputs, self-generated output bias, tendency to prefer answers with factual errors over short/grammatically-bad ones in some studies, and high-score bias in non-Latin languages.",
            "citation_title": "",
            "mention_or_use": "mention",
            "task_domain": "Multiple NLG tasks (pairwise comparisons, summarization, dialogue, multilingual evaluation)",
            "llm_judge_model": "ChatGPT, GPT-4 and other LLMs across cited studies",
            "human_evaluation_setup": "Human evaluations used for comparison; various datasets and criteria in cited works",
            "metrics_compared": "Agreement/consistency, bias measures, robustness checks under perturbation and adversarial attacks",
            "reported_differences": "LLM evaluators show systematic preferences (position bias, verbosity bias, self-preference), and performance varies significantly by dataset, criterion, and whether text is human-generated.",
            "llm_specific_limitations": "Position/order sensitivity, favoring longer outputs, preferring outputs generated by same model (self-bias), spurious ranking of factually flawed answers as better than short/grammatically poor ones (Wu & Aji 2025), and systematic high-score bias in some languages (Hada et al. 2024b).",
            "notable_failure_cases": "Order effects changing pairwise comparison outputs; LLMs assigning higher scores to verbose or self-generated outputs; cross-lingual miscalibration (non-Latin scripts) producing inflated scores.",
            "mitigation_strategies": "Randomize or split-align-merge input order (Li et al. 2023b); include references and strict evaluation criteria; fine-tune evaluators; use multi-LLM peer-review/aggregation; use human oversight and checklist-based evaluation; perform robustness/adversarial stress testing.",
            "uuid": "e6056.6",
            "source_info": {
                "paper_title": "LLM-based NLG Evaluation: Current Status and Challenges",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mitigations-and-collaboration",
            "name_full": "Mitigation strategies and human-LLM collaborative evaluation (checklists, peer-examination, fine-tuning, auditing, multi-agent debates)",
            "brief_description": "Surveyed mitigation approaches include careful prompt engineering, split-align-merge to remove position bias, multi-LLM peer-examination and debate frameworks, fine-tuning open LLMs on curated evaluation data, and human-LLM collaborative pipelines (checklists, human review) to improve reliability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "task_domain": "General NLG evaluation (summarization, translation, dialogue, instruction-following outputs)",
            "llm_judge_model": "Methods applied to ChatGPT/GPT-4 and open-source LLMs (LLaMA family, Vicuna, Mistral variants) as discussed across the survey",
            "human_evaluation_setup": "Human involvement ranges from designing checklists (COEVAL) to partial human review of LLM-produced scores (≈20% of LLM scores revised in one study) and sample-assignment frameworks (HMCEval) to reduce human effort while retaining accuracy.",
            "metrics_compared": "Accuracy/consistency with human judgment, annotation cost/effort, robustness under adversarial tests, reproducibility",
            "reported_differences": "Hybrid human-LLM workflows improve accuracy and reduce human cost compared to pure human evaluation or LLM-only evaluation; specialized fine-tuned judges can approach GPT-4 performance with lower cost and better reproducibility.",
            "llm_specific_limitations": "Some biases persist when training data uses GPT-4 annotations (self-bias); migrating fine-tuned models to newer foundation LLMs is costly; overambitious fine-tuning targets can fail (AUTO-J/CritiqueLLM issues).",
            "notable_failure_cases": "Fine-tuned evaluation models trained on noisy LLM annotations inherit biases; collaborative pipelines still require human oversight for ~20% of cases (COEVAL).",
            "mitigation_strategies": "Use checklist-based human-LM pipelines (COEVAL), sample-assignment frameworks (HMCEval) to optimize human work, peer-examination across multiple LLMs (Bai et al.), decentralized multi-LLM aggregation, randomization during training to remove position bias (JudgeLM), targeted fine-tuning on high-quality human annotations (Themis), and LLM-assisted human auditing systems (AdaTest/AdaTest++).",
            "uuid": "e6056.7",
            "source_info": {
                "paper_title": "LLM-based NLG Evaluation: Current Status and Challenges",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 2,
            "sanitized_title": "large_language_models_are_stateoftheart_evaluators_of_translation_quality"
        },
        {
            "paper_title": "Is chatgpt a good NLG evaluator? A preliminary study.",
            "rating": 2,
            "sanitized_title": "is_chatgpt_a_good_nlg_evaluator_a_preliminary_study"
        },
        {
            "paper_title": "Large language models are not yet human-level evaluators for abstractive summarization.",
            "rating": 2,
            "sanitized_title": "large_language_models_are_not_yet_humanlevel_evaluators_for_abstractive_summarization"
        },
        {
            "paper_title": "Simple LLM prompting is state-of-the-art for robust and multilingual dialogue evaluation",
            "rating": 2,
            "sanitized_title": "simple_llm_prompting_is_stateoftheart_for_robust_and_multilingual_dialogue_evaluation"
        },
        {
            "paper_title": "On the blind spots of model-based evaluation metrics for text generation",
            "rating": 2,
            "sanitized_title": "on_the_blind_spots_of_modelbased_evaluation_metrics_for_text_generation"
        },
        {
            "paper_title": "Split and merge: Aligning position biases in large language model based evaluators",
            "rating": 2,
            "sanitized_title": "split_and_merge_aligning_position_biases_in_large_language_model_based_evaluators"
        },
        {
            "paper_title": "HMCEval: human-machine collaborative evaluation framework (Zhang, Ren, and de Rijke 2021)",
            "rating": 1,
            "sanitized_title": "hmceval_humanmachine_collaborative_evaluation_framework_zhang_ren_and_de_rijke_2021"
        },
        {
            "paper_title": "AdaTest: automated test-case generation with LLM assistance (Ribeiro & Lundberg 2022)",
            "rating": 1,
            "sanitized_title": "adatest_automated_testcase_generation_with_llm_assistance_ribeiro_lundberg_2022"
        },
        {
            "paper_title": "Peer-examination / PRD: peer rank and discussion improve LLM-based evaluations (Ruosen Li et al.)",
            "rating": 1,
            "sanitized_title": "peerexamination_prd_peer_rank_and_discussion_improve_llmbased_evaluations_ruosen_li_et_al"
        }
    ],
    "cost": 0.01915825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLM-based NLG Evaluation: Current Status and Challenges
14 May 2025</p>
<p>Mingqi Gao gaomingqi@pku.edu.cn 
Xinyu Hu huxinyu@pku.edu.cn 
Xunjian Yin xjyin@pku.edu.cn 
Jie Ruan ruanjie@stu.pku.edu.cn 
Xiao Pu puxiao@stu.pku.edu.cn 
Xiaojun Wan wanxiaojun@pku.edu.cn 
Hu, Yin, Ruan, PuWan Gao 
Anil 
Andrew M Rohan 
Orhan Dai 
Melvin Firat 
Dmitry Johnson 
Alexandre Lepikhin 
Siamak Passos 
Emanuel Shakeri 
Paige Taropa 
Zhifeng Bailey 
Eric Chen 
Jonathan H Chu 
Laurent Clark 
Shafey El 
Yanping Huang 
Kathy Meier-Hellstern 
Gaurav Mishra 
Erica Moreira 
Mark Omernick 
Kevin Robinson 
Sebastian Ruder 
Yi Tay 
Kefan Xiao 
Yuanzhong Xu 
Yujing Zhang 
Gustavo Hernández Ábrego 
Junwhan Ahn 
Jacob Austin 
Paul Barham 
Jan A Botha 
James Bradbury 
Siddhartha Brahma 
Kevin Brooks 
Michele Catasta 
Yong Cheng 
Colin Cherry 
Christopher A Choquette-Choo 
Aakanksha Chowdhery 
Clément Crepy 
Shachi Dave 
Mostafa Dehghani 
Sunipa Dev 
Jacob Devlin 
Mark Díaz 
Nan Du 
Ethan Dyer 
Vladimir Feinberg 
Fangxiaoyu Feng 
Vlad Fienber 
Ashktorab 
Michael Zahra 
Qian Desmond 
James M Pan 
Martin Santillan Johnson 
Elizabeth M Cooper 
Rahul Daly 
Tejaswini Nair 
Swapnaja Pedapati 
Werner Achintalwar 
2024 Geyer 
Anna Bavaresco 
Raffaella Bernardi 
Leonardo Bertolazzi 
Desmond Elliott 
Raquel Fernández 
Albert Gatt 
Esam Ghaleb 
Mario Giulianelli 
Michael Hanna 
Alexander Koller 
André F T Martins 
Philipp Mondorf 
Vera Neplenbroek </p>
<p>Peking University</p>
<p>Peking University</p>
<p>Peking University</p>
<p>Peking University</p>
<p>Peking University</p>
<p>Peking University</p>
<p>Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology</p>
<p>Markus Freitag
Xavier Garcia</p>
<p>Sebastian Gehrmann
Lucas Gonzalez</p>
<p>LLM-based NLG Evaluation: Current Status and Challenges
14 May 20252E4C135524E0B7072EE7A22B97AE1259arXiv:2402.01383v3[cs.CL]
Evaluating natural language generation (NLG) is a vital but challenging problem in natural language processing.Traditional evaluation metrics mainly capturing content (e.g.n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years.Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM collaborative evaluation.In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively.Lastly, we discuss several open problems in this area and point out future research directions.Contents</p>
<p>Introduction</p>
<p>The evaluation of natural language generation (NLG) is an important but challenging issue.The lack of a single standard answer and the presence of multiple quality criteria make evaluating NLG more challenging than other NLP tasks.For example, in news summarization, a good summary should capture the key information from the source document, remain faithful to the source document, and be expressed in logically coherent and fluent language, but there is not a single "correct" way to achieve this.The inherent difficulty of NLG evaluation means that human evaluation is always needed and regarded as the gold standard.However, due to the high cost and time-consuming nature of human evaluation, automatic evaluation metrics remain indispensable and play a crucial role in model development.Over the past two decades, many automatic evaluation metrics such as BLEU (Papineni et al. 2002) and BARTScore (Yuan, Neubig, and Liu 2021) have been developed, but none have been fully satisfactory.Some studies (Sai et al. 2021;He et al. 2023a) have highlighted their deficiencies in robustness, such as insensitivities, biases, or even loopholes when evaluating challenging texts.</p>
<p>Recently, large language models (LLMs) have emerged and demonstrated unprecedented capacities in following instructions, understanding content, and generating text, which inspires researchers to utilize LLMs for NLG evaluation.Although this is a research direction that only emerged in 2023, the past year has seen an enormous amount of relevant work.While there have been surveys on automatic evaluation metrics or human evaluation practices in NLG evaluation (Celikyilmaz, Clark, and Gao 2020;Hämäläinen and Al-Najjar 2021;Zhou et al. 2022;Sai, Mohankumar, and Khapra 2023;Gehrmann, Clark, and Sellam 2023;Zhou, Ringeval, and Portet 2023), none of them addresses LLM-based evaluation approach, and a comprehensive survey of this area is urgently needed.</p>
<p>The survey will mainly focus on research on LLM-based approaches for NLG evaluation, which involves language models with over one billion parameters.Moreover, it mainly considers the typical scope of NLG tasks where both input and output are natural languages including machine translation, text summarization, story generation, dialogue response generation, data-to-text, text simplification, paraphrase generation, grammatical error correction, and creative writing.Broader areas like evaluation of LLMs are not included (Zhuang et al. 2023;Ma et al. 2025) because this work focuses on LLMs used for evaluation, rather than the evaluation of LLMs' capabilities.We search the literature on Google Scholar with an end date of June 2024 with keywords.Since this is a new direction that emerged in 2023, a considerable number of arXiv preprints are included in addition to papers published in *ACL venues or other related venues.About 100 pieces of work will be included.To maintain focus, this paper neither discusses datasets and benchmarks in NLG evaluation (Gehrmann et al. 2021;Kim et al. 2024b) nor analyzes evaluation metrics statistically (Ni'mah et al. 2023;Xiao et al. 2023).As shown in Figure 1, we categorize related studies into four categories according to how LLMs are utilized for NLG evaluation:</p>
<p>LLM</p>
<p>• LLM-derived Metrics ( § 2): developing or deriving evaluation metrics from embeddings or generation probabilities of LLMs.</p>
<p>• Prompting LLMs ( § 3): directly inquiring of existing LLMs via specific prompts and processes designed for evaluation.</p>
<p>• Fine-tuning LLMs ( § 4): using labeled evaluation data to fine-tune existing LLMs and improving their NLG evaluation capabilities.</p>
<p>• Human-LLM Collaborative Evaluation ( § 5): leveraging distinctive strengths of both human evaluators and LLMs to achieve robust and nuanced evaluations through human-LLM collaboration.</p>
<p>LLMs have driven NLG evaluation toward a more human-centered direction, and the four categories we propose reflect this evolution: LLM-derived metrics are a continuation of traditional evaluation metrics and can only handle coarse-grained evaluation; prompting and fine-tuning methods enable users to express flexible evaluation requirements in natural language; collaborative evaluation takes it a step further, making it possible for humans and LLMs to leverage their strength respectively.We will review each type of evaluation method and discuss the pros and cons, respectively.Last but not least, we will provide our suggestions and conclusions, and discuss future directions in this area ( § 6).</p>
<p>It is worth stating that since LLM-based evaluation has shown unprecedented generality across NLG tasks, we do not summarize the literature for each task separately.Nevertheless, we will draw a list documenting all the approaches in this survey, indicating which NLG tasks each approach has been experimented on.</p>
<p>LLM-derived Metrics</p>
<p>LLM-derived metrics can be viewed as a continuation of early model-based NLG evaluation metrics such as BERTScore and BARTScore, replacing traditional pretrained language models with stronger LLMs.Such works can be categorized into two main types: embedding-based metrics (ES et al. 2024) and probability-based metrics.The latter can be further divided into two categories based on different ways of using probabilities: directly converting the probabilities into scores (Fu et al. 2024;Varshney et al. 2023) and leveraging the variation in probabilities under changed conditions (Jia et al. 2023;Xie et al. 2023).</p>
<p>Embedding-based Metrics</p>
<p>The embedding-based methods, like BERTScore, generally utilize representations of language models and thus compute the semantic similarity between the reference and the target text to evaluate, with different possible ways of implementation.However, unlike traditional embedding-based evaluation metrics that require references, many LLM-based embedding evaluation metrics do not.This is because their application scenarios and implementation methods differ from those of traditional metrics.For example, when ES et al. ( 2024) evaluate the answer relevance of Retrieval Augmented Generation, given the original question q and the answer Y to be evaluated, they first prompt the LLM to generate n possible questions q i for Y .Then, the relevance of Y is represented by the average similarity between q i and q, denoted as n i=1 sim(q i , q), where sim(q i , q) refers to the cosine similarity of the embeddings of q i and q.The embedding is generated by OpenAI text-embedding-ada-002, which can efficiently convert text into a 1536-dimensional vector, capturing semantic information and ensuring that similar texts are positioned close to each other in the vector space.Furthermore, Sheng et al. (2024) developed a more sophisticated method based on embeddings from the open-source decoder-only LLM, utilizing Principal Component Analysis to adapt it for both pointwise scoring and pairwise comparison.</p>
<p>Probability-based Metrics</p>
<p>To better utilize the knowledge inherent in language models, probability-based methods like BARTScore formulate text generation evaluation as conditional probability comparison, positing that the better the quality of the target text, the higher the likelihood that models should be able to generate it.Recently, GPTScore (Fu et al. 2024) has established tailored evaluation templates for each aspect to effectively guide multiple LLMs for NLG evaluation, including GPT3 (Wang et al. 2022), OPT (Zhang et al. 2022), and FLAN (Chung et al. 2024).The core idea of GPTScore is that a good generative language model is more likely to assign higher probabilities to high-quality text generated in response to a given instruction and context.Specifically, given a generative large language model θ, context information X (such as a source document), output text Y = {y 1 , y 2 , . . ., y m } containing m tokens to be evaluated, and instruction I that specifies the requirement for the LLMs to generate text that can flexibly correspond to different evaluation aspects (e.g., generating a factually consistent summary for the aspect of consistency), GPTScore is defined as: GPTScore(X, Y, I, θ) = m i=1 log P (y i |y &lt;i , X, I, θ)</p>
<p>Similarly, Murugadoss et al. (2025) score the task output Y to be evaluated by its perplexity under the corresponding large language model θ, given only the task context X.They believe this approach is unbiased by prompts, which transparently measures alignment with model training data.Furthermore, such methods have also been applied to the hallucination detection of the LLM-generated text (Varshney et al. 2023) with three different attempts for calculating the probability score.</p>
<p>On the other hand, some works leverage the variation in probabilities under changed conditions as the evaluation metric.FFLM (Jia et al. 2023) proposes to evaluate the faithfulness of the target text by calculating a combination of probability changes based on the intuition that the generation probability of a given text segment increases when more consistent information is provided, and vice versa.Similarly, DELTASCORE (Xie et al. 2023) measures the quality of different story aspects according to the likelihood difference between pre-and post-perturbation states with LLMs including GPT-3.5 (text-davinci-003) that provide logits.They believe that the sensitivity to specific perturbations indicates the quality of related aspects, and their experiments demonstrate the effectiveness of their approach.</p>
<p>Pros and Cons</p>
<p>Traditional NLG evaluation approaches always fall short due to their surface-form similarity when the target text and reference convey the same meaning but use different expressions.In contrast, LLM-derived metrics offer a remedy for the limitation and demonstrate stronger correlations with human judgments benefiting from the evolving modeling techniques.However, the flaws within LLMs can lead to some issues, as introduced in the following:</p>
<p>Robustness.Some research has investigated the robustness of LLM-derived metrics and found that they lack robustness in different attack scenarios.Specifically, He et al. (2023b) develops a set of stress tests to assess the robustness of various model-based metrics on some common NLG tasks.They show a catalogue of the blind spots and potential errors identified that are not detected by different metrics.</p>
<p>Efficiency.Compared to traditional metrics, LLM-derived evaluation methods are more time-consuming and require more computational resources, especially when adopting LLMs with quite large parameter scales.To address this, Eddine et al. (2022) propose an approach to learning a lightweight version of LLM-derived metrics, and some fast LLM inference and serving tools like popular vLLM (Kwon et al. 2023) have been launched.vLLM improves memory utilization during inference through the PagedAttention algorithm, as well as the optimized memory management and batching strategies, thereby increasing LLMs' generation throughput.However, closedsource LLMs often do not make their parameters, representations, or logits public and available, thus making it impossible to apply LLM-derived methods to them.</p>
<p>Fairness.Sun et al. (2022) assess the social bias across various metrics for NLG evaluation on six sensitive attributes: race, gender, religion, physical appearance, age, and socioeconomic status.Their findings reveal that model-based metrics carry noticeably more social bias than traditional metrics.Relevant biases can be categorized into two types: intrinsic bias encoded within pre-trained language models and extrinsic bias injected during the computation of similarity.Therefore, current LLM-derived methods may have similar issues.</p>
<p>Prompting LLMs</p>
<p>The remarkable generation abilities of LLMs have expanded the possibilities for NLG evaluation.For a long time, human evaluation has been viewed as the gold standard for NLG evaluation.Recently, some studies claim that LLMs are on par with crowdsourcing annotators in several tasks (Törnberg 2023;Gilardi, Alizadeh, and Kubli 2023;Ostyakova et al. 2023;Cegin, Simko, and Brusilovsky 2023).This raises questions about whether LLMs could replace human evaluators.Studies in this area often involve feeding LLMs with detailed prompts that include both instructions and the text to be evaluated, with LLMs producing the evaluation outcomes.An example of prompting LLMs is shown in Figure 2. From this example, we can see that such a prompt is quite similar to the guidelines given to human evaluators.The main differences between this prompting method for LLMs and LLM-derived metrics are twofold: (1) LLM-derived metrics generally do not involve highly human-like prompts that require the LLM to perform an evaluation.(2) The evaluation results from prompting LLMs are typically generated directly by the LLM, whereas LLM-derived metrics require further transformation from embeddings and probabilities.We will describe existing works according to the five elements that they mainly focus on: Representative studies on prompting LLMs for NLG evaluation.</p>
<p>• Evaluation Methods: The way the evaluation results of LLM evaluators are obtained, such as scoring and comparison.</p>
<p>• Task Instructions: How LLM evaluators should read or manipulate different parts to complete the annotation.</p>
<p>•</p>
<p>Input Content:</p>
<p>The target text to be evaluated and other required content.</p>
<p>Other required content including source documents, references, and external knowledge is provided as needed.</p>
<p>• Evaluation Criteria: The general definition of how good or bad the text to be evaluated is in a particular aspect of quality, e.g.fluency, faithfulness.</p>
<p>•</p>
<p>Role and interaction:</p>
<p>The roles LLM evaluators play in the evaluation and the interactions between them.</p>
<p>Evaluation Methods</p>
<p>Diverse evaluation methods have been employed in prompting LLMs to obtain its preferences for the text to be evaluated: scoring, comparison, ranking, boolean QA, and error analysis (Table 1).Scoring.Scoring is the most commonly used evaluation method in human evaluation for NLG (van der Lee et al. 2021), and it is naturally applied to LLM-based</p>
<p>Prompt</p>
<p>Please act as an impartial and helpful evaluator for natural language generation (NLG).You will be given a news article and a summary written for it.Your task is to evaluate the quality of the summary strictly based on the given evaluation criterion.Please make sure you read and understand these instructions carefully.</p>
<p>Evaluation Criterion: Consistency: the factual alignment between the summary and the summarized source.A factually consistent summary contains only statements that are entailed by the source document.Annotators were also asked to penalize summaries that contained hallucinated facts.</p>
<p>News Article: Paul Merson has restarted his row with Andros Townsend after the Tottenham midfielder was brought on with only seven minutes remaining in his team's 0-0 draw with Burnley on Sunday.'Just been watching the game, did you miss the coach?#RubberDub #7minutes,' Merson … Summary: Paul Merson was brought on with only seven minutes remaining in his team's 0-0 draw with Burnley.Andros Townsend scored the Tottenham midfielder in the 89th minute … Evaluation Form: Begin the evaluation by providing your analysis concisely and accurately, and then on the next line, start with "Rating:" followed by your rating on a Likert scale from 1 to 5 (higher means better).</p>
<p>Response</p>
<p>The summary contains several inconsistencies with the facts presented in the article.Firstly, it states that "Paul Merson was brought on with only seven minutes remaining in his team's 0-0 draw with Burnley," which is incorrect as it was Andros Townsend who was brought on in that context, not … Rating: 2</p>
<p>Figure 2</p>
<p>An example of prompting LLMs to evaluate the aspect of consistency of the summary.There are role and interaction, task instructions, evaluation criteria, input content, and evaluation methods in the prompt, as well as the evaluation results, including the rating and explanation generated by LLMs.</p>
<p>evaluation.Chiang and Lee (2023a) have conducted relevant studies early, using a Likert scale from 1 to 5 to evaluate story generation and adversarial attacks with InstructGPT (Lee 2024) and ChatGPT1 , showing that the evaluation results of LLMs are consistent with expert human evaluators.Kocmi and Federmann (2023b) discover GPT-3.5 and GPT-4 achieve the state-of-the-art accuracy of evaluating translation quality compared to human labels with a rating scale from 1 to 5 or 0 to 100, outperforming all the results from the metric shard task of WMT22 (Freitag et al. 2022).Furthermore, Wang et al. (2023a) experiment on five datasets across summarization, story generation, and data-to-text, and ChatGPT with similar rating scales achieves the state-of-the-art or comparative correlations with human judgments in most settings, compared with prior metrics.Similar conclusions are also observed in open-domain dialogue response generation (Lin and Chen 2023).Besides English, Mendonça et al. (2023) show that ChatGPT with simple rating prompts is a strong evaluator for multilingual dialogue evaluation, surpassing prior metrics based on encoders.</p>
<p>Comparison.Different from absolute scoring, comparison refers to choosing the better of the two.Luo, Xie, and Ananiadou (2023); Gao et al. (2023) use ChatGPT to compare the factual consistency of two summaries.AuPEL (Wang et al. 2023c) evaluate personalized text generation from three aspects in the form of comparison with the PaLM 2 family (Anil et al. 2023).According to Liusie, Manakul, and Gales (2024), pairwise comparison is better than scoring when medium-sized LLMs (e.g.FlanT5 (Chung et al. 2024) and Llama2 (Touvron et al. 2023)) are adopted as evaluators.</p>
<p>Ranking.Ranking can be viewed as an extended form of comparison.In comparison, only two examples are involved at a time, whereas in ranking, the order of more than two examples needs to be decided at once.Ji et al. (2023) use ChatGPT to rank five model-generated responses across several use cases at once, indicating the ranking preferences of ChatGPT align with those of humans to some degree.Similarly, GPTRank is a method to rank summaries in a list-wise manner (Liu et al. 2024b).Moreover, Liu et al. (2024a) compare different evaluation methods in LLM-based summarization including scoring, comparison, and ranking, showing that the optimal evaluation method for each backbone LLM may vary.</p>
<p>Boolean QA.Boolean QA requires LLMs to answer "Yes" or "No" to a question.It is adopted more in scenarios where human annotations are binary, such as grammaticality (Hu et al. 2023), faithfulness of summaries and statements (Luo, Xie, and Ananiadou 2023;Gao et al. 2023;ES et al. 2024;Hu et al. 2023), factuality of generated text (Fu et al. 2023;Guan et al. 2024;Manakul, Liusie, and Gales 2023), and answerability of generated questions (Wang, Funakoshi, and Okumura 2023).</p>
<p>Error Analysis.Error Analysis refers to the evaluation of a text by looking for errors that occur in the text according to a set of predefined error categories.Multidimensional Quality Metrics (MQM) (Jain et al. 2023) is an error analysis framework prevalent in machine translation evaluation.According to MQM, Lu et al. (2023); Kocmi and Federmann (2023a) use ChatGPT or GPT-4 to automatically detect translation quality error spans.BOOOOKSCORE (Chang et al. 2024), an LLM-based evaluation metric, assesses the coherence of book summaries by identifying eight types of errors.</p>
<p>Task Instructions</p>
<p>In human evaluation, task instruction usually comes in the form of a task description or evaluation steps.They can also exist at the same time.The task description states the annotation in a more general way, and the evaluation steps, which can be considered as Chain-of-Thought, explicitly describe what to do at each step.In the context of prompting LLMs for NLG evaluation, we discuss three broad categories of influences: various templates of prompts (Leiter et al. 2023;Kim et al. 2023;Kotonya et al. 2023;He, Zhang, and Roth 2024), in-context examples (Jain et al. 2023;Kotonya et al. 2023;Hasanbeig et al. 2023), and whether LLMs are required to provide analyses or explanations (Chiang and Lee 2023b;Naismith, Mulcaire, and Burstein 2023).</p>
<p>Form and requirements.Several studies from an Eval4NLP 2023 shared task (Leiter et al. 2023) have explored task instructions in various settings.For example, Kim et al. (2023) conduct experiments on different templates and lengths of task descriptions and evaluation steps, finding that providing clear and straightforward instructions, akin to those explained to humans, is more effective.Kotonya et al. (2023) generate task instructions with LLMs or improve existing task instructions with LLMs.Moreover, Leiter and Eger (2024) conduct a larger-scale prompt exploration for the evaluation of machine translation and summarization based on the Eval4NLP 2023 shared task.Somewhat differently, He, Zhang, and Roth (2024) evaluate generative reasoning using LLMs by asking them first to generate their own answers, and then conduct a quantitative analysis of the text to be evaluated.Additionally, explicit evaluation requirements and output formats are typically included in the instructions, and the evaluation results are extracted using regular expression matching.Early LLMs may sometimes provide unrecognizable evaluation results or refuse to conduct evaluation due to their limited instruction-following capabilities (Gao et al. 2023).This issue can be mitigated through multiple sampling or setting random outputs, and it basically does not exist in the currently more advanced and powerful LLMs.</p>
<p>Analysis and explanations.LLMs are able to include analysis or explanation in their evaluations, which is a key point that distinguishes them from previous automatic evaluation metrics.Early explorations into prompting LLMs for NLG evaluation mostly do not examine the impact of whether LLMs are required to analyze and explain on evaluation results.However, Chiang and Lee (2023b) explore different types of evaluation instructions in summarization evaluation and dialogue evaluation, finding that explicitly asking large models to provide analysis or explanation achieve higher correlation with human judgments.Besides, the quality of the analysis and explanation generated by LLMs itself requires additional manual evaluation (Leiter et al. 2023).Naismith, Mulcaire, and Burstein (2023) compare the explanations written by humans and generated by GPT-4 and conduct a simple corpus analysis on the generated explanations, finding that GPT-4 has strong potential to produce ratings that are comparable to human ratings on discourse coherence, accompanied by clear rationales.</p>
<p>In-context examples.Similarly to other fields, sometimes demonstrations are needed when prompting LLMs for NLG evaluation.Specifically, Jain et al. (2023) use only in-context examples as task instructions, relying on LLMs to evaluate the quality of summaries.In scenarios where task descriptions or evaluation steps are included, Kotonya et al. (2023) compare the performance of LLMs as evaluators in both zero-shot and one-shot settings, finding that one-shot prompting does not bring improvements.Moreover, Hasanbeig et al. (2023) improve the performance of LLM evaluators by updating the in-context examples iteratively.</p>
<p>Input Content</p>
<p>The types of input content mainly depend on the evaluation criteria and are relatively fixed.For most task-specific evaluation criteria, such as the faithfulness of a summary (Luo, Xie, and Ananiadou 2023;Gao et al. 2023), the source document is needed in addition to the target text to be evaluated.For task-independent criteria, such as fluency (Hu et al. 2023;Chiang and Lee 2023b), only the text to be evaluated needs to be provided, though many works also provide the source document (Wang et al. 2023a;Liusie, Manakul, and Gales 2024).Other types of input content can be provided as required by the specific task.Kocmi and Federmann (2023b) use two different settings when evaluating machine translation: providing references and not providing references and find that GPT-4 without references can also outperform all existing reference-based metrics.Guan et al. (2024) provide relevant facts and context when evaluating whether a text conforms to the facts.Exceptionally, Shu et al. (2024) add the output of other automatic evaluation metrics to the input of the LLM.</p>
<p>Evaluation Criteria</p>
<p>The evaluation targeting specific aspects is used in numerous studies of human evaluation for NLG, such as text summarization, story generation, dialogue, and text simplification.Evaluation criteria, i.e., the definitions of aspects are key in this context.Most evaluation criteria in LLM-based evaluation are directly derived from human evaluation.However, a few studies have attempted to let LLMs generate or improve evaluation criteria.Liu et al. (2024c) use a few human-rated examples as seeds to let LLMs draft some candidate evaluation criteria, and then further filter them based on the performance of LLMs using these criteria on a validation set, to obtain the final evaluation criteria.Kim et al. (2024d) designed an LLM-based interactive evaluation system, which involves using LLMs to review the evaluation criteria provided by users, including eliminating ambiguities in criteria, merging criteria with overlapping meanings, and decomposing overly broad criteria.Additionally, Ye et al. (2024) propose a hierarchical aspect classification system with 12 subcategories, demonstrating that under the proposed fine-grained aspect definitions, human evaluation and LLMbased evaluation are highly correlated.Besides, the chain-of-aspects approach improves LLMs' ability to evaluate on a specific aspect by having LLMs score on some related aspects before generating the final score (Gong and Mao 2023).</p>
<p>Role and Interaction</p>
<p>We include in this section the evaluation strategies that either use the same LLMs in different ways or involve different LLMs (Bai et al. 2023;Li, Patel, and Du 2023;Cohen et al. 2023).The former can be further divided into chain-style (Yuan et al. 2024;Fu et al. 2023;Hu et al. 2023) and network-style interactions (Chan et al. 2024;Zhang et al. 2023;Saha et al. 2024;Wu et al. 2023).</p>
<p>Chain-style interaction.Inspired by human evaluators, Yuan et al. (2024) have LLMs score a batch of examples to be evaluated each time.Specifically, the evaluation process is divided into three stages: analysis, ranking, and scoring.Similar to QA-based evaluation metrics (Durmus, He, and Diab 2020), Fu et al. (2023) assess the faithfulness of summaries in two stages: treating LLMs as question generators to generate a question from the summary; then having LLMs answer the question using the source document.Differently, when Hu et al. (2023) use GPT-4 to evaluate the faithfulness of summaries, it first asks GPT-4 to extract event units from the summary, then verifies whether these event units meet the requirements, and finally judges whether the event units are faithful to the source document.</p>
<p>Network-style interaction.Unlike chain-style interactions, network-style interactions involve the dispersion and aggregation of information.In network-style interactions, LLMs on the same layer play similar roles.ChatEval (Chan et al. 2024) is a framework for evaluating content through debates among multiple LLMs, with three communication strategies designed among the three types of LLMs: One-By-One, Simultaneous-Talk, and Simultaneous-Talk-with-Summarizer.Zhang et al. (2023) find that under certain conditions, widening and deepening the network of LLMs can better align its evaluation with human judgments.Saha et al. (2024) propose a branch-solvemerge strategy, assigning LLMs the roles of decomposing problems, solving them, and aggregating answers, thereby improving the accuracy and reliability of evaluations.Wu et al. (2023) assume that different people such as politicians and the general public have different concerns about the quality of news summaries, use LLMs to play different roles in evaluation accordingly, and aggregate the results finally.</p>
<p>Different LLMs.Different from having the same LLM play different roles, some research has used different LLMs (such as GPT-4 and Claude) in their studies.The use of a single LLM as evaluator may introduce bias, resulting in unfair evaluation results.In light of this, Bai et al. ( 2023) design a decentralized Peer-examination method, using different LLMs as evaluators and then aggregating the results.Further, Li, Patel, and Du (2023) let different LLMs serve as evaluators in pairwise comparisons and then have them go through a round of discussion to reach the final result.Additionally, Cohen et al. (2023) evaluate the factuality of texts through the interaction of two LLMs, where the LLM that generated the text acts as the examinee and the other LLM as the examiner.</p>
<p>Pros and Cons</p>
<p>The benefits of prompting LLMs for NLG evaluation are exciting.First, for the first time, people can express evaluation criteria and evaluation methods in natural language within the prompts given to LLMs, providing great flexibility.Where previously people needed to design specific evaluation metrics for different NLG tasks or even different aspects of a single task, now they only need to modify the prompts for LLMs.Secondly, surprisingly, LLMs have the ability to generate explanations while assessing texts, making this approach somewhat interpretable.Furthermore, in many NLG tasks, prompting LLMs for evaluation has achieved state-of-the-art correlations with human judgments.</p>
<p>However, as many studies have pointed out, this type of approach still has many limitations.Wang et al. (2024a) note that when using ChatGPT and GPT-4 for pairwise comparisons, the order of the two texts can affect the evaluation results, which is known as position bias.To alleviate this issue, Li et al. (2023b) propose a strategy of splitting, aligning, and then merging the two texts to be evaluated into the prompt.Also, LLM evaluators tend to favor longer, more verbose responses (Zheng et al. 2023) and responses generated by themselves (Liu et al. 2023a).Wu and Aji (2025) show that compared to answers that are too short or grammatically incorrect, answers with factual errors are considered better by LLMs.Liu et al. (2023b) demonstrate through adversarial meta-evaluation that LLMs without references are not suitable for evaluating dialogue responses in closed-ended scenarios: they tend to score highly on responses that conflict with the facts in the dialogue history.Zhang et al. (2024) also present the robustness issues of LLMs in dialogue evaluation through adversarial perturbations.Shen et al. (2023) indicate that LLM evaluators have a lower correlation with human assessments when scoring high-quality summaries.In addition, Hada et al. (2024b) state that LLMbased evaluators have a bias towards high scores, especially in non-Latin languages like Chinese and Japanese.Bavaresco et al. (2024) find that the performance of LLM-based evaluators exhibits significant variance depending on the dataset, evaluation criteria, and whether the evaluated texts are human-generated.Beyond these shortcomings of performance, both ChatGPT and GPT-4 are proprietary models, and their opacity could lead to irreproducible evaluation results.</p>
<p>Fine-tuning LLMs</p>
<p>As mentioned above, despite the exciting performance of prompting LLMs like Chat-GPT and GPT-4 for NLG evaluation, several shortcomings in practice are inevitable, such as high costs, possibly irreproducible results, and potential biases in LLMs.In response, recent research has shifted towards fine-tuning smaller, open-source LLMs specifically for evaluation purposes, aiming to achieve performance close to GPT-4 in NLG evaluation.Representative works of this type include PandaLM (Wang et (Hu et al. 2024), CompassJudger-1 (Cao et al. 2024) and Self-Taught (Wang et al. 2024b).Their main ideas are similar, involving the elaborate construction of high-quality evaluation data, followed by fine-tuning open-source foundation LLMs with specific methods.Nevertheless, there are certain discrepancies in the designs across different works, such as the usage of references and evaluation criteria.We have summarized the key different components of these methods in Table 2 and Table 3 for comparison, which will be elaborated in the following sections.</p>
<p>Data Construction</p>
<p>Diverse data with high-quality annotations is crucial for the fine-tuning of evaluation models, which mainly involves task scenarios, inputs, target texts to evaluate, and evaluation results.Early NLG evaluation research primarily focused on conventional NLG tasks, such as summarization and dialogue generation.Thus, the task scenarios, inputs, and target texts refer to the corresponding NLP task, source inputs of the task, and outputs generated by specialized systems based on task requirements, respectively.And mainstream datasets for these tasks predominantly employ human annotators to provide evaluation results, which are often considered reliable.</p>
<p>With the recent rise of LLMs, the spectrum of NLG tasks has been broadened to scenarios of instruction and response that are more aligned with human needs.Traditional tasks like summarization with corresponding source inputs can be viewed as kinds of instructions and requirements.Meanwhile, responses generated by various general LLMs generally serve as the target texts now and require more flexible evaluation so that the performance of different LLMs can be compared, promoting further developments.Therefore, to keep pace with the current advancement of modeling techniques, most evaluation methods have adopted the similar instruction-response scenario.</p>
<p>The primary differences in these works actually lie in the construction of instructions, with the purpose of improving either diversity or reliability for the better generalization ability of the fine-tuned model.PandaLM and JudgeLM entirely sample from common instruction datasets, such as Alpaca 52K, while CritiqueLLM adopts small-scale sampling followed by ChatGPT augmentation.In contrast, Prometheus and INSTRUCTSCORE rely on GPT-4 to generate all the instructions based on seed data, whereas Auto-J and Shepherd use real-world data.Moreover, since large-scale human annotation is impractical, most works utilize GPT-4 as the powerful annotator, except for PandaLM and Shepherd, which use GPT-3.5 and human annotation on small-scale community data, respectively.Specifically, Themis focuses on NLG tasks and combines existing human evaluations with additional evaluations from GPT-4, selecting more consistent training data.Self-Taught uses the evaluation results from the model to finetune itself (Llama-3-70B), considering it already possesses strong capabilities.During the construction, these studies basically all design detailed prompts or guidance and apply heuristic filtering strategies and post-processing methods to mitigate noise.Overall, despite the possible higher quality of human annotation, the corresponding drawback is the difficulty in constructing large-scale datasets, which in turn may hinder adequate model training, while using LLMs for construction is the opposite situation.</p>
<p>Evaluation Method</p>
<p>As with prompting LLMs, the evaluation methods adopted in these works are highly diversified, involving different evaluation criteria, result modes, and usages of the reference.Given that current instruction-response scenarios encompass different types of tasks, it is unsuitable to specify unified evaluation criteria as in traditional NLG tasks.However, some works still do it this way, while some other methods let LLM annotators adaptively and implicitly reflect the required criteria in their evaluations, like PandaLM, TIGERScore, and AUTO-J.In particular, AUTO-J has meticulously crafted 332 evaluation criteria, matched to different tasks.Furthermore, Prometheus and Themis explicitly incorporate evaluation criteria into the evaluation instructions, and CompassJudger can work either with or without evaluation criteria, enabling flexible evaluation based on various customized criteria.</p>
<p>More details about the evaluation methods are shown in Table 3.All the works require models to provide detailed information, such as reasons for their evaluation results.And the MQM mode can achieve more informative error analysis, offering stronger interpretability.Moreover, some works do not necessarily require references and then have greater value in practice.And a more optimal method is to concurrently support both reference-based and reference-free evaluations as JudgeLM and CritiqueLLM.</p>
<p>Fine-tuning Implementation</p>
<p>The fine-tuning process is implemented by different studies on their selected opensource foundation LLMs, like LLaMA, and respective constructed data, with some targeted settings.Specifically, Prometheus maintains balanced data distributions during fine-tuning, including the length and label.JudgeLM eliminates potential biases by randomly swapping sample pairs to be compared and randomly removing references.INSTRUCTSCORE utilizes GPT-4 to provide error annotations for the intermediate outputs of the fine-tuned model for further supervised reinforcement.And based on some preliminary experiments and manual analysis, TIGERScore determines appropriate ratios of different types of data during fine-tuning, which are claimed to be crucial by them.Moreover, CritiqueLLM implements separately, with and without references, and explores the effects of data and model scale.Themis employs additional ratingguided preference optimization after the fine-tuning process.Specifically, Self-Taught utilizes the evaluation results of the fine-tuned model itself for self-iterative optimization, leading to surprising improvements.Compared to the vanilla fine-tuning setting, these methods have improved the efficiency of model training and the robustness of evaluations.</p>
<p>Pros and Cons</p>
<p>The shortcomings of prompting LLMs for NLG evaluation can be significantly alleviated by the customized construction of training data and specifically fine-tuned LLMs.For instance, most models in Table 2 have less than 14B parameters, facilitating low-cost inference in practice and good reproducibility, with performance comparable to GPT4.And specific measures can be adopted to prevent certain biases found in GPT4 during different stages, such as randomly changing the order of training pairs for position bias.Furthermore, this type of approach allows for continuous iteration and improvement of the model to address potential deficiencies or emerging issues discovered in future applications.</p>
<p>However, some inherent biases associated with GPT4 may still persist, like selfbiases, as the data construction of most methods employs GPT4 for critical evaluation annotation.On the other hand, many studies have chosen open-source foundation LLMs spanning three generations of the Llama series.With the recent rapid updates and improvements of open-source LLMs, it is intuitive that employing a more powerful foundation LLM should lead to better evaluation performance of the fine-tuned model.However, this means repetitive fine-tuning processes and computational expenses from scratch since directly migrating existing fine-tuned models to the new foundation LLM is challenging.</p>
<p>Additionally, although many existing methods aspire to more flexible and comprehensive evaluation through fine-tuning, demanding excessive evaluation settings may ultimately lead to poor performance or failure in model training, as AUTO-J and CritiqueLLM were found to have difficulties with criteria and references, respectively.However, there are some disagreements here since Prometheus, JudgeLM, and Com-passJudger show different results, indicating such a seemingly straightforward finetuning process is actually quite complex.Moreover, considering the different evaluation settings in existing works, conducting a horizontal comparison among them is challenging.These issues require further exploration in future research.</p>
<p>Human-LLM Collaborative Evaluation</p>
<p>Human evaluation remains the gold standard for NLG due to its ability to capture nuanced aspects of quality.However, it is expensive, time-consuming, and prone to subjective biases (van der Lee et al. 2021;Deriu et al. 2021;Li et al. 2023a).While LLMs offer a promising avenue for automated evaluation, their reliability and correlation with human judgment are still areas of active development (Li et al. 2023b;Liu et al. 2023b).Human-LLM collaborative evaluation seeks to leverage the strengths of both: the nuanced judgment of humans and the scalability and efficiency of LLMs.This chapter explores emerging paradigms in this collaborative space, focusing on how humans and LLMs can work together to improve the accuracy, efficiency, and trustworthiness of NLG evaluation.This includes collaborative approaches like: traditional evaluation tasks such as scoring and explaining (Zhang, Ren, and de Rijke 2021;Li et al. 2023a); general evaluation tasks such as testing and debugging (Ribeiro and Lundberg 2022); auditing NLG models to ensure fairness (Rastogi et al. 2023); aligning LLM-assisted evaluation of LLM outputs with human preferences (Shankar et al. 2024); addressing the intricate challenge of scalable oversight (Raji and Dobbe 2024;Saunders et al. 2022).</p>
<p>Human-Guided LLM Evaluation</p>
<p>Some works (Zhang, Ren, and de Rijke 2021;Li et al. 2023a) focus on approaches where LLMs perform the primary evaluation task, but with significant guidance and oversight from humans.This guidance can take several forms, from designing detailed evaluation criteria to refining LLM outputs.</p>
<p>One common method is called checklist-based evaluation.A key challenge in openended NLG tasks is the lack of consistent evaluation criteria.Li et al. (2023a) address this with COEVAL, a collaborative pipeline where humans design a task-specific checklist.LLMs then use this checklist to generate initial evaluations and explanations, drawing on developments in explainable NLP (Yin and Neubig 2022;Jung et al. 2022;Ribeiro and Lundberg 2022;Ye et al. 2023).Humans then scrutinize these LLM-generated evaluations, refining scores and explanations.This approach leverages the LLM's ability to process large amounts of text while retaining human oversight to ensure accuracy and reduce outliers.Notably, human review still leads to revisions in approximately 20% of LLM scores, highlighting the importance of human judgment.Furthermore, InteractEval (Chu, Kim, and Yi 2025) combines human and LLM-generated attributes using Think Aloud methods to create questions and produce final prediction scores.Think Aloud methods mean that human experts verbalize their thoughts and LLMs articulate their knowledge to generate text attribute insights using sample texts and evaluation rubrics, which highlights the necessity of effectively combining humans and LLMs in an automated checklist-based text evaluation.</p>
<p>Collaborative assignment is also useful for human-guided LLM evaluation.Zhang, Ren, and de Rijke (2021) propose HMCEval, a framework that frames dialogue evaluation as a sample assignment problem.This approach aims to optimize the allocation of evaluation tasks between humans and machines to maximize accuracy while minimizing human effort.HMCEval achieves high accuracy (99%) with significantly reduced human involvement (half the effort).Besides, EvalAssist (Ashktorab et al. 2024) can help practitioners refine evaluation criteria using both direct and pairwise assessment strategies.Ashktorab et al. (2024) also examine how users refine their criteria and identify key differences between the two evaluation approaches examined how users refine their criteria and identified key differences between the two evaluation approaches.</p>
<p>LLM-Assisted Human Evaluation</p>
<p>Some works (Ribeiro and Lundberg 2022;Rastogi et al. 2023;Pozdniakov et al. 2024) explore scenarios where humans remain the primary evaluators, but LLMs provide assistance to improve efficiency, identify flaws, or audit for biases.Ribeiro and Lundberg (2022) introduce AdaTest, a system where LLMs generate unit tests to identify bugs in a target NLG model.Human feedback guides the LLM, significantly increasing the effectiveness of bug detection (5-10x improvement).This demonstrates the power of LLMs in generating diverse test cases, guided by human intuition.In the task of evaluating machine translation systems, Zouhar, Kocmi, and Sachan (2024) assist annotators by pre-filling error annotations with recall-oriented automatic quality estimation, which achieves the effect of reducing the time per span annotation by half while maintaining the same annotation quality level and further cutting the annotation budget by almost 25%.</p>
<p>Addressing biases and irresponsible behavior in LLMs is crucial (Blodgett et al. 2020;Jones and Steinhardt 2022).AdaTest++ (Rastogi et al. 2023), drawing on human-AI collaboration research, facilitates collaborative auditing.Humans leverage their strengths in schematization and hypothesis testing, while LLMs assist in identifying a wide range of failure modes.This collaborative approach uncovered both previously known and under-reported issues.</p>
<p>Evaluating LLMs on complex tasks can be challenging even for humans (Chen et al. 2021;Nakano et al. 2021;Li et al. 2022;Menick et al. 2022).The concept of scalable oversight (Raji and Dobbe 2024) suggests using AI to assist in evaluation.Saunders et al. (2022) explore using LLM-generated critiques to help humans identify flaws in model outputs, demonstrating that this form of assistance improves human performance.What's more, Pozdniakov et al. (2024) focus on designing conversational user interfaces, which helps educators to use LLMs to evaluate assignments of students.</p>
<p>Pros and Cons</p>
<p>Human-LLM collaborative evaluation offers a compelling balance between the accuracy of human judgment and the efficiency of automated methods.Key advantages include: (1) Efficiency and Cost-Effectiveness: LLMs can significantly reduce the time and resources required for evaluation.(2) Complementary Strengths: Humans excel at nuanced judgment and critical thinking, while LLMs excel at processing large amounts of data and generating diverse outputs.(3) Improved Accuracy: Combining human and LLM strengths can lead to more accurate and reliable evaluations than either approach alone.</p>
<p>However, challenges remain: (1) Prompt Sensitivity: LLM evaluation results can be sensitive to the phrasing of prompts, requiring careful prompt engineering (Li et al. 2023a;Rastogi et al. 2023).( 2) Confidence Calibration: LLMs' ability to accurately assess their own confidence is still limited, making it difficult to know when to trust their judgments.(3) Need for Human Oversight: While reduced, human supervision is still necessary, limiting the potential for full automation.(4) Explainability: Ensuring the collaborative process is transparent and understandable can be challenging.</p>
<p>Conclusions and Future Trends</p>
<p>Comparison with traditional evaluation metrics.</p>
<p>Traditional evaluation metrics are criticized for their poor correlation with human judgments (Stent, Marge, and Singhai 2005), uninterpretable evaluation results (Zhang, Vogel, and Waibel 2004), and inability to adapt to specific evaluation criteria (Wiseman, Shieber, and Rush 2017), which are being greatly mitigated by LLM-based evaluation.However, the higher cost, the requirements for computing resources, and the issues of reproducibility may be the downside.</p>
<p>Comparison between different types of LLM-based NLG evaluation.</p>
<p>We compare different types of LLM-based evaluation according to flexibility and reproducibility due to the difficulty of comparing the effectiveness of different types of methods in various scenarios.</p>
<p>Flexibility.Human-LLM Collaborative Evaluation &gt; Prompting LLMs &gt; Fine-tuning LLMs &gt; LLM-derived Metrics.Human-LLM Collaborative Evaluation involves human annotators, which provides the highest flexibility.LLM-derived Metrics are typically designed to evaluate specific aspects, such as text similarity, and do not fully allow Performance.Human-LLM Collaborative Evaluation &gt; Fine-tuning LLMs ≈ Prompting LLMs &gt; LLM-derived Metrics.We compare the performance of different LLM-based evaluation approaches on the most commonly used NLG evaluation benchmark on summarization, SummEval (Fabbri et al. 2021), as shown in Table 4.When using the same LLMs, LLM-derived metrics perform worse than directly prompting LLMs for evaluation, and the latter is more convenient.Moreover, among methods of fine-tuning LLMs, only models focused on NLG evaluation scenarios, such as Themis, outperform prompting-based methods, including that using GPT-4.Other studies either use relatively outdated foundation LLMs or lack training on specific evaluation aspects like those in SummEval, leading to relatively weaker performance.Furthermore, Human-LLM Collaborative Evaluation enhances the LLM evaluation by incorporating checklists elaborated with human expert insights and LLM knowledge, resulting in the strongest performance.</p>
<p>Cost.LLM-derived Metrics ≈ Prompting open-source LLMs &lt; Fine-tuning LLMs ≈ Prompting proprietary LLMs &lt; Human-LLM Collaborative Evaluation.When using the same open-source LLM, the inference costs of LLM-derived metrics, prompting LLM, and fine-tuning LLM methods are the same, while fine-tuning LLM incurs additional training costs.When prompting proprietary LLMs, the cost is high and mainly concentrated in API calls during evaluation, making it difficult to directly compare with the training cost required for fine-tuning LLM.Moreover, human-LLM collaborative evaluation requires the involvement of human experts for each task, making it the most expensive approach.</p>
<p>Future Directions</p>
<p>Unified benchmarks for LLM-based NLG evaluation approaches.As mentioned above, each of the studies that fine-tuned LLMs to construct specialized evaluation models uses different settings and data during testing, making them incomparable.In the research on prompting LLMs for NLG evaluation, there are some publicly available human judgments on the same NLG task, such as SummEval for summarization.However, the existing human judgments have many problems.Firstly, most of the existing data only involve one type of NLG task and a single human evaluation method (e.g., scoring), making it difficult to evaluate LLMs' performance on different tasks, as well as using different evaluation methods on the same task.Secondly, many of the texts in these human judgments are generated by outdated models (such as Pointer Network) and do not include texts generated by more advanced LLMs.Lastly, many human evaluation datasets are too small in scale.There is an urgent need for large-scale, highquality human evaluation data covering various NLG tasks and evaluation methods as a benchmark.</p>
<p>NLG evaluation for low-resource languages and new task scenarios.Almost all existing research focuses on English data.However, it is doubtful whether LLMs have similar levels of NLG evaluation capability for texts in other languages, especially lowresource languages.As (Zhang et al. 2024) points out, we should be more cautious about using LLMs to evaluate texts in non-Latin languages.We believe that the lack of evaluation capability of LLM-based evaluators on low-resource languages may be due to the insufficient presence of these languages in the pretraining corpus.Therefore, further fine-tuning on certain low-resource languages may be a potential strategy to address this issue, and Hada et al. (2024a) have already shown promising preliminary results.Additionally, existing research mainly focuses on more traditional NLG tasks such as translation, summarization, and dialogue.However, there are many new scenarios in reality with different requirements and evaluation criteria.For example, using LLMs to automatically evaluate scientific reviews could be valuable in identifying and flagging content that is unfaithful or unclear, alerting reviewers to potential issues.Research on low-resource languages and new task scenarios will provide a more comprehensive understanding of LLMs' evaluation capabilities.</p>
<p>Diverse forms of human-LLM collaborative NLG evaluation.According to the literature reviewed above, there is little research on collaborative evaluation between humans and LLMs.Neither humans nor LLMs are perfect, and each has its strengths.Since the ultimate goal of NLG research is to evaluate text quality more accurately and efficiently, we believe that collaboration between humans and LLMs can achieve better results than pure human evaluation or automatic evaluation.In the collaboration between humans and LLMs, technologies in the field of human-computer interaction may bring new implementation methods to the collaboration.In addition, what roles humans and LLMs should play in the evaluation and how they can better complement each other are still worth researching.</p>
<p>Figure 1 Schematic representation of our proposed four categories of LLM-based NLG evaluation.</p>
<p>Table 1
1Related WorkEvaluation MethodNLG TaskChiang and Lee (2023a)ScoringStory GenerationWang et al. (2023a)ScoringSummarization, Data-to-text &amp; Story GenerationKocmi and Federmann (2023b)ScoringTranslationLin and Chen (2023)ScoringDialogueMendonça et al. (2023)ScoringDialogueNaismith, Mulcaire, and Burstein (2023)ScoringDiscourse GenerationLiusie, Manakul, and Gales (2024)Scoring &amp; ComparisonSummarization, Dialogue &amp; Data-to-textWang et al. (2023c)ComparisonPersonalized Text GenerationJi et al. (2023)RankingOpen-end Text GenerationLiu et al. (2024b)Scoring, Ranking &amp; ComparisonSummarizationWang, Funakoshi, and Okumura (2023)Boolean QAQuestion GenerationManakul, Liusie, and Gales (2023)Boolean QAFact VerificationGuan et al. (2024)Boolean QAFact VerificationES et al. (2024)Boolean QARetrieval Augmented GenerationKocmi and Federmann (2023a)Error AnalysisTranslationLu et al. (2023)Error AnalysisTranslationChang et al. (2024)Error AnalysisSummarization</p>
<p>al.
Data ConstructionMethodInstruction SourceAnnotatorScaleFoundation LLMPandaLMAlpaca 52KGPT-3.5300KLLaMA 7BPrometheusGPT-4 ConstructionGPT-4100KLLaMA-2-Chat 7B &amp; 13BPrometheus 2FEEDBACK COLLECTIONGPT-4200KMistral-7B Mixtral-8x7BShepherdCommunity Critique Data &amp; 9 NLP Tasks DataHuman1317LLaMA 7BTIGERScore23 Distinctive Text Generation DatasetsGPT-448KLLaMA-2 7B &amp; 13BINSTRUCTSCOREGPT-4 ConstructionGPT-440KLLaMA 7BAUTO-JReal-world User Queries from Preference DatasetsGPT-44396LLaMA-2-Chat 13BCritiqueLLMAlignBench &amp; ChatGPT AugmentationGPT-49332ChatGLM-2 6B, 12B &amp; 66BJudgeLMGPT4All-LAION, ShareGPT Alpaca-GPT4 &amp; Dolly-15KGPT-4100KVicuna 7B, 13B &amp; 33BThemisNLG-Eval with 58 NLG Evaluation DatasetsHuman &amp; GPT-467KLlama-3-8BSelf-TaughtScreened WildChatLlama-3-70B20KLlama-3-70BCompassJudger-1Sampling from existing datasetsMixture900KQwen-2.5 1.5B, 7B, 14B &amp; 32B</p>
<p>Table 2
2
Comparison of the different key components among the representative methods of fine-tuning LLMs (Part 1).</p>
<p>(Li et al. 2024)24)Kim et al. 2024a), Prometheus 2(Kim et al. 2024c), Shepherd(Wang et al. 2023b), TIGERScore(Jiang et al. 2024), INSTRUCTSCORE(Xu et al. 2023), Auto-J(Li et al. 2024), CritiqueLLM(Ke et al. 2023), JudgeLM (Zhu, Wang, and Wang 2023), Themis</p>
<p>Table 3
3
Comparison of the different key components among the representative methods of fine-tuning LLMs (Part 2).
MethodResult ModeEvaluation Method DetailsSpecific CriteriaReference RequiredPandaLMComparisonReason &amp; ReferenceUnifiedNoPrometheusScoringReasonExplicitYesPrometheus 2Scoring &amp; ComparisonReasonExplicitYesShepherdOverall JudgementError Identifying &amp; RefinementUnifiedNoTIGERScoreMQMError AnalysisImplicitNoINSTRUCTSCOREMQMError AnalysisImplicitYesAUTO-JScoring &amp; ComparisonReasonImplicitNoCritiqueLLMScoringReasonUnifiedFlexibleJudgeLMScoring &amp; ComparisonReasonUnifiedFlexibleThemisScoringReasonExplicitNoSelf-TaughtComparisonReasonUnifiedNoCompassJudger-1Scoring &amp; ComparisonReasonExplicitNo</p>
<p>. Zhu, Lianghui, Xinggang Wang, and Xinlong Wang.2023.Judgelm: Fine-tuned large language models are scalable judges.CoRR, abs/2310.17631.Zhuang, Ziyu, Qiguang Chen, Longxuan Ma, Mingda Li, Yi Han, Yushan Qian, Haopeng Bai, Zixian Feng, Weinan Zhang, and Ting Liu.2023.Through the lens of core competency: Survey on evaluation of large language models.CoRR, abs/2308.07902.Zouhar, Vilém, Tom Kocmi, and Mrinmaya Sachan.2024.Ai-assisted human evaluation of machine translation.CoRR, abs/2406.12419.
Linguistics
https://openai.com/blog/chatgpt/
AcknowledgmentsThis work was supported by Beijing Science and Technology Program (Z231100007423011) andChu, Kim, and Yi (2025).evaluation criteria to be expressed in natural language, making them the least flexible.When comparing Prompting LLMs and Fine-tuning LLMs, the former, which uses proprietary models, generally performs better at following instructions compared to smaller open-source models.Reproducibility.LLM-derived Metrics ≈ Prompting LLMs &gt; Fine-tuning LLMs &gt; Human-LLM Collaborative Evaluation.Human-LLM Collaborative Evaluation requires human annotators, and the recruitment and training of these annotators pose greater challenges to reproducibility.LLM-derived Metrics and Prompting LLMs do not modify the existing models, and therefore have better reproducibility than Fine-tuning LLMs.However, they may still become non-reproducible if proprietary models are deprecated.
Llms instead of human judges? A large scale empirical study across 20 NLP evaluation tasks. Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya K Surikuchi, Ece Takmaz, Alberto Testoni, CoRR, abs/2406.184032024</p>
<p>Language (technology) is power: A critical survey of "bias" in NLP. Su Blodgett, Solon Lin, Hal Barocas, Iii Daumé, Hanna M Wallach, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020OnlineAssociation for Computational Linguistics2020. July 5-10, 2020</p>
<p>Compassjudger-1: All-in-one judge model helps model evaluation and evolution. Maosong Cao, Alexander Lam, Haodong Duan, Hongwei Liu, Songyang Zhang, Kai Chen, CoRR, abs/2410.162562024</p>
<p>Chatgpt to replace crowdsourcing of paraphrases for intent classification: Higher diversity and comparable model robustness. Ján Cegin, Jakub Simko, Peter Brusilovsky, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Evaluation of text generation: A survey. Asli Celikyilmaz, Elizabeth Clark, Jianfeng Gao, 2020. 2006.14799</p>
<p>Chateval: Towards better llm-based evaluators through multi-agent debate. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaOpenReview2024. May 7-11, 2024net</p>
<p>Booookscore: A systematic exploration of book-length summarization in the era of llms. Yapei Chang, Kyle Lo, Tanya Goyal, Mohit Iyyer, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaOpenReview2024. May 7-11, 2024net</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Felipe Petroski Such. Joshua Achiam, Vedant Misra, Evan MorikawaJan LeikeIlya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374</p>
<p>Can large language models be an alternative to human evaluations?. David Chiang, Hung-Yi Cheng-Han, Lee, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a. July 9-14, 20231ACL 2023</p>
<p>A closer look into using large language models for automatic evaluation. David Chiang, Hung-Yi Cheng-Han, Lee, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023b. December 6-10, 2023</p>
<p>Think together and work better: Combining humans' and llms' think-aloud outcomes for effective text evaluation. Seongyeub Chu, Jong Woo Kim, Mun Yong, Yi , Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, CHI 2025. the 2025 CHI Conference on Human Factors in Computing Systems, CHI 2025ACM2025. 26 April 2025-1 May 2025108923</p>
<p>Scaling instruction-finetuned language models. Hyung Chung, Le Won, Shayne Hou, Barret Longpre, Yi Zoph, William Tay, Yunxuan Fedus, Xuezhi Li, Mostafa Wang, Siddhartha Dehghani, Albert Brahma, Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Y Yu, Yanping Zhao, Andrew M Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, J. Mach. Learn. Res. 25532024</p>
<p>LM vs LM: detecting factual errors via cross examination. Roi Cohen, May Hamri, Mor Geva, Amir Globerson, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Survey on evaluation methods for dialogue systems. Jan Deriu, Álvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset, Eneko Agirre, Mark Cieliebak, Artif. Intell. Rev. 5412021</p>
<p>FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. Esin Durmus, He He, Mona T Diab, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020OnlineAssociation for Computational Linguistics2020. July 5-10, 2020</p>
<p>Frugalscore: Learning cheaper, lighter and faster evaluation metrics for automatic text generation. Moussa Eddine, Guokan Kamal, Antoine J Shang, .-P Tixier, Michalis Vazirgiannis, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022. May 22-27, 20221ACL 2022</p>
<p>Ragas: Automated evaluation of retrieval augmented generation. Shahul Es, Jithin James, Luis Espinosa Anke, Steven Schockaert, Proceedings of the 18th Conference of the European Chapter. the 18th Conference of the European ChapterMaltaAssociation for Computational Linguistics2024. March 17-22, 2024EACL 2024 -System Demonstrations, St. Julians</p>
<p>Summeval: Re-evaluating summarization evaluation. Alexander R Fabbri, Wojciech Kryscinski, Bryan Mccann, Caiming Xiong, Richard Socher, Dragomir R Radev, Trans. Assoc. Comput. 2021</p>
<p>Results of WMT22 metrics shared task: Stop using BLEU -neural metrics are better and more robust. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-Kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George F Foster, Alon Lavie, F T André, Martins, Proceedings of the Seventh Conference on Machine Translation, WMT 2022. the Seventh Conference on Machine Translation, WMT 2022Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-8, 2022</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics2024. June 16-21, 20241NAACL 2024</p>
<p>Are large language models reliable judges? A study on the factuality evaluation capabilities of llms. Xue- Fu, Md Yong, Cheng Tahmid Rahman Laskar, Shashi Chen, T N Bhushan, CoRR, abs/2311.006812023</p>
<p>Human-like summarization evaluation with chatgpt. Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, Xiaojun Wan, CoRR, abs/2304.025542023</p>
<p>Sebastian Gehrmann, Tosin P Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu Anuoluwapo, Antoine Bosselut, Raghavi Khyathi, Miruna-Adriana Chandu, Dipanjan Clinciu, Kaustubh D Das, Wanyu Dhole, Esin Du, Ondrej Durmus, Chris Dusek, Varun Emezue, Cristina Gangal, Tatsunori Garbacea, Yufang Hashimoto, Yacine Hou, Harsh Jernite, Yangfeng Jhamtani, Shailza Ji, Dhruv Jolly, Faisal Kumar, Aman Ladhak, Mounica Madaan, Khyati Maddela, Saad Mahajan, Mahamood, Prasad Bodhisattwa, Pedro Henrique Majumder, Angelina Martins, Simon Mcmillan-Major, Mille, Moin Emiel Van Miltenburg, Shashi Nadeem, Vitaly Narayan, Rubungo Nikolaev, Salomey Andre Niyongabo, Osei, P Ankur, Laura Parikh, Niranjan Perez-Beltrachini, Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla, Hendrik Cabezudo, Nishant Strobelt, Wei Subramani, Diyi Xu, Akhila Yang, Jiawei Yerukola, Zhou, CoRR, abs/2102.01672The GEM benchmark: Natural language generation, its evaluation and metrics. 2021</p>
<p>Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text. Sebastian Gehrmann, Elizabeth Clark, Thibault Sellam, J. Artif. Intell. Res. 772023</p>
<p>Chatgpt outperforms crowd-workers for text-annotation tasks. Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli, CoRR, abs/2303.150562023</p>
<p>Coascore: Chain-of-aspects prompting for NLG evaluation. Peiyuan Gong, Jiaxin Mao, CoRR, abs/2312.103552023</p>
<p>Language models hallucinate, but may excel at fact verification. Jian Guan, Jesse Dodge, David Wadden, Minlie Huang, Hao Peng, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics2024. June 16-21, 20241NAACL 2024</p>
<p>METAL: towards multilingual meta-evaluation. Rishav Hada, Varun Gumma, Mohamed Ahmed, Kalika Bali, Sunayana Sitaram, Findings of the Association for Computational Linguistics: NAACL 2024. Mexico City, MexicoAssociation for Computational Linguistics2024a. June 16-21, 2024</p>
<p>Are large language model-based evaluators the solution to scaling up multilingual evaluation?. Rishav Hada, Varun Gumma, Adrian De Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram, Findings of the Association for Computational Linguistics: EACL 2024. St. Julian's, MaltaAssociation for Computational Linguistics2024b. March 17-22, 2024</p>
<p>Human evaluation of creative NLG systems: An interdisciplinary survey on recent papers. Mika Hämäläinen, Khalid Al-Najjar, CoRR, abs/2108.003082021</p>
<p>ALLURE: auditing and improving llm-based evaluation of text using iterative in-context-learning. Hosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Felipe Vieira Frujeri, Ida Momennejad, CoRR, abs/2309.137012023</p>
<p>Socreval: Large language models with the socratic method for reference-free reasoning evaluation. Hangfeng He, Hongming Zhang, Dan Roth, Findings of the Association for Computational Linguistics: NAACL 2024. Mexico City, MexicoAssociation for Computational Linguistics2024. June 16-21, 2024</p>
<p>On the blind spots of model-based evaluation metrics for text generation. Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James R Glass, Yulia Tsvetkov, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023a. July 9-14, 20231ACL 2023</p>
<p>On the blind spots of model-based evaluation metrics for text generation. Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James R Glass, Yulia Tsvetkov, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b. July 9-14, 20231ACL 2023</p>
<p>Themis: A reference-free NLG evaluation language model with flexibility and interpretability. Xinyu Hu, Li Lin, Mingqi Gao, Xunjian Yin, Xiaojun Wan, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, FL, USAAssociation for Computational Linguistics2024. 2024. November 12-16, 2024</p>
<p>Decipherpref: Analyzing influential factors in human preference judgments via GPT-4. Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Hassan Foroosh, Fei Liu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Multi-dimensional evaluation of text summarization with in-context learning. Sameer Jain, Vaishakh Keshava, Mysore Swarnashree, Patrick Sathyendra, Pengfei Fernandes, Graham Liu, Chunting Neubig, Zhou, Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 2023</p>
<p>Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences. Yunjie Ji, Yan Gong, Yiping Peng, Chao Ni, Peiyan Sun, Dongyu Pan, Baochang Ma, Xiangang Li, CoRR, abs/2303.076102023</p>
<p>Zero-shot faithfulness evaluation for text summarization with foundation language model. Qi Jia, Siyu Ren, Yizhu Liu, Kenny Q Zhu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Tigerscore: Towards building explainable metric for all text generation tasks. Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, Wenhu Chen, Trans. Mach. Learn. Res. 2024. 2024</p>
<p>Capturing failures of large language models via human cognitive biases. Erik Jones, Jacob Steinhardt, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>Maieutic prompting: Logically consistent reasoning with recursive explanations. Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 20222022</p>
<p>Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation. Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang, CoRR, abs/2311.187022023</p>
<p>Which is better? exploring prompting strategy for llm-based metrics. Joonghoon Kim, Sangmin Lee, Seung Hun Han, Saeran Park, Jiyoon Lee, Kiyoon Jeong, Pilsung Kang, Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems. the 4th Workshop on Evaluation and Comparison of NLP SystemsBali, IndonesiaAssociation for Computational Linguistics2023. Eval4NLP 2023. November 1, 2023</p>
<p>Prometheus: Inducing fine-grained evaluation capability in language models. Seungone Kim, Jamin Shin, Yejin Choi, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaOpenReview2024a. May 7-11, 2024net</p>
<p>The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models. Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Choi, Sheikh Shafayat, Jinheon Baek, Sue , Hyun Park, Hyeonbin Hwang, Jinkyung Jo, Hyowon Cho, Haebin Shin, Seongyun Lee, Hanseok Oh, Noah Lee, Namgyu Ho, June Se, Miyoung Joo, Yoonjoo Ko, Lee, Chae Hyungjoo, CoRR, abs/2406.05761Bill Yuchen Lin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo2024bJamin Shin, Joel Jang, Seonghyeon Ye</p>
<p>Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, CoRR, abs/2405.015352024c</p>
<p>Evallm: Interactive evaluation of large language model prompts on user-defined criteria. Tae Kim, Yoonjoo Soo, Jamin Lee, Young-Ho Shin, Juho Kim, Kim, Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024. the CHI Conference on Human Factors in Computing Systems, CHI 2024Honolulu, HI, USAACM2024d. May 11-16, 202430621</p>
<p>GEMBA-MQM: detecting translation quality error spans with GPT-4. Tom Kocmi, Christian Federmann, Proceedings of the Eighth Conference on Machine Translation, WMT 2023. the Eighth Conference on Machine Translation, WMT 2023SingaporeAssociation for Computational Linguistics2023a. December 6-7, 2023</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, Proceedings of the 24th Annual Conference of the European Association for Machine Translation, EAMT 2023. the 24th Annual Conference of the European Association for Machine Translation, EAMT 2023Tampere, FinlandEuropean Association for Machine Translation2023b. June 2023</p>
<p>Little giants: Exploring the potential of small llms as evaluation metrics in summarization in the eval4nlp 2023 shared task. Neema Kotonya, Saran Krishnasamy, Joel R Tetreault, Alejandro Jaimes, Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems. the 4th Workshop on Evaluation and Comparison of NLP SystemsBali, IndonesiaAssociation for Computational Linguistics2023. Eval4NLP 2023. November 1, 2023</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023. the 29th Symposium on Operating Systems Principles, SOSP 2023Koblenz, GermanyACM2023. October 23-26, 2023</p>
<p>Human evaluation of automatically generated text: Current trends and best practice guidelines. Chris Van Der Lee, Albert Gatt, Emiel Emiel Van Miltenburg, Krahmer, Comput. Speech Lang. 671011512021</p>
<p>Instructpatentgpt: Training patent language models to follow instructions with human feedback. Jieh-Sheng Lee, CoRR, abs/2406.168972024</p>
<p>Prexme! large scale prompt exploration of open source llms for machine translation and summarization evaluation. Christoph Leiter, Steffen Eger, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, FL, USAAssociation for Computational Linguistics2024. 2024. November 12-16, 2024</p>
<p>The eval4nlp 2023 shared task on prompting large language models as explainable metrics. Christoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao, Rotem Dror, Steffen Eger, Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems. the 4th Workshop on Evaluation and Comparison of NLP SystemsBali, IndonesiaAssociation for Computational Linguistics2023. Eval4NLP 2023. November 1, 2023</p>
<p>Generative judge for evaluating alignment. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei Liu, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaOpenReview2024. May 7-11, 2024net</p>
<p>Collaborative evaluation: Exploring the synergy of large language models and humans for open-ended generation evaluation. Qintong Li, Leyang Cui, Lingpeng Kong, Wei Bi, CoRR, abs/23102023a. 19740</p>
<p>PRD: peer rank and discussion improve large language model based evaluations. Ruosen Li, Teerth Patel, Xinya Du, CoRR, abs/2307.027622023</p>
<p>Competition-level code generation with alphacode. Yujia Li, David H Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien De Masson D'autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, CoRR, abs/2203.07814Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022</p>
<p>Split and merge: Aligning position biases in large language model based evaluators. Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu, CoRR, abs/2310.014322023b</p>
<p>Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. Yen-Ting Lin, Yun-Nung Chen, CoRR, abs/2305.137112023</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023a. December 6-10, 2023</p>
<p>Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization. Yixin Liu, Alexander R Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, Arman Cohan, Findings of the Association for Computational Linguistics: NAACL 2024. Mexico City, MexicoAssociation for Computational Linguistics2024a. June 16-21, 2024</p>
<p>On learning to summarize with large language models as references. Yixin Liu, Kejian Shi, Katherine He, Longtian Ye, Alexander R Fabbri, Pengfei Liu, Dragomir Radev, Arman Cohan, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics2024b. June 16-21, 20241NAACL 2024</p>
<p>Evaluate what you can't evaluate: Unassessable generated responses quality. Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang, Hinrich Schütze, CoRR, abs/2305.146582023b</p>
<p>Calibrating llm-based evaluator. Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024Torino, ItalyELRA and ICCL2024c. 20-25 May, 2024</p>
<p>LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models. Adian Liusie, Potsawee Manakul, Mark J F Gales, Proceedings of the 18th Conference of the European Chapter. Long Papers. the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational Linguistics2024. March 17-22, 20241</p>
<p>Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt. Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, Dacheng Tao, CoRR, abs/2303.138092023</p>
<p>Chatgpt as a factual inconsistency evaluator for abstractive text summarization. Zheheng Luo, Qianqian Xie, Sophia Ananiadou, CoRR, abs/2303.156212023</p>
<p>Pragmatics in the era of large language models: A survey on datasets, evaluation, opportunities and challenges. Bolei Ma, Yuting Li, Wei Zhou, Ziwei Gong, Janet Yang, Katja Liu, Annemarie Jasinskaja, Julia Friedrich, Frauke Hirschberg, Barbara Kreuter, Plank, CoRR, abs/2502.123782025</p>
<p>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. Potsawee Manakul, Adian Liusie, Mark J F Gales, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Simple LLM prompting is state-of-the-art for robust and multilingual dialogue evaluation. John Mendonça, Patrícia Pereira, Paulo João, Alon Carvalho, Isabel Lavie, Trancoso, CoRR, abs/2308.167972023</p>
<p>Teaching language models to support answers with verified quotes. Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, H Francis Song, Martin J Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, Nat Mcaleese, CoRR, abs/2203.111472022</p>
<p>Evaluating the evaluator: Measuring llms' adherence to task evaluation instructions. Bhuvanashree Murugadoss, Christian Pölitz, Ian Drosos, Nick Vu Le, Carina Suzana Mckenna, Chris Negreanu, Advait Parnin, Sarkar, AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence. Philadelphia, PA, USAAAAI Press2025. February 25 -March 4, 2025</p>
<p>Automated evaluation of written discourse coherence using GPT-4. Ben Naismith, Phoebe Mulcaire, Jill Burstein, Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications, BEA@ACL 2023. the 18th Workshop on Innovative Use of NLP for Building Educational Applications, BEA@ACL 2023Toronto, CanadaAssociation for Computational Linguistics2023. 13 July 2023</p>
<p>Webgpt: Browser-assisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, CoRR, abs/2112.09332Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman2021</p>
<p>NLG evaluation metrics beyond correlation analysis: An empirical metric preference checklist. Ni'mah, Meng Iftitahu, Vlado Fang, Mykola Menkovski, Pechenizkiy, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231ACL 2023</p>
<p>Chatgpt vs. crowdsourcing vs. experts: Annotating open-domain conversations with speech functions. Lidiia Ostyakova, Veronika Smilga, Kseniia Petukhova, Maria Molchanova, Daniel Kornev, Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL 2023. the 24th Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL 2023Prague, CzechiaAssociation for Computational Linguistics2023. September 11 -15, 2023</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, PA, USAACL2002. July 6-12, 2002</p>
<p>Large language models meet user interfaces: The case of provisioning feedback. Stanislav Pozdniakov, Jonathan Brazil, Solmaz Abdi, Aneesha Bakharia, Shazia Sadiq, Dragan Gasevic, Paul Denny, Hassan Khosravi, CoRR, abs/2404.110722024</p>
<p>Concrete problems in AI safety. Inioluwa Raji, Roel Deborah, Dobbe, 2024revisited. CoRR, abs/2401.10899</p>
<p>Supporting human-ai collaboration in auditing llms with llms. Charvi Rastogi, Marco Túlio Ribeiro, Nicholas King, Harsha Nori, Saleema Amershi, Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2023. the 2023 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2023Montréal, QC, CanadaACM2023. August 8-10, 2023</p>
<p>Adaptive testing and debugging of NLP models. Marco Ribeiro, Scott M Túlio, Lundberg, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics2022. May 22-27, 20221ACL 2022</p>
<p>Branch-solve-merge improves large language model evaluation and generation. Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, Xian Li, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics2024. June 16-21, 20241NAACL 2024</p>
<p>Perturbation checklists for evaluating NLG evaluation metrics. Ananya B Sai, Tanay Dixit, Yashpal Dev, Sreyas Sheth, Mitesh M Mohan, Khapra, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaDominican Republic2021</p>
<p>A survey of evaluation metrics used for NLG systems. Ananya B Sai, Akash Kumar Mohankumar, Mitesh M Khapra, ACM Comput. Surv. 552392023</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, CoRR, abs/2206.05802Jan Leike. 2022Jonathan Ward</p>
<p>Who validates the validators? aligning llm-assisted evaluation of LLM outputs with human preferences. Shreya Shankar, J D Zamfirescu-Pereira, Bjoern Hartmann, Aditya G Parameswaran, Ian Arawjo, Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, UIST 2024. the 37th Annual ACM Symposium on User Interface Software and Technology, UIST 2024Pittsburgh, PA, USAACM2024. October 13-16, 202413114</p>
<p>Large language models are not yet human-level evaluators for abstractive summarization. Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, Lidong Bing, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Repeval: Effective text evaluation with LLM representation. Shuqian Sheng, Yi Xu, Tianhang Zhang, Zanwei Shen, Luoyi Fu, Jiaxin Ding, Lei Zhou, Xiaoying Gan, Xinbing Wang, Chenghu Zhou, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, FL, USAAssociation for Computational Linguistics2024. 2024. November 12-16, 2024</p>
<p>Fusion-eval: Integrating assistant evaluators with llms. Lei Shu, Nevan Wichers, Liangchen Luo, Yun Zhu, Yinxiao Liu, Jindong Chen, Lei Meng, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: EMNLP 2024 -Industry Track. the 2024 Conference on Empirical Methods in Natural Language Processing: EMNLP 2024 -Industry TrackMiami, Florida, USAAssociation for Computational Linguistics2024. November 12-16, 2024</p>
<p>Evaluating evaluation methods for generation in the presence of variation. Amanda Stent, Matthew Marge, Mohit Singhai, Computational Linguistics and Intelligent Text Processing, 6th International Conference, CICLing 2005. Lecture Notes in Computer Science. Mexico City, MexicoSpringer2005. February 13-19, 20053406</p>
<p>Bertscore is unfair: On social bias in language model-based metrics for text generation. Tianxiang Sun, Junliang He, Xipeng Qiu, Xuanjing Huang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 20222022</p>
<p>Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning. Petter Törnberg, CoRR, abs/2304.065882023</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Aurélien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288</p>
<p>A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, Dong Yu, CoRR, abs/2307.039872023</p>
<p>Is chatgpt a good NLG evaluator? A preliminary study. Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, CoRR, abs/2303.040482023a</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, Zhifang Sui, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024a. August 11-16, 20241ACL 2024</p>
<p>Self-taught evaluators. Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, Xian Li, CoRR, abs/2408.026662024b</p>
<p>Shepherd: A critic for language model generation. Tianlu Wang, Ping Yu, Ellen Xiaoqing, Sean O Tan, Ramakanth 'brien, Jane Pasunuru, Olga Dwivedi-Yu, Luke Golovneva, Maryam Zettlemoyer, Asli Fazel-Zarandi, Celikyilmaz, CoRR, abs/2308.045922023b</p>
<p>Automated evaluation of personalized text generation using large language models. Yaqing Wang, Jiepu Jiang, Mingyang Zhang, Cheng Li, Yi Liang, Qiaozhu Mei, Michael Bendersky, CoRR, abs/2310.115932023c</p>
<p>Pandalm: An automatic evaluation benchmark for LLM instruction tuning optimization. Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang, ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, AustriaOpenReview2024c. May 7-11, 2024net</p>
<p>Language models with image descriptors are strong few-shot video-language learners. Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit Bansal, Heng Ji, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>Automatic answerability evaluation for question generation. Zifan Wang, Kotaro Funakoshi, Manabu Okumura, CoRR, abs/2309.125462023</p>
<p>Challenges in data-to-document generation. Sam Wiseman, Stuart M Shieber, Alexander M Rush, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational Linguistics2017. 2017. September 9-11, 2017</p>
<p>Style over substance: Evaluation biases for large language models. Minghao Wu, Alham Fikri, Aji , Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025. the 31st International Conference on Computational Linguistics, COLING 2025Abu Dhabi, UAEAssociation for Computational Linguistics2025. January 19-24, 2025</p>
<p>Large language models are diverse role-players for summarization evaluation. Ning Wu, Ming Gong, Linjun Shou, Shining Liang, Daxin Jiang, Natural Language Processing and Chinese Computing -12th National CCF Conference, NLPCC 2023. Lecture Notes in Computer Science. Foshan, ChinaSpringer2023. October 12-15, 202314302Proceedings, Part I</p>
<p>Evaluating evaluation metrics: A framework for analyzing NLG evaluation metrics using measurement theory. Ziang Xiao, Susu Zhang, Vivian Lai, Q Vera, Liao, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Deltascore: Fine-grained story evaluation with perturbations. Zhuohan Xie, Miao Li, Trevor Cohn, Jey Han Lau, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>INSTRUCTSCORE: towards explainable text generation evaluation with automatic feedback. Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, Lei Li, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>FLASK: fine-grained language model evaluation based on alignment skill sets. Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaOpenReview2024. May 7-11, 2024net</p>
<p>Complementary explanations for effective in-context learning. Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Veselin Stoyanov, Greg Durrett, Ramakanth Pasunuru, Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 2023</p>
<p>Interpreting language models with contrastive explanations. Kayo Yin, Graham Neubig, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 20222022</p>
<p>Batcheval: Towards human-like text evaluation. Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Boyuan Pan, Heda Wang, Yao Hu, Kan Li, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024. August 11-16, 20241ACL 2024</p>
<p>Bartscore: Evaluating generated text as text generation. Weizhe Yuan, Graham Neubig, Pengfei Liu, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021. 2021. December 6-14, 2021</p>
<p>A comprehensive analysis of the effectiveness of large language models as automatic dialogue evaluators. Chen Zhang, Luis Fernando, D' Haro, Yiming Chen, Malu Zhang, Haizhou Li, Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence. Vancouver, CanadaAAAI Press2024. February 20-27, 20242014</p>
<p>OPT: open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, CoRR, abs/2205.010682022</p>
<p>Wider and deeper LLM networks are fairer LLM evaluators. Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, Yongbin Li, CoRR, abs/2308.018622023</p>
<p>A human-machine collaborative framework for evaluating malevolence in dialogues. Yangjun Zhang, Pengjie Ren, Maarten De Rijke, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021Association for Computational Linguistics2021. August 1-6, 20211Virtual Event</p>
<p>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?. Ying Zhang, Stephan Vogel, Alex Waibel, Proceedings of the Fourth International Conference on Language Resources and Evaluation, LREC 2004. the Fourth International Conference on Language Resources and Evaluation, LREC 2004Lisbon, PortugalEuropean Language Resources Association2004. May 26-28, 2004</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2023. 2023. 2023. December 10 -16, 2023</p>
<p>Deconstructing NLG evaluation: Evaluation practices, assumptions, and their implications. Kaitlyn Zhou, Su Lin Blodgett, Adam Trischler, Hal Daumé, Iii , Kaheer Suleman, Alexandra Olteanu, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022Seattle, WA, United StatesAssociation for Computational Linguistics2022. July 10-15, 2022</p>
<p>A survey of evaluation methods of generated medical textual reports. Yongxin Zhou, Fabien Ringeval, François Portet, Proceedings of the 5th Clinical Natural Language Processing Workshop, ClinicalNLP@ACL 2023. the 5th Clinical Natural Language Processing Workshop, ClinicalNLP@ACL 2023Toronto, CanadaAssociation for Computational2023. July 14, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
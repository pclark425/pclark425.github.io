<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8200 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8200</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8200</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-277621966</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.04485v1.pdf" target="_blank">Building LLM Agents by Incorporating Insights from Computer Systems</a></p>
                <p><strong>Paper Abstract:</strong> LLM-driven autonomous agents have emerged as a promising direction in recent years. However, many of these LLM agents are designed empirically or based on intuition, often lacking systematic design principles, which results in diverse agent structures with limited generality and scalability. In this paper, we advocate for building LLM agents by incorporating insights from computer systems. Inspired by the von Neumann architecture, we propose a structured framework for LLM agentic systems, emphasizing modular design and universal principles. Specifically, this paper first provides a comprehensive review of LLM agents from the computer system perspective, then identifies key challenges and future directions inspired by computer system design, and finally explores the learning mechanisms for LLM agents beyond the computer system. The insights gained from this comparative analysis offer a foundation for systematic LLM agent design and advancement.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8200.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8200.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An openended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal, open-ended agentic system applied to game environments that the paper lists as using both short-term (context) and long-term (external) memory mechanisms and in-context learning for behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An openended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An open-ended embodied multimodal agent designed for game environments; listed in the paper as employing reasoning and both short- and long-term memory while using in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Multimodal large language model (GPT-4) used as the controller for the agent (as reported in the paper's summary table); specific size/architecture details are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Game (game environment listed in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Generic game environment (the paper's table denotes 'Game' as the environment class for Voyager), i.e., interactive game tasks requiring action generation and long-horizon behavior; no further benchmark specifics or dataset names are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term (context window) and long-term (external / retrieval-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Not specified in detail in this paper; the paper classifies Voyager as having both short-term context-window memory and long-term external memory (RAG-style/external store). The paper describes long-term memory generally as external storage (text/images/code/trajectories) and short-term memory as the model context; memory read is typically similarity-based retrieval and retrieval-augmented generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>General framework: retrieved memory content M_r is provided as input to the cognition module (Equation (1)), i.e., retrieved memory is concatenated/added to the model input as context (retrieval-augmented generation). The paper does not report agent-specific integration details beyond this general RAG/context-window integration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>No ablation or quantitative comparison between memory strategies or memory vs no-memory baselines for Voyager is reported in this paper; Voyager is only listed in the summary table as having long- and short-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Not identified empirically in this paper. The paper advocates retrieval-augmented long-term memory combined with context-window short-term memory and calls for finer-grained hierarchies (including cache) as promising directions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>This paper does not report Voyager-specific failure cases. More generally, the paper highlights limitations relevant to agents like Voyager: restricted context window capacity for short-term memory, sparse use of long-term memory across prior work, lack of cache-layer designs, and unexplored read/write speed and efficiency trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>The paper recommends adopting a memory hierarchy (short-term context + cache + long-term external store), using retrieval-augmented generation for long-term memory, exploring cache modules to hold frequently-used content, and investigating DMA-like mechanisms to transfer long-term memory to short-term memory without always routing through the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Building LLM Agents by Incorporating Insights from Computer Systems', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8200.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8200.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jarvis-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-world, multi-task multimodal agent described in the paper as memory-augmented and applied in game environments; listed as using both short-term and long-term memory and leveraging in-context learning and other learning paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Jarvis-1</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An open-world multi-task multimodal agent characterized in the paper as memory-augmented; listed as operating in game environments and using both short-term context and long-term memory stores.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Memory-augmented multimodal large language model (GPT-4) used as the controller according to the summary table; exact model size/architecture specifics are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Game (game environment listed in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Generic game environment (paper's summary table lists 'Game' as the environment for Jarvis-1); the paper does not provide further benchmark specifics or metrics for Jarvis-1.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term (context window) and long-term (external / retrieval-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Described at high level only: Jarvis-1 is labeled in the table as 'memory-augmented multimodal language models'—consistent with the paper's general description of long-term external memory (RAG-like stores) plus short-term context. No agent-specific low-level architecture or key-value format is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>At the framework level, memory retrieval provides content M_r which is fed into the cognition module (Equation (1)); the paper conveys this as retrieval-augmented input to the LLM (concatenated/used as additional context) but provides no Jarvis-1-specific integration pseudocode or parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>This paper does not present Jarvis-1 ablations comparing memory strategies or memory vs no-memory baselines; Jarvis-1 is referenced only in the summary table as memory-augmented.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>No empirical best-memory strategy is claimed. The paper broadly recommends memory hierarchies and retrieval-augmented long-term memory for improved agent capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No Jarvis-1-specific failures are reported here. The paper raises general limitations: limited context-window short-term memory, lack of cache layers in existing agents, unexplored memory read/write latency and efficiency, and that long-term memory remains underused in current agent designs.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>The paper recommends refining memory with finer-grained hierarchies (introducing cache between context and long-term store), exploring DMA-like direct transfers from long-term to short-term memory to avoid overloading the LLM, and making memory the center of data flow for agents that must handle large external information over long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Building LLM Agents by Incorporating Insights from Computer Systems', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Voyager: An openended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models <em>(Rating: 2)</em></li>
                <li>Keep calm and explore: Language models for action generation in text-based games <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8200",
    "paper_id": "paper-277621966",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An openended embodied agent with large language models",
            "brief_description": "A multimodal, open-ended agentic system applied to game environments that the paper lists as using both short-term (context) and long-term (external) memory mechanisms and in-context learning for behavior.",
            "citation_title": "Voyager: An openended embodied agent with large language models",
            "mention_or_use": "mention",
            "agent_name": "Voyager",
            "agent_description": "An open-ended embodied multimodal agent designed for game environments; listed in the paper as employing reasoning and both short- and long-term memory while using in-context learning.",
            "llm_model_name": "GPT-4",
            "llm_model_description": "Multimodal large language model (GPT-4) used as the controller for the agent (as reported in the paper's summary table); specific size/architecture details are not provided in this paper.",
            "benchmark_name": "Game (game environment listed in paper)",
            "benchmark_description": "Generic game environment (the paper's table denotes 'Game' as the environment class for Voyager), i.e., interactive game tasks requiring action generation and long-horizon behavior; no further benchmark specifics or dataset names are provided in this paper.",
            "memory_used": true,
            "memory_type": "short-term (context window) and long-term (external / retrieval-augmented)",
            "memory_architecture": "Not specified in detail in this paper; the paper classifies Voyager as having both short-term context-window memory and long-term external memory (RAG-style/external store). The paper describes long-term memory generally as external storage (text/images/code/trajectories) and short-term memory as the model context; memory read is typically similarity-based retrieval and retrieval-augmented generation.",
            "memory_integration_strategy": "General framework: retrieved memory content M_r is provided as input to the cognition module (Equation (1)), i.e., retrieved memory is concatenated/added to the model input as context (retrieval-augmented generation). The paper does not report agent-specific integration details beyond this general RAG/context-window integration.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "No ablation or quantitative comparison between memory strategies or memory vs no-memory baselines for Voyager is reported in this paper; Voyager is only listed in the summary table as having long- and short-term memory.",
            "best_memory_strategy": "Not identified empirically in this paper. The paper advocates retrieval-augmented long-term memory combined with context-window short-term memory and calls for finer-grained hierarchies (including cache) as promising directions.",
            "limitations_or_failure_cases": "This paper does not report Voyager-specific failure cases. More generally, the paper highlights limitations relevant to agents like Voyager: restricted context window capacity for short-term memory, sparse use of long-term memory across prior work, lack of cache-layer designs, and unexplored read/write speed and efficiency trade-offs.",
            "recommendations_or_conclusions": "The paper recommends adopting a memory hierarchy (short-term context + cache + long-term external store), using retrieval-augmented generation for long-term memory, exploring cache modules to hold frequently-used content, and investigating DMA-like mechanisms to transfer long-term memory to short-term memory without always routing through the LLM.",
            "uuid": "e8200.0",
            "source_info": {
                "paper_title": "Building LLM Agents by Incorporating Insights from Computer Systems",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Jarvis-1",
            "name_full": "Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models",
            "brief_description": "An open-world, multi-task multimodal agent described in the paper as memory-augmented and applied in game environments; listed as using both short-term and long-term memory and leveraging in-context learning and other learning paradigms.",
            "citation_title": "Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models",
            "mention_or_use": "mention",
            "agent_name": "Jarvis-1",
            "agent_description": "An open-world multi-task multimodal agent characterized in the paper as memory-augmented; listed as operating in game environments and using both short-term context and long-term memory stores.",
            "llm_model_name": "GPT-4",
            "llm_model_description": "Memory-augmented multimodal large language model (GPT-4) used as the controller according to the summary table; exact model size/architecture specifics are not provided in this paper.",
            "benchmark_name": "Game (game environment listed in paper)",
            "benchmark_description": "Generic game environment (paper's summary table lists 'Game' as the environment for Jarvis-1); the paper does not provide further benchmark specifics or metrics for Jarvis-1.",
            "memory_used": true,
            "memory_type": "short-term (context window) and long-term (external / retrieval-augmented)",
            "memory_architecture": "Described at high level only: Jarvis-1 is labeled in the table as 'memory-augmented multimodal language models'—consistent with the paper's general description of long-term external memory (RAG-like stores) plus short-term context. No agent-specific low-level architecture or key-value format is provided in this paper.",
            "memory_integration_strategy": "At the framework level, memory retrieval provides content M_r which is fed into the cognition module (Equation (1)); the paper conveys this as retrieval-augmented input to the LLM (concatenated/used as additional context) but provides no Jarvis-1-specific integration pseudocode or parameters.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "This paper does not present Jarvis-1 ablations comparing memory strategies or memory vs no-memory baselines; Jarvis-1 is referenced only in the summary table as memory-augmented.",
            "best_memory_strategy": "No empirical best-memory strategy is claimed. The paper broadly recommends memory hierarchies and retrieval-augmented long-term memory for improved agent capabilities.",
            "limitations_or_failure_cases": "No Jarvis-1-specific failures are reported here. The paper raises general limitations: limited context-window short-term memory, lack of cache layers in existing agents, unexplored memory read/write latency and efficiency, and that long-term memory remains underused in current agent designs.",
            "recommendations_or_conclusions": "The paper recommends refining memory with finer-grained hierarchies (introducing cache between context and long-term store), exploring DMA-like direct transfers from long-term to short-term memory to avoid overloading the LLM, and making memory the center of data flow for agents that must handle large external information over long horizons.",
            "uuid": "e8200.1",
            "source_info": {
                "paper_title": "Building LLM Agents by Incorporating Insights from Computer Systems",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Voyager: An openended embodied agent with large language models",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models",
            "rating": 2,
            "sanitized_title": "jarvis1_openworld_multitask_agents_with_memoryaugmented_multimodal_language_models"
        },
        {
            "paper_title": "Keep calm and explore: Language models for action generation in text-based games",
            "rating": 2,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        }
    ],
    "cost": 0.01154125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Building LLM Agents by Incorporating Insights from Computer Systems
6 Apr 2025</p>
<p>Yapeng Mi 
Perception Cognition Memory Tool Action Input Control Unit Memory Unit</p>
<p>Zhi Gao 
Perception Cognition Memory Tool Action Input Control Unit Memory Unit</p>
<p>Xiaojian Ma 
Perception Cognition Memory Tool Action Input Control Unit Memory Unit</p>
<p>Qing Li 
Perception Cognition Memory Tool Action Input Control Unit Memory Unit</p>
<p>Building LLM Agents by Incorporating Insights from Computer Systems
6 Apr 2025DDA8A70D57CF37951EDAA9A3F015EF47arXiv:2504.04485v1[cs.CV]Total size of images in '/images': 55643 bytes code: print(f " Total")
LLM-driven autonomous agents have emerged as a promising direction in recent years.However, many of these LLM agents are designed empirically or based on intuition, often lacking systematic design principles, which results in diverse agent structures with limited generality and scalability.In this paper, we advocate for building LLM agents by incorporating insights from computer systems.Inspired by the von Neumann architecture, we propose a structured framework for LLM agentic systems, emphasizing modular design and universal principles.Specifically, this paper first provides a comprehensive review of LLM agents from the computer system perspective, then identifies key challenges and future directions inspired by computer system design, and finally explores the learning mechanisms for LLM agents beyond the computer system.The insights gained from this comparative analysis offer a foundation for systematic LLM agent design and advancement.</p>
<p>Introduction</p>
<p>In recent years, autonomous agents have emerged as a promising direction, with widespread applications in various domains, such as computer use (Hong et al., 2024;Lin et al., 2024), code assistance (Wang et al., 2024a;Yang et al., 2024), and others (Kim et al., 2024;Fan et al., 2025).These agents, powered by large language models (LLMs) (Achiam et al., 2023) as controllers, integrate key components such as memory, tools, and action while allowing effective environmental interaction.Currently, many studies (Xie et al., 2024;Durante et al., 2024) rely on intuition or experience to design agents, often lacking systematic design principles.As a result, the significant divergence among these designs makes it difficult to establish a standardized structure with generality and scalability.At the same time, the lack of a systematic approach to developing LLM agents limits the ability to guide future research directions.Therefore, we are faced with a critical question: How can general LLM agents be systematically constructed and evolved?</p>
<p>In this paper, we advocate for building LLM agents by incorporating insights from computer systems.Looking back at the evolution of computer system design (Campbell-Kelly &amp; Aspray, 1996), we find that it once witnessed a proliferation of various structural approaches.In the early days of computing, systems were often designed with specific architectures tailored to particular tasks, such as the ABC computer in 1942 for solving linear equations (Burks et al., 1946).This specificity meant that these systems lacked general-purpose capabilities.As time goes on, modern computer systems have largely converged on a unified theoretical framework: the von Neumann architecture, which has significantly advanced the field.Similarly, the design of agents today revolves around system-level considerations, much like computer systems design.A straightforward comparison between the von Neumann architecture and the agent architecture shows their high similarity in both structural design and task workflows, as shown in Figure 1.This parallel offers numerous opportunities for comparative analysis between these two fields.For example, both systems have memory modules, and the computer storage hierarchy can guide agents' finer-grained memory design.Furthermore, computer systems have developed several golden insights over time, such as parallelization.These principles can be effectively transferred to the design of LLM agents, providing foundational guidance for their construction.</p>
<p>Therefore, this paper proposes building and evolving general LLM agents by drawing insights from computer systems, including von Neumann architecture and other related principles.We will conduct a comprehensive analysis of the LLM agent compared with the computer systems, then investigate the potential future research direction inspired by the computer systems, and explore the learning capability of agents beyond the computer systems.Specifically, this work provides a comparative analysis of LLM agents for the framework, inspired by the von Neumann architecture (Section 2).We propose LLM agents as a collection of distinct modules that dynamically interact with the environment.This framework effectively encapsulates existing research while inspiring future directions.Then, this paper will discuss potential directions inspired by computers, including finer-grained memory, parallelization, and other potential advances(Section 3).Finally, we analyze current learning mechanisms and explore how such agents can learn from their environment, through which the agents could go beyond some limitations of computer systems(Section 4).We believe that better learning methods are key to the evolution of LLM agents.To the best of our knowledge, this is the first analogy drawn between computer systems and LLM agents, and we hope it can benefit the community.</p>
<p>LLM Agent Framework Inspired by Von Neumann Architecture</p>
<p>One important insight from computer systems is the von Neumann architecture, which established the foundational framework of modern computing by integrating a central processing unit, memory, and input/output systems (Burks et al., 1946).In this section, we follow a systematic perspective and draw inspiration from von Neumann architecture (Figure 1) to explore how to construct general LLM agents.We posit that an LLM agent comprises interconnected components: perception, cognition, memory, tools, and action, which dynamically interact with each other and the environment, as shown in Figure 2. We conducted a literature review based on this definition.During this process, we also conducted a comparative analysis with the von Neumann architecture, revealing shared principles in system design between the two.</p>
<p>To further support the foundational framework, this paper provides a concise formulation.To formalize, each module is denoted by its uppercase initial (e.g., P for perception).</p>
<p>The framework is defined as F = (P, C, M, T, A).Specifically, the LLM agent also receives observations from the external environment, denoted o.At time step t, the action a t can be derived using the following equation:
a t = A C P (o 1 , a 1 , . . . , o t−1 , a t−1 , o t ), M r , T c . (1)
Here, M r represents the content retrieved by the memory module, and T c represents the output of the calling tool.This equation briefly illustrates how our framework generates the action a t by integrating the historical sequence starting from the initial prompt-guided observation o 1 , followed by actions and observations, combined with memory content and tool outputs.More details can be found in Appendix B.</p>
<p>Perception</p>
<p>For agents interacting in real-world environments, the capability of perception is indispensable, similar to the role of input modules in computer systems.Just as computers use input such as keyboards and cameras, agents gather various types of information, such as text, images, video, and audio.</p>
<p>Based on existing research, the perception of agents can be divided into two parts: unimodal and multimodal.</p>
<p>Unimodal: Unimodal perception means that the agent perceives only one type of information.Due to the significant achievements of LLMs in natural language processing, early agents used LLMs as core processors, relying on the model's inherent perception capabilities.In previous studies (Wang et al., 2023;Shen et al., 2024), these agents only utilize textual information in environments for processing and planning, and restrict accurate understanding of multimodal information, driving the development of subsequent multimodal perception agents.</p>
<p>Multimodal: With recent advancements in multimodal LLMs, more agents (Hong et al., 2024;Kim et al., 2024) now adopt multimodal models as controllers, enabling the integration of diverse modalities such as text, images, and videos.For complex scenarios, external encoders (Fan et al., 2022) can also be used to convert information into forms perceivable by agents.In summary, multimodal has become a mainstream direction for agent perception.</p>
<p>Notably, a crucial principle of von Neumann architecture is that various types of information are transformed into a common representation within the system.Computer systems represent information in a data space of 0s and 1s, modern agents represent perceived information in the language space, which offers several distinct advantages.Firstly, language space demonstrates higher fault tolerance during agent task execution due to the superior generalization capabilities of natural language.Secondly, language space enables the expression of more abstract and ambiguous concepts, which is often beyond the capacity of conventional computer systems.Lastly, since the output in the process of task completion can be observed in natural language, representing information in the language space inherently provides better Interpretability.</p>
<p>Cognition</p>
<p>The cognition module plays a crucial role in LLM agents, much like the position of the control unit in the computer architecture.Cognition governs the agent's decisions, encompassing planning and reasoning capabilities.Planning is an essential capability for almost every LLM agent, while reasoning involves various thought processes such as chain of thought and reflection to facilitate better planning.Analogously, planning can be seen as the central function of the control unit, while reasoning represents the underlying logic that drives its decision-making processes.</p>
<p>Planning: Planning is a foundational capability of cognition.Given a goal,OK, planning decides on a sequence of actions (a 0 , a 1 , . . ., a n ) that will lead to a state achieving the goal.Early studies (Liu et al., 2023;Yao et al., 2020) explored the use of external planners to assist planning.With the further enhancement of LLMs in reasoning (Wei et al., 2022), planning is expected to be incorporated into the controllers themselves.For example, WebDreamer (Gu et al., 2024) uses LLMs to simulate candidate actions and evaluate their outcomes to select the optimal step.Therefore, we can see that future planning methods are likely to be centered around LLMs, requiring less scaffolding.This is largely attributed to the improvement of reasoning capabilities of LLMs, which is crucial in cognition.</p>
<p>Reasoning: Reasoning is a fundamental unit of cognition, and it supports the thinking capability of LLM agents.Objectively, the development of reasoning is closely tied to LLMs, significantly expanding their ability to think and enabling them to solve more complex problems.In recent years, the advancement of reasoning based on LLMs began with CoT (Wei et al., 2022), which allows the model to decompose complex problems into a sequence of intermediate reasoning steps.CoT-SC (Wang et al., 2022) builds upon CoT by generating multiple answers simultaneously and selecting the optimal one.ToT (Yao et al., 2024) adopts a tree structure to explore multiple reasoning paths and selfevaluate for a globally optimal decision.These methods enable the cognition module to engage in deep thinking, thereby significantly enhancing its decision-making capabilities.</p>
<p>Another key capability in reasoning is known as selfrefinement or reflection.This ability allows LLM agents to reflect on their past actions and further optimize future decisions.Essentially, it transforms a linear thought process into a cyclical one by treating the decision-making process as continuous.The ReAct (Yao et al., 2022) framework is a notable pioneer in this area.It enhances LLMs by integrating task-specific actions for environmental interaction and natural language reasoning for situational reflection.Reflexion (Shinn et al., 2024) converts binary or scalar feedback from the environment into textual summaries, which are then incorporated as additional context for LLM agents in subsequent iterations.Most self-refinement mechanisms enable agents to improve through trial-and-error processes, enhancing their robustness.</p>
<p>Notably, in the history of electronic computing, the evolution of CPUs, following the unified von Neumann architecture, has consistently been the most critical development in computers.From this perspective, the cognition module of LLM agents is likely to follow a similar trajectory.In the future, advanced planning and reasoning units may emerge as the core direction for LLM agents, further underscoring the central role of cognition in their development.</p>
<p>Memory</p>
<p>In the von Neumann architecture, the memory unit serves as the central repository for both data and instructions.This essential unit can be analogously extended to LLM agents, where a memory-like component is constructed for storing states, knowledge, or learned behaviors, facilitating decision-making and dynamic adaptation.In computer systems, the memory units follow a memory hierarchy, which is a good insight from computer systems.We find a strong analogy between the memory between computers and LLM agents, as illustrated in Figure 3. Short-term memory is smaller than long-term memory but is fast, and long-term memory restores information longer but short-term memory cannot.Therefore, memory of LLM agents can be categorized into short-term and long-term memory, accompanied by associated read and write operations, aligning with the aforementioned perspective.</p>
<p>Short-Term Memory: Short-term memory is often incorporated into LLMs, like primary memory in computer architecture, as illustrated in Figure 3. Short-term memory refers to an LLM agent's contextual understanding within its context window, which is critical for immediate operations.In cer-  tain agent task interaction workflows, short-term memory is often used to store contextual input-output information related to the task, including reasoning details, tool usage data, and past state information.Therefore, it is often referred to as working memory, which supports contextually appropriate responses during ongoing interactions.</p>
<p>Long-Term Memory: In contrast, long-term memory is designed to store information for extended periods, enabling LLM agents to retrieve and utilize past knowledge or skills across different interactions, like disks in the memory hierarchy (see Figure 3).Long-term memory encompasses a diverse range of content types, including text, images, codes, trajectories, profile information, and successful demonstrations, significantly broadening the scope of information that agents can leverage.Long-term memory is typically implemented through external memory systems, enhancing an agent's reliability.</p>
<p>Memory Read and Write: Memory reading and writing are fundamental operations in the von Neumann architecture, serving as the critical link between the CPU and memory.</p>
<p>Similarly, LLM agents are increasingly equipped with mechanisms to read and write their memory.These capabilities allow agents to adapt dynamically to changing environments by editing stored information, ensuring that the memory remains relevant to specific tasks.For memory reading, most methods select memory values that exhibit the highest similarity to the query object.Furthermore, many agents adopt Retrieval-Augmented Generation (Lewis et al., 2020) to form a complete reading and generation workflow.Generative Agents (Park et al., 2023) retrieve necessary information in text form based on relevance, recency, and importance.Memory writing often follows the storage format of memory reading.For example, in MemoChat (Lu et al., 2023), agents summarize each conversation segment by identifying the main topics discussed and storing them as keys to index memory pieces.This structured approach facilitates efficient memory retrieval during reading operations.Memory reading and writing operations are collaborative, collectively enhancing the capabilities of LLM agents.Moreover, from a computational perspective, the speed of memory read and write operations remains an area to be explored for LLM agents, as it directly impacts information processing and task execution efficiency.</p>
<p>Tool</p>
<p>A unique feature is that LLM agents could utilize external tools, which is similar to a logic unit in computer systems.</p>
<p>As the logic unit is the true executor of arithmetic operations, the tools within LLM agents concrete execution of tasks.</p>
<p>Tool usage is often presented as APIs, enabling agents to select the appropriate tools based on their objectives quickly.</p>
<p>Commonly used tools in existing methods are diverse and depend on the design of the toolset, such as calculators for arithmetic operations, and Google search for retrieving the latest information.There are two main interactions in tool use, which are named tool retrieval and tool calling.</p>
<p>Tool Retrieval: When given a question that needs tools to solve, LLM agents perform reasoning and planning to generate responses containing tool information.Tool retrieval means selecting the right tools using a retriever or LLM.</p>
<p>For instance, Gorilla (Patil et al., 2023) employs BM25 and GPT-Index to construct a retriever to implement tool retrieval.ToolLLM (Qin et al., 2023) trains a Sentence-BERT model to serve as a tool retriever, allowing highly efficient retrieval of relevant tools.</p>
<p>Tool Calling: As for tool calling, it means a right tool is correctly provided with required parameters and executed successfully to return results for LLM agents, making it a successful call.In EasyTool (Yuan et al., 2024), it improves LLMs' understanding of tool functions and parameter requirements by prompting ChatGPT to rewrite tool descriptions.ConAgents (Shi et al., 2024) presents a multiagent collaborative framework, incorporating a dedicated execution agent responsible for parameter extraction and tool invocation.Notably, LLMs sometimes act as tools, representing the execution end beyond decision-making, not limited to specific instruments.</p>
<p>Action</p>
<p>Similar to the output modules in computer systems, the action module in LLM agents serves as the interface for interaction with environments.The functionality and mechanisms of the action module are typically goal-oriented and closely tied to LLM agents' operating environments.Fundamentally, the action module transforms high-level actions into low-level actions through appropriate conversions.The action module can be simply categorized into two types: internal actions and external actions.</p>
<p>Internal Actions: Internal Actions refer to the internal operations performed by LLM agents during task execution, including memory read/write, tool invocation, and reason-ing actions.These internal actions modify agents' internal state, enabling it to better learn and adapt to environments.Specifically, reasoning represents a novel internal action that does not exist in traditional computer systems, highlighting that LLM agents have a broader action space due to their ability to operate within abstract semantic spaces.</p>
<p>External Actions: External actions refer to the actions output by LLM agents that target external environments.These actions are closely tied to the external context and primarily include execution actions in GUI or embodied environments, as well as interaction actions in natural language dialogue.For example, SayCan (Ahn et al., 2022) generates actionable steps for robots, ShowUI (Lin et al., 2024) outputs actions and coordinates for virtual environments, and MDagents (Kim et al., 2024) perform conversational actions to provide medical advice.It is worth noting that some action formats require specific agents to use an additional action processing function to generate executable actions.</p>
<p>Environment</p>
<p>In computer systems, computers interact with environments in ways designed by humans.Similarly, an agent system only exerts its unique capabilities through environmental interaction.The interaction environment of agent LLMs is highly diverse; it could be a human (Chen et al., 2023b), a game (Wang et al., 2023), or a real-world setting (Ahn et al., 2022).Typically, an agent acts and receives new observations and feedback from the environment after execution, and this process is called a complete interaction loop.What's more, people design effective human-computer interaction ecosystems, such as VSCode.Similarly, there is a significant unexplored ecosystem space between LLM agents and external environments.For example, SWE-agent (Yang et al., 2024) constructs a friendly interface between LLM agents and software engineering.This analogy highlights the potential for enhanced environmental interaction.</p>
<p>In summary, inspired by the von Neumann architecture, we designed the corresponding LLM agent structure while conducting a comparative analysis.We also find that this structure effectively encompasses existing work, as shown in Table 1.We can observe the rise of multimodal agents, most of which possess reasoning capabilities, yet long-term memory is not widely utilized.This indicates that the framework we designed effectively summarizes the current development trends.This analogy-based design confirms the significant similarities between the two designs, inspiring us to draw insights from computer systems.</p>
<p>Future Directions Inspired by Computers</p>
<p>To guide future research, it is important to address key challenges and realize the future direction.Building on At the same time, viewing from a computer perspective allows us to identify shortcomings in current agent designs and gain further inspiration.In this section, we outline some future directions and critical challenges that demand attention.These challenges highlight the opportunities to advance the field.</p>
<p>The Principles of Building Agents</p>
<p>Numerous principles have been accumulated in the evolution of computers (Appendix C), which guide the development of computer architectures (Saltzer &amp; Kaashoek, 2009).By drawing an analogy and comparing computer system design principles, we can see that building LLM agents also requires certain principles.Here, we advocate the principle of "Build agents for demand".This means that agents should be designed only when a problem cannot be directly solved by an LLM itself or by constructing a workflow.For example, when an AI is needed to autonomously use various tools and browse the internet to complete tasks, agents should be built.This principle is accompanied by the adherence to three design principles: (1) Abstraction: Hide implementation details and provide simplified interfaces to reduce complexity.</p>
<p>(2) Modularity: Break agents into functional modules with clear interfaces.</p>
<p>(3) Scalability: Agents should be designed to scale efficiently with growing users, data, or computational demands without major changes.A design decision diagram is shown in Figure 4.Moreover, we believe that more principles will be discovered.</p>
<p>Principles of Building LLM Agents</p>
<p>Can LLMs solve the task?</p>
<p>Yes No</p>
<p>Use LLMs Can a workflow solve the task?</p>
<p>Yes No</p>
<p>Use a workflow with LLMs Design LLM agents Follow three rules: abstraction, modularity and scalability</p>
<p>Refining Memory for Greater Impact</p>
<p>The importance and role of memory are well recognized.In computer systems, memory is structured into finer-grained levels, incorporating cache mechanisms and diverse readand-write operations to optimize performance and efficiency.However, such explorations remain insufficient in the domain of LLM agents, highlighting the need to refine their memory mechanisms for greater impact.</p>
<p>Finer-Grained Memory: As shown in Figure 3, computer systems form a fine-grained memory hierarchy, including components like main memory, cache, and registers.Agents can adopt this idea to design analogous modules.The existing context window is similar to main memory, and databases are akin to disks.We have found that the cache module is missing in agents.Therefore, we call for re-search into the cache module in LLM agents.Commonly used information in agents can be stored in a cache-like module, enabling quick retrieval of past experiences to efficiently complete complex tasks.This cache module not only reduces memory pressure but also facilitates the training of LLM agents.Furthermore, the construction of the memory pyramid is largely driven by the principle of locality, which emphasizes focusing on the "most important 20%."We posit that a similar form of locality exists during the interactions of LLM agents, which could serve as a valuable guide for future memory design.</p>
<p>DMA is Useful in Memory: When drawing an analogy with computer memory mechanisms, we found that Direct Memory Access (DMA) (Khawaja &amp; Khan, 2014) can be effective for agents.DMA improves data transfer efficiency by enabling peripheral devices (such as hard drives) to exchange data with memory directly, bypassing the CPU and enhancing overall system performance.For LLM agents, this means that access instructions do not always have to go through the LLM.This approach can optimize data retrieval by allowing long-term memory to be written directly to short-term memory.At the same time, LLMs can focus on handling other tasks more efficiently.For example, VideoAgent (Fan et al., 2025) employs some unique memory querying tools to retrieve long-term memory during the question-answering process without requiring direct LLM access, enabling it to perform effectively in long-horizon video understanding.</p>
<p>Memory as the Center of Data Flow: Throughout computer system development, the von Neumann architecture initially centered on the CPU but shifted to a memorycentric model to optimize data storage and retrieval efficiency.In current agent designs, LLMs drive the agent's reasoning processes.However, when agents interact in realworld environments, the limitations of the context window within models restrict their ability to respond effectively to rapidly changing conditions.This raises the possibility that future agent systems may adopt a memory-centric approach to handle vast external information and enable real-time interaction more effectively.Existing works, such as Generative Agents (Park et al., 2023), have explored designs in this direction, but real-time interaction remains an area requiring further study.</p>
<p>Multi-Core System</p>
<p>Multi-core mechanisms in computer systems enable parallel processing by integrating multiple processing cores within a single CPU, allowing concurrent execution of tasks to enhance performance and efficiency (Hennessy &amp; Patterson, 2011).Each core operates independently, handling separate threads or processes, improving multitasking and throughput.This architecture optimizes resource utilization and power efficiency, making it ideal for modern computing workloads.We advocate incorporating a multi-core mechanism into LLM agents, utilizing an appropriate number of models to serve as the cognition module within the agent.Specifically, the unique Big.LITTLE (ARM, 2011) architecture in multi-core systems refers to a design where high-performance "big" cores and energy-efficient "little" cores are used together, enabling optimal performance and power consumption.LLM agents can draw inspiration from this design.Larger models can be responsible for complex planning and reasoning tasks, while smaller models can manage basic operations such as dialogue and interactions with memory and tool modules.This design is analogous to the human brain's System 1 and System 2, reflecting the rationality of such a design.</p>
<p>Parallelization and Pipelining</p>
<p>Parallelization: Parallelization in computers refers to the technique of executing multiple instructions simultaneously by utilizing multiple processing threads.This highlights the correctness of breaking down large tasks into sub-tasks within agents, while also suggesting that existing agents may benefit from parallel processing.Considering parallelization from both single-agent and multi-agent perspectives, for a single agent, parallelization means that a task can be processed simultaneously by leveraging multiple tools or LLMs to achieve acceleration.In a multi-agent system, a task can be distributed among different agents, which then integrate their outputs to achieve parallel processing and improve accuracy.</p>
<p>Pipelining: Pipelining in computers is a technique that improves instruction throughput by overlapping the execution of multiple instructions.It divides the instruction cycle into discrete stages, where different stages of multiple instructions can be processed simultaneously.This allows for continuous data flow and minimizes idle time, enhancing overall performance.A similar pipelining approach can also be adopted in open-world agents to improve the speed and accuracy of processing continuous external environmental feedback, which is particularly evident in embodied intelligence and autonomous driving.</p>
<p>In addition to the mechanisms mentioned above, we believe that many insights can be drawn from computer systems.These directions further illustrate the strong correlation between the two fields, suggesting that the current design of LLM agents can fully leverage advancements in computer systems.Furthermore, the software layers developed through the evolution of computing-such as software systems and user interfaces-can also be applied to the advancement of LLM agents, highlighting their potential.</p>
<p>Go beyond Computers: Learning Capability in LLM Agents</p>
<p>If LLM agents only include the aforementioned modules and interaction mechanisms, their roles in an open-world setting would resemble that of a computer, which cannot achieve Artificial General Intelligence (AGI).To answer how general agents will evolve after construction, we find that the critical factor lies in the learning mechanism, which does not exist in computer systems.Therefore, in this section, we will explore the future learning methods of agents to go beyond computers.The analysis and summary of learning approaches in existing studies can also be found in Table 1.</p>
<p>LLM Learning: LLMs are at the heart of LLM agents, and most agent learning is achieved through the learning capabilities of LLMs.Key learning methods include Incontext Learning (ICL), Fine-tuning (FT), and Reinforcement Learning (RL).In-context learning is a paradigm that allows language models to learn tasks given only a few examples in the form of demonstration (Dong et al., 2022).Specifically, early studies such as ReAct (Yao et al., 2022) and Voyager (Wang et al., 2023) use ICL to drive LLMs in building agents, achieving good results.However, since LLMs are not specifically trained for agent tasks and incontext learning does not modify model parameters, the performance improvements achievable through in-context learning remain limited.Fine-tuning is a technique that updates model parameters on a given dataset to enable an agent to learn specific tasks.Unlike fine-tuning for LLMs, agent fine-tuning involves training on diverse levels of agent trajectory data.Consequently, many studies (Chen et al., 2023a;Zeng et al., 2023;Yin et al., 2024) focus on obtaining high-quality trajectory operation data, aiming to equip LLM agents with robust capabilities in tool usage, memory retrieval, and long-term trajectory planning.However, it cannot address out-of-domain challenges due to the absence of environmental learning.To address this issue, recent studies have attempted to integrate RL into the learning process of LLM agents, enabling them to interact with their environment.For example, AGILE (He et al., 2024) treats the construction of LLM agents as an RL problem, using the LLM as a policy model and fine-tuning it through Proximal Policy Optimization (PPO) (Schulman et al., 2017).DigiRL (Bai et al., 2024) trains device control agents via a two-stage fine-tuning process: first initializing the model with offline RL and then transitioning from offline to online RL.However, RL training in LLM agents often suffers from instability and the challenge of designing effective reward functions, which require further research to address.It is worth noting that these learning methods are not isolated.The practice has shown that an LLM agent often employs multiple methods simultaneously during the learning process.This highlights that the integration of the aforemen-tioned three approaches will likely form a standardized learning process for agents in the future.</p>
<p>Memory Manage and Tool Update: In addition, updating the memory and tool modules is another promising direction, as such updates can enable agents to develop unique characteristics.Memory management refers to the processes of storing and organizing information during such interactions, forming a memory module characterized by specific environmental features.For instance, Generative Agents (Park et al., 2023) place a memory stream at their core, extracting observations from the environment into the main memory and retrieving them based on specific rules.Without memory management, agents would lack consistent behaviors, underscoring the effectiveness of learning with memory management mechanisms.ICAL (Sarch et al., 2024) introduces a method for building a memory of multimodal experiences from sub-optimal demonstrations and human feedback.It demonstrates that by distilling and filtering experiences, as the agent's library of examples grows, the agent becomes more efficient.Tool update refers to the process by which tools are updated through interaction.</p>
<p>The motivation for tool updates arises from the fact that some tools often become outdated or malfunction, such as an agent failing to retrieve updated data from an outdated API endpoint.One interesting work named CLOVA (Gao et al., 2024) introduces a novel training-validation prompt tuning scheme to efficiently update tools while avoiding catastrophic forgetting.This work demonstrates that tool learning can optimize both the model and the tools themselves.In summary, the learnable components of LLM agents are not limited to LLMs; both the memory and tool modules can be updated and learned through interactions.</p>
<p>Conclusion</p>
<p>This paper advocates for a novel perspective on building LLM agents by drawing insights from computer systems, particularly the von Neumann architecture.To support our claim, we have drawn inspiration from the von Neumann architecture and designed a structured framework consisting of distinct modules for agent components-perception, cognition, memory, tools, and actions.We then summarize future directions by drawing analogies to the development of computer systems.Key areas for improvement include establishing principles for building LLM agents, refining memory structures, utilizing multi-core systems, and leveraging parallelization and pipelining to boost LLM agent efficiency.Finally, we have investigated the learning mechanisms of LLM agents and pointed out that improved learning methods are key to their future evolution.By leveraging these insights, we aim to guide the systematic construction and evolution of general LLM agents.In addition, these insights still require thorough experimental validation, which we will continue to explore in our future research.</p>
<p>B.2. Cognition Module</p>
<p>The cognition module serves as the core reasoning engine, responsible for planning and decision-making.It takes perceptual inputs x t , memory content M r , and tool use output T c as inputs to generate intermediate decisions.</p>
<p>C : (X , M, T ) → D,</p>
<p>where D represents the cognitive decision output space.Advanced reasoning techniques such as Chain-of-Thought (CoT), Tree-of-Thought (ToT), and Reflection mechanisms are employed to enhance decision-making.</p>
<p>B.3. Memory Module</p>
<p>Memory management in the framework includes both short-term (context window) and long-term (retrieval-augmented generation, RAG) storage, formulated as:
M : H → M,(4)
where H represents the historical interaction sequence, and M denotes the memory content retrieved for current processing.Key memory operations include:</p>
<p>• Memory Write: Storing new experience m t .</p>
<p>• Memory Read: Retrieving relevant historical data.</p>
<p>• Memory Manage: Optimizing memory utilization.</p>
<p>B.4. Tool Module</p>
<p>The tool module enables interaction with external toolset sources such as Google Search and Arxiv.It includes operations like retrieval, calling, and updating:
T : (Q, E) → T , (5)
where Q is the query space, E denotes tool set resources, and T represents the tool output used in decision-making.</p>
<p>B.5. Action Module</p>
<p>The action module selects and executes actions based on cognitive outputs and tool assistance:
a t = A(C(x t , M r , T c )),(6)
Figure 1 .
1
Figure 1.An analogy between von Neumann architecture and LLM agent architecture with task execution workflows (Appendix A).</p>
<p>Figure 2 .
2
Figure 2. A framework diagram of LLM agents inspired by the von Neumann architecture.This framework illustrates the different components of LLM agents and specific classifications within each module, along with the interactions between the modules.</p>
<p>Figure 3 .
3
Figure 3.An intuitive comparison between computer memory hierarchy and agent memory hierarchy.</p>
<p>Figure 4 .
4
Figure 4.A design decision diagram illustrating the principles of building LLM agents.</p>
<p>Table 1 .
1
Summary of module inclusion in landmark LLM agent studies based on the proposed framework.Specifically, we also include a summary of the learning mechanisms of LLM agents.We abbreviate the learning mechanism subclasses using initials.
AgentPerceptionCognitionMemoryToolEnvironmentLearning MechanismsModelReasoning Long-Term Short-TermLLMMemory ToolWebGPT (Nakano et al., 2021)UnimodalGPT-3✗✗✓✓GUIFT&amp;RL✗✗SayCan (Ahn et al., 2022)MultimodalPaLM✓✗✓✓Physical WorldICL✗✗ReAct (Yao et al., 2022)UnimodalPaLM✓✗✓✓QAICL✗✗Voyager (Wang et al., 2023)MultimodalGPT-4✓✓✓✓GameICL✓✓Generative Agents (Park et al., 2023)UnimodalGPT-3.5✓✓✓✗Virtual WorldICL✓✗AppAgent (Zhang et al., 2023)MultimodalGPT-4✗✓✓✗GUIICL✗✗ChemCrow (Bran et al., 2023)UnimodalGPT-4✓✗✓✓ChemistryICL✗✗MemGPT (Packer et al., 2023)UnimodalGPT-4✗✓✓✗QAICL✓✗ChatDev (Qian et al., 2023)UnimodalGPT-3.5✗✓✓✓SoftwareICL✓✗LEO (Huang et al., 2024)MultimodalVicuna-7B✓✗✓✗Virtual WorldICL&amp;FT✗✗VideoAgent (Fan et al., 2025)MultimodalGPT-4✓✓✓✓QAICL✓✗Agent Q (Putta et al., 2024)MultimodalLLaMA-3✓✗✓✗GUIFT&amp;RL✗✗Reflexion (Shinn et al., 2024)UnimodalGPT-3✓✓✓✓Code Execution ICL&amp;RL✓✗HuggingGPT (Shen et al., 2024)MultimodalGPT-4✓✗✓✓QAICL✗✓Jarvis-1 (Wang et al., 2024b)MultimodalGPT-4✓✓✓✓GameICL✓✓SWE-agent (Yang et al., 2024)UnimodalGPT-4✓✗✓✓SoftwareICL✗✗DigiRL (Bai et al., 2024)Multimodal DigiRL-1.3B✗✗✓✓GUIFT&amp;RL✗✗CLOVA (Gao et al., 2024)MultimodalGPT-4✓✗✓✓QAICL&amp;FT✗✓MDagents (Kim et al., 2024)MultimodalGPT-4✓✓✓✗QAICL✓✗OS-Copilot (Wu et al., 2024)MultimodalGPT4✓✓✓✓GUIICL&amp;FT✓✓
detailed analyses of various components of LLM agents and the comparison drawn with the von Neumann architecture, we observe striking similarities that can inspire agent design.</p>
<p>for time step t, it has:a t = A C P (o 1 , a 1 , ..., o t−1 , a t−1 , o t ), M r , T c ,(7)where T c is the retrieved tool information.Actions can be categorized into internal and external types.For external actions, they alter the external environment, transitioning from o t−1 to o t .Internal actions involve searching memory, invoking tools, and performing reasoning, which do not directly affect external observations but instead modify the internal state of the LLM agents.
B.6. Learning ParadigmsOur framework supports multiple learning approaches to enhance performance over time:B.6.1. IN-CONTEXT LEARNING
In-context learning allows the model to make predictions based on provided examples without updating parameters:a t = arg max a∈A P (a | o 1 , a 1 , . .., o t−1 , a t−1 , o t )(8)</p>
<p>State Key Laboratory of General Artificial Intelligence, BIGAI
Harbin Institute of Technology
School of Intelligence Science and Technology, Peking University. Correspondence to: Qing Li <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#100;&#121;&#108;&#97;&#110;&#46;&#108;&#105;&#113;&#105;&#110;&#103;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#100;&#121;&#108;&#97;&#110;&#46;&#108;&#105;&#113;&#105;&#110;&#103;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a>.
A. Analogy ExamplesOur Figure1compares von Neumann architecture and LLM agent architecture with task execution workflows.The structural comparison on the left side demonstrates the similarity between the two architectures, showing that they are highly alike in terms of modules and their interconnections.On the right side, the workflow comparison during task execution illustrates that they also share similarities in how tasks are performed.A.1. von Neumann architectureTo intuitively demonstrate the working process of a computer (von Neumann architecture), the example in our diagram is presented using the following code.The following Python script named run.pycalculates the total size of all image files in a given directory.if <strong>name</strong> == "<strong>main</strong>": directory = sys.argv[1]Calculate_Sizes(directory)A.2. LLM agent architectureFor LLM agent architecture, we ask some agentic systems like ChatGPT.The query is "Which electronic product do I use and check its latest price".Since the diagram we use is a preliminary conceptual framework mainly intended to illustrate the similarity between the two.Additionally, to protect personal privacy, we do not provide execution diagrams for the examples here.You can try this prompt using the most advanced model on your personal ChatGPT.B. Framework FormalizationIn this appendix, we provide a detailed description of each component in the proposed framework F = (P, C, M, T, A), as illustrated in Figure2.This framework consists of perception, cognition, memory, tool usage, and action execution, facilitating interaction with an open-world environment.B.1. Perception ModuleThe perception module processes external inputs from the environment, supporting both unimodal and multimodal data processing:where O represents raw observations, and X denotes the extracted feature representations.This module enables the agent to interpret sensory data from multiple sources such as images, text, and audio.It is noteworthy that past observations are often combined with past actions for perception, where actions also serve as an external source of information.Of course, in different designs, past actions and observations can be stored in memory, but this transformation does not bring about significant changes.B.6.2. FINE-TUNINGFine-tuning adjusts model parameters using a dataset D with gradient-based optimization:where L represents the loss function and θ are model parameters.B.6.3. REINFORCEMENT LEARNINGIn reinforcement learning (RL), the agent learns through interactions with the environment by maximizing cumulative rewards:where π is the policy, γ is the discount factor, and R represents the reward function.C. Golden insights of computer system designGolden insights of computer system design Abstraction: Hide implementation details and provide simplified interfaces to reduce complexity.Modularity: Break the system into functional modules with clear interfaces.Unified data representation: Unified data representation ensures seamless module integration.Scalability: Systems should be designed to handle growth in terms of users, data, or computational requirements without significant re-architecture.The end-to-end principle: This principle states that certain functions in a system should be implemented at the endpoints rather than in the middle of a communication system.For example, reliability mechanisms like error checking are more effective when placed at the application's endpoints rather than in intermediate network layers.The principle of least privilege: A system component or user should only have the minimum privileges necessary to perform its tasks.This minimizes the risk of misuse or errors affecting the overall system.Layering: Systems should be built in layers, where each layer provides a specific set of services to the layer above while using the services of the layer below.This hierarchical approach aids in abstraction and simplifies debugging and maintenance.The robustness principle (Postel's Law): "Be conservative in what you do, be liberal in what you accept from others."This principle encourages designing systems to be strict in output and tolerant in input to promote interoperability.Fail-Fast Systems: A system should detect and report errors as early as possible, rather than allowing them to propagate unnoticed.This helps maintain system integrity and simplifies debugging.Concurrency: Systems should efficiently manage multiple simultaneous activities, taking advantage of parallelism when possible.Transparency: A system should aim to hide complexity from users where appropriate, such as in distributed systems, where failures or location details are often masked.Trade-off: System design involves balancing trade-offs, such as performance vs. correctness, simplicity vs. flexibility, and latency vs. throughput.
. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>ARM. Big.little processing. ARM White Paper. ARM Holdings. 2011</p>
<p>Digirl: Training in-the-wild devicecontrol agents with autonomous reinforcement learning. H Bai, Y Zhou, M Cemri, J Pan, A Suhr, S Levine, A Kumar, Advances in Neural Information Processing Systems (NeurIPS). 2024</p>
<p>A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, Chemcrow, arXiv:2304.05376Augmenting largelanguage models with chemistry tools. 2023arXiv preprint</p>
<p>Preliminary discussion of the logical design of an electronic computing instrument. Institute for Advanced Study. A W Burks, H H Goldstine, J Von Neumann, 1946</p>
<p>Computer: A History of the Information Machine. M Campbell-Kelly, W Aspray, 1996Westview PressBoulder, CO</p>
<p>B Chen, C Shu, E Shareghi, N Collier, K Narasimhan, S Yao, Fireact, arXiv:2310.05915Toward language agent fine-tuning. 2023aarXiv preprint</p>
<p>Llava-interactive: An all-in-one demo for image chat, segmentation, generation and editing. W.-G Chen, I Spiridonova, J Yang, J Gao, C Li, arXiv:2311.005712023barXiv preprint</p>
<p>A survey on in-context learning. Q Dong, L Li, D Dai, C Zheng, J Ma, R Li, H Xia, J Xu, Z Wu, T Liu, Annual Meeting of the Association for Computational Linguistics (ACL). 2022</p>
<p>Agent ai: Surveying the horizons of multimodal interaction. Z Durante, Q Huang, N Wake, R Gong, J S Park, B Sarkar, R Taori, Y Noda, D Terzopoulos, Y Choi, arXiv:2401.035682024arXiv preprint</p>
<p>Building open-ended embodied agents with internet-scale knowledge. L Fan, G Wang, Y Jiang, A Mandlekar, Y Yang, H Zhu, A Tang, D.-A Huang, Y Zhu, A Anandkumar, Minedojo, Advances in Neural Information Processing Systems. 202235</p>
<p>Videoagent: A memory-augmented multimodal agent for video understanding. Y Fan, X Ma, R Wu, Y Du, J Li, Z Gao, Q Li, European Conference on Computer Vision. Springer2025</p>
<p>Clova: A closed-loop visual assistant with tool usage and update. Z Gao, Y Du, X Zhang, X Ma, W Han, S.-C Zhu, Q Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Y Gu, B Zheng, B Gou, K Zhang, C Chang, S Srivastava, Y Xie, P Qi, H Sun, Y Su, arXiv:2411.06559Is your llm secretly a world model of the internet? model-based planning for web agents. 2024arXiv preprint</p>
<p>Agile: A novel reinforcement learning framework of llm agents. Y He, G Huang, Y Lin, H Zhang, Y Zhang, H Li, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>Computer Architecture: A Quantitative Approach. J L Hennessy, D A Patterson, 2011Morgan Kaufmann</p>
<p>Cogagent: A visual language model for gui agents. W Hong, W Wang, Q Lv, J Xu, W Yu, J Ji, Y Wang, Z Wang, Y Dong, M Ding, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>An embodied generalist agent in 3d world. J Huang, S Yong, X Ma, X Linghu, P Li, Y Wang, Q Li, S.-C Zhu, B Jia, S Huang, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)2024</p>
<p>Direct memory access: Overview and applications. K K A M Khawaja, A A Khan, International Journal of Computer Applications. 9412014</p>
<p>Adaptive collaboration strategy for llms in medical decision making. Y Kim, C Park, H Jeong, Y S Chan, X Xu, D Mcduff, C Breazeal, H W Park, Advances in Neural Information Processing Systems (NeurIPS). 2024</p>
<p>Crafting papers on machine learning. P Langley, Proceedings of the 17th International Conference on Machine Learning (ICML 2000). P Langley, the 17th International Conference on Machine Learning (ICML 2000)Stanford, CAMorgan Kaufmann2000</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W.-T Yih, T Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>K Q Lin, L Li, D Gao, Z Yang, S Wu, Z Bai, W Lei, L Wang, M Z Shou, Showui, arXiv:2411.17465One vision-language-action model for gui visual agent. 2024arXiv preprint</p>
<p>B Liu, Y Jiang, X Zhang, Q Liu, S Zhang, J Biswas, P Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023arXiv preprint</p>
<p>Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. J Lu, S An, M Lin, G Pergola, Y He, D Yin, X Sun, Y Wu, arXiv:2308.082392023arXiv preprint</p>
<p>R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, arXiv:2112.09332Browser-assisted question-answering with human feedback. 2021arXiv preprint</p>
<p>C Packer, S Wooders, K Lin, V Fang, S G Patil, I Stoica, J E Gonzalez, Memgpt, arXiv:2310.08560Towards llms as operating systems. 2023arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023</p>
<p>S G Patil, T Zhang, X Wang, J E Gonzalez, Gorilla, arXiv:2305.15334Large language model connected with massive apis. 2023arXiv preprint</p>
<p>Agent q: Advanced reasoning and learning for autonomous ai agents. P Putta, E Mills, N Garg, S Motwani, C Finn, D Garg, R Rafailov, arXiv:2408.071992024arXiv preprint</p>
<p>C Qian, X Cong, C Yang, W Chen, Y Su, J Xu, Z Liu, M Sun, arXiv:2307.07924Communicative agents for software development. 62023arXiv preprint</p>
<p>Y Qin, S Liang, Y Ye, K Zhu, L Yan, Y Lu, Y Lin, X Cong, X Tang, B Qian, arXiv:2307.16789Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Principles of Computer System Design: An Introduction. J H Saltzer, M F Kaashoek, 2009Morgan Kaufmann</p>
<p>Vlm agents generate their own memories: Distilling experience into embodied programs. G Sarch, L Jang, M J Tarr, W W Cohen, K Marino, K Fragkiadaki, Advances in Neural Information Processing Systems (NeurIPS). 2024</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Solving ai tasks with chatgpt and its friends in hugging face. Y Shen, K Song, X Tan, D Li, W Lu, Y Zhuang, Hugginggpt, Advances in Neural Information Processing Systems. 362024</p>
<p>Z Shi, S Gao, X Chen, Y Feng, L Yan, H Shi, D Yin, P Ren, S Verberne, Z Ren, arXiv:2403.03031Learning to use tools via cooperative and interactive agents. 2024arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Voyager: An openended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>X Wang, B Li, Y Song, F F Xu, X Tang, M Zhuge, J Pan, Y Song, B Li, J Singh, arXiv:2407.16741An open platform for ai software developers as generalist agents. 2024aarXiv preprint</p>
<p>Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. Z Wang, S Cai, A Liu, Y Jin, J Hou, B Zhang, H Lin, Z He, Z Zheng, Y Yang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024b</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Os-copilot: Towards generalist computer agents with self-improvement. Z Wu, C Han, Z Ding, Z Weng, Z Liu, S Yao, T Yu, L Kong, arXiv:2402.074562024arXiv preprint</p>
<p>Large multimodal agents: A survey. J Xie, Z Chen, R Zhang, X Wan, G Li, arXiv:2402.151162024arXiv preprint</p>
<p>Swe-agent: Agentcomputer interfaces enable automated software engineering. J Yang, C E Jimenez, A Wettig, K Lieret, S Yao, K Narasimhan, O Press, Advances in Neural Information Processing Systems (NeurIPS). 2024</p>
<p>Keep calm and explore: Language models for action generation in text-based games. S Yao, R Rao, M Hausknecht, K Narasimhan, arXiv:2010.029032020arXiv preprint</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.03629Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Agent lumos: Unified and modular training for open-source language agents. D Yin, F Brahman, A Ravichander, K Chandu, K.-W Chang, Y Choi, B Y Lin, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>S Yuan, K Song, J Chen, X Tan, Y Shen, R Kan, D Li, D Yang, arXiv:2401.06201Easytool: Enhancing llm-based agents with concise tool instruction. 2024arXiv preprint</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. A Zeng, M Liu, R Lu, B Wang, X Liu, Y Dong, J Tang, Annual Meeting of the Association for Computational Linguistics (ACL). 2023</p>
<p>C Zhang, Z Yang, J Liu, Y Han, X Chen, Z Huang, B Fu, G Yu, Appagent, arXiv:2312.13771Multimodal agents as smartphone users. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
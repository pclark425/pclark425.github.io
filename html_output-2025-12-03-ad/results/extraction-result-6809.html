<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6809 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6809</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6809</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-272910569</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.17539v1.pdf" target="_blank">Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain. To address this issue, some studies employ the approach of propositional logic to further enhance logical reasoning abilities of LLMs. However, the potential omissions in the extraction of logical expressions in these methods can cause information loss in the logical reasoning process, thereby generating incorrect results. To this end, we propose Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information from input context, and utilizes the generated logical information as an additional augmentation to the input prompts, thereby enhancing the capability of logical reasoning. The LoT is orthogonal to existing prompting methods and can be seamlessly integrated with them. Extensive experiments demonstrate that LoT boosts the performance of various prompting methods with a striking margin across five logical reasoning tasks. In particular, the LoT enhances Chain-of-Thought's performance on the ReClor dataset by +4.35%; moreover, it improves Chain-of-Thought with Self-Consistency's performance on LogiQA by +5%; additionally, it boosts performance of Tree-of-Thoughts on ProofWriter dataset by +8%.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6809.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6809.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting paradigm that (1) extracts propositional symbols and logical expressions from input text via LLMs, (2) expands them using propositional logical laws (implemented in a Python deduction module), and (3) translates the expanded expressions back to natural language and appends them to the original prompt to augment LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, GPT-3.5-turbo-instruct, GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as a prompt augmentation method on off-the-shelf transformer LLMs (GPT-3.5 variants and GPT-4) to improve strict logical reasoning by injecting derived natural-language logical propositions into the context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + prompt-based neuro-symbolic augmentation (propositional-logic extraction/extension/translation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not a training method — evaluated using logical reasoning benchmarks (ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO); no new model fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Propositional-symbol extraction via LLMs, symbolic logical expansion using logical laws (double negation, contraposition, transitivity) implemented in Python, then translation of derived expressions back into natural language for prompt augmentation; compatible with Chain-of-Thought, Self-Consistency, CoT-SC, and Tree-of-Thoughts.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Collection of logical reasoning benchmarks: ReClor and LogiQA (human-authored reading-comprehension logical questions), RuleTaker/ProofWriter (synthetic and rulebase entailment and proof tasks), FOLIO (diverse first-order logic reasoning examples).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>propositional entailment / multi-step logical deduction / proof-style reasoning (varies per dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>See per-dataset aggregated results: LoT alone (GPT-3.5): ReClor 56.02%, LogiQA 35.30%, RuleTaker 61.40%, ProofWriter 59.50%, FOLIO 75.00%. LoT alone (GPT-4): ReClor 77.98%, LogiQA 57.60%, RuleTaker 65.30%, ProofWriter 66.70%, FOLIO 86.00%. LoT combined with best prompting (LoT + CoT-SC(5)) achieved (GPT-3.5) ReClor 60.87%, LogiQA 39.50%, RuleTaker 65.50%, ProofWriter 67.50%, FOLIO 83.00%; (GPT-4) ReClor 82.61%, LogiQA 61.00%, RuleTaker 66.50%, ProofWriter 80.50%, FOLIO 86.00%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>LoT improved baseline prompting in most experiments (35/40 comparisons). Example deltas: LoT+CoT over CoT (GPT-3.5) on ReClor +4.35% (52.17%→56.52%); LoT improved CoT-SC on LogiQA by +5% (GPT-3.5: 34.50%→39.50%); LoT improved ToT on ProofWriter by +8% (GPT-4: ToT 70%→LoT+ToT 78%). Some negative deltas: LoT+SC slightly underperformed SC on RuleTaker and ProofWriter with GPT-4 (-1% and -1.5%), attributed to errors in logic extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Injecting expanded logical propositions as natural-language augmentations substantially improves strict logical reasoning across diverse datasets and prompting methods, is orthogonal to existing chain/tree prompt topologies, and can recover from some extraction errors because LLMs can reinterpret natural-language expansions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Supports a limited set of connectives and logical laws (¬, →, ∧ and three logical laws). Relies on LLM-based extraction which can produce omissions, repetitions, or erroneous extractions (LLM illusion/bias), causing occasional degraded performance; expansion and prompt complexity increase with additional connectives; sometimes overlaps with CoT's intrinsic deduction (leading to small/zero gains).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6809.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6809.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large commercial transformer-based LLM from OpenAI used as an evaluation backbone for LoT and baseline prompting methods in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pretrained transformer-based language model (proprietary), used here as an off-the-shelf reasoner; experiments apply prompting techniques (Direct, CoT, SC, CoT-SC, ToT) and LoT augmentations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting (Direct, Chain-of-Thought, Self-Consistency, Chain-of-Thought with Self-Consistency, Tree-of-Thoughts) optionally augmented by LoT.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See LoT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step logical deduction, reading-comprehension logical reasoning, proof generation/entailment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baseline (Direct) and prompting results reported in paper (excerpt): Direct 74.20% (ReClor), CoT 77.39% (ReClor), SC(5) 73.91% (ReClor), CoT-SC(5) 80.43% (ReClor). With LoT augmentations, LoT+CoT-SC(5) reaches 82.61% (ReClor). Full tables provide per-dataset values (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>GPT-4 benefits from LoT across most datasets; LoT+CoT-SC(5) often yields best results (e.g., ReClor improved from 80.43%→82.61%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-capacity LLM (GPT-4) attains strong baseline performance and still gains meaningful improvements from LoT augmentation, though extraction errors can occasionally offset gains on specific datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Same limitations as LoT when used with it (sensitivity to extraction errors); specific size/training corpus not disclosed in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6809.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6809.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo / GPT-3.5-turbo-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI GPT-3.5 family variants used as experimental backbones to evaluate LoT and baseline prompt methods; GPT-3.5-turbo-instruct was used to run SatLM implementation for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, GPT-3.5-turbo-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based chat models (GPT-3.5 family) used for zero-shot and few-shot prompting experiments with and without LoT augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompting (Direct, CoT, SC, CoT-SC) with LoT augmentation in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See LoT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step logical deduction, reading-comprehension logical reasoning, proof generation/entailment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative entries from paper (GPT-3.5): Direct ReClor 46.20%, CoT 52.17%, LoT 56.02%, LoT+CoT 56.52%, LoT+CoT-SC(5) 60.87% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>LoT yields consistent gains over baselines on GPT-3.5; e.g., CoT (52.17%) → LoT+CoT (56.52%) on ReClor (+4.35%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LoT yields sizeable improvements on mid-tier LLMs (GPT-3.5), often larger relative gains than for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Model capacity limits absolute performance; LoT still subject to extraction errors and limited logic law coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6809.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6809.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps from LLMs to improve multi-step problem solving and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to GPT-3.5 and GPT-4 in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model but a prompting method applied on transformer LLMs to produce step-by-step reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Stepwise decomposition of multi-step problems into intermediate natural-language reasoning steps produced by the model.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See LoT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step logical deduction and reading-comprehension logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples in paper: CoT (GPT-3.5) ReClor 52.17%; CoT (GPT-4) ReClor 77.39% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT generally improves over Direct prompting; LoT+CoT often improves further (e.g., +4.35% on ReClor for GPT-3.5).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT provides a basis for reasoning improvements; LoT is orthogonal and can augment CoT, though in some cases CoT already recovers hidden info overlapping with LoT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>CoT can be unfaithful (generated chain not consistent with final answer); may overlap with LoT capabilities so marginal gains can be small; unfaithfulness motivates neuro-symbolic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6809.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6809.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought with Self-Consistency (CoT-SC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that samples multiple Chain-of-Thought reasoning traces and aggregates answers (majority voting) to improve robustness and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to GPT-3.5 and GPT-4 in experiments (CoT-SC(5) indicates 5 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-ensemble method combining CoT step generation with majority voting over multiple sampled chains.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + chain-of-thought sampling + self-consistency aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Sample multiple CoT reasoning paths and use majority vote to select final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See LoT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step logical deduction and reading-comprehension logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples: CoT-SC(5) (GPT-3.5) ReClor 58.70%; CoT-SC(5) (GPT-4) ReClor 80.43%. LoT+CoT-SC(5) (GPT-3.5) ReClor 60.87% (+2.17).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT-SC often outperforms individual CoT traces; LoT+CoT-SC often yields best overall results across datasets in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining LoT with CoT-SC gives consistently strong performance (often best), demonstrating LoT is complementary to ensemble-of-reasoning-path approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Computational overhead due to multiple samples; LoT+CoT-SC sometimes underperforms CoT-SC in some setups due to extraction bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6809.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6809.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Majority-vote aggregation of multiple direct or reasoning outputs (sampling-based ensemble) to improve answer robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to GPT-3.5 and GPT-4 in experiments (SC(5) indicates 5 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aggregation method applied to multiple sampled outputs from an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + sampling + majority-vote aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Aggregate multiple sampled outputs (Direct or CoT traces) and take majority answer.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See LoT entry.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>logical reasoning / multi-step deduction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples: SC(5) (GPT-3.5) ReClor 56.52%; SC(5) (GPT-4) ReClor 73.91%. LoT+SC improved many scores (e.g., ReClor GPT-4: 80.43% with LoT+SC(5) reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>LoT often improves SC (many positive deltas); paper reports LoT+SC achieved highest accuracy on FOLIO with GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SC is an effective aggregation technique and benefits from LoT augmentations in most settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Sampling increases compute; extraction deviations in LoT can sometimes worsen SC results on specific datasets (RuleTaker/ProofWriter with GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6809.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6809.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A search-based prompting topology that explores a tree of intermediate 'thought' states and supports backtracking and branch exploration to solve complex problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied with GPT-4 in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Search/topology-based prompting that generates multiple candidate states at each step, verifies state success against constraints, and explores branches up to a given depth.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + tree-search prompting topology</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Thought-state generation, branch exploration, success verification and backtracking (few-shot prompting used in paper experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter (depth 5)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>ProofWriter: synthetic natural-language rulebases and many entailment/proof questions; here used with deep deduction (depth 5) to test ToT.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step deductive reasoning / proof search</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On ProofWriter (depth 5) with GPT-4: Direct 51%, ToT 70%, LoT+ToT 78% (LoT+ToT = +8% over ToT). Table and figure detail these gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>ToT substantially outperforms Direct on deep deductive tasks; LoT further improves ToT (total states +2.14%, successful states +2.59%, full-reasoning rate +2%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LoT augments ToT's exploration space and successful-state yield, producing meaningful gains on deep-rule proof tasks (e.g., +8% absolute accuracy on ProofWriter depth-5).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>ToT is computationally heavier; performance depends on quality of propositions available—LoT's extraction errors can still affect ToT but often LoT increases ToT's successful exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6809.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6809.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SatLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SatLM (Satisfiability-aided Language Models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic approach that uses LLMs to produce declarative task specifications which are then solved by an automated theorem/SAT solver to yield final answers, aiming for faithful logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Satlm: Satisfiability-aided language models using declarative prompting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Implemented with GPT-3.5-turbo-instruct in comparative experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neuro-symbolic pipeline: LLMs generate formal symbolic encodings of problems and a satisfiability/theorem solver is invoked to derive solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + external SAT/theorem prover (neuro-symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Symbolic formalization by LLM followed by solver-based deduction (external theorem/SAT solver).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Automated theorem/SAT solver (as in SatLM) used to derive final answers from declarative specifications generated by the LLM; in paper SatLM's pipeline was re-used for comparison and invoked via GPT-3.5-turbo-instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ReClor (subset of 46 LSAT-like examples used for direct comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>ReClor: LSAT/GMAT style logical reasoning items; subset selected to match prior neuro-symbolic evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>propositional/first-order style logical entailment on reading-comprehension items</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports SatLM performed poorly under their setup compared to prompting baselines; precise numeric values are shown in Figure 4 (paper) but not enumerated in main text tables; relative comparisons: LoT outperformed SatLM across Reclor and prompting methods (Direct +1.74%, CoT +2.18%, SC +6.52%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>LoT significantly outperformed SatLM on Reclor and other measures; the paper attributes SatLM's weaker performance to information-loss and semantic mistakes during symbolic extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Neuro-symbolic methods like SatLM risk critical information loss during symbolic extraction (e.g., mis-parsing 'can' as 'abilities'), which harms solver-based reasoning; LoT's retention of original natural-language context helps avoid catastrophic propagation of symbolic extraction errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on correctness of symbolic extraction; small semantic extraction errors propagate directly to solver outputs, causing poor final answers; paper reports SatLM confusion in mapping natural-language predicates (case study).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6809.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6809.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LINC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LINC (neuro-symbolic pipeline combining LLMs with first-order provers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses LLMs as semantic parsers to convert natural-language premises and conclusions into first-order logic formulas, then uses an external prover to deduce results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neuro-symbolic pipeline: LLM-based semantic parsing to FOL + external theorem prover (e.g., Prover9) for deductive reasoning (discussed as related work and contrasted with LoT).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + symbolic theorem prover</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Symbolic formalization followed by external theorem-prover deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>First-order logic prover (e.g., Prover9) used to derive formal symbolic deductions after LLM translation; paper notes LINC can lose factual information during symbolic extraction (example: lost constants Person(Harry), Book(Walden)).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>first-order logic deduction (as applied in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Mentioned as having information-loss problems due to omissions during symbolic extraction; motivates LoT design which preserves original text and augments with natural-language logical inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Symbolic solvers can be undermined by omitted facts in the extraction step; LoT aims to avoid full dependence on black-box symbolic solvers by keeping expansions in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Vulnerable to information loss during parsing to symbolic form; errors in symbolization directly propagate to prover results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6809.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6809.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic approach that uses LLMs to convert natural language problems into symbolic formulas and then uses a symbolic solver to reason, with a self-refinement module that uses solver error messages to correct formalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline combining LLM symbolic formalization, symbolic solvers for reasoning, and an LLM-based self-refinement loop informed by solver error messages.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + symbolic solver + refinement loop (neuro-symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Symbolic formalization by LLM, external symbolic solver, and iterative self-refinement of the formalization.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>External symbolic/theorem solver used to derive answers and provide feedback for refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>symbolic logical reasoning / theorem proving (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a neuro-symbolic approach that attempts to address unfaithful reasoning by combining symbolic solvers with LLMs and refinement from solver feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Still susceptible to information loss in initial formalization; complexity of integrating solver feedback into reliable self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Satlm: Satisfiability-aided language models using declarative prompting <em>(Rating: 2)</em></li>
                <li>Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6809",
    "paper_id": "paper-272910569",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "LoT",
            "name_full": "Logic-of-Thought prompting",
            "brief_description": "A prompting paradigm that (1) extracts propositional symbols and logical expressions from input text via LLMs, (2) expands them using propositional logical laws (implemented in a Python deduction module), and (3) translates the expanded expressions back to natural language and appends them to the original prompt to augment LLM reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, GPT-3.5-turbo-instruct, GPT-4",
            "model_description": "Applied as a prompt augmentation method on off-the-shelf transformer LLMs (GPT-3.5 variants and GPT-4) to improve strict logical reasoning by injecting derived natural-language logical propositions into the context.",
            "model_size": null,
            "architecture_type": "transformer + prompt-based neuro-symbolic augmentation (propositional-logic extraction/extension/translation)",
            "training_data": "Not a training method — evaluated using logical reasoning benchmarks (ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO); no new model fine-tuning reported.",
            "reasoning_method": "Propositional-symbol extraction via LLMs, symbolic logical expansion using logical laws (double negation, contraposition, transitivity) implemented in Python, then translation of derived expressions back into natural language for prompt augmentation; compatible with Chain-of-Thought, Self-Consistency, CoT-SC, and Tree-of-Thoughts.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO",
            "benchmark_description": "Collection of logical reasoning benchmarks: ReClor and LogiQA (human-authored reading-comprehension logical questions), RuleTaker/ProofWriter (synthetic and rulebase entailment and proof tasks), FOLIO (diverse first-order logic reasoning examples).",
            "task_type": "propositional entailment / multi-step logical deduction / proof-style reasoning (varies per dataset)",
            "performance_metric": "accuracy (%)",
            "performance_value": "See per-dataset aggregated results: LoT alone (GPT-3.5): ReClor 56.02%, LogiQA 35.30%, RuleTaker 61.40%, ProofWriter 59.50%, FOLIO 75.00%. LoT alone (GPT-4): ReClor 77.98%, LogiQA 57.60%, RuleTaker 65.30%, ProofWriter 66.70%, FOLIO 86.00%. LoT combined with best prompting (LoT + CoT-SC(5)) achieved (GPT-3.5) ReClor 60.87%, LogiQA 39.50%, RuleTaker 65.50%, ProofWriter 67.50%, FOLIO 83.00%; (GPT-4) ReClor 82.61%, LogiQA 61.00%, RuleTaker 66.50%, ProofWriter 80.50%, FOLIO 86.00%.",
            "comparison_with_baseline": "LoT improved baseline prompting in most experiments (35/40 comparisons). Example deltas: LoT+CoT over CoT (GPT-3.5) on ReClor +4.35% (52.17%→56.52%); LoT improved CoT-SC on LogiQA by +5% (GPT-3.5: 34.50%→39.50%); LoT improved ToT on ProofWriter by +8% (GPT-4: ToT 70%→LoT+ToT 78%). Some negative deltas: LoT+SC slightly underperformed SC on RuleTaker and ProofWriter with GPT-4 (-1% and -1.5%), attributed to errors in logic extraction.",
            "key_findings": "Injecting expanded logical propositions as natural-language augmentations substantially improves strict logical reasoning across diverse datasets and prompting methods, is orthogonal to existing chain/tree prompt topologies, and can recover from some extraction errors because LLMs can reinterpret natural-language expansions.",
            "limitations": "Supports a limited set of connectives and logical laws (¬, →, ∧ and three logical laws). Relies on LLM-based extraction which can produce omissions, repetitions, or erroneous extractions (LLM illusion/bias), causing occasional degraded performance; expansion and prompt complexity increase with additional connectives; sometimes overlaps with CoT's intrinsic deduction (leading to small/zero gains).",
            "uuid": "e6809.0",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A large commercial transformer-based LLM from OpenAI used as an evaluation backbone for LoT and baseline prompting methods in the paper's experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large pretrained transformer-based language model (proprietary), used here as an off-the-shelf reasoner; experiments apply prompting techniques (Direct, CoT, SC, CoT-SC, ToT) and LoT augmentations.",
            "model_size": null,
            "architecture_type": "transformer",
            "training_data": null,
            "reasoning_method": "Prompting (Direct, Chain-of-Thought, Self-Consistency, Chain-of-Thought with Self-Consistency, Tree-of-Thoughts) optionally augmented by LoT.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO",
            "benchmark_description": "See LoT entry.",
            "task_type": "multi-step logical deduction, reading-comprehension logical reasoning, proof generation/entailment",
            "performance_metric": "accuracy (%)",
            "performance_value": "Baseline (Direct) and prompting results reported in paper (excerpt): Direct 74.20% (ReClor), CoT 77.39% (ReClor), SC(5) 73.91% (ReClor), CoT-SC(5) 80.43% (ReClor). With LoT augmentations, LoT+CoT-SC(5) reaches 82.61% (ReClor). Full tables provide per-dataset values (Table 3).",
            "comparison_with_baseline": "GPT-4 benefits from LoT across most datasets; LoT+CoT-SC(5) often yields best results (e.g., ReClor improved from 80.43%→82.61%).",
            "key_findings": "High-capacity LLM (GPT-4) attains strong baseline performance and still gains meaningful improvements from LoT augmentation, though extraction errors can occasionally offset gains on specific datasets.",
            "limitations": "Same limitations as LoT when used with it (sensitivity to extraction errors); specific size/training corpus not disclosed in paper.",
            "uuid": "e6809.1",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5-turbo / GPT-3.5-turbo-instruct",
            "brief_description": "OpenAI GPT-3.5 family variants used as experimental backbones to evaluate LoT and baseline prompt methods; GPT-3.5-turbo-instruct was used to run SatLM implementation for comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, GPT-3.5-turbo-instruct",
            "model_description": "Transformer-based chat models (GPT-3.5 family) used for zero-shot and few-shot prompting experiments with and without LoT augmentation.",
            "model_size": null,
            "architecture_type": "transformer",
            "training_data": null,
            "reasoning_method": "Prompting (Direct, CoT, SC, CoT-SC) with LoT augmentation in experiments.",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO",
            "benchmark_description": "See LoT entry.",
            "task_type": "multi-step logical deduction, reading-comprehension logical reasoning, proof generation/entailment",
            "performance_metric": "accuracy (%)",
            "performance_value": "Representative entries from paper (GPT-3.5): Direct ReClor 46.20%, CoT 52.17%, LoT 56.02%, LoT+CoT 56.52%, LoT+CoT-SC(5) 60.87% (Table 3).",
            "comparison_with_baseline": "LoT yields consistent gains over baselines on GPT-3.5; e.g., CoT (52.17%) → LoT+CoT (56.52%) on ReClor (+4.35%).",
            "key_findings": "LoT yields sizeable improvements on mid-tier LLMs (GPT-3.5), often larger relative gains than for GPT-4.",
            "limitations": "Model capacity limits absolute performance; LoT still subject to extraction errors and limited logic law coverage.",
            "uuid": "e6809.2",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought prompting (CoT)",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps from LLMs to improve multi-step problem solving and interpretability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to GPT-3.5 and GPT-4 in experiments",
            "model_description": "Not a model but a prompting method applied on transformer LLMs to produce step-by-step reasoning traces.",
            "model_size": null,
            "architecture_type": "transformer + chain-of-thought prompting",
            "training_data": null,
            "reasoning_method": "Stepwise decomposition of multi-step problems into intermediate natural-language reasoning steps produced by the model.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO",
            "benchmark_description": "See LoT entry.",
            "task_type": "multi-step logical deduction and reading-comprehension logical reasoning",
            "performance_metric": "accuracy (%)",
            "performance_value": "Examples in paper: CoT (GPT-3.5) ReClor 52.17%; CoT (GPT-4) ReClor 77.39% (Table 3).",
            "comparison_with_baseline": "CoT generally improves over Direct prompting; LoT+CoT often improves further (e.g., +4.35% on ReClor for GPT-3.5).",
            "key_findings": "CoT provides a basis for reasoning improvements; LoT is orthogonal and can augment CoT, though in some cases CoT already recovers hidden info overlapping with LoT.",
            "limitations": "CoT can be unfaithful (generated chain not consistent with final answer); may overlap with LoT capabilities so marginal gains can be small; unfaithfulness motivates neuro-symbolic methods.",
            "uuid": "e6809.3",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CoT-SC",
            "name_full": "Chain-of-Thought with Self-Consistency (CoT-SC)",
            "brief_description": "A method that samples multiple Chain-of-Thought reasoning traces and aggregates answers (majority voting) to improve robustness and accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to GPT-3.5 and GPT-4 in experiments (CoT-SC(5) indicates 5 samples)",
            "model_description": "Prompt-ensemble method combining CoT step generation with majority voting over multiple sampled chains.",
            "model_size": null,
            "architecture_type": "transformer + chain-of-thought sampling + self-consistency aggregation",
            "training_data": null,
            "reasoning_method": "Sample multiple CoT reasoning paths and use majority vote to select final answer.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO",
            "benchmark_description": "See LoT entry.",
            "task_type": "multi-step logical deduction and reading-comprehension logical reasoning",
            "performance_metric": "accuracy (%)",
            "performance_value": "Examples: CoT-SC(5) (GPT-3.5) ReClor 58.70%; CoT-SC(5) (GPT-4) ReClor 80.43%. LoT+CoT-SC(5) (GPT-3.5) ReClor 60.87% (+2.17).",
            "comparison_with_baseline": "CoT-SC often outperforms individual CoT traces; LoT+CoT-SC often yields best overall results across datasets in the paper.",
            "key_findings": "Combining LoT with CoT-SC gives consistently strong performance (often best), demonstrating LoT is complementary to ensemble-of-reasoning-path approaches.",
            "limitations": "Computational overhead due to multiple samples; LoT+CoT-SC sometimes underperforms CoT-SC in some setups due to extraction bias.",
            "uuid": "e6809.4",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SC",
            "name_full": "Self-Consistency (SC)",
            "brief_description": "Majority-vote aggregation of multiple direct or reasoning outputs (sampling-based ensemble) to improve answer robustness.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to GPT-3.5 and GPT-4 in experiments (SC(5) indicates 5 samples)",
            "model_description": "Aggregation method applied to multiple sampled outputs from an LLM.",
            "model_size": null,
            "architecture_type": "transformer + sampling + majority-vote aggregation",
            "training_data": null,
            "reasoning_method": "Aggregate multiple sampled outputs (Direct or CoT traces) and take majority answer.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "ReClor, LogiQA, RuleTaker, ProofWriter, FOLIO",
            "benchmark_description": "See LoT entry.",
            "task_type": "logical reasoning / multi-step deduction",
            "performance_metric": "accuracy (%)",
            "performance_value": "Examples: SC(5) (GPT-3.5) ReClor 56.52%; SC(5) (GPT-4) ReClor 73.91%. LoT+SC improved many scores (e.g., ReClor GPT-4: 80.43% with LoT+SC(5) reported).",
            "comparison_with_baseline": "LoT often improves SC (many positive deltas); paper reports LoT+SC achieved highest accuracy on FOLIO with GPT-4.",
            "key_findings": "SC is an effective aggregation technique and benefits from LoT augmentations in most settings.",
            "limitations": "Sampling increases compute; extraction deviations in LoT can sometimes worsen SC results on specific datasets (RuleTaker/ProofWriter with GPT-4).",
            "uuid": "e6809.5",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree-of-Thoughts (ToT)",
            "brief_description": "A search-based prompting topology that explores a tree of intermediate 'thought' states and supports backtracking and branch exploration to solve complex problems.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "mention_or_use": "use",
            "model_name": "Applied with GPT-4 in experiments",
            "model_description": "Search/topology-based prompting that generates multiple candidate states at each step, verifies state success against constraints, and explores branches up to a given depth.",
            "model_size": null,
            "architecture_type": "transformer + tree-search prompting topology",
            "training_data": null,
            "reasoning_method": "Thought-state generation, branch exploration, success verification and backtracking (few-shot prompting used in paper experiments).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "ProofWriter (depth 5)",
            "benchmark_description": "ProofWriter: synthetic natural-language rulebases and many entailment/proof questions; here used with deep deduction (depth 5) to test ToT.",
            "task_type": "multi-step deductive reasoning / proof search",
            "performance_metric": "accuracy (%)",
            "performance_value": "On ProofWriter (depth 5) with GPT-4: Direct 51%, ToT 70%, LoT+ToT 78% (LoT+ToT = +8% over ToT). Table and figure detail these gains.",
            "comparison_with_baseline": "ToT substantially outperforms Direct on deep deductive tasks; LoT further improves ToT (total states +2.14%, successful states +2.59%, full-reasoning rate +2%).",
            "key_findings": "LoT augments ToT's exploration space and successful-state yield, producing meaningful gains on deep-rule proof tasks (e.g., +8% absolute accuracy on ProofWriter depth-5).",
            "limitations": "ToT is computationally heavier; performance depends on quality of propositions available—LoT's extraction errors can still affect ToT but often LoT increases ToT's successful exploration.",
            "uuid": "e6809.6",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SatLM",
            "name_full": "SatLM (Satisfiability-aided Language Models)",
            "brief_description": "A neuro-symbolic approach that uses LLMs to produce declarative task specifications which are then solved by an automated theorem/SAT solver to yield final answers, aiming for faithful logical reasoning.",
            "citation_title": "Satlm: Satisfiability-aided language models using declarative prompting",
            "mention_or_use": "use",
            "model_name": "Implemented with GPT-3.5-turbo-instruct in comparative experiments",
            "model_description": "Neuro-symbolic pipeline: LLMs generate formal symbolic encodings of problems and a satisfiability/theorem solver is invoked to derive solutions.",
            "model_size": null,
            "architecture_type": "transformer + external SAT/theorem prover (neuro-symbolic)",
            "training_data": null,
            "reasoning_method": "Symbolic formalization by LLM followed by solver-based deduction (external theorem/SAT solver).",
            "external_tool_used": true,
            "external_tool_description": "Automated theorem/SAT solver (as in SatLM) used to derive final answers from declarative specifications generated by the LLM; in paper SatLM's pipeline was re-used for comparison and invoked via GPT-3.5-turbo-instruct.",
            "benchmark_name": "ReClor (subset of 46 LSAT-like examples used for direct comparison)",
            "benchmark_description": "ReClor: LSAT/GMAT style logical reasoning items; subset selected to match prior neuro-symbolic evaluations.",
            "task_type": "propositional/first-order style logical entailment on reading-comprehension items",
            "performance_metric": "accuracy (%)",
            "performance_value": "Paper reports SatLM performed poorly under their setup compared to prompting baselines; precise numeric values are shown in Figure 4 (paper) but not enumerated in main text tables; relative comparisons: LoT outperformed SatLM across Reclor and prompting methods (Direct +1.74%, CoT +2.18%, SC +6.52%).",
            "comparison_with_baseline": "LoT significantly outperformed SatLM on Reclor and other measures; the paper attributes SatLM's weaker performance to information-loss and semantic mistakes during symbolic extraction.",
            "key_findings": "Neuro-symbolic methods like SatLM risk critical information loss during symbolic extraction (e.g., mis-parsing 'can' as 'abilities'), which harms solver-based reasoning; LoT's retention of original natural-language context helps avoid catastrophic propagation of symbolic extraction errors.",
            "limitations": "Relies on correctness of symbolic extraction; small semantic extraction errors propagate directly to solver outputs, causing poor final answers; paper reports SatLM confusion in mapping natural-language predicates (case study).",
            "uuid": "e6809.7",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LINC",
            "name_full": "LINC (neuro-symbolic pipeline combining LLMs with first-order provers)",
            "brief_description": "A method that uses LLMs as semantic parsers to convert natural-language premises and conclusions into first-order logic formulas, then uses an external prover to deduce results.",
            "citation_title": "Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "Neuro-symbolic pipeline: LLM-based semantic parsing to FOL + external theorem prover (e.g., Prover9) for deductive reasoning (discussed as related work and contrasted with LoT).",
            "model_size": null,
            "architecture_type": "transformer + symbolic theorem prover",
            "training_data": null,
            "reasoning_method": "Symbolic formalization followed by external theorem-prover deduction.",
            "external_tool_used": true,
            "external_tool_description": "First-order logic prover (e.g., Prover9) used to derive formal symbolic deductions after LLM translation; paper notes LINC can lose factual information during symbolic extraction (example: lost constants Person(Harry), Book(Walden)).",
            "benchmark_name": "",
            "benchmark_description": "",
            "task_type": "first-order logic deduction (as applied in cited work)",
            "performance_metric": "",
            "performance_value": "",
            "comparison_with_baseline": "Mentioned as having information-loss problems due to omissions during symbolic extraction; motivates LoT design which preserves original text and augments with natural-language logical inferences.",
            "key_findings": "Symbolic solvers can be undermined by omitted facts in the extraction step; LoT aims to avoid full dependence on black-box symbolic solvers by keeping expansions in natural language.",
            "limitations": "Vulnerable to information loss during parsing to symbolic form; errors in symbolization directly propagate to prover results.",
            "uuid": "e6809.8",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Logic-LM",
            "name_full": "Logic-LM",
            "brief_description": "A neuro-symbolic approach that uses LLMs to convert natural language problems into symbolic formulas and then uses a symbolic solver to reason, with a self-refinement module that uses solver error messages to correct formalization.",
            "citation_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "Pipeline combining LLM symbolic formalization, symbolic solvers for reasoning, and an LLM-based self-refinement loop informed by solver error messages.",
            "model_size": null,
            "architecture_type": "transformer + symbolic solver + refinement loop (neuro-symbolic)",
            "training_data": null,
            "reasoning_method": "Symbolic formalization by LLM, external symbolic solver, and iterative self-refinement of the formalization.",
            "external_tool_used": true,
            "external_tool_description": "External symbolic/theorem solver used to derive answers and provide feedback for refinement.",
            "benchmark_name": "",
            "benchmark_description": "",
            "task_type": "symbolic logical reasoning / theorem proving (as cited)",
            "performance_metric": "",
            "performance_value": "",
            "comparison_with_baseline": "",
            "key_findings": "Cited as a neuro-symbolic approach that attempts to address unfaithful reasoning by combining symbolic solvers with LLMs and refinement from solver feedback.",
            "limitations": "Still susceptible to information loss in initial formalization; complexity of integrating solver feedback into reliable self-refinement.",
            "uuid": "e6809.9",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Satlm: Satisfiability-aided language models using declarative prompting",
            "rating": 2,
            "sanitized_title": "satlm_satisfiabilityaided_language_models_using_declarative_prompting"
        },
        {
            "paper_title": "Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
            "rating": 2,
            "sanitized_title": "linc_a_neurosymbolic_approach_for_logical_reasoning_by_combining_language_models_with_firstorder_logic_provers"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 1,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        }
    ],
    "cost": 0.0182585,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models
26 Sep 2024</p>
<p>Tongxuan Liu tongxuan.ltx@mail.ustc.edu.cn 
University of Science
Technology of China</p>
<p>Wenjiang Xu xuwenjiang2024@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences</p>
<p>Weizhe Huang 
University of Science
Technology of China</p>
<p>Xingyu Wang wangxingyu2024@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences</p>
<p>Jiaxing Wang wangjiaxing41@jd.com 
Hailong Yang hailong.yang@buaa.edu.cn 
Beihang University</p>
<p>Jing Li 
University of Science
Technology of China</p>
<p>Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models
26 Sep 20244AC778D9469DC66973D44AC653E0F6FCarXiv:2409.17539v1[cs.CL]Logical Propositions: A: It's ChristmasB: Cindy's hair is braidedC: It's a holidayD: Today is Easter A→BD→¬BB→CC→D
Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory.Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain.To address this issue, some studies employ the approach of propositional logic to further enhance logical reasoning abilities of LLMs.However, the potential omissions in the extraction of logical expressions in these methods can cause information loss in the logical reasoning process, thereby generating incorrect results.To this end, we propose Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information from input context, and utilizes the generated logical information as an additional augmentation to the input prompts, thereby enhancing the capability of logical reasoning.The LoT is orthogonal to existing prompting methods and can be seamlessly integrated with them.Extensive experiments demonstrate that LoT boosts the performance of various prompting methods with a striking margin across five logical reasoning tasks.In particular, the LoT enhances Chain-of-Thought's performance on the ReClor dataset by +4.35%; moreover, it improves Chain-of-Thought with Self-Consistency's performance on LogiQA by +5%; additionally, it boosts performance of Tree-of-Thoughts on ProofWriter dataset by +8%.</p>
<p>Introduction</p>
<p>In recent years, Large Language Models (LLMs) have demonstrated excellent capabilities across various NLP tasks [1,2,23].However, even the most advanced LLMs exhibit limited performance in mathematics and complex logical reasoning tasks [3,14].Chain-of-Thought (CoT) prompting [11,28,17] has emerged as a promising approach to improve logical reasoning capabilities, which enhances reasoning abilities by adding intermediate steps in the reasoning process.Subsequent research, such as [5,6,29], has sought to simulate human reasoning processes by expanding the Chain-of-Thought into more complex reasoning topology.Tree-of-Thoughts (ToT) [29] extends into a tree-like reasoning topology, exploring more reasoning branches at each step and supporting backtracking.Graph-of-Thoughts (GoT) [5] supports a graph reasoning topology, allowing for Figure 1: Comparison between LINC and LoT.The left part involves the workflow of LINC, which converts problems into logical expressions and then derives results using Prover9.However, LINC loses fact information Person(Harry) and Book(Walden), leading to errors.On the right side, LoT generates supplementary logical information "If a person reads a book, that person become smarter" that is seamlessly integrated into the original prompt, thereby enhancing the capability of LLMs to produce accurate results.the aggregation of multiple thoughts into new ones.STaR [33] and Chain-of-Thought with Self-Consistency (CoT-SC) [27] generate multiple chains of thought or reasoning paths, selecting the most optimized and consistent answers from these.However, [4,12,16,24] observe that LLMs occasionally exhibit unfaithful reasoning, wherein the derived conclusions do not adhere to the previously generated reasoning chain.</p>
<p>To tackle the challenge of the unfaithfulness in the reasoning process, researchers have proposed many neuro-symbolic methods that integrate LLMs with symbolic reasoning, such as Faithful Chainof-Thought [16], LINC [18], Logic-LM [20] and SatLM [30].These methods follow a similar process: Initially, the problem and objectives are translated into symbolic expressions.Subsequently, symbolic results are derived through external tools such as symbolic solvers.Finally, it's optional to explain symbolic results using LLMs or interpreters.However, these existing neuro-symbolic methods inevitably suffer from the issue of information loss, which results from omissions in the extraction of logical expressions and directly leads to incorrect intermediate reasoning processes.As illustrated in the Figure 1, in the extraction process of logical expressions in LINC, two key pieces of hidden information "Harry is a person" and "Walden is a book" are lost, which makes it impossible for the symbolic solver Prover9 to obtain the correct reasoning result.</p>
<p>To address the issue of information loss, in this paper, we propose a novel Logic-of-Thought (LoT) prompting method.Specifically, LoT extracts propositions and logical expressions from the input context, expands these expressions according to logical reasoning laws, translates the expanded logical expressions back into natural language, and utilizes the extended logical descriptions as additional augmentation to the input prompts for LLMs.The LoT prompting preserves the original input prompt while appending logical information described in natural language to guide the LLM's reasoning.Thus, LoT prompting prevents complete dependence on the symbolic solver and also avoids the potential issue of information loss inherent in existing methods' symbolic extraction processes.Additionally, the LoT prompting approach is compatible and orthogonal to existing prompting methods, enabling seamless integration of these methods.To validate the effectiveness of LoT, we conduct extensive experiments to evaluate its capability in boosting various prompting methods such as CoT, SC, CoT-SC and ToT across five logical reasoning datasets.Experimental results demonstrate that LoT prompting can seamlessly integrate with existing prompting methods and significantly boost their performance in logical reasoning.Specifically, LoT significantly enhances the performance of CoT on the ReClor dataset, achieving an improvement in accuracy up to +4.35%.Furthermore, LoT improves the SC's performance on the ReClor dataset by a remarkable +6.52%.Moreover, LoT boost the accuracy of CoT-SC on LogiQA by +5%.Additionally, LoT effectively elevates the performance of ToT on the ProofWriter dataset, resulting in a significant improvement of +8%.</p>
<p>The main contributions of this paper are as follows:</p>
<ol>
<li>
<p>We propose a novel prompting method Logic-of-Thought (LoT) to address the issue of information loss in existing neuro-symbolic methods by generating logical proposition descriptions as augmentations for original prompts.</p>
</li>
<li>
<p>We integrate LoT with a variety of distinct prompting techniques, including Chain-of-Thought (CoT), Self-Consistency (SC), Chain-of-Thought with Self-Consistency (CoT-SC), Tree-of-Thoughts (ToT), by leveraging the orthogonal capabilities of LoT.</p>
</li>
<li>
<p>We conduct extensive experiments to evaluate the effectiveness of LoT in enhancing the capabilities of different prompting techniques across diverse logical reasoning tasks.The results demonstrate the significant effectiveness of LoT in boosting the performance of various prompting methods.</p>
</li>
</ol>
<p>Preliminary</p>
<p>As this study focuses on logical reasoning tasks, we first provide some definitions and symbols about the propositional logic system, which will be used throughout the paper.</p>
<p>• Propositions are defined as declarative sentences that have clear truth-value characteristic and cannot be simultaneously true and false.In this context, propositions are considered fundamental elements of logical expressions.We use standard uppercase letters such as A, B, C to symbolize specific propositions, exemplified by statements like "you have keyboarding skills", and lowercase letters such as p, q, r to refer to any proposition.</p>
<p>• Connectives are defined as operators on propositions, which can operate on a single proposition or link propositions together to form a new logical expression, which is defined as a single proposition or a combination of propositions through connectives.In this study, We mainly focus on three connectives: ¬, → and ∧.Herein, negative ¬ denotes the negation operation for a specific logical symbol (e.g., ¬p represents the negation of p).Implication → signifies a sufficient condition or causal relationship between two propositions (e.g., p → q indicates that p is a sufficient condition for q).Conjunction ∧ also operates on two propositions, which represents that the entire expression is true only if both propositions are true (e.g., p ∧ q indicates p and q).</p>
<p>• Logical reasoning laws are defined as the deducing relation between two logical expressions.Meanwhile, ⇒ signifies that a certain logical expression can infer another logical expression.⇔ signifies that two logical expressions can be mutually inferred.In this study, we utilize three basic logical reasoning laws: the Double Negation Law ¬¬p ⇔ p, the Contraposition Law (p → q) ⇔ (¬q → ¬p), and the Transitive Law (p → q) ∧ (q → r) ⇒ (p → r), which all align with human intuition and are fundamental and widely used in propositional logic [7].</p>
<p>With these basic symbols and definitions, we can construct a propositional logic system and analyze complex logical reasoning processes.While the logic system setting presented here is straightforward, our paper primarily concentrates on introducing a new prompting paradigm to address information loss in existing neuro-symbolic methods.Moreover, notable enhancements have already been achieved within this setting (See Section 4.4).Therefore, we leave the exploration of incorporating more diverse connectives and laws to strengthen the logic system in our method to future work.</p>
<p>Logical Reasoning Laws:</p>
<p>Transitive Law: (p → q) ∧ (q → r) ⇒ (p → r) Double Negation Law：¬¬p ⇔ p Contraposition Law: (p → q) ⇔ (¬q → ¬p)</p>
<p>Extended Logical Expressions: C → A C → A:</p>
<p>If you are able to write your essays using a word processing program, then you have keyboarding skills.</p>
<p>Logical Deduction:</p>
<p>(¬A → ¬B) ∧ (¬B → ¬C) ⇒ (¬A → ¬C)
(¬A → ¬C) ⇒ (C → A)</p>
<p>LLMs Input</p>
<p>Figure 2: The framework of LoT consisting of three phases.On the left side of the diagram is the Logic Extraction phase, where we employ LLMs to extract propositions and logical relations.In the middle is the Logic Extension phase, where we apply logical reasoning laws to derive logical expressions.On the right side is the Logic Translation phase, where we utilize LLMs to translate logical expressions into their natural language descriptions.phase, the expanded logical expressions are translated into natural language descriptions of logical information through LLMs.And then, the logical information is incorporated into the input prompt, forming a comprehensive and novel input prompt for LLMs.The following sections provide detailed introduction to the phases of Logic Extraction, Logic Extension, and Logic Translation.</p>
<p>Logic Extraction.In the Logic Extraction phase, we use LLMs to extract formal logic expressions from the input context through two stages.Firstly, we instruct LLMs to select sentences containing conditional reasoning relationships from the input context to generate collection of sentences with logical relationships.Subsequently, we use LLMs to extract the set of propositional symbols P and the set of logical expressions E from the collection.During the process of Logic Extraction, LLMs identify propositions with similar meanings and represent them using identical propositional symbols.Furthermore, LLMs analyze the logical relationships between propositions from their natural language descriptions.For propositions expressing opposite meanings, the negation ¬ is added.When a conditional relationship exists between two propositions, the implication → is used to connect their corresponding propositional symbols.For example, as depicted in Figure 2, LLMs extract the same meaning description "be able to use a computer" from two different sentences, symbolized as B.Then, through analyzing its logical relationship with other propositions, LLMs apply ¬ to B and another proposition A and add → between them, which results in a new logical expression ¬A → ¬B.</p>
<p>Logic Extension.During the Logic Extension phase, we apply logical reasoning laws to the collection of logical expressions from the Logic Extraction phase.These logical expressions can be further expanded using a Python program to implement logical deduction.As illustrated in the Figure 2, the extracted logical expressions ¬A → ¬B and ¬B → ¬C serve as inputs for our logical deduction program.Through expansion based on Transitive Law and Contraposition Law, we finally obtain the new expression C → A, which will be used in the next phase.</p>
<p>Logic Translation.During the Logic Translation phase, we use LLMs to translate the generated extended logical expressions into natural language descriptions.Subsequently, we combine the natural language descriptions of propositional symbols according to the extended logical expressions to form a new part of the original input prompt.Through this approach, we inject the deduced logical information as additional augmentation into the original prompt, thus avoiding information loss.As shown in Figure 2, by associating C with its description "be able to write your essays using a word processing program", A with its description "have keyboarding skills", and → with the logical description "if...then...", we can translate the aforementioned logical expression C → A back to its natural language description and add it to original prompts as new input prompts.</p>
<p>Experiments</p>
<p>Tasks and Datasets</p>
<p>In the experiment, we employ five logical reasoning datasets: ReClor [32], LogiQA [15], RuleTaker [8], ProofWriter [22], and FOLIO [9].</p>
<p>• The ReClor dataset is collected from standardized test logical reasoning questions, including the Law School Admission Test (LSAT) and the Graduate Management Admission Test (GMAT).Each question is composed of a context, a question, and four answer options, with only one correct answer.• The LogiQA dataset is derived from expert-written questions for testing human logical reasoning, specifically the reading comprehension section, which is designed by domain experts to evaluate the logical reasoning ability of test participants.• The RuleTaker dataset is automatically generated via programming, utilizing connectives such as conjunction ∧, negation ¬, and implication →.Each question comprises a context and a conclusion.• The ProofWriter dataset comprises numerous small rulebases composed of facts and rules, and contains RuleTaker-style datasets with 500k questions, answers and proofs over naturallanguage rulebases.• The FOLIO dataset is a comprehensive and diverse dataset designed for reasoning in natural language, which is characterized by its human annotations, open-domain nature, and logical complexity.It boasts first-order logic (FOL) annotations, comprising 1,435 unique examples of conclusions.</p>
<p>Baselines</p>
<p>We consider five prompting approaches and one neuro-symbolic method, including: (1) Direct prompting, which simply answers the question without any reasoning process; (2) Self-Consistency (SC) [27], which employs majority voting to aggregate responses from multiple Direct prompting, represented as SC(5) when it involves 5 reasoning paths; (3) CoT [11,28,17], which utilizes a progressive thinking approach for reasoning; (4) CoT-SC [27], which applies majority voting to aggregate multiple Chain-of-Thought, represented as CoT-SC(5) when involving 5 reasoning paths;</p>
<p>(5) ToT [29], which models the reasoning process as a thought search tree; (6) SatLM [30], which leverages automated theorem provers to assist LLMs in reasoning.We conduct an evaluation of our LoT and all of the above baselines, and assess the integration of four prompting methods with LoT: CoT, SC, CoT-SC and ToT.</p>
<p>Experiment Setup</p>
<p>We utilize three pre-trained models: GPT-3.5-turbo-instruct,GPT-3.5-turbo[19], and GPT-4 [1] and conduct three types of experiments for comparison:</p>
<p>Main experiments.Main experiments employ four prompting methods including Direct, CoT, SC, CoT-SC and combination of these prompting methods with LoT on GPT-3.5-turbo and GPT-4 across five datasets.The experiments conduct here utilize the zero-shot prompt, employing default values for temperature, top_p, and top_k.For ReClor, we select 46 data entries from the dataset that examine deductive reasoning abilities in the implementation section.Regarding LogiQA, we randomly extract 200 data entries within the dataset.As for RuleTaker, we randomly select 200 data entries from the test set of the dataset, with the depth of reasoning randomly distributed between 1-5 layers.Concerning ProofWriter, we randomly extract 200 data entries with a depth of 5 from the validation set of the dataset.In relation to FOLIO, we extract 100 data entries from the validation set of the dataset.</p>
<p>Comparison between LoT and SatLM.We conduct a comparative analysis of SatLM's performance, benchmarking it against LoT, LoT+CoT, and LoT+CoT-SC using the ReClor dataset.Consistent with the approach outlined in [30], we utilize the GPT-3.5-turbo-instruct to leverage the SatLM implementation.To ensure compatibility with our experimental setup, we select a subset of the ReClor datasets, specifically a subset containing 46 data entries, as they closely mirror the LSAT dataset previously tested in [30].</p>
<p>Comparison between LoT and ToT.In this study, we evaluate the performance enhancement achieved by LoT under the guidance of ToT on the ProofWriter dataset, leveraging GPT-4.The ToT prompt employed in our analysis is based on the work presented in [31].In the experiments, the Direct, ToT, and LoT+ToT approaches are all implemented using few-shot prompting.For the ToT-related experiments, each successful state explores up to five new states.The success or failure of a state is verified by assessing its compliance with the established rules.The exploration process terminates either after achieving four successful state explorations or when no new states are available for exploration.The experiment utilizes 100 randomly selected data entries with a depth of 5 from the ProofWriter validation set.</p>
<p>A detailed description of the prompts used in all experiments can be found in the Appendix E.</p>
<p>Main Results</p>
<p>In this section, we integrate LoT prompting with four baseline prompting methods, namely Direct, CoT, SC and CoT-SC, to conduct a comparative analysis of whether LoT enhances logical reasoning abilities across five distinct datasets.The results presented in Table 1 reveal some key observations:</p>
<p>• Combining LoT with existing prompting methods consistently achieves best performance, which highlights the superiority of our methods.Specifically, LoT+CoT-SC(5) outperforms all other methods across all five datasets with GPT-3.5-turbo and four datasets with GPT-4.</p>
<p>LoT+SC achieves the highest accuracy rate in the FOLIO dataset with GPT-4.</p>
<p>• LoT prompting enhances the performance of four baseline prompting methods in most experiments, suggesting that LoT can be seamlessly integrated into existing prompting methods to further improve the logical reasoning ability of LLMs.Among total 40 comparisons (including four baseline prompting methods across five datasets with two LLMs), LoT significantly enhances the performance of baseline prompting methods in 35 instances.For example, in comparison to CoT using GPT-4, LoT+CoT can achieve great improvements in accuracy on five datasets, ReClor(+1.74%),LogiQA(+2.10%),RuleTaker(+3.20%),ProofWriter(+0.90%)and FOLIO(+0.40%),respectively.</p>
<p>• We find that a few negligible improvements all occur when CoT or CoT-SC is integrated with LoT.We conduct an analysis of specific examples of CoT and CoT+LoT on the dataset and observe that this is attributed to CoT's capacity to gradually deduce concealed information, which interestingly overlaps with the capabilities of LoT.(The Appendix D illustrates an example of the overlap in capabilities between CoT and LoT.)</p>
<p>• Upon utilizing GPT-4 exclusively on the RuleTaker and ProofWriter datasets, we observe that LoT+SC marginally trailed behind SC, recording a decline of 1% and 1.5% respectively.We find that the primary factor contributing to this issue within the LoT framework is the deviation in logical information extracted during the Logic Extraction process.To illustrate this, we provide an example in the Appendix C, analyzing a failure case where</p>
<p>Extended context:</p>
<p>Any great ape can learn to recognize its reflection in a mirror.Any animal that recognizes its reflection in a mirror has a sense of self.An animal has a sense of the minds of others only if it possesses a sense of self.</p>
<p>Question:</p>
<p>Which one of the following follows logically from the information above?Options: A. All great apes have the ability to develop a sense of the minds of others.B. Any animal that is incapable of developing a sense of self is not a great ape.C. If an animal is not a great ape it is not capable of developing a sense of the minds of others.D. Any animal that does not recognize its reflection in a mirror does not have a sense of self.</p>
<p>Input</p>
<p>LLMs</p>
<p>Extended context:</p>
<p>Any great ape can learn to recognize its reflection in a mirror.Any animal that recognizes its reflection in a mirror has a sense of self.An animal has a sense of the minds of others only if it possesses a sense of self.If a great ape can learn to recognize its reflection in a mirror, then it has a sense of self.</p>
<p>Question:</p>
<p>Which one of the following follows logically from the information above?Options:</p>
<p>A. All great apes have the ability to develop a sense of the minds of others.B. Any animal that is incapable of developing a sense of self is not a great ape.C. If an animal is not a great ape it is not capable of developing a sense of the minds of others.D. Any animal that does not recognize its reflection in a mirror does not have a sense of self.LoT's information extraction during the Logic Extraction phase is flawed.This example demonstrates how the information bias during this phase leads to an erroneous final result.</p>
<p>LoT</p>
<p>• LoT achieves significant enhancements in the accuracy of Direct across all datasets and outperforms CoT in eight out of ten sets of comparative data.Thus, this provides compelling evidence that the standalone utilization of LoT can achieve or even exceed the logical reasoning capability exhibited by CoT.In this section, we conduct a comprehensive comparison and analysis of LoT with a neuro-symbolic approach SatLM to delve deeper into their respective capabilities and potential differences.</p>
<p>Comparative Study of LoT and SatLM
6 D W/ 0 ' LU H F W / R 7 &amp; R 7 / R 7 &amp; R 7 6 &amp; / R 7 6 &amp; &amp; R 7 6 &amp; / R 7 &amp; R 7 6 &amp; $FFXUDF\
Performance Study.From Figure 4, it can be first observed that LoT significantly outperforms SatLM in terms of accuracy on the Reclor dataset as well as obtains notable improvements across various prompting methods, including Direct (+1.74%),CoT (+2.18%), and SC (+6.52%), which also shows LoT's effectiveness.But we observe that in this set of experiments, the performance of LoT+CoT-SC is inferior to that of CoT-SC.We speculate that this is due to a bias in the extraction of logical information, which is also discussed in Section 4.4.</p>
<p>Furthermore, SatLM exhibits poor performance under our setup compared to prompting methods.This is in line with our motivation that neuro-symbolic methods like SatLM, are more likely to encounter the issue of information loss when extracting logical symbolic expressions, significantly compromising their overall performance.Conversely, LoT maintains the original context, ensuring that possible minor information loss during the Logic Extraction phase may not necessarily exert a critical impact on the final results.</p>
<p>Case Study.We provide a comparative case study between SatLM and LoT based on our experiments.As depicted in Figure 3 logical extraction, SatLM erroneously employs "abilities" to represent "can", leading to semantic errors in constraints.Additionally, SatLM confuses "has a sense of self" with "has a sense of the minds of others" and only utilizes "possesses" to represent them together.In contrast to SatLM, LoT successfully extracts logical proposition descriptions and symbolizes them.Here, we have a very interesting finding: when directly examining the extracted logical expressions, a small mistake in A → B results in an incorrect A → C (i.e., we cannot infer general "animal" from specific "great ape").However, when translating the deduced logical expressions A → C into natural language, LLMs recognize the subordinate relationship between "ape" and "animal" and correct this error, resulting in correct augmentation to prompts and right answers.This reflects that LoT fully leverages the LLM's understanding of natural language descriptions, enabling it to correct errors from earlier phases in the three-phase process.This avoids the pitfalls of neuro-symbolic methods, which rely entirely on the accuracy of logical symbol extraction, where errors in intermediate results directly propagate to errors in the final outcome.In this experiment, we assess the enhancing effect of LoT on ToT, which is a prompting method characterized by its complex reasoning topology.As shown in Figure 5, we can observe that under the complex reasoning scenario with a deduction depth of 5 in the ProofWriter dataset, Direct only achieves an accuracy rate of 51%, which is nearly the same as random guessing (50%).The accuracy rate of ToT is +19% higher than the Direct prompting, reaching 70%, which shows that ToT can assist LLMs in better solving multi-step reasoning.The accuracy rate of LoT+ToT reaches 78%, an +8% increase in accuracy compared to ToT, indicating that LoT can effectively enhance the ability of ToT in complex logical reasoning.</p>
<p>To further investigate the influence of LoT on ToT, we have carefully analyze a range of indices within ToT, including the total count of states, full reasoning (wherein four successful states explorations are achieved), and the tally of successful states.Firstly, as shown in Table 2, we observe a notable +2.14% increase in the overall states of LoT+ToT compared to ToT.This suggests that LoT facilitates an expanded exploration scope for ToT.Moreover, a higher level of full reasoning indicates a more comprehensively explored space and it is evident that LoT has augmented ToT's full reasoning by +2%.Furthermore, compared to ToT, LoT+ToT exhibits a +2.59% increase in the average number of successful states, indicating that the LoT can significantly enhance the effectiveness of ToT's exploratory states.In the Appendix B, we present a thorough analysis of an example, comparing the exploration of states when using ToT, and LoT+ToT respectively.</p>
<p>5 Related Work</p>
<p>Prompting Approaches for LLMs Reasoning</p>
<p>Numerous studies are dedicated to exploring enhancements in the logical reasoning capabilities of LLMs.CoT prompting [28], which breaks down a multi-step reasoning problem into multiple intermediate steps to gradually generate answers, has significantly improved logical reasoning, mathematical logic, and interpretability.Zero-shot CoT [11] integrates zero-shot learning with CoT, controlling LLMs to generate thought chains for answering questions through prompting.CoT-SC [27] generates multiple thought chains, and the final answer is obtained through majority voting, which significantly enhances the performance of the thought chain method.Least-To-Most [35] prompting deconstructs a problem into multiple sub-questions, addressing them step by step, with the answer to the previous sub-question serving as the input for the next.Similar decomposition methods of sub-problems include Lambada [10] and the Divide-and-Conquer [34].[13] employs a process-supervised method, providing feedback on the intermediate reasoning process to enhance logical reasoning capabilities.[21,33,36] select optimal candidates from multiple chains of thought.</p>
<p>ToT [29] and GoT [5] achieve logical branching and the aggregation of multiple thoughts by utilizing more complex reasoning topologies.</p>
<p>Neuro-symbolic Approaches for LLMs Reasoning</p>
<p>The neuro-symbolic methods, which combine LLMs with symbolic reasoning, are considered an effective approach to address the issue of unfaithful reasoning and enhance the logical reasoning ability of LLMs.LReasoner [26] proposes a framework for context extension that expands the logical information contained in the context by applying logical reasoning laws.Additionally, it utilizes data augmentation algorithms to better capture logical information.LogicAsker [25] proposes an enhancement of LLMs' logical reasoning capabilities, which are based on a set of propositions and collections of predicate logic.Logic-LM [20] initially utilizes LLMs to transform natural language problems into symbolic formulas.Subsequently, a symbolic solver is determined to reason about the formalized problems.Moreover, a self-refinement module is introduced, which utilizes error messages from the symbolic solver to modify the symbolic formalization.The SatLM [30] utilizes LLMs to generate declarative task specifications rather than imperative programs, and leverages readily available automated theorem solver to derive the final answers.In LINC [18], LLMs acts as a semantic parser, translating premises and conclusions from natural language into first-order logic expressions.These expressions are then offloaded to an external theorem solver for deductive reasoning.</p>
<p>Limitations</p>
<p>Although our proposed LoT has achieved excellent performance in various logical reasoning tasks, there are still some limitations in LoT.Firstly, current LoT supports a limited set of connectives and logical reasoning laws.More connectives and logical reasoning laws in LoT means more complex prompt design in the Logic Extraction and Logic Translation phase, and increased difficulty in logical deducing in the Logic Extension phase.In the future, we will try to include additional connectives and logical reasoning laws in LoT to further enhance the logical reasoning capabilities.</p>
<p>Additionally, LoT employs LLMs to extract logical symbols and expressions but illusion issues inherent in LLMs can lead to problems such as repetition of expressions, omission of logical relationships, and deviations in logical propositions and expressions.(In Appendix C, we conduct a comprehensive analysis of an example illustrating a failure in Logic Extraction using LoT, resulting from deviations in extracting logical expressions.)</p>
<p>Conclusion</p>
<p>In this paper, we introduce a symbolic-enhancement prompting approach, named Logic-of-Thought (LoT), designed to address the challenge of information loss inherent in existing neuro-symbolic methods.LoT leverages propositional logic to derive expanded logical information from input context, serving as a supplementary augmentation to the original prompts, to enhance logical reasoning capabilities of LLMs.Notably, LoT exhibits compatibility with widely used prompting techniques, including Chain-of-Thought (CoT), Self-Consistency (SC), Chain-of-Thought with Self-Consistency (CoT-SC), Tree-of-Thoughts (ToT), and can be seamlessly integrated with them.In the experiments, we demonstrate that LoT prompting significantly boosts the performance of various existing prompting methods across multiple logical reasoning datasets.In the future, we will explore more logical relationships and logical reasoning laws and support additional prompting methods to further enhance LoT's logical reasoning capabilities.</p>
<p>A More Details of Main Results</p>
<p>In Table 1, we present the experimental results of integrating LoT with various prompting methods.Table 3 and 4 further provide detailed results including standard deviations using GPT-3.5-turboand GPT-4, respectively.The results demonstrate the effectiveness of our LoT.</p>
<p>Method</p>
<p>B Comparative Study of States in ToT and LoT+ToT</p>
<p>We present a comprehensive analysis of an illustrative example, comparing the exploration of states when utilizing ToT and LoT+ToT.In Figure 6, we can observe that in LoT+ToT, LoT generates the logical description "If things are rough, then things are round", from which ToT further generates 4 successful states.The corresponding premises are: (1)"If Charlie is round, then Charlie is young and nice", (2)"Charlie is not young", (3)"If Charlie is quiet and round, then Charlie is young", (4)"If Charlie is round and rough, then Charlie is white".Subsequently, the generated information by the LoT and ToT serves as an enhancement to the input prompt, enabling LLMs to produce correct results.Compared to using ToT alone, the logical description generated by LoT enables ToT to generate an additional four successful states, which leads to the correct results.This indicates that LoT enhances the total number of states as well as the number of successful states, thereby expanding the reasoning space and improving the accuracy of ToT reasoning.</p>
<p>Mistake in Logic Extraction</p>
<p>D An Example of Overlap Capabilities between CoT and LoT</p>
<p>The following example illustrates the overlap in capabilities between CoT and LoT.# Whether this inference is correct: Harry is smarter than before .</p>
<p>----------------------------------------------------------------------# LoT: ## Logic Extraction: 2. When a person reads a book , that person gains knowledge .## LLM Answer: Correct ----------------------------------------------------------------------# CoT: Let ' s think step by step : Given that Harry read the book " Walden " by Henry Thoreau , it can be concluded that he gained knowledge from reading the book .Therefore , based on the context provided , it is reasonable to conclude that Harry is smarter than before .</p>
<h2>LLM Answer: Correct</h2>
<p>Listing 1: An Example of Overlap Capabilities Between CoT and LoT.</p>
<p>E Full Set of Prompts E.1 Logic Extraction Prompt in LoT</p>
<h1>Logic Extraction Prompt for ReClor and LogiQA: Please use uppercase English letters such as A , B , C , etc .to identify all possible propositions .Do not include negative tones such as " not " in the propositions .For example , if the sentence is " It is not bored ," you should use " A : bored " to represent it .</h1>
<p>Next , for each proposition , use the symbol to represent its negative form .For example , the negative form of proposition A can be expressed as A .</p>
<p>Now , please carefully analyze the context and find causal relationship between propositions seriously .A causal expression is only established when the context directly supports this relationship .Use arrows (→) to indicate causal relationships , for example , " If A , then B " , " B if A " and " A causes B " etc .can be represented as A→B .</p>
<p>Finally , output propositions and causal expressions .</p>
<h1>Logic Extraction Prompt for RuleTaker, ProofWriter and FOLIO: Please use uppercase English letters such as A , B , C , etc .to identify all possible propositions .Do not include negative tones such as " not " in the propositions .For example , if the sentence is " It is not bored ," you should use " A : bored " to represent it .</h1>
<p>Next , for each proposition , use the symbol to represent its negative form .For example , the negative form of proposition A can be expressed as ¬A .Now , please carefully analyze the context and find causal relationship between propositions .A causal expression is only established when the context directly supports this relationship .Use arrows (→) to indicate causal relationships , for example , " If A , then B " , " B if A " and " A causes B " etc .can be represented as A→B .</p>
<p>Finally , output propositions and causal expressions .</p>
<p>E.2 Logic Translation Prompt in LoT</p>
<h1>Logical Translation Prompt for All: Please use the provided propositions to translate each expression into a complete sentence .</h1>
<p>¬A represents the negation of proposition A , the arrow (→) represents the causal relationship , and A→B represents if A , then B .</p>
<p>Only output the sentences in a paragraph !</p>
<h1></h1>
<p>Declare variables: animals = EnumSort([great_ape, animal]) abilities = Function([animals] → [bool]) possesses = Function([animals] → [bool]) # Constraints: 1. ForAll([a:animals], Implies(abilities(a), possesses(a))) 2. ForAll([a:animals], Implies(possesses(a), abilities(a))) 3. ForAll([a:animals], Implies(possesses(a), abilities(a))) # Which one of the following follows logically from the information above?(A)is_valid(ForAll([a:animals], Implies(abilities(a), possesses(a)))) (B)is_valid(ForAll([a:animals], Implies(Not(abilities(a)), Not(a == great_ape)))) (C)is_valid(ForAll([a:animals], Implies(Not(a == great_ape), Not(abilities(a))))) (D)is_valid(ForAll([a:animals], Implies(Not(possesses(a)), Not(abilities(a)))))Information mistake: Mistakenly using "abilities" to represent "can" from the first sentence, and causing the semantic errors of constraints.Confusing 'has a sense of self' and 'has a sense of the minds of others' together.Only use "possesses" to represent.</p>
<p>Figure 3 :
3
Figure 3: A comparative case of SatLM and LoT from the ReClor dataset.</p>
<p>Figure 4 :
4
Figure 4: Comparison between SatLM and LoT in the ReClor dataset.</p>
<ol>
<li>6 Figure 5 :
65
Figure 5: Comparison between ToT and LoT+ToT in the ProofWriter dataset.</li>
</ol>
<p>Figure 7 :
7
Figure 7: Errors in Logic Extraction using LoT.</p>
<p>3 .
3
If a person gains knowledge , they become smarter .----A: a person reads a book , B : person gains knowledge , C : become smarter A → B, B → C ## Logic Extension: A → C ## Logic Translation: If a person reads a book , that person become smarter .## Extended context: 1. Books contain tons of knowledge .2. When a person reads a book , that person gains knowledge .3. If a person gains knowledge , they become smarter .4. Harry read the book " Walden " by Henry Thoreau . 5. If a person reads a book , that person become smarter .</p>
<p>Figure 2presents an overview of LoT, which consists of three phases.Firstly, in the Logic Extraction phase, propositions and logical relations are extracted from the input context using LLMs to output logical expressions.Secondly, in the Logic Extension phase, the logical expressions are expanded through Python-implemented logical rules.Thirdly, in the Logic Translation If you have no keyboarding skills at all, you will not be able to use a computer.And if you are not able to use a computer, you will not be able to write your essays using a word processing program.Question:If the statements above are true, which one of the following must also be true on the basis of them?
Context:Logical Propositions: A: have keyboarding skills;B: be able to use a computer;C: be able to write your essays using a wordprocessing programOptions:A. If you are not able to write your essays using a wordprocessing program, you have no keyboarding skills.B. If you are able to write your essays using a word processingprogram, you have at least some keyboarding skills.C. If you are not able to write your essays using a wordprocessing program, you are not able to use a computer.D. If you have some keyboarding skills, you will be able towrite your essays using a word processing program.3 Methodology If you have no keyboarding skills at all, you will not be able to use a computer. And if you are not able to use a computer, you will not be able to write your essays using a word processing program. Question: ...... Options: Overview. Context: ......</p>
<p>Extended Context:</p>
<p>If you have no keyboarding skills at all, you will not be able to use a computer.And if you are not able to use a computer, you will not be able to write your essays using a word processing program.If you are able to write your essays using a word processing program, then you have keyboarding skills.</p>
<p>symbol A symbol B symbol C Logical Expressions: ¬A → ¬B , ¬B →¬C Input Logic Extraction Logic Extension Logic Translation Output Question:</p>
<p>If the statements above are true, which one of the following must also be true on the basis of them?Options: A. If you are not able to write your essays using a word processing program, you have no keyboarding skills.B. If you are able to write your essays using a word processing program, then you have at least some keyboarding skills.√ C. If you are not able to write your essays using a word processing program, you are not able to use a computer.D. If you have some keyboarding skills, you will be able to write your essays using a word processing program.</p>
<p>Table 1 :
1
↑9.82 35.30↑0.70 61.40 ↑2.60 59.50 ↑5.00 75.00 ↑0.80 77.98 ↑5.81 57.60 ↑2.80 65.30 ↑2.70 66.70 ↑2.00 86.00 ↑3.00 Main results of combining LoT with various prompting methods.The number in green indicates an enhancement in performance, while the number in red signifies a decline in performance.For comprehensive details with standard deviation, refer to the Appendix A.
MethodGPT-3.5-turboGPT-4ReClorLogiQARuleTaker ProofWriter FOLIOReClorLogiQARuleTaker ProofWriter FOLIODirect46.2034.6058.8054.5074.2072.1754.8062.6064.7083.00LoT 56.02 CoT 52.1734.0060.7058.8078.0077.3955.4062.4075.9084.40LoT + CoT56.52 ↑4.35 36.50 ↑2.50 61.60 ↑0.90 61.50 ↑2.7078.00 ↑0.00 79.13 ↑1.74 57.50 ↑2.10 65.60 ↑3.20 76.80 ↑0.9084.80 ↑0.40SC(5)56.5236.6059.0057.5076.0073.9155.5065.5067.5085.00LoT + SC(5)58.70 ↑2.18 38.00 ↑1.40 60.00 ↑1.00 60.00 ↑2.5078.60 ↑2.60 80.43 ↑6.52 58.50 ↑3.00 64.50 ↓1.00 66.00 ↓1.5088.00 ↑3.00CoT-SC(5)58.7034.5065.5061.5080.0080.4356.5063.5080.5084.00LoT + CoT-SC(5) 60.87 ↑2.17 39.50 ↑5.00 65.50 ↑0.00 67.50 ↑6.0083.00 ↑3.00 82.61 ↑2.18 61.00 ↑4.50 66.50 ↑3.00 80.50 ↑0.0086.00 ↑2.00</p>
<p>Table 2 :
2
Comparison of reasoning states between LoT and LoT+ToT.
MethodTotal StatesSuccessful States Full Reasoning(%)ToT18.707.7090LoT+ToT 19.10 ↑2.13%8.09 ↑5.06%92 ↑2%
, SatLM induces information mistakes and loss.Specifically, during</p>
<p>Table 3 :
3
Main results of combining LoT with various prompting methods on GPT-3.5-turbo.The standard deviation is presented and the best results are bold.
GPT-3.5-turboReClorLogiQARuleTaker ProofWriterFOLIODirect46.20±2.68 34.60±1.83 58.80±2.58 54.50±2.35 74.20±2.14CoT52.17±4.56 34.00±1.50 60.70±2.69 58.80±2.06 78.00±3.85SC(5)56.52±0.00 36.60±0.00 59.00±0.00 57.50±0.00 76.00±0.00CoT-SC(5)58.70±0.00 34.50±0.00 65.50±0.00 61.50±0.00 80.00±0.00LoT56.02±3.20 35.30±1.44 61.40±1.07 59.50±2.12 75.00±2.83LoT + CoT56.52±2.71 36.50±1.92 61.60±1.53 61.50±1.21 78.00±2.15LoT + SC(5)58.70±0.00 38.00±0.00 60.00±0.00 60.00±0.00 78.60±0.00LoT + CoT-SC(5) 60.87±0.00 39.50±0.00 65.50±0.00 67.50±0.00 83.00±0.00MethodGPT-4ReClorLogiQARuleTaker ProofWriterFOLIODirect74.20±2.14 72.17±2.24 54.80±0.93 62.60±1.02 64.70±1.21CoT77.39±2.94 55.40±1.88 62.40±1.43 75.90±1.24 84.40±1.85SC(5)73.91±0.00 55.50±0.00 65.50±0.00 67.50±0.00 85.00±0.00CoT-SC(5)80.43±0.00 56.50±0.00 63.50±0.00 80.50±0.00 84.00±0.00LoT77.98±3.88 57.60±1.22 65.30±0.51 66.70±1.50 86.00±1.26LoT + CoT79.13±3.98 57.50±1.05 65.60±1.24 76.80±2.11 84.80±0.40LoT + SC(5)80.43±0.00 58.50±0.00 64.50±0.00 66.00±0.00 88.00±0.00LoT + CoT-SC(5) 82.61±0.00 61.00±0.00 66.50±0.00 80.50±0.00 86.00±0.00</p>
<p>Table 4 :
4
Main results of combining LoT with various prompting methods on GPT-4.The standard deviation is presented and the best results are bold.</p>
<p>Bob is rough.Charlie is nice.Charlie is rough.Charlie is white.Dave is not white.Gary is cold.Gary is quiet.Rough, white things are quiet.If Charlie is round and Charlie is quiet then Charlie is rough.All young things are round.Nice things are white.Rough, nice things are cold.If something is round and rough then it is nice.If something is rough then it is young.If things are rough, then things are round.Bob is rough.Charlie is nice.Charlie is rough.Charlie is white.Dave is not white.Gary is cold.Gary is quiet.Rough, white things are quiet.If Charlie is round and Charlie is quiet then Charlie is rough.All young things are round.Nice things are white.Rough, nice things are cold.If something is round and rough then it is nice.If something is rough then it is young.Bob is rough.Charlie is nice.Charlie is rough.Charlie is white.Dave is not white.Gary is cold.Gary is quiet.Rough, white things are quiet.If Charlie is round and Charlie is quiet then Charlie is rough.All young things are round.Nice things are white.Rough, nice things are cold.If something is round and rough then it is nice.If something is rough then it is young.Bob is rough.Charlie is nice.Charlie is rough.Charlie is white.Dave is not white.Gary is cold.Gary is quiet.Rough, white things are quiet.If Charlie is round and Charlie is quiet then Charlie is rough.All young things are round.Nice things are white.Rough, nice things are cold.If something is round and rough then it is nice.If something is rough then it is young.If things are rough, then things are round.If Charlie is round, then Charlie is young and nice.Charlie is not young.If Charlie is quiet and round, then Charlie is young.If Charlie is round and rough, then Charlie is white.Case study of state exploration in ToT and LoT+ToT.C An Error Case of LoT Figure 7 illustrates an instance of inaccuracies in extracting logical information during the Logic Extraction process, leading to erroneous logical expressions and errors in the final outcome.When LoT selects sentences with logical relationships, there are biases in the information extracted by LLMs.The sentences "Today is Easter, but Cindy's hair is still braided", "Cindy's hair is braided, which means it must be a special occasion", and "If it's a holiday, Cindy will most likely have her hair braided" all exhibit inaccuracies.Subsequently, extracted logical expressions, such as D → ¬B, B → C, C → D exhibit errors.These accumulated errors result in erroneous generated logical descriptions and incorrect final outcomes.Cindy only braids her hair on Christmas.-This cannot be true because the given context states that Cindy always braids her hair on Christmas, but today is Easter and her hair is still braided.B. Cindy dislikes braiding her hair since it takes too long.-This could be true, as there is no information given about Cindy's feelings towards braiding her hair.C. Cindy only braids her hair on holidays.-This cannot be true because the given context states that Cindy always braids her hair on Christmas, which is a holiday, but today is Easter and her hair is still braided.D. Cindy never braids her hair during July or August.-This could be true, as there is no information given about Cindy braiding her hair during July or August.Therefore, the correct answer is C. Cindy only braids her hair on holidays.
Failed state Successful state ToT LLMs Answer: False Correct answer: True ToT inference ending ToT Figure 6: Extended expressions: Extended context: Conclusion: Charlie is quiet. New premises of successful states: 1. If Charlie is round, then Charlie is young and nice. 2. Charlie is not young. 3. If Charlie is quiet and round, then Charlie is young. 4. If Charlie is round and rough, then Charlie is white. ToT 1 2 3 4 Extended context: Conclusion: Charlie is quiet. LLMs ToT inference ending Answer: True Correct answer: True LoT+ToT A→C, A→D, A→¬B, A→¬D, B→D, C→¬B 1. If it's Christmas, Cindy will always braid her hair. 2. Today is Easter, but Cindy's hair is still braided. 3. Cindy's hair is braided, which means it must be a special occasion. 4. If it's a holiday, Cindy will most likely have her hair braided. A. Sentences containing conditional reasoning relationships: Context: Conclusion: Charlie is quiet. Context: Conclusion: Charlie is quiet. Answer: 5. Even though it's not Christmas, Cindy's hair is still braided.</p>
<p>In the example, LoT first extracts propositions A, B, and C from the context and identifies the relationships A → B and B → C.Then, it extends to a new expression A → C.This new expression is translated into additional logical information "If a person reads a book, that person becomes smarter".This logical information directly links "Harry read the book" and "become smarter" in the context, helping LLMs correctly infer the answer.CoT's reasoning process involves first deriving proposition B from proposition A based on the second sentence, then deriving proposition C from proposition B based on the third sentence, ultimately arriving at the answer.We can see that both CoT and LoT handle this problem by linking conditional statements and reasoning step by step, indicating that CoT and LoT sometimes have overlapping capabilities.Books contain tons of knowledge .2. When a person reads a book , that person gains knowledge .3. If a person gains knowledge , they become smarter .4. Harry read the book " Walden " by Henry Thoreau .</p>
<h1>Context:1.</h1>
<p>E.3 SatLM Prompt# SatLM Prompt for LSAT: Nine different treatments are available for a certain illness : three antibiotics (F , G , and H ) three dietary regimens (M , N , and O ) and three physical therapies (U , V , and W ) .For each case of the illness , a doctor will prescribe exactly five of the treatments , in accordance with the following conditions : If two of the antibiotics are prescribed , the remaining antibiotic cannot be prescribed .There must be exactly one dietary regimen prescribed .If O is not prescribed , F cannot be prescribed .If W is prescribed , F cannot be prescribed .G cannot be prescribed if both N and U are prescribed .V cannot be prescribed unless both H and M are prescribed .Question : If O is prescribed for a given case , which one of the following is a pair of treatments both of which must also be prescribed for that case ?Implies ( prescribed ( W ) , Not ( prescribed ( F ) ) ) Implies ( And ( prescribed ( N ) , prescribed ( U ) ) , Not ( prescribed ( G ) ) ) Implies ( prescribed ( V ) , And ( prescribed ( H ) , prescribed ( M ) ) ) solve ( Implies ( prescribed ( O ) , And ( prescribed ( U ) , prescribed ( V ) ) ) ) # ( A ) solve ( Implies ( prescribed ( O ) , And ( prescribed ( G ) , prescribed ( V ) ) ) ) # ( B ) solve ( Implies ( prescribed ( O ) , And ( prescribed ( N ) , prescribed ( U ) ) ) ) # ( C ) solve ( Implies ( prescribed ( O ) , And ( prescribed ( U ) , prescribed ( V ) ) ) ) # ( D ) solve ( Implies ( prescribed ( O ) , And ( prescribed ( U ) , prescribed ( W ) ) ) ) # ( E )E.4 ToT Prompt# ToT Prompt used for Final Conclusion: {{# system }} Suppose you are one of the greatest AI scientists , logicians and mathematicians .Let us think step by step .Read and analyze the " Premises " first , then judge whether the " Hypothesis " is True , False .Please make sure your reasoning is directly deduced from the " Premises " and " Propositions " other than introducing unsourced common knowledge and unsourced information by common sense reasoning .----{{/ system }} {{~# each examples }} {{# user }} ---" Premises ": "{{ this .premises }}" " Hypothesis ": "{{ this .conclusion }}" {{/ user }} {{# assistant }} " Thoughts ": " Let us think step by step .From the premises , we can deduce propositions : {{ this .propositions }}" {{/ assistant }} {{# assistant }} " Reasoning ": " Let us think step by step , {{ this .reasoning }}" {{/ assistant }} {{# assistant }} " Recall the Hypothesis ": "{{ this .conclusion }}" {{/ assistant }} {{# assistant }} " Judgement ": " Now we know that the Hypothesis is {{ this .judgement }}{{/ assistant }} {{~/ each }} {{# user }} ---" Premises ": "{{ premises }}" " Hypothesis ": "{{ conclusion }}" {{/ user }} {{# assistant }} " Thoughts ": " Let us think step by step .From the premises , we can deduce propositions : {{ propositions }}" {{/ assistant }} {{# assistant }} " Recall the Hypothesis ": "{{ conclusion }}" {{/ assistant }} {{# assistant }} " Reasoning ": " Let us think step by step , {{/ assistant }} {{# assistant }} {{ gen " reasoning " temperature =0.7 max_tokens =300 stop =[ ' textbackslash n ']}}{{/ assistant }} {{# assistant }} " Recall the Hypothesis ": "{{ conclusion }}" {{/ assistant }} {{# assistant }} " Judgement ": " Now we know that the Hypothesis is {{/ assistant }} {{# assistant }} {{ gen " judgement " temperature = temperature max_tokens =1 stop = ' textbackslash n '}} {{/ assistant }} # ToT Prompt used for Generate Proposition: {{# system }} Suppose you are one of the greatest AI scientists , logicians and mathematicians .Let us think step by step .Please use Logical Reasoning Rules ( LRR ) to deduce a " Proposition " from two given " Premises " and the proposition does not include " if ". Logical Reasoning Rules ( LRR ) : 1. " Two premises ": " If A , then B .A is true ."then " Proposition ": " B is true ." 2. " Two premises ": " If A , then B .B is not true ."then " Proposition ": " A is not true " 3. " Two premises ": " A is either C or D .A is not C ." then " Proposition ": " A is D ."Please make sure that the " Proposition " is logically correct .Please make sure that the " Proposition " is not a duplicate of the " Premises ".Please make sure your reasoning is directly deduced from the " Premises " and " Propositions " other than introducing unsourced common knowledge and unsourced information by common sense reasoning .Please remember that your " Proposition " should be useful to determine whether the " Hypothesis " is True , False .----{{# system }} {{~# each examples }} {{# user }} ---'' Premises ": "{{ this .premises }}" We want to deduce more propositions to determine the correctness of the following " Hypothesis ": '' Hypothesis ": "{{ this .conclusion }}" Can you deduce a new " Proposition " from at least two given " Premises "? {{# user }} {{# assistant }} " Proposition ": "{{ this .propositions }}" {{/ assistant }} {{~/ each }} {{# user }} ---Premises ": "{{ this .premises }}" We want to deduce more propositions to determine the correctness of the following " Hypothesis ": '' Hypothesis ": "{{ this .conclusion }}" Can you deduce a new " Proposition " from at least two given " Premises "? {{# user }} {{# assistant }} " Proposition ": " {{/ assistant }} {{# assistant }} {{ gen " proposition " temperature = temperature max_tokens =50 stop = ' textbackslash \n '}} {{/ assistant }} # ToT Prompt used for Validate Deduction: {{# system }} Suppose you are one of the greatest AI scientists , logicians and mathematicians .Let us think step by step .Please use the Logical Reasoning Rules ( LRR ) to determine whether the deduction of the given " Premises " to a " Proposition " is valid or not , reply with True or False .Logical Reasoning Rules ( LRR ) : 1. " Two premises ": " If A , then B .A is true ."then " Proposition ": " B is true ." 2. " Two premises ": " If A , then B .If B , then C ." then " Proposition ": " If A , then C ." 3. " Two premises ": " If A , then B .B is not true ."then " Proposition ": " A is not true " 4. " Two premises ": " A is either C or D .A is not C ." then " Proposition ": " A is D ."----{{/ system }} {{~# each examples }} {{# user }} ---" Premises ": "{{ this .premises }}" " Proposition ": "{{ this .propositions }}" {{/ user }} {{# assistant }} " Judgement ": " Is this deduction valid ?{{ this .validation }}" {{/ assistant }} {{~/ each }} {{# user }} ---" Premises ": "{{ premises }}" " Proposition ": "{{ propositions }}" {{/ user }} {{# assistant }} " Judgement ": " Is this deduction valid ?{{/ assistant }} {{# assistant }} {{ gen " validation " temperature = temperature max_tokens =1 stop = ' textbackslash n '}} {{/ assistant }} # ToT Prompt used for sourced deduction: {{# system }} Suppose you are one of the greatest AI scientists , logicians and mathematicians .Let us think step by step .Please determine whether the " Proposition " is directly deduced from the " Premises " with certainty other than introducing unsourced information by common sense reasoning , reply with True or False .----{{/ system }} {{~# each examples }} {{# user }} ---" Premises ": "{{ this .premises }}" " Proposition ": "{{ this .propositions }}" {{/ user }} {{# assistant }} " Judgement ": " Is this proposition directly deduced from the premises ?{{ this .sourced }}" {{/ assistant }} {{~/ each }} {{# user }} ---" Premises ": "{{ premises }}" " Proposition ": "{{ propositions }}" {{/ user }} {{# assistant }} " Judgement ": " Is this proposition directly deduced from the premises ?{{/ assistant }} {{# assistant }} {{ gen " sourced " temperature = temperature max_tokens =1 stop = ' textbackslash n '}}{{/ assistant }}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>Konstantine Arkoudas, arXiv:2308.03762Gpt-4 can't reason. 2023arXiv preprint</p>
<p>Llms with chain-of-thought are non-causal reasoners. Guangsheng Bao, Hongbo Zhang, Linyi Yang, Cunxiang Wang, Yue Zhang, arXiv:2402.160482024arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwaśniewski, Jürgen Müller, Lukas Gianinazzi, arXiv:2401.14295Topologies of reasoning: Demystifying chains, trees, and graphs of thoughts. 2024arXiv preprint</p>
<p>Propositional logic: deduction and algorithms. Hans Kleine, Büning , Theodor Lettmann, 1999Cambridge University Press48</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, Deepak Ramachandran, arXiv:2212.13894Lambada: Backward chaining for automated reasoning in natural language. 2022arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Measuring faithfulness in chain-of-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, arXiv:2307.137022023arXiv preprint</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023arXiv preprint</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023arXiv preprint</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, arXiv:2007.081242020arXiv preprint</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, arXiv:2301.13379Faithful chain-of-thought reasoning. 2023arXiv preprint</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.001142021arXiv preprint</p>
<p>Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Alex Theo X Olausson, Benjamin Gu, Cedegao E Lipkin, Armando Zhang, Joshua B Solar-Lezama, Roger Tenenbaum, Levy, arXiv:2310.151642023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Automatic prompt augmentation and selection with chain-of-thought from labeled data. Kashun Shum, Shizhe Diao, Tong Zhang, arXiv:2302.128222023arXiv preprint</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel Bowman, Advances in Neural Information Processing Systems. 202436</p>
<p>Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-Tse Huang, Pinjia He, Wenxiang Jiao, Michael R Lyu, arXiv:2401.00757A &amp; b== b &amp; a: Triggering logical reasoning failures in large language models. 2024arXiv preprint</p>
<p>Logic-driven context extension and data augmentation for logical reasoning of text. Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, Nan Duan, Findings of the Association for Computational Linguistics: ACL 2022. 2022</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Satlm: Satisfiability-aided language models using declarative prompting. Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett, Advances in Neural Information Processing Systems. 202436</p>
<p>Yang Zhang Yifan, Yuan Jingqin, Yao Yang, Chi-Chih Andrew, arXiv:2308.04371Cumulative reasoning with large language models. 2024arXiv preprint</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, arXiv:2002.043262020arXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>Guiding large language models with divide-and-conquer program for discerning problem solving. Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, Yan Liu, arXiv:2402.053592024arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, arXiv:2205.106252022arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9554 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9554</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9554</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-277104139</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.14229v3.pdf" target="_blank">HA-VLN 2.0: An Open Benchmark and Leaderboard for Human-Aware Navigation in Discrete and Continuous Environments with Dynamic Multi-Human Interactions</a></p>
                <p><strong>Paper Abstract:</strong> Vision-and-Language Navigation (VLN) has been studied mainly in either discrete or continuous settings, with little attention to dynamic, crowded environments. We present HA-VLN 2.0, a unified benchmark introducing explicit social-awareness constraints. Our contributions are: (i) a standardized task and metrics capturing both goal accuracy and personal-space adherence; (ii) HAPS 2.0 dataset and simulators modeling multi-human interactions, outdoor contexts, and finer language-motion alignment; (iii) benchmarks on 16,844 socially grounded instructions, revealing sharp performance drops of leading agents under human dynamics and partial observability; and (iv) real-world robot experiments validating sim-to-real transfer, with an open leaderboard enabling transparent comparison. Results show that explicit social modeling improves navigation robustness and reduces collisions, underscoring the necessity of human-centric approaches. By releasing datasets, simulators, baselines, and protocols, HA-VLN 2.0 provides a strong foundation for safe, socially responsible navigation research.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9554",
    "paper_id": "paper-277104139",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0069575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HA-VLN 2.0: AN OPEN BENCHMARK AND LEADER-BOARD FOR HUMAN-AWARE NAVIGATION IN DIS-CRETE AND CONTINUOUS ENVIRONMENTS WITH DY-NAMIC MULTI-HUMAN INTERACTIONS
9 Oct 2025</p>
<p>Yifei Dong 
University of Washington</p>
<p>Fengyi Wu 
University of Washington</p>
<p>Qi He 
University of Washington</p>
<p>Zhi-Qi Cheng 
University of Washington</p>
<p>Heng Li 
University of Washington</p>
<p>Minghan Li 
University of Washington</p>
<p>Zebang Cheng 
University of Washington</p>
<p>Yuxuan Zhou 
University of Washington</p>
<p>Jingdong Sun 
University of Washington</p>
<p>Qi Dai 
University of Washington</p>
<p>Alexander G Hauptmann 
University of Washington</p>
<p>Carnegie Mellon University 
University of Washington</p>
<p>Microsoft Research 
University of Washington</p>
<p>HA-VLN 2.0: AN OPEN BENCHMARK AND LEADER-BOARD FOR HUMAN-AWARE NAVIGATION IN DIS-CRETE AND CONTINUOUS ENVIRONMENTS WITH DY-NAMIC MULTI-HUMAN INTERACTIONS
9 Oct 202555485CE0BCB0FD9AFA571489E6906E2BarXiv:2503.14229v3[cs.AI]
Vision-and-Language Navigation (VLN) has been studied mainly in either discrete or continuous settings, with little attention to dynamic, crowded environments.We present HA-VLN 2.0, a unified benchmark introducing explicit socialawareness constraints.Our contributions are: (i) a standardized task and metrics capturing both goal accuracy and personal-space adherence; (ii) HAPS 2.0 dataset and simulators modeling multi-human interactions, outdoor contexts, and finer language-motion alignment; (iii) benchmarks on 16,844 socially grounded instructions, revealing sharp performance drops of leading agents under human dynamics and partial observability; and (iv) real-world robot experiments validating sim-to-real transfer, with an open leaderboard enabling transparent comparison.Results show that explicit social modeling improves navigation robustness and reduces collisions, underscoring the necessity of human-centric approaches.By releasing datasets, simulators, baselines, and protocols, HA-VLN 2.0 provides a strong foundation for safe, socially responsible navigation research.</p>
<p>INTRODUCTION</p>
<p>Vision-and-Language Navigation (VLN) Anderson et al. (2018); Zhang et al. (2024b) challenges embodied agents to interpret natural-language instructions and reach specified goals in photorealistic simulators or real-world environments Gu et al. (2022); Wang et al. (2022).Although recent advances have delivered strong performance in controlled benchmarks, existing methods are typically confined to either discrete (DE) or continuous (CE) settings, neglecting the complexities of crowded, human-populated spaces, where agents must contend with unpredictable human behaviors, reason under partial observability, and ensure socially compliant navigation Anderson et al. (2021); Kadian et al. (2020); Yu et al. (2024).Bridging these gaps is essential for moving VLN from simulation prototypes toward robust real-world deployment Wu et al. (2024); Gao et al. (2024).</p>
<p>Motivation and Open Challenges.Despite recent progress, VLN research still faces three fundamental limitations that restrict its real-world applicability.First, social awareness remains underexplored: human participants in the scene are commonly overlooked or reduced to inert obstacles, preventing the agent from respecting personal space or reacting to bystanders' activities (see Figure 1).Second, finer-grained instructions are not well captured in existing corpora Paduraru et al. (2021); Kong et al. (2024).Commands such as "Turn to your left, and go past the chair" rarely reflect real-world contexts like "Turn to your left, where you will see someone taking a brief pause ... on the chair" in Figure 1.Third, static-environment assumptions neglects real-time re-planning when people traverse corridors or gather spontaneously.In practice, social navigation demands partial observability and dynamic route adjustment.Addressing these issues requires a benchmark that unifies Preprint.</p>
<p>Human Path</p>
<p>(with certain track)</p>
<p>Scan id: 5LpN3gDmAk7</p>
<p>Start by ascending the stairs, where you will notice an individual pacing forward while engaged in a phone conversation ① , be careful not to bump into him.Continue your movement upward until you reach the top.Next to the doorway, you will see another person on the phone ② .Proceed straight through the doorway, then move forward into the bedroom.Turn to your left, where you will see someone taking a brief pause to have a snack on the chair ③</p>
<p>. Walk past the chair, taking care not to disturb him, and make your way out through the doorway leading to the rooftop.Once on the rooftop, you will observe a couple having a quiet, Figure 1: HA-VLN 2.0 Navigation Scenario.HA-VLN 2.0 adds four key challenges: (i) unified discrete/continuous navigation with denser crowds, richer activities, and mixed indoor-outdoor scenes; (ii) stricter social-distance and collision constraints under partial observability; (iii) instructions explicitly grounded in human activities and spatial cues, improving language-vision alignment; and (iv) robust real-time planning amid occlusion and multi-human dynamics.Example: key positions (e.g., ➀, ➁) align with instructional cues referring to specific human behaviors.When the agent encounters a bystander on the phone (➁, Decision A), it intelligently turns right to avert a potential collision.On the right, RGB and Depth observations illustrate the agent's panoramic view preceding decisions A, B, and C, capturing its dynamic responses to nearby humans.</p>
<p>DE and CE with explicit regime disclosure, supports socially grounded finer-grained instructions, and incorporates human-centric metrics for navigation in dynamic multi-human environments.</p>
<p>Toward Human-Aware VLN.Early progress, notably HA-VLN 1.0 framework Li et al. (2024) introduced dynamic humans into VLN, yet several shortcomings limited its realism and reproducibility (Appendix Table A1).Motion data in HAPS 1.0 Li et al. (2024) suffered from alignment errors and limited diversity, restricting coverage of everyday activities.The benchmark also exhibited a discrete-navigation bias, with its simulator largely confined to viewpoint hops Krantz et al. (2020) rather than physics-consistent low-level control Krantz et al. (2021).Multi-human interactions were underdeveloped, typically modeling only a single individual in simplified scenarios.Finally, instruction generation remained coarse and object-centric, omitting temporally varying activities and offering little control over granularity.These limitations call for a benchmark that standardizes regime disclosure, expands motion fidelity and diversity, incorporates multi-human interactions, and supports finer-grained socially grounded instructions across both discrete and continuous settings.</p>
<p>Our Contributions.In response, we introduce HA-VLN 2.0, a unified benchmark coupling discrete (DE) and continuous (CE) navigation paradigms with explicit social-awareness constraints.It comprises the HAPS 2.0 dataset, featuring 486 SMPL-based motion sequences across 26 region types, rigorously annotated via multi-view verification (around 430 annotation hours).HA-VLN 2.0 includes established simulators (HA-VLN-DE, HA-VLN-CE) with multi-human interactions, outdoor environments, real-time rendering, and precise collision management for up to 910 human models across 428 regions in 90 scans.A unified API enables seamless comparisons across modes (Fig. 2; Sec. 3).Additionally, we expand R2R-CE Krantz et al. (2020) with 16,844 socially grounded instructions and introduce two robust baseline agents, HA-VLN-VL with Transformerbased grounding and HA-VLN-CMA with cross-modal attention for replanning, both validated under comprehensive human-centric metrics (Sec.4).Finally, we demonstrate successful sim-to-real transfer in real-world robot validation and provide a public evaluation leaderboard (Sec.5.2).Specifically, HA-VLN 2.0 offers four key advancements:</p>
<ol>
<li>
<p>Cross-paradigm task standardization &amp; Metrics.We unify DE and CE navigation under social-awareness constraints, ensuring consistent goals and human-centric evaluations (Sec.2).</p>
</li>
<li>
<p>HAPS 2.0 &amp; Dual simulators (large-scale build).We release HAPS 2.0 (486 SMPL sequences) and two established simulators (HA-VLN-DE, HA-VLN-CE) that incorporate multi-view human Preprint.annotation (∼ 430 human-hours), outdoor scenes, dual-thread rendering, and rigorous collision checks for up to 910 active individuals with interactions (Fig. 2; Sec. 3).</p>
</li>
</ol>
<p>3.</p>
<p>Comprehensive benchmarking with finer-grained instructions.We augment R2R-CE with 16,844 socially-grounded instructions and benchmark multiple agents under unified metrics, unveiling challenges arising from multi-human dynamics and partial observability.(Sec.4).</p>
<ol>
<li>Real-robot validation and public leaderboard.We robustly demonstrate sim-to-real transfer using a physical robot successfully navigating crowded indoor areas, and provide a public leaderboard for comprehensive discrete-continuous evaluations in multi-human scenarios (Sec.5.2).</li>
</ol>
<p>THE UNIFIED HUMAN-AWARE VLN TASK</p>
<p>Motivation and Overview.HA-VLN 1.0 Li et al. (2024) introduced dynamic humans into VLN, but its discrete-environment (DE) focus limited ecological validity and hindered systematic study of continuous control and realistic multi-human interactions.To address this, we formalize HA-VLN 2.0, a unified benchmark that integrates DE and CE under explicit human-centric constraints.Under this setting, agents must parse instructions that reference ongoing human activities (e.g., "Go upstairs where someone is pacing on the phone"), anticipate plausible human trajectories, maintain socially compliant distances, and adapt plans online in densely populated, photorealistic 3D scenes (Fig. 1).We next make this specification precise by unifying state and action across regimes.</p>
<p>Unified State and Action Spaces.HA-VLN 2.0 defines a shared state and action interface bridging DE and CE.At each timestep t, the agent state is
s t = ⟨p t , o t , Θ FOV t ⟩,(1)
where p t is the agent's 3D position, o t its orientation, and Θ FOV t its egocentric visual observation.In DE, agents hop among predefined viewpoints with RGB observations; in CE, they perceive RGB+D within a 90 • field of view and execute fine-grained increments (e.g., 0.25 m forward, 15 • rotation).Crucially, DE and CE now share a unified action space, A = {a forward , a left , a right , a up , a down , a stop }, (2) enabling direct and fair comparison across paradigms (Fig. 2).</p>
<p>Human-Aware Enhanced Constraints.HA-VLN 2.0 extends far beyond HA-VLN 1.0's sparse, static settings by introducing unified constraints that substantially increase realism and complexity in both DE and CE: (i) Dynamic Human Models: continuous trajectories from the HAPS 2.0 dataset capturing diverse behaviors and dense crowds; (ii) Personal-Space Enforcement: standardized proximity thresholds (3 m in DE; overlapping radii in CE) to ensure equitable cross-paradigm evaluation; (iii) Human-Focused Instructions: natural-language directives grounded in dynamic human behaviors, requiring precise alignment between text and visual context.All annotations are curated through a validated multi-stage pipeline (Sec.3), ensuring both realism and reproducibility.</p>
<p>Unified Dynamics and Partial Observability.HA-VLN 2.0 formalizes a unified partially observable Markov decision process (POMDP) spanning both DE and CE settings, whereas HA-VLN 1.0 considered partial observability only in DE.Successor states s t+1 depend jointly on agent actions and stochastic human dynamics (e.g., sudden path blockage or unexpected entry).Agents must therefore infer latent human intentions and strategically balance exploration (discovering alternate routes) with exploitation (committing to viable trajectories), reflecting the fundamental trade-offs inherent in navigation through dynamic, human-populated environments.</p>
<p>Key Challenges of DE-CE Synergies.Unifying DE and CE exposes three challenges for socially intelligent navigation: (i) Socially Compliant Navigation: collision-free movement that adapts to evolving personal-space boundaries; (ii) Human-Aligned Instruction Grounding: accurate interpretation of natural-language instructions amid dynamic human activities; (iii) Adaptive Path Replanning: trajectory adjustment in response to human interactions that modify accessibility.DE supports rapid prototyping and large-scale evaluation, while CE offers motion fidelity indispensable for bridging simulation and real-world deployment.Together, these synergies establish HA-VLN 2.0 as the first benchmark uniting efficient simulation with realistic human-populated environments, motivating a unified simulator and corresponding agent framework introduced next.</p>
<p>Agent Environment Interaction</p>
<p>Human "collision"</p>
<p>HA-VLN SIMULATOR</p>
<p>To support the unified HA-VLN task, we build a simulator that embeds dynamically moving humans in both discrete and continuous 3D environments.Unlike Li et al. (2024), which treated humans as static obstacles, our simulator models high fidelity motions, interactions among multiple humans, and socially grounded dynamics such as spontaneous movements, group activities, and personal space constraints.Using the upgraded HAPS 2.0 dataset, it improves motion diversity, spatial alignment, and realism over HAPS 1.0 (Table A2) and provides 486 curated sequences across indoor and outdoor scenes.a minimum distance ϵ = 1m from other objects or leaving the region (details in Appx.B.2).This yields natural placements that reflect realistic social behaviors and spatial relations.</p>
<dl>
<dt>Human Activity Annotation: Fine-Level.Building on coarse placements, fine-level annotation refinement leverages multi-camera observations, ensuring precise alignment of motions with scene geometry.Inspired by 3D skeleton capture systems Ji et al. (2018); Petrovich et al. (2021), we deploy nine RGB cameras around each human model (Fig. 2; see also Fig. A1).Each camera is located at p cam , shifted by (∆ x , ∆ y , ∆ z ) from the human position p h , with rotation angles θ lr and θ ud .Horizontal shifts are set as ∆ x , ∆ y = ϵ and the vertical shift as ∆ z .For camera i (i = 1, . . ., 8),</dt>
<dt>θ i ud is defined as: tan θ i ud = 0 : i is odd ∆z √ 2ϵ</dt>
<dd>i is even and the left-right angle θ i lr = πi 8 , while the overhead camera (i = 9) has θ 9 lr = 0 and θ 9 ud = π 2 .This multi-view setup provides dense RGB coverage, enabling fine adjustments to resolve inconsistencies like mesh-object clipping.This stage took over 430 hours of annotation, yielding 529 models across 374 regions in 90 scans.</dd>
</dl>
<p>Human Activity Annotation: Multi-Human Enrichment.In Stage 2 (Fig. 2), we enrich scene diversity and interactions through a human-in-the-loop approach Ding et al. ( 2024), adding new characters and complex motion paths into regions R with existing activities h i at positions p hi .Regional context, including objects O within 6 meters of h i and their positions, is provided to LLMs to generate diverse multi-human scenarios, which are refined in four rounds of manual review for scene consistency.Based on curated descriptions, new motions are placed relative to objects and annotated using the multi-camera method from Stage 1, enabling complex actions such as walking downstairs (details in Appx.B.5).After two annotation stages, the dataset comprises 910 human models across 428 regions in 90 scans (Fig. 3(a)(b)), including 111 outdoor humans, 72 two-person interactions, 59 three-person groups, and 15 four-person groups.Among these, 268 involve complex motions such as climbing stairs, substantially enriching the dataset with realistic behaviors.Detailed statistics are provided in Appx.B.8.This two-stage system enables precise modeling of social interaction spaces and personal boundaries, supporting agents in learning socially appropriate navigation strategies.To close the loop, agents perceive these dynamics through a navigation mesh (navmesh) Savva et al. (2019).Collisions are flagged when bounding volumes overlap, i.e., when interobject distances fall below the sum of their radii, triggering an automatic revert.This integration ensures agents not only experience dynamic and socially realistic environments but also learn to respect personal space and navigate effectively in dense human crowds.challenges.To align with continuous motions, we map positions to discrete nodes Li et al. (2024), apply small offsets for refinement, and integrate 627 annotated humans across 90 buildings.</p>
<p>Unified API.We provide a unified API supporting both modes with three core functions: (i) Human State Queries for retrieving bounding volumes, motion frames, and semantic annotations of nearby humans; (ii) Dynamic Scene Updates to notify agents of newly moved humans or environmental changes; and (iii) Collision Checks to evaluate whether a proposed move (e.g., forward step or viewpoint hop) would intersect with a human.By integrating HAPS 2.0, coarse-to-fine annotation, real-time multi-human rendering, and a single API across discrete and continuous settings, the HA-VLN Simulator establishes a comprehensive testbed for socially aware navigation.Figs.A2, A3, and A4 showcase the simulator's ability to capture diverse human behaviors, while Tables A1 and  A2 highlight its advantages over prior simulators and the improvements of HAPS 2.0 relative to HAPS 1.0.Appendix B.7 provides details on environment scales, latency, and usage examples.</p>
<p>HA-VLN AGENTS</p>
<p>To ground the unified HA-VLN task in our HA-VLN simulator, we introduce the Human-Aware Room-to-Room (HA-R2R) dataset and two baseline agents, HA-VLN-VL and HA-VLN-CMA.These agents are designed as reference implementations rather than final solutions, offering a starting point for developing more advanced models.They emphasize essential social capabilities including maintaining personal space, avoiding collisions, and adapting to bystanders, under the dynamic conditions of HA-VLN 2.0.As shown in Figs.A8 and 5, human behaviors add substantial complexity, motivating the dataset design and agent baselines described in the following paragraphs.</p>
<p>HA-R2R Dataset.The Room-to-Room in Continuous Environment (R2R-CE) dataset Krantz et al. (2020) supports continuous navigation but lacks explicit modeling of human interactions.We therefore extend it into HA-R2R, which contributes 16,844 curated instructions emphasizing social nuance, covering conversations, corridor crossings, and near-collision events.Table A3 presents representative directives, while Fig. A6 visualizes the expanded human-centric vocabulary.1999)) with a streamlined imitation learning objective, isolating the contribution of multimodal grounding.At timestep t, the agent updates its hidden state s t and predicts an action distribution:
s t , p a t = HA-VLN-VL(s t−1 , X, V t ), (3
) where X is the tokenized instruction (often referencing multiple humans) and V t encodes the fused RGB-depth view.A Transformer with a specialized state token attends jointly to visual and linguistic tokens, and final probabilities are derived via pooled attention:
p a t = AveragePool l s,v .
(4) Fine-tuned from Prevalent Hong et al. (2021) on HA-R2R, HA-VLN-VL demonstrates how stronger grounding alone benefits navigation under socially complex conditions (Appendix C.6).</p>
<p>HA-VLN-CMA Agent.HA-VLN-CMA emphasizes collision avoidance and real-time adaptation.Built on cross-modal attention (CMA) Krantz et al. (2020), it fuses textual embeddings l = BERT(I) with visual features v t = ResNet(o t ).Multi-head attention produces a joint representation f t , which an MLP maps to action probabilities: 5.17 4.07 0.43 0.24 7.72 6.31 0.61 0.12 5.43 6.94 0.58 0.17
P (a t | f t ) = Softmax MLP action (f t ) .(5)</p>
<p>EXPERIMENTS</p>
<p>Evaluation Metrics.We evaluate performance on the HA-VLN 2.0 benchmark using two suites of metrics.</p>
<p>(1) Social compliance.only at the endpoint of a step.Specifically, increasing the step size (from 0.1 m to 1.0 m), approximating DE-style navigation, can improve performance.We also conducted an additional experiment (Table A4) in which a 1.0 m step was treated as four 0.25 m sub-steps, and a 2.25 m step as nine 0.25 m sub-steps, with collisions checked after each sub-step.When evaluated on BEVBert in the val unseen split, the agents failed to navigate effectively with both 1.0 m and 2.25 m step sizes, with   SR dropping close to zero.These results highlight the need to account for the potentially "teleportlike" movement behaviors in DE when considering complementarity.(4) Sensor Modalities.Table 5 confirms that either adding depth or RGB consistently lowers collisions and raises SR, reflecting the importance of 3D cues for navigating around moving bystanders.</p>
<p>LEADERBOARD &amp; REAL-WORLD VALIDATION</p>
<p>HA-R2R Test Dataset &amp; Leaderboard.Building on R2R-CE, we present HA-R2R, featuring 16,844 instructions across 90 building scans with 910 annotated human models (see Secs. 3 &amp; 4).While retaining path continuity from R2R-CE, we introduce refined goals to emphasize social awareness.The test partition of HA-R2R contains 3,408 instructions across 18 withheld buildings and intentionally emphasizes multi-human routes.To assess performance on this challenging test split, we host leaderboards for HA-R2R-DE and HA-R2R-CE benchmarks, evaluating both collision-related metrics (TCR, CR) and navigation metrics (NE, SR).We prepare an interactive interface shown in Figure 6 (a), where participants can explore the simulator from nine different views to examine all the annotated human motions and the surrounding environments.This allows them to gain a deeper understanding of the challenging dynamic scenarios we provide.Submissions may include agent code or trajectories, providing reproducible, server-side evaluations and setting a new benchmark for human-centric, dynamic VLN research.</p>
<p>Real-World Validation &amp; Setup.We deploy our trained agents on a Unitree Go2-EDU quadruped, equipped with Intel Realsense D435i RGB-D camera, MID360 3D LiDAR, and IMU for onboard perception and control.As (office, living room, hallway, lobby), each populated by 2-4 free-moving volunteers.Implementation details and more visual examples are provided in Appendix D.5.The agent navigates safely in moderately congested conditions but faces challenges in tight corridors or sudden crowd convergence, highlighting the need for robust re-planning under partial observability.</p>
<p>CONCLUSION</p>
<p>We presented HA-VLN 2.0, a unified framework that standardizes discrete and continuous VLN under explicit human-centric constraints.By integrating dynamic human motion, refined annotations, and high-fidelity simulators, our HA-R2R dataset emphasizes human-centric instructions.Experiments show social awareness, multi-human interactions, and partial observability greatly increase complexity, reducing advanced agents' performance.Nevertheless, our approach balances safety, efficiency, and personal space.Real-world tests confirm sim-to-real transfer, while our public leaderboard standardizes evaluations.By releasing all data, simulators, agents, and tools, we promote socially responsible, context-aware navigation in dynamic, human-populated environments.</p>
<p>Preprint.</p>
<p>bystanders, interpret language mentioning people and their behaviors, and uphold social standards.This integrated setup results in a benchmark that closely aligns with real-world navigation demands.</p>
<p>A.  In particular, these agents may not recognize the need to maintain safe distances, avoid disturbing activities, or adapt routes with active bystanders.HA-VLN agents address these gaps by navigating among multiple, moving humans and adhering to social norms.They interpret finegrained, human-centric instructions and leverage visual cues that reflect real-world interactions, ensuring collision-free, respectful travel.This fusion of social compliance and human dynamics sets HA-VLN apart, aligning agent behavior more closely with real-world challenges Dong et al. (2025).</p>
<p>B SIMULATOR DETAILS</p>
<p>B.1 HAPS DATASET 2.0</p>
<p>We develop HAPS 2.0 to address the shortcomings of its predecessor Li et al. (2024), particularly in terms of mismatches between textual descriptions and motion data, as well as the limited diversity of region-motion associations.</p>
<p>Motion-Description Alignment.The original HAPS dataset contains 435 motion categories, each defined by a region (e.g., hallway) and a textual description (e.g., "Someone talking on the phone while pacing").However, more than half of these pairs do not match accurately.We therefore conduct a two-round manual verification, where multiple volunteers determine whether each pair is valid.Motions that fail both rounds are removed, yielding 172 precisely aligned motions.</p>
<p>Diversifying Region-Motion Relationships.In the initial dataset, each region was tied to only a few rigidly defined motions (e.g., hallway mostly involves "pacing on a phone," stairs focuses on "sliding down a banister" or "decorating the stairway").Such narrow mappings cause biases and limit the realism of agent navigation.To remedy this, we reorganize region-motion associations,   adapting the same motion to fit various environments, including both indoor and outdoor scenes.For instance, "talking on the phone" is re-contextualized to reflect whether someone is pacing upstairs or moving around a meeting room.This broader approach offers more faithful representations of human behavior and reduces environmental biases, thus improving real-world applicability.</p>
<p>HAPS 2.0 vs. HAPS 1.0.Table A2 quantitatively contrasts HAPS 2.0 with HAPS 1.0.We recruit 26 volunteers to evaluate every motion in both datasets on two 1-10 scales (motion accuracy, motionenvironment compatibility).A motion is deemed a failure if it scores under 3 in either category or below 5 in both.As shown, HAPS 2.0 achieves higher accuracy (8.5 vs. 6.3),better compatibility (8.1 vs. 5.9), and zero failures (0 vs. 120).It also increases motion diversity (486 vs. 435) and overall annotation effort (430+ vs. 320 hours).Moreover, HAPS 2.0 refines annotation workflows and simulator design for enhanced generalization.</p>
<p>Altogether, HAPS 2.0 includes 26 distinct regions across 90 architectural scenes, covering 486 human activities in both indoor and outdoor contexts.Fig. A2 illustrates these improvements.By offering more accurate, flexible, and diverse depictions of human actions, HAPS 2.0 provides a robust foundation for research in human motion analysis, social navigation, and beyond.</p>
<p>B.2 COARSE ANNOTATION USING PSO</p>
<p>We adopt a coarse-to-fine strategy for positioning human motions in 3D scans.Initially, we define each region by boundary coordinates B lo = (x lo , y lo , z lo ), B hi = (x hi , y hi , z hi ), and compile an object list O = {j 1 , j 2 , . . ., j n } with positions p ji .We then use Particle Swarm Optimization (PSO) Kennedy &amp; Eberhart (1995) (more details are provided in Algorithm A1) to locate each motion h i at an optimal position p opt .</p>
<p>Safe Distance Constraint.We set ϵ = 1 m as the minimum clearance between humans and objects, ensuring a realistic layout while leaving space for agent passage.</p>
<p>Adaptive Penalties.Our fitness function applies penalties to placements that violate constraints (e.g., intersecting walls or overlapping humans).This strategy discourages infeasible poses and promotes plausible scene geometry alignments.The resulting coarse alignment establishes a starting point, after which we apply finer manual or semi-automated adjustments to refine multi-human interactions and ensure consistent coverage of diverse motion types.</p>
<p>Preprint.Filter human motions H ′ ⊆ H matching r;</p>
<p>Algorithm A1</p>
<p>3:</p>
<p>Match objects O with human motions H ′ based on semantic similarity to form pairs (h i , j i );</p>
<p>4:</p>
<p>for each pair (h i , j i ) do 5:</p>
<p>Define search space S ← ⟨x lo , x hi ⟩ × ⟨z lo , z hi ⟩ × ⟨y lo , y hi ⟩ around object j i ;</p>
<p>6:</p>
<p>Initialize PSO with particles randomly positioned within S;</p>
<p>7:</p>
<p>Convergence criteria ← minimal fitness change; 8: repeat 9:</p>
<p>for each particle p in the swarm do 10:</p>
<p>Compute position p h of particle p; Optional:
z h ≥ z ji + ∆ z ; (Height offset) 19:
end for 20:</p>
<p>Update particle velocities and positions using PSO update equations;</p>
<p>21:</p>
<p>until convergence criteria met 22:</p>
<p>Assign best particle position p h to h i ;</p>
<p>23:</p>
<p>if no feasible solution found then 24:</p>
<p>Adjust PSO parameters and retry;</p>
<p>25:</p>
<p>end if</p>
<p>26:</p>
<p>end for 27: end while</p>
<p>B.3 FINE ANNOTATION USING A MULTI-CAMERA SETUP</p>
<p>To refine the coarse placements of human motions, we draw inspiration from 3D skeleton-capture methods Ji et al. (2018); Petrovich et al. (2021) and deploy nine RGB cameras, each positioned around the motion site.As shown in Fig. A1, this arrangement provides a comprehensive multiview perspective, revealing potential collisions or misalignments between the human figure and surrounding objects.</p>
<p>Camera Positions &amp; Angles.For each camera i (i = 1, 2, . . ., 8), we set its 3D location p cam to shift by ∆ x , ∆ y , and ∆ z from the base position p h .Horizontal rotation θ i lr is uniformly spaced at Preprint.</p>
<p>Scan id: 5LpN3gDmAk7</p>
<p>Scan id: ARNzJeq3xxb</p>
<p>Scan id: RPmz2sHmrrY</p>
<p>Scan id: YmJkqBEsHnH Scan id: VFuaQ6m2Qom</p>
<p>Scan id: JeFG25nYj2p</p>
<p>Scan id: E9uDoFAP3SH</p>
<p>Scan id: 8194nk5LbLH Scan id: gTV8FGcVJC9 πi 8 , while vertical rotation θ i ud depends on whether i is odd or even:
tan θ i ud = 0, if i is odd, ∆z √ 2 ϵ , if i is even. (A1)
For the ninth camera (overhead view), θ 9 lr = 0 and θ 9 ud = π 2 .These settings are ideal for general views and can be further adjusted in constrained spaces (e.g., narrow closets) or scenes requiring specialized viewpoints.</p>
<p>B.4 FINE ANNOTATION PROTOCOL</p>
<p>We adopt the following six-step procedure to fine-tune a human's position and orientation:</p>
<ol>
<li>Initial View.Generate an overall preview of the human figure at p h (Fig. A1(a)).2. Multi-Camera Observations.Collect images from the nine cameras (Figs.A1(b)-(c)).Adjust camera angles or offsets as necessary, particularly in tight scenes like small bathrooms or closets.3. Vertical Collision Checks.Inspect overhead Camera 9 to detect vertical overlaps (e.g., arms interpenetrating a table).If collisions exist, identify the nearest side camera to determine how best to shift the figure.4. Horizontal Translation.Modify ∆ x and ∆ y accordingly-if a nearby camera (e.g., Camera 1) reveals front-facing overlaps, shift p h by adding or subtracting based on Camera 1's perspective.5. Side Cameras Review.Examine Cameras 2-8 to catch lingering overhang or collisions.Adjust the figure's position proportionally, typically referencing a standard human height of 1.5 m to gauge whether shifts remain plausible.6. Finalize Output.Upon confirming a collision-free layout, automatically generate the final video render and corresponding JSON metadata files.</li>
</ol>
<p>This multi-camera process systematically eliminates misalignments, ensuring each human model remains properly integrated within the environment.The result is a more realistic portrayal of multihuman interactions and improved fidelity for downstream tasks.</p>
<p>B.5 MULTI-HUMAN INTERACTION &amp; MOVEMENT ENRICHMENT</p>
<p>To diversify scenes and amplify interactivity, we place additional characters into regions already featuring human motion annotations.This enables more complex interactions and varied motion trajectories.Manual insertion of extra characters, however, is time-consuming and prone to subjective bias, limiting data reliability and diversity.</p>
<p>Human-in-the-Loop Method.We employ large language models (LLMs) such as ChatGPT-4 and LLaMA-3-8B-Instruct to propose plausible multi-human scenarios.Each prompt integrates details about existing human motions, object positions, and regional context, guiding the LLMs to generate rich, multi-character interactions.Our prompt design uses a system prompt and few-shot examples (Listings 1 and 2) to ensure clarity and detail.For instance, we collect each human's position and identify objects within 6 m, describing relative distances and orientations.The LLMs then construct additional human activities suited to the scene, merging them into cohesive multi-person narratives.</p>
<p>Iterative Annotation Workflow.After the LLMs produce candidate interactions, we merge outputs from ChatGPT-4 and LLaMA-3-8B-Instruct, then manually refine and validate them over four rounds Ding et al. (2024); Cheng et al. (2024).This process corrects inconsistencies and ensures contextual alignment.We subsequently place new human motions according to the generated descriptions, leveraging our multi-camera technique (Sec.B.3) for precise annotation of complex activities (e.g., stair-walking, see Fig. A3).</p>
<p>Examples of Enriched Interactions.Fig. 2 demonstrates how additional humans can populate a living room: "two people sit on the couch, sharing popcorn on a small table," while "a third friend stands in front of the TV, cheering."Such enriched scenes capture realistic multi-human behaviors-from casual gatherings to active cheering-offering agents a broader range of social cues for navigation and interaction.</p>
<p>"system": "You are an expert in 3 D scene understanding, specializing in generating detailed multi-person interaction scenarios in complex environments.I will provide you with information about the current human motion, object positions, and the region context.Your task is to infer possible additional human interactions and describe their actions in relation to the objects and scene.""example": ... //examples in Listing 2 "user": Input: [human_motion]: \"...\"; [Region]: \"...\"; [Object_1]: \"... \"; [Object_2]: \"...\"; [Object_3]: \"...\";</p>
<p>Based on these inputs, provide a detailed, objective description of additional people in the scene, their locations, and their interactions with objects and each other.Ensure the interactions are plausible based on the given layout and actions.</p>
<p>Output: ... //generation</p>
<p>Listing 1: Format of our LLM prompt for multi-human interactions &amp; movements enrichment.</p>
<h1>Example 1 Input:</h1>
<p>[human_motion]: "tv:An individual enjoying a sports game, cheering loudly.\"[region]: "\tv.\"[TV]: "\4 meters in front of the couch, placed on a white media console against the wall.\"[couch]: "\Against the wall, 4 meters away facing the TV.\" [table]: "\1.5 meters in front of the couch, holding some snacks and drinks.\"</p>
<p>Output: "Three friends are gathered in the living room, watching a movie together.Two of them are seated comfortably on the couch, which is positioned against the wall, facing the TV.They are sharing popcorn and drinks placed on a table 1.5 meters in front of the couch.The atmosphere is relaxed, with one friend fully focused on the movie, casually eating popcorn.Another friend seated next to him.In front of the TV, a third friend stands closer, about 2 meters from the couch, is more animated, loudly cheering as they switch their attention to a sports game playing on a different screen."</p>
<p>Listing 2: Few-shot example for for multi-human interactions &amp; movements enrichment.</p>
<p>B.6 REAL-TIME HUMAN RENDERING</p>
<p>We integrate dynamic human models into simulation through a multi-threaded pipeline inspired by Producer-Consumer principles and Java-style signaling (Algorithm A2).This setup enables agents to observe and respond to human motions in real time, facilitating adaptable navigation policies.</p>
<p>System Initialization.We begin by loading the environment E, the set of human motions H, and an object template manager T that handles 3D model templates efficiently.</p>
<p>Signal Sender Thread (Thread 1).At intervals ∆t, Thread 1 places "refresh" signals into a queue Q.If Q is full, it pauses until earlier signals are processed, preventing data overload.This thread models a continuous stream of human motion updates at a fixed frequency.</p>
<p>Main Simulation Thread (Thread 2).When the agent is about to act, Thread 2 checks Q for pending refresh signals.It calculates the current frame index t as (signals processed mod N ), where N is the total length of the human motion sequence.Template manager T then removes outdated models and loads frame t into the environment, adjusting each figure's position and orientation.</p>
<p>(3) Navigation Support.An A*-based planner computes candidate trajectories while accounting for both dynamic humans and static obstacles.During execution, we monitor any divergence between the agent's chosen route and the planner's recommended path.This method highlights humancentric obstacles and informs the agent's short-term re-planning steps.Our unified API supports real-time detection, tracking, and socially compliant navigation decisions in both discrete and continuous modes.It simplifies multi-human scene management, ensures intuitive collision handling, and provides robust path-planning assistance-together forming a foundation for advanced humanaware navigation algorithms.c) compare the 15 most frequent motions before and after multi-human enrichments.While the total number of motions increases, we also embed additional movement patterns and group interactions into existing actions.For instance, "talking on the phone while pacing" may now involve extended pacing distances or layered scenarios like "a couple having a quiet conversation" or "friends sharing stories over dinner."Movement Distance Analysis.Fig. A4(d) displays the distribution of trajectory lengths for actively moving humans.Specifically, 22.4% cover distances up to 0.5 m, 37.3% reach 0.5-1 m, 25.0% span 1-1.5 m, 11.6% extend 1.5-2 m, and the remaining 3.7% exceed 2 m.This wide range reflects the diverse indoor and outdoor behaviors encompassed in the dataset.</p>
<p>Human Impact Analysis.As shown in Fig. A4(e), humans exert a notable influence on navigation paths: 35.5% of the 16,844 paths in HA-VLN physically intersect with human motion, while 49.7% of viewpoints are indirectly affected (i.e., humans are visible along the route).These statistics underline the importance of accounting for human presence and movement trajectories when designing real-world navigation agents.</p>
<p>C AGENT DETAILS C.1 HA-R2R INSTRUCTION EXAMPLES</p>
<p>Table A3 illustrates four sample instructions from the Human-Aware Room-to-Room (HA-R2R) dataset.These examples encompass multiple scenarios: multi-human interactions (e.g., 1, 2, 3), direct agent-human encounters (e.g., 1, 2, 3), situations with four or more bystanders (e.g., 3), and paths devoid of humans (e.g., 4).Together, they demonstrate how HA-R2R challenges an agent with diverse human-aligned instructions.</p>
<p>C.2 HA-R2R INSTRUCTION GENERATION</p>
<p>To create enriched instructions for HA-R2R, we use ChatGPT-4o and LLaMA-3-8B-Instruct to expand upon R2R-CE's original textual data.Our strategy involves a carefully crafted few-shot prompt, combining a system prompt (Listing 3) and few-shot examples (Listing 4).</p>
<p>Prompt Structure.The system prompt lays out guidelines for generating instructions that emphasize social context.It encourages mentioning human activities and interactions relevant to navigation paths Wu et al. (2025).Few-shot examples then illustrate the desired format, including references to human behavior (e.g., "someone quietly making a phone call; keep your voice down as you proceed"), positional references, and object interactions.</p>
<p>Iterative Refinement.In early trials, the models sometimes produced extraneous or subjective content, lacking sufficient detail on human activities.We iteratively refined the system prompt and examples, clarifying the need for neutral tone, accuracy, and contextual alignment with humanrelated scenarios.In each round, we analyzed model outputs, identified discrepancies, and adjusted examples to showcase more detailed, coherent, and socially aware instructions.This process guided Table A3: Instruction Samples from the HA-R2R Dataset.Text in purple highlights human-related actions/movements, while text in blue indicates explicit agent-human interaction cues.These examples illustrate how HA-R2R integrates dynamic human considerations and social awareness into navigation instructions.</p>
<ol>
<li>Exit the library and turn left.As you proceed straight ahead, you will enter the bedroom, where you can observe a person actively searching for a lost item, perhaps checking under the bed or inside drawers.Continue moving forward, ensuring you do not disturb his search.As you pass by, you might see a family engaged in a casual conversation on the porch or terrace, be careful not to bump into them.Maintain your course until you reach the closet.Stop just outside the closet and await further instructions.2. Begin your path on the left side of the dining room, where a group of friends is gathered around a table, enjoying dinner and exchanging stories with laughter.As you move across this area, be cautious not to disturb their gathering.The dining room features a large table and chairs.Proceed through the doorway that leads out of the dining room.Upon entering the hallway, continue straight and then make a left turn.As you walk down this corridor, you might notice framed pictures along the walls.The sound of laughter and conversation from the dining room may still be audible as you move further away.Continue down the hallway until you reach the entrance of the office.Here, you will observe a person engaged in taking photographs, likely focusing on capturing the view from a window or an interesting aspect of the room.Stop at this point, ensuring you are positioned at the entrance without obstructing the photographer's activity.3. Starting in the living room, you can observe an individual practicing dance moves, possibly trying out new steps.As you proceed straight ahead, you will pass by couches where a couple is engaged in a quiet, intimate conversation, speaking softly to maintain their privacy.Continue moving forward, ensuring you navigate around any furniture or obstacles in your path.As you transition into the hallway, notice another couple enjoying a date night at the bar, perhaps sharing drinks and laughter.Maintain a steady course without disturbing them, keeping to the right side of the hallway.Upon reaching the end of your path, you will find yourself back in the living room.Here, a person is checking their appearance in a hallway mirror, possibly adjusting their attire or hair.Stop by the right candle mounted on the wall, ensuring you are positioned without blocking any pathways.4. Begin by leaving the room and turning to your right.Proceed down the hallway, be careful of any human activity or objects along the way.As you continue, look for the first doorway on your right.Enter through this doorway and advance towards the shelves.Once you reach the vicinity of the shelves, come to a halt and wait there.During this movement, avoid any obstacles or disruptions in the environment.We fuse these two feature streams along with a directional encoding d i indicating spatial orientation:
v i = v rgb i W rgb ; v d i W depth ; d i W merge ,([CLS] [SEP] a t p t s x Q x K x V 1 t s − t V X Multi-layer Transformer v Q v K v V s Q s K s V t s 1 t s −</li>
</ol>
<p>RGB Image</p>
<p>Depth Image Preprint.</p>
<p>D EXPERIMENTS DETAILS D.1 EVALUATION METRICS</p>
<p>We adopt a two-tier evaluation protocol for HA-VLN, measuring both perception (human awareness) and navigation (task completion).Perception metrics track how effectively the agent detects and responds to dynamic humans, while navigation metrics assess overall performance.</p>
<p>Total Collision Rate (TCR).Given the strong impact of human activities around critical nodes (viewpoints), we manage dynamic humans to ensure precise measurement.For navigation instance i, let A c i be the set of human activities at these critical nodes.We define:
TCR = L i=1 ( c i − | A c i | ) L , (A15)
where c i counts collisions within 1 m of a human.TCR quantifies how often collisions occur in human-occupied zones.</p>
<p>Collision Rate (CR).CR is the fraction of navigation instances incurring at least one collision, conditioned on the fraction β of instructions influenced by humans:
CR = L i=1 min c i − |A c i |, 1 βL . (A16)
Unlike TCR, CR highlights whether a collision occurred at all-offering insight into safety over entire trajectories.</p>
<p>Navigation Error (NE).NE is the mean distance between agent's final position and intended target:
NE = L i=1 d i L ,(A17)
where d i is the agent-target distance at episode end.</p>
<p>Success Rate (SR).SR measures the ratio of episodes completed with zero collisions, and checks if the agent stops sufficiently close to the goal Anderson et al. (2018), we provide the equation for the collision check part here:
SR = L i=1 I c i − |A c i | = 0 L , (A18)
where I is 1 if the agent avoids collisions, and 0 otherwise.</p>
<p>D.2 GROUND TRUTH PATH ANNOTATION</p>
<p>In HA-VLN-CE, the agent must reach within 3 m of the target while minimizing collisions.To label ground-truth paths, we use an A*-based heuristic search that identifies the shortest viable route, dynamically re-planning when obstacles block progress.</p>
<p>D.3 FURTHER DISCUSSION ON STEP SIZE</p>
<p>In Table A4, a 1.0 m step was treated as four 0.25 m sub-steps, and a 2.25 m step as nine 0.25 m substeps, with collisions checked after each sub-step.When evaluated on the val unseen split, BEVBert agent fails to navigate effectively with both 1.0 m and 2.25 m step sizes (SR drops to zero).Failures with Human Crossing.In Fig. A8, the agent performs well when no bystanders are present.Yet in a human-populated setting, it fails to adjust at step 37 when a volunteer crosses its path, leading to collision.</p>
<p>Collision vs. Avoidance.Fig. 5 similarly shows two scenarios.At step 39 in the top pane, a direct approach by a bystander overwhelms the agent, causing a collision.In the bottom pane at step 22, the agent successfully deviates upon sensing a person nearby, avoiding any collision altogether.These visualizations confirm that dynamic human presence greatly complicates navigation, highlighting the need for robust social-aware models.challenging, especially in narrow passages or at congested junctions.Appendix D.5 further details performance under varying crowd densities.</p>
<p>Visual Demonstrations.Figs.A9, A11, and A13 show the robot traversing distinct indoor environments-offices, living rooms, and hallways-guided by natural-language instructions.In Fig. 6 (b), the robot navigates around multiple people, leveraging camera inputs to avoid collisions through minor path adjustments.Although the agent typically succeeds in reaching its destination, collisions remain possible when bystanders change their trajectories unexpectedly.Figs.A10, A12, and A14 illustrate such scenarios, highlighting real-time challenges in unpredictable, human-inhabited spaces.More demos on our project webpage, further illustrate robot's performance and underscore how human-aware training aids sim-to-real transfer in dynamic indoor environments.</p>
<p>Insights.These experiments confirm that simulation-trained, multi-human navigation policies can indeed transfer to physical robots.However, further refinement in collision forecasting and reactive control is needed to handle unpredictable human behavior in tight indoor settings.</p>
<p>E USE OF LLMS</p>
<p>Large Language Models (LLMs) were used to aid in the writing and polishing of the manuscript.Specifically, we used an LLM to assist in refining the language, improving readability, and ensuring clarity in various sections of the paper.The model helped with tasks such as sentence rephrasing, grammar checking, and enhancing the overall flow of the text.</p>
<p>It is important to note that the LLM was not involved in the ideation, research methodology, or experimental design.All research concepts, ideas, and analyses were developed and conducted by the authors.The of the LLM were solely focused on improving the linguistic quality of the paper, with no involvement in the scientific content or data analysis.</p>
<p>The authors take full responsibility for the content of the manuscript, including any text generated or polished by the LLM.We have ensured that the LLM-generated text adheres to ethical guidelines and does not contribute to plagiarism or scientific misconduct.</p>
<p>Figure 2 :
2
Figure 2: HA-VLN Simulator.Unlike HA3D, which modeled sparse and static human activities in discrete settings, HA-VLN incorporates rich and dynamic human behaviors using HAPS 2.0 (172 activities, 486 models, 58k frames).Annotation involves two stages: (i) coarse-to-fine optimization via PSO and multi-view camera setups, and (ii) human-in-the-loop refinement for realistic crowd dynamics.Real-time rendering updates motions through a signaling mechanism, facilitating collision detection and dynamic agent-environment interactions.These improvements bridge discrete evaluation (DE) and realistic continuous navigation (CE), establishing a robust foundation for benchmarks in socially intelligent navigation.</p>
<p>Figure 3 :
3
Figure 3: Motion Analysis.(a) Top three motions from Stage 1 (without enrichment) and Stage 2 (with enrichment).(b) Overall activity statistics, comparing interaction types, movement distances, and the number of models.Enrichment expands both the variety and dynamic range of human activities.</p>
<p>Discrete vs. Continuous Settings.HA-VLN-CE (Continuous) allows agents to move in realvalued increments (e.g., 0.25 m forward, 15 • turns), supporting fine-grained collision avoidance and adaptive social behavior.As shown in Fig. A4, each scene can host up to 10 humans, with simulation speeds of 30-60 FPS on standard GPUs.HA-VLN-DE (Discrete) extends HA3D Li et al. (2024) by incorporating HAPS 2.0 data across indoor and outdoor environments.Agents hop among panoramic viewpoints while humans move continuously, preserving core social-navigation Preprint.</p>
<p>Figure 4 :
4
Figure 4: HA-R2R Dataset Analysis.(a) Distribution of instruction length by human group size (none to &gt;3).(b) Comparison of instruction lengths between HA-R2R and R2R-CE.We generate these enriched instructions via targeted LLM prompts (Appendix C.2), capturing diverse social scenarios.This augmentation shifts navigation from static paths to socially contingent routes, e.g., "avoid the couple chatting near the bar."Comparative analyses (Appendix C.3) highlight both the annotation workload and HA-R2R's potential for human-aware navigation.HA-VLN-VL Agent.The HA-VLN-VL focuses on visual-language alignment.Adapted from Recurrent VLN-BERT Hong et al. (2021), it replaces actor-critic methods (e.g., A2C Konda &amp; Tsitsiklis (1999)) with a streamlined imitation learning objective, isolating the contribution of multimodal grounding.At timestep t, the agent updates its hidden state s t and predicts an action distribution:s t , p a t = HA-VLN-VL(s t−1 , X, V t ), (3) where X is the tokenized instruction (often referencing multiple humans) and V t encodes the fused RGB-depth view.A Transformer with a specialized state token attends jointly to visual and linguistic tokens, and final probabilities are derived via pooled attention:</p>
<p>Figure 5 :
5
Figure 5: Agent Trajectory Examples (HA-VLN-CMA * ).The top row demonstrates a failed navigation scenario where the agent fails to avoid an oncoming human, ultimately resulting in a collision.In contrast, the bottom row showcases a successful navigation: the agent proactively adjusts its trajectory to the left, effectively avoiding human interference and completing the task without collision.</p>
<p>Fig</p>
<p>Fig. A7(b) outlines the architecture (details in Appendix C.7).To address partial observability and unpredictable motion, we adopt Environmental Dropout (Envdrop) Tan et al. (2019) to simulate occlusions and Dataset Aggregation (DAgger)Ross et al. (2011) for iterative error correction.These strategies enhance re-planning when agents face obstacles or unexpected behaviors.Figs.A8 and 5 illustrate agent responses to bystanders, showing that collision risk and route deviation increase sharply in crowded passages.HA-VLN-CMA re-plans aggressively when blocked, whereas HA-VLN-VL leverages textual grounding to maintain appropriate distances.This contrast highlights our dual contributions: a socially enriched dataset (HA-R2R) and two baseline agents serving as extensible reference points.These baselines are not final solutions but starting points for the community to build, refine, and extend toward more advanced human-aware navigation models.Sec. 5 evaluates both agents on HA-VLN 2.0, demonstrating complementary strengths.</p>
<p>Figure 6 :
6
Figure 6: (a).Interactive interface we provide to explore 910 annotated human models and environments in HA-VLN 2.0 simulator from nine views.(b).Human-aware navigation with multiple bystanders.Left: Instruction provided to the robot.Right: A third-person view illustrates the robot's trajectory among dynamic bystanders, and selected robot observations from onboard camera.</p>
<p>Figure 6 (b) illustrates, experiments are conducted in four indoor spaces Preprint.</p>
<p>A. 3
3
AGENTS FOR VLN TASKS From early attention-based and reinforcement-learning approaches Ma et al. (2019); Qi et al. (2020); Wang et al. (2019) to modern vision-language pre-training Lu et al. (2019); Hao et al. (2020); Li et al. (2020), VLN agents have grown increasingly adept at parsing instructions and navigating complex environments.However, most existing solutions, including EnvDrop Tan et al. (2019), PREVA-LENT Hao et al. (2020) and VLN-BERT Hong et al. (2021), rely on panoramic navigation, streamlining the action space but limiting realism of their movement.Recent efforts like NavGPT Zhou et al. (2024) and NaVid Zhang et al. (2024a) explore continuous, egocentric navigation in partially dynamic worlds, yet they still lack explicit attention to human-aligned instructions or social compliance.</p>
<p>al. (2006),DRIF Blukis et al. (2018),VLN-R2R Anderson  et al. (2018),TOUCHDOWN Chen et al. (2019),REVERIE Qi et al. (2020), Dial-FRED Gao et al. (2022) VNLA Nguyen et al. (2019), CVDN Thomason et al. (2020), R4R Jain et al. (2019), RxR Ku et al. (2020), EQA Das et al. (2018), IQA Gordon et al. al. (2019), AuxRN Zhu et al. (2020), PREVALENT Hao et al. (2020), RelGraph Hong et al. (2020), HAMT Chen et al. (2021), NavCoT Lin et al. (2025) Rec-VLNBERT Hong et al. (2021), EnvEdit Li et al. (2022), Airbert Guhur et al. (2021), Lily Lin et al. (2023), ScaleVLN Wang et al. (2023) ✓ × ✓ NavGPT Zhou et al. (2024), NaVid Zhang et al. (2024a), Student Force Anderson et al.</p>
<p>Figure A1 :
A1
Figure A1: Multi-View Camera Setup.(a) A sample scene overview.(b) Schematic illustrating the nine camera placements around the human figure, noting key coordinates and rotations.(c) Example snapshots from the nine directional cameras, each providing a distinct viewpoint for accurate motion annotation.</p>
<p>Figure A2 :
A2
Figure A2: Overview of HA-VLN Scenes.These examples illustrate annotated human subjects across multiple scans in the HA-VLN simulator, highlighting a range of well-aligned motions, movements, and interactions (both with objects and with other humans).</p>
<p>Figure A3 :
A3
Figure A3: Movement Examples.We present representative frames from a single set of human motions, each annotated with its corresponding movement.Activities include ascending stairs, running, and pacing.For clarity, we highlight four camera views (Cameras 2, 4, 6, 8) within the multi-camera setup to provide a comprehensive perspective of human behaviors.(Zoom in for finer details.)</p>
<p>B. 8
8
HUMAN ACTIVITIES ANNOTATION DATA ANALYSIS Human Distribution by Region.Fig. A4(a) illustrates the distribution of 910 humans across 26 region types in 90 buildings, averaging about nine individuals per building.Even though each person moves independently, this distribution ensures robust and dynamic multi-human interactions, closely mirroring real-world scenarios.Motion Frequency Analysis.Figs.A4(b)-(</p>
<p>Figure A7 :Figure A8 :
A7A8
Figure A7: Network Structures.(a) HA-VLN-VL adopts a BERT-like transformer with a specialized state token.RGB and depth inputs are compressed by ResNet-50 and concatenated, while instruction tokens feed a BERT-like encoder.A multi-layer transformer computes cross-modal attention, producing per-step action probabilities via average-pooling and a final projection.In both architectures, continuous or discrete commands are then derived for navigation based on the agent's policy output.(b) HA-VLN-CMA employs a cross-modal attention (CMA) module combined with a GRU policy.RGB and depth images are first processed by two ResNet-50 encoders and fused into a single feature stream, which attends to the instruction tokens; the fused features are then fed into a GRU and MLP to predict actions.</p>
<p>Figure A9 :STOPFigure A10 :
A9A10
Figure A9: Navigation success in an office (left: no humans, right: with humans).Top: The given instruction for the robot.Middle: A third-person view of the robot's path.Bottom: The robot's selected view.</p>
<p>lo , y lo , z lo ) and B hi = (x hi , y hi , z hi ), and an object set O = {j 1 , j 2 , . . ., j n } with positions p ji .We filter H to retain motions consistent with r, forming H ′ .Each motion h i ∈ H ′ is paired with an object j i ∈ O via semantic similarity, producing (h i , j i ) pairs.
Preprint.The system includes two modules, HA-VLN-CE (continuous) and HA-VLN-DE(discrete), with a unified API (Sec. 3) for human state queries, dynamic scene updates, and collisionchecks. Fig. 2 places these components in the agent's action and observation loop, forming the basisfor the annotation, rendering, and interaction mechanisms that follow.HAPS 2.0 Dataset. Human motion naturally adapts to and interacts with surrounding environments.The Human Activity and Pose Simulation (HAPS) Dataset 2.0 extends HAPS 1.0 Li et al. (2024)with two major advances: (i) refined and diversified human motions and (ii) region-aware activ-ity descriptions (details in Sec. B.1). HAPS 2.0 defines 26 regions across 90 architectural scenesand contributes 486 validated activity descriptions covering indoor and outdoor contexts. Thesedescriptions, verified by human surveys and quality control using ChatGPT-4 Brown et al. (2020),explicitly ground actions in regions (e.g., "workout gym exercise: an individual running on a tread-mill"). The Motion Diffusion Model (MDM) Guy et al. (2022), built on the SMPL framework,converts these descriptions into 486 3D human motion models H, yielding 120-frame sequencesH = ⟨h 1 , h 2 , . . . , h 120 ⟩ that capture fine-grained motion and shape information * . Fig. A2 illustratesrepresentative contexts, while Fig. A3 shows sample motions (e.g., climbing stairs, running).Human Activity Annotation: Coarse-Level. To integrate HAPS 2.0 into our simulator, we adopta coarse-to-fine strategy. At the coarse level, each region R is defined by a label r, boundarycoordinates B lo = (x ParticleSwarm Optimization (PSO) Kennedy &amp; Eberhart (1995) (Alg. A1) then determines the optimalplacement p hi opt around j i , bounded by R and penalized if violating constraints such as maintaining</p>
<p>Table 1 :
1
HA-VLN-CE Results Across Validation (Seen/Unseen) and Test Splits."HA-VLN-CMA
Validation SeenValidation UnseenTestAgentRetrainedZero-shotRetrainedZero-shotRetrainedZero-shot7.6363.09 0.77 0.05 7.8863.84 0.75 0.04 7.3447.06 0.77 0.07 7.9563.96 0.76 0.03 7.3047.55 0.76 0.07 7.8962.14 0.74 0.04HA-VLN-CMA-DA6.1117.45 0.61 0.17 6.9537.85 0.72 0.07 7.0027.25 0.69 0.09 7.0538.22 0.73 0.05 7.1228.33 0.69 0.08 6.9836.53 0.73 0.06HA-VLN-CMA  *5.613.34 0.60 0.17 7.1029.99 0.69 0.11 6.238.10 0.69 0.10 6.6232.48 0.70 0.09 6.649.23 0.72 0.09 7.0931.80 0.75 0.09HA-VLN-VL5.024.44 0.52 0.20 7.82
* " denotes the full version of HA-VLN-CMA (+DA +EV).Metrics include NE (Navigation Error, meters), TCR (Total Collision Rate), CR (Collision Rate per step), and SR (Success Rate), with lower NE/TCR/CR and higher SR indicating better performance.All agents receive panoramic RGBD observations at each location.NE↓ TCR↓ CR↓ SR↑ NE↓ TCR↓ CR↓ SR↑ NE↓ TCR↓ CR↓ SR↑ NE↓ TCR↓ CR↓ SR↑ NE↓ TCR↓ CR↓ SR↑ NE↓ TCR↓ CR↓ SR↑ HA-VLN-CMA-Base 3.67 0.45 0.05 5.35 6.63 0.59 0.14 7.15 3.97 0.46 0.06 5.52 5.96 0.63 0.14 7.41 3.38 0.58 0.07 BEVBert An et al. (2023) 5.53 3.64 0.46 0.27 6.11 4.29 0.47 0.19 5.51 4.71 0.55 0.21 6.10 5.72 0.56 0.15 6.33 4.25 0.58 0.18 6.54 4.39 0.54 0.14 ETPNav An et al. (2024)</p>
<p>Table 2 :
2
(3)et al. (2024) awareness, we use Total Collision Rate (TCR) and Collision Rate (CR).TCR measures the overall frequency of collisions, while CR reflects the proportion of socially inappropriate interactions.factorsincludingcontinuousversusdiscretesettings,cross-domaingeneralizationcapabilities,humanpresenceandinteractionenrichment,stepsizevariations,andsensor modality configurations.These analyses investigate their respective impacts on human-aware navigation performance and reveal complementary knowledge between the DE and CE approaches.(2)Wedeploy and evaluate HA-VLN 2.0 agents in real-world robotic scenarios across diverse layouts (office spaces, living rooms, hallways, and lobbies) with free-moving human volunteers (Sec.5.2,AppendixD.5).We systematically benchmark two notable continuous navigation agents, BEVBertAn et al. (2023)and ETPNavAn et al. (2024), together with our HA-VLN-CMA and HA-VLN-VL agents in Table1.Each approach is trained/evaluated under two configurations: Retrained, where agents are trained/evaluated solely on HA-VLN-CE benchmark (HA-VLN-CE simulator + HA-R2R instruction dataset), and Zero-shot, where agents are trained solely on VLN-CE benchmark (VLN-CE simulator + R2R-CE) and evaluated on our benchmark.Table1shows pronounced gains when models incorporate HA-VLN-CE benchmark.For instance, BEVBert's SR increases from 0.19 to 0.27 in seen split and from 0.15 to 0.21 in unseen.In stark contrast, Table3shows that BEVBert trained on our benchmark performs comparably to the VLN-CE-trained one on VLN-CE benchmark (SR: 0.35 vs. 0.37).This bidirectional evaluation suggests that explicit references to dynamic crowd behavior enhance real-world navigational readiness and confirm the robustness of HA-VLN-CE.Figure5presents navigation visualizations of HA-VLN-CMA DE Table4(a) shows in human presence ablations, replacing humans with cylinders drops TCR by around 36% and raises SR by around 10%, while removing human interaction enrichment drops TCR by up to 22% and raises SR by up to 25%, confirming humans are not merely treated as generic moving obstacles during navigation.(3)StepSize.Table 4 (b) indicates a degree of knowledge complementarity between DE and CE navigation when collisions are detected
5.1 BENCHMARKING AGENTS ON HA-VLN 2.0HA-VLN-CE.
Anderson et al. (2018).We report Navigation Error (NE) and Success Rate (SR).A trajectory is deemed successful under SR not only when the agent stops sufficiently close to the goalAnderson et al. (2018), but also when it demonstrates effective obstacle avoidance.Formal definitions of these metrics are provided in Appendix D.1.We evaluate agents in two settings: (1) We assess the performance of HA-VLN 2.0 agents alongside several top agents on the HA-VLN 2.0 benchmark, utilizing both HA-VLN-CE (continuous) and HA-VLN-DE (discrete) (Sec.5.1).We conduct extensive analysis and ablation studies examining Preprint.key*agentontheHA-VLN-CEbenchmark, including one successful and one failed example.These examples demonstrate that dynamic human activities indeed increase the difficulty of navigation, while also making the scenarios more realistic and reflective of real-world challenges.HA-VLN-DE.Table 2 compares top discrete agents on both VLN and HA-VLN-DE benchmarks, showing discrete agents can achieve moderate SR yet suffer high collisions in crowded scenes.performance of agents trained on VLN vs. HA-VLN-DE (Unseen).All agents use panoramic RGB observations.Analysis &amp; Ablation Studies.(1)Cross-domainGeneralization.Table3reveals that HA-R2Rtrained agents achieve comparable SR to R2R-CE-trained agents (0.27 vs. 0.29) on R2R-CE validation set, while they outperform by +28.6% SR on the HA-R2R validation set, showcasing HA-R2R improves in-domain performance while maintaining cross-domain robustness.(2) Human Presence and Interaction Enrichment.</p>
<p>Table 3 :
3
Cross Domain Evaluation of BEVBert (CE) vs. Rec (PREVALENT) (DE).
Each model is trained/-</p>
<p>Table 4 :
4
An et al. (2024) of Human Presence (hp) and Interaction Enrichment (enrich).We evaluate without hp (replace human with cylinders) and without enrich (skip interaction &amp; movement enrichment in Sec. 3, Appendix B.5) on both CE and DE settings.Rec (PRE) denotes Rec (PREVALENT).Right: (b).Impact of Step Size on Navigation.Here the collision is detected only at endpoint of a step, thus increasing step size transitions from finer-grained control to more discrete (teleport-potential) steps (default step size for CE is 0.25m).We show results for both BEVBertAn et al. (2023)and ETPNavAn et al. (2024)on seen/unseen.
hp enrich Env AgentNE↓TCR↓CR↓SR↑AgentStep SizeValidation (Seen)Validation (Unseen)CE BEVBert6.105.720.560.15NE↓ TCR↓ CR↓ SR↑ NE↓ TCR↓ CR↓ SR↑✓✓CE ETPNav7.407.940.710.080.105.658.430.50 0.23 5.41 12.60 0.54 0.22DE Rec (PRE)7.310.310.790.22BEVBert0.25 (CE Default) 5.53 0.40 5.603.64 1.770.46 0.27 5.51 0.39 0.28 5.634.71 2.630.55 0.21 0.44 0.25CE BEVBert6.32 (↑3.6%) 5.11 (↓10.7%) 0.46 (↓17.9%) 0.17 (↑13.3%)1.005.820.420.21 0.29 5.540.630.26 0.26✓✗CE ETPNav7.35 (↓0.6%) 6.12 (↓22.9%) 0.63 (↓11.3%) 0.10 (↑25.0%)2.257.660.090.10 0.03 7.230.100.10 0.03DE Rec (PRE) 7.52 (↑2.9%) 0.27 (↓12.9%) 0.64 (↓19.0%) 0.27 (↑22.7%)0.105.15 11.70 0.54 0.20 5.47 18.66 0.64 0.16✗✗CE BEVBert CE ETPNav DE Rec (PRE) 7.33 (↑0.3%) 0.19 (↓38.7%) 0.42 (↓46.8%) 0.26 (↑18.2%) 6.13 (↑0.5%) 3.25 (↓43.2%) 0.35 (↓37.5%) 0.19 (↑26.7%) 7.75 (↑4.7%) 4.47 (↓43.7%) 0.53 (↓25.4%) 0.14 (↑75.0%)ETPNav0.25 (CE Default) 5.17 0.40 5.11 1.00 6.67 2.25 7.614.07 2.43 0.49 0.100.43 0.24 5.43 0.36 0.26 5.32 0.25 0.24 6.76 0.10 0.02 7.216.94 3.77 0.79 0.130.58 0.17 0.46 0.21 0.32 0.17 0.12 0.03</p>
<p>Table 5 :
5
Ablation on RGB/Depth Inputs.We compare BEVBert An et al. (2023) and ETPNav An et al. (2024) on seen/unseen validations.✓ denotes the sensor is enabled, while ✗ is disabled.Blue cells highlight performance changes (in %) upon removing/adding a modality.Best viewed in color.
AgentRGB DepthValidation (Seen)Validation (Unseen)NE↓TCR↓CR↓SR↑NE↓TCR↓CR↓SR↑✗✓5.68 (↑2.7%)3.77 (↑3.6%)0.48 (↑4.3%)0.25 (↓7.4%)5.50 (↑0.2%) 4.73 (↑0.4%) 0.53 (↑3.6%)0.20 (↓4.8%)✓✓5.533.640.460.275.514.710.550.21✓✓5.174.070.430.245.436.940.580.17
BEVBert An et al. (2023) ✓ ✗ 6.23 (↑12.6%)4.55 (↑25.0%)0.49 (↑6.5%) 0.19 (↑29.6%)5.79 (↑5.1%) 4.97 (↑5.5%) 0.53 (↑3.6%) 0.15 (↓28.6%)ETPNav An et al. (2024) ✓ ✗ 6.14 (↑18.8%)6.07 (↑49.1%)0.56 (↑30.2%)0.17 (↑29.2%)6.38 (↑17.5%)7.44 (↑7.2%) 0.65 (↑12.1%)0.13 (↓23.5%)✗ ✓ 4.92 (↑4.8%) 5.45 (↑33.9%)0.55 (↑27.9%)0.21 (↓12.5%)5.94 (↑9.4%) 7.23 (↑4.2%) 0.65 (↑12.1%)0.16 (↓5.9%)</p>
<p>Savva et al. (2019)provides high-performance simulation without extensive multi-human or social-compliance features.By contrast, our HA-VLN Simulator unifies dynamic human activities, photorealistic rendering, and social-compliance requirements.Agents perceive and react to evolving bystander behaviors-such as avoiding collisions or maintaining personal space-using both discrete and continuous navigation.Specifically, we introduce 675 scenes (across 90 scenarios), 122 motion types, and a cohesive framework that supports instruction-driven dynamic human interactions.By supporting both discrete and continuous action spaces, HA-VLN further broadens its potential for addressing diverse navigation goals and real-world deployment challenges.
2 SIMULATORS FOR VLN TASKSA reliable simulator is essential for developing and evaluating VLN agents. Early simulators likeMatterport3D Anderson et al. (2018) and House3D Wu et al. (2018) offered photorealistic or syn-thetic indoor environments but lacked mobile humans. Others, such as AI2-THOR Kolve et al.(2017) and Gibson Xia et al. (2018), introduced more interactive elements yet typically assumedstatic or purely synthetic contexts, thus limiting their applicability for studying social compliance.Google Street View, used in some outdoor navigation tasks, presents static imagery with occasionalhumans in the scene but lacks dynamic or interactive elements. HA3D Li et al. (2024) moved a stepfurther by including human activities and instructions referencing people, though it did not mandatesocially compliant navigation. HabiCrowd Vuong et al. (2024) integrated crowds into photorealis-tic domains, improving visual diversity but omitting human-aligned instructions. Similarly, Habi-tat 3.0</p>
<p>Table A1 :
A1
Comparison of VLN tasks, simulators, and agents based on (1) Socially Compliant Navigation, (2) Human-aligned Instructions and Visual Cues, and (3) Dynamic Environments with Human Activities.</p>
<p>Table A2 :
A2
Comparison of HAPS 1.0 vs. HAPS 2.0.We show the total number of motion categories, average accuracy and compatibility scores (both on a 1-10 scale), the number of failure cases (e.g., severe motiondescription mismatches), and total annotation time.HAPS 2.0 features more diverse motions, improved motionenv alignment, and reduced failures, albeit at higher annotation effort.
Annotation Time (hours)HAPS 1.0 Li et al. (2024)4356.35.9120320 (verified by Li et al. (2024))HAPS 2.0 (ours)4868.58.10430+
DatasetsMotions ↑ Accuracy (1-10) ↑ Compatibility (1-10) ↑ Failure Cases ↓</p>
<p>Coarse Annotation via PSO Require: Region R ← ⟨r, B lo , B hi ⟩, where r is region label and boundary coordinates B lo = (x lo , y lo , z lo ) and B hi = (x hi , y hi , z hi ); object list O ← {j 1 , j 2 , . . ., j n } with positions p ji ← (x ji , y ji , z ji ); human motion set H; minimum safe distance ϵ ← 1 m; height offset ∆ z ← 0.75 m.Ensure: Final positions p h ← (x h , y h , z h ) for each human motion h ∈ H.</p>
<p>1: while not all human motions placed do 2:</p>
<p>Table A4 :
A4
An et al. (2023)ze Combination on Navigation.In this experiment, we treat 1m step as four 0.25m steps, and 2.25m step as nine 0.25m steps.In this case, collisions are detected every 0.25m.We show results for BEVBertAn et al. (2023)on unseen validation.
Step Size NE↓ TCR↓ CR↓ SR↑1.006.8526.970.94 0.0042.258.79 112.78 0.97 0.000
AppendixThis supplementary material provides expanded details and results that complement the main paper.Section A offers a comprehensive literature survey focusing on three key research challenges.Section B describes our dataset construction, annotation protocols, real-time rendering methods, API design, and additional insights on annotation data.Section C presents an in-depth overview of the HA-R2R dataset and the proposed navigation agents.Finally, Section D includes detailed evaluation metrics, additional numerical results, visualized navigation outcomes, and real-world robot validation studies, each supplemented with thorough analysis.For further resources, access project page https://ha-vln-project.vercel.app/,and the datasets are provided at DATASETS.A RELATED WORKThis appendix surveys the evolution of Vision-and-Language Navigation (VLN) tasks, simulators, and agent designs, with particular attention to how Human-Aware VLN (HA-VLN) 2.0 advances the state of the art.We focus on three key aspects deemed critical for bridging the Sim2Real gap:(1) Socially Compliant Navigation, (2) Human-Aligned Instructions and Visual Cues, and (3) Dynamic Environments with Human Activities and Interactions.TableA1summarizes how prior work compares under these dimensions.A.1 DEVELOPMENT OF VLN TASKSEarly VLN tasks focused on basic indoor navigation-exemplified by Room-to-Room (R2R)Anderson et al. (2018);Fried et al. (2018);Gu et al. (2022);Ku et al. (2020)-and outdoor tasks likeTOUCHDOWN Chen et al. (2019)andMARCO MacMahon et al. (2006).Later efforts such asREVERIE Qi et al. (2020)andVNLA Nguyen et al. (2019)introduced object-centric or goaldriven navigation.While these approaches expanded the range of tasks, they typically overlooked real human behavior and social contexts.Dialogue-based tasks (e.g., DialFREDGao et al. (2022),CVDN Thomason et al. (2020)) incorporated interactive elements but did not account for dynamically moving bystanders or social-distance constraints.Initiatives like VLN-CEKrantz et al. (2020)moved closer to real-world conditions by enabling continuous navigation, yet remained devoid of explicit human factorsJain et al. (2019);Ku et al. (2020);Nguyen et al. (2019);Thomason et al. (2020).HA3D Li et al. (2024)addressed human motion and included human-oriented instructions but did not require agents to conform to social norms-e.g., maintaining safe distances or refraining from disturbing ongoing activities.Our proposed HA-VLN 2.0 addresses these gaps by embedding all three essential elements, socially compliant navigation, human-referenced instructions, and dynamic human activities, into a single framework.Agents must plan routes among unpredictable Continuous Environment (CE).Our API in continuous mode mainly focuses on three components:(1) Human Activity Monitoring, (2) Environmental Perception, and (3) Navigation Support.Detection resultsCounting resultsFigureA5: The visualization of Human Counting.(1) Human Activity Monitoring.We track and analyze human activity in real time as in Sec. 3. When collisions occur, the agent reverts to its prior position, and we identify whether the obstacle is human or an inanimate object.For human collisions, we log the coordinates and motion state to inform potential reward-shaping strategies.Distance and orientation estimates derive from agent-human coordinate data.For instance, we employ the Grounding-DINO Liu et al. (2024)detector on RGB inputs with the prompt "human" to count individuals.Fig.A5illustrates how human detection bounding boxes enable real-time counting.(2) Environmental Perception.We maintain a dynamic scene graph comprising static elements (e.g., buildings, furniture) and moving humans.The agent continuously updates this graph by fusing positional changes and human motion data in its vicinity.This ensures real-time awareness of human activities for downstream decisions.Preprint.where W rgb , W depth , and W merge are learnable projection matrices with ReLU activation.The directional encoding d i is constructed by repeating (cos θ i t , sin θ i t ) 32 times, where θ i t measures the relative heading offset of the agent.The fused embedding v i ∈ R d is either 512 or 768 dimensions, matching the requirements of our HA-VLN-CMA or HA-VLN-VL agent, respectively.Both ResNet backbones remain fixed during training, ensuring consistent and stable representations from the RGB and depth channels throughout the learning process.C.5 TEXT EMBEDDINGSFor the HA-VLN-VL agent, we utilize text embeddings fromPREVALENT Hao et al. (2020), which was pre-trained on 6.58M image-text-action triplets, thereby capturing broad contextual cues for navigation.Conversely, the HA-VLN-CMA agent adopts embeddings fromBERT Devlin (2018), also widely used for its strong language representations.Formally, let ℓ = {w 1 , . . ., w n } be a sequence of tokens representing the instruction.Each token w i is mapped to a one-hot vector e i ∈ R V , where V is the vocabulary size.An embedding matrix E ∈ R V ×d then projects e i into a continuous d-dimensional space:In this manner, each discrete token w i is transformed into a trainable embedding x i , forming the foundation of the model's linguistic understanding.token is updated by appending agent's previously executed action a t and projecting resulting vector:A5) where s ′ t is the final Transformer-layer output, and W s is a learnable projection matrix.Visual Attention.To decide the next action, we compute attention scores between s t and the set of visual tokens V t = {v 1 , v 2 , . . ., v n }, covering navigable directions plus a "stop" option:where Q s is derived from s t and K v from v i ∈ V t .The model then aggregates these attention scores via an average-pooling step:A7) yielding an action distribution over possible moves.The agent selects:Training Objective.HA-VLN-VL is optimized through a combination of supervised imitation learning-to mimic ground-truth trajectories-and optional reinforcement learning, which rewards safe and efficient paths.As depicted in Fig.A7(a), the model continuously refines its understanding of language instructions and visual cues, offering robust and socially aware navigation.C.7 HA-VLN-CMA STRUCTUREArchitecture Overview.HA-VLN-CMA is a dual-stream visual-language agent featuring Cross-Modal Attention (CMA) and a recurrent decoder for navigation in human-populated scenarios (see Fig.A7(b)).It processes two visual channels-RGB and Depth-alongside language instructions, then outputs an action at each time step.Dual-Stream Visual Encoding.Following Sec.C.4, each observation o t is split into:where ResNet rgb and ResNet depth are separate backbones for RGB and Depth, respectively.The fused feature representation isA10) where W rgb , W depth , and W merge are projection matrices, and d i is a direction encoding (Sec.C.4).Language Encoder.Textual instructions {w 1 , . . ., w T } are transformed into contextual embeddings l = BERT(w 1 , . . ., w T ).(A11) These embeddings capture the semantic structure of the instruction and serve as input to the crossmodal module.Cross-Modal Attention &amp; Recurrent Decoding.At time step t, we attend to the language features using multi-head attention:whereMulti-head attention helps handle lengthy and detailed instructions by learning multiple representations in parallel.Next, we combine the resulting multimodal embeddings with the previous action a t−1 in a GRUbased decoder:A13) where f t−1 is the previous hidden state.Finally, an MLP outputs the action distribution:A14) where MLP(f t ) = W a f t + b a , and a t is sampled from P (a t |f t ).Training Objectives.HA-VLN-CMA is trained end-to-end with a mixture of imitation learning (to mimic ground-truth paths) and reinforcement learning (to encourage collision-free, socially compliant navigation).By learning from both paradigms, the agent refines its ability to balance path efficiency and safe distancing in human-populated environments.Preprint.Time StepsRobot ViewThirid-Person View Move forward and make a right turn toward the sofa.After reaching the sofa, proceed to the dining table and come to a stop.Move forward and make a right turn toward the sofa.After reaching the sofa, proceed to the dining table and come to a stop.You might encounter a person seated at the table who may then stand up and walk toward another room.Please avoid them.0.42 0.17 0.43 0.17 0.49 0.20 0.43 0.17 0.44 0.18D.5 VALIDATION ON REAL-WORLD ROBOTSTo deploy our navigation agents on physical hardware, the robot is equipped with an NVIDIA Jetson NX for AI inference and a Raspberry Pi 4B for motion control.The Jetson handles core navigation computations (receiving camera images and inferring action commands), while the Pi executes high-level movement directives such as turn left or move forward.We set a minimum step size of 0.25 m and a rotation increment of 15 degrees.An onboard IMU continuously monitors the robot's orientation and position, ensuring movement commands align with issued directives.Preprint.Setup.Our evaluations use a Unitree GO2-EDU quadruped, featuring the Intel Realsense D435i camera providing RGB imagery and a 3D LiDAR below camera for detection.IMU refines positional and orientational control, enabling consistent motions.The quadruped rotates to get the panoramic view at each step.We evaluate our agents in four types of everyday indoor environments (each with three instances)-office, living room, hallway, and lobby-under two conditions: (i) w/o human presence (no bystanders) and (ii) w/ human presence (2-4 free-moving volunteers).This setup simulates realistic indoor traffic patterns and partial observability.Observations.As illustrated in Fig.6(b), the robot frequently pauses or yields to avoid oncoming pedestrians.In the absence of bystanders, it navigates smoothly (Fig.A9), but collisions arise in cramped corridors or when crowds converge suddenly (Fig.A10).We observe similar patterns in living-room environments (Figs.A11-A12) and hallways (Fig.A13).TableA5shows the average NSR (Navigation Success Rate) across 30 trials in each instance.While human presence invariably lowers NSR, HA-VLN-VL consistently outperforms HA-VLN-CMA-Base, demonstrating stronger adaptability to dynamic motion.Also, TableA5shows agents trained on HA-VLN achieve higher NSR (0.18 vs. 0.12) than VLN-CE, demonstrating HA-R2R's sim-toreal gain under realistic conditions.Still, partial observability and abrupt group formations remain
Multimodal map pre-training for language-guided navigation. Dong An, Yuankai Qi, Yangguang Li, Yan Huang, Liang Wang, Tieniu Tan, Jing Shao, Bevbert, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Etpnav: Evolving topological planning for vision-language navigation in continuous environments. Dong An, Hanqing Wang, Wenguan Wang, Zun Wang, Yan Huang, Keji He, Liang Wang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2018</p>
<p>Sim-to-real transfer for vision-and-language navigation. Peter Anderson, Ayush Shrivastava, Joanne Truong, Arjun Majumdar, Devi Parikh, Dhruv Batra, Stefan Lee, Conference on Robot Learning. 2021</p>
<p>Mapping navigation instructions to continuous control actions with position-visitation prediction. Valts Blukis, Dipendra Misra, Ross A Knepper, Yoav Artzi, Conference on Robot Learning. 2018</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems. 202033</p>
<p>Touchdown: Natural language navigation and spatial reasoning in visual street environments. Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, Yoav Artzi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>History aware multimodal transformer for vision-and-language navigation. Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, Ivan Laptev, Advances in Neural Information Processing Systems. 202134</p>
<p>Shield: Llm-driven schema induction for predictive analytics in ev battery supply chain disruptions. Zhi-Qi Cheng, Yifei Dong, Aike Shi, Wei Liu, Yuzhi Hu, Jason O' Connor, Alexander G Hauptmann, Kate S Whitefoot, arXiv:2408.053572024arXiv preprint</p>
<p>Embodied question answering. Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2018</p>
<p>Jacob Devlin, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 2018arXiv preprint</p>
<p>Data augmentation using llms: Data perspectives, learning paradigms and challenges. Bosheng Preprint, Chengwei Ding, Ruochen Qin, Tianze Zhao, Xinze Luo, Guizhen Li, Wenhan Chen, Junjie Xia, Anh Tuan Hu, Shafiq Luu, Joty, arXiv:2403.029902024arXiv preprint</p>
<p>Securing the skies: A comprehensive survey on anti-uav methods, benchmarking, and future directions. Yifei Dong, Fengyi Wu, Sanjian Zhang, Guangyu Chen, Yuzhi Hu, Masumi Yano, Jingdong Sun, Siyu Huang, Feng Liu, Qi Dai, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Speaker-follower models for vision-and-language navigation. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell, Advances in Neural Information Processing Systems. 201831</p>
<p>Vision-language navigation with embodied intelligence: A survey. Peng Gao, Peng Wang, Feng Gao, Fei Wang, Ruyue Yuan, arXiv:2402.143042024arXiv preprint</p>
<p>Dialfred: Dialogue-enabled agents for embodied instruction following. Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, Govind Thattai, Gaurav S Sukhatme, IEEE Robotics and Automation Letters. 742022</p>
<p>Iqa: Visual question answering in interactive environments. Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2018</p>
<p>Vision-and-language navigation: A survey of tasks, methods, and future directions. Jing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, Xin Wang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Airbert: In-domain pretraining for vision-and-language navigation. Pierre-Louis Guhur, Makarand Tapaswi, Shizhe Chen, Ivan Laptev, Cordelia Schmid, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Tevet Guy, Raab Sigal, Gordon Brian, Shafir Yonatan, Cohen-Or Daniel, H Bermano Amit, arXiv:2209.14916Mdm: Human motion diffusion model. 2022arXiv preprint</p>
<p>Towards learning a generic agent for vision-and-language navigation via pre-training. Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Language and visual entity relationship graph for agent navigation. Yicong Hong, Cristian Rodriguez, Yuankai Qi, Qi Wu, Stephen Gould, Advances in Neural Information Processing Systems. 202033</p>
<p>A recurrent vision-and-language bert for navigation. Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJune 2021</p>
<p>Stay on the path: Instruction fidelity in vision-and-language navigation. Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, Jason Baldridge, arXiv:1905.122552019arXiv preprint</p>
<p>A large-scale rgb-d database for arbitrary-view human action recognition. Yanli Ji, Feixiang Xu, Yang Yang, Fumin Shen, Heng Tao Shen, Wei-Shi Zheng, Proceedings of the 26th ACM international Conference on Multimedia. the 26th ACM international Conference on Multimedia2018</p>
<p>Sim2real predictivity: Does evaluation in simulation predict real-world performance?. Abhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia Chernova, Dhruv Batra, IEEE Robotics and Automation Letters. 542020</p>
<p>Vizdoom: A doom-based ai research platform for visual reinforcement learning. Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, Wojciech Jaśkowski, 2016 IEEE Conference on Computational Intelligence and Games. 2016</p>
<p>Particle swarm optimization. Preprint , James Kennedy, Russell Eberhart, Proceedings of ICNN'95-International Conference on Neural Networks. ICNN'95-International Conference on Neural Networks19954</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, arXiv:1712.05474Ai2-thor: An interactive 3d environment for visual ai. 2017arXiv preprint</p>
<p>Actor-critic algorithms. Vijay Konda, John Tsitsiklis, Advances in Neural Information Processing Systems. S Solla, T Leen, K Müller, MIT Press199912</p>
<p>Controllable navigation instruction generation with chain of thought prompting. Xianghao Kong, Jinyu Chen, Wenguan Wang, Hang Su, Xiaolin Hu, Yi Yang, Si Liu, European Conference on Computer Vision. Springer2024</p>
<p>Beyond the navgraph: Vision-and-language navigation in continuous environments. Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, Stefan Lee, European Conference on Computer Vision. 2020</p>
<p>Waypoint models for instruction-guided navigation in continuous environments. Jacob Krantz, Aaron Gokaslan, Dhruv Batra, Stefan Lee, Oleksandr Maksymets, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, Jason Baldridge, arXiv:2010.079542020arXiv preprint</p>
<p>Envedit: Environment editing for vision-and-language navigation. Jialu Li, Hao Tan, Mohit Bansal, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Human-aware vision-and-language navigation: Bridging simulation to reality with dynamic human interactions. Minghan Li, Heng Li, Zhi-Qi Cheng, Yifei Dong, Yuxuan Zhou, Jun-Yan He, Qi Dai, Teruko Mitamura, Alexander G Hauptmann, arXiv:2406.192362024arXiv preprint</p>
<p>Oscar: Object-semantics aligned pretraining for vision-language tasks. Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao, European Conference on Computer Vision. 2020</p>
<p>Navcot: Boosting llm-based vision-and-language navigation via learning disentangled reasoning. Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2025</p>
<p>Learning vision-and-language navigation from youtube videos. Kunyang Lin, Peihao Chen, Diwei Huang, Thomas H Li, Mingkui Tan, Chuang Gan, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, European Conference on Computer Vision. 2024</p>
<p>Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, Advances in Neural Information Processing Systems. 201932</p>
<p>Self-monitoring navigation agent via auxiliary progress estimation. Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Alregib, Zsolt Kira, Richard Socher, Caiming Xiong, arXiv:1901.030352019arXiv preprint</p>
<p>Walk the talk: Connecting language, knowledge, and action in route instructions. Matt Macmahon, Brian Stankiewicz, Benjamin Kuipers, Def. 262006</p>
<p>Vision-based navigation with language-based assistance via imitation learning with indirect intervention. Khanh Nguyen, Debadeepta Dey, Chris Brockett, Bill Dolan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Challenges of real-world reinforcement learning:definitions, benchmarks &amp; analysis. Cosmin Paduraru, Daniel J Mankowitz, Gabriel Dulac-Arnold, Jerry Li, Nir Levine, Sven Gowal, Todd Hester, Machine Learning Journal. 2021</p>
<p>Action-conditioned 3d human motion synthesis with transformer vae. Mathis Petrovich, Michael J Black, Gül Varol, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, arXiv:2310.13724So Yeon Min, et al. Habitat 3.0: A co-habitat for humans, avatars and robots. 2023arXiv preprint</p>
<p>Reverie: Remote embodied visual referring expression in real indoor environments. Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, Anton Van Den, Hengel, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. Stéphane Ross, Geoffrey Gordon, Drew Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statistics2011</p>
<p>Habitat: A platform for embodied ai research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Learning to navigate unseen environments: Back translation with environmental dropout. Licheng Hao Tan, Mohit Yu, Bansal, arXiv:1904.041952019arXiv preprint</p>
<p>Vision-and-dialog navigation. Jesse Thomason, Michael Murray, Maya Cakmak, Luke Zettlemoyer, Conference on Robot Learning. 2020</p>
<p>Habicrowd: A high performance simulator for crowd-aware visual navigation. An Vuong, Toan Nguyen, Minh Nhat Vu, Baoru Huang, Thieu Binh, Anh Vo, Nguyen, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems. 2024</p>
<p>Towards versatile embodied navigation. Hanqing Wang, Wei Liang, Luc Van Gool, Wenguan Wang, 2022</p>
<p>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang, Wang , Lei Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Scaling data generation in vision-and-language navigation. Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, Yu Qiao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Govig: Goal-conditioned visual navigation instruction generation. Fengyi Wu, Yifei Dong, Zhi-Qi Cheng, Yilong Dai, Guangyu Chen, Hang Wang, Qi Dai, Alexander G Hauptmann, arXiv:2508.095472025arXiv preprint</p>
<p>Vision-language navigation: a survey and taxonomy. Wansen Wu, Tao Chang, Xinmeng Li, Quanjun Yin, Yue Hu, Neural Computing and Applications. 3672024</p>
<p>Building generalizable agents with a realistic and rich 3d environment. Yi Wu, Yuxin Wu, Georgia Gkioxari, Yuandong Tian, arXiv:1801.022092018arXiv preprint</p>
<p>Gibson env: Real-world perception for embodied agents. Fei Preprint, Xia, Zhiyang Amir R Zamir, Alexander He, Jitendra Sax, Silvio Malik, Savarese, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2018</p>
<p>Albert Yu, Adeline Foote, Raymond Mooney, Roberto Martín-Martín, arXiv:2405.10020Natural language can help bridge the sim2real gap. 2024arXiv preprint</p>
<p>Navid: Video-based vlm plans the next step for vision-andlanguage navigation. Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, Wang He, arXiv:2402.158522024aarXiv preprint</p>
<p>Vision-and-language navigation today and tomorrow: A survey in the era of foundation models. Yue Zhang, Ziqiao Ma, Jialu Li, Yanyuan Qiao, Zun Wang, Joyce Chai, Qi Wu, Mohit Bansal, Parisa Kordjamshidi, arXiv:2407.070352024barXiv preprint</p>
<p>Navgpt: Explicit reasoning in vision-and-language navigation with large language models. Gengze Zhou, Yicong Hong, Qi Wu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Vision-language navigation with selfsupervised auxiliary reasoning tasks. Fengda Zhu, Yi Zhu, Xiaojun Chang, Xiaodan Liang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Algorithm A2 Real-time Human Rendering. Preprint, Simulation Require: Simulation environment E; Human motion data H; Signal queue Q with maximum size M ← 120; Total frames N ← 120; Frame interval ∆t. Ensure: Continuous real-time rendering of H within E. 1: Initialize simulator E, object template manager T in E, human motion data H and signal queue Q. </p>
<p>Initialize total signals sent and processed to 0. </p>
<p>Thread 1: Signal sender thread 4: while true do 5: if not Q.full() then 6: Enqueue signal "REFRESH HUMAN. into Q</p>
<p>In our discrete setting, all agent and human positions are tracked via a real-time navigational graph displayed in a 2D top-down view. Each human's activity is stored as a tuple ⟨p h , d agent , θ relative , a status ⟩, where p h is the human's 2D coordinate, d agent is the distance to the agent, θ relative is the relative orientation, and a status indicates activity state. This representation supports efficient, simultaneous tracking of multiple humans in a discrete viewpoint space. Multi-Entity Detection &amp; Tracking. We employ object detection on each discrete panorama to identify humans, assigning unique IDs for continuous monitoring throughout the navigation process. By linking recognized human poses to specific graph nodes, we anchor their activities to well-defined spatial references. User Interface. A specialized UI presents a bird's-eye view of the 2D graph, allowing researchers to visualize, annotate, and adjust human behaviors in real time. This interface significantly streamlines data annotation and analysis for discrete human-aware navigation research. ChatGPT-4o and LLaMA-3-8B-Instruct toward generating instructions that fully integrate humancentric elements-such as bystander activities, relevant spatial cues, and subtle behavioral recommendations. The final HA-R2R instructions thus reflect enriched scene descriptions where agents must account for diverse, real-world nuances involving human presence. C.3 HA-R2R DATA ANALYSIS Word Frequency Analysis. We conduct a word frequency study on HA-R2R to gauge its capacity for representing realistic, human-centric scenarios. Figs. A6(a) and (b) illustrate frequently used nouns and verbs, confirming the dataset's focus on both spatial navigation and social interactions. Nouns. The five most common nouns are room, hallway, turn, area, and path, with room alone appearing over 15,000 times. Load template τ h into T ; 25: Add human o h to E using template τ h. 8Upon starting a new episode, Q is cleared, and signal counters reset, so human motions revert to frame 0, maintaining consistency across episodes. This real-time process keeps human activities synchronized with agent's action cycle, creating dynamic scenes where agents must adapt to changing bystander locations and behaviors. B.7 API DESIGN Discrete Environment (DE). Other notable terms (person, doorway, kitchen) highlight spatial complexity and social elements such as conversation, activities, and someone. Verbs. The five most frequent verbs-is, continue, proceed, ensuring, be-reveal an action-oriented narrative, while additional terms (engaged, observe, notice, avoid, maintain) underscore instructions geared toward social awareness and precise route-following</p>
<p>A6(c) shows that most instructions contain 20-60% human-related content, reflecting the dataset's emphasis on people in everyday scenes. Comparisons of word clouds in Figs. A6(d) and (e) confirm that while both human-aligned and nonhuman segments use common navigational verbs (walk, left, right), instructions involving humans introduce additional social context (couple, man, painting). Human Impact Analysis. Fig. This integration of interpersonal cues</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8070 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8070</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8070</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-62f441d5078bf77927c370364367c20f4e0010e6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/62f441d5078bf77927c370364367c20f4e0010e6" target="_blank">LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper presents a comprehensive survey of the LLMs-as-judges paradigm from five key perspectives: Functionality, Methodology, Applications, Meta-evaluation, and Limitations.</p>
                <p><strong>Paper Abstract:</strong> The rapid advancement of Large Language Models (LLMs) has driven their expanding application across various fields. One of the most promising applications is their role as evaluators based on natural language responses, referred to as ''LLMs-as-judges''. This framework has attracted growing attention from both academia and industry due to their excellent effectiveness, ability to generalize across tasks, and interpretability in the form of natural language. This paper presents a comprehensive survey of the LLMs-as-judges paradigm from five key perspectives: Functionality, Methodology, Applications, Meta-evaluation, and Limitations. We begin by providing a systematic definition of LLMs-as-Judges and introduce their functionality (Why use LLM judges?). Then we address methodology to construct an evaluation system with LLMs (How to use LLM judges?). Additionally, we investigate the potential domains for their application (Where to use LLM judges?) and discuss methods for evaluating them in various contexts (How to evaluate LLM judges?). Finally, we provide a detailed analysis of the limitations of LLM judges and discuss potential future directions. Through a structured and comprehensive analysis, we aim aims to provide insights on the development and application of LLMs-as-judges in both research and practice. We will continue to maintain the relevant resource list at https://github.com/CSHaitao/Awesome-LLMs-as-Judges.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8070.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8070.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Survey-summary</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs-as-Judges (survey-level comparison vs humans)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-level summary in this survey contrasting LLM-based evaluators with human evaluation, noting advantages (scalability, interpretability, flexibility) and multiple failure modes and limitations (prompt sensitivity, bias, inconsistency, lack of deep reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>general / multiple downstream tasks (NLG, summarization, annotation, review, reasoning, RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>various LLMs (survey-level)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>survey discusses multiple LLMs (GPT-4, ChatGPT, GPT-4V, Llama variants, etc.) and both single-LLM and multi-LLM configurations; specifics vary by cited work</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>experts or crowdworkers (general; described as human annotations / crowdworkers / domain experts depending on task)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>sensitivity to prompt templates; inherited dataset biases; inconsistent judgments (non-transitivity between pairwise/pointwise); lower reliability on tasks needing deep logical reasoning or domain expertise; self-evaluation failures without external validators</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LLM judges are scalable, adaptable, and provide interpretable textual explanations, but their outputs can be influenced by prompt design, suffer from biases learned from training corpora, produce inconsistent rankings/violations of transitivity, and are less reliable than humans on deep-reasoning or knowledge-intensive evaluations; hybrid human-AI setups can mitigate some limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>scalability; reproducibility; ability to adapt criteria per context; interpretability via natural-language explanations; lower cost and faster than manual human annotation at scale</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>survey synthesizes many experimental settings; common patterns include pointwise/pairwise/listwise prompts, use of explanations+higher-order criteria, single vs multi-LLM ensembles, and human-AI hybrid pipelines; no single fixed protocol</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8070.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8070.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>He-85-GPT4-vs-MTurk</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>He et al. — GPT-4 compared to MTurk crowdworkers on text annotation accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey cites He et al. [85], reporting a head-to-head on annotation accuracy where GPT-4 achieved a higher accuracy than the best-performing MTurk workers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>text annotation / crowdsourced annotation workflows</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4 applied as annotator in crowdsourced workflows (details in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>MTurk crowdworkers</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (annotation correctness relative to gold labels)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>83.6</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4 achieved 83.6% annotation accuracy reported vs a highest MTurk accuracy of 81.5%, suggesting GPT-4 can match or exceed typical crowdworker accuracy in some annotation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>higher annotation accuracy in the cited study; reduced human labor and cost</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>crowdsourced annotation workflow comparison; reported best MTurk worker accuracy (81.5%) vs GPT-4 (83.6%); further experimental details not provided in survey</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8070.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8070.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gilardi-71-ChatGPT-vs-humans</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gilardi et al. — ChatGPT vs crowdsourced workers on tweet/news classification tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey cites Gilardi et al. [71], who found ChatGPT outperformed crowdsourced workers on stance detection, topic detection, and framing across a set of tweets and news articles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>stance detection, topic detection, framing classification on tweets and news articles</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>6,183 tweets and news articles (as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>ChatGPT used as automatic classifier/annotator in the cited experiments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>crowdsourced workers</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT outperformed crowdsourced workers on the examined classification tasks; no numerical agreement metric is reported in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>better classification performance (as reported) and likely greater annotation speed and scalability</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>annotation/classification of 6,183 tweets/news items; comparison against crowdworkers; precise scoring metrics not provided in survey</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8070.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8070.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tornberg-217-ChatGPT4-classifiers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Törnberg et al. — ChatGPT-4 vs human classifiers on political-leaning classification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey cites Törnberg et al. [217], reporting ChatGPT-4 surpassed human classifiers in accuracy/reliability and exhibited bias levels comparable to or lower than humans for user political-leaning classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>classification of Twitter users' political leanings</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>ChatGPT-4 applied as classifier for political-leaning from tweet content</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>human classifiers (unspecified; likely crowdsourced or expert annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT-4 surpassed human classifiers in accuracy and reliability in this task and had bias levels comparable to or lower than human classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>higher accuracy/reliability and no worse (possibly lower) bias levels compared to humans</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>classification of Twitter users by political leaning based on tweet content; specifics not provided in survey</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8070.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8070.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zhou-297-paper-review-eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zhou et al. — evaluation of LLMs as automated paper reviewers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey cites Zhou et al. [297], who examined whether LLMs can reliably perform automated paper review and found current LLMs are insufficiently reliable, especially for tasks requiring logical reasoning or deep domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>automated paper review / academic peer-review simulation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>various LLMs (cited work evaluated current LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Not specified in survey; evaluation focused on reviewer-style judgments</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>human peer reviewers or domain experts (implied as ground-truth reference)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>insufficient reliability on logical reasoning tasks; lack of deep knowledge for paper-review judgments; overall unverifiable assessments in some cases</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LLMs are not yet reliable substitutes for human reviewers in paper review contexts, particularly when deep domain expertise or rigorous logical analysis is required.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>potential to scale review steps but currently limited by reasoning and knowledge-depth deficits</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>assessment of LLMs' suitability for automated paper review; details in cited work</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8070.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8070.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chiang-36-rate-explain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chiang et al. — effect of combining ratings with explanations on correlation with humans</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey cites Chiang et al. [36], showing that combining rating with explanation (rate-explain) or explanation-then-rate (explain-rate) increases correlation between LLM judgments and human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>improving alignment of LLM ratings with human ratings across evaluation tasks (general NLG evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>LLMs used with rate+explain prompting</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Prompting strategy (rate+explain) rather than a single specific LLM</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>human raters (used as reference for correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>correlation with human ratings (e.g., Pearson/Spearman implied)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Providing explanations alongside ratings improves the correlation between LLM-generated ratings and human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>higher human-alignment when asked to produce explanations along with ratings</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>prompting strategies that require LLMs to produce explanations and/or ratings (rate-explain or explain-rate); numerical correlation improvements not reported in survey</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8070.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8070.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Daynauth-45-recalibration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Daynauth et al. — calibration between automated evaluators and human preferences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey cites Daynauth et al. [45], who quantify discrepancies between human preferences and automated evaluations (bias toward higher token counts) and propose a recalibration procedure for GPT-based scorers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>language-model evaluation (general scoring of generated text)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-based scorers (GPTScorers)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-based scoring models whose raw outputs show bias toward longer token counts</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>human preference annotations (used to quantify discrepancy)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>statistical discrepancy; t-test and Bayesian statistics used to quantify bias</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>bias toward higher token counts (over-rewarding longer outputs); systematic discrepancy between automated scores and human preferences</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Automated GPT scorers can be biased by superficial features (e.g., length) relative to human judgments; statistical recalibration can reduce such biases.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>once recalibrated, automated scoring can be more reliable and reproducible</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>analysis using Bayesian statistics and t-tests to measure and then recalibrate GPT-based scorers; specific datasets/protocols in cited work</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8070.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8070.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BiasAlert-61-tool</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BiasAlert — external knowledge tool for bias detection outperforming GPT-4-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey cites BiasAlert [61], a tool that integrates external human knowledge with LLM reasoning to detect social bias in open-text outputs and reportedly outperforms GPT-4-as-a-judge in various scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>social bias detection in LLM-generated text</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>BiasAlert system (LLM + external knowledge) contrasted with GPT-4-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>BiasAlert integrates external human knowledge sources with LLM reasoning; compared against GPT-4 used alone as judge</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>not specified (human knowledge integrated into BiasAlert)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>GPT-4-as-judge underperforms compared to hybrid tool in bias detection scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Augmenting LLM judgment with external human knowledge can yield better bias-detection performance than using GPT-4 alone.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>LLM-alone is powerful, but hybrid systems that incorporate curated external knowledge can outperform single-LLM judges on sensitive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>comparative evaluation across multiple bias-detection scenarios; precise metrics not provided in survey</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8070.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8070.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sottana-206-avg-GPT4-human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sottana et al. — multi-round scoring and averaging human and GPT-4 rankings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Survey cites Sottana et al. [206], who apply a multi-round scoring process and average human and GPT-4 rankings to mitigate subjectivity in evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>multi-round evaluation / ranking of model outputs</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4 (used alongside human raters)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4 used to rank outputs across multiple rounds, combined with human rankings</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>human raters (used to produce rankings averaged with GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>subjectivity in single-round evaluations by humans or GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Averaging rankings from humans and GPT-4 across rounds can reduce subjectivity and produce more stable evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>automated consistency across rounds; when combined with humans can produce more robust rankings</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>multi-round scoring; human and GPT-4 evaluations averaged to mitigate subjectivity; details in cited work</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8070",
    "paper_id": "paper-62f441d5078bf77927c370364367c20f4e0010e6",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "Survey-summary",
            "name_full": "LLMs-as-Judges (survey-level comparison vs humans)",
            "brief_description": "High-level summary in this survey contrasting LLM-based evaluators with human evaluation, noting advantages (scalability, interpretability, flexibility) and multiple failure modes and limitations (prompt sensitivity, bias, inconsistency, lack of deep reasoning).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
            "evaluation_task": "general / multiple downstream tasks (NLG, summarization, annotation, review, reasoning, RAG)",
            "dataset_name": null,
            "judge_model_name": "various LLMs (survey-level)",
            "judge_model_details": "survey discusses multiple LLMs (GPT-4, ChatGPT, GPT-4V, Llama variants, etc.) and both single-LLM and multi-LLM configurations; specifics vary by cited work",
            "human_evaluator_type": "experts or crowdworkers (general; described as human annotations / crowdworkers / domain experts depending on task)",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "sensitivity to prompt templates; inherited dataset biases; inconsistent judgments (non-transitivity between pairwise/pointwise); lower reliability on tasks needing deep logical reasoning or domain expertise; self-evaluation failures without external validators",
            "qualitative_findings": "LLM judges are scalable, adaptable, and provide interpretable textual explanations, but their outputs can be influenced by prompt design, suffer from biases learned from training corpora, produce inconsistent rankings/violations of transitivity, and are less reliable than humans on deep-reasoning or knowledge-intensive evaluations; hybrid human-AI setups can mitigate some limitations.",
            "advantages_of_llm_judge": "scalability; reproducibility; ability to adapt criteria per context; interpretability via natural-language explanations; lower cost and faster than manual human annotation at scale",
            "experimental_setting": "survey synthesizes many experimental settings; common patterns include pointwise/pairwise/listwise prompts, use of explanations+higher-order criteria, single vs multi-LLM ensembles, and human-AI hybrid pipelines; no single fixed protocol",
            "uuid": "e8070.0",
            "source_info": {
                "paper_title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "He-85-GPT4-vs-MTurk",
            "name_full": "He et al. — GPT-4 compared to MTurk crowdworkers on text annotation accuracy",
            "brief_description": "Survey cites He et al. [85], reporting a head-to-head on annotation accuracy where GPT-4 achieved a higher accuracy than the best-performing MTurk workers.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "evaluation_task": "text annotation / crowdsourced annotation workflows",
            "dataset_name": null,
            "judge_model_name": "GPT-4",
            "judge_model_details": "GPT-4 applied as annotator in crowdsourced workflows (details in cited work)",
            "human_evaluator_type": "MTurk crowdworkers",
            "agreement_metric": "accuracy (annotation correctness relative to gold labels)",
            "agreement_score": 83.6,
            "reported_loss_aspects": "",
            "qualitative_findings": "GPT-4 achieved 83.6% annotation accuracy reported vs a highest MTurk accuracy of 81.5%, suggesting GPT-4 can match or exceed typical crowdworker accuracy in some annotation tasks.",
            "advantages_of_llm_judge": "higher annotation accuracy in the cited study; reduced human labor and cost",
            "experimental_setting": "crowdsourced annotation workflow comparison; reported best MTurk worker accuracy (81.5%) vs GPT-4 (83.6%); further experimental details not provided in survey",
            "uuid": "e8070.1",
            "source_info": {
                "paper_title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Gilardi-71-ChatGPT-vs-humans",
            "name_full": "Gilardi et al. — ChatGPT vs crowdsourced workers on tweet/news classification tasks",
            "brief_description": "Survey cites Gilardi et al. [71], who found ChatGPT outperformed crowdsourced workers on stance detection, topic detection, and framing across a set of tweets and news articles.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "evaluation_task": "stance detection, topic detection, framing classification on tweets and news articles",
            "dataset_name": "6,183 tweets and news articles (as reported)",
            "judge_model_name": "ChatGPT",
            "judge_model_details": "ChatGPT used as automatic classifier/annotator in the cited experiments",
            "human_evaluator_type": "crowdsourced workers",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "",
            "qualitative_findings": "ChatGPT outperformed crowdsourced workers on the examined classification tasks; no numerical agreement metric is reported in the survey summary.",
            "advantages_of_llm_judge": "better classification performance (as reported) and likely greater annotation speed and scalability",
            "experimental_setting": "annotation/classification of 6,183 tweets/news items; comparison against crowdworkers; precise scoring metrics not provided in survey",
            "uuid": "e8070.2",
            "source_info": {
                "paper_title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Tornberg-217-ChatGPT4-classifiers",
            "name_full": "Törnberg et al. — ChatGPT-4 vs human classifiers on political-leaning classification",
            "brief_description": "Survey cites Törnberg et al. [217], reporting ChatGPT-4 surpassed human classifiers in accuracy/reliability and exhibited bias levels comparable to or lower than humans for user political-leaning classification.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "evaluation_task": "classification of Twitter users' political leanings",
            "dataset_name": null,
            "judge_model_name": "ChatGPT-4",
            "judge_model_details": "ChatGPT-4 applied as classifier for political-leaning from tweet content",
            "human_evaluator_type": "human classifiers (unspecified; likely crowdsourced or expert annotators)",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "",
            "qualitative_findings": "ChatGPT-4 surpassed human classifiers in accuracy and reliability in this task and had bias levels comparable to or lower than human classifiers.",
            "advantages_of_llm_judge": "higher accuracy/reliability and no worse (possibly lower) bias levels compared to humans",
            "experimental_setting": "classification of Twitter users by political leaning based on tweet content; specifics not provided in survey",
            "uuid": "e8070.3",
            "source_info": {
                "paper_title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Zhou-297-paper-review-eval",
            "name_full": "Zhou et al. — evaluation of LLMs as automated paper reviewers",
            "brief_description": "Survey cites Zhou et al. [297], who examined whether LLMs can reliably perform automated paper review and found current LLMs are insufficiently reliable, especially for tasks requiring logical reasoning or deep domain knowledge.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "evaluation_task": "automated paper review / academic peer-review simulation",
            "dataset_name": null,
            "judge_model_name": "various LLMs (cited work evaluated current LLMs)",
            "judge_model_details": "Not specified in survey; evaluation focused on reviewer-style judgments",
            "human_evaluator_type": "human peer reviewers or domain experts (implied as ground-truth reference)",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "insufficient reliability on logical reasoning tasks; lack of deep knowledge for paper-review judgments; overall unverifiable assessments in some cases",
            "qualitative_findings": "LLMs are not yet reliable substitutes for human reviewers in paper review contexts, particularly when deep domain expertise or rigorous logical analysis is required.",
            "advantages_of_llm_judge": "potential to scale review steps but currently limited by reasoning and knowledge-depth deficits",
            "experimental_setting": "assessment of LLMs' suitability for automated paper review; details in cited work",
            "uuid": "e8070.4",
            "source_info": {
                "paper_title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Chiang-36-rate-explain",
            "name_full": "Chiang et al. — effect of combining ratings with explanations on correlation with humans",
            "brief_description": "Survey cites Chiang et al. [36], showing that combining rating with explanation (rate-explain) or explanation-then-rate (explain-rate) increases correlation between LLM judgments and human ratings.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "evaluation_task": "improving alignment of LLM ratings with human ratings across evaluation tasks (general NLG evaluation)",
            "dataset_name": null,
            "judge_model_name": "LLMs used with rate+explain prompting",
            "judge_model_details": "Prompting strategy (rate+explain) rather than a single specific LLM",
            "human_evaluator_type": "human raters (used as reference for correlation)",
            "agreement_metric": "correlation with human ratings (e.g., Pearson/Spearman implied)",
            "agreement_score": null,
            "reported_loss_aspects": "",
            "qualitative_findings": "Providing explanations alongside ratings improves the correlation between LLM-generated ratings and human ratings.",
            "advantages_of_llm_judge": "higher human-alignment when asked to produce explanations along with ratings",
            "experimental_setting": "prompting strategies that require LLMs to produce explanations and/or ratings (rate-explain or explain-rate); numerical correlation improvements not reported in survey",
            "uuid": "e8070.5",
            "source_info": {
                "paper_title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Daynauth-45-recalibration",
            "name_full": "Daynauth et al. — calibration between automated evaluators and human preferences",
            "brief_description": "Survey cites Daynauth et al. [45], who quantify discrepancies between human preferences and automated evaluations (bias toward higher token counts) and propose a recalibration procedure for GPT-based scorers.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "evaluation_task": "language-model evaluation (general scoring of generated text)",
            "dataset_name": null,
            "judge_model_name": "GPT-based scorers (GPTScorers)",
            "judge_model_details": "GPT-based scoring models whose raw outputs show bias toward longer token counts",
            "human_evaluator_type": "human preference annotations (used to quantify discrepancy)",
            "agreement_metric": "statistical discrepancy; t-test and Bayesian statistics used to quantify bias",
            "agreement_score": null,
            "reported_loss_aspects": "bias toward higher token counts (over-rewarding longer outputs); systematic discrepancy between automated scores and human preferences",
            "qualitative_findings": "Automated GPT scorers can be biased by superficial features (e.g., length) relative to human judgments; statistical recalibration can reduce such biases.",
            "advantages_of_llm_judge": "once recalibrated, automated scoring can be more reliable and reproducible",
            "experimental_setting": "analysis using Bayesian statistics and t-tests to measure and then recalibrate GPT-based scorers; specific datasets/protocols in cited work",
            "uuid": "e8070.6",
            "source_info": {
                "paper_title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "BiasAlert-61-tool",
            "name_full": "BiasAlert — external knowledge tool for bias detection outperforming GPT-4-as-a-judge",
            "brief_description": "Survey cites BiasAlert [61], a tool that integrates external human knowledge with LLM reasoning to detect social bias in open-text outputs and reportedly outperforms GPT-4-as-a-judge in various scenarios.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "evaluation_task": "social bias detection in LLM-generated text",
            "dataset_name": null,
            "judge_model_name": "BiasAlert system (LLM + external knowledge) contrasted with GPT-4-as-a-judge",
            "judge_model_details": "BiasAlert integrates external human knowledge sources with LLM reasoning; compared against GPT-4 used alone as judge",
            "human_evaluator_type": "not specified (human knowledge integrated into BiasAlert)",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "GPT-4-as-judge underperforms compared to hybrid tool in bias detection scenarios",
            "qualitative_findings": "Augmenting LLM judgment with external human knowledge can yield better bias-detection performance than using GPT-4 alone.",
            "advantages_of_llm_judge": "LLM-alone is powerful, but hybrid systems that incorporate curated external knowledge can outperform single-LLM judges on sensitive tasks",
            "experimental_setting": "comparative evaluation across multiple bias-detection scenarios; precise metrics not provided in survey",
            "uuid": "e8070.7",
            "source_info": {
                "paper_title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Sottana-206-avg-GPT4-human",
            "name_full": "Sottana et al. — multi-round scoring and averaging human and GPT-4 rankings",
            "brief_description": "Survey cites Sottana et al. [206], who apply a multi-round scoring process and average human and GPT-4 rankings to mitigate subjectivity in evaluation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "",
            "evaluation_task": "multi-round evaluation / ranking of model outputs",
            "dataset_name": null,
            "judge_model_name": "GPT-4 (used alongside human raters)",
            "judge_model_details": "GPT-4 used to rank outputs across multiple rounds, combined with human rankings",
            "human_evaluator_type": "human raters (used to produce rankings averaged with GPT-4)",
            "agreement_metric": null,
            "agreement_score": null,
            "reported_loss_aspects": "subjectivity in single-round evaluations by humans or GPT-4",
            "qualitative_findings": "Averaging rankings from humans and GPT-4 across rounds can reduce subjectivity and produce more stable evaluations.",
            "advantages_of_llm_judge": "automated consistency across rounds; when combined with humans can produce more robust rankings",
            "experimental_setting": "multi-round scoring; human and GPT-4 evaluations averaged to mitigate subjectivity; details in cited work",
            "uuid": "e8070.8",
            "source_info": {
                "paper_title": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.015528750000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods</h1>
<p>HAITAO LI, Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University, China
QIAN DONG, Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University, China
JUNJIE CHEN, Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University, China
HUIXUE SU, Gaoling School of Artificial Intelligence, Renmin University of China, China
YUJIA ZHOU, Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University, China
QINGYAO AI, Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University, China
ZIYI YE, Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University, China
YIQUN LIU, Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University, China</p>
<p>The rapid advancement of Large Language Models (LLMs) has driven their expanding application across various fields. One of the most promising applications is their role as evaluators based on natural language responses, referred to as "LLMs-as-judges". This framework has attracted growing attention from both academia and industry due to their excellent effectiveness, ability to generalize across tasks, and interpretability in the form of natural language. This paper presents a comprehensive survey of the LLMs-as-judges paradigm from five key perspectives: Functionality, Methodology, Applications, Meta-evaluation, and Limitations. We begin by providing a systematic definition of LLMs-as-Judges and introduce their functionality (Why use LLM judges?). Then we address methodology to construct an evaluation system with LLMs (How to use LLM judges?). Additionally, we investigate the potential domains for their application (Where to use LLM judges?) and discuss methods for evaluating them in various contexts (How to evaluate LLM judges?). Finally, we provide a detailed analysis of the limitations of LLM judges and discuss potential future directions.</p>
<p>Authors' addresses: Haitao Li, liht22@mails.tsinghua.edu.cn, Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University, Beijing, China; Qian Dong, dq22@mails.tsinghua.edu.cn, Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University, Beijing, China; Junjie Chen, chenjj826@gmail. com, Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University, Beijing, China; Huixue Su, suhuixue@ruc.edu.cn, Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; Yujia Zhou, suhuixue@ruc.edu.cn, Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University, Beijing, China; Qingyao Ai, aiqy@tsinghua.edu.cn, Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University, Beijing, China; Ziyi Ye, yeziyi1998@gmail.com, Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University, Beijing, China; Yiqun Liu, yiqunliu@tsinghua.edu.cn, Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University, Beijing, China.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Through a structured and comprehensive analysis, we aim aims to provide insights on the development and application of LLMs-as-judges in both research and practice. We will continue to maintain the relevant resource list at https://github.com/CSHaitao/Awesome-LLMs-as-Judges.</p>
<p>Additional Key Words and Phrases: Large Language Models, Evaluation, LLMs-as-Judges</p>
<h1>ACM Reference Format:</h1>
<p>Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu. 2024. LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods. 1, 1 (December 2024), 60 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn</p>
<h2>1 INTRODUCTION</h2>
<p>Studies on evaluation methods have long been a key force in guiding the development of modern Artificial Intelligence (AI) [23]. AI researchers have continuously sought to measure and validate the intelligence of AI models through various tasks [23, 75]. In the mid-20th century, AI evaluation primarily centered on assessing algorithm performance in specific tasks, such as logical reasoning and numerical computation [164]. Traditional machine learning tasks like classification and regression often use programmable and statistical metrics, including accuracy, precision, and recall. With the emergence of deep learning, the complexity of AI systems grew rapidly, prompting a shift in evaluation standards [119]. The evaluation of AI has expanded from pre-defined, programmable machine metrics to more flexible, robust evaluators for solving complex, realistic tasks. A typical example is the Turing Test [67, 222], which determines whether an AI model can exhibit human-like intelligent behavior through dialogue with humans. The Turing Test provides a fundamental guideline in the evaluation of AI models, especially on AI models' intelligence in flexible and realistic environments.</p>
<p>Recently, the emergence of Large Language Models (LLMs) and generative AI serves as a new milestone in the evolution of AI evaluation. LLMs exhibit remarkable generalization and adaptability, showcasing strong transfer capabilities across previously unseen tasks and diverse domains [1, 9]. However, their powerful capabilities also present new challenges for evaluation. Due to the highly generative and open-ended nature of their outputs, standardized metrics are often insufficient for a comprehensive evaluation. For example, in natural language generation (NLG) tasks, traditional metrics like BLEU [173] and ROUGE [139] often fail to capture key aspects such as text fluency, logical coherence, and creativity. Moreover, modern AI evaluation extends beyond task performance and must account for the ability to address complex, dynamic problems in real-world scenarios, including robustness, fairness, and interpretability. Human annotations, frequently regarded as the "ground truth," can offer comprehensive insights and valuable feedback. By gathering responses from experts or users, researchers can gain a deeper understanding of a model's performance, practicality, and potential risks. However, collecting them are typically time-consuming and resource-intensive, making it challenging to scale up for large-scale evaluation.</p>
<p>In this context, a new paradigm has emerged to replace humans and statistical metrics with LLMs in evaluation, referred to as LLMs-as-judges [5, 14, 14, 221]. Compared to traditional evaluation methods, LLMs-as-judges show significant strengths. First, LLM judges can adjust their evaluation criteria based on the specific task context, rather than relying on a fixed set of metrics, making the evaluation process more flexible and refined. Second, LLM judges can generate interpretive evaluations, offering more comprehensive feedback on model performance and enabling researchers to gain deeper insights into the evaluater's strengths and weaknesses. Finally, LLM judges offer a scalable and reproducible alternative to human evaluation, significantly reducing the costs and time associated with human involvement.</p>
<p>Despite its great potential and significant advantages, LLMs-as-judges also face several critical challenges. For example, the evaluation results of LLMs are often influenced by the prompt template, which can lead to biased or inconsistent assessments [260]. Considering that LLMs are trained on extensive text corpus, they may also inherit various implicit biases, impacting the fairness and reliability of their assessments [267]. Moreover, distinct tasks and domains require specific evaluation criteria, making it difficult for LLMs to adapt their standards dynamically to specific contexts.</p>
<p>Considering the vast potential of this field, this survey aims to systematically review and analyze the current state and key challenges of the LLMs-as-judges. As shown in Figures 1 and 2, we discuss existing research across five key perspectives: 1) Functionality: Why use LLM judges, 2) Methodology: How to use LLM judges, 3) Application: Where to use LLM judges, 4) Metaevaluation: How to evaluate LLM judges and 5) Limitation: Existing problems of LLM judges. We explore the key challenges confronting LLMs-as-judges and hope to provide a clearer guideline for their future development.</p>
<p>In summary, the main contributions of this paper are as follows:
(1) Comprehensive and Timely Survey: We present the extensive survey on the emerging paradigm of LLMs-as-judges, systematically reviewing the current state of research and developments in this field. By examining LLMs as performance evaluators based on their generated natural language, we highlight the unique role of LLMs in shaping the future of AI evaluation.
(2) Systematic Analysis Across Five Key Perspectives: We organize our survey around five critical aspects: Functionality, Methodology, Application, Meta-evaluation, and Limitation. This structured approach allows for a nuanced understanding of how and why LLMs are utilized as evaluators, their practical implementations, and reliability concerns.
(3) Current Challenges and Future Research Directions: We discuss the existing challenges for adopting LLMs-as-judges, highlighting potential research opportunities and directions while offering a forward-looking perspective on the future development of this paradigm, encouraging researchers to delve deeper into this exciting area. We also provide an opensource repository at https://github.com/CSHaitao/Awesome-LLMs-as-Judges, with the goal of fostering a collaborative community and advancing best practices in this area.</p>
<p>The organization of this paper is as follows. In Section (§2), we provide the formal definition of LLMs-as-judges. Then, Section (§3) reviews existing work from the perspective of "Why use LLM judges". Following that, Section (§4) covers "How to use LLM judges", summarizing the current technical developments in LLMs-as-judges. Section (§5) discusses "Where to use LLM judges", focusing on their application domains. In Section (§6), we review the metrics and benchmarks used for evaluating LLMs-as-judges. Section (§7) discusses the limitations and challenges of LLM judges. We discuss major future work in Sections (§8) and (§9) to conclude the paper.</p>
<h1>2 PRELIMINARIES</h1>
<p>In this section, we will provide a formal definition of LLMs-as-judges, aiming to encompass all current evaluation paradigms and methods, thereby offering readers a clear and thorough understanding. Figure 3 presents an overview of the LLMs-as-judges system.</p>
<p>The LLMs-as-judges paradigm is a flexible and powerful evaluation framework where LLMs are employed as evaluative tools, responsible for assessing the quality, relevance, and effectiveness of generated outputs based on defined evaluation criteria. This framework leverages the extensive knowledge and deep contextual understanding of LLMs, enabling it to flexibly adapt to various</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Taxonomy of LLMs-as-judges in functionality, methodology, application, meta-evaluation.
tasks in NLP and machine learning. We formalize the input-output structure of the LLMs-asJudges paradigm, unifying various evaluation scenarios into a unified perspective. Specifically, the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Taxonomy of LLMs-as-judges in limitation and future work.
evaluation process can be defined as follows:</p>
<p>$$
(Y, \mathcal{E}, \mathcal{F})=E(\mathcal{T}, \mathcal{C}, \mathcal{X}, \mathcal{R})
$$</p>
<p>where $E$ is the evaluation function, taking the evaluation type $\mathcal{T}$, evaluation criteria $\mathcal{C}$, evaluation item $\mathcal{X}$ and optional references $\mathcal{R}$ as input. Based on these inputs, the LLM can produces three outputs: evaluation result $\mathcal{Y}$, explanation $\mathcal{E}$ and feedback $\mathcal{F}$. Different input-output configurations correspond to distinct methods and objectives. This unified formulation brings together diverse evaluation paradigms, offering a structured framework for categorizing and understanding various approaches within LLMs-as-judges.</p>
<h1>2.1 Evaluation Function $E$</h1>
<p>The evaluation function $E$ in the context of LLMs-as-judges can be categorized into three primary configurations: Single-LLM systems, Multi-LLM systems, and Hybrid systems that combine LLMs with human evaluators. Each of these configurations serves distinct purposes, offers different advantages, and faces unique challenges.</p>
<ul>
<li>Single-LLM Evaluation System [141, 145, 280]: A single LLM evaluation system relies on a single model to perform the evaluation tasks. It is simple to deploy and scale, making it efficient for tasks that don't require specialized evaluation. However, its flexibility is limited, as it may struggle with tasks that demand specialized knowledge or reasoning over complex inputs. Additionally, if not properly trained, a single model may introduce biases, leading to inaccurate evaluations.</li>
<li>Multi-LLM Evaluation Systems [22, 39, 132]: A Multi-LLM evaluation system combines multiple models that work together to perform evaluation tasks. These models may interact through various mechanisms, such as collaboration, or competition, to refine their outputs and achieve more accurate results. By leveraging the strengths of different models, a multi-model system can cover a broader range of evaluation criteria and provide a more comprehensive assessment. However, this comes at a higher computational cost and requires more resources, making deployment and maintenance more challenging, particularly for large-scale tasks. Moreover, while cooperation between models often enhances evaluation results, the methods through which these models achieve consensus or resolve differences remain key areas of ongoing exploration.</li>
<li>Human-AI Collaboration System [131, 192, 230]: In this system, LLMs work alongside human evaluators, combining the efficiency of automated evaluation with the nuanced judgment</li>
</ul>
<p>of human expertise. This configuration allows human evaluators to mitigate potential biases in the LLM's output and provide subjective insights into complex evaluation tasks. While this system offers greater reliability and depth, it comes with challenges in coordinating between the models and humans, ensuring consistent evaluation standards, and integrating feedback. Additionally, the inclusion of human evaluators increases both the cost and time required for the evaluation process, making it less scalable than purely model-based systems.</p>
<h1>2.2 Evaluation Input</h1>
<p>In the LLMs-as-judges paradigm, in addition to the evaluation item $\mathcal{X}$, LLM judges typically receive three other types of inputs: Evaluation Type $\mathcal{T}$, Evaluation Criteria $\mathcal{C}$, and Evaluation References $\mathcal{R}$. The following provides a detailed explanation:
2.2.1 Evaluation Type $\mathcal{T}$. The Evaluation Type $\mathcal{T}$ defines the specific evaluation mode, determining how the evaluation will be conducted. It typically includes three approaches: pointwise, pairwise, and listwise evaluation.</p>
<ul>
<li>Pointwise Evaluation [109, 229, 266]: This method evaluates each candidate item individually based on the specified criteria. For example, in a text summarization task, the LLM might evaluate each generated summary separately, assigning a score based on factors like informativeness, coherence, and conciseness. Although pointwise evaluation is simple and easy to apply, it may fail to capture the relative quality differences between candidates and can be influenced by biases arising from evaluating items in isolation.</li>
<li>Pairwise Evaluation [20, 84, 89, 188]: This method involves directly comparing two candidate items to determine which one performs better according to the specified criteria. It is commonly used in preference-based tasks. For example, given two summaries of a news article, the LLM may be asked to decide which summary is more coherent or informative. Pairwise evaluation closely mirrors human decision-making processes by focusing on relative preferences rather than assigning absolute scores. This approach is especially effective when the differences between outputs are subtle and difficult to quantify.</li>
<li>Listwise Evaluation [87, 166, 263, 302]: This method is designed to collectively evaluate the entire list of candidate items, evaluating and ranking them based on the specific criteria. It is often applied in ranking tasks, such as document retrieval in search engines, where the objective is to determine the relevance of the documents in relation to a user query. Listwise evaluation takes into account the interactions between multiple candidates, making it well-suited for applications that require holistic analysis.</li>
</ul>
<p>In general, these three evaluation modes are not entirely independent. pointwise scores can be aggregated to create pairwise comparisons or used to construct a ranked list. Similarly, pairwise preferences can be organized into a complete ranking list for listwise analysis. However, these transformations are not always reliable within the LLMs-as-judges framework [149]. For example, in pointwise evaluation, output $A$ may receive a score of 5 , while output $B$ receives a score of 4 , yet, a direct pairwise comparison might not consistently yield $A&gt;B$ due to potential bias. Additionally, LLM judges do not always satisfy transitivity in their judgments. For instance, given pairwise preferences where $z_{i}&gt;z_{j}$ and $z_{j}&gt;z_{k}$, the LLM may not necessarily yield $z_{i}&gt;z_{k}$. These inconsistencies contribute to concerns about the reliability and trustworthiness of the LLM-as-Judge framework, which we will discuss in detail in Section (§7).
2.2.2 Evaluation Criteria $\mathcal{C}$. The evaluation criteria $\mathcal{C}$ define the specific standards that determine which aspects of the output should be assessed. These criteria are designed to cover a broad range</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Overview of the LLMs-as-judges system.
of quality attributes and can be tailored based on the nature of the task. Typically, the criteria encompass the following aspects:</p>
<ul>
<li>Linguistic Quality [34, 59, 283]: This category evaluates the language-related features of the output, such as fluency, grammatical accuracy, coherence, and Conciseness. Linguistic quality is crucial in tasks like text generation, machine translation, and summarization, where clarity and readability are essential.</li>
<li>Content Accuracy [30, 101, 213]: This dimension focuses on the correctness and relevance of the content. It includes evaluating aspects such as factual accuracy, ensuring that the output does not contain misleading or incorrect information. Content accuracy is particularly crucial in tasks such as code generation and fact-checking.</li>
<li>Task-Specific Metrics [96, 138, 213]: In addition to general quality metrics, many tasks require evaluation based on standards specific to their respective domains. These standards may include metrics such as informativeness (assessing whether the output provides comprehensive and valuable information) or completeness (ensuring all key aspects of the input are covered). Other criteria may include diversity, well-structured content, and logical clarity.</li>
</ul>
<p>In addition to providing clear evaluation criteria, offering several examples can also be beneficial for the assessment. By incorporating well-structured examples, LLMs can better align its output with user expectations, especially when handling complex tasks or ambiguous queries.</p>
<p>2.2.3 Evaluation References $\mathcal{R}$. Evaluation References $\mathcal{R}$ are optional. Depending on the availability of evaluation reference, the evaluation process can be broadly divided into reference-based and reference-free scenarios.</p>
<ul>
<li>Reference-Based Evaluation [66, 104]: The reference-based evaluation leverages reference data to determine whether the performance meets the expected standards. It is commonly applied in tasks where the quality of the output can be objectively judged by its similarity to established reference. In Natural Language Generation (NLG) tasks, this method is widely used to evaluate the resemblance between generated content and reference content. For example, in machine translation or text summarization, an LLM can compare the generated translations or summaries against high-quality references. The key strength of this approach is its well-defined benchmarking process; however, its effectiveness may be constrained by the quality and variety of the reference data.</li>
<li>Reference-Free Evaluation [82, 194, 292]: The reference-free evaluation does not rely on a specific reference $\mathcal{R}$, instead, it evaluates $\mathcal{X}$ based on intrinsic quality standards or its alignment with the source context. For example, when assessing language fluency or content coherence, an LLM can autonomously generate evaluation results using internal grammatical and semantic rules. This method is widely used in fields like sentiment analysis and dialogue generation. The main advantage of this approach is its independence from specific references, providing greater flexibility for open-ended tasks. However, its drawback lies in the difficulty of obtaining satisfactory evaluations in domains where the LLM lacks relevant knowledge.</li>
</ul>
<h1>2.3 Evaluation Output</h1>
<p>In the LLMs-as-judges paradigm, the LLM typically generates three types of outputs: the evaluation result $\mathcal{Y}$, the explanation $\mathcal{E}$, and the feedback $\mathcal{F}$. Below are detailed descriptions.</p>
<ul>
<li>Evaluation Result $\mathcal{Y}$ [188, 286]: The evaluation result $\mathcal{Y}$ is the primary output, which can take the form of a numerical score, a ranking, a categorical label, or a qualitative assessment. It reflects the quality, relevance, or performance of the candidate items according to the specified evaluation criteria. For example, in a machine translation task, $\mathcal{Y}$ could be a score indicating translation quality, while in a dialogue generation task, it might be a rating of coherence and appropriateness on a scale from 1 to 5 . The evaluation result $\mathcal{Y}$ provides a clear measure of performance, enabling researchers to effectively compare different models or outputs.</li>
<li>Explanation $\mathcal{E}[252,270]$ : The explanation $\mathcal{E}$ provides detailed reasoning and justifications for the evaluation result. It offers insights into why certain result received higher or lower scores, highlighting specific features of the candidate item that influenced the evaluation. For example, in a summarization task, the LLM judges might explain that the score was lowered due to missing critical information or the presence of redundant content. The explanation component enhances transparency, allowing users to understand the decision-making process of the LLM and gain deeper insights into the strengths and weaknesses of the evaluated content.</li>
<li>Feedback $\mathcal{F}[32,155]$ : The feedback $\mathcal{F}$ consists of actionable suggestions or recommendations aimed at improving the evaluated output. Unlike the evaluation result, which merely indicates performance, the feedback component is designed to guide the refinement of the content. For instance, in a creative writing task, feedback might include recommendations for enhancing the narrative flow or improving clarity. This component is especially valuable for the iterative development of the evaluated item, as it provides concrete pointers that help both the LLM and content creators enhance the quality of the generated outputs.
Depending on the intended purpose and specific requirements of the evaluation, the LLM judges can generate various combinations of the three outputs $(\mathcal{Y}, \mathcal{E}, \mathcal{F})$ for a given task. In most cases,</li>
</ul>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Overview of the Functionality of LLMs-as-judges.
providing explanation $\mathcal{E}$ not only helps users better understand and trust the evaluation results but also leads to more human-aligned and accurate evaluation result $\mathcal{Y}$. Moreover, generating feedback $\mathcal{F}$ generally demands a higher level of model capability, as it requires not only assessing the quality of the input but also providing concrete, actionable recommendations for improvement.</p>
<h1>3 FUNCTIONALITY</h1>
<p>As an emerging evaluation paradigm, LLMs-as-judges play a significant role across various scenarios. Based on their functionality, we categorize the application of LLM evaluators into three main directions: Performance Evaluation (§3.1), Model Enhancement (§3.2), and Data Construction (§3.3). In this section, we will delve into these functionalities, explore their potential, and discuss specific implementation methods.</p>
<h3>3.1 Performance Evaluation</h3>
<p>Performance evaluation represents the most fundamental application objective of an LLM judges, serving as the cornerstone for understanding and optimizing their other function. It can be broadly divided into two components: Responses Evaluation (§3.1.1) and Model Evaluation (§3.1.2). Response Evaluation focuses on aspects such as the quality, relevance, and coherence, and fluency of the responses for a given task. In contrast, model evaluation takes a holistic approach, assessing the overall capabilities of LLMs. Although these two aspects are interconnected, they focus on different levels of analysis, providing multidimensional insights into performance.
3.1.1 Responses Evaluation. The purpose of evaluating responses is to identify better answers within the context of a specific question or task, which can enhance overall decision-making. These responses can originate from either AI models or humans. Evaluation criteria typically consider general attributes such as accuracy, relevance, coherence, and fluency. However, in practical applications, the evaluation of responses often requires customized metrics tailored to specific tasks. For instance, in the education domain, the focus may be more on the inspirational and educational value of the answers.</p>
<p>LLM judges have also been widely applied in the assessment of text response [141, 228, 297]. Lin et al. [141] propose LLM-Eval, a unified framework employing a single-prompt strategy to evaluate the performance of open-domain dialogue systems across multiple dimensions, including content, grammar, relevance, and appropriateness. Wang et al. [228] proposed an article scoring and feedback system tailored to different genres, such as essays, narratives, and question-answering articles. Using BERT and ChatGPT models, they enabled automated scoring and detailed feedback, showcasing the potential of LLMs in article evaluation. Moreover, Zhou et al. [297] conduct a detailed evaluation of whether LLMs can serve as reliable tools for automated paper review. Their findings indicate that current LLMs are still not sufficiently reliable for such tasks, particularly in scenarios requiring logical reasoning or a deep knowledge base.</p>
<p>Furthermore, the evaluation of a single response is not limited to assessing the quality of the final answer but can also extend to analyzing the response process [3, 124, 188]. For instance, this can include evaluating whether retrieval is necessary at a given step, the relevance of the retrieved documents, and the interpretability of the response. For example, ARES [188] uses LLM judges to evaluate RAG systems across three dimensions: Contextual Relevance, Answer Faithfulness, and Answer Relevance. Similarly, Asai et al. [3] proposed SELF-RAG, which employs reflective token to determine whether retrieval is required and to self-assess the quality of generated outputs. Lei et al. [124] introduced LLMs to evaluate the quality of generated explanations, demonstrating the effectiveness of LLMs in understanding and generating explanations for recommendation tasks.
3.1.2 Model Evaluation. Model evaluation typically begins with assessing individual responses and then extends to analyzing overall capabilities. This wider perspective aims to analyze the model's performance across various tasks or domains, such as coding ability, instruction-following proficiency, reasoning, and other specialized skills relevant to its intended applications.</p>
<p>A common and straightforward approach is to represent model performance using average performance on static benchmarks [138, 213, 292]. LLM judges assess the model's performance using a set of carefully designed metrics, which results in a performance ranking. This method is widely adopted due to its simplicity and comparability. For example, task sets can be designed to evaluate the model's knowledge coverage, reasoning depth, and language generation quality [116, 148, 202], or real-world scenarios can be simulated to assess the model's ability to handle complex situations [144, 219].</p>
<p>As the demand for evaluation increases, the evaluation process has gradually shifted from traditional static testing to more dynamic, interactive assessments [10, 273, 286]. LLMs-as-judges has pioneered this approach, similar to Chatbot Arena [292], a crowdsourced platform that collects anonymous votes on LLM performance and ranks them using Elo scores. Auto-Arena [153, 286] and LMExam [10] assess model capabilities by using LLMs as both question setters and evaluators. These frameworks innovatively combine diverse question generation, multi-turn question-answering evaluation, and a decentralized model-to-model evaluation mechanism, providing more detailed and granular performance assessments. Additionally, KIEval [273] introduces an LLM-driven "interactor" role, which evaluates the knowledge mastery and generation abilities of LLMs through dynamic multi-turn conversations. These dynamic evaluation methods effectively address data leakage and evaluation bias issues common in traditional benchmark tests.</p>
<h1>3.2 Model Enhancement</h1>
<p>In addition to Performance Evaluation, LLMs-as-judges is also widely used for Model Enhancement. From training to inference, LLMs-as-judges plays a key role in improving model performance. Its application in model enhancement offers a novel optimization pathway for artificial intelligence,</p>
<p>fostering the refinement and personalization of intelligent systems across a broader spectrum of real-world applications.
3.2.1 Reward Modeling During Training. A primary application of LLMs-as-judges is in reward modeling during training, particularly in reinforcement learning with feedback [21, 74, 239, 257, 274]. LLM judges assign scores to model outputs by evaluating them against human-defined criteria, guiding optimization toward desired behaviors. This ensures alignment with human values, improving the quality and relevance of the generated outputs and improving the effectiveness of LLMs in real-world tasks.</p>
<p>A series of works, such as SRLMs [274], OAIF [74], and RLAIF [121], have enabled LLMs to become their own reward models. This overcomes the traditional RLHF dependency on fixed reward models, allowing the model to iteratively reward and self-optimize, fostering self-evolution through continuous self-assessment. RELC [21] tackles the challenge of sparse rewards in traditional RL by introducing a Critic Language Model (Critic LM) to evaluate intermediate generation steps. This dense feedback at each step helps mitigate reward sparsity, offering more detailed guidance to the model during training.</p>
<p>However, using the same LLM for both policy generation and reward modeling can pose challenges in ensuring the accuracy of the rewards. This dual role setup may lead to accumulated biases and preference data noise, which can undermine the training effectiveness. To address this issue, CREAM [239] introduces cross-iteration consistency constraints to regulate the training process and prevent the model from learning unreliable preference data. This significantly enhances reward consistency and alignment performance. In addition, CGPO [257] groups tasks by category (such as dialogue, mathematical reasoning, safety, etc.) and uses "Mixed Judges" to assign a specific reward model to each task group. This ensures that the reward signals are closely aligned with the task objectives, thereby preventing conflicts between different goals.
3.2.2 Acting as Verifier During Inference. During inference, LLM judges serve as verifier, responsible for selecting the optimal response from multiple candidates [15, 15, 137, 160, 265]. By comparing the outputs based on various metrics, such as factual accuracy and reasoning consistency, they are able to identify the best fit for the given task or context, thereby optimizing the inference process or improving the quality of the generated results.</p>
<p>One of the simplest applications is Best-of-N sampling [102, 209], where the model is sampled N times, and the best result is selected to improve model performance. Similarly, Wang et al. [235] introduced a promising sampling method called self-consistency, where n samples are drawn from the judge model, and the average score is output. These sampling methods enhance inference stability by selecting the best result from multiple evaluations. Further optimization strategies include the Tree of Thoughts (ToT) [265] method, which models the problem-solving process as a tree structure. This allows the model to explore multiple solution paths and optimize path selection through self-assessment mechanisms. The Graph of Thoughts (GoT) [15] method extends this concept by introducing directed graphs, where the non-linear interactions between nodes improve the efficiency and precision of multi-step reasoning. In both methods, LLM judges play a crucial role in guiding the model to select the most promising paths, thereby enhancing the quality and accuracy of reasoning.</p>
<p>Similarly, Lightman et al. [137] discuss how step-by-step validation can enhance the performance of LLMs in multi-step reasoning tasks, particularly in the domain of mathematics. SE-GBS [250] integrates self-assessment into the multi-step reasoning decoding process, generating scores that reflect logical correctness and further ensuring the accuracy and consistency of the reasoning chain. The REPS [105] improves the accuracy and reliability of reasoning validation models by comparing reasoning paths pairwise, verifying their logical consistency and factual basis. Also, Musolesi et</p>
<p>al. [160] proposed Creative Beam Search, with the LLM acting as a judge to simulate the human creative selection process, thereby enhancing the diversity and creativity of the generated results.
3.2.3 Feedback for Refinement. After receiving the initial response, LLM judges provide actionable feedback to iteratively improve output quality. By analyzing the response based on specific task criteria, such as accuracy, coherence, or creativity, the LLM can identify weaknesses in the output and offer suggestions for improvement. This iterative refinement process plays a crucial role in applications that require adaptability [32, 93, 155, 176, 261].</p>
<p>SELF-REFINE [155] enables LLMs to iteratively improve output quality through feedback generated by the model itself, without requiring additional training or supervision data. On the other hand, SELF-DEBUGGING [32] demonstrates a practical application of self-correction in code generation by identifying and rectifying errors through self-explanation and feedback. This approach has significantly enhanced the performance of LLMs across various code generation tasks.</p>
<p>In addition to refining response quality, LLMs judges are also widely used to enhance reasoning abilities. For example, REFINER [176] optimizes the reasoning performance of LLMs through interactions between a generator model and a critic model. In this framework, the generator model is responsible for producing intermediate reasoning steps, while the critic model analyzes these steps and provides detailed feedback, such as identifying calculation errors or logical inconsistencies. Xu et al. [261] propose a multi-agent collaboration strategy to enhance the reasoning abilities of LLMs by simulating the academic peer review process. The framework is divided into three stages: generation, review, and revision. Agents provide feedback and attach confidence scores to refine the initial answers, with the final result determined through majority voting.</p>
<p>While the feedback and correction mechanisms of LLMs judges are continually evolving, the limitations of self-feedback in improving quality should not be overlooked. Research on SelfCorrect [93, 223] shows that, the intrinsic self-correction capabilities of LLMs often fall short of effectively improving reasoning quality. Valmeekam et al. [224] also raise concerns about the effectiveness of LLMs as self-validation tools in the absence of reliable external validators. Future research can focus on improving the accuracy of feedback provided by these LLM judges and incorporating external validation mechanisms to optimize their performance in complex reasoning tasks.</p>
<h1>3.3 Data Construction</h1>
<p>Data collection is a crucial stage in the development of machine learning systems, especially those driven by the rapid advancements in deep learning. The quality of the data directly determines the performance of the trained models. The LLMs-as-judges has significantly transformed the landscape of data collection, substantially reducing reliance on human effort. In this section, we will explore the pivotal role of LLMs-as-judges in data collection from two key perspectives: Data Annotation (§3.3.1) and Data Synthesize (§3.3.2).
3.3.1 Data Annotation. Data Annotation involves leveraging LLM judges to label large, unlabeled datasets efficiently [71, 79, 85, 217]. By utilizing the advanced natural language understanding and reasoning capabilities of LLMs, the annotation process can be automated to a significant extent, enabling the generation of high-quality labels with reduced human intervention.</p>
<p>LLMs have demonstrated remarkable potential in text annotation tasks, consistently outperforming traditional methods and human annotators in various settings. He et al. [85] evaluated the performance of GPT-4 in crowdsourced data annotation workflows, particularly in text annotation tasks. Their comparative study revealed that, even with best practices, the highest accuracy achievable by MTurk workers was $81.5 \%$, whereas GPT-4 achieved an accuracy of $83.6 \%$. Similarly, Gilardi et al. [71] analyzed 6,183 tweets and news articles, demonstrating that ChatGPT outperformed</p>
<p>crowdsourced workers in tasks such as stance detection, topic detection, and framing. Törnberg et al. [217] further investigated the classification of Twitter users' political leanings based on their tweet content. Their findings revealed that ChatGPT-4 not only surpassed human classifiers in accuracy and reliability but also exhibited bias levels that were comparable to or lower than those of human classifiers.</p>
<p>As technology advances, more and more research is exploring their application in multimodal data annotation. For example, the FullAnno [79] uses the GPT-4V model to generate image annotations, significantly improving the quality of image descriptions through a multi-stage annotation process. Furthermore, Latif et al. [117] explored the application of LLMs in speech emotion annotation, demonstrating that, with data augmentation, LLM-annotated samples can significantly enhance the performance of speech emotion recognition models. By integrating text, audio features, and gender information, the effectiveness of LLM-based annotations was further improved, highlighting their potential in advancing multimodal annotation tasks.</p>
<p>As LLMs perform excellently in annotation tasks, researchers are actively exploring methods to further improve annotation quality and address potential challenges. For example, AnnoLLM [83] introducedthe "explain-then-annotate" method, which enhances both the accuracy and transparency of annotations by prompting the LLM to justify its label assignments. Additionally, the LLMAAA [282] framework incorporates an active learning strategy to efficiently select highinformation samples for annotation, thereby mitigating the effects of noisy labels and reducing the reliance on costly human annotation. These approach not only enhance the performance of task-specific models but also offer new perspectives on the efficient application of LLMs in annotation workflows.
3.3.2 Data Synthesize. The goal of Data Synthesis is to create entirely new data, either from scratch or based on seed data, while ensuring it is similar in distribution to real data. Data Synthesis enables the generation of diverse data samples, enhancing a model's generalization ability to unseen examples while reducing reliance on sensitive real-world data [2, 52, 108, 157, 235, 268].</p>
<p>In recent years, advancements in LLMs have led to significant improvements in both the quality and efficiency of data synthesis methods. In this domain, methods like SELFEE [268] and SynPO [52] have effectively enhanced the alignment capabilities of LLMs by leveraging small amounts of labeled data and iteratively generating preference-aligned data. Arif et al. [2] also introduce a multi-agent workflow for generating optimized preference datasets.</p>
<p>SELF-INSTRUCT [235] and Evol-Instruct [254, 278] represent innovative approaches to improving model alignment and performance through self-generated instruction data. SELF-INSTRUCT [235] requires minimal human annotation, instead relying on self-generated instruction data to align pre-trained models. Evol-Instruct [254, 278] further enhances LLM performance by automatically generating instruction data, significantly boosting model capabilities.</p>
<p>STaR [277] and ReSTEM [199] are research efforts aimed at enhancing reasoning capabilities through synthetic data. STaR [277] employs a self-guided iterative process to improve model performance on complex reasoning tasks, offering an effective solution for tackling increasingly sophisticated reasoning challenges in the future. ReSTEM [199], on the other hand, utilizes a self-training approach based on the expectation-maximization framework to enhance the problemsolving capabilities of large language models, particularly in areas such as solving mathematical problems and generating code.</p>
<h1>4 METHODOLOGY</h1>
<p>The use of LLM judges requires careful methodological considerations to ensure the accuracy and consistency of judgments. Researchers have developed various approaches according to the</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. Overview of the Methodology of LLMs-as-judges.
complexity and specific requirements of different judgment tasks, each offering unique advantages. In this section, we categorize these methodologies into three broad approaches: Single-LLM System (§4.1): evaluation by a single-LLM, Multi-LLM System (§4.2): evaluation by cooperation among multi-LLMs, and Human-AI Collaboration (§4.3): evaluation by cooperation of LLMs and Human. Figure 5 presents an overview of methodology.</p>
<h1>4.1 Single-LLM System</h1>
<p>Single-LLM System relies on a single model to perform judgment tasks, with its effectiveness largely determined by the LLM's capabilities and the strategies used to process input data. This approach can generally be divided into three fundamental components: Prompt Engineering (§4.1.1), Tuning (§4.1.2), and Post-processing (§4.1.3) of model outputs.
4.1.1 Prompt-based. Prompt engineering [189] involves crafting clear and structured input prompts tailored to elicit accurate and contextually appropriate responses from LLM judges. This approach is crucial for ensuring that LLMs grasp the complexities of specific tasks and provide relevant, consistent, and goal-aligned judgments. In many cases, well-designed prompts significantly reduce the need for extensive model training.</p>
<p>In-Context Learning. In-Context Learning (ICL) is a distinctive capability of LLMs that allows them to dynamically adapt to evaluation tasks using carefully curated examples or explanations within the prompt [53]. Several recent methods have demonstrated the power of ICL in LLM-asjudges, showcasing how it enhances the flexibility and performance of LLMs in diverse settings. For example, GPTScore [68] leverages the few-shot learning capability of generative pre-trained models to evaluate generated text. By using relevant examples to customize prompts, it provides a flexible, training-free approach to assess multiple aspects of text quality. Similarly, LLM-EVAL [141] incorporates carefully crafted examples into prompts, proposing a unified, multi-dimensional automatic evaluation method for open-domain dialogue. Another notable example is TALEC [280],</p>
<p>a model-based evaluation method that leverages in-context learning to enable users to set custom evaluation criteria for LLMs in specific domains. Through careful prompt engineering, users can iteratively adjust the examples to refine the evaluation process as needed. In addition, Jain et al. [94] proposed the In-Context Learning-based Evaluator (ICE) for multi-dimensional text evaluation. ICE leverages LLMs and a small number of in-context examples to evaluate generated text summaries, achieving competitive results.</p>
<p>While ICL can enable effective evaluation, it is not without challenges. One major issue is that the model's responses may be influenced by the selection of prompt examples, potentially leading to bias [62, 78, 290, 296]. To address this issue, Hasanbeig et al. proposed ALLURE [81], a comprehensive protocol designed to mitigate bias in ICL for LLMs during text evaluation. ALLURE [81] improves evaluator accuracy by iteratively incorporating discrepancies between its assessments and annotated data into the learning context. Moreover, after uncovering the existence of symbol bias within LLM evaluators when using ICL, Song et al. [204] proposed two effective mitigation strategy prompt templates, Many-Shot with Reference (MSwR) and Many-Shot without Reference (MSoR), to bolster the reliability and precision of LLM-based assessments.</p>
<p>Step-by-step. Step-by-step involves breaking down complex evaluation tasks into fine-grained components, leveraging the reasoning capabilities of LLMs to simplify the evaluation process. The most straightforward example of which is perhaps Chain-of-Thought (CoT) [113, 242]. Building on that, frameworks like G-EVAL [145] have been proposed to assess the quality of NLG outputs. G-EVAL [145] combines CoT with a form-filling paradigm, allowing the LLM to assess outputs in a structured manner. Similarly, ICE-Score [304] introduces a step-by-step framework for evaluating code, in which the LLM is instructed with task definitions, evaluation criteria, and detailed evaluation steps. By breaking the task down into clear steps, ICE-Score [304] improves the quality and consistency of code evaluation. Also, ProtocoLLM [271] employs a similar step-by-step approach to evaluate the specialized capabilities of LLMs in generating scientific protocols. Portia [134] achieves better evaluation results in a lightweight yet effective manner. It divides the answer into multiple parts, aligns similar content between candidate answers, and then merges them back into a single prompt for evaluation by the LLM.</p>
<p>Some studies break down evaluations into two steps: "explanation-rating." This approach suggests that providing an explanation enhances the reliability of the rating. Chiang et al. [36] offer empirical guidelines to improve the quality of LLM evaluations, demonstrating that combining rating with explanation (rate-explain) or explanation with rating (explain-rate) leads to higher correlations with human ratings. Another effective strategy is to decompose complex evaluation standards into specific, discrete criteria, allowing the LLM to assess each aspect independently. FineSurE [203] is an advanced example of this method, offering a framework for the fine-grained evaluation of text summarization quality. It breaks down the evaluation into multiple dimensions, such as faithfulness, completeness, and conciseness. Through detailed analysis, including fact-checking and key fact alignment, FineSurE [203] outperforms traditional methods in terms of evaluation accuracy.</p>
<p>Definition Augmentation. The Enhanced Definition approach involves refining prompts to inject improved evaluation criteria, establish assessment principles, or incorporate external knowledge into the LLM judge's decision-making process. Some studies focus on enriching and clarifying the prompts to ensure that the evaluation criteria are both comprehensive and welldefined.</p>
<p>For example, Liu et al. propose AUTOCALIBRATE [146], a multi-stage, gradient-free approach. This method involves the drafting, revision, and application of calibrated criteria, and it automatically calibrates and aligns an LLM-based evaluator to match human preferences for NLG quality assessment. Furthermore, SALC [76] enables LLMs to autonomously generate context-aware evaluation criteria for self-assessment, overcoming the limitations of static, human-defined metrics. On</p>
<p>the other hand, the LLM-as-a-Personalized-Judge approach [54] introduces a novel perspective by incorporating diverse evaluative roles and principles. This allows LLMs to adapt to complex, varied evaluation scenarios, resulting in more nuanced and context-sensitive assessments.</p>
<p>Another key aspect of Definition Augmentation is the retrieval of external knowledge, which helps reduce hallucinations and provides more factual support. For instance, BiasAlert [61], a tool designed to detect social bias in LLM-generated open-text outputs. It integrates external human knowledge with the LLM judge's inherent reasoning capabilities to reliably identify and mitigate bias, outperforming GPT4-as-A-Judge across various scenarios. Moreover, Chen et al. [33] found that within retrieval-augmented generation (RAG) frameworks, LLM judges do not exhibit a significant self-preference effect during evaluation.</p>
<p>Multi-turn Optimization. Multi-turn optimization involves iterative interactions between the evaluator and the evaluated entity, refining evaluation results through diverse forms of feedback, thus fostering deeper analysis and a progressive improvement in evaluation quality [295]. Unlike traditional methods that rely on predefined criteria, Xu et al. proposed ACTIVE-CRITIC [256], enabling LLMs to infer evaluation criteria from data and dynamically optimize prompts through multiple rounds of interaction. Moreover, Some studies [10, 153, 273, 286] leverage LLMs as question designers to engage in dynamic interactions with the evaluated entities, adjusting the questions and task design in real time. This allows for flexible modification of the evaluation content based on the performance of the evaluated entity, thereby enabling more comprehensive assessments.
4.1.2 Tuning-based. Tuning involves training a pre-existing LLM on a specialized dataset to adapt it to specific judgment tasks. It's especially useful when the judgment domain involves highly specialized knowledge or nuanced decision-making [92].</p>
<p>Score-based Tuning. Score-based tuning involves using data with scores to train models and enhance their ability to predict judgment scores based on specific evaluation criteria [28, 47, 229].</p>
<p>Many studies have explored the enhancement of LLM-as-judges by fine-tuning them on humanlabeled datasets. For instance, PHUDGE [47], fine-tuned from the Phi-3 model, achieves state-of-the-art performance in terms of latency and throughput when automatically evaluating the quality of outputs from LLMs. This fine-tuning process equips the model with the necessary judgment skills, enabling it to assess various types of content in a structured and accurate manner. Additionally, ECT [229] introduces a novel method for transferring scoring capabilities from LLMs to lighter models. This allows the lighter models to function as effective reward models for sequence generation tasks, enhancing sequence generation models through reinforcement learning and reranking approaches. AttrScore [276] is another framework for evaluating attribution and identifying specific types of attribution errors, using a curated test set from a generative search engine and simulated examples from existing benchmarks. The above research highlights that LLMs can better align their decision-making process with humans through fine-tuning with human-constructed datasets.</p>
<p>In addition to human-labeled data, some studies have also attempted to fine-tune models using synthetic datasets like SorryBench [249] generated for evaluation tasks. These datasets are often created through rule-based methods or by generating artificial evaluation examples, which also give rise to some metrics like TIGERScore [99]. SELF-J [266] is a self-training framework for developing judge models to evaluate LLMs' adherence to human instructions without human-annotated quality scores. SELF-J [266] proposes selective instruction following, allowing systems to decline lowquality instructions. FENCE [252] is another factuality evaluator designed to provide claim-level feedback to language model generators. It details a data augmentation approach that enriches public datasets with textual critiques and diverse source documents from various tools, thereby enhancing factuality without introducing lesser-known facts. Utilizing synthetic training data to</p>
<p>fine-tune lightweight language model judges and employing prediction-powered inference (PPI) for statistical confidence to mitigate potential prediction errors, ARES [188] can automatically assess RAG systems.</p>
<p>Preference-based Learning. Preference-based learning focuses on training LLMs to make inferences and learn based on preferences, enabling the development of more adaptive and customizable evaluation capabilities.</p>
<p>Initially, researchers leverage these data in conjunction with advanced techniques like Direct Preference Optimization (DPO) [181] to train LLMs for more nuanced evaluative capabilities. In this method, the model is trained to predict which of two outputs is preferred according to humanlike values, rather than learning a scalar reward signal. Such self-improving approach is well reflected in Meta-Rewarding [245]. Con-J [270] trains a generative judge by using the DPO loss on contrastive judgments and the SFT loss on positive judgments to align LLMs with human values. In terms of evaluating other LLMs effectively in open-ended scenarios, JudgeLM [301] addresses key biases in the fine-tuning process with a high-quality preference dataset. Another typical method is PandaLM [236], which is trained on a reliable human-annotated preference dataset, focusing extends beyond just the objective correctness of responses, and addresses vital subjective factors. Moreover, Self-Taught [231] is another approach to train LLMs as effective evaluators without relying on human-annotated preference judgments, using synthetic training data only. Through an iterative self-improvement scheme, LLM judges are able to produce reasoning traces and final judgments. Not quite the same, FedEval-LLM [84] fine-tunes many personalized LLMs without relying on labeled datasets to provide domain-specific evaluation, mitigating biases associated with single referees. It is designed to assess the performance of LLMs on downstream tasks, at the same time, ensuring privacy preservation.</p>
<p>As research has progressed, newer methods have emerged that combine both score-based and preference-based data to refine model evaluation capabilities, not to mention some novel metrics like INSTRUCTSCORE [258]. FLAMe [226] is an example of such an approach. It's a family of Foundational Large Autorater Models which significantly improves generalization to a wide variety of held-out tasks using both pointwise and pairwise methods during training. As generative judge model, AUTO-J [130] addresses challenges in generality, flexibility, and interpretability by training on a diverse dataset containing scoring and preference. To critique and refine the outputs of large language models, Shepherd [232] leverages a high-quality feedback dataset to identify errors and suggest improvements across various domains. In the domain of NLG, X-EVAL [142] consists of a vanilla instruction tuning stage and an enhanced instruction tuning stage that exploits connections between fine-grained evaluation aspects. Notably, Themis [88] also achieved outstanding results acting as a reference-free NLG evaluation language model designed for flexibility and interpretability. Similarly, CritiqueLLM [106] provides effective and explainable evaluations of LLM outputs, and uses a dialogue-based prompting method to generate high-quality referenced and reference-free evaluation data. Self-Rationalization [220] enhances LLM performance by iteratively fine-tuning the judge via DPO, which allows LLMs to learn from their own reasoning. Based on pointwise and pairwise dataset, CompassJudger-1 [20] acts as an open-source, versatile LLM for efficient and accurate evaluation of other LLMs. Likewise, Zhou et al. [294] introduces a systematic framework for bias reduction, employing calibration for closed-source models and contrastive training for open-source models. Apart from that, HALU-J [227] is designed to enhance hallucination detection in LLMs by selecting pertinent evidence and providing detailed critiques. PROMETHEUS [109] and PROMETHEUS 2 [110] are open-source LLMs specialized for fine-grained evaluation that can generalize to diverse, real-world scoring rubrics beyond a single-dimensional preference, supporting both direct assessment and pairwise ranking, and can evaluate based on custom criteria. What's more, the following PROMETHEUS-VISION [122] fills the gap in the visual field. As for</p>
<p>Table 1. An Overview of Fine-Tuning Methods in Single-LLM Evaluation (Sorted in ascending alphabetical order).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Data Construction</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Tuning Method</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Base LLM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Annotator</td>
<td style="text-align: center;">Domain</td>
<td style="text-align: center;">Scale</td>
<td style="text-align: center;">Evaluation Type</td>
<td style="text-align: center;">Technique</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AIES [188]</td>
<td style="text-align: center;">Human \&amp; LLM</td>
<td style="text-align: center;">RAG System</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">PPI</td>
<td style="text-align: center;">DeBERTa-v3-Large</td>
</tr>
<tr>
<td style="text-align: center;">AttzScore [276]</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">63.8 K</td>
<td style="text-align: center;">Pointwise</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">Multiple LLMs</td>
</tr>
<tr>
<td style="text-align: center;">AUTO-J [130]</td>
<td style="text-align: center;">Human \&amp; GPT-4</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">4396</td>
<td style="text-align: center;">Pointwise \&amp; Pairwise</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">Llama2-13B-Chat</td>
</tr>
<tr>
<td style="text-align: center;">CompassJudger-1 [20]</td>
<td style="text-align: center;">Human \&amp; LLM</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">900 K</td>
<td style="text-align: center;">Pointwise, Pairwise, <br> \&amp; Generative</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">Qwen2.3 Series</td>
</tr>
<tr>
<td style="text-align: center;">Con-J [270]</td>
<td style="text-align: center;">Human \&amp; ChatGPT</td>
<td style="text-align: center;">Creation, Math, \&amp; Code</td>
<td style="text-align: center;">220 K</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">SFT \&amp; DPO</td>
<td style="text-align: center;">Qwen2-7B-Instruct</td>
</tr>
<tr>
<td style="text-align: center;">CritiqueLLM [106]</td>
<td style="text-align: center;">Human \&amp; GPT-4</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">7722</td>
<td style="text-align: center;">Pointwise \&amp; Pairwise</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">ChatGLM3-6B</td>
</tr>
<tr>
<td style="text-align: center;">ECT [229]</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">Machine Translation, Text Style Transfer, \&amp; Summarization</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Pointwise</td>
<td style="text-align: center;">SFT \&amp; RLHF</td>
<td style="text-align: center;">RoBERTa</td>
</tr>
<tr>
<td style="text-align: center;">FedEval-LLM [84]</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;"><code>Instruct-tuning &amp; Summary</code></td>
<td style="text-align: center;">5K, 10K, per client</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">LoRA</td>
<td style="text-align: center;">Llama-7B</td>
</tr>
<tr>
<td style="text-align: center;">FENCE [252]</td>
<td style="text-align: center;">Human \&amp; LLM</td>
<td style="text-align: center;"><code>Summarization, QA, &amp; Dialogue</code></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Pointwise</td>
<td style="text-align: center;">SFT \&amp; DPO</td>
<td style="text-align: center;">Llama3-8B-Chat</td>
</tr>
<tr>
<td style="text-align: center;">FLAMe [226]</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">5.3 M</td>
<td style="text-align: center;">Pointwise, Pairwise, Classification, <br> \&amp; Open-ended generation</td>
<td style="text-align: center;">RLHF</td>
<td style="text-align: center;">PaLM-2-24B</td>
</tr>
<tr>
<td style="text-align: center;">HALU-J [227]</td>
<td style="text-align: center;">GPT-4-Turbo <br> \&amp; GPT-5.5-Turbo</td>
<td style="text-align: center;">Multiple-Evidence <br> Hallucination Detection</td>
<td style="text-align: center;">2663</td>
<td style="text-align: center;">Pointwise \&amp; Pairwise</td>
<td style="text-align: center;">SFT \&amp; DPO</td>
<td style="text-align: center;">Mistral-7B-Instruct</td>
</tr>
<tr>
<td style="text-align: center;">HelpSteer2 [237]</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Pointwise \&amp; Pairwise</td>
<td style="text-align: center;">PPI \&amp; RLHF</td>
<td style="text-align: center;">Llama3.1-70B-Instruct</td>
</tr>
<tr>
<td style="text-align: center;">INSTRUCTSCORE [258]</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">40 K</td>
<td style="text-align: center;">Pointwise \&amp; Pairwise</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">Llama-7B</td>
</tr>
<tr>
<td style="text-align: center;">JudgeLM [301]</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">Open-ended Tasks</td>
<td style="text-align: center;">100 K</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">Vicuna Series</td>
</tr>
<tr>
<td style="text-align: center;">LLaVA-Critic [253]</td>
<td style="text-align: center;">GPT-4e</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">113 K</td>
<td style="text-align: center;">Pointwise \&amp; Pairwise</td>
<td style="text-align: center;">DPO</td>
<td style="text-align: center;">LLaVA-OneVision(OV) 7B \&amp; 72B</td>
</tr>
<tr>
<td style="text-align: center;">Meta-Rewarding [285]</td>
<td style="text-align: center;">Llama3</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">20 K</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">DPO</td>
<td style="text-align: center;">Llama3-8B-Instruct</td>
</tr>
<tr>
<td style="text-align: center;">OffsetBias [174]</td>
<td style="text-align: center;">Human \&amp; LLM</td>
<td style="text-align: center;">Bias Detection</td>
<td style="text-align: center;">268 K</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">RLHF</td>
<td style="text-align: center;">Llama3-8B-Instruct</td>
</tr>
<tr>
<td style="text-align: center;">FunduLM [236]</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">300 K</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">Llama-7B</td>
</tr>
<tr>
<td style="text-align: center;">PHUDA2 [47]</td>
<td style="text-align: center;">Human \&amp; GPT-4</td>
<td style="text-align: center;">NLG</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Pointwise \&amp; Pairwise</td>
<td style="text-align: center;">LoRA</td>
<td style="text-align: center;">Plo-3</td>
</tr>
<tr>
<td style="text-align: center;">PROMETHEUS [109]</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">100 K</td>
<td style="text-align: center;">Pointwise</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">Llama2-Chat-7B \&amp; 13B</td>
</tr>
<tr>
<td style="text-align: center;">PROMETHEUS2 [118]</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">300 K</td>
<td style="text-align: center;">Pointwise \&amp; Pairwise</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">Mistral-7B \&amp; Mistral-8a7B</td>
</tr>
<tr>
<td style="text-align: center;">PROMETHEUS-VISION [122]</td>
<td style="text-align: center;">GPT-4V</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">15K</td>
<td style="text-align: center;">Pointwise</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">Lleva-1.5</td>
</tr>
<tr>
<td style="text-align: center;">SELF-J [266]</td>
<td style="text-align: center;">Human \&amp; GPT-4</td>
<td style="text-align: center;">Common, Coding, \&amp; Academic</td>
<td style="text-align: center;">5.7 M</td>
<td style="text-align: center;">Pointwise</td>
<td style="text-align: center;">LoRA</td>
<td style="text-align: center;">Llama2-13B</td>
</tr>
<tr>
<td style="text-align: center;">Self-Rationalization [220]</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Pointwise \&amp; Pairwise</td>
<td style="text-align: center;">SFT \&amp; DPO</td>
<td style="text-align: center;">Llama3.1-8B-Instruct</td>
</tr>
<tr>
<td style="text-align: center;">Self-Taught [231]</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">20 K</td>
<td style="text-align: center;">Pairwise</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Llama3-70B-Instruct</td>
</tr>
<tr>
<td style="text-align: center;">Shepherd [232]</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Various</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Pointwise \&amp; Pairwise</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">Llama-7B</td>
</tr>
<tr>
<td style="text-align: center;">SorryBench [249]</td>
<td style="text-align: center;">Human \&amp; GPT-4</td>
<td style="text-align: center;">Unsafe Topics</td>
<td style="text-align: center;">2.7 K</td>
<td style="text-align: center;">Pointwise</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">Multiple LLMs</td>
</tr>
<tr>
<td style="text-align: center;">Themis [59]</td>
<td style="text-align: center;">Human \&amp; GPT-4</td>
<td style="text-align: center;">NLG</td>
<td style="text-align: center;">67 K</td>
<td style="text-align: center;">Pointwise \&amp; Pairwise</td>
<td style="text-align: center;">SFT \&amp; DPO</td>
<td style="text-align: center;">Llama3-8B</td>
</tr>
<tr>
<td style="text-align: center;">TIGERScore [99]</td>
<td style="text-align: center;">Human \&amp; GPT-4</td>
<td style="text-align: center;">Text Generation</td>
<td style="text-align: center;">42 K</td>
<td style="text-align: center;">Pointwise</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">Llama2-7B \&amp; 13B</td>
</tr>
<tr>
<td style="text-align: center;">X-EVAL [142]</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">NLG</td>
<td style="text-align: center;">55,602</td>
<td style="text-align: center;">Pointwise \&amp; Pairwise</td>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">Plan-T3</td>
</tr>
</tbody>
</table>
<p>various multimodal tasks, LLaVA-Critic [253] demonstrates its effectiveness in providing reliable evaluation scores and generating reward signals for preference learning, highlighting the potential of open-source LMMs in self-critique and evaluation.
4.1.3 Post-processing. Post-processing involves further refining evaluation results to extract more precise and reliable outcomes. This step typically includes analyzing the initial outputs to identify patterns, inconsistencies, or areas requiring improvement, followed by targeted adjustments and in-depth analysis. By addressing these issues, post-processing ensures that the evaluation results are not only accurate but also aligned with the specific objectives and standards of the task.</p>
<p>Probability Calibration. During the post-hoc process of the model output, some studies use rigorous mathematical derivations to quantify the differences, thereby optimizing them. For instance, Daynauth et al. [45] investigates the discrepancy between human preferences and automated evaluations in language model assessments, particularly employs Bayesian statistics and a t-test to quantify bias towards higher token counts, and develops a recalibration procedure to adjust the GPTScorers. Apart from that, ProbDiff [247] is another novel self-evaluation method for LLMs that assesses model efficacy by computing the probability discrepancy between initial responses and their revised versions. Moreover, Liusie et al. [150] introduces a Product of Experts (PoE) framework for efficient comparative assessment using LLMs, which yield an expression that can be maximized with respect to the underlying set of candidates. This paper proposes two experts, a soft Bradley-Terry expert and a Gaussian expert that has closed-form solutions. Unlike from frameworks above, CRISPR [264] is a novel bias mitigation method for LLMs executing instruction-based tasks,</p>
<p>which identifies and prunes bias neurons with probability calibration, reducing bad performance without compromising pre-existing knowledge.</p>
<p>Text Reprocessing. In LLMs-as-judges, text reprocessing methods are essential for enhancing the accuracy and reliability of evaluation outcomes. Specifically, text processing can improve the evaluation process by integrating multiple evaluation results or outcomes from several rounds of assessment. For example, Sottana et al. [206] employs a multi-round evaluation process. Each round involves scoring model outputs based on specific criteria, with the human and GPT-4 evaluations ranking model performances from best to worst and averaging these rankings to mitigate subjectivity. For the single-response evaluation, AUTO-J [130] employs a "divide-andconquer" strategy. Critiques that either adhere to or deviate from the scenario-specific criteria are consolidated to form a comprehensive evaluation judgment and then generate the final assessment. Consistent with former aforementioned studies, Yan et al. [262] introduces a post-processing method to consolidate the relevance labels generated by LLMs. It demonstrates that this approach effectively combines both the ranking and labeling abilities of LLMs through post-processing. Furthermore, REVISEVAL [281] is a novel evaluation paradigm that enhances the reliability of LLM Judges by generating response-adapted references through text revision capabilities of LLMs. Apart from that, Tessler et al. [214] explores the use of AI as a mediator in democratic deliberation, aiming to help diverse groups find common ground on complex social and political issues. With the goal of maximizing group approval, the researchers developed the "Habermas Machine", which iteratively generate group statements based on individual opinions.</p>
<p>Another category of text reprocessing methods involves task transformation, primarily focusing on the conversion between open-ended and multiple-choice question (MCQ) formats. Ren et al. [186] explores the use of self-evaluation to enhance the selective generation capabilities of LLMs. Specifically, the authors reformulate open-ended generation tasks into token-level prediction tasks, reduce sequence-level scores to token-level scores to improve quality calibration. Conversely, Myrzakhan et al. [161] introduces the Open-LLM-Leaderboard, a new benchmark for evaluating LLMs using open-style questions, which eliminates selection bias and random guessing issues associated with multiple-choice questions. It presents a method to identify suitable open-style questions and validate the correctness of LLM open-style responses against human-annotated ground-truths.</p>
<h1>4.2 Multi-LLM System</h1>
<p>Multi-LLM Evaluation harnesses the collective intelligence of multiple LLMs to bolster the robustness and reliability of evaluations. By either facilitating inter-model communication or independently aggregating their outputs, these systems can effectively mitigate biases, leverage complementary strengths across different models, refine decision-making precision, and foster a more nuanced understanding of complex judgments.
4.2.1 Communication. Communication means the dynamic flow of information between LLMs, which is pivotal for sparking insights and sharing rationales during the judgment process. Recent research has shown that communication among LLMs can enable emergent abilities through their interactions [261], leading to a cohesive decision-making process and better judgment performance. The Multi-LLM system can benefit from LLM interactions in two ways: cooperation and competition.</p>
<p>Cooperation. Multi-LLMs can work together to achieve a common goal with information and rationales sharing through interactions to enhance the overall evaluation process. For example, Zhang et al. [285] proposed an architecture named WideDeep to aggregate information at the LLM's neuro-level. In addition, Xu et al. [261] introduced a multi-agent collaboration strategy that mimics the academic peer review process to enhance complex reasoning in LLMs. The approach involves</p>
<p>agents creating solutions, reviewing each other's work, and revising their initial submissions based on feedback. Similarly, ABSEval [136] utilizes four agents for answer synthesize, critique, execution, and commonsense, to build the overall workflow. Although the cooperation can complement each other's strengths between LLMs to a certain degree, this method still includes the risk of groupthink, where similar models reinforce each other's biases rather than providing diverse insights.</p>
<p>Competition. Multi-LLMs systems can also benefit from competitive or adversarial communication, i.e., LLMs argue or debate to evaluate each other's outputs [22, 132, 158, 286]. Such multi-LLMs systems could be categorized into centralized and decentralized structures [168].</p>
<p>In the centralized structure, a single central LLM acts as the orchestrator of the conversation, highlighting the efficiency of a unified decision-making process. Auto-Arena [286] is such a novel framework that automates the evaluation of LLMs through agent peer battles and committee discussions, aiming to provide timely and reliable assessments. In detail, the framework conducts multi-round debates between LLM candidates, and uses an LLM judge committee to decide the winner. Inspired by courtroom dynamics, Bandi and Harrasse [12] propose two architectures, MORE and SAMRE, which utilize multiple advocates and iterative debates to dynamically assess LLM outputs.</p>
<p>In contrast, the decentralized structure emphasizes a collective intelligence where all models engage in direct communication, promoting a resilient and distributed decision-making structure. In the domain of LLM debates, Moniri et al. [158] introduced a unique automated benchmarking framework, employing another LLM as the judge to assess not only the models' domain knowledge but also their abilities in problem definition and inconsistency recognition. ChatEval [22] is another multi-agent debate framework that utilizes multiple LLMs with diverse role prompts and communication strategies on open-ended questions and traditional NLG tasks, significantly improves evaluation performance compared to single-agent methods. Moreover, PRD [132] applied peer rank and discussion to address issues like self-enhancement and positional bias in current LLM evaluation methods, leading to better alignment with human judgments and a path for fair model capability ranking.
4.2.2 Aggregation. Alternatively, in multi-LLM systems without communication, judgments are independently generated by multiple models, which are subsequently synthesized into a final decision through various aggregation strategies. Techniques such as majority vote, weighted averages, and prioritizing the highest confidence predictions, each play a crucial role. These methods allow each model to assess without interference, and eventually extract and combine the most effective elements from each model's response.</p>
<p>Simple voting methods, such as majority voting, by selecting the most frequent answers, offers a straightforward approach to synthesize evaluations. For example, Badshah et al. [8] introduced a reference-guided verdict method for evaluating free-form text using multiple LLMs as judges. Combining these LLMs through majority vote significantly improves the reliability and accuracy of evaluations, particularly for complex tasks. Furthermore, PoLL [225] demonstrates that using a diverse panel of smaller models as judges through max voting and average pooling is not only an effective method for evaluating LLM performance, but also reduces intra-model bias of a single large model. Language-Model-as-an-Examiner [10] is another benchmarking framework to evaluate the performance of foundation models on open-ended question answering through voting. In the peer-examination mechanism, the LM serves as a knowledgeable examiner that formulates questions and evaluates responses in a reference-free manner. What's more, multi-LLM evaluation could also be used in improving dataset quality. Choi et al. [38] provided an enhanced dataset, MULTI-NEWS+, which is the result of a cleansing strategy leveraging CoT and majority voting to identify and exclude irrelevant documents through LLM-based data annotation.</p>
<p>Weighted scoring aggregation involves assigning different importance to different model outputs, either by aggregating multiple overall scores for the same response or by combining assessments of different aspects of the response to form a comprehensive evaluation. On the one hand, through a peer-review mechanism, PiCO [165] allows LLMs to answer unlabeled questions and evaluate each other without human annotations. It formalizes the evaluation as a constrained optimization problem, maximizing the consistency between LLMs' capabilities and corresponding weights. Likewise, PRE [27, 39] can automatically evaluate LLMs through a peer-review process. It selects qualified LLMs as reviewers through a qualification exam and aggregates their ratings using weights which is proportional to their agreement of humans, demonstrating effectiveness and robustness in evaluating text summarization tasks. In the field of recommendation explanations, Zhang et al. [284] suggests that ensembles like averaging ratings of multiple LLMs can enhance evaluation accuracy and stability. On the other hand, for example, AIME [175] is an evaluation protocol that utilizes multiple LLMs that each with a specific role independently generate an evaluation on separate criteria and then combine them via concatenation. Similarly, a paper introduces HD-EVAL [147], which iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition. By decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, pruning insignificant criteria with attribution, and further decomposing significant criteria, HD-EVAL demonstrates its superiority.</p>
<p>Apart from weighting methods, there are some other advance mathematical aggregation techniques, such as Bayesian methods and graph-based approaches, offering more robust ways to handle uncertainties and inconsistencies across multiple evaluators. Notably, a paper introduces two calibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian Dawid-Skene [70], to address the win rate estimation bias when using many LLMs as evaluators for text generation quality. In addition to that, GED [90] addresses inconsistencies in LLM preference evaluations by leveraging multiple weak evaluators to construct preference graphs, and then utilize DAG structure to ensemble and denoise these graphs for better, non-contradictory evaluation results.</p>
<p>LLM-based aggregation is a grand-new perspective like Fusion-Eval [198]. It's a novel framework that integrates various assistant evaluators using LLMs, each of which specializes in assessing distinct aspects of responses, to enhance the correlation of evaluation scores with human judgments for natural language systems.</p>
<p>In addition to the above direct use of multiple model evaluation, the cascade framework employs a tiered approach, where weaker models are used initially for evaluations, and stronger models are engaged only when higher confidence is required, optimizing resource use and enhancing evaluation precision. Jung et al. [103] proposes "Cascaded Selective Evaluation" to ensure high agreement with human judgments while using cheaper models. Similar to the work above, Huang et al. [92] proposes CascadedEval, a novel method integrating proprietary models, in order to compensate for the limitations of fine-tuned judge models.</p>
<h1>4.3 Human-AI Collaboration System</h1>
<p>Human-AI Collaboration Systems bridge the gap between automated LLM judgments and the essential need for human oversight, particularly in high-stakes domains such as law, healthcare, and education. Human evaluators act either as the ultimate deciders, or as intermediaries who verify and refine model outputs. By incorporating human insights, Hybrid systems can ensure the final judgment is more reliable and aligned with ethical considerations, and empower continuous model improvement through feedback loops.</p>
<p>In many Human-AI Collaboration systems, human evaluators play a vital role during the evaluation process itself, actively collaborating with the LLMs to review and refine the generated outputs. For example, COEVAL[131] introduces a collaborative evaluation pipeline where LLMs generate</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
(c) 2024 Association for Computing Machinery.</p>
<p>XXXX-XXXX/2024/12-ART \$15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
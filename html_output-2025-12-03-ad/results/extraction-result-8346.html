<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8346 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8346</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8346</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-273228678</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.06555v1.pdf" target="_blank">ING-VP: MLLMs cannot Play Easy Vision-based Games Yet</a></p>
                <p><strong>Paper Abstract:</strong> As multimodal large language models (MLLMs) continue to demonstrate increasingly competitive performance across a broad spectrum of tasks, more intricate and comprehensive benchmarks have been developed to assess these cutting-edge models. These benchmarks introduce new challenges to core capabilities such as perception, reasoning, and planning. However, existing multimodal benchmarks fall short in providing a focused evaluation of multi-step planning based on spatial relationships in images. To bridge this gap, we present ING-VP, the first INteractive Game-based Vision Planning benchmark, specifically designed to evaluate the spatial imagination and multi-step reasoning abilities of MLLMs. ING-VP features 6 distinct games, encompassing 300 levels, each with 6 unique configurations. A single model engages in over 60,000 rounds of interaction. The benchmark framework allows for multiple comparison settings, including image-text vs. text-only inputs, single-step vs. multi-step reasoning, and with-history vs. without-history conditions, offering valuable insights into the model's capabilities. We evaluated numerous state-of-the-art MLLMs, with the highest-performing model, Claude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the anticipated standard. This work aims to provide a specialized evaluation framework to drive advancements in MLLMs' capacity for complex spatial reasoning and planning. The code is publicly available at https://github.com/Thisisus7/ING-VP.git.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8346.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8346.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.5Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source multimodal large language model (Anthropic) evaluated in this paper across six vision-based puzzle games (Sokoban, Maze, Sudoku, 8-queens, Tower of Hanoi, 15-puzzle) under image-text and text-only, one-step and multi-step, with/without-history settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3.5Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal LLM from Anthropic; used via official API in this paper (no architecture/training details provided in-paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic and planning puzzles requiring precise 2D spatial relations (Sokoban, Maze, 15-puzzle, Sudoku, 8-queens) and stack-based planning (Tower of Hanoi).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot evaluation. Models given either an image + text representation or text-only representation of state; asked to produce actions in a constrained JSON format. Experiments run as interactive multi-turn episodes until task completion or step budget exhausted. Settings: one-step (output full plan) vs multi-step (one move per round), with-history vs without-history.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No model-specific learned planner introduced; authors used prompt-constrained JSON outputs. For multi-step outputs the paper experimented with 'Step-wise Best-of-N' (BoN) and 'Forced Planning' as decoding/elicitation strategies; models otherwise operate zero-shot. Observed strategies: Claude-3.5Sonnet tends to produce longer-than-necessary instruction sequences and exhibits a fixed sampling behaviour (repeats same invalid action in multi-step setting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregate reported average accuracy for Claude-3.5Sonnet across ING-VP = 3.37% (paper-stated average). Additional reported metrics (per Table 1) include completion degree and action efficiency (Table 1 reports these per-model but the paper cites the 3.37% average accuracy as headline).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Detailed error analysis of 555 Claude-3.5Sonnet errors (image-text: 279, text-only: 276) shows high rates of perceptual errors (~55.2% of relevant errors in image-text) and planning errors (~41.9%); models can identify elements and counts but fail to reliably determine precise relative positions. Authors interpret these results as evidence that models do not reliably form correct spatial representations for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Outperforms many open-source models but still far below human performance (humans easily complete most tasks; exception: 8-queens). Compared with GPT-4o, Claude-3.5Sonnet had better one-step performance but worse multi-step due to sampling determinism; text-only often outperforms image-text across models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Major failure modes: inability to process precise relative positions; high perceptual error rate in image-text setting; planning errors over multi-step sequences (losing track of state); tendency to output overly long instruction sequences; in multi-step setting Claude often repeated the same invalid action until attempts exhausted; rarely used provided undo option in tasks with history.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8346.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source multimodal model from OpenAI evaluated on ING-VP; included in comparisons across the six puzzle games and the multiple experimental settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source OpenAI multimodal model; used via official API in the study (no architecture/parameter count given in-paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic and planning puzzles requiring 2D spatial relations and stepwise planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same ING-VP zero-shot interactive setup (image-text / text-only; one-step / multi-step; with/without-history). Outputs constrained to JSON and extracted by regex for environment execution.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot outputs under constrained JSON prompts. Comparison experiments with contraposed sampling behaviours (GPT-4o more flexible / diverse outputs vs. Claude more fixed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports GPT-4o among the best closed-source models; an explicit example value in text: GPT-4o overall accuracy ranks above many open-source models (specific headline numbers: GPT-4o reported with higher multi-step robustness than Claude in some settings). Exact overall accuracy for GPT-4o is reported in Table 1 (see paper Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative observation: GPT-4o can often identify elements but still fails at precise location judgments; declines in accuracy with increased plan length (planning capacity analysis on Maze shows sharp drop as steps increase).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to Claude-3.5Sonnet: GPT-4o produced more diverse responses and outperformed Claude in multi-step settings (Claude's fixed sampling harmed multi-turn recovery). Outperforms most open-source models but remains far below humans.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles with precise relative location estimation; planning degrades with increased number of required steps; action efficiency may be higher than accuracy (i.e., many actions change state without leading to completion).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8346.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4v</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's visual-capable GPT-4 variant evaluated in ING-VP; tested on image-text puzzles in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal OpenAI model (visual-capable GPT-4 variant); used via API in experiments (no parameter count given in-paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Vision + planning puzzles requiring spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Image-text zero-shot IGNV-P settings with forced JSON outputs. Evaluated in the same one-step and multi-step settings.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot visual-to-action generation under constrained prompt format; sometimes failed to adhere to response format or refused responses in the authors' runs, limiting retrieval of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper explicitly states GPT-4V had very low accuracy (example: reported overall accuracy ~0.32% in headline comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Observed to have significant failures in image comprehension and/or compliance with output format; performance indicates limited reliable spatial reasoning in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs worse than top closed-source models (Claude-3.5Sonnet, GPT-4o) and many open-source models in some settings; had operational issues (refusal or format noncompliance) that hindered evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Refused to respond or failed to adhere to required response format in some runs, preventing output extraction; extremely low completion/accuracy on ING-VP.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8346.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o Mini</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller, cost-efficient OpenAI GPT-4o variant included in evaluation; reported to have low accuracy on the ING-VP puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source, smaller GPT-4o variant from OpenAI; used via official API for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based vision planning puzzles requiring spatial knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot image-text and text-only interactive sessions with JSON response enforcement, both one-step and multi-step settings.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot prompting; no special internal planning techniques applied by the authors for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports GPT-4o mini overall accuracy ~1.05% (headline comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Low accuracy indicates limited spatial planning capability on ING-VP tasks; text-only vs image-text differences remain consistent with other models (text-only often better).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs worse than GPT-4o, Claude-3.5Sonnet and top open-source model InternVL2-Llama3-76B in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Generally fails to generate sufficiently precise spatial instructions; low completion and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8346.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.5 Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source model (DeepMind/Google) included in the ING-VP evaluation suite and measured across all game types and settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.5Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal model from DeepMind/Google; used via official API per paper (no parameter count provided in-paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Deterministic grid and planning puzzles needing spatial judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot interactive benchmark with identical prompt suite and JSON output constraints; image-text and text-only conditions, one-step/multi-step, with/without history.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot response generation; no model-specific tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper lists Gemini-1.5Pro among closed-source evaluated models; specific numeric accuracies/metrics are recorded in Table 1 (paper). Headline comparisons indicate it is among the better closed-source models but still low absolute accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Similar behavior to other closed-source models: can identify some elements but cannot reliably derive precise relative positions for planning across multiple steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs comparably to GPT-4o and Claude variants in some metrics; still far below human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails on precise positional reasoning and long-horizon planning; action efficiency often higher than accuracy, indicating state-changing but non-progressing actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8346.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternVL2-Llama3-76B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVL2-Llama3-76B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal model (Shanghai AI Lab) and the best-performing open-source model on ING-VP in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternVL2-Llama3-76B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal model checkpoint (name implies Llama3 backbone); used from public checkpoint (link given in Appendix A).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>76B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based logic and planning puzzles requiring spatial representations and stepwise planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot evaluation with identical prompts (JSON outputs). Evaluated in image-text and text-only, one-step/multi-step, with/without-history settings.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot generation; no additional chain-of-thought or internal planning technique beyond variations in prompt settings and multi-step decoding strategies (BoN/Forced Planning explored at dataset level).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported overall accuracy = 2.50% (paper-stated headline for best open-source model). Completion degree and action efficiency are reported per Table 1 for this model in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Although among the best open-source models, the low absolute accuracy shows limited spatial imagination and planning; text-only > image-text trend holds.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Ranks behind top closed-source models (Claude-3.5Sonnet, GPT-4o) but ahead of many other open-source models (Internvl2-40B, Internvl2-26B, Internvl2-8B).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same general failure modes: inability to precisely localize elements, poor multi-step planning, and producing longer-than-necessary action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8346.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Internvl2-40B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVL2-40B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source multimodal model included in the study's open-source suite; evaluated identically to other models on ING-VP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Internvl2-40B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal checkpoint referenced in Appendix A; used directly in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based puzzles requiring spatial knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot, JSON constrained outputs; image-text and text-only one-step/multi-step settings.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot prompt-driven generation; no special planning module.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Per-model metrics are reported in Table 1; overall accuracies are low (order of a few percent or less) and completion/action-efficiency metrics are presented in-paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Model exhibits same qualitative failures as other open-source models (perceptual location errors, planning breakdowns).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms compared to top closed-source models and the best open-source InternVL2-Llama3-76B.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails at precise position judgments and multi-step plan maintenance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8346.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Internvl2-26B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVL2-26B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source multimodal model (26B) evaluated on ING-VP; part of the InternVL2 family tested in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Internvl2-26B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal model; used as a public checkpoint in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>26B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based planning and spatial puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot interactive evaluation with JSON output constraints and the same experimental conditions as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt-driven zero-shot responses; no additional planning techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 1 (low accuracies, completion degree and efficiency metrics available in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Similar qualitative failure patterns: poor localization, planning instability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs below the larger InternVL2-Llama3-76B and below closed-source leaders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limited multi-step planning and spatial localization capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8346.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Internvl2-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVL2-8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small open-source multimodal model (8B) in the InternVL2 family evaluated on ING-VP; included to show scale-related performance differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Internvl2-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal model checkpoint (8B parameters); evaluated in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid planning / combinatorial placement puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot image-text / text-only interactive tasks with JSON-constrained outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No special strategies; standard zero-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 1: very low accuracy (order of single percent or less); specific numbers in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Shows stronger limitations than larger models; fails most levels.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms larger InternVL2 variants and closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Severe difficulty with perceptual localization and multi-step planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8346.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternVL-Chat-v1.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVL-Chat-v1.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source conversational multimodal model variant included in evaluations, to assess chat/interactive performance on puzzle tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternVL-Chat-v1.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source chat-capable multimodal model checkpoint; used in the interactive multi-turn evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Interactive grid- and placement-based puzzles requiring spatial planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Multi-turn interactive evaluation emphasized for chat models; same JSON output constraints and history/no-history settings applied.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard chat-style zero-shot prompting; no extra internal planning mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 1 with low accuracies (paper gives full per-model breakdown); overall performance below top closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Limited; frequently fails to track state across turns and to produce correct spatial actions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Comparable to other mid/low-tier open-source models; lower than InternVL2-Llama3-76B.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Poor handling of precise positional information and multi-step plan maintenance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8346.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CogVLM2-19B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CogVLM2-Llama3-chat-19B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source multimodal model (CogVLM2 family) evaluated in the benchmark; included to represent another public VLM baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CogVLM2-19B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal model (name/size indicated in Appendix A); used as a public checkpoint in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>19B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid-based and combinatorial spatial puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot interactive evaluation, JSON output format, same settings as other open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt-driven generation; no added planning augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 1: low overall accuracy and completion rates (paper contains detailed per-model metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Exhibits same failure modes: inaccurate positional perception and planning breakdowns in multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs on par with other mid-tier open-source VLMs; lower than InternVL2-Llama3-76B and closed-source leaders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails at fine-grained spatial judgments and sustained multi-step planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8346.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-VL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-VL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source vision-language model (DeepSeek-AI) included in the benchmark evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-VL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source VLM (referenced from DeepSeek-AI in Appendix A); used directly in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Vision-based planning tasks requiring spatial inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Standard ING-VP zero-shot, JSON-constrained interactive evaluation across image-text/text-only and one/multi-step settings.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Zero-shot prompting; no extra planning modules added by authors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 1 with low accuracies and modest action-efficiency numbers (detailed per-model metrics in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Limited; same qualitative shortcomings as other open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs similarly to other open-source VLMs and below closed-source leaders.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Perception of precise locations and multi-step plan fidelity are weak.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8346.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MiniCPM-V2.6</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MiniCPM-V2.6</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multimodal model included in the ING-VP evaluation suite.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MiniCPM-V2.6</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source multimodal model (referenced and linked in Appendix A); used as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Grid and combinatorial spatial puzzles requiring exact relative position understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot interactive evaluation with prompts requiring JSON outputs; one-step and multi-step experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>No additional strategies beyond zero-shot prompting and the standard BoN/Forced Planning experiments at benchmark level.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Low accuracy and completion rates reported in Table 1 (paper contains the numeric breakdown).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Limited; fails to reliably perceive detailed positions in images and to plan over multiple steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Performs similarly to other small/mid open-source models and worse than larger open- and closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails on precise location interpretation and long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e8346.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Another Claude family closed-source model (Anthropic) included in the evaluation suite for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-3 Opus</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source Anthropic multimodal model; used in experiments (no explicit size/architecture details in-paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Visual planning and combinatorial puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot interactive experiments with JSON formatting enforced; identical task conditions to other closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt-driven zero-shot responses; no additional planning augmentations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Table 1; accuracy and completion metrics are low compared to human performance and vary across settings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Shows typical closed-source behavior: some object recognition but poor fine-grained positional reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Generally below Claude-3.5Sonnet in headline metrics but comparative ranking depended on specific setting (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Perceptual and planning errors similar to other models; difficulty with multi-step state tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e8346.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI closed-source model included in the model suite for ING-VP comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source GPT-4 family model from OpenAI used via API in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D grid and combinatorial puzzles requiring spatial planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot, JSON-constrained interactive evaluation across image-text/text-only and one-step/multi-step settings.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Standard zero-shot response generation; no special planning augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Metrics for GPT-4 Turbo appear in Table 1; overall accuracy low and comparable to other closed-source variants (detailed breakdown in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Exhibits the same limitations: perceptual location errors and planning breakdowns on longer sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Comparable to other GPT-4 family variants in many settings but still far from human-level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Perceptual imprecision and limited multi-step planning capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8346.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e8346.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summary-ING-VP-benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ING-VP (INteractive Game-based Vision Planning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark introduced in this paper to evaluate spatial imagination and multi-step reasoning of MLLMs using six puzzle games and multiple controlled settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ING-VP benchmark (task suite)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model; benchmark consisting of 6 games  50 levels each (300 levels total), with both image and text representations, multiple experimental settings (image-text vs text-only, one-step vs multi-step, with-history vs without-history).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Set of vision-based planning/logic puzzles requiring spatial representation and multi-step planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Zero-shot interactive sessions; responses constrained to JSON and parsed by environment; metrics: accuracy (main), completion degree, and action efficiency. Step budgets limited (authors constrained optimal solutions to 8 moves for level design).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Benchmark-level experiments include Step-wise Best-of-N (BoN) and Forced Planning decoding strategies, with explicit analysis of one-step vs multi-step elicitation effects and prompt sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Across >60,000 round interactions, top performing model Claude-3.5Sonnet achieved average accuracy 3.37%; best open-source model InternVL2-Llama3-76B achieved 2.50%; many models had accuracies near or below single-digit percentages. Completion degree and action efficiency reported per-model in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Benchmark analyses show consistent failures in precise relative location perception and multi-step planning across models; error breakdowns and planning-capacity analyses (e.g., Maze with 4/12/16 steps) provide evidence that spatial planning degrades strongly with longer horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Paper compares closed-source vs open-source models, image-text vs text-only, one-step vs multi-step, and with/without-history; human baseline (average human easily solves tasks except 8-queens) is far above models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Design choices include level difficulty selection (no grading included) and not feeding previous-state images in multi-step with-history to keep evaluation efficient (noted as limitation); models frequently failed perceptually and in planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns <em>(Rating: 2)</em></li>
                <li>Visualization-of-thought elicits spatial reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Is your multi-modal model an all-around player? <em>(Rating: 1)</em></li>
                <li>A benchmark for LLMs as intelligent agents (Smartplay / related game-based evaluations) <em>(Rating: 1)</em></li>
                <li>LVLM-eHub / LAMM / other multimodal benchmark papers (collection cited in ING-VP related work) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8346",
    "paper_id": "paper-273228678",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "Claude-3.5Sonnet",
            "name_full": "Claude-3.5 Sonnet",
            "brief_description": "A closed-source multimodal large language model (Anthropic) evaluated in this paper across six vision-based puzzle games (Sokoban, Maze, Sudoku, 8-queens, Tower of Hanoi, 15-puzzle) under image-text and text-only, one-step and multi-step, with/without-history settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3.5Sonnet",
            "model_description": "Closed-source multimodal LLM from Anthropic; used via official API in this paper (no architecture/training details provided in-paper).",
            "model_size": null,
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "Grid-based logic and planning puzzles requiring precise 2D spatial relations (Sokoban, Maze, 15-puzzle, Sudoku, 8-queens) and stack-based planning (Tower of Hanoi).",
            "task_setup": "Zero-shot evaluation. Models given either an image + text representation or text-only representation of state; asked to produce actions in a constrained JSON format. Experiments run as interactive multi-turn episodes until task completion or step budget exhausted. Settings: one-step (output full plan) vs multi-step (one move per round), with-history vs without-history.",
            "mechanisms_or_strategies": "No model-specific learned planner introduced; authors used prompt-constrained JSON outputs. For multi-step outputs the paper experimented with 'Step-wise Best-of-N' (BoN) and 'Forced Planning' as decoding/elicitation strategies; models otherwise operate zero-shot. Observed strategies: Claude-3.5Sonnet tends to produce longer-than-necessary instruction sequences and exhibits a fixed sampling behaviour (repeats same invalid action in multi-step setting).",
            "performance_metrics": "Aggregate reported average accuracy for Claude-3.5Sonnet across ING-VP = 3.37% (paper-stated average). Additional reported metrics (per Table 1) include completion degree and action efficiency (Table 1 reports these per-model but the paper cites the 3.37% average accuracy as headline).",
            "evidence_of_spatial_reasoning": "Detailed error analysis of 555 Claude-3.5Sonnet errors (image-text: 279, text-only: 276) shows high rates of perceptual errors (~55.2% of relevant errors in image-text) and planning errors (~41.9%); models can identify elements and counts but fail to reliably determine precise relative positions. Authors interpret these results as evidence that models do not reliably form correct spatial representations for planning.",
            "comparisons": "Outperforms many open-source models but still far below human performance (humans easily complete most tasks; exception: 8-queens). Compared with GPT-4o, Claude-3.5Sonnet had better one-step performance but worse multi-step due to sampling determinism; text-only often outperforms image-text across models.",
            "limitations_or_failure_cases": "Major failure modes: inability to process precise relative positions; high perceptual error rate in image-text setting; planning errors over multi-step sequences (losing track of state); tendency to output overly long instruction sequences; in multi-step setting Claude often repeated the same invalid action until attempts exhausted; rarely used provided undo option in tasks with history.",
            "uuid": "e8346.0",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A closed-source multimodal model from OpenAI evaluated on ING-VP; included in comparisons across the six puzzle games and the multiple experimental settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Closed-source OpenAI multimodal model; used via official API in the study (no architecture/parameter count given in-paper).",
            "model_size": null,
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "Grid-based logic and planning puzzles requiring 2D spatial relations and stepwise planning.",
            "task_setup": "Same ING-VP zero-shot interactive setup (image-text / text-only; one-step / multi-step; with/without-history). Outputs constrained to JSON and extracted by regex for environment execution.",
            "mechanisms_or_strategies": "Zero-shot outputs under constrained JSON prompts. Comparison experiments with contraposed sampling behaviours (GPT-4o more flexible / diverse outputs vs. Claude more fixed).",
            "performance_metrics": "Paper reports GPT-4o among the best closed-source models; an explicit example value in text: GPT-4o overall accuracy ranks above many open-source models (specific headline numbers: GPT-4o reported with higher multi-step robustness than Claude in some settings). Exact overall accuracy for GPT-4o is reported in Table 1 (see paper Table 1).",
            "evidence_of_spatial_reasoning": "Qualitative observation: GPT-4o can often identify elements but still fails at precise location judgments; declines in accuracy with increased plan length (planning capacity analysis on Maze shows sharp drop as steps increase).",
            "comparisons": "Compared to Claude-3.5Sonnet: GPT-4o produced more diverse responses and outperformed Claude in multi-step settings (Claude's fixed sampling harmed multi-turn recovery). Outperforms most open-source models but remains far below humans.",
            "limitations_or_failure_cases": "Struggles with precise relative location estimation; planning degrades with increased number of required steps; action efficiency may be higher than accuracy (i.e., many actions change state without leading to completion).",
            "uuid": "e8346.1",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4v",
            "name_full": "GPT-4V",
            "brief_description": "OpenAI's visual-capable GPT-4 variant evaluated in ING-VP; tested on image-text puzzles in the benchmark.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4V",
            "model_description": "Closed-source multimodal OpenAI model (visual-capable GPT-4 variant); used via API in experiments (no parameter count given in-paper).",
            "model_size": null,
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "Vision + planning puzzles requiring spatial reasoning.",
            "task_setup": "Image-text zero-shot IGNV-P settings with forced JSON outputs. Evaluated in the same one-step and multi-step settings.",
            "mechanisms_or_strategies": "Zero-shot visual-to-action generation under constrained prompt format; sometimes failed to adhere to response format or refused responses in the authors' runs, limiting retrieval of outputs.",
            "performance_metrics": "Paper explicitly states GPT-4V had very low accuracy (example: reported overall accuracy ~0.32% in headline comparisons).",
            "evidence_of_spatial_reasoning": "Observed to have significant failures in image comprehension and/or compliance with output format; performance indicates limited reliable spatial reasoning in these tasks.",
            "comparisons": "Performs worse than top closed-source models (Claude-3.5Sonnet, GPT-4o) and many open-source models in some settings; had operational issues (refusal or format noncompliance) that hindered evaluation.",
            "limitations_or_failure_cases": "Refused to respond or failed to adhere to required response format in some runs, preventing output extraction; extremely low completion/accuracy on ING-VP.",
            "uuid": "e8346.2",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o mini",
            "name_full": "GPT-4o Mini",
            "brief_description": "A smaller, cost-efficient OpenAI GPT-4o variant included in evaluation; reported to have low accuracy on the ING-VP puzzles.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o mini",
            "model_description": "Closed-source, smaller GPT-4o variant from OpenAI; used via official API for evaluation.",
            "model_size": null,
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "Grid-based vision planning puzzles requiring spatial knowledge.",
            "task_setup": "Zero-shot image-text and text-only interactive sessions with JSON response enforcement, both one-step and multi-step settings.",
            "mechanisms_or_strategies": "Zero-shot prompting; no special internal planning techniques applied by the authors for this model.",
            "performance_metrics": "Paper reports GPT-4o mini overall accuracy ~1.05% (headline comparison).",
            "evidence_of_spatial_reasoning": "Low accuracy indicates limited spatial planning capability on ING-VP tasks; text-only vs image-text differences remain consistent with other models (text-only often better).",
            "comparisons": "Performs worse than GPT-4o, Claude-3.5Sonnet and top open-source model InternVL2-Llama3-76B in this benchmark.",
            "limitations_or_failure_cases": "Generally fails to generate sufficiently precise spatial instructions; low completion and accuracy.",
            "uuid": "e8346.3",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Gemini-1.5Pro",
            "name_full": "Gemini-1.5 Pro",
            "brief_description": "A closed-source model (DeepMind/Google) included in the ING-VP evaluation suite and measured across all game types and settings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-1.5Pro",
            "model_description": "Closed-source multimodal model from DeepMind/Google; used via official API per paper (no parameter count provided in-paper).",
            "model_size": null,
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "Deterministic grid and planning puzzles needing spatial judgments.",
            "task_setup": "Zero-shot interactive benchmark with identical prompt suite and JSON output constraints; image-text and text-only conditions, one-step/multi-step, with/without history.",
            "mechanisms_or_strategies": "Zero-shot response generation; no model-specific tuning reported.",
            "performance_metrics": "Paper lists Gemini-1.5Pro among closed-source evaluated models; specific numeric accuracies/metrics are recorded in Table 1 (paper). Headline comparisons indicate it is among the better closed-source models but still low absolute accuracy.",
            "evidence_of_spatial_reasoning": "Similar behavior to other closed-source models: can identify some elements but cannot reliably derive precise relative positions for planning across multiple steps.",
            "comparisons": "Performs comparably to GPT-4o and Claude variants in some metrics; still far below human baseline.",
            "limitations_or_failure_cases": "Fails on precise positional reasoning and long-horizon planning; action efficiency often higher than accuracy, indicating state-changing but non-progressing actions.",
            "uuid": "e8346.4",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "InternVL2-Llama3-76B",
            "name_full": "InternVL2-Llama3-76B",
            "brief_description": "An open-source multimodal model (Shanghai AI Lab) and the best-performing open-source model on ING-VP in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InternVL2-Llama3-76B",
            "model_description": "Open-source multimodal model checkpoint (name implies Llama3 backbone); used from public checkpoint (link given in Appendix A).",
            "model_size": "76B",
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "Grid-based logic and planning puzzles requiring spatial representations and stepwise planning.",
            "task_setup": "Zero-shot evaluation with identical prompts (JSON outputs). Evaluated in image-text and text-only, one-step/multi-step, with/without-history settings.",
            "mechanisms_or_strategies": "Zero-shot generation; no additional chain-of-thought or internal planning technique beyond variations in prompt settings and multi-step decoding strategies (BoN/Forced Planning explored at dataset level).",
            "performance_metrics": "Reported overall accuracy = 2.50% (paper-stated headline for best open-source model). Completion degree and action efficiency are reported per Table 1 for this model in the paper.",
            "evidence_of_spatial_reasoning": "Although among the best open-source models, the low absolute accuracy shows limited spatial imagination and planning; text-only &gt; image-text trend holds.",
            "comparisons": "Ranks behind top closed-source models (Claude-3.5Sonnet, GPT-4o) but ahead of many other open-source models (Internvl2-40B, Internvl2-26B, Internvl2-8B).",
            "limitations_or_failure_cases": "Same general failure modes: inability to precisely localize elements, poor multi-step planning, and producing longer-than-necessary action sequences.",
            "uuid": "e8346.5",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Internvl2-40B",
            "name_full": "InternVL2-40B",
            "brief_description": "Open-source multimodal model included in the study's open-source suite; evaluated identically to other models on ING-VP.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Internvl2-40B",
            "model_description": "Open-source multimodal checkpoint referenced in Appendix A; used directly in experiments.",
            "model_size": "40B",
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "Grid-based puzzles requiring spatial knowledge.",
            "task_setup": "Zero-shot, JSON constrained outputs; image-text and text-only one-step/multi-step settings.",
            "mechanisms_or_strategies": "Zero-shot prompt-driven generation; no special planning module.",
            "performance_metrics": "Per-model metrics are reported in Table 1; overall accuracies are low (order of a few percent or less) and completion/action-efficiency metrics are presented in-paper.",
            "evidence_of_spatial_reasoning": "Model exhibits same qualitative failures as other open-source models (perceptual location errors, planning breakdowns).",
            "comparisons": "Underperforms compared to top closed-source models and the best open-source InternVL2-Llama3-76B.",
            "limitations_or_failure_cases": "Fails at precise position judgments and multi-step plan maintenance.",
            "uuid": "e8346.6",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Internvl2-26B",
            "name_full": "InternVL2-26B",
            "brief_description": "Open-source multimodal model (26B) evaluated on ING-VP; part of the InternVL2 family tested in the benchmark.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Internvl2-26B",
            "model_description": "Open-source multimodal model; used as a public checkpoint in experiments.",
            "model_size": "26B",
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "Grid-based planning and spatial puzzles.",
            "task_setup": "Zero-shot interactive evaluation with JSON output constraints and the same experimental conditions as other models.",
            "mechanisms_or_strategies": "Prompt-driven zero-shot responses; no additional planning techniques.",
            "performance_metrics": "Reported in Table 1 (low accuracies, completion degree and efficiency metrics available in paper).",
            "evidence_of_spatial_reasoning": "Similar qualitative failure patterns: poor localization, planning instability.",
            "comparisons": "Performs below the larger InternVL2-Llama3-76B and below closed-source leaders.",
            "limitations_or_failure_cases": "Limited multi-step planning and spatial localization capability.",
            "uuid": "e8346.7",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Internvl2-8B",
            "name_full": "InternVL2-8B",
            "brief_description": "Small open-source multimodal model (8B) in the InternVL2 family evaluated on ING-VP; included to show scale-related performance differences.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Internvl2-8B",
            "model_description": "Open-source multimodal model checkpoint (8B parameters); evaluated in the benchmark.",
            "model_size": "8B",
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "2D grid planning / combinatorial placement puzzles.",
            "task_setup": "Zero-shot image-text / text-only interactive tasks with JSON-constrained outputs.",
            "mechanisms_or_strategies": "No special strategies; standard zero-shot prompting.",
            "performance_metrics": "Reported in Table 1: very low accuracy (order of single percent or less); specific numbers in paper.",
            "evidence_of_spatial_reasoning": "Shows stronger limitations than larger models; fails most levels.",
            "comparisons": "Underperforms larger InternVL2 variants and closed-source models.",
            "limitations_or_failure_cases": "Severe difficulty with perceptual localization and multi-step planning.",
            "uuid": "e8346.8",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "InternVL-Chat-v1.5",
            "name_full": "InternVL-Chat-v1.5",
            "brief_description": "An open-source conversational multimodal model variant included in evaluations, to assess chat/interactive performance on puzzle tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "InternVL-Chat-v1.5",
            "model_description": "Open-source chat-capable multimodal model checkpoint; used in the interactive multi-turn evaluations.",
            "model_size": null,
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "Interactive grid- and placement-based puzzles requiring spatial planning.",
            "task_setup": "Multi-turn interactive evaluation emphasized for chat models; same JSON output constraints and history/no-history settings applied.",
            "mechanisms_or_strategies": "Standard chat-style zero-shot prompting; no extra internal planning mechanisms.",
            "performance_metrics": "Reported in Table 1 with low accuracies (paper gives full per-model breakdown); overall performance below top closed-source models.",
            "evidence_of_spatial_reasoning": "Limited; frequently fails to track state across turns and to produce correct spatial actions.",
            "comparisons": "Comparable to other mid/low-tier open-source models; lower than InternVL2-Llama3-76B.",
            "limitations_or_failure_cases": "Poor handling of precise positional information and multi-step plan maintenance.",
            "uuid": "e8346.9",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CogVLM2-19B",
            "name_full": "CogVLM2-Llama3-chat-19B",
            "brief_description": "Open-source multimodal model (CogVLM2 family) evaluated in the benchmark; included to represent another public VLM baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CogVLM2-19B",
            "model_description": "Open-source multimodal model (name/size indicated in Appendix A); used as a public checkpoint in the experiments.",
            "model_size": "19B",
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "Grid-based and combinatorial spatial puzzles.",
            "task_setup": "Zero-shot interactive evaluation, JSON output format, same settings as other open-source models.",
            "mechanisms_or_strategies": "Prompt-driven generation; no added planning augmentation.",
            "performance_metrics": "Reported in Table 1: low overall accuracy and completion rates (paper contains detailed per-model metrics).",
            "evidence_of_spatial_reasoning": "Exhibits same failure modes: inaccurate positional perception and planning breakdowns in multi-step tasks.",
            "comparisons": "Performs on par with other mid-tier open-source VLMs; lower than InternVL2-Llama3-76B and closed-source leaders.",
            "limitations_or_failure_cases": "Fails at fine-grained spatial judgments and sustained multi-step planning.",
            "uuid": "e8346.10",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DeepSeek-VL",
            "name_full": "DeepSeek-VL",
            "brief_description": "An open-source vision-language model (DeepSeek-AI) included in the benchmark evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeepSeek-VL",
            "model_description": "Open-source VLM (referenced from DeepSeek-AI in Appendix A); used directly in the experiments.",
            "model_size": null,
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "Vision-based planning tasks requiring spatial inference.",
            "task_setup": "Standard ING-VP zero-shot, JSON-constrained interactive evaluation across image-text/text-only and one/multi-step settings.",
            "mechanisms_or_strategies": "Zero-shot prompting; no extra planning modules added by authors.",
            "performance_metrics": "Reported in Table 1 with low accuracies and modest action-efficiency numbers (detailed per-model metrics in paper).",
            "evidence_of_spatial_reasoning": "Limited; same qualitative shortcomings as other open-source models.",
            "comparisons": "Performs similarly to other open-source VLMs and below closed-source leaders.",
            "limitations_or_failure_cases": "Perception of precise locations and multi-step plan fidelity are weak.",
            "uuid": "e8346.11",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MiniCPM-V2.6",
            "name_full": "MiniCPM-V2.6",
            "brief_description": "An open-source multimodal model included in the ING-VP evaluation suite.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MiniCPM-V2.6",
            "model_description": "Open-source multimodal model (referenced and linked in Appendix A); used as a baseline in experiments.",
            "model_size": null,
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "Grid and combinatorial spatial puzzles requiring exact relative position understanding.",
            "task_setup": "Zero-shot interactive evaluation with prompts requiring JSON outputs; one-step and multi-step experiments.",
            "mechanisms_or_strategies": "No additional strategies beyond zero-shot prompting and the standard BoN/Forced Planning experiments at benchmark level.",
            "performance_metrics": "Low accuracy and completion rates reported in Table 1 (paper contains the numeric breakdown).",
            "evidence_of_spatial_reasoning": "Limited; fails to reliably perceive detailed positions in images and to plan over multiple steps.",
            "comparisons": "Performs similarly to other small/mid open-source models and worse than larger open- and closed-source models.",
            "limitations_or_failure_cases": "Fails on precise location interpretation and long-horizon planning.",
            "uuid": "e8346.12",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Claude-3 Opus",
            "name_full": "Claude-3 Opus",
            "brief_description": "Another Claude family closed-source model (Anthropic) included in the evaluation suite for comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-3 Opus",
            "model_description": "Closed-source Anthropic multimodal model; used in experiments (no explicit size/architecture details in-paper).",
            "model_size": null,
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "Visual planning and combinatorial puzzles.",
            "task_setup": "Zero-shot interactive experiments with JSON formatting enforced; identical task conditions to other closed-source models.",
            "mechanisms_or_strategies": "Prompt-driven zero-shot responses; no additional planning augmentations reported.",
            "performance_metrics": "Reported in Table 1; accuracy and completion metrics are low compared to human performance and vary across settings.",
            "evidence_of_spatial_reasoning": "Shows typical closed-source behavior: some object recognition but poor fine-grained positional reasoning.",
            "comparisons": "Generally below Claude-3.5Sonnet in headline metrics but comparative ranking depended on specific setting (see Table 1).",
            "limitations_or_failure_cases": "Perceptual and planning errors similar to other models; difficulty with multi-step state tracking.",
            "uuid": "e8346.13",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4 Turbo",
            "name_full": "GPT-4 Turbo",
            "brief_description": "An OpenAI closed-source model included in the model suite for ING-VP comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 Turbo",
            "model_description": "Closed-source GPT-4 family model from OpenAI used via API in experiments.",
            "model_size": null,
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "2D grid and combinatorial puzzles requiring spatial planning.",
            "task_setup": "Zero-shot, JSON-constrained interactive evaluation across image-text/text-only and one-step/multi-step settings.",
            "mechanisms_or_strategies": "Standard zero-shot response generation; no special planning augmentation.",
            "performance_metrics": "Metrics for GPT-4 Turbo appear in Table 1; overall accuracy low and comparable to other closed-source variants (detailed breakdown in paper).",
            "evidence_of_spatial_reasoning": "Exhibits the same limitations: perceptual location errors and planning breakdowns on longer sequences.",
            "comparisons": "Comparable to other GPT-4 family variants in many settings but still far from human-level performance.",
            "limitations_or_failure_cases": "Perceptual imprecision and limited multi-step planning capacity.",
            "uuid": "e8346.14",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Summary-ING-VP-benchmark",
            "name_full": "ING-VP (INteractive Game-based Vision Planning benchmark)",
            "brief_description": "Benchmark introduced in this paper to evaluate spatial imagination and multi-step reasoning of MLLMs using six puzzle games and multiple controlled settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ING-VP benchmark (task suite)",
            "model_description": "Not a model; benchmark consisting of 6 games  50 levels each (300 levels total), with both image and text representations, multiple experimental settings (image-text vs text-only, one-step vs multi-step, with-history vs without-history).",
            "model_size": null,
            "puzzle_name": "Sokoban; Maze; Sudoku; 8-queens; Tower of Hanoi; 15-puzzle",
            "puzzle_type": "Set of vision-based planning/logic puzzles requiring spatial representation and multi-step planning.",
            "task_setup": "Zero-shot interactive sessions; responses constrained to JSON and parsed by environment; metrics: accuracy (main), completion degree, and action efficiency. Step budgets limited (authors constrained optimal solutions to 8 moves for level design).",
            "mechanisms_or_strategies": "Benchmark-level experiments include Step-wise Best-of-N (BoN) and Forced Planning decoding strategies, with explicit analysis of one-step vs multi-step elicitation effects and prompt sensitivity.",
            "performance_metrics": "Across &gt;60,000 round interactions, top performing model Claude-3.5Sonnet achieved average accuracy 3.37%; best open-source model InternVL2-Llama3-76B achieved 2.50%; many models had accuracies near or below single-digit percentages. Completion degree and action efficiency reported per-model in Table 1.",
            "evidence_of_spatial_reasoning": "Benchmark analyses show consistent failures in precise relative location perception and multi-step planning across models; error breakdowns and planning-capacity analyses (e.g., Maze with 4/12/16 steps) provide evidence that spatial planning degrades strongly with longer horizons.",
            "comparisons": "Paper compares closed-source vs open-source models, image-text vs text-only, one-step vs multi-step, and with/without-history; human baseline (average human easily solves tasks except 8-queens) is far above models.",
            "limitations_or_failure_cases": "Design choices include level difficulty selection (no grading included) and not feeding previous-state images in multi-step with-history to keep evaluation efficient (noted as limitation); models frequently failed perceptually and in planning.",
            "uuid": "e8346.15",
            "source_info": {
                "paper_title": "ING-VP: MLLMs cannot Play Easy Vision-based Games Yet",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns",
            "rating": 2,
            "sanitized_title": "puzzlevqa_diagnosing_multimodal_reasoning_challenges_of_language_models_with_abstract_visual_patterns"
        },
        {
            "paper_title": "Visualization-of-thought elicits spatial reasoning in large language models",
            "rating": 2,
            "sanitized_title": "visualizationofthought_elicits_spatial_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Is your multi-modal model an all-around player?",
            "rating": 1,
            "sanitized_title": "is_your_multimodal_model_an_allaround_player"
        },
        {
            "paper_title": "A benchmark for LLMs as intelligent agents (Smartplay / related game-based evaluations)",
            "rating": 1,
            "sanitized_title": "a_benchmark_for_llms_as_intelligent_agents_smartplay_related_gamebased_evaluations"
        },
        {
            "paper_title": "LVLM-eHub / LAMM / other multimodal benchmark papers (collection cited in ING-VP related work)",
            "rating": 1,
            "sanitized_title": "lvlmehub_lamm_other_multimodal_benchmark_papers_collection_cited_in_ingvp_related_work"
        }
    ],
    "cost": 0.02299925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ING-VP: MLLMS CANNOT PLAY EASY VISION-BASED GAMES YET
9 Oct 2024</p>
<p>Haoran Zhang 
Equal Contributions</p>
<p>Hangyu Guo 
Equal Contributions</p>
<p>Shuyue Guo 
Equal Contributions</p>
<p>Meng Cao 
Equal Contributions</p>
<p>Wenhao Huang 
Equal Contributions</p>
<p>Jiaheng Liu 
Equal Contributions</p>
<p>Ge Zhang 
Equal Contributions</p>
<p>Bytedance Inc 
Equal Contributions</p>
<p>Mbzuai 
Equal Contributions</p>
<p>ING-VP: MLLMS CANNOT PLAY EASY VISION-BASED GAMES YET
9 Oct 2024C832CD8E1E9F240013155C1547DF053DarXiv:2410.06555v1[cs.CL]
As multimodal large language models (MLLMs) continue to demonstrate increasingly competitive performance across a broad spectrum of tasks, more intricate and comprehensive benchmarks have been developed to assess these cutting-edge models.These benchmarks introduce new challenges to core capabilities such as perception, reasoning, and planning.However, existing multimodal benchmarks fall short in providing a focused evaluation of multi-step planning based on spatial relationships in images.To bridge this gap, we present ING-VP, the first INteractive Game-based Vision Planning benchmark, specifically designed to evaluate the spatial imagination and multi-step reasoning abilities of MLLMs.ING-VP features 6 distinct games, encompassing 300 levels, each with 6 unique configurations.A single model engages in over 60,000 rounds of interaction.The benchmark framework allows for multiple comparison settings, including image-text vs. text-only inputs, single-step vs. multi-step reasoning, and with-history vs. without-history conditions, offering valuable insights into the model's capabilities.We evaluated numerous state-of-the-art MLLMs, with the highest-performing model, Claude-3.5Sonnet, achieving an average accuracy of only 3.37%, far below the anticipated standard.This work aims to provide a specialized evaluation framework to drive advancements in MLLMs' capacity for complex spatial reasoning and planning.The code is publicly available at</p>
<p>Additionally, it offers a highly efficient interactive environment for both inference and analysis.</p>
<p>INTRODUCTION</p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities in natural language processing, generation, and even textual complex reasoning and planning (Zhao et al., 2023).Building upon this powerful foundation of LLMs, integrating visual inputs has led to the development of even Preprint more powerful models (OpenAI, 2024;Anil et al., 2023a), a.k.a multimodal large language models (MLLMs).</p>
<p>Despite demonstrating impressive performance in handling most general multimodal tasks, the effectiveness of MLLM in multimodal reasoning and planning still remains unclear.Moreover, recent studies (Lu et al., 2024;Dai et al., 2024) indicate that vision-language training might degrade the textual capabilities of MLLMs, suggesting that MLLMs built upon LLMs could be impaired when adapted to multimodal reasoning and planning tasks.Consequently, there is an urgent need for a test that incorporates multimodal complex reasoning and planning cases to guide the subsequent enhancements of MLLMs.</p>
<p>To address this issue, existing studies generally utilize visual question answering (VQA) (Antol et al., 2015;Kafle &amp; Kanan, 2017) and game-based evaluations (Wu et al., 2023;Bellemare et al., 2013) to assess the visual reasoning capabilities of MLLMs.In general, VQA necessitates a verified ground-truth answer that relies on human annotations.But acquiring these annotations is both costly and time-consuming.Moreover, the absence of interaction and planning in typical VQA tasks poses difficulties in evaluating the reasoning and planning capabilities of advanced MLLMs.The tasks presented in these benchmarks are overly simplistic (Yue et al., 2023) or only test reasoning within domain-specific knowledge (Yue et al., 2023;Zhang et al., 2024a), which mainly evaluates the LLM knowledge of MLLMs rather than the perception, reasoning, and planning of MLLMs.Therefore, recent studies (Xu et al., 2024;Chia et al., 2024) prompt MLLMs to interact with digital game environments, which are measured by game outcomes and scores, leading to the game-based evaluation.Unlike VQA tasks, these methods can evaluate the multi-step reasoning capabilities and even spatial imagination of MLLMs, which is crucial function of human cognition, allowing us to interact with realistic environments (Wu et al., 2024).Despite the effectiveness, these works are typically restricted to individual games with complex rules, involve time-consuming evaluation episodes, and fail to effectively assess the models' generalization capabilities in multimodal planning.Considering these challenges, our goal is to develop a generalizable and efficient benchmark to evaluate the multi-step planning abilities of MLLMs, providing insights for subsequent improvements of MLLMs with complex multi-step reasoning.</p>
<p>To fill this gap, in this paper, we introduce the INteractive Game-based Vision Planning benchmark (ING-VP), meticulously focusing on evaluating the spatial imagination and multi-step reasoning abilities of MLLMs. Figure 1 shows games, evaluation settings, and the interactive process in our ING-VP.To construct our ING-VP, we initially collect six games featuring easily understandable rules.In each game, we collect 50 levels, each comprising both an image and a text representation of the current state, providing vision and textual inputs for MLLMs, as illustrated in Figure 2. To assess the spatial imagination and planning capabilities of MLLMs, we establish six experimental settings, which prompt the models to perform single-step and multi-step reasoning, with or without historical interaction.During the evaluation, we employ MLLMs to interact within the environment until the game is completed.To evaluate model performance comprehensively, beyond merely determining whether a model can finish a game, we also use the model's action efficiency and the remaining steps to complete the game as evaluation metrics.</p>
<p>With our ING-VP, we test 15 open-and closed-source MLLMs and analyze their performance on our test cases.We first support the benchmark designed to evaluate the multi-step reasoning and spatial imagination capabilities of MLLMs -ING-VP bench.Then we analyze these capabilities of current open-and closed-source MLLMs, despite a performance gap, the leading open-source model, InternVL2-Llama3-76B, achieves an accuracy of 2.50%, ranking just behind Claude-3.5Sonnet, GPT-4o, and Gemini-1.5Pro.Notably, its performance significantly surpasses that of GPT-4o mini, which stands at 1.05%, and GPT-4v, which records a mere 0.32%.We also conduct a detailed analysis of these models' performance, the evidence shows that:</p>
<p> The inability to process the relative positions of elements is one of the primary issues with MLLM perception.</p>
<p> Even the most advanced MLLMs have very limited planning capabilities, far below the performance of ordinary humans on these simple tasks.</p>
<p> Current models tend to generate instructions that are much longer than necessary to complete the levels.While this can improve accuracy on simple levels, it also indirectly reveals that MLLMs are "uncertain" about the correct solution.</p>
<p>RELATED WORK</p>
<p>Multimodal Large Language Models.LLMs (Achiam et al., 2023;Anil et al., 2023b) have demonstrated their ability of generating human-like texts to understand and respond to complex instructional queries.The successes of LLMs has elicited the burgeoning proliferation of multi-modal LLMs (Alayrac et al., 2022;Li et al., 2023b;Liu et al., 2024;Sun et al., 2024;Jin et al., 2023b), which is designed to process and integrate multiple types of data.The primary attempt Flamingo (Alayrac et al., 2022) endows visual-language models with in-context few-shot learning capabilities by trained on large-scale interleaved text-image data.BLIP2 (Li et al., 2023b) designs a Q-Former architecture to align the visual-textual knowledge during the pre-training phase.LLaVA (Liu et al., 2024) collect GPT-4 generated multimodal language-image instruction-following data and train a general-purpose visual-language assistant.Beyond multimodal understanding, EMU-2 (Sun et al., 2024) and LaVIT (Jin et al., 2023b) take one step further and act as generative multimodal model to support visual prompting and object-grounded generation.</p>
<p>MLLM Benchmarks.The development of MLLMs has highlighted the critical need of benchmarks for thorough evaluations.Although traditional visual-language tasks (e.g., visual question answering (Antol et al., 2015;Kafle &amp; Kanan, 2017) and image captioning (Lin et al., 2014;Plummer et al., 2015)) can be used as evaluation benchmarks, they are too strict and require the exact match with the ground-truth answers.To this end, LVLM-eHub (Xu et al., 2023) andLAMM (Yin et al., 2024) reformulate exiting public datasets as evaluation samples and employ human annotators or GPT to assess the quality.MME (Li et al., 2024), MMBench (Liu et al., 2023b) and SEED-Bench (Li et al., 2024) construct multiple-choice questions to mitigate the subjectivity and instability of GPT evaluation.MMMU (Yue et al., 2024) evaluate the advanced perception and reasoning of MLLMs on specific domains (e.g., science, business).</p>
<p>Game-based Evaluations.Digital games are acknowledged as essential in the pursuit of artificial general intelligence since they present complex challenges requiring advanced reasoning and cognitive skills.These challenges make digital games an ideal benchmark for evaluating the capabilities of MLLMs (Wu et al., 2023;Bellemare et al., 2013;Hu et al., 2024;Sweetser, 2024;Xu et al., 2024) including the environment perception (Hong et al., 2023;Akoury et al., 2023), memory construction (Zhu et al., 2023;Zhang et al., 2024b;Ding et al., 2023;Park et al., 2022;Liu et al., 2023a), reasoning (Liu et al., 2023a;Wang et al., 2023a;Qian et al., 2023;Huang et al., 2022) and decision-making (Chen et al., 2023;Zhou et al., 2023;Jin et al., 2023a;Qian et al., 2023).Several methods focus on semantic-level perception of environmental elements including locations, objects or actions in games.They either use basic text input of user ideas (Li et al., 2023a) or game state variables and dialogues (Akoury et al., 2023;Park et al., 2022;2023).Role-based inputs, e.g., the inclusion of character, story, role-related information (Hong et al., 2023;Wang et al., 2023b) and skills (Gong et al., 2023) are often included.TorchCraft (Synnaeve et al., 2016) is presented to use real-time strategy games such as StarCraft: Brood War to serve as a benchmark for AI research.The Chess game has long been employed as an AI testing ground (Noever et al., 2020;Stckl, 2021;Toshniwal et al., 2022).Chess Transformer (Noever et al., 2020) 2.</p>
<p>ING-VP features 6 games that are conceptually simple yet cognitively challenging: Sokoban, Maze, Sudoku, 8-queens, Tower of Hanoi, and 15-puzzle.The simplicity lies in the easily comprehensible rules and the ability to encapsulate complete level information within a single image, facilitating comprehensive reasoning.The challenge stems from the requirement for models to precisely capture core visual elements and their spatial relationships, necessitating multi-step reasoning to successfully complete each level.We meticulously craft 6 reasoning settings, enabling researchers to systematically identify the strengths and limitations of target models through comparative analysis of performance across these settings.</p>
<p>SIX INFERENCE SETTINGS</p>
<p>One-step: Image and Text-only Settings In the One-step with Image setting, we provide the model solely with an image depicting the initial game state and prompt it to generate comprehensive instructions for level completion.The One-step Text-only setting follows an identical approach, with the key distinction being the replacement of the image input with its corresponding textual representation.</p>
<p>Multi</p>
<p>GAME SELECTION</p>
<p>We chose six games that are widely recognized, have straightforward rules, and operate in a deterministic environment, making them ideal representatives for our study.In a deterministic environment, the outcome of every action taken by an agent is predictable and certain.Such an environment can be formally defined using a Markov Decision Process (MDP).The model employs a strategy  to determine the next action a t based on the current state s t and all previous actions a 0:t1 , represented as:
a t = (s t , a 0:t1 )(1)
The planning process of MLLMs can be expressed as:
S  = (S, A, G, n)(2)
Where S  is the future sequence of states, which terminates upon achieving the goal G or exhausting the available moves n; S is the current sequence of states; A represents the current sequence of actions.</p>
<p>DATA COLLECTION</p>
<p>Sokoban.It involves pushing crates onto designated storage locations within a warehouse maze.We select 50 levels from the Sasquatch dataset1 .To mitigate difficulty and prevent data leakage, we employ the A-star algorithm to constrain each level to a maximum of 8 steps for completion.</p>
<p>Maze.The Maze game challenges players to navigate from a starting point to a target through a network of paths.We employ a Depth-First Search (DFS) algorithm to automatically generate 50 solvable levels, each with an 11x11 grid size.We also constrain the solution length to a maximum of 8 steps.</p>
<p>8-Queens.The 8-Queens puzzle challenges people to place eight queens on an 8x8 chessboard such that no two queens threaten each other.N-Queens is a special game due to its standard formulation: models could potentially solve it without visual input, relying solely on memorized patterns from training data.To ensure that visual reasoning is essential, we modify the puzzle by manually placing the first queen in a different position for each level.The image presented to the MLLMs shows this initial configuration, requiring them to reason from this starting point to complete the puzzle.</p>
<p>Sudoku.Sudoku is a logic-based number placement puzzle that requires filling a 9x9 grid such that each row, column, and 3x3 subgrid contains all digits from 1 to 9 without repetition.A well-formed Sudoku puzzle with a unique solution requires a minimum of 17 initial clues.For our benchmark, we curate a set of 50 puzzles with each puzzle contain 71 clues from a Kaggle dataset 2 , ensuring each puzzle meets this criterion.We then manually generate corresponding images for each level to maintain consistency with our benchmark's visual reasoning focus.</p>
<p>Hanoi The Tower of Hanoi is a classic mathematical puzzle that involves transferring a stack of disks of varying diameters from one rod to another, adhering to the constraint that a larger disk must never be placed atop a smaller one.In our implementation, each problem instance consists of four rods and five disks, with an optimal solution requiring a minimum of 8 moves.</p>
<p>15-Puzzle It's a classical sliding tile puzzle comprising a 4x4 grid with 15 numbered tiles and one vacant space.The objective is to rearrange the tiles into numerical order through a series of sliding movements.In our implementation, we employ the Breadth-First Search (BFS) algorithm to explore solution paths, constraining the search depth to 8 moves as previous games.</p>
<p>EXPERIMENTS</p>
<p>We conduct a comprehensive evaluation of both open-source and closed-source MLLMs, employ a zero-shot setting to faithfully emulate the human puzzle-solving process, given the unique nature of our tasks.A uniform set of prompts was applied across all models.The complete set of 36 prompts is presented in the Appendix B.</p>
<p>BASELINES</p>
<p>MLLMs.We consider a comprehensive suite of mainstream large multimodal models.Closed-source models include GPT-4o, GPT-4o Mini, GPT-4v, GPT-4 Turbo, Claude-3.5Sonnet, Claude-3 Opus, and Gemini-1.5Pro.Open-source models consist of CogVLM2-19B, DeepSeek-VL, Internvl-Chat-v1.5, Internvl2-8B, Internvl2-26B, Internvl2-40B, InternVL2-Llama3-76B, and MiniCPM-V2.6.We utilize each model's official API for closed-source systems or the publicly available checkpoint for open-source implementations, More information of these models can be found in the Appendix A.</p>
<p>Evaluation.We present a systematic interactive environment for evaluating all MLLMs, where models interact with the game environment until either completing the task or exhausting the allotted steps.We constrain the model's output action instructions to JSON format through prompts and extract them using regular expressions.The correctly extracted instructions are then used as input for the game environment.After the game state changes, the new state is fed back to the model for the next round of inference.We employ three metrics: accuracy, completion degree, and action efficiency.( 1) Accuracy is our main metric, it measures whether the model can complete the task within the specified number of steps.( 2) Completion degree is determined by the final state of the game environment after interaction with the model.The ING-VP benchmark poses a substantial challenge to current MLLMs: Even the most advanced model, Claude-3.5Sonnet, achieves an accuracy of only 3.37%.In contrast, an average human can easily complete all of these tasks (8-queens is an exception), highlighting a significant gap between model performance and human capabilities on the ING-VP benchmark.</p>
<p>Performance disparity between open-source and closed-source models persists: While the performance of closed-source models on ING-VP is far from satisfactory, they still outperform the open-source models.The best-performing open-source model, InternVL2-Llama3-76B, achieves an accuracy of 2.50%, which remains lower than Claude-3.5Sonnet, GPT-4o and Gemini-1.5Pro.</p>
<p>For MLLMs, the greatest challenge in perception is understanding location information.According to our observations of the inference results, the most advanced models, such as Claude-3.5Sonnet and GPT-4o, can generally identify the elements present and even count the quantity of each in the Sokoban game.However, they struggle to accurately determine precise location information, leading to very low inference accuracy and degree of task completion.</p>
<p>Merely breaking down the steps is unhelpful and may even be counterproductive.In text-only tasks, Claude-3.5Sonnet and GPT-4o achieve accuracy rates of 2.30% and 3.30%, respectively, in the multi-step setting, which are lower than their 8.00% and 4.30% accuracy in the one-step setting.For the ING-VP benchmark, thinking step by step does not work and even has a negative effect.We Preprint believe that MLLMs rely heavily on pattern matching based on prior training data, generating outputs from similar inputs rather than engaging in actual planning.In this section, We conduct a comprehensive range of analyses to explore the generative capabilities of MLLMs in a broader context, while also dissecting the nuanced output tendencies of current models.We hope our results can provides valuable insights that can inform future model design and training strategies.</p>
<p>Error Analysis.We collate and analyze 555 errors (image-text: 279, text-only: 276) made by Claude-3.5Sonnet in one-step setting, as illustrated in Figure 3.It is important to note that while we categorize each case under distinct error types, in many instances the model exhibited errors in both comprehension and reasoning.Our classification follows contextual cues: when the model provided invalid instructions from the outset, we labele it as an understanding error.Conversely, if the model deviated from the correct solution at an intermediate step, we classify it as a reasoning error.Below, we summarize key observations based on these error types:</p>
<p> Perceptual Errors (55.2%/-%):These errors occur exclusively in the image-text setting.</p>
<p>While current models are generally able to recognize overall attributes of an image-such as identifying the game genre and its components, their ability to accurately interpret fine details, including the specific size and precise location of each element, remains limited (e.g., see Figure 7.This perceptual limitation represents a major contributor to the elevated error rates in this setting. Textual Understanding Errors (2.9%/58.0%):Textual understanding errors manifest in two main forms: a misinterpretation of specific prompts or an inability to correctly parse data structures or character matrices used to represent game levels in the text-only setting (as shown in Figure 8).These errors indicate that the model struggles to generalize its understanding when presented with text structures not commonly encountered in its training data. Planning Errors (41.9%/42.0%):Planning errors constitute another major issue for Claude-3.5Sonnet.In these cases, the model initially provides plausible steps but eventually fails due to its inability to correctly track or judge the game state after several steps (see Figure 9).This suggests a breakdown in maintaining consistent reasoning over multi-step processes. Other Errors: During error analysis, we observe that Claude-3.5Sonnet and GPT-4o never refused to answer queries, and all responses were accurately extracted.However, models such as GPT-4V displayed issues like refusal to respond or failure to adhere to the required response format, which hindered our ability to retrieve the outputs.</p>
<p>Planning Capacity Analysis.We select the game where models performed best-Maze-and introduced three additional difficulty levels: 4 steps, 12 steps, and 16 steps, by adjusting only the number of moves required to complete the level, while maintaining the same level structure.This allowed us to closely examine the planning capabilities of the most advanced MLLMs, Claude-3.5Sonnet and GPT-4o, as shown in FIgure 4. Our findings showed a significant decline in both accuracy and completion degree as the number of required steps increased.However, action efficiency, which emphasizes perception and judgment of the current state, was not notably affected, since modifying the step count without altering the overall layout had little impact on this metric.</p>
<p>Comparative Analysis.We compare the results across different metrics, settings, and models, aiming to highlight the characteristics of current MLLMs. Results differ across metrics.Of the three metrics provided by ING-VP, accuracy-being the most stringent-typically yields the lowest scores.The primary reason action efficiency is often significantly higher than both completion rate and accuracy is that models frequently generate instructions that alter the game state, but these changes have minimal impact on successfully completing the level.A notable example is Gemini-1.5Pro, which achieves an average action efficiency of 76.52% on the 15-puzzle, yet only 0.67% and 3.42% in accuracy and completion rate, respectively.</p>
<p> Image-text vs. Text-only.Comparing the performance of each model in the image-text and text-only settings, we found that most test subjects performed better in the text-only setting.This highlights that limitations in image comprehension remain a key factor constraining the performance of MLLMs.</p>
<p> Multi-step vs. One-step.According to the results in Table 3, for most models, multi-step setting improves accuracy compared to one-step.However, there are exceptions, such as Claude-3.5Sonnet.We compare the output of Claude-3.5Sonnet and GPT-4o and find that, despite we set the same parameters for closed-source models, Claude-3.5Sonnet's sampling strategy is more fixed than GPT-4o's.As a result, when the model produces an invalid action in a certain state, it tends to repeatedly generate the same action until all attempts are exhausted.GPT-4o, being more flexible, is better at generating diverse responses.Therefore, although Claude-3.5Sonnet performs better than GPT-4o in one-step tasks, the opposite is true for multi-step tasks.One example is shown in Figure 5.</p>
<p> With-history vs. Without-history.In our tasks, incorporating the model's historical output as the input for subsequent rounds did not lead to improved performance.Additionally, we introduce an undo option for Sokoban, Sudoku, and N-Queens in the with-history setting.Interestingly, despite the models frequently reaching a state where undoing moves was necessary to complete the level, almost none utilized this feature.This suggests that the models struggle with processing precise positional information and are unable to accurately assess whether the current state is solvable.</p>
<p>TWO THINKING ABOUT PLANNING</p>
<p>A holistic approach may outperform a divide-and-conquer strategy.When humans are tasked with completing a planning problem, whether in a single or multi-step process, it typically involves three key phases: understanding the goal, devising a plan, and breaking down the steps.Large models should operate similarly, yet when presented with the same game level, their outputs differ significantly between one-step and multi-step settings, as highlighted in Table 1.Notably, even the initial steps diverge between the two approaches.To explore the planning capabilities of the model further, we employ two methods to adjust the multi-step output:</p>
<p> Step-wise Best of N (BoN): The model generates ten candidate responses at each step, with the most frequent answer selected as the final output. Forced Planning: The model is required to complete its entire plan before producing a final answer, akin to the one-step setting.FIgure 6 illustrates an example of these methods in action, despite these adjustments, the multi-step approach failed to match the performance of the one-step setting.This suggests that, for the large models, even when given identical image, one-step and multi-step tasks are fundamentally different, with the former better eliciting the model's planning capabilities.</p>
<p>Small changes in the prompt phrasing can substantially influence the model's planning effectiveness.A thorough comparison of single-step and multi-step outputs reveals not only differences but also distinct tendencies.For instance, in Maze and Sokoban games, Claude-3.5Sonnet favors "U (Up)" and "D (Down)" in the one-step mode, whereas it prefers "L (Left)" and "R (Right)" in the multi-step mode.Given that most of the prompt wording remains consistent between the two settings, our results indicate that subtle variations can profoundly affect the model's response distribution.We leave more detailed experiments as future work.</p>
<p>CONCLUSION</p>
<p>In this work, we introduce ING-VP, an interactive game-based vision planning benchmark designed to evaluate the spatial imagination and planning capabilities of MLLMs.Our experimental results reveal that even the most advanced MLLMs struggle to achieve satisfactory performance on game tasks that humans find trivial.This underperformance stems from multiple factors: existing models often fail to generate accurate perceptions of images, and they face even greater challenges in making inferences and plans based on their understanding.We believe that ING-VP is of noteworthy to the community's deeper understanding of MLLMs, and can also advance MLLMs' capabilities in comprehension and planning within visual contexts.</p>
<p>LIMITATIONS</p>
<p>Despite its strengths, ING-VP has certain limitations.We deliberately omit difficulty grading settings.</p>
<p>Including simpler levels would significantly increase the likelihood of models completing tasks by chance after sufficient steps, potentially compromising the reliability of our results.Conversely, incorporating more challenging levels would yield little insight, given that MLLMs already struggle with current difficulty levels, and could negatively impact inference efficiency.Furthermore, ING-VP does not exhaustively cover all possible game types.Instead, we focus on selecting well-known and representative games to ensure relevance and broad applicability.Finally, to address efficiency concerns, we do not use images of previous states as input in the multi-step with history setting.These considerations provide clear directions for future enhancements to our benchmark.</p>
<p>B PROMPTS</p>
<p>The following is the comprehensive list of 36 prompts utilized in our experiments.Output Instructions: Please use JSON as your output format: {"output": "{rod-x}{rod-y}"}, which means move the disk on rod-x to rod-y</p>
<p>Instruction:</p>
<p>Please output only one step and your output must meet required format {"output": "{rod-x}{rod-y}"} and not output anything else:</p>
<p>Preprint</p>
<p>Maze</p>
<p>System: You are a player of Maze game.And you will be given an image of a level of the Maze game.Your task is to move from your current position through the floor to the destination.</p>
<p>Rules:</p>
<ol>
<li>
<p>Red area: your current position.</p>
</li>
<li>
<p>Green area: destination.</p>
</li>
<li>
<p>Black area: wall, unable to pass.</p>
</li>
<li>
<p>White area: floor, able to pass.Output Instructions:</p>
</li>
<li>
<p>Provide movement instructions using only the 4 letters: "L" (left), "R" (right), "U" (up), "D" (down).</p>
</li>
<li>
<p>Use JSON as your output format: {"output": "L"} or {"output": "R"} or {"output": "U"} or {"output": "D"}.</p>
</li>
<li>
<p>Output only one move at a time, wait for confirmation before proceeding to the next step.</p>
</li>
</ol>
<p>Instruction:</p>
<p>Please output only one step, and your output must be one of the following: {"output": "L"} or {"output": "R"} or {"output": "U"} or {"output": "D"} and not output anything else:</p>
<p>15-puzzle</p>
<p>System:</p>
<p>You are a player of n-puzzle game.And you will be given an image of a level of the n-puzzle game.Please finish the n-puzzle based on the image provided.</p>
<p>Rules:</p>
<p>1.The board is a square grid of size 4 * 4;</p>
<ol>
<li>
<p>The board contains 15 numbered tiles and one empty space;</p>
</li>
<li>
<p>The goal is to rearrange the tiles so that they are in ascending order from the top left corner of the board;</p>
</li>
<li>
<p>Valid moves are up, down, left, and right.</p>
</li>
</ol>
<p>Output Instructions:</p>
<ol>
<li>
<p>Use JSON as your output format: {"output": number}.</p>
</li>
<li>
<p>if the number is around the empty space, they will swap positions.Instruction: Please output only one step and your output must meet required format {"output": number}.Please do not output anything else.</p>
</li>
</ol>
<p>8-queens</p>
<p>System:</p>
<p>You are a player of n-queens game.And you will be given an image of a level of the n-queens game.Your task is to generate coordinates one at a time to complete the n-queens problem on a board where the first queen is already placed.</p>
<p>Preprint</p>
<p>Sudoku</p>
<p>System:</p>
<p>You are a player of Sudoku game.And you will be given an image of a level of the Sudoku game.Please finish the sudoku puzzle based on the image provided, one step at a time.</p>
<p>Rules:</p>
<p>1.In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9 exactly once without repeating.</p>
<ol>
<li>You need to determine the number to fill in the blank based on the existing numbers.</li>
</ol>
<p>Output Instructions:</p>
<p>1.The top left number is at row 0, column 0; the bottom right number is at row 8, column 8.</p>
<ol>
<li>
<p>Use JSON as your output format: {"output": {"{row}{column}": {number}}}.</p>
</li>
<li>
<p>The range of {row} and {column} are 0-8, the range of {number} is 1-9.Instruction: Please output only one step and your output must meet required format {"output": {"{row}{column}": {number}}}, and not output anything else: 3.Your task is to move all the disks to rod "D" 4.Only one disk can be moved at a time 5.Only the top disk can be moved 6.At no time should a large disk be placed on top of a small disk.</p>
</li>
</ol>
<p>Output Instructions: Please use JSON as your output format: {"output": "{rod-x}{rod-y}"}, which means move the disk on rod-x to rod-y Instruction:</p>
<p>Dictionary representation: {text-representation-path}</p>
<p>Please output only one step based on the given rules and dictionary representation, and your output must meet required format {"output": "{rod-x}{rod-y}"}.Please do not output anything else.</p>
<p>Preprint</p>
<p>Maze</p>
<p>System:</p>
<p>You are a player of Maze game.And you will be given a text matrix of a level of the Maze game.Your task is to move from your current position through the floor to the destination.</p>
<p>Information of text matrix:</p>
<ol>
<li>
<p>'S': your current position.</p>
</li>
<li>
<p>'X': destination.</p>
</li>
<li>
<p>'+': wall, unable to pass.4. ' ': floor, able to pass.</p>
</li>
</ol>
<p>Output Instructions:</p>
<ol>
<li>
<p>Provide movement instructions using only the 4 letters: "L" (left), "R" (right), "U" (up), "D" (down).</p>
</li>
<li>
<p>Use JSON as your output format: {"output": "L"} or {"output": "R"} or {"output": "U"} or {"output": "D"}.</p>
</li>
<li>
<p>Output only one move at a time, wait for confirmation before proceeding to the next step.</p>
</li>
</ol>
<p>Instruction: Text matrix: {text-representation-path}</p>
<p>Please output only one step based on the given rules and text matrix, and your output must be one of the following: {"output": "L"} or {"output": "R"} or {"output": "U"} or {"output": "D"}.Please do not output anything else.</p>
<p>15-puzzle</p>
<p>System:</p>
<p>You are a player of n-puzzle game.And you will be given a list representation of a level of the n-puzzle game.Please finish the n-puzzle based on the list representation provided.</p>
<p>Illustration of given list representation:</p>
<p>1.The main list represents the board of size 4 * 4; 2. The main list contains 4 sublist, each sublist represents a row, and contains 4 elements; 3. The board contains 15 numbered tiles from 1 to 15 and one empty space, empty space is represented as 0; 4. The goal is to rearrange the elements to [ [1,2,3,4], [5,6,7,8], [9,10,11,12],</p>
<p>[13,14,15,0]] 5. Valid moves are up, down, left, and right.Instructions:</p>
<ol>
<li>Use JSON as your output format: {"output": number}.2. if the number is around the empty space, they will swap positions.</li>
</ol>
<p>Instruction: List representation: {text-representation-path}</p>
<p>Please output only one step based on given list representation and your output must meet required format {"output": number}.Please do not output anything else.</p>
<p>Preprint</p>
<p>8-queens</p>
<p>System: You are a player of n-queens game.And you will be given a coordinate of the existing queens of a level of the n-queens game.Your task is to generate coordinates one at a time to complete the n-queens problem on a board where the first queen is already placed.</p>
<p>Rules: Each queen must be placed in such a way that no two queens threaten each other.</p>
<p>1.No two queens can share the same row.</p>
<ol>
<li>
<p>No two queens can share the same column.</p>
</li>
<li>
<p>No two queens can share the same diagonal.</p>
</li>
</ol>
<p>Instructions:</p>
<p>1.An 8 x 8 chessboard with 8 queens.</p>
<ol>
<li>
<p>The coordinate range is from 0 to 7.</p>
</li>
<li>
<p>The position of the first queen is already given, so do not include it in your answer.</p>
</li>
<li>
<p>Output the coordinates of each queen one at a time in the JSON format: {"output":</p>
</li>
</ol>
<p>[row, col]} 5.If your chess piece violates the three rules, it will be ignored.</p>
<p>Instruction:</p>
<p>The coordinate of the existing queens (including the first queen): {text-representation-path} 1. first number: row index, range from 0 to 7 2. second number: column index, range from 0 to 7 Please output only one step based on given coordinate and your output must meet required format {"output": [row, col]}.And do not output anything else.</p>
<p>Sokoban</p>
<p>System: You are a player of Sokoban game.And you will be given a text matrix of a level of the Sokoban game.Your task is to complete this level by outputting movement instructions based on the given text matrix one step at a time.</p>
<p>Objective: Move all boxes onto the docks (goals).</p>
<p>Rules:</p>
<ol>
<li>
<p>Movement: The player can move up (U), down (D), left (L), or right (R).Output Instructions:</p>
</li>
<li>
<p>Use JSON as your output format: {"output": "{rod-x}{rod-y}"}, which means move the disk on rod-x to rod-y, 2. This is a multi-turn conversation.The conversation history provided below may be helpful to you.Instruction: This is a multi-turn conversation.The conversation history provided below may be helpful to you.</p>
</li>
</ol>
<p>Conversation history: {conversation-history-path}</p>
<p>Please output only one step and your output must meet required format {"output": "{rod-x}{rod-y}"} and not output anything else:</p>
<p>Maze</p>
<p>System: You are a player of Maze game.And you will be given an image of a level of the Maze game.Your task is to move from your current position through the floor to the destination.Preprint you.</p>
<p>Conversation history: {conversation-history-path}</p>
<p>Please output only one step, and your output must be one of the following: {"output": "L"} or {"output": "R"} or {"output": "U"} or {"output": "D"} and not output anything else:</p>
<p>15-puzzle</p>
<p>System:</p>
<p>You are a player of n-puzzle game.And you will be given an image of a level of the n-puzzle game.Please finish the n-puzzle based on the image provided.</p>
<p>Rules:</p>
<p>1.The board is a square grid of size 4 * 4;</p>
<ol>
<li>
<p>The board contains 15 numbered tiles and one empty space;</p>
</li>
<li>
<p>The goal is to rearrange the tiles so that they are in ascending order from the top left corner of the board; 4. Valid moves are up, down, left, and right.</p>
</li>
</ol>
<p>Output Instructions:</p>
<ol>
<li>
<p>Use JSON as your output format: {"output": number}.</p>
</li>
<li>
<p>if the number is around the empty space, they will swap positions.</p>
</li>
<li>
<p>You will obtain a multi-turn conversation.The conversation history provided below may be helpful to you.Instruction: This is a multi-turn conversation.The conversation history provided below may be helpful to you.</p>
</li>
</ol>
<p>Conversation history: {conversation-history-path}</p>
<p>Please output only one step and your output must meet required format {"output": number}.Please do not output anything else.</p>
<p>8-queens</p>
<p>System: You are a player of n-queens game.And you will be given an image of a level of the n-queens game.Your task is to generate coordinates one at a time to complete the n-queens problem on a board where the first queen is already placed.</p>
<p>Rules: Each queen must be placed in such a way that no two queens threaten each other.</p>
<p>1.No two queens can share the same row.</p>
<ol>
<li>
<p>No two queens can share the same column.</p>
</li>
<li>
<p>No two queens can share the same diagonal.</p>
</li>
</ol>
<p>Instructions:</p>
<p>1.An 8 x 8 chessboard with 8 queens.</p>
<ol>
<li>
<p>The coordinate range is from 0 to 7. Preprint 3. The position of the first queen (red color) is already given, so do not include it in your answer.</p>
</li>
<li>
<p>Output the coordinates of each queen one at a time in the JSON format: {"output": [row, col]}.</p>
</li>
<li>
<p>If you think you are in an irreversible error state and want to return to the state at a certain step in history, use: "{"output": {number}}", where {number} is the step number.</p>
</li>
</ol>
<p>6.If your chess piece violates the three rules, it will be ignored.</p>
<ol>
<li>You will obtain a multi-turn conversation.The conversation history provided below may be helpful to you.Instruction: This is a multi-turn conversation.The conversation history provided below may be helpful to you.</li>
</ol>
<p>Conversation history: conversation-history-path</p>
<p>Please output only one step and your output must meet required format {"output": [row, col]}, and not output anything else:</p>
<p>Sokoban</p>
<p>System: You are a player of Sokoban game.And you will be given an image of a level of the Sokoban game.Your task is to complete this level by outputting movement instructions based on this image one step at a time.</p>
<ol>
<li>You will obtain a multi-turn conversation.The conversation history provided below may be helpful to you.Instruction: This is a multi-turn conversation.The conversation history provided below may be helpful to you.</li>
</ol>
<p>Conversation history: {conversation-history-path}</p>
<p>Please output only one step, and your output must be one of the following: "output": "L" or "output": "R" or "output": "U" or "output": "D" and not output anything else:</p>
<p>Sudoku</p>
<p>System: You are a player of Sudoku game.And you will be given an image of a level of the Sudoku game.Please finish the sudoku puzzle based on the image provided, one step at a time.</p>
<p>Rules:</p>
<p>1.In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9 exactly once without repeating.</p>
<ol>
<li>You need to determine the number to fill in the blank based on the existing numbers.</li>
</ol>
<p>Output Instructions:</p>
<p>1.The top left number is at row 0, column 0; the bottom right number is at row 8, column 8.</p>
<ol>
<li>
<p>Use JSON as your output format: {"output": {"{row}{column}": {number}}}.</p>
</li>
<li>
<p>The range of {row} and {column} are 0-8, the range of {number} is 1-9.</p>
</li>
<li>
<p>If you think you are in an irreversible error state and want to return to the state at a certain step in history, use: "{"output": {number}}", where {number} is the step number.</p>
</li>
</ol>
<p>5.You will obtain a multi-turn conversation.The conversation history provided below may be helpful to you.Instruction: Please output only one step and your output must meet required format {"output": {"{row}{column}": {number}}}, and not output anything else: 3. The board contains 15 numbered tiles from 1 to 15 and one empty space, empty space is represented as 0;</p>
<ol>
<li>The goal is to rearrange the elements to [ [1,2,3,4], [5,6,7,8], [9,10,11,12], [13,14,15,0]] 5. Valid moves are up, down, left, and right.</li>
</ol>
<p>Instructions:</p>
<ol>
<li>
<p>Use JSON as your output format: {"output": number}.</p>
</li>
<li>
<p>if the number is around the empty space, they will swap positions.</p>
</li>
<li>
<p>You will obtain a multi-turn conversation.The conversation history provided below may be helpful to you.Instruction: List representation: {text-representation-path} This is a multi-turn conversation.The conversation history provided below may be helpful to you.</p>
</li>
</ol>
<p>Conversation history: {conversation-history-path}</p>
<p>Please output only one step based on given list representation and your output must meet required format {"output": number}.Please do not output anything else.</p>
<p>8-queens</p>
<p>System: You are a player of n-queens game.And you will be given a coordinate of the existing queens of a level of the n-queens game.Your task is to generate coordinates one at a time to complete the n-queens problem on a board where the first queen is already placed.</p>
<p>Rules: Each queen must be placed in such a way that no two queens threaten each other.</p>
<p>1.No two queens can share the same row.</p>
<ol>
<li>
<p>No two queens can share the same column.</p>
</li>
<li>
<p>No two queens can share the same diagonal.Instructions:</p>
</li>
</ol>
<p>1.An 8 x 8 chessboard with 8 queens.2. The coordinate range is from 0 to 7.</p>
<ol>
<li>
<p>The position of the first queen is already given, so do not include it in your answer.Note:</p>
</li>
<li>
<p>Use JSON as your output format: {"output": ["AC", "AD", ...]}, which means move the top disk on rod A to rod C, then move the top disk on rod A to rod D and so on.</p>
</li>
</ol>
<p>Your answer:</p>
<p>Maze</p>
<p>This is an image of a level of the Maze game.Your task is to move from your current position through the floor to the destination.Rules:</p>
<p>Preprint</p>
<p>Your answer:</p>
<p>Sudoku</p>
<p>This is an image of a level of the Sudoku game.</p>
<p>Please finish the sudoku puzzle based on the image provided.Rules:</p>
<p>1.In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9 exactly once without repeating.</p>
<ol>
<li>You need to determine the number to fill in the blank based on the existing numbers.</li>
</ol>
<p>instructions:</p>
<p>1.The top left number is at row 0, column 0; the bottom right number is at row 8, column 8.</p>
<ol>
<li>
<p>Use JSON as your output format: {"output": {"{row}{column}": {number}, "{row}{column}": {number}, ...}}.</p>
</li>
<li>
<p>The range of {row} and {column} are 0-8, the range of {number} is 1-9.</p>
</li>
</ol>
<p>Your answer:</p>
<p>B.6 ONE-STEP TEXT-ONLY</p>
<p>Hanoi</p>
<p>This is an dictionary representation of a level of the Tower of Hanoi game.</p>
<p>Please finish the Tower of Hanoi puzzle based on the dictionary representation provided.</p>
<p>Dictionary representation: {text-representation-path}</p>
<p>Rules:</p>
<p>1.There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e</p>
<ol>
<li>
<p>Your task is to move all the disks to rod "D"</p>
</li>
<li>
<p>Only one disk can be moved at a time 4.Only the top disk can be moved 5.At no time should a large disk be placed on top of a small disk.</p>
</li>
</ol>
<p>Note:</p>
<ol>
<li>Use JSON as your output format: {"output": ["AC", "AD", ...]}, which means move the top disk on rod A to rod C, then move the top disk on rod A to rod D and so on.</li>
</ol>
<p>Your answer:</p>
<p>Maze</p>
<p>This is an dictionary representation of a level of the Tower of Hanoi game.</p>
<p>Please finish the Tower of Hanoi puzzle based on the dictionary representation provided.</p>
<p>Dictionary representation: {text-representation-path}</p>
<p>Rules:</p>
<ol>
<li>red area: your current position 2. green area: destination</li>
</ol>
<p>Figure 1 :
1
Figure 1: The overview of ING-VP benchmark.ING-VP comprises 6 distinct games, conducts 3 comparative analyses across 6 experimental settings, and evaluates 5 key capabilities of MLLMs.Additionally, it offers a highly efficient interactive environment for both inference and analysis.</p>
<p>Figure 2 :
2
Figure 2: ING-VP examples sampled from each game.Includes pictures and text representations of Sokoban, Maze, Sudoku, 8-queens, Tower of Hanoi, and 15-puzzle.</p>
<p>Figure 3 :
3
Figure 3: Error distribution over Claude-3.5Sonnet's 555 errors across different tasks and settings.</p>
<p>Figure 4 :
4
Figure 4: Maze level accuracy of Claude-3.5Sonnet and GPT-4o across 4 difficulty levels.</p>
<p>Figure 5 :
5
Figure 5: An example showcasing Claude 3.5-Sonnet with a fixed output paradigm.</p>
<p>Figure 6 :
6
Figure 6: An example of results for the Claude-3.5Sonnet in four settings.</p>
<p>B. 1
1
MULTI-STEP WITH IMAGE WITHOUT HISTORY Hanoi System: You are a player of Hanoi game.And you will be given an image of a level of the Tower of Hanoi game.Please finish the Tower of Hanoi puzzle based on the image provided.You must follow the rules of Hanoi game: 1.There are 4 rods: A, B, C, D; and 5 disks: a,b, c, d, e2.Your task is to move all the disks to rod "D" 3.Only one disk can be moved at a time 4.Only the top disk can be moved 5.At no time should a large disk be placed on top of a small disk.</p>
<p>B. 2
2
MULTI-STEP TEXT-ONLY WITHOUT HISTORY Hanoi System: You are a player of Hanoi game.And you will be given an dictionary representation of a level of the Tower of Hanoi game.Please finish the Tower of Hanoi puzzle based on the dictionary representation provided.You must follow the rules of Hanoi game: 1.There are 4 rods: A, B, C, D 2. And 5 disks: a, b, c, d, e; for size: a  b  c  d  e</p>
<p>B. 4
4
MULTI-STEP TEXT-ONLY WITH HISTORY Hanoi System: You are a player of Hanoi game.And you will be given an dictionary representation of a level of the Tower of Hanoi game.Please finish the Tower of Hanoi puzzle based on the dictionary representation provided.You must follow the rules of Hanoi game: 1.There are 4 rods: A, B, C, D 2. And 5 disks: a, b, c, d, e; for size: a  b  c  d  e 3.Your task is to move all the disks to rod "D" 4.Only one disk can be moved at a time Preprint 15-puzzle System: You are a player of n-puzzle game.And you will be given a list representation of a level of the n-puzzle game.Please finish the n-puzzle based on the list representation provided.Illustration of given list representation: 1.The main list represents the board of size 4 * 4; 2. The main list contains 4 sublist, each sublist represents a row, and contains 4 elements;</p>
<p>4 . 1 .
41
Output the coordinates of each queen one at a time in the JSON format: {"output": [row, col]} Preprint Please output only one step based on given number string and your output must meet required format {"output": {"{row}{column}": {number}}}.And do not output anything else: B.5 ONE-STEP WITH IMAGE Hanoi This is an image of a level of the Tower of Hanoi game.Please finish the Tower of Hanoi puzzle based on the image provided.Rules: There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e 2. Your task is to move all the disks to rod "D" 3.Only one disk can be moved at a time 4.Only the top disk can be moved 5.At no time should a large disk be placed on top of a small disk.</p>
<p>Figure 8 :
8
Figure 8: A sample case of textual understanding error.Hanoi -Multi-step -Text-only -Withhistory -Level 13. 45</p>
<p>Figure 10 :
10
Figure 10: A sample case of output comparison.Maze -Multi-step -Image-text -Withouthistory -Level 33.47</p>
<p>Figure 11 :
11
Figure 11: A sample case of output comparison.15-Puzzle -Multi-step -Image-text -Withouthistory -Level 45.48</p>
<p>Table 1 :
1
Main results for the best-performing MLLMs (LLMs).
PreprintImage-textText-onlyModelMetricMulti-stepOne-stepMulti-stepOne-stepOverallw/o history w/ historyw/o history w/ historyClosed Source ModelAcc.0.300.307.002.302.308.003.37Claude-3.5 SonnetComp.3.904.3021.904.905.2016.809.50Eff.26.9023.1048.4017.6018.5042.0029.42Acc.3.302.000.303.303.304.302.75GPT-4oComp.6.705.2012.905.805.4013.808.30Eff.19.2014.2033.7018.7018.3047.8025.32Acc.1.000.302.705.704.302.302.72Gemini-1.5-ProComp.5.903.809.608.206.508.507.08Eff.34.7027.8042.8019.5018.5037.7030.17Acc.0.700.300.002.002.301.001.05GPT-4o miniComp.3.403.406.605.205.908.905.57Eff.13.208.2035.2019.5017.3040.1022.25Acc.0.000.001.300.000.300.300.32GPT-4VComp.2.902.904.302.603.003.403.18Eff.8.807.205.5016.8017.408.5010.70Acc.nullnullnull2.302.301.001.87GPT-4 TurboComp.nullnullnull4.804.809.106.23Eff.nullnullnull12.2012.3041.0021.83Acc.nullnullnull2.302.301.001.87Claude-3 OpusComp.nullnullnull4.804.8010.705.07Eff.nullnullnull12.4012.3040.8021.83Open Source ModelAcc.2.672.333.002.331.673.002.50InternVL2-Llama3-76BComp.9.076.288.308.328.035.887.65Eff.17.5515.1336.1821.1329.3032.9525.58Acc.2.331.331.671.672.002.331.89Internvl2-26BComp.4.805.225.655.255.275.225.23Eff.10.589.2211.9310.229.2716.7211.32Acc.1.671.672.671.002.001.671.78Internvl2-40BComp.5.685.437.875.034.088.086.03Eff.18.3712.9822.2215.3315.2234.1618.82Acc.1.330.672.001.671.332.001.50Cogvlm2-19BComp.5.905.686.585.685.027.636.08Eff.15.7516.4527.1213.7512.8531.3719.55Acc.1.000.330.331.330.671.670.89Internvl2-8BComp.2.602.583.332.632.503.832.91Eff.5.905.274.973.054.276.034.91Acc.0.670.330.000.330.330.670.39Internvl-Chat-v1.5Comp.6.306.304.575.806.004.185.53Eff.14.9014.2225.6811.7010.8727.2717.44Acc.0.670.331.000.330.000.000.39deepseek-VLComp.3.472.723.652.684.183.923.44Eff.11.8011.2216.408.389.5715.9012.21Acc.0.33000.670.3300.22MiniCPM-V2.6Comp.3.783.334.173.622.684.223.63Eff.11.1810.6217.7310.086.3721.8812.98</p>
<p>The closer the final state is to the cleared state, the higher the score; if it deviates, the score decreases accordingly.(3)Action efficiency represents whether each instruction output by the model effectuates a change in the game state.The computation method for action efficiency is as follows:
n# of efficient actions for level iAction Efficiency =i=1# of total actions for level ia. Image-text41.9%55.2%2.9%58.0%
n 4.2 MAIN RESULTSIn this section, we examine the spatial reasoning and planning abilities of current MLLMs using the ING-VP benchmark.The results are presented in Table1, please see the Appendix C for the complete results.Our key observations are as follows:</p>
<p>Table 2 :
2
List of all models involved in the ING-VP.</p>
<ol>
<li>Pushing Boxes: The player can push one box at a time by moving towards it.Boxes can only be pushed, not pulled.3. Grid Limitations: The player and boxes can only move into empty spaces.Walls and other boxes block movement.
PreprintB.3 MULTI-STEP WITH IMAGE WITH HISTORYHanoiSystem:You are a player of Hanoi game. And you will be given an image of a level of the Tower ofHanoi game.Please finish the Tower of Hanoi puzzle based on the image provided.You must follow the rules of Hanoi game:1. There are 4 rods: A, B, C, D; and 5 disks: a, b, c, d, e2. Your task is to move all the disks to rod "D"3. Only one disk can be moved at a time4. Only the top disk can be moved5. At no time should a large disk be placed on top of a small disk.Restrictions:1. A box cannot be pushed if there is another box or a wall directly behind it.2. The player cannot move through boxes or walls.Illustration of given text matrix:1. '.': dock2. '$': box3. '*': box on the dock (can also be pushed)
http://www.abelmartin.com/rj/sokobanJS/Skinner/David%20W.%20Skinner%
-%20Sokoban.htm 2 https://www.kaggle.com/datasets/informoney/4-million-sudoku-puzzles-easytohard
A MODEL LISTList of all models involved in the ING-VP.Organization Model Access Closed Source Model OpenAI GPT-4o https://openai.com/index/hello-gpt-4o/GPT-4o mini https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/GPT-4v https://openai.com/index/gpt-4v-system-card/GPT-4 Turbo https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4Anthropic Claude-3.5Sonnet https://www.anthropic.com/news/claude-3-5-sonnetClaude-3 Opus https://www.anthropic.com/news/claude-3-familyGoogle Deepmind Gemini-1.5Pro https://deepmind.google/technologies/gemini/pro/Open Source Model Shanghai AI Laboratory InternVL2-Llama3-76B https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B InternVL2-40B https://huggingface.co/OpenGVLab/InternVL2-40B InternVL2-26B https://huggingface.co/OpenGVLab/InternVL2-26B InternVL2-8B https://huggingface.co/OpenGVLab/InternVL2-8B InternVL-Chat-V1-5 https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5 Zhipu AI CogVLM2-Llama3-chat-19B https://github.com/THUDM/CogVLM2DeepSeek-AI DeepSeek-VL-7B-chat https://github.com/deepseek-ai/DeepSeek-VLModelBest Inc MiniCPM-V 2.6 https://github.com/OpenBMB/MiniCPM-VPreprintRules: Each queen must be placed in such a way that no two queens threaten each other.1.No two queens can share the same row.2. No two queens can share the same column.3. No two queens can share the same diagonal.Instructions:1.An 8 x 8 chessboard with 8 queens.2. The coordinate range is from 0 to 7.3. The position of the first queen (red color) is already given, so do not include it in your answer.4. Output the coordinates of each queen one at a time in the JSON format: {"output":[row, col]} 5.If your chess piece violates the three rules, it will be ignored.Instruction: Please output only one step and your output must meet required format {"output": [row, col]}, and not output anything else:SokobanSystem: You are a player of Sokoban game.And you will be given an image of a level of the Sokoban game.Your task is to complete this level by outputting movement instructions based on this image one step at a time.Objective: Move all boxes onto the designated storage locations (goals).Rules:1. Movement: The player can move up (U), down (D), left (L), or right (R).Pushing Boxes:The player can push one box at a time by moving towards it.Boxes can only be pushed, not pulled.3. Grid Limitations: The player and boxes can only move into empty spaces.Walls and other boxes block movement.4. '@': worker (or agent) 5. '+': worker on the dock 6. ' ': floor 7. '#': wall Instructions:1. Provide movement instructions using only the 4 letters: "L" (left), "R" (right), "U" (up), "D" (down).2. Use JSON as your output format: {"output": "L"} or {"output": "R"} or {"output": "U"} or {"output": "D"}.Instruction: Text matrix: {text-representation-path} Please output only one step based on text matrix, and your output must be one of the following: {"output": "L"} or {"output": "R"} or {"output": "U"} or {"output": "D"}.And do not output anything else:SudokuSystem: You are a player of Sudoku game.And you will be given a number string of a level of the Sudoku game.Please finish the sudoku puzzle based on the number string provided, one step at a time.Illustration of the given number string:1.This string contains 81 numbers in total, ranges from 0 to 9.2. 0 represents a blank, you need to fill in the blank with a suitable number, ranges from 1 to 9.3. the first number is the top left number, the last number is the bottom right number.Rules:1.In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9 exactly once without repeating.2. You need to determine the number to fill in the blank based on the existing numbers.Instructions:1.The top left number is at row 0, column 0; the bottom right number is at row 8, column 8.2.Use JSON as your output format: "output": "rowcolumn": number.3. The range of row and column are 0-8, the range of number is 1-9.Instruction: Number string: {text-representation-path}Please output only one step based on given number string and your output must meet required format {"output": {"{row}{column}": {number}}}.And do not output anything else:Objective: Move all boxes onto the designated storage locations (goals).Rules:1. Movement: The player can move up (U), down (D), left (L), or right (R). 1. Provide movement instructions using only the 4 letters: "L" (left), "R" (right), "U" (up), "D" (down).2. Use JSON as your output format: {"output": "L"} or {"output": "R"} or {"output":"U"} or {"output": "D"}.3.If you think you are in an irreversible error state and want to return to the state at a certain step in history, use: "{"output": {number}}", where {number} is the step number.Preprint 5.Only the top disk can be moved 6.At no time should a large disk be placed on top of a small disk.Instructions:1. Use JSON as your output format: {"output": "{rod-x}{rod-y}"}, which means move the disk on rod-x to rod-y 2.You will obtain a multi-turn conversation.The conversation history provided below may be helpful to you.Instruction:Dictionary representation: {text-representation-path}Conversation history: {conversation-history-path}Please output only one step based on the given rules and dictionary representation, and your output must meet required format {"output": "{rod-x}{rod-y}"}.Please do not output anything else.MazeSystem:You are a player of Maze game.And you will be given a text matrix of a level of the Maze game.Your task is to move from your current position through the floor to the destination.Information of text matrix:1. 'S': your current position.2. 'X': destination.3. '+': wall, unable to pass.4. ' ': floor, able to pass.Output Instructions:1. Provide movement instructions using only the 4 letters: "L" (left), "R" (right), "U" (up), "D" (down).2. Use JSON as your output format: {"output": "L"} or {"output": "R"} or {"output":"U"} or {"output": "D"}.3. Output only one move at a time, wait for confirmation before proceeding to the next step.4.You will obtain a multi-turn conversation.The conversation history provided below may be helpful to you.Instruction:This is a multi-turn conversation.The conversation history provided below may be helpful to you.Conversation history: {conversation-history-path}Please output only one step based on the given rules and text matrix, and your output must be one of the following: {"output": "L"} or {"output": "R"} or {"output": "U"} or {"output": "D"}.Please do not output anything else.Preprint 5.If your chess piece violates the three rules, it will be ignored.6.You will obtain a multi-turn conversation.The conversation history provided below may be helpful to you.Instruction:The coordinate of the existing queens (including the first queen): {text-representation-path} 1. first number: row index, range from 0 to 7 2. second number: column index, range from 0 to 7 This is a multi-turn conversation.The conversation history provided below may be helpful to you.Conversation history: {conversation-history-path}Please output only one step based on given coordinate and your output must meet required format {"output": [row, col]}.And do not output anything else.SokobanSystem: You are a player of Sokoban game.And you will be given a text matrix of a level of the Sokoban game.Your task is to complete this level by outputting movement instructions based on the given text matrix one step at a time.Objective: Move all boxes onto the docks (goals).Rules:1. Movement: The player can move up (U), down (D), left (L), or right (R). 1. Provide movement instructions using only the 4 letters: "L" (left), "R" (right), "U" (up), "D" (down).2. Use JSON as your output format: {"output": "L"} or {"output": "R"} or {"output":"U"} or {"output": "D"}.Preprint3. If you think you are in an irreversible error state and want to return to the state at a certain step in history, use: "{"output": {number}}", where {number} is the step number.4. You will obtain a multi-turn conversation.The conversation history provided below may be helpful to you.Instruction:This is a multi-turn conversation.The conversation history provided below may be helpful to you.Conversation history: {conversation-history-path}Please output only one step based on text matrix, and your output must be one of the following: {"output": "L"} or {"output": "R"} or {"output": "U"} or {"output": "D"}.And do not output anything else:SudokuSystem: You are a player of Sudoku game.And you will be given a number string of a level of the Sudoku game.Please finish the sudoku puzzle based on the number string provided, one step at a time.Illustration of the given number string:1.This string contains 81 numbers in total, ranges from 0 to 9. 2. 0 represents a blank, you need to fill in the blank with a suitable number, ranges from 1 to 9. 3. the first number is the top left number, the last number is the bottom right number.Rules:1.In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9 exactly once without repeating.2.You need to determine the number to fill in the blank based on the existing numbers.Instructions:1.The top left number is at row 0, column 0; the bottom right number is at row 8, column 8. 2. Use JSON as your output format: "output": "rowcolumn": number.3. The range of row and column are 0-8, the range of number is 1-9.4. If you think you are in an irreversible error state and want to return to the state at a certain step in history, use: "{"output": {number}}", where {number} is the step number.5.You will obtain a multi-turn conversation.The conversation history provided below may be helpful to you.Instruction: Number string: {text-representation-path}This is a multi-turn conversation.The conversation history provided below may be helpful to you.Conversation history: {conversation-history-path} Preprint 2. THe number1, number2, ... means if number1 is around the empty space, they will swap positions first; after that, if number2 is around the empty space, number2 and the empty space will swap positions too, and so on.Your answer:8-queensThis is an image of a level of the n-queens game.Your task is to generate a list of coordinates to complete the n-queens problem on a board where the first queen is already placed.Follow these rules: Each queen must be placed in such a way that no two queens threaten each other.1.No two queens can share the same row.2. No two queens can share the same column.3. No two queens can share the same diagonal.Note:1.An 8 x 8 chessboard with 8 queens.2. The coordinate range is from 0 to 7.3. The position of the first queen (red color) is already given, so do not include it in your answer.4. Your output should be in the JSON format: {"output": 1. Provide movement instructions using only the 4 letters: "L" (left), "R" (right), "U" (up), "D" (down).2. For example, if you want to move two cells down, three cells to the right, one cell up, and two cells to the left, the example output: {"output": "DDRRRULL"} 5.If your chess piece violates the three rules, it will be ignored.Your answer:SokobanThis is a text matrix of a level of the Sokoban game.Your task is to complete this level by outputting movement instructions based on this text matrix.Text matrix: {text-representation-path}Objective: Move all boxes onto the docks (goals).Rules:1. Movement: The player can move up (U), down (D), left (L), or right (R).Pushing Boxes:The player can push one box at a time by moving towards it.Boxes can only be pushed, not pulled.3. Grid Limitations: The player and boxes can only move into empty spaces.Walls and other boxes block movement.Restrictions:1.A box cannot be pushed if there is another box or a wall directly behind it.2. The player cannot move through boxes or walls.Illustration:1. dashed grid: dock 2. yellow box: box on the dock (can also be pushed)brown box: box on the floorInstructions:1. Provide movement instructions using only the 4 letters: "L" (left), "R" (right), "U" (up), "D" (down).2. For example, if you want to move two cells down, three cells to the right, one cell up, and two cells to the left, the example output: {"output": "DDRRRULL"} Your answer:SudokuThis is a number string of a level of the Sudoku game.Please finish the sudoku puzzle based on the number string provided, one step at a time.Number string: {text-representation-path}Illustration:1.This string contains 81 numbers in total, ranges from 0 to 9. 2. 0 represents a blank, you need to fill in the blank with a suitable number, ranges from 1 to 9. 3. the first number is the top left number, the last number is the bottom right number.Rules: Preprint 1.In sudoku, each row, column, and 3x3 grid must contain all the digits from 1 to 9 exactly once without repeating.2. You need to determine the number to fill in the blank based on the existing numbers.instructions:1.The top left number is at row 0, column 0; the bottom right number is at row 8, column 8.2. Use JSON as your output format: {"output": {"{row}{column}": {number}, "{row}{column}": {number}, ...}}.3. The range of {row} and {column} are 0-8, the range of {number} is 1-9.Your answer:C DETAILED RESULTS
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</li>
</ol>
<p>A framework for exploring player perceptions of llmgenerated dialogue in commercial video games. Nader Akoury, Qian Yang, Mohit Iyyer, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in neural information processing systems. 202235</p>
<p>. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anas White, Anders Andreassen, Lakshman Tamara Von Glehn, Mehran Yagati, Lucas Kazemi, Misha Gonzalez, Jakub Khalman, Sygnowski, 2023aA family of highly capable multimodal models. CoRR, abs/2312.11805</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023bPalm 2 technical report. arXiv preprint</p>
<p>Vqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>The arcade learning environment: An evaluation platform for general agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, Journal of Artificial Intelligence Research. 472013</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. Ken Yew, Vernon Chia, Yan Toh, Deepanway Han, Lidong Ghosal, Soujanya Bing, Poria, arXiv:2403.133152024arXiv preprint</p>
<p>Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping, Nvlm: Open frontier-class multimodal llms. 2024arXiv preprint</p>
<p>Designgpt: Multiagent collaboration in design. Shiying Ding, Xinyi Chen, Yan Fang, Wenrui Liu, Yiwu Qiu, Chunlei Chai, 2023 16th International Symposium on Computational Intelligence and Design (ISCID). IEEE2023</p>
<p>Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, arXiv:2309.09971Emergent gaming interaction. 2023arXiv preprint</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352Meta programming for multi-agent collaborative framework. 2023arXiv preprint</p>
<p>Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, Ling Liu, arXiv:2404.02039A survey on large language model-based game agents. 2024arXiv preprint</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.056082022arXiv preprint</p>
<p>Alphablock: Embodied finetuning for vision-language reasoning in robot manipulation. Chuhao Jin, Wenhui Tan, Jiange Yang, Bei Liu, Ruihua Song, Limin Wang, Jianlong Fu, arXiv:2305.188982023aarXiv preprint</p>
<p>Unified language-vision pretraining with dynamic discrete visual tokenization. Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, arXiv:2309.046692023barXiv preprint</p>
<p>An analysis of visual question answering algorithms. Kushal Kafle, Christopher Kanan, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017</p>
<p>Seed-bench: Benchmarking multimodal large language models. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, Ying Shan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Advances in Neural Information Processing Systems. 2023a36</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023b</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, Lawrence Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerSeptember 6-12, 2014. 2014Proceedings, Part V 13</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 202436</p>
<p>Llm-powered hierarchical language agent for real-time human-ai coordination. Jijia Liu, Chao Yu, Jiaxuan Gao, Yuqing Xie, Qingmin Liao, Yi Wu, Yu Wang, arXiv:2312.152242023aarXiv preprint</p>
<p>Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, arXiv:2307.06281Is your multi-modal model an all-around player?. 2023barXiv preprint</p>
<p>Deepseek-vl: Towards real-world vision-language understanding. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, Chong Ruan, CoRR, abs/2403.055252024</p>
<p>The chess transformer: Mastering play using generative language models. David Noever, Matt Ciolino, Josh Kalin, arXiv:2008.040572020arXiv preprint</p>
<p>Gpt-4o system card. 2024CoRROpenAI</p>
<p>Social simulacra: Creating populated prototypes for social computing systems. Sung Joon, Lindsay Park, Carrie Popowski, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. the 35th Annual ACM Symposium on User Interface Software and Technology2022</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023</p>
<p>Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. Liwei Bryan A Plummer, Chris M Wang, Juan C Cervantes, Julia Caicedo, Svetlana Hockenmaier, Lazebnik, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, arXiv:2307.07924Communicative agents for software development. 20236arXiv preprint</p>
<p>Generative multimodal models are in-context learners. Andreas Stckl ; Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, Xinlong Wang, Proceedings of the International Conference on Recent Advances in Natural Language Processing. the International Conference on Recent Advances in Natural Language Processing2021. 2021. 2024Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</p>
<p>Large language models and video games: A preliminary scoping review. Penny Sweetser, Proceedings of the 6th ACM Conference on Conversational User Interfaces. the 6th ACM Conference on Conversational User Interfaces2024</p>
<p>Torchcraft: a library for machine learning research on real-time strategy games. Nantas Gabriel Synnaeve, Alex Nardelli, Soumith Auvolat, Timothe Chintala, Zeming Lacroix, Florian Lin, Nicolas Richoux, Usunier, arXiv:1611.006252016arXiv preprint</p>
<p>Large language models are pretty good zero-shot video game bug detectors. Reza Mohammad, Finlay Taesiri, Yihe Macklon, Hengshuo Wang, Cor-Paul Shen, Bezemer, arXiv:2210.025062022arXiv preprint</p>
<p>Glitchbench: Can large multimodal models detect video game glitches?. Reza Mohammad, Tianjun Taesiri, Cor-Paul Feng, Anh Bezemer, Nguyen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Chess as a testbed for language model state tracking. Shubham Toshniwal, Sam Wiseman, Karen Livescu, Kevin Gimpel, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Avalon's game of thoughts: Battle against deception through recursive contemplation. Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, Gao Huang, arXiv:2310.013202023aarXiv preprint</p>
<p>Zekun Moore, Wang , Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, arXiv:2310.00746Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. 2023barXiv preprint</p>
<p>Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, Furu Wei, arXiv:2404.03622Visualization-of-thought elicits spatial reasoning in large language models. 2024arXiv preprint</p>
<p>Yue Wu, Xuan Tang, Tom M Mitchell, Yuanzhi Li, Smartplay, arXiv:2310.01557A benchmark for llms as intelligent agents. 2023arXiv preprint</p>
<p>Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, Ping Luo, arXiv:2306.09265Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. 2023arXiv preprint</p>
<p>Xinrun Xu, Yuxin Wang, Chaoyi Xu, Ziluo Ding, Jiechuan Jiang, Zhiming Ding, Brje F Karlsson, arXiv:2403.10249A survey on game playing agents and large models: Methods, applications, and challenges. 2024arXiv preprint</p>
<p>Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark. Jiong Zhenfei Yin, Jianjian Wang, Zhelun Cao, Dingning Shi, Mukai Liu, Xiaoshui Li, Zhiyong Huang, Lu Wang, Lei Sheng, Bai, Advances in Neural Information Processing Systems. 202436</p>
<p>MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, CoRR, abs/2311.165022023</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>CMMMU: A chinese massive multi-discipline multimodal understanding benchmark. Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu, Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai, Fengji Zhang, Chenghua Lin, Wenhao Huang, Wenhu Chen, Jie Fu, CoRR, abs/2401.119442024a</p>
<p>Sprint: Scalable policy pre-training via language instruction relabeling. Jesse Zhang, Karl Pertsch, Jiahui Zhang, Joseph J Lim, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024b</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, CoRR, abs/2303.18223A survey of large language models. 2023</p>
<p>Agents: An open-source framework for autonomous language agents. Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, arXiv:2309.078702023arXiv preprint</p>
<p>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Wang, arXiv:2305.171442023arXiv preprintRestrictions: 1. A box cannot be pushed if there is another box or a wall directly behind it</p>
<p>Illustration: 1. dashed grid: dock 2. yellow box: box on the dock (can also be pushed) 3. brown box: box on the floor 4. goal: push all the boxes onto the docks Output Instructions: 1. Provide movement instructions using only the 4 letters. The player cannot move through boxes or walls. L" (left), "R" (right), "U" (up), "D" (down</p>
<p>Instruction: Please output only one step, and your output must be one of the following. Use JSON as your output format: {"output": "L"} or {"output": "R"} or {"output": "U"} or {"output": "D"}.. output": "L" or "output": "R" or "output": "U" or "output": "D" and not output anything else: Rules: 1. Red area: your current position</p>
<p>Green area: destination. </p>
<p>Black area: wall, unable to pass. </p>
<p>Output Instructions: 1. Provide movement instructions using only the 4 letters. White area: floor, able to pass. L" (left), "R" (right), "U" (up), "D" (down</p>
<p>Output only one move at a time. wait for confirmation before proceeding to the next step</p>
<p>You will obtain a multi-turn conversation. The conversation history provided below may be helpful to you. Instruction: This is a multi-turn conversation. The conversation history provided below may be helpful to 1. red area: your current position 2. green area: destination 3. black area: wall. unable to pass 4. white area: floor, able to pass Output Instructions: 1. Provide movement instructions using only the 4 letters: "L" (left), "R" (right), "U" (up), "D" (down</p>
<p>For example, if you want to move two cells down, three cells to the right, one cell up. and two cells to the left, the example output: {"output": "DDRRRULL"} Your answer: 15-puzzle This is an image of a level of the n-puzzle game. Your task is to generate a list of numbers to complete the n-puzzle problem. Rules: 1. The board is a square grid of size 4 * 4</p>
<p>The board contains 15 numbered tiles and one empty space. </p>
<p>The goal is to rearrange the tiles so that they are in ascending order from the top left corner of the board. </p>
<p>Valid moves are up, down, left, and right. Instructions: 1. Use JSON as your output format: {"output": [number1, number2, number3. Preprint 3. black area: wall, unable to pass 4. white area: floor, able to pass Output Instructions: 1. Provide movement instructions using only the 4 letters: "L" (left), "R" (right), "U" (up), "D" (down</p>
<p>For example, if you want to move two cells down, three cells to the right, one cell up. and two cells to the left, the example output: {"output": "DDRRRULL"} Your answer: 15-puzzle This is a list representation of a level of the n-puzzle game. Please finish the n-puzzle based on the list representation provi ded. List representation: {text-representation-path} Rules: 1. The board is a square grid of size 4 * 4</p>
<p>The board contains 15 numbered tiles and one empty space. </p>
<p>The goal is to rearrange the tiles so that they are in ascending order from the top left corner of the board. </p>
<p>Valid moves are up, down, left, and right. Instructions: 1. Use JSON as your output format: {"output. number1, number2, number3, ...]}</p>
<p>Your answer: 8-queens This is a level of the n-queens game. Your task is to generate coordinates to complete the n-queens problem on a board where the first queen is already placed. The coordinate of the first queen: {text-representation-path} Follow these rules. , The, means if number1 is around the empty space, they will swap positions first; after that, if number2 is around the empty space, number2 and the empty space will swap positions too, and so on. Each queen must be placed in such a way that no two queens threaten each other</p>
<p>No two queens can share the same row. </p>
<p>No two queens can share the same column. </p>
<p>No two queens can share the same diagonal. Note: 1. An 8 x 8 chessboard with 8 queens. </p>
<p>The coordinate range is from 0 to 7. </p>
<p>The position of the first queen (red color) is already given, so do not include it in your answer. </p>            </div>
        </div>

    </div>
</body>
</html>
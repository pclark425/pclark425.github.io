<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8320 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8320</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8320</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-264591439</p>
                <p><strong>Paper Title:</strong> Large Language Models are Visual Reasoning Coordinators</p>
                <p><strong>Paper Abstract:</strong> Visual reasoning requires multimodal perception and commonsense cognition of the world. Recently, multiple vision-language models (VLMs) have been proposed with excellent commonsense reasoning ability in various domains. However, how to harness the collective power of these complementary VLMs is rarely explored. Existing methods like ensemble still struggle to aggregate these models with the desired higher-order communications. In this work, we propose Cola, a novel paradigm that coordinates multiple VLMs for visual reasoning. Our key insight is that a large language model (LLM) can efficiently coordinate multiple VLMs by facilitating natural language communication that leverages their distinct and complementary capabilities. Extensive experiments demonstrate that our instruction tuning variant, Cola-FT, achieves state-of-the-art performance on visual question answering (VQA), outside knowledge VQA, visual entailment, and visual spatial reasoning tasks. Moreover, we show that our in-context learning variant, Cola-Zero, exhibits competitive performance in zero and few-shot settings, without finetuning. Through systematic ablation studies and visualizations, we validate that a coordinator LLM indeed comprehends the instruction prompts as well as the separate functionalities of VLMs; it then coordinates them to enable impressive visual reasoning capabilities.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8320.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8320.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VSR (Cola-FT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Spatial Reasoning dataset evaluation with Cola instruction-finetuned FLAN-T5 coordinator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of an instruction-finetuned FLAN-T5 coordinator LLM (Cola-FT) that integrates captions and plausible answers from multiple VLMs (OFA, BLIP) to solve Visual Spatial Reasoning (VSR) multiple-choice tasks involving spatial relations (e.g., under, in front of, facing).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5 (as coordinator in Cola-FT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder transformer LLM (FLAN-T5) instruction-finetuned to act as a coordinator: it ingests labeled captions and plausible answers produced by vision-language models (OFA, BLIP) and outputs a choice. The paper studies multiple FLAN-T5 sizes and also reports in-context (Cola-Zero) behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B (primary reported size for Cola-FT / Cola-Zero evaluations); additional experiments reported across sizes from ~80M up to 11B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Visual Spatial Reasoning (VSR) dataset</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Image-based spatial relation classification; tests ~65 spatial relations (e.g., under, in front of, facing) requiring visual spatial understanding and relational positioning</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Multiple-choice formulation: prompt template concatenates VLM captions (OFA, BLIP) and their plausible answers with explicit labels (e.g., "OFA's description:") plus the question ("does the image describe '<hypothesis>'?") and choices; Cola-FT is instruction-finetuned (teacher forcing, greedy decoding) for one epoch on the dataset. Cola-Zero uses the same prompt for 0-/k-shot in-context evaluation without finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Coordinator LLM fuses VLM-produced captions and plausible answers in natural-language prompts and learns to weight/interpret each VLM's signals; two adaptation modes: (1) Cola-FT (instruction finetuning of LLM only) and (2) Cola-Zero (in-context few-/zero-shot). Analysis tools include input-gradient saliency to inspect token relevance and optional rationale-generation prompting (training to output rationales before answers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cola-FT (FLAN-T5 11B) accuracy on VSR (test, zero-shot split): 67.0% (multiple-choice accuracy); Cola-Zero (FLAN-T5 11B) 0-shot: 55.8%, 2-shot: 54.9%. Baselines: VisualBERT 54.0%, LXMERT 63.2%, ViLT 62.4%, simple ensemble baseline 51.4%. (All metrics as reported in Table 2 of the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Empirical improvement of Cola-FT over single VLMs and ensemble baselines on VSR indicates the LLM learned to integrate visual cues to answer spatial relation questions; saliency visualizations (input-gradient attribution) show Cola-FT attends to question, choices, and VLM plausible answers for spatial outputs; qualitative examples (figures) demonstrate correct coordination to infer spatial relations even when individual VLM answers are noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared with single-model baselines and prior VSR models: Cola-FT (67.0%) outperforms VisualBERT (54.0%), LXMERT (63.2%), ViLT (62.4%) and ensemble baselines (51.4%). Cola-Zero (in-context) underperforms Cola-FT but still outperforms simple ensemble baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported failure modes include cases where VLM captions/answers are insufficient or incorrect and the coordinator LLM is misled (examples shown in Figure 11); swapping or perturbing VLM caption/answer labels at training/eval significantly degrades performance, indicating reliance on correct labelling of which VLM provided which signal; Cola-Zero performance drops with long inputs and does not scale well with too many VLMs (Cola-Zero saturates earlier); rationale-generation did not improve accuracy and sometimes hurt performance due to low-quality ground-truth rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Visual Reasoning Coordinators', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8320.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8320.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLEVR (Cola-FT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLEVR compositional visual reasoning evaluation with Cola instruction-finetuned coordinator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of the Cola paradigm (LLM coordinator integrating VLM captions/answers) on CLEVR, a synthetic dataset testing compositional and spatial visual reasoning (attributes, counting, comparisons, and spatial relationships).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5 (LLM coordinator) with InstructBLIP / InstructBLIP-based VLMs in some configurations</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>FLAN-T5 used as a coordinator LLM (instruction-finetuned in Cola-FT) that ingests VLM textual outputs (e.g., InstructBLIP captions/answers) and produces the answer; experiments include InstructBLIP XL/XXL as VLMs paired with FLAN-T5 coordinator.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>FLAN-T5 coordinator 11B in reported Cola-FT CLEVR results; InstructBLIP VLMs of XL (3B) and XXL (11B) sizes are referenced in the experimental table.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>CLEVR (Compositional Language and Elementary Visual Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Synthetic scene-based compositional reasoning with many questions testing spatial relations, attribute identification, counting, comparisons and logical operators; requires precise spatial/compositional understanding of rendered 3D scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Cola prompt template similar to other tasks: concatenate captions and plausible answers from VLMs and the CLEVR question/choices; Cola-FT instruction-finetuned (one epoch) FLAN-T5 coordinator outputs answers. Cola-Zero in-context 2-shot also evaluated in some configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Coordinator LLM integrates multiple VLM textual signals; no explicit symbolic program execution or external symbolic module is used — reasoning relies on natural-language coordination and LLM's learned reasoning capacity; optional training to output rationales was explored but did not improve final accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cola-FT (InstructBLIP XL+XXL with FLAN-T5 11B coordinator) achieves 54.3% accuracy on CLEVR (Table 2). Cola-Zero (InstructBLIP + FLAN-T5 11B) 2-shot achieves 34.4%. For comparison, InstructBLIP alone reported 33.7% (XL) and 16.6% (XXL) in the same table entries.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Improvement of Cola-FT over single VLMs on CLEVR implies the coordinator LLM leverages VLM-generated scene descriptions to answer compositional spatial questions; qualitative examples and ablations indicate the LLM uses captions and plausible answers to infer spatial relations and compositional attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to InstructBLIP alone (reported XL/XXL numbers), Cola-FT shows higher accuracy on CLEVR (54.3% vs. 33.7%/16.6% reported for InstructBLIP variants). Cola-Zero in-context performance is substantially lower than Cola-FT but comparable to some VLM-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>CLEVR experiments reveal that naive prompting without finetuning (Cola-Zero) is weak on this synthetic compositional task; training to output rationales did not improve performance (possibly due to low-quality rationale supervision); longer captions do not improve and can reduce performance; errors arise when VLM captions/answers omit or misdescribe critical spatial relationships, leading the coordinator to incorrect choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Visual Reasoning Coordinators', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning <em>(Rating: 2)</em></li>
                <li>Visual Spatial Reasoning <em>(Rating: 2)</em></li>
                <li>Socratic Models: Composing zero-shot multimodal reasoning with language <em>(Rating: 1)</em></li>
                <li>From images to textual prompts: Zero-shot vqa with frozen large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8320",
    "paper_id": "paper-264591439",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "VSR (Cola-FT)",
            "name_full": "Visual Spatial Reasoning dataset evaluation with Cola instruction-finetuned FLAN-T5 coordinator",
            "brief_description": "Evaluation of an instruction-finetuned FLAN-T5 coordinator LLM (Cola-FT) that integrates captions and plausible answers from multiple VLMs (OFA, BLIP) to solve Visual Spatial Reasoning (VSR) multiple-choice tasks involving spatial relations (e.g., under, in front of, facing).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLAN-T5 (as coordinator in Cola-FT)",
            "model_description": "Encoder-decoder transformer LLM (FLAN-T5) instruction-finetuned to act as a coordinator: it ingests labeled captions and plausible answers produced by vision-language models (OFA, BLIP) and outputs a choice. The paper studies multiple FLAN-T5 sizes and also reports in-context (Cola-Zero) behavior.",
            "model_size": "11B (primary reported size for Cola-FT / Cola-Zero evaluations); additional experiments reported across sizes from ~80M up to 11B",
            "puzzle_name": "Visual Spatial Reasoning (VSR) dataset",
            "puzzle_type": "Image-based spatial relation classification; tests ~65 spatial relations (e.g., under, in front of, facing) requiring visual spatial understanding and relational positioning",
            "task_setup": "Multiple-choice formulation: prompt template concatenates VLM captions (OFA, BLIP) and their plausible answers with explicit labels (e.g., \"OFA's description:\") plus the question (\"does the image describe '&lt;hypothesis&gt;'?\") and choices; Cola-FT is instruction-finetuned (teacher forcing, greedy decoding) for one epoch on the dataset. Cola-Zero uses the same prompt for 0-/k-shot in-context evaluation without finetuning.",
            "mechanisms_or_strategies": "Coordinator LLM fuses VLM-produced captions and plausible answers in natural-language prompts and learns to weight/interpret each VLM's signals; two adaptation modes: (1) Cola-FT (instruction finetuning of LLM only) and (2) Cola-Zero (in-context few-/zero-shot). Analysis tools include input-gradient saliency to inspect token relevance and optional rationale-generation prompting (training to output rationales before answers).",
            "performance_metrics": "Cola-FT (FLAN-T5 11B) accuracy on VSR (test, zero-shot split): 67.0% (multiple-choice accuracy); Cola-Zero (FLAN-T5 11B) 0-shot: 55.8%, 2-shot: 54.9%. Baselines: VisualBERT 54.0%, LXMERT 63.2%, ViLT 62.4%, simple ensemble baseline 51.4%. (All metrics as reported in Table 2 of the paper.)",
            "evidence_of_spatial_reasoning": "Empirical improvement of Cola-FT over single VLMs and ensemble baselines on VSR indicates the LLM learned to integrate visual cues to answer spatial relation questions; saliency visualizations (input-gradient attribution) show Cola-FT attends to question, choices, and VLM plausible answers for spatial outputs; qualitative examples (figures) demonstrate correct coordination to infer spatial relations even when individual VLM answers are noisy.",
            "comparisons": "Directly compared with single-model baselines and prior VSR models: Cola-FT (67.0%) outperforms VisualBERT (54.0%), LXMERT (63.2%), ViLT (62.4%) and ensemble baselines (51.4%). Cola-Zero (in-context) underperforms Cola-FT but still outperforms simple ensemble baselines.",
            "limitations_or_failure_cases": "Reported failure modes include cases where VLM captions/answers are insufficient or incorrect and the coordinator LLM is misled (examples shown in Figure 11); swapping or perturbing VLM caption/answer labels at training/eval significantly degrades performance, indicating reliance on correct labelling of which VLM provided which signal; Cola-Zero performance drops with long inputs and does not scale well with too many VLMs (Cola-Zero saturates earlier); rationale-generation did not improve accuracy and sometimes hurt performance due to low-quality ground-truth rationales.",
            "uuid": "e8320.0",
            "source_info": {
                "paper_title": "Large Language Models are Visual Reasoning Coordinators",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CLEVR (Cola-FT)",
            "name_full": "CLEVR compositional visual reasoning evaluation with Cola instruction-finetuned coordinator",
            "brief_description": "Evaluation of the Cola paradigm (LLM coordinator integrating VLM captions/answers) on CLEVR, a synthetic dataset testing compositional and spatial visual reasoning (attributes, counting, comparisons, and spatial relationships).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLAN-T5 (LLM coordinator) with InstructBLIP / InstructBLIP-based VLMs in some configurations",
            "model_description": "FLAN-T5 used as a coordinator LLM (instruction-finetuned in Cola-FT) that ingests VLM textual outputs (e.g., InstructBLIP captions/answers) and produces the answer; experiments include InstructBLIP XL/XXL as VLMs paired with FLAN-T5 coordinator.",
            "model_size": "FLAN-T5 coordinator 11B in reported Cola-FT CLEVR results; InstructBLIP VLMs of XL (3B) and XXL (11B) sizes are referenced in the experimental table.",
            "puzzle_name": "CLEVR (Compositional Language and Elementary Visual Reasoning)",
            "puzzle_type": "Synthetic scene-based compositional reasoning with many questions testing spatial relations, attribute identification, counting, comparisons and logical operators; requires precise spatial/compositional understanding of rendered 3D scenes.",
            "task_setup": "Cola prompt template similar to other tasks: concatenate captions and plausible answers from VLMs and the CLEVR question/choices; Cola-FT instruction-finetuned (one epoch) FLAN-T5 coordinator outputs answers. Cola-Zero in-context 2-shot also evaluated in some configurations.",
            "mechanisms_or_strategies": "Coordinator LLM integrates multiple VLM textual signals; no explicit symbolic program execution or external symbolic module is used — reasoning relies on natural-language coordination and LLM's learned reasoning capacity; optional training to output rationales was explored but did not improve final accuracy.",
            "performance_metrics": "Cola-FT (InstructBLIP XL+XXL with FLAN-T5 11B coordinator) achieves 54.3% accuracy on CLEVR (Table 2). Cola-Zero (InstructBLIP + FLAN-T5 11B) 2-shot achieves 34.4%. For comparison, InstructBLIP alone reported 33.7% (XL) and 16.6% (XXL) in the same table entries.",
            "evidence_of_spatial_reasoning": "Improvement of Cola-FT over single VLMs on CLEVR implies the coordinator LLM leverages VLM-generated scene descriptions to answer compositional spatial questions; qualitative examples and ablations indicate the LLM uses captions and plausible answers to infer spatial relations and compositional attributes.",
            "comparisons": "Compared to InstructBLIP alone (reported XL/XXL numbers), Cola-FT shows higher accuracy on CLEVR (54.3% vs. 33.7%/16.6% reported for InstructBLIP variants). Cola-Zero in-context performance is substantially lower than Cola-FT but comparable to some VLM-only baselines.",
            "limitations_or_failure_cases": "CLEVR experiments reveal that naive prompting without finetuning (Cola-Zero) is weak on this synthetic compositional task; training to output rationales did not improve performance (possibly due to low-quality rationale supervision); longer captions do not improve and can reduce performance; errors arise when VLM captions/answers omit or misdescribe critical spatial relationships, leading the coordinator to incorrect choices.",
            "uuid": "e8320.1",
            "source_info": {
                "paper_title": "Large Language Models are Visual Reasoning Coordinators",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
            "rating": 2,
            "sanitized_title": "clevr_a_diagnostic_dataset_for_compositional_language_and_elementary_visual_reasoning"
        },
        {
            "paper_title": "Visual Spatial Reasoning",
            "rating": 2,
            "sanitized_title": "visual_spatial_reasoning"
        },
        {
            "paper_title": "Socratic Models: Composing zero-shot multimodal reasoning with language",
            "rating": 1,
            "sanitized_title": "socratic_models_composing_zeroshot_multimodal_reasoning_with_language"
        },
        {
            "paper_title": "From images to textual prompts: Zero-shot vqa with frozen large language models",
            "rating": 1,
            "sanitized_title": "from_images_to_textual_prompts_zeroshot_vqa_with_frozen_large_language_models"
        }
    ],
    "cost": 0.015414,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models are Visual Reasoning Coordinators</p>
<p>Liangyu Chen 
Bo Li 
Sheng Shen 
Jingkang Yang 
Chunyuan Li 
Kurt Keutzer 
Trevor Darrell 
Ziwei Liu ziwei.liu@ntu.edu.sg </p>
<p>♥ S-Lab
Nanyang Technological University ♣ University of California
Berkeley</p>
<p>Microsoft Research
Redmond</p>
<p>Large Language Models are Visual Reasoning Coordinators
B894D72CC872F65CA11F2F3287C3ADD9
Visual reasoning requires multimodal perception and commonsense cognition of the world.Recently, multiple vision-language models (VLMs) have been proposed with excellent commonsense reasoning ability in various domains.However, how to harness the collective power of these complementary VLMs is rarely explored.Existing methods like ensemble still struggle to aggregate these models with the desired higher-order communications.In this work, we propose Cola, a novel paradigm that coordinates multiple VLMs for visual reasoning.Our key insight is that a large language model (LLM) can efficiently coordinate multiple VLMs by facilitating natural language communication that leverages their distinct and complementary capabilities.Extensive experiments demonstrate that our instruction tuning variant, Cola-FT, achieves state-of-the-art performance on visual question answering (VQA), outside knowledge VQA, visual entailment, and visual spatial reasoning tasks.Moreover, we show that our in-context learning variant, Cola-Zero, exhibits competitive performance in zero and few-shot settings, without finetuning.Through systematic ablation studies and visualizations, we validate that a coordinator LLM indeed comprehends the instruction prompts as well as the separate functionalities of VLMs; it then coordinates them to enable * Equal Contribution.†Project Lead.</p>
<p>Introduction</p>
<p>Visual reasoning is a crucial task that demands models to not only comprehend and interpret visual information but also to apply high-level cognition to derive logical solutions [40,120,60].The field has received significant attention from the machine learning community because of its potential to enable a wide range of intelligent applications, such as intelligent tutoring systems [5,94,69], automated image captioning [103], and virtual assistants [88,50].To perform visual reasoning effectively, a model must possess both visual perception capabilities and strong logic reasoning abilities.</p>
<p>While classic visual reasoners typically rely on complex architectures [117,61,116] or are unable to generalize beyond the training dataset [121,72], recent advancements in large pretrained models have shown that vision-language models (VLMs) can achieve impressive performance on visual reasoning tasks even under zero-shot settings [104,52,51,49,48,3].Meanwhile, large language models (LLMs) have also demonstrated robust zero-shot commonsense reasoning abilities on the natural language processing (NLP) applications [8,15,109].Several recent studies have attempted to combine such complementary VLMs and LLMs for visual reasoning.For example, PICa [115] utilizes image captioning models to generate textual prompts for GPT-3 [8], and adapts GPT-3 to solve the visual question answering (VQA) tasks in an in-context few-shot learning manner.Socratic Models [123] allow VLMs and LLMs to communicate through prompt engineering to unlock zero-shot multimodal reasoning capabilities.</p>
<p>On the premise that current studies have focused on the interactions among heterogeneous models (specifically, among VLM and LLMs), in this work, we shift to examine how to reconcile homogeneous expert models (e.g., multiple VLMs) with an LLM in a coordinative paradigm.Inspired by the findings in CICERO [65] that LLMs capture strong strategic planning and negotiation abilities in coordinating multiple agents, we propose Cola, a novel model ensemble approach that utilizes an LLM as the coordinator in between multiple VLMs.A key finding of this study is that given multiple VLMs with different preferred patterns in describing the visual context and predicting plausible answers in natural languages, an LLM can coordinate and integrate their respective strengths efficiently and effectively.We present two variants of Cola, namely Cola-FT and Cola-Zero, where FT corresponds to an instruction finetuning approach and Zero stands for an in-context learning approach to adapt the coordinator LLM for visual reasoning.Figure 1 provides an overview of Cola and conventional model ensemble approaches.</p>
<p>Existing work on model ensembles usually focuses on manipulating model weights [36] or predictions [111,22], while remaining cumbersome, if possible, to implement on prevalent end-to-end black box model APIs, like GPT-4 [70], Google Bard, Anthropic Claude, etc.To address this issue, prompt ensembles [107,106,73] sample model outputs (e.g., rationales) in natural languages to boost chainof-thought reasoning [109].Recent studies on augmented LLM such as [86,93,58] have delved into developing a comprehensive strategy that enables LLMs to utilize external tools.These tools comprise multiple off-the-shelf models, web search engines, Python functions [97], and rule-based modules, which are instrumental in performing complex tasks.Despite these efforts, the power of prompt ensembles to aggregate multiple models remains untouched.In contrast, we show that Cola leverages language prompts generated from multiple expert models to make model ensembles.</p>
<p>Systematic experiments demonstrate that Cola performs at the pinnacle of ability on VQA, outside knowledge VQA, visual entailment, and visual spatial reasoning tasks.Specifically, Cola-FT achieves state-of-the-art performance on A-OKVQA [89], OK-VQA [63], e-SNLI-VE [21], and VSR datasets [56], even when compared with methods that adopt larger models or require more training computations.Cola-FT also demonstrates competitive capabilities on VQA v2 [27], and compositional reasoning tasks (GQA [35] and CLEVR [40]).Perhaps surprisingly, we find that Cola-Zero demonstrates comparable performance without finetuning, as an emerging ability of larger language models.Compared to a single VLM and ensemble modeling, both Cola-FT and Cola-Zero improve the performance substantially across most datasets.They are even effective with the recent large multimodal models like InstructBLIP [18] which embeds a pretrained LLM inside itself.Besides, we conduct a thorough analysis of perturbed VLM caption or plausible answer labels and saliency visualization to investigate how Cola recognizes each VLM's individual functionalities and then performs coordination behaviors.We conjecture that, in principle, any language-expressing reasoning task can be usefully augmented with coordinative language models that learn to aggregate multiple expert models, even via in-context learning.</p>
<p>In summary, our contributions are as follows:</p>
<p>• Cola: a novel paradigm that utilizes a language model as a coordinator between multiple vision-language models to integrate their respective strengths for visual reasoning ( §2).• State-of-the-art performance: Cola achieves the pinnacles on a challenging suite of diverse visual reasoning tasks and datasets( §3.2). • Systematic analysis: our experiments reveal how Cola comprehends the instruction prompts, then coordinates them to capture impressive visual reasoning capabilities ( §3.3, §3.4,§3.6).</p>
<p>Cola</p>
<p>We formulate various visual reasoning tasks as a multi-class classification problem for simplicity.</p>
<p>Given an image v ∈ V and a question-like prompt q ∈ Q, the reasoner is required to select an answer a from the candidate set A = {a}.In the case that the reasoner outputs a text sequence s v,q , we map s to a prediction P (v, q) = sim(T (s v,q ), T ({a})) where T transforms text sequences into text embeddings (we use a all-mpnet-base-v2 model [81] here), and sim denotes cosine similarity.</p>
<p>Ensemble Modeling aggregates multiple models' predictions in order to improve the overall performance (Figure 1a).For instance, one common practice is averaging over n models:
P (v, q) = 1 n n i=1 P i (v, q),(1)
where P i (v, q) denotes the prediction of the i th model on input (v, q).</p>
<p>Cola &amp; Templates General Prompt Template</p>
<p>Answer the following multiple-choice question by OFA and BLIP's description and their answers to the visual question.OFA and BLIP are two different vision-language models to provide clues.OFA's description: <OFA caption> BLIP's description: <BLIP caption> Q: <Question> OFA's answer: <OFA answer> BLIP's answer: <BLIP answer> Choices: <Choices to the question> A: Table 1: LM prompt template.The LM is instructed to coordinate VLMs.Each question set defines visual context, question (and choices), and plausible answers.</p>
<p>An overview of Cola is shown in Figure 1c.We use OFA [104] and BLIP [52] as the VLMs.LLMs include encoder-decoder (FLAN-T5 [16]) and decoder-only (Vicuna-1.5[125], Mistral [39]) transformers.We first prompt each VLM to output captions and plausible answers independently.We then concatenate the instruction prompt, the question with choices, captions, and plausible answers to fuse all contexts for the LLM to reason, coordinate, and answer.</p>
<p>Image captioning gives important visual context to reason from.We first employ i th VLM to describe each image respectively to get visual descriptions c i (v).We use ofa-large for OFA and blip-image-captioning-large for BLIP, both implemented by the Hugging Face Transformers library [110].</p>
<p>Plausible answers by the VLMs to the question provide clues and patterns of VLMs for the LM to consider and coordinate.Similar to captioning, we prompt each i th VLM using the image-question pair to get a plausible answer âi (v, q).We use ofa-large for OFA and blip-vqa-base for BLIP.Following OFA, our prompt template varies by task category.For the VQA tasks, we leave the original question unchanged.For the visual entailment and visual spatial reasoning tasks, our prompt template is " does the image describe "<text premise>" ?".</p>
<p>Prompt template is shown in Table 1.First, we designed an instruction prompt for LM to understand the requirement to coordinate VLMs to answer the visual reasoning question.We then concatenate the captions from each VLM model, with the VLM identification labels in natural languages (referred to as VLM caption labels in Figure 3), such as "OFA's description: <OFA caption>".Next, the question and its plausible answers provided by VLMs (with similar identification labels referred to as VLM answer labels in Figure 3) are concatenated.We follow [16] to include the choices of question (for multiple-choice questions in A-OKVQA, e-SNLI-VE, and VSR) and "A:" to prompt for answers.Overall, the prompt for LLM input is given by:
Prompt(v, q) = Template({(c i (v), âi (v, q)) | i = 1, • • • , n}).
(2)</p>
<p>More specific prompt templates on each dataset are provided in Appendix A.7.</p>
<p>Cola-FT</p>
<p>Instruction Tuning of Cola is initialized with pretrained checkpoints.Given the question q based on the image v, the LM predicts the answer in the form of sequence
s v,q = LLM(Prompt(v, q)).(3)
To optimize the LLM, we use the language modeling loss for next-token prediction, with the teacher forcing strategy.We only finetune the LLM (while not the VLMs) to follow the common paradigm of ensemble modeling and simplify the method (Figure 1).</p>
<p>Inference deploys the same prompt as Table 1 to align with instruction tuning.We resort to the greedy decoding strategy for conditional sequence generation at both instruction tuning and inference.</p>
<p>Cola-Zero</p>
<p>In-context learning is an emerging ability of the LLM models pretrained on documents of longrange coherence.By learning input and output format from demonstration, in-context learners learn to perform a downstream task simply by conditioning on a prompt consisting of input-output examples [114].The coordinator LLM, finetuned on instruction prompts with examples, is capable of in-context few-shot learning and zero-shot learning (see Figures 6 and 7).</p>
<p>Cola-Zero is the in-context few-shot/zero-shot learning variant of Cola, without instruction tuning.</p>
<p>For in-context k-shot learning, we modify the prompt (Table 1) to include k input-output examples sampled from the training set.For zero-shot learning, the prompt remains the same as Table 1.</p>
<p>Experiments</p>
<p>First, the experimental setups and basic methods are described in this section.The main quantitative results are then presented in Table 2. Next, we analyze qualitative visualizations and scaling to verify the effectiveness of the Cola paradigm in different settings.Further details on datasets, training, evaluation, and experimental analysis can be found in Appendix A.</p>
<p>Baseline Methods</p>
<p>State-of-the-art Methods are in two broad categories, VLM alone, and VLM combined with LLM.In Table 2, for a fair comparison, we detail the techniques (whether finetuning or in-context learning is required) used for training VLMs and LLMs, and the number of training epochs.</p>
<p>Ensemble Modeling can be considered the most basic baseline for aggregating VLMs.It represents the base performance that the combination of VLMs can achieve on the target task when not trained.</p>
<p>We implement an averaging ensemble (Equation (1)) of cosine similarity between VLM output and each choice of a question as our ensemble baseline.</p>
<p>Overall Performance</p>
<p>In Table 2, we first observe that Cola-FT achieves state-of-the-art (SOTA) performance on four datasets (A-OKVQA, OK-VQA, e-SNLI-VE, VSR), with merely 1 epoch of instruction tuning and a medium-sized language model.In contrast, many previous SOTA methods require finetuning more epochs than Cola-FT (e.g., VLC-BERT, PromptCap on A-OKVQA).Some also use much larger language models, such as GPT-3 (175B) [8] and OPT (175B) [124].Cola-FT outperforms OFA-X on e-SNLI-VE, although the latter is finetuned on much more related tasks and data (c.f.Cola-Zero answer forehand stove/oven no no</p>
<p>Cola-FT answer forehand stove/oven maybe no</p>
<p>Cola-FT answer</p>
<p>(swapped VLM answer labels) backhand microwave maybe yes Cola-FT is trained on each one dataset only in Table 2).In addition, the lighter variant Cola-Zero also achieves comparable performance to most baseline methods through in-context few-shot and zeroshot learning, without training any model parameter.To evaluate the performance of Cola with large multimodal models that embed large language models inside, we also assembled InstructBLIP [18] models based on FLAN-T5 XL and XXL and tested on A-OKVQA dataset.In Table 2, we report the multiple-choice accuracies on A-OKVQA.See Appendix A.4 for direct answer results.</p>
<p>Qualitative Examples</p>
<p>In Figure 2, we exhibit several qualitative examples.The language coordinator determines the correctness of VLM plausible answers implicitly, given their captions and the caption and answer labels.The leftmost example (a tennis player playing) demonstrates a case when captions are not informative to guide the LLM for predictions.Between OFA and BLIP's plausible answers, the LLM follows the answer of BLIP.In contrast, in the left example (an oven next to a fridge), again with trivial captions, the LLM follows OFA's plausible answer instead.</p>
<p>It's all plausible answers, captions, VLM answer/caption labels, and the world knowledge the LLM encodes in itself, that contribute to the final decision of the language coordinator.The rightmost example presents the scenario of inconsistency between captions and answers.OFA describes the image as "an elephant is loaded onto a truck in yangon."Though, it agrees that "the truck is away from the elephant".With Cola-FT, The LLM coordinates OFA's correct caption and BLIP's correct answer to make a reasonable prediction.</p>
<p>Notably, we observe a scenario in which captions can be more informative than plausible answers to guide LLM.Next, we perturb caption labels by swapping the VLM caption labels at instruction tuning and evaluation (#5), specifically "OFA's description: " and "BLIP's description: ", by a chance of 50%.Under such settings, the LLM fails to acquire the preferred patterns of VLM for captioning, though the overall visual context is preserved.The results underperform Cola-FT, which verifies that VLM caption labels improve Cola-FT performance.Notably, the VLM (plausible) answer labels are more important to the LLM's decision: a considerable gap exists between (#6) and Cola-FT.In #6, the LLM fails to learn the separate functionalities of VLM when answer labels are perturbed.This highlights that the performance gained from the coordination between BLIP and OFA, but not the strong reasoning capabilities of the LLM, FLAN-T5.</p>
<p>Naturally, we ask what if the LLM can learn the patterns each VLM answers, but they cannot apply it at inference?We input correct VLM answer labels at instruction tuning and swap labels at evaluation (#7).Consequently, #7 falls behind Cola-FT with a smaller but still considerable margin.The results suggest that learning and applying the separate functionalities of VLMs is important for the coordinator LLM to make predictions.See Appendix A.9 for more ablation studies.Scaling Cola with More VLMs.By decoding the top-k (k=5) results from three identical (OFA-base) models on the A-OKVQA validation set, both the answers and captions may exhibit slight variations.Cola demonstrated significant performance improvements compared to a single VLM or ensemble, as shown in Table 3.Furthermore, the performance gap between the ensemble baselines and Cola based on three different models (OFA-tiny/medium/base) is even more substantial, as depicted in Table 4.We further scale the number of OFA-base and find that Cola-FT satures at 5 VLMs (49.77%) and Cola-Zero (2-shot in-context learning) satures at 3 VLMs (47.71%).We observe that long input harms the performance of Cola-Zero, which is negative for scaling with more VLMs (Figure 4).LMs with a larger context window size [12,68,9] are promising to further improve the performance of Cola, which we leave for future works.</p>
<p>Scaling Cola</p>
<p>Scaling Model Size.We conduct experiments on scaling the coordinator LLM size to see if there are ramifications when operating at a larger scale.Figure 5 reveals that Cola-FT performance increases as the LLM (FLAN-T5) model size increases.Notably, Cola-FT/small, with only 80M parameters, could achieve 65% MC accuracy on A-OKVQA validation set, which is far beyond our baseline methods (55%).Cola-Zero, under the in-context learning paradigm, achieves competitive performance when the model grows to a billion-parameter scale.This observation on Cola-Zero can be regarded as a proof-of-concept that potentially reveals Cola-Zero's emerging abilities (inherited from FLAN-T5 [16]) on visual reasoning tasks at a relatively large scale.Cola-FT is effective with small models, but Cola-Zero is an emerging ability on larger models only.x-axis is distorted for optimal display.In-context Learning &amp; Low-data Instruction Tuning.We conduct experiments on different data scales to verify Cola's performance varying from zero-shot to full-shot under in-context learning and full-finetune paradigm.As shown in Figure 6, with Cola-Zero, few-shot exemplars substantially improve performance compared to zero-shot learning.As [16,108] revealed, exemplars potentially help the model better understand the output format and understand the instructions in Table 1.Cola-Zero for in-context few-shot learning outperforms zero-shot learning by a large margin, being on par with low-data Cola-FT without instruction tuning.We also observe Cola-FT's substantial performance gain when finetuning shots increase to 1000 and beyond.</p>
<p>80M</p>
<p>Saliency Visualization</p>
<p>As shown in Figure 7, we visualize the importance of the input prompt tokens by input-gradient saliency feature attribution [19], implementing with Ecco [2].The input tokens that are more relevant to predict the output token "grass" are highlighted in darker colors.In the given example, both Cola-FT and Cola-Zero predict the correct answer and find the relevant clues from visual context and plausible answers.Figure 7(b) shows that Cola-Zero attributes the output more to the instructions in the prompt template.This explains Cola-Zero's competitive performance, a consequence of FLAN instruction tuning [108].After instruction tuning, Cola-FT focuses more on the most informative parts of input: the question, choices, as well as VLMs' plausible answers.</p>
<p>Can Cola-FT Explain its Answers?</p>
<p>We modify the prompt template of Cola so that the model would output the logical conduction process, allowing us to observe the specific behaviors of the LLM during its coordination of VLMs.We finetune Cola-FT to output rationales before answers, by A-OKVQA ground truth rationales.In the modified prompt, we ask the model to provide rationales.For the leftmost example of Figure 8, Cola-FT outputs "Rationale: People might sit here to rest.The umbrellas are on the riverwalk.The answer is:rest ".OFA gives a reasonable answer (but out of choices) to the question while BLIP gives an irrelevant answer.In this case, both answers are wrong.However, either Cola-Zero or Cola-FT is able to infer from captions and plausible answers to give the correct answer "to rest".The rationale suggests that the LLM understands the scene that the umbrellas are on the riverwalk and guesses that people might sit here to rest based on commonsense.The final answer is correct.For the leftmost example of Figure 11, the rationale output is "The bike is parked in a no parking zone.The bike is parked next to a pedestrian crossing sign.The answer is:no parking".Both VLMs are wrong in their plausible answers.OFA's answer "boating" is semantically correct as the correct answer is "kayaking", though it's not the correct answer because this is a multiple-choice question.Cola-Zero gives a wrong answer "OFA" which is obviously wrong because "OFA" is the name of one of the VLMs given in the prompt and it's out of the choices too.However, Cola-FT gives the correct answer "kayaking", recognizing the correct choice based on prompts of captions and plausible answers after being finetuned.Even though the OFA and BLIP captions fail to identify that the people in the water are on a canoe.The LLM identifies that the people in the water are associated with the canoe.The rationale is valid and helpful, though repetitive.The final answer is correct.</p>
<p>To force the LLM to output rationale does not improve the reasoning performance of Cola (w/t rationale 74.3% vs. w/o rationale 77.7%, on A-OKVQA val set).This might be attributed to the low-quality ground truth rationales provided by the A-OKVQA dataset that we use to train the LLM.Such rationales are just short and objective descriptions of the scene, without suggesting the underlying outside knowledge to answer the question.Therefore, training the LLM to output rationale is harmful, though it derives insights into the LLM's behaviors during reasoning.</p>
<p>3.8 Does Cola-FT Transfer across Tasks?Table 5: Cola-FT is generalizable across out-ofdistribution tasks.In most cases, the performances surpass Cola-Zero (in-context 2-shot learning results in brackets) on target datasets.We examine Cola-FT's generalization ability across tasks.From Table 5, we observe the zero-shot performances on target datasets after instruction tuning on a certain source dataset.</p>
<p>Although each dataset varies in question types and prompt templates (see detailed comparisons in Appendix A.7), we find that Cola-FT maintains competitive performance when zero-shot transferred to a new task, outperforming Cola-Zero in-context 2-shot learning and ensemble baselines (see also Table 2).</p>
<p>Related Work</p>
<p>Visual Reasoning.Beyond unimodal reasoning tasks such as question answering (QA) [100,14,119,7], visual reasoning extends high-level cognition to visual domains, requiring an intelligent agent to derive rational solutions [40,35,84,120,38,112].Several tasks have been introduced to address visual reasoning, such as VQA [1], in which models are expected to provide answers to questions related to an image, and visual entailment [113], where the model is required to determine if a text description is consistent with the visual content provided.</p>
<p>Classic visual reasoning methods employ an image encoder along with a reasoning block that utilizes attention mechanisms [102,121,122,105], neuro-symbolic methods [101,117,61], or external knowledge [62,28,13].</p>
<p>Recent progress in large pretrained models has led to the development of LLMs that capture exceptional commonsense reasoning capabilities [78,16,15].These LLMs can potentially replace the reasoning module in visual reasoning tasks, and LLMs' lack of perception can be compensated by incorporating multiple VLMs trained on different domains [76,104,52].However, there is still a lack of research on how to harness the collective power of these separate VLMs for visual reasoning tasks.More related works are in Appendix B.</p>
<p>Model Ensemble.Model ensemble is a powerful machine learning technique that combines the predictions of multiple models to improve the overall performance of a given task [20].The variance and bias of the final predictions decrease, resulting in a more robust and accurate model [83].To this end, common methods include averaging [111], voting [37], interpolation [36], weighting the predictions based on model performance [22], or stacking the models [10].</p>
<p>Ensemble methods have been challenging for generative tasks like visual reasoning, where a simple combination is not applicable to heterogeneous models due to their enormous and varying input/output token spaces.To address the issue, Socratic Models (SMs) [123] use prompt engineering to guide the heterogeneous pretrained multimodal models through natural language discussions.With a similar goal, [54] proposes a closed-loop iterative consensus optimization method to utilize the strengths of individual models.However, previous methods do not fully adapt to the intrinsic patterns of different models, particularly in the visual reasoning scenario.Recent studies, such as CICERO [65], have shown that LLMs possess strong social intelligence in coordinating multiple agents, which inspires us to reorganize pretrained mixed-modal models with a focus on adapting LLMs.More recently, Toolformer [86] and HuggingGPT [93] further demonstrate LLMs' abilities to leverage, coordinate, and incorporate the results from external sources such as other models or even APIs to solve complex tasks.While the external tools are called in sequential order in existing work, we study coordinating multiple tools (specifically, expert models) in parallel in this work.</p>
<p>Discussion</p>
<p>Question Format.Datasets like VQA v2 and OK-VQA contain open-ended questions, while A-OKVQA, e-SNLI-VE, and VSR use multiple-choice.Converting VQA v2 and OK-VQA to classification introduces complexities for traditional ensemble methods, as evident in Table 2. Classic methods struggle with generative models like API-based GPT-4, underscoring Cola's value as an endto-end ensemble strategy for extensive (vision-)language models.Moreover, Cola-Zero's efficiency also relies on the question format -it's easier for LLMs to answer when given choices like in A-OKVQA.Conversely, Cola-FT finetunes LLMs to discern answer formats (Figure 7).</p>
<p>Limitations.Visual reasoning is a diverse topic.This work demonstrates the first step toward applying end-to-end language models for visual reasoning.While the LLMs perform well on the discussed datasets, there is a large body of visual reasoning tasks to evaluate in future works, such as intention prediction and rationale explanation.</p>
<p>Future Works.First, exploring the use of non-parametric tools for visual reasoning would be useful to enhance Cola's performance.Second, Cola's use can be extended to other reasoning and planning tasks, such as image generation and action planning, by coordinating multiple models in parallel.Third, by improving inter-model communications, Cola can be more interpretable and safe for high-stakes applications.</p>
<p>Conclusion.</p>
<p>In this paper, we have proposed a novel paradigm for visual reasoning that harnesses the power of multiple VLMs by utilizing a coordination mechanism, where an LLM acts as a coordinator who communicates with VLMs to integrate their respective strengths.Experiments show that reasoning performance is substantially improved by LLM finetuning or in-context learning.Our results provide a promising step towards building multi-component intelligent systems that capture multimodal reasoning capabilities in a human-like way.</p>
<p>Following the common experiment protocols, we employ a teacher forcing and greedy decoding strategy for fine-tuning.</p>
<p>A.3 Evaluation Details</p>
<p>As specified, we use the validation or test set multiple choice accuracy as the evaluation metric.In A-OKVQA, we report val/test accuracy, and val accuracy in e-SNLI-VE, test (zero-shot split) accuracy in VSR.For simplicity and consistency, we evaluate ablation experiments on A-OKVQA validation set.Following the common experiment protocols [32,74], we report the single run results for performance comparison.</p>
<p>The exemplars at the inference of Cola-Zero are randomly sampled from the training set, i.e., supposedly help the LLM learn the input data distribution and output format but do not leak relevant information to the evaluation question.</p>
<p>A.4 A-OKVQA Direct Answer Results</p>
<p>In addition to MC accuracy, we present the direct answer (DA) accuracy of models on the A-OKVQA validation set in Tables 7 and 8. Extension of Figure 6.</p>
<p>A.5 Qualitative Examples</p>
<p>In this section, we provide more qualitative examples on A-OKVQA (Figure 8), e-SNLI-VE (Figure 9), and VSR (Figure 10) datasets.</p>
<p>Due to the large span of the three figures, for better visibility, we put the detailed description directly in each figure's caption part.We illustrate how Cola-FT and Cola-Zero process the VLMs answers in each example.Overall, in these examples, we can observe that even if BLIP and OFA provide wrong answers, Cola can still present the correct answer based on the captions provided by OFA and BLIP, as well as the choice set.This may illustrate how Cola amazingly accomplishes visual reasoning tasks via coordinating BLIP and OFA.</p>
<p>A.6 Failure Cases</p>
<p>In Figure 11, we provide a few failed cases to analyze the specific behavior of Cola.</p>
<p>The leftmost example's correct answer is kayaking, but there are no hints from OFA and BLIP's answers and captions.Therefore Cola-Zero incorrectly provides the answer OFA without sufficient information as hints, while surprisingly Cola-FT answered correctly from OFA's boating answer.</p>
<p>Question</p>
<p>Why might people sit here?</p>
<p>The room can be described as what?</p>
<p>In what type of location are they playing with the body board?</p>
<p>What is in front of the monitor?</p>
<p>OFA caption colorful umbrellas on the riverwalk living room layout and decor medium size how to decorate a small living room dining combo mant person, left, and person look at a painting of a great white shark.</p>
<p>a desk with a computer, a lamp, a laptop, and a plant.The left example again has insufficient information from captions.While BLIP answers no and OFA answers yes, Cola-FT chooses to answer maybe, which looks natural but unfortunately picks the wrong choice.</p>
<p>BLIP caption
a
The right example's captions contain enough information this time.But both Cola-FT and Cola-Zero are misled by BLIP's wrong answer no parking.</p>
<p>The rightmost example also has insufficient information from captions.In this situation, Cola has no choice but to believe either BLIP or OFA's answer, but it mistakenly prefers BLIP's wrong answer.</p>
<p>A.7 Prompt Templates</p>
<p>Across three datasets, the prompt template is roughly the same, with minor differences mainly in the format of the questions and choices.We list the prompt templates adopted in A-OKVQA and e-SNLI-VE/VSR in Table 9 and Table 10, respectively.</p>
<p>A.8 Parameter-efficient Finetuning</p>
<p>To further reduce the computation cost in model adaptation, we explored parameter-efficient finetuning (PEFT) techniques to reduce finetuning parameter counts.Specifically, we use (IA) 3 [57], which finetunes an overhead of 1 million parameters, equivalent to 0.01% of the full parameters of FLAN-T5-XXL.</p>
<p>Compared to full finetuning, (IA) 3 requires more iterations to converge.The performance of a (IA) 3 finetuned FLAN-T5-XXL model is on par with a fully finetuned FLAN-T5-Small (80 million parameters) counterpart (Figure 5).Notably, the former is associated with more computation and memory footprint as a consequence of more parameters in the forward pass.A: Table 10: e-SNLI-VE/VSR prompt template for the LLM.The LLM is instructed to coordinate VLMs.Each question set defines visual context, hypothesis, and plausible answers.</p>
<p>B.2 Instruction-based Learning</p>
<p>Recent advances in the capabilities of language models have piqued researchers' curiosity in the field of instruction-based learning [26,64,87,24].The core of instruction-based learning is to explore the knowledge of the language model itself.In contrast to prompt learning to stimulate the language model's ability to complete blanks, instruction tuning more focuses on activating the language model's comprehension by giving obvious instructions to models and expecting correct feedback.Earlier work [67] finetune BART [46] using instructions and few-shot exemplars in question answering, text classification, and text modification.Their findings suggest that few-shot instruction tuning improves performance on unseen tasks.[66] finetunes GPT-2 Large and also observes that few-shot exemplar instruction tuning could improve performance.[85] finetunes T5-11B with more diverse instruction templates and observe similar improvements in zero-shot learning.More recent work [108] performs large-scale experiments with a 137B FLAN-T5 model and instruction-tune it on over 60 datasets verbalized via instruction templates.They observe FLAN-T5 substantially improves over zero-shot GPT-3 (175B) on 20 of 25 evaluation datasets.OpenAI also releases InstructGPT [71] based on GPT-3 [8], it makes use of human annotations to steer desired model behavior through both instruction Accuracy # Finetuning Params Finetuning 77.73 11B (100%) PEFT, (IA) 3  63.76 1M (0.01%) tuning and reinforcement learning of human feedback.They discover that InstructGPT is favored by humans over unmodified GPT-3.</p>
<p>B.3 Visual Reasoning</p>
<p>Beyond the uni-modal reasoning tasks such as question answering (QA) [100,41,14,80,79,23,75,17,96,25,127,119,7], visual reasoning requires models to not only understand and interpret visual information but also to apply high-level cognition to derive rational solutions [40,35,4,59,60,84,120,34]. Several tasks have been introduced to address visual reasoning, such as visual question answering (VQA) [1], in which models are expected to provide answers to questions related to an image and visual entailment (VE) [113], where the model is required to determine the similarity or relationship between a given image and a description.Classic visual reasoning methods have employed an image encoder and a text encoder, along with a reasoning block that utilizes attention mechanisms [121,72,122,105], neuro-symbolic methods [117,61,116], or external knowledge [62,28,13] to perform reasoning.</p>
<p>Recent progress in large pre-trained models has led to the development of language models (LLMs) that possess exceptional commonsense reasoning capabilities [78,16,15,77].These models can potentially replace the reasoning block in visual reasoning tasks, and LLMs' lack of perception can be compensated by incorporating multiple vision-language models (VLMs) trained on different domains [76,104,52].For example, PICa [115] converts the image into captions that GPT-3 [8] can understand, and adapts GPT-3 to solve the VQA task in a few-shot manner by providing a few in-context VQA examples.However, there is still a lack of research on how to harness the collective power of these complementary VLMs for visual reasoning tasks.</p>
<p>B.4 Model Ensembling</p>
<p>Model ensembling is a powerful machine learning technique that combines the predictions of multiple models to improve the overall performance of a given task [20].Classic model ensembling methods include simple averaging, weighting the predictions based on model performance, and stacking the models.By combining the predictions of multiple models, ensembling can reduce the variance and bias of the final predictions, resulting in a more robust and accurate model [83].Ensemble methods have been shown to perform well in a wide range of tasks, including image classification, natural language processing, and time series forecasting.However, when it turns to multimodal tasks such as visual reasoning, a simple combination is not applicable to heterogeneous models as their inputs and outputs vary.</p>
<p>The Mixture-of-Experts (MoE) [91,82,126,45,47] can be conceptualized as a model ensemble strategy implemented at the level of network architecture.MoE-based multi-modal models [33] excel in leveraging the specific strengths of each expert, thereby delivering the performance that often outstrips that of any individual expert.In these networks, the credibility of each expert's output is dynamically weighted, facilitating a comprehensive and nuanced response to multimodal tasks.</p>
<p>However, even within this sophisticated framework, challenges can arise, particularly when managing heterogeneous pre-trained multimodal models.To address this problem, an innovative approach known as Socratic Models (SMs) [123] has been proposed.SMs employ prompt engineering to guide these diverse models through multimodal discussions, effectively combining their varied knowledge.This method promotes a more harmonious and effective integration of different models, enhancing the ensemble's ability to handle complex tasks.</p>
<p>With a similar goal, [54] proposes a closed-loop iterative consensus optimization method to utilize the strengths of individual models.However, previous methods do not fully explore the potential of a centralized solution or adapt to the separate functionalities of different models, particularly in the visual reasoning scenario.Recent studies, such as CICERO [65], have shown that language models possess strong capabilities in coordinating multiple agents, which inspires us to reorganize pre-trained multimodal models with a focus on the language models.</p>
<p>Broader Impact</p>
<p>This study inherits ethical risks of biases from pretrained VLMs and LLMs, depending on their training data.We suggest the users consider the possible biases in reasoning and prompt the model to interpret its predictions in natural languages when necessary.</p>
<p>OFA caption tennis player hits a return to tennis player during their men's singles second round match at a refrigerator covered in a variety of stickers.a coyote is seen in this undated file photo.(credit: ktla an elephant is loaded onto a truck in yangon.photo: afp BLIP caption a man in a blue shirt is playing tennis a refrigerator with many pictures on it a dog running through the grass in a field a man riding a motorcycle with a truck behind him Choices ['forehand', 'backhand', 'serve', 'dropshot'] ['mixer', 'stove/oven', 'refrigerator', 'microwave'] ['yes', 'maybe', 'no'] ['yes', 'no']</p>
<p>Figure 2 :
2
Figure 2: Qualitative examples.The correct choices are underlined.Leftmost: a commonsense question example of A-OKVQA; LLM follows the answer of BLIP.Left: a visual question example of A-OKVQA; LLM follows the answer of OFA.Right: an example of e-SNLI-VE; LLM chooses another option after coordination.Rightmost: an example of VSR; LLM predicts based on the caption of OFA and the answer of BLIP.Cola-Zero answers are inferenced in zero-shot settings.The bottom row, Cola-FT (swapped VLM answer labels), indicates that the LLM follows the answer of certain VLMs based on their separate functionalities.LLM answers are associated with the distribution of VLM answer labels.</p>
<p>Figure 4 :
4
Figure 4: Scaling of # Models.</p>
<p>Cola-Zero, in-context 2-shot learning Cola-Zero, zero-shot learning</p>
<p>Figure 5 :
5
Figure 5: Cola performances versus the LLM (FLAN-T5) sizes.</p>
<p>Figure 6 :
6
Figure6: Low-data Cola-FT and Cola-Zero performances.x-axis is distorted for optimal display.In-context Learning &amp; Low-data Instruction Tuning.We conduct experiments on different data scales to verify Cola's performance varying from zero-shot to full-shot under in-context learning and full-finetune paradigm.As shown in Figure6, with Cola-Zero, few-shot exemplars substantially improve performance compared to zero-shot learning.As[16,108] revealed, exemplars potentially help the model better understand the output format and understand the instructions in Table1.Cola-Zero for in-context few-shot learning outperforms zero-shot learning by a large margin, being on par with low-data Cola-FT without instruction tuning.We also observe Cola-FT's substantial performance gain when finetuning shots increase to 1000 and beyond.</p>
<p>Figure 7 :
7
Figure7: Visualization of input token saliency.We visualize the relevancy between input tokens and the output token "grass" by feature attribution[19].The more salient tokens are highlighted in darker boxes.Cola-FT focuses on the question, choices, and VLMs' plausible answers in (a).While as shown in (b), Cola-Zero pays extra attention to instructions and VLM labels, as a consequence of FLAN-T5 instruction tuning[16].</p>
<p>e-SNLI-VE / VSR Prompt TemplateAnswer the following multiple-choice question by OFA and BLIP's description and their answers to the visual question.OFA and BLIP are two different vision-language models to provide clues.OFA's description: <OFA caption> BLIP's description: <BLIP caption> Q: does the image describe <hypothesis> ?OFA's answer: <OFA answer> BLIP's answer: <BLIP answer> e-SNLI-VE Choices: [yes, no, maybe] VSR Choices: [yes, no]</p>
<p>Table 2 :
2
Overall performance.Model Spec.denotes specification where we summarize the detailed VLMs and LMs adopted in each method and their parameters.FT and ICT denote finetuning and in-context learning, respectively.Downward arrows indicate that fewer FT and ICT are more efficient.The accuracy metric varies slightly in different datasets.In A-OKVQA, we report both val/test accuracies, and val accuracy in VQA v2, OK-VQA, e-SNLI-VE, GQA, and CLEVR; test (zeroshot split) accuracy in VSR.Upward arrows indicate higher accuracy is better.We mark the best performance on each dataset with bold font and second-best with underlines.
MethodVision-language ModelLarge Language ModelAccuracy ↑Model Spec.FT ↓Model Spec.ICL ↓FT ↓Visual Question Answering (VQA v2)MetaLM [31]Pretrained Encoder350k stepsMetaLM (1.3B)-350k steps41.1PNP-VQA [99]BLIP-Caption (446M)-UnifiedQAv2 [43] (11B) 0-shot-63.3BLIP-2 [51]CLIP [76] (1.2B trainable) 5 epochsFLAN-T5 (3B)--81.6BLIP-2 [51]CLIP [76] (1.2B trainable) 5 epochsOPT [124] (6.7B)--82.2Ensemble----68.0Cola-ZeroBLIP+OFA (384M+472M)-FLAN-T5 (11B)2-shot-69.1Cola-FT-FLAN-T5 (11B)-1 epoch83.7Outside Knowledge Visual Question Answering, Multiple Choice (A-OKVQA)PromptCap [32]OFA (472M)2 epochsGPT-3 (175B)0-shot--/ 73.2Img2Prompt [29]BLIP (384M)-OPT (175B)0-shot-42.9 / 40.7Prophet-MC [90] MCAN-large [118] (56M)6 epochsGPT-3 (175B)16-shot-76.4 / 73.6Ensemble----56.6 / 54.9Cola-Zero Cola-ZeroBLIP+OFA (384M+472M)--FLAN-T5 (11B) FLAN-T5 (11B)0-shot 2-shot--65.4 / 61.6 70.4 / 66.5Cola-FT-FLAN-T5 (11B)-1 epoch77.7 / 74.0Cola-Zero-FLAN-T5 (11B)0-shot-68.0 / 66.5Cola-Zero-FLAN-T5 (11B)2-shot-72.3 / 72.3Cola-FTInstructBLIP [18]-FLAN-T5 (11B)-1 epoch78.1 / 76.7Cola-ZeroXL+XXL-Vicuna (7B)2-shot-63.9 / 63.0Cola-FT(3B+11B)-Vicuna (7B)-1 epoch68.6 / 66.9Cola-Zero-Mistral (7B)2-shot-69.3 / 66.2Cola-FT-Mistral (7B)-1 epoch74.3 / 71.8Outside Knowledge Visual Question Answering, Direct Answer (OK-VQA)PromptCap [32]OFA (472M)2 epochsGPT-3 (175B)0-shot-58.8Prophet [90]MCAN-large [118] (56M)6 epochsGPT-3 (175B)16-shot-61.1Ensemble----39.2Cola-Zero Cola-ZeroBLIP+OFA (384M+472M)--FLAN-T5 (11B) FLAN-T5 (11B)0-shot 2-shot--39.4 39.4Cola-FT-FLAN-T5 (11B)-1 epoch62.4Visual Entailment (e-SNLI-VE)e-UG [42]UNITE (86M)400 epochsGPT-2 (117M)-400 epochs79.5OFA-X [74]OFA (472M)10 epochs---80.9Ensemble----48.8Cola-Zero Cola-ZeroBLIP+OFA (384M+472M)--FLAN-T5 (11B) FLAN-T5 (11B)0-shot 2-shot--56.2 57.8Cola-FT-FLAN-T5 (11B)-1 epoch81.6Visual Spatial Reasoning (VSR)VisualBERT [53]VisualBERT (110M)100 epochs---54.0LXMERT [98]LXMERT (110M)100 epochs---63.2ViLT [44]ViLT (88M)30 epochs---62.4Ensemble----51.4Cola-Zero Cola-ZeroBLIP+OFA (384M+472M)--FLAN-T5 (11B) FLAN-T5 (11B)0-shot 2-shot--55.8 54.9Cola-FT-FLAN-T5 (11B)-1 epoch67.0Compositional Question Answering, Real Images (GQA)BLIP [52]BLIP (384M)----41.7OFA [104]OFA (472M)----58.0VisProg [30]ViLT (88M)-GPT-3 (175B)8-shot-50.5Cola-FTBLIP+OFA (384M+472M)-FLAN-T5 (11B)-1 epoch60.3Compositional Question Answering, Synthetic Images (CLEVR)InstructBLIP [18]XL (3B) XXL (11B)--------33.7 16.6Cola-ZeroInstructBLIP-FLAN-T5 (11B)2-shot-34.4Cola-FTXL+XXL (3B+11B)-FLAN-T5 (11B)-1 epoch54.3
QuestionWhat type of shot is the man hitting?What appliance is next to an appliance that is highly decorated?Does this image describe "puppy running after a stick in grass" ?Does this image describe "The truck is away from the elephant" ?</p>
<p>Table 3 :
3
Performance of ensemble methods based on three identical models.
MethodsA-OKVQAe-SNLI-VEOFA-tiny39.0350.20OFA-medium42.4551.04OFA-base45.7652.60Ensemble (majority voting)46.7153.94Ensemble (average)46.6254.41Cola-Zero (2-shot)49.3757.63Cola-FT54.2663.68</p>
<p>Table 4 :
4
Performance of ensemble methods based on three different models.</p>
<p>Table 6 :
6
Cola-FT training time of FLAN-T5-XXL for each dataset.We finetune a subset of GQA.
V100 hours A-OKVQA e-SNLI-VE VSR GQA VQA v2 OK-VQA CLEVRCola-FT128824801224</p>
<p>Table 7 :
7
A-OKVQA validation set DA performance.Extension of Figure5.
FLAN-T5-SmallFLAN-T5-BaseFLAN-T5-XLFLAN-T5-XXLCola-FT56.560.664.165.4Cola-Zero (2-shot)30.334.657.661.0Cola-Zero (0-shot)28.636.055.059.31-shot 2-shot 3-shot 4-shotCola-Zero60.261.060.759.2</p>
<p>Table 8 :
8
Cola-Zero in-context few-shot learning DA performance on A-OKVQA validation set.</p>
<p>A-OKVQA qualitative examples.Leftmost: LLM doesn't use BLIP and OFA's answers, but may observe from captions to derive the correct final answer.Left: As shown on the left, LLM does not follow the wrong answers from OFA and BLIP but gets the correct answers from captions.Right: With both OFA and BLIP answering incorrectly, LLM derives the correct one from both VLMs' captions and answers.Rightmost: After assessing the questions, answers, and captions, LLM goes with OFA's answer and rewrites it to match the expression in the choices.The correct choices are underlined.Cola-Zero answers are given in zero-shot settings.
colorful umbrellaa dining room table with aa man holding a surfboarda desk with a computerumbrella with colorfulglass table and chairswhile another man isand a lampumbrellasstanding next to himChoices['to testify', 'to rest', 'to['tidy', 'messy', 'on fire',['room', 'beach', 'park',['keyboard', 'phone',shop', 'get tattoo']'destroyed']'store']'mouse', 'headphones']OFA answer to eatliving roombedrooma keyboardBLIP answer yesdining roombeachmonitorCola-Zeroto resttidybeachkeyboardanswerCola-FTto resttidyroomkeyboardanswerFigure 8:</p>
<p>Table 9 :
9
Figure 9: e-SNLI-VE qualitative examples.Leftmost:As the connection to daredevil is not obvious in BLIP and OFA's captions, although Cola-Zero is misled, Cola-FT correctly answers maybe.Left: Similar to the left example, Cola-FT answer correctly as no obvious connections are seen from the captions to this question.Right: Similar to the left example, the fact of catch catfish is not reasonable from the captions, Cola-FT picks the correct answer maybe.Rightmost: As girl gets hit is not obvious in BLIP and OFA's captions and answers, Cola-Zero and Cola-FT both follow BLIP to choose the correct answer no.The correct choices are underlined.Cola-Zero answers are given in zero-shot settings.Answer the following multiple-choice question by OFA and BLIP's description and their answers to the visual question.OFA and BLIP are two different vision-language models to provide clues.VQA prompt template for the LLM, for VQA v2 / OK-VQA / A-OKVQA.The LLM is instructed to coordinate VLMs.Each question set defines visual context, question with choices, and plausible answers.
QuestionDoes the image describe "Does the image describe "Does this image describeDoes this image describeA professional daredevil "?the dog is a shitz " ?"Two twenty-somethings"A little girl gets hit by aprepare to catch salmonwoman riding a bike" ?while other older mencatch catfish" ?OFA caption person doing a flip on aa dog jumping out of themen repairing fishing netsa man and a woman on amountain bikewater.on the beach in zanzibar,tandem biketanzaniaBLIPa man doing a trick on aa dog jumping over rocksa man sitting on a boata man and woman riding acaptionbike in the airin the waterwith a fishing net netbicycle in a parking lotChoices['yes', 'maybe', 'no']['yes', 'maybe', 'no']['yes', 'maybe', 'no']['yes', 'maybe', 'no']OFA answer yesnoyesyesBLIP answer yesnoyesnoCola-ZeroyesnononoanswerCola-FTmaybemaybemaybenoanswerA.9 Extended Ablation StudiesDo caption labels offer useful information to LLM? How would more prompt variations affectthe performance of Cola? We tested Cola-Zero with and without caption labels on A-OKVQAvalidation set, observing a slight decrease in performance when without them (70.39% w/t vs. 69.97%w/o). More ablative experiments showed that removing the VLM's answer labels led to a substantialdrop in performance (70.39% w/t vs. 67.62% w/o). Removing the model characteristic descriptionsalso led to a decrease (70.39% w/t vs. 68.37% w/o).Do longer image captions improve reasoning performance? On A-OKVQA validation set, wetested longer image descriptions (&gt;50 tokens) but found no gain compared to Cola or single VLMs.Longer captions decreased FLAN-T5+OFA's accuracy by 0.61% and FLAN-T5 with BLIP by 0.69%on the A-OKVQA validation set. Cola (captions &lt;30 tokens) reached 77.73%, outperformingindividual VLMs. Longer captions lacked meaningful visual context, possibly due to short text andimage pairs in their training datasets. This experiment reaffirms Cola's effectiveness in aggregatingindividual VLM functionalities.B Extended Related Works
[95]1,6]tuning Large Language ModelsLarge language models[8,71,6]pretrained on massive amounts of unstructured data have gradually demonstrated great performance by finetuning on additional task-specific instances.Finetuning a large language model can be considerably more sample efficient than re-training from scratch, although acceptable performance may still require a considerable quantity of data[95].RecentVQA Prompt Template</p>
<p>Table 11 :
11
[57] 3[57]parameter-efficient tuning (PEFT) performance.We finetune a FLAN-T5-XXL model on the A-OKVQA training set and evaluate it on the A-OKVQA validation set.</p>
<p>BLIP: https://github.com/salesforce/BLIP
OFA: https://huggingface.co/OFA-Sys
AcknowledgementsThis research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2022-01-029). Besides, this project is supported by NTU NAP, MOE AcRF Tier 2 (MOE-T2EP20221-0012), and under the RIE2020 Industry Alignment Fund -Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as advise from the industry partner(s).A Experimental DetailsIn this section, we elaborate on our training and evaluation details, prompt templates, and more qualitative examples for analysis.A.1 DatasetsOur experiments are conducted on a challenging suite of three diverse visual reasoning tasks, including outside knowledge VQA, visual entailment, and visual spatial reasoning.For each task, we select the following dataset respectively.Visual Question Answering v2<a href="VQA v2">27</a> is a large-scale benchmark containing over 1 million images from the COCO dataset and more than 250,000 human-generated question-answer pairs.The dataset is designed to test the ability of machine learning models to understand both the visual content of an image and the meaning behind natural language questions.The questions in VQA v2 cover a wide range of topics and are often open-ended, requiring models to reason and generalize about the world.VQA v2 has been widely used to evaluate the performance of state-of-the-art models in the field of computer vision and natural language processing.Augmented Outside Knowledge VQA<a href="A-OKVQA">89</a> contains about 25k questions paired with both multiple choice (MC) answer options.Unlike most existing VQA datasets, the questions in A-OKVQA cannot often be answered by querying the knowledge base, but rather involve some type of commonsense reasoning and outside knowledge about the situation portrayed in the image.Outside Knowledge VQA<a href="OK-VQA">63</a> includes more than 14,000 questions that require external knowledge to answer.The answers are provided in free-text direct answer form.Both A-OKVQA and OK-VQA sample images from the COCO dataset, with no overlapping.e-SNLI-VE[21]dataset is an extended version of SNLI-VE dataset[113], which contains about 190k question pairs and human-annotated natural language explanations for the ground-truth labels.The text premise provides a statement about the contents of the image.The task is to determine whether the statement is true or false based on the image content.Visual Spatial Reasoning<a href="VSR">56</a> consists of 65 spatial relations (e.g., under, in front of, facing, etc.) of instances in images.VSR has more than 10k question pairs, associated with 6940 images from MS COCO[55].GQA[35]dataset consists of 22M questions about various day-to-day images.The questions are about compositional question answering based on scene graphs.In our evaluation, we only use the text of the question as model input, but not the scene graphs.Compositional Language and Elementary Visual Reasoning<a href="CLEVR">40</a> is a synthetic dataset with questions that test various aspects of visual reasoning including attribute identification, counting, comparison, spatial relationships, and logical operations.The dataset contains 700k questions in the training set and 150k in the validation set.A.2 Instruction Tuning DetailsWe adopt pretrained BLIP[52]1 and OFA[104]2 as VLMs unless specified otherwise, and freeze their parameters without updating.The instruction tuning only happens on the language model part.The training set of each dataset is used for finetuning.We use the whole training set unless otherwise specified in the low-data instruction tuning discussion.We use an AdaFactor optimizer[92]at the learning rate of 1e-4 for all Cola-FT experiments.The batch size is by default set to 16, though we find Cola-FT insensitive to batch size.We finetune and evaluate the models on NVIDIA V100 or A100 GPUs.The finetuning time is shown in Table6.Cola-Zero answerCola-FT answer yes no no yes   works have finetuned task-specific models that demonstrate amazing capabilities in many real-world applications, such as Copilot for program synthesis[11].
Vqa: Visual question answering. Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C Lawrence Zitnick, Devi Parikh, Dhruv Batra, International Journal of Computer Vision. 123252015</p>
<p>Ecco: An open source library for the explainability of transformer language models. Alammar, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System DemonstrationsAssociation for Computational LinguisticsAugust 2021</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in Neural Information Processing Systems. 202235</p>
<p>Neuro-symbolic visual reasoning: Disentangling. Saeed Amizadeh, Hamid Palangi, Alex Polozov, Yichen Huang, Kazuhito Koishida, International Conference on Machine Learning. PMLR202025</p>
<p>Intelligent tutoring systems. Franklin John R Anderson, Brian J Boyle, Reiser, Science. 22846981985</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.07258202122arXiv preprint</p>
<p>Causalqa: A benchmark for causal question answering. Alexander Bondarenko, Magdalena Wolska, Stefan Heindorf, Lukas Blübaum, Axel-Cyrille Ngonga Ngomo, Benno Stein, Pavel Braslavski, Matthias Hagen, Martin Potthast, ACL. 2022925</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 2020. 2, 4, 223325</p>
<p>Recurrent memory transformer. Aydar Bulatov, Yury Kuratov, Mikhail Burtsev, Advances in Neural Information Processing Systems. 202235</p>
<p>Stackgenvis: Alignment of data, algorithms, and models for stacking ensemble learning using performance metrics. Angelos Chatzimparmpas, Rafael M Martins, Kostiantyn Kucher, Andreas Kerren, IEEE Transactions on Visualization and Computer Graphics. 2722020</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 202123arXiv preprint</p>
<p>Extending context window of large language models via positional interpolation. Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian, arXiv:2306.155952023arXiv preprint</p>
<p>Lako: Knowledge-driven visual question answering via late knowledge-to-text injection. Zhuo Chen, Yufen Huang, Jiaoyan Chen, Yuxia Geng, Yin Fang, Jeff Z Pan, Ningyu Zhang, Wen Zhang, ArXiv, abs/2207.1288820221025</p>
<p>Coarse-to-fine question answering for long documents. Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, Jonathan Berant, ACL. 2017925</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 20221025arXiv preprint</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.114162022. 3, 8, 91025arXiv preprint</p>
<p>Tydi qa: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics. Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, Jennimaria Palomaki, 202025</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang , Albert Li, Pascale Fung, Steven C H Hoi, ArXiv, abs/2305.065002023. 2, 5, 6</p>
<p>Extraction of salient sentences from labelled documents. Misha Denil, Alban Demiraj, Nando De Freitas, arXiv:1412.6815201489arXiv preprint</p>
<p>Ensemble methods in machine learning. Thomas G Dietterich, International workshop on multiple classifier systems. Springer20001025</p>
<p>Virginie Do, Oana-Maria Camburu, arXiv:2004.03744Zeynep Akata, and Thomas Lukasiewicz. e-snli-ve: Corrected visual-textual entailment with natural language explanations. 2020219arXiv preprint</p>
<p>A weighted majority voting ensemble approach for classification. Alican Dogan, Derya Birant, 2019 4th International Conference on Computer Science and Engineering (UBMK). 2019210</p>
<p>Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, Michael Auli, arXiv:1907.09190Eli5: Long form question answering. 201925arXiv preprint</p>
<p>Making pre-trained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, arXiv:2012.15723202024arXiv preprint</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 2021Transactions of the Association for Computational Linguistics25</p>
<p>Learning from natural instructions. Dan Goldwasser, Dan Roth, Machine learning. 942242014</p>
<p>Making the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017219</p>
<p>Kat: A knowledge augmented transformer for vision-and-language. Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, Jianfeng Gao, arXiv:2112.0861420211025arXiv preprint</p>
<p>Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Boyang Li, Dacheng Tao, Steven Ch Hoi, arXiv:2212.10846From images to textual prompts: Zero-shot vqa with frozen large language models. 2022arXiv preprint</p>
<p>Visual programming: Compositional visual reasoning without training. Tanmay Gupta, Aniruddha Kembhavi, ArXiv, abs/2211.115592022</p>
<p>Language models are general-purpose interfaces. Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei, arXiv:2206.063362022arXiv preprint</p>
<p>Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, Jiebo Luo, arXiv:2211.09699Promptcap: Prompt-guided task-aware image captioning. 2022520arXiv preprint</p>
<p>Language is not all you need: Aligning perception with language models. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, arXiv:2302.14045202325arXiv preprint</p>
<p>A diagnostic study of visual question answering with analogical reasoning. Ziqi Huang, Hongyuan Zhu, Ying Sun, Dongkyu Choi, Cheston Tan, Joo-Hwee Lim, 2021 IEEE International Conference on Image Processing (ICIP). IEEE202125</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019. 2, 91925</p>
<p>Patching open-vocabulary models by interpolating weights. Gabriel Ilharco, Mitchell Wortsman, Yitzhak Samir, Shuran Gadre, Hannaneh Song, Simon Hajishirzi, Ali Kornblith, Ludwig Farhadi, Schmidt, arXiv:2208.055922022210arXiv preprint</p>
<p>Evaluating deep neural network ensembles by majority voting cum meta-learning scheme. Anmol Jain, Aishwary Kumar, Seba Susan, Soft Computing and Signal Processing: Proceedings of 3rd ICSCSP 2020. Springer20222</p>
<p>Abstract visual reasoning with tangram shapes. Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert D Hawkins, Yoav Artzi, EMNLP. 2022</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017. 2, 91925</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, ACL. 201725</p>
<p>e-vil: A dataset and benchmark for natural language explanations in vision-language tasks. Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do, Zeynep Akata, Thomas Lukasiewicz, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Unifiedqa-v2: Stronger generalization via broader cross-format training. Daniel Khashabi, Yeganeh Kordi, Hannaneh Hajishirzi, arXiv:2202.123592022arXiv preprint</p>
<p>Vilt: Vision-and-language transformer without convolution or region supervision. Wonjae Kim, Bokyung Son, Ildoo Kim, International Conference on Machine Learning. PMLR2021</p>
<p>Base layers: Simplifying training of large, sparse models. Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, Luke Zettlemoyer, International Conference on Machine Learning. PMLR202125</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.13461201924BartarXiv preprint</p>
<p>Sparse mixture-of-experts are domain generalizable learners. Bo Li, Yifei Shen, Jingkang Yang, Yezhen Wang, Jiawei Ren, Tong Che, Jun Zhang, Ziwei Liu, arXiv:2206.04046202225arXiv preprint</p>
<p>Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, Ziwei Liu, arXiv:2306.05425Mimic-it: Multi-modal in-context instruction tuning. 2023arXiv preprint</p>
<p>Otter: A multi-modal model with in-context instruction tuning. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, Ziwei Liu, 2023</p>
<p>Multimodal foundation models: From specialists to general-purpose assistants. Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, arXiv:2309.100202023arXiv preprint</p>
<p>Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.12597202325arXiv preprint</p>
<p>Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, arXiv:2201.120862022. 2, 3, 5, 10, 1925arXiv preprint</p>
<p>Liunian Harold, Li , Mark Yatskar, Cho-Jui Da Yin, Kai-Wei Hsieh, Chang, arXiv:1908.03557Visualbert: A simple and performant baseline for vision and language. 2019arXiv preprint</p>
<p>Composing ensembles of pre-trained models via iterative consensus. Shuang Li, Yilun Du, Joshua B Tenenbaum, Antonio Torralba, Igor Mordatch, ArXiv, abs/2210.1152220221025</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, Lawrence Zitnick, European conference on computer vision. Springer201419</p>
<p>Fangyu Liu, Guy Emerson, Nigel Collier, arXiv:2205.00363Visual spatial reasoning. 2022219arXiv preprint</p>
<p>Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, Colin Raffel, arXiv:2205.0563820222125arXiv preprint</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, arXiv:2304.09842Chameleon: Plug-and-play compositional reasoning with large language models. 2023arXiv preprint</p>
<p>Mikołaj Małkiński, Jacek Mańdziuk, arXiv:2201.12382Deep learning methods for abstract visual reasoning: A survey on raven's progressive matrices. 202225arXiv preprint</p>
<p>Mikołaj Małkiński, Jacek Mańdziuk, arXiv:2202.10284A review of emerging research directions in abstract visual reasoning. 2022225arXiv preprint</p>
<p>The neurosymbolic concept learner: Interpreting scenes words and sentences from natural supervision. Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, Jiajun Wu, ArXiv, abs/1904.1258420191025</p>
<p>Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based vqa. Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, Marcus Rohrbach, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20211025</p>
<p>Ok-vqa: A visual question answering benchmark requiring external knowledge. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. the IEEE/cvf conference on computer vision and pattern recognition2019219</p>
<p>Programs with common sense. RLE and MIT computation center. John Mccarthy, 196024Cambridge, MA, USA</p>
<p>Human-level play in the game of diplomacy by combining language models with strategic reasoning. Ai Research Fundamental, Team Diplomacy, Anton Meta, Noam Bakhtin, Emily Brown, Gabriele Dinan, Colin Farina, Daniel Flaherty, Andrew Fried, Jonathan Goff, Hengyuan Gray, Hu, Science. 102262022</p>
<p>Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi, Metaicl, arXiv:2110.15943Learning to learn in context. 202124arXiv preprint</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, arXiv:2104.08773202124arXiv preprint</p>
<p>Learning to compress prompts with gist tokens. Jesse Mu, Xiang , Lisa Li, Noah Goodman, arXiv:2304.084672023arXiv preprint</p>
<p>Intelligent tutoring systems: an overview. S Hyacinth, Nwana, Artificial Intelligence Review. 441990</p>
<p>. OpenAI. Gpt-4 technical report. 22023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, arXiv:2203.0215520222224arXiv preprint</p>
<p>Visualcomet: Reasoning about the dynamic context of a still image. Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, Yejin Choi, European Conference on Computer Vision. 2020225</p>
<p>Boosted prompt ensembles for large language models. Silviu Pitis, Andrew Michael R Zhang, Jimmy Wang, Ba, arXiv:2304.059702023arXiv preprint</p>
<p>Harnessing the power of multi-task pretraining for ground-truth level natural language explanations. Björn Plüster, Jakob Ambsdorf, Lukas Braach, Jae Hee Lee, Stefan Wermter, arXiv:2212.042312022520arXiv preprint</p>
<p>Open-retrieval conversational question answering. Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, Bruce Croft, Mohit Iyyer, ACM SIGIR. 202025</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, PMLR, 2021. 5International Conference on Machine Learning. 1025</p>
<p>Scaling language models: Methods, analysis &amp; insights from training gopher. Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.11446202125arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 21140252020</p>
<p>Explain yourself! leveraging language models for commonsense reasoning. Nazneen Rajani, Bryan Mccann, Caiming Xiong, Richard Socher, ACL. 201925</p>
<p>Coqa: A conversational question answering challenge. Siva Reddy, Danqi Chen, Christopher D Manning, 2019Transactions of the Association for Computational Linguistics25</p>
<p>Sentence-bert: Sentence embeddings using siamese bertnetworks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics</p>
<p>Scaling vision with sparse mixture of experts. Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Susano André, Daniel Pinto, Neil Keysers, Houlsby, Advances in Neural Information Processing Systems. 34252021</p>
<p>Ensemble learning: A survey. Omer Sagi, Lior Rokach, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery. 84252018</p>
<p>Reasoning about actions over visual and linguistic modalities: A survey. Keyur Shailaja, Maitreya Sampat, Subhasish Patel, Yezhou Das, Chitta Yang, Baral, arXiv:2207.075682022925arXiv preprint</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, arXiv:2110.08207202124arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023210arXiv preprint</p>
<p>Exploiting cloze questions for few shot text classification and natural language inference. Timo Schick, Hinrich Schütze, arXiv:2001.07676202024arXiv preprint</p>
<p>Industrial virtual assistants: Challenges and opportunities. Benedikt Schmidt, Reuben Borrison, Andrew Cohen, Marcel Dix, Marco Gärtler, Martin Hollender, Benjamin Klöpper, Sylvia Maczey, Shunmuga Siddharthan, Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers. the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers2018</p>
<p>A-okvqa: A benchmark for visual question answering using world knowledge. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, Roozbeh Mottaghi, arXiv:2206.017182022219arXiv preprint</p>
<p>Prompting large language models with answer heuristics for knowledge-based visual question answering. Zhenwei Shao, Zhou Yu, Meng Wang, Jun Yu, arXiv:2303.019032023arXiv preprint</p>
<p>Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean, arXiv:1701.06538Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. 201725arXiv preprint</p>
<p>Adafactor: Adaptive learning rates with sublinear memory cost. Noam Shazeer, Mitchell Stern, International Conference on Machine Learning. PMLR201819</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.175802023210arXiv preprint</p>
<p>Intelligent tutoring systems. Derek Sleeman, John Seely Brown, 1982Academic PressLondon</p>
<p>Learning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. 20203322</p>
<p>On the importance of diversity in question generation for qa. Shubham Md Arafat Sultan, Ramón Chandel, Vittorio Fernandez Astudillo, Castelli, ACL. 202025</p>
<p>Vipergpt: Visual inference via python execution for reasoning. Dídac Surís, Sachit Menon, Carl Vondrick, arXiv:2303.081282023arXiv preprint</p>
<p>Hao Tan, Mohit Bansal, arXiv:1908.07490Lxmert: Learning cross-modality encoder representations from transformers. 2019arXiv preprint</p>
<p>Plug-and-play vqa: Zero-shot vqa by conjoining large pretrained models with zero training. Anthony Meng, Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, Steven Ch Hoi, arXiv:2210.087732022arXiv preprint</p>
<p>Newsqa: A machine comprehension dataset. Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, Kaheer Suleman, Rep4NLP@ACL. 2016925</p>
<p>Learning situation hyper-graphs for video question answering. Aisha Urooj, Hilde Kuehne, Bo Wu, Kim Chheu, Walid Bousselham, Chuang Gan, Niels Lobo, Mubarak Shah, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Gamr: A guided attention model for (visual) reasoning. Mohit Vaishnav, Thomas Serre, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Show and tell: Lessons learned from the 2015 mscoco image captioning challenge. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, IEEE transactions on pattern analysis and machine intelligence. 201639</p>
<p>Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, Hongxia Yang, International Conference on Machine Learning. 2022. 2, 3, 5, 10, 1925</p>
<p>Vlmo: Unified vision-language pretraining with mixture-of-modality-experts. Wenhui Wang, Hangbo Bao, Li Dong, Furu Wei, ArXiv, abs/2111.0235820211025</p>
<p>Rationaleaugmented ensembles in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, arXiv:2207.007472022arXiv preprint</p>
<p>Selfconsistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021824arXiv preprint</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032022arXiv preprint</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsAssociation for Computational LinguisticsOctober 2020</p>
<p>Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. Mitchell Wortsman, Gabriel Ilharco, Ya Samir, Rebecca Gadre, Raphael Roelofs, Ari S Gontijo-Lopes, Hongseok Morcos, Ali Namkoong, Yair Farhadi, Simon Carmon, Ludwig Kornblith, Schmidt, Proceedings of the 39th International Conference on Machine Learning. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, Sivan Sabato, the 39th International Conference on Machine LearningPMLRJul 202216210</p>
<p>Star: A benchmark for situated reasoning in real-world videos. Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, Chuang Gan, Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS). </p>
<p>Visual entailment: A novel task for fine-grained image understanding. Ning Xie, Farley Lai, Derek Doran, Asim Kadav, ArXiv, abs/1901.0670620191025</p>
<p>An explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, arXiv:2111.020802021arXiv preprint</p>
<p>An empirical study of gpt-3 for few-shot knowledge-based vqa. Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, Lijuan Wang, AAAI. 2022225</p>
<p>Clevrer: Collision events for video representation and reasoning. Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, Joshua B Tenenbaum, ArXiv, abs/1910.014422019225</p>
<p>Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Joshua B Tenenbaum, ArXiv, abs/1810.0233820181025</p>
<p>Deep modular co-attention networks for visual question answering. Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, Qi Tian, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Conversational question answering: A survey. Munazza Zaib, Wei Emma Zhang, Adnan Quan Z Sheng, Yang Mahmood, Zhang, Knowledge and Information Systems. 9252022</p>
<p>Ke Qin, Zaharaddeen Karami Lawal, and Yuezhou Dong. Vqa and visual reasoning: An overview of recent datasets, methods and challenges. Rufai Yusuf Zakari, Jim Wilson Owusu, Hailin Wang, arXiv:2212.132962022925arXiv preprint</p>
<p>From recognition to cognition: Visual commonsense reasoning. Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019. 20181025</p>
<p>Merlot: Multimodal neural script knowledge models. Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, Yejin Choi, Neural Information Processing Systems. 20211025</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. Andy Zeng, Adrian S Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, Peter R Florence, ArXiv, abs/2204.0059820221025</p>
<p>Opt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.01068202245arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.056852023arXiv preprint</p>
<p>Mixture-of-experts with expert choice routing. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, Advances in Neural Information Processing Systems. 20223525</p>
<p>Retrieving and reading: A comprehensive survey on open-domain question answering. Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, Tat-Seng Chua, arXiv:2101.00774202125arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
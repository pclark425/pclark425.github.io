<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7603 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7603</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7603</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-460609e217fd59eaa34f5e11a820661f8ec8d7b6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/460609e217fd59eaa34f5e11a820661f8ec8d7b6" target="_blank">INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work fine-tunes a LLAMA model to create an evaluative metric that can produce a diagnostic report aligned with human judgment, and achieves performance levels on par with state-of-the-art metrics like COMET22, which was fine-tuned on human ratings.</p>
                <p><strong>Paper Abstract:</strong> The field of automatic evaluation of text generation made tremendous progress in the last few years. In particular, since the advent of neural metrics, like COMET, BLEURT and SEScore2, the newest generation of metrics show a high correlation with human judgment. Unfortunately, quality scores generated with neural metrics are not interpretable and it is unclear which part of the generation output is criticized by the metrics. To address this limitation, we present I NSTRUCT S CORE , an open-source, explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT4, we fine-tune a LLAMA model to create an evaluative metric that can produce a diagnostic report aligned with human judgment. We evaluate I NSTRUCT S CORE on the WMT22 Zh-En translation task, where our 7B model surpasses other LLM-based baselines, including those based on 175B GPT3. Impressively, our I NSTRUCT S CORE , even without direct super-vision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which was fine-tuned on human ratings. 1</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7603.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7603.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 structured prompting (synthetic error synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used with structured prompts to synthesize errorful candidate texts and explanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses GPT-4 with a rigid, templated prompt to generate synthetic (pseudo) reference–candidate pairs annotated with error type, error location, severity label, and a natural-language explanation; these serve as training data for fine-tuning a 7B LLaMA-based evaluation model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following large language model used as a knowledge/expertise oracle to synthesize labeled training examples (errors + explanations) according to a provided template.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Synthetic error synthesis for training an explainable evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate pseudo-reference and errorful candidate sentences with structured annotations (error type, location, severity, explanation) to train an explainable metric.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Highly-structured template prompt (explicit fields for 'Paraphrase correct translation', 'Incorrect Translation', then repeated blocks: 'Error type', 'Major/minor', 'Error location', 'Explanation').</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompt explicitly specifies number of errors, error types, and severity labels. The prompt requests a paraphrased pseudo-reference to avoid simple lexical overlap. Examples provided in Table 1 of the paper; outputs must contain explicit spans and aligned 'incorrect -> correct' phrase pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Downstream metric correlations and human-alignment (segment-level Kendall tau, Pearson, human alignment score)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>InSTRUCTSCORE (7B LLaMA fine-tuned on GPT-4-synthesized data) achieved, e.g., WMT22 Zh→En: Kendall 40.3, Pearson 51.9 (segment-level) and outperformed other unsupervised metrics in 8/9 directions (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Generated 10k raw sentences via GPT-4 across 100 domains; for each sentence, GPT-4 was prompted to inject 1–5 errors from MQM-defined types and generate locations/explanations; used as LM fine-tuning data for LLaMA-7B.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>The paper reports InSTRUCTSCORE significantly outperforms all other unsupervised metrics in 8/9 directions using William's pairwise significance test (p < 0.05).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7603.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7603.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation input formats (4 scenarios)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Four evaluation input/prompt scenarios (reference-only; reference + additional data; multimodal source + reference; reference + world knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors explicitly separate evaluation into four input/prompt presentation styles and train a separate checkpoint for each format, arguing the input presentation changes the model's evaluation behavior and coverage across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InSTRUCTSCORE (fine-tuned LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A fine-tuned LLaMA-7B model that takes (reference, hypothesis) and outputs structured diagnostic reports; separate checkpoints trained for each input-format scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-generation evaluation across diverse NLG tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Judge candidate text quality relative to references under different input/prompt information settings (just reference vs. reference + extra contextual inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Four distinct input formats: (1) reference only; (2) reference + additional data; (3) reference with different-source modalities; (4) reference + world knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>For each scenario the authors generate 10k (candidate,reference) pairs and produce structured diagnostic reports; they train a separate checkpoint per scenario (4 checkpoints total); evaluation decoding uses greedy decoding (temperature=0).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Segment-level Kendall tau and Pearson correlations; ranking accuracy for some tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Each scenario: 10k synthetic training pairs, trained 3 epochs with LR 2e-5, batch size 128; separate checkpoint per scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7603.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7603.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretrained initialization effect (LLaMA vs LLaMA2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of differing pretrained LLaMA initializations on downstream metric performance (LLaMA-7B vs LLaMA2-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper compares two initializations (LLaMA-7B and LLaMA2-7B) with identical training procedure and shows sizable, task-dependent performance differences: one initialization is better on some tasks and the other on others.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InSTRUCTSCORE (LLaMA) / InSTRUCTSCORE (LLaMA2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same fine-tuning pipeline and synthetic training data applied to two different 7B pretrained checkpoints (LLaMA and LLaMA2); outputs are structured diagnostic reports and scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple NLG evaluation benchmarks (WMT22, WebNLG20, Flicker3k, Commongen, BAGEL)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess the metric's correlation with human judgments across translation, data-to-text, captioning, commonsense generation, and keyword-to-dialogue.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Identical (reference+hypothesis) structured input; only difference is base pretrained model.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>model initialization</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Both models are fine-tuned with the same synthetic dataset and settings; comparison isolates the effect of pretraining/initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Segment-level Kendall / Pearson (τ / ρ) and accuracy for Commongen</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>InSTRUCTSCORE (LLaMA): WMT22 Zh→En τ/ρ = 40.3 / 51.9; WebNLG20 39.5 / 59.0; Flicker3k 30.1 / 34.6; Commongen Acc 58.2; BAGEL 25.6 / 34.2. InSTRUCTSCORE (LLaMA2): WMT22 39.6 / 48.9; WebNLG20 39.0 / 54.1; Flicker3k 21.1 / 21.8; Commongen Acc 69.9; BAGEL 31.0 / 46.2 (Table 12).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Task-dependent: LLaMA initialization outperforms LLaMA2 on translation, data-to-text, and captioning (e.g., Zh→En τ +0.7, ρ +2.9 in some reports), while LLaMA2 strongly outperforms on commonsense generation and unseen BAGEL (Commongen Acc +11.7 absolute; BAGEL τ/ρ +5.4 / +12.0).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same fine-tuning / synthetic training process applied to two different 7B base checkpoints; results reported per benchmark in Table 12.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7603.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7603.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sampling / decoding format in refinement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-p sampling for candidate diagnostic outputs and greedy decoding for evaluation (format of generation during refinement affects final alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>During the automatic critique and self-training refinement, the authors sample multiple diagnostic reports per input using top-p sampling (temperature=0.8, p=0.9), rank with GPT-4 feedback, and fine-tune on the best-aligned outputs; evaluation uses greedy decoding (temperature=0). This change in generation/prompt procedure yields substantial human-alignment and precision/recall gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InSTRUCTSCORE (LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned LLaMA metric; refinement pipeline uses sampled diagnostic outputs (via top-p sampling) and GPT-4 meta-feedback to select aligned outputs for further fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Refinement on WMT-style translation diagnostic reporting (Zh→En)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate multiple candidate diagnostic reports per (hypothesis,reference) pair, use GPT-4 to identify failure modes and alignment scores, select best outputs for further fine-tuning to improve output explanations and human alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Sampling-based generation for refinement: top-p sampling to obtain n candidate structured diagnostic outputs for each input; then selection based on external LLM feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>generation/decoding strategy</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Refinement sampling: temperature=0.8, p=0.9, n=8 candidates per input; selection uses GPT-4 prompts that map outputs to failure modes (M1–M6, G1–G4); final evaluation decoding with temperature=0 (greedy).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Human alignment score (fraction of correct fields), Kendall τ, Pearson ρ, precision and recall on annotated fields</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Before refinement: Kendall 0.404, Pearson 0.515, Human score 0.773, Precision 0.778, Recall 0.824. After refinement: Kendall 0.403, Pearson 0.519, Human score 0.879, Precision 0.894, Recall 0.856 (Tables 6 & 7).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Before refinement numbers above (human 0.773, precision 0.778, recall 0.824).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Human alignment +0.106 absolute; Precision +0.116 absolute (11.6%); Recall +0.032 absolute (3.2%); Pearson +0.004 absolute; Kendall essentially unchanged.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Refinement used 2,000 Chinese→English translations, sampled 8 candidate outputs per pair (top-p sampling with temp=0.8, p=0.9), obtained GPT-4 feedback on 16k candidates, selected high-alignment outputs (4,777 examples) to fine-tune further.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7603.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7603.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chat-domain evaluator sensitivity (GPT-3.5/GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 and GPT-4 evaluation baselines exhibit strong performance on 'Chat' domain data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors report that GPT-3.5 and GPT-4 baselines outperform InSTRUCTSCORE and other unsupervised metrics specifically in the Chat domain, suggesting domain/format of inputs (chatty conversational style) interacts with evaluator prompts and model strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-following chat models used as baseline evaluators in the metrics comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WMT22 metric evaluation across domains (including Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate correlation between automatic metric scores and human MQM judgements in different domains (News, Conversation/Chat, Social, E-Commerce).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Evaluation-of-output using LLM-as-judge prompts (system-level prompts for producing a scalar or rationale); domain-specific candidate outputs (Chat domain has more conversational style inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input domain / prompt-target combination</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Paper notes GPT-3.5/GPT-4 baselines perform well on Chat domain compared to InSTRUCTSCORE; no per-domain numeric breakdown provided in the main text besides a plotted figure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Segment-level Kendall correlation (τ) per domain</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative: GPT-3.5/GPT-4 baselines exceed InSTRUCTSCORE in Chat domain (graphically noted in Figure 3), while InSTRUCTSCORE surpasses unsupervised metrics in other domains.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Domain-stratified evaluation on WMT22 Zh→En. InSTRUCTSCORE trained principally on English-dominant synthetic data and shows domain variation.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7603.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7603.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GEMBA-GPT4 tie-producing effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GEMBA-GPT4 evaluator produces many ties which affects Kendall vs Pearson comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes that GEMBA-GPT4 produces a large set of ties in its outputs; this causes Kendall tau to understate differences while Pearson reflects larger differences, and reports numeric gaps between GEMBA-GPT4 and InSTRUCTSCORE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GEMBA-GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A GPT-4 based metric/approach used as an LLM-based evaluation baseline; its tendency to output tied scores influences metric-to-human correlation statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WMT22 metric evaluation (Zh→En)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare automatic metric outputs to MQM human judgements using Kendall and Pearson correlations; analyze differences due to output tie density.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>LLM-as-metric baseline producing scalar scores; output distribution includes many exact ties.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>scoring output distribution / evaluator prompt</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Paper reports GEMBA-GPT4 Kendall 38.2 / Pearson 37.4 (Table 5) versus InSTRUCTSCORE 40.3 / 51.9. The authors attribute the much larger Pearson gap to GEMBA-GPT4 producing many ties, which inflate Kendall's apparent similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Segment-level Kendall (τ) and Pearson (ρ)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GEMBA-GPT4 τ/ρ = 38.2 / 37.4 (Table 5); InSTRUCTSCORE τ/ρ = 40.3 / 51.9; the authors also state InSTRUCTSCORE outperforms GEMBA-GPT4 by 0.021 Kendall and 0.145 Pearson in another wording (reported in text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Reported differences: +0.021 Kendall and +0.145 Pearson (as reported in text), corresponding to the tabulated point differences above.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Comparison on WMT22 datasets; authors caution Kendall tau-b favors ties and can mask differences if one metric produces many ties.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 2)</em></li>
                <li>G-eval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Gptscore: Evaluate as you desire <em>(Rating: 2)</em></li>
                <li>BLEURT: Learning robust metrics for text generation <em>(Rating: 2)</em></li>
                <li>COMET-22: Unbabel-IST 2022 submission for the metrics shared task <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7603",
    "paper_id": "paper-460609e217fd59eaa34f5e11a820661f8ec8d7b6",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "GPT-4 structured prompting (synthetic error synthesis)",
            "name_full": "GPT-4 used with structured prompts to synthesize errorful candidate texts and explanations",
            "brief_description": "The paper uses GPT-4 with a rigid, templated prompt to generate synthetic (pseudo) reference–candidate pairs annotated with error type, error location, severity label, and a natural-language explanation; these serve as training data for fine-tuning a 7B LLaMA-based evaluation model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Instruction-following large language model used as a knowledge/expertise oracle to synthesize labeled training examples (errors + explanations) according to a provided template.",
            "model_size": null,
            "task_name": "Synthetic error synthesis for training an explainable evaluation metric",
            "task_description": "Generate pseudo-reference and errorful candidate sentences with structured annotations (error type, location, severity, explanation) to train an explainable metric.",
            "problem_format": "Highly-structured template prompt (explicit fields for 'Paraphrase correct translation', 'Incorrect Translation', then repeated blocks: 'Error type', 'Major/minor', 'Error location', 'Explanation').",
            "format_category": "prompt style",
            "format_details": "Prompt explicitly specifies number of errors, error types, and severity labels. The prompt requests a paraphrased pseudo-reference to avoid simple lexical overlap. Examples provided in Table 1 of the paper; outputs must contain explicit spans and aligned 'incorrect -&gt; correct' phrase pairs.",
            "performance_metric": "Downstream metric correlations and human-alignment (segment-level Kendall tau, Pearson, human alignment score)",
            "performance_value": "InSTRUCTSCORE (7B LLaMA fine-tuned on GPT-4-synthesized data) achieved, e.g., WMT22 Zh→En: Kendall 40.3, Pearson 51.9 (segment-level) and outperformed other unsupervised metrics in 8/9 directions (Table 5).",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Generated 10k raw sentences via GPT-4 across 100 domains; for each sentence, GPT-4 was prompted to inject 1–5 errors from MQM-defined types and generate locations/explanations; used as LM fine-tuning data for LLaMA-7B.",
            "statistical_significance": "The paper reports InSTRUCTSCORE significantly outperforms all other unsupervised metrics in 8/9 directions using William's pairwise significance test (p &lt; 0.05).",
            "uuid": "e7603.0"
        },
        {
            "name_short": "Evaluation input formats (4 scenarios)",
            "name_full": "Four evaluation input/prompt scenarios (reference-only; reference + additional data; multimodal source + reference; reference + world knowledge)",
            "brief_description": "The authors explicitly separate evaluation into four input/prompt presentation styles and train a separate checkpoint for each format, arguing the input presentation changes the model's evaluation behavior and coverage across tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InSTRUCTSCORE (fine-tuned LLaMA-7B)",
            "model_description": "A fine-tuned LLaMA-7B model that takes (reference, hypothesis) and outputs structured diagnostic reports; separate checkpoints trained for each input-format scenario.",
            "model_size": "7B",
            "task_name": "Text-generation evaluation across diverse NLG tasks",
            "task_description": "Judge candidate text quality relative to references under different input/prompt information settings (just reference vs. reference + extra contextual inputs).",
            "problem_format": "Four distinct input formats: (1) reference only; (2) reference + additional data; (3) reference with different-source modalities; (4) reference + world knowledge.",
            "format_category": "input modality / prompt style",
            "format_details": "For each scenario the authors generate 10k (candidate,reference) pairs and produce structured diagnostic reports; they train a separate checkpoint per scenario (4 checkpoints total); evaluation decoding uses greedy decoding (temperature=0).",
            "performance_metric": "Segment-level Kendall tau and Pearson correlations; ranking accuracy for some tasks",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Each scenario: 10k synthetic training pairs, trained 3 epochs with LR 2e-5, batch size 128; separate checkpoint per scenario.",
            "statistical_significance": null,
            "uuid": "e7603.1"
        },
        {
            "name_short": "Pretrained initialization effect (LLaMA vs LLaMA2)",
            "name_full": "Effect of differing pretrained LLaMA initializations on downstream metric performance (LLaMA-7B vs LLaMA2-7B)",
            "brief_description": "The paper compares two initializations (LLaMA-7B and LLaMA2-7B) with identical training procedure and shows sizable, task-dependent performance differences: one initialization is better on some tasks and the other on others.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InSTRUCTSCORE (LLaMA) / InSTRUCTSCORE (LLaMA2)",
            "model_description": "Same fine-tuning pipeline and synthetic training data applied to two different 7B pretrained checkpoints (LLaMA and LLaMA2); outputs are structured diagnostic reports and scores.",
            "model_size": "7B",
            "task_name": "Multiple NLG evaluation benchmarks (WMT22, WebNLG20, Flicker3k, Commongen, BAGEL)",
            "task_description": "Assess the metric's correlation with human judgments across translation, data-to-text, captioning, commonsense generation, and keyword-to-dialogue.",
            "problem_format": "Identical (reference+hypothesis) structured input; only difference is base pretrained model.",
            "format_category": "model initialization",
            "format_details": "Both models are fine-tuned with the same synthetic dataset and settings; comparison isolates the effect of pretraining/initialization.",
            "performance_metric": "Segment-level Kendall / Pearson (τ / ρ) and accuracy for Commongen",
            "performance_value": "InSTRUCTSCORE (LLaMA): WMT22 Zh→En τ/ρ = 40.3 / 51.9; WebNLG20 39.5 / 59.0; Flicker3k 30.1 / 34.6; Commongen Acc 58.2; BAGEL 25.6 / 34.2. InSTRUCTSCORE (LLaMA2): WMT22 39.6 / 48.9; WebNLG20 39.0 / 54.1; Flicker3k 21.1 / 21.8; Commongen Acc 69.9; BAGEL 31.0 / 46.2 (Table 12).",
            "baseline_performance": null,
            "performance_change": "Task-dependent: LLaMA initialization outperforms LLaMA2 on translation, data-to-text, and captioning (e.g., Zh→En τ +0.7, ρ +2.9 in some reports), while LLaMA2 strongly outperforms on commonsense generation and unseen BAGEL (Commongen Acc +11.7 absolute; BAGEL τ/ρ +5.4 / +12.0).",
            "experimental_setting": "Same fine-tuning / synthetic training process applied to two different 7B base checkpoints; results reported per benchmark in Table 12.",
            "statistical_significance": null,
            "uuid": "e7603.2"
        },
        {
            "name_short": "Sampling / decoding format in refinement",
            "name_full": "Top-p sampling for candidate diagnostic outputs and greedy decoding for evaluation (format of generation during refinement affects final alignment)",
            "brief_description": "During the automatic critique and self-training refinement, the authors sample multiple diagnostic reports per input using top-p sampling (temperature=0.8, p=0.9), rank with GPT-4 feedback, and fine-tune on the best-aligned outputs; evaluation uses greedy decoding (temperature=0). This change in generation/prompt procedure yields substantial human-alignment and precision/recall gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InSTRUCTSCORE (LLaMA-7B)",
            "model_description": "Fine-tuned LLaMA metric; refinement pipeline uses sampled diagnostic outputs (via top-p sampling) and GPT-4 meta-feedback to select aligned outputs for further fine-tuning.",
            "model_size": "7B",
            "task_name": "Refinement on WMT-style translation diagnostic reporting (Zh→En)",
            "task_description": "Generate multiple candidate diagnostic reports per (hypothesis,reference) pair, use GPT-4 to identify failure modes and alignment scores, select best outputs for further fine-tuning to improve output explanations and human alignment.",
            "problem_format": "Sampling-based generation for refinement: top-p sampling to obtain n candidate structured diagnostic outputs for each input; then selection based on external LLM feedback.",
            "format_category": "generation/decoding strategy",
            "format_details": "Refinement sampling: temperature=0.8, p=0.9, n=8 candidates per input; selection uses GPT-4 prompts that map outputs to failure modes (M1–M6, G1–G4); final evaluation decoding with temperature=0 (greedy).",
            "performance_metric": "Human alignment score (fraction of correct fields), Kendall τ, Pearson ρ, precision and recall on annotated fields",
            "performance_value": "Before refinement: Kendall 0.404, Pearson 0.515, Human score 0.773, Precision 0.778, Recall 0.824. After refinement: Kendall 0.403, Pearson 0.519, Human score 0.879, Precision 0.894, Recall 0.856 (Tables 6 & 7).",
            "baseline_performance": "Before refinement numbers above (human 0.773, precision 0.778, recall 0.824).",
            "performance_change": "Human alignment +0.106 absolute; Precision +0.116 absolute (11.6%); Recall +0.032 absolute (3.2%); Pearson +0.004 absolute; Kendall essentially unchanged.",
            "experimental_setting": "Refinement used 2,000 Chinese→English translations, sampled 8 candidate outputs per pair (top-p sampling with temp=0.8, p=0.9), obtained GPT-4 feedback on 16k candidates, selected high-alignment outputs (4,777 examples) to fine-tune further.",
            "statistical_significance": null,
            "uuid": "e7603.3"
        },
        {
            "name_short": "Chat-domain evaluator sensitivity (GPT-3.5/GPT-4)",
            "name_full": "GPT-3.5 and GPT-4 evaluation baselines exhibit strong performance on 'Chat' domain data",
            "brief_description": "The authors report that GPT-3.5 and GPT-4 baselines outperform InSTRUCTSCORE and other unsupervised metrics specifically in the Chat domain, suggesting domain/format of inputs (chatty conversational style) interacts with evaluator prompts and model strengths.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5 / GPT-4",
            "model_description": "Large instruction-following chat models used as baseline evaluators in the metrics comparisons.",
            "model_size": null,
            "task_name": "WMT22 metric evaluation across domains (including Chat)",
            "task_description": "Evaluate correlation between automatic metric scores and human MQM judgements in different domains (News, Conversation/Chat, Social, E-Commerce).",
            "problem_format": "Evaluation-of-output using LLM-as-judge prompts (system-level prompts for producing a scalar or rationale); domain-specific candidate outputs (Chat domain has more conversational style inputs).",
            "format_category": "input domain / prompt-target combination",
            "format_details": "Paper notes GPT-3.5/GPT-4 baselines perform well on Chat domain compared to InSTRUCTSCORE; no per-domain numeric breakdown provided in the main text besides a plotted figure.",
            "performance_metric": "Segment-level Kendall correlation (τ) per domain",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": "Qualitative: GPT-3.5/GPT-4 baselines exceed InSTRUCTSCORE in Chat domain (graphically noted in Figure 3), while InSTRUCTSCORE surpasses unsupervised metrics in other domains.",
            "experimental_setting": "Domain-stratified evaluation on WMT22 Zh→En. InSTRUCTSCORE trained principally on English-dominant synthetic data and shows domain variation.",
            "statistical_significance": null,
            "uuid": "e7603.4"
        },
        {
            "name_short": "GEMBA-GPT4 tie-producing effects",
            "name_full": "GEMBA-GPT4 evaluator produces many ties which affects Kendall vs Pearson comparisons",
            "brief_description": "The paper notes that GEMBA-GPT4 produces a large set of ties in its outputs; this causes Kendall tau to understate differences while Pearson reflects larger differences, and reports numeric gaps between GEMBA-GPT4 and InSTRUCTSCORE.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GEMBA-GPT4",
            "model_description": "A GPT-4 based metric/approach used as an LLM-based evaluation baseline; its tendency to output tied scores influences metric-to-human correlation statistics.",
            "model_size": null,
            "task_name": "WMT22 metric evaluation (Zh→En)",
            "task_description": "Compare automatic metric outputs to MQM human judgements using Kendall and Pearson correlations; analyze differences due to output tie density.",
            "problem_format": "LLM-as-metric baseline producing scalar scores; output distribution includes many exact ties.",
            "format_category": "scoring output distribution / evaluator prompt",
            "format_details": "Paper reports GEMBA-GPT4 Kendall 38.2 / Pearson 37.4 (Table 5) versus InSTRUCTSCORE 40.3 / 51.9. The authors attribute the much larger Pearson gap to GEMBA-GPT4 producing many ties, which inflate Kendall's apparent similarity.",
            "performance_metric": "Segment-level Kendall (τ) and Pearson (ρ)",
            "performance_value": "GEMBA-GPT4 τ/ρ = 38.2 / 37.4 (Table 5); InSTRUCTSCORE τ/ρ = 40.3 / 51.9; the authors also state InSTRUCTSCORE outperforms GEMBA-GPT4 by 0.021 Kendall and 0.145 Pearson in another wording (reported in text).",
            "baseline_performance": null,
            "performance_change": "Reported differences: +0.021 Kendall and +0.145 Pearson (as reported in text), corresponding to the tabulated point differences above.",
            "experimental_setting": "Comparison on WMT22 datasets; authors caution Kendall tau-b favors ties and can mask differences if one metric produces many ties.",
            "statistical_significance": null,
            "uuid": "e7603.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 2
        },
        {
            "paper_title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 2
        },
        {
            "paper_title": "Gptscore: Evaluate as you desire",
            "rating": 2
        },
        {
            "paper_title": "BLEURT: Learning robust metrics for text generation",
            "rating": 2
        },
        {
            "paper_title": "COMET-22: Unbabel-IST 2022 submission for the metrics shared task",
            "rating": 2
        }
    ],
    "cost": 0.0196795,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>InstructScore: Explainable Text Generation Evaluation with Fine-grained Feedback</h1>
<p>Wenda $\mathbf{X u}^{\ddagger}$, Danqing Wang ${ }^{\S}$, Liangming Pan ${ }^{\S}$, Zhenqiao Song ${ }^{\S}$, Markus Freitag ${ }^{\dagger}$, William Yang Wang ${ }^{\ddagger}$, Lei $\mathbf{L i}^{\ddagger}$<br>${ }^{\S}$ University of California, Santa Barbara, ${ }^{\dagger}$ Google Research, ${ }^{\ddagger}$ Carnegie Mellon University<br>{wendaxu, danqingwang, liangmingpan, zhenqiao, william}@cs.ucsb.edu<br>freitag@google.com leili@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics do not provide explicit explanation of their verdict, nor associate the scores with defects in the generated text. To address this limitation, we present InSTRUCTSCORE, a fine-grained explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate InSTRUCTSCORE on a variety of generation tasks, including translation, captioning, data-to-text, and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our InSTRUCTSCORE, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which were fine-tuned on human ratings.</p>
<h2>1 Introduction</h2>
<p>Although large language models (LLMs) have led to significant progress in various natural language tasks (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023), it remains a challenge to automatically evaluate the quality of text generation across versatile tasks. Traditional word overlap metrics, such as $n$-gram matching, BLEU (Papineni et al., 2002), and chrF (Popović, 2015), along with distance-based metrics like TER (Snover et al., 2006) do not best align with human experts' judgements (Freitag et al., 2021a). They primarily focus on surface form differences between reference and candidate texts (Freitag et al., 2020). On the other hand, recent learned metrics such as BERTScore (Zhang et al., 2019), BLEURT (Sellam et al., 2020a), COMET (Rei et al., 2022) and
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: InSTRUCTSCORE generates a comprehensive error diagnostic report for text generation tasks, including error type, location, severity label, and explanation. Based on this report, InSTRUCTSCORE counts the number of major errors (each worth -5 ) and minor errors (each worth -1 ), ultimately assigning a final score.</p>
<p>SEScore (Xu et al., 2022b,a) show a higher correlation with humans on text generation tasks. However, all these metrics produce a single numerical score. These learned metrics lack interpretation of predictions nor link the scores with individual defects in the candidate text.</p>
<p>How can we devise a fine-grained explanationbased text generation metric capable of pinpointing concrete error locations, identifying error types, assigning severity labels, and justifying the final score-all simultaneously without relying on human-annotated data. In this paper, we propose InSTRUCTSCORE, a method to learn an explainable text generation metric without using human annotated ratings. InstructScore provides both a numerical score and a natural language error explanation. To this end, we first extract latent evaluation knowledge from an instruction-following</p>
<p>model, such as GPT-4 <em>OpenAI (2023)</em>, to construct a synthetic dataset with a predetermined explanation structure. Next, we determine a range of explanation failure modes and devise automated feedback to meta-evaluate error explanations. Finally, we further fine-tune INSTRUCTSCORE model on self-generated outputs that optimize feedback scores, resulting in diagnostic reports that are better aligned with humans.</p>
<p>We have conduct experiments on a variety of text generation tasks: machine translation, table-to-text, image captioning, commonsense generation, and keyword-to-dialogue generation. Our experimental findings show that the unsupervised INSTRUCTSCORE outperforms prior strong baselines on all these tasks. It achieves the best results for the unseen keyword-to-dialogue generation task. Surprisingly, INSTRUCTSCORE surpasses the supervised BLEURT in 6 out of 9 directions and closely matches state-of-the-art COMET22 in machine translation. Furthermore, we identify a range of failure modes and design an automatic pipeline to pinpoint explanation failures. Our refinement step improves human score by 13.7%, leading to a more accurate alignment with human judgment.</p>
<p>Our INSTRUCTSCORE enjoys the following advantages: (i) Compact yet competitive: INSTRUCTSCORE's 7B version displays strong performance compared to metrics based on closed-source 175B LLMs. (ii) Explainable: INSTRUCTSCORE provides natural language explanations to justify numerical scores. (iii) Generalizable: The unsupervised training pipeline does not require human-annotations, making it easily adaptable to different domains and tasks.</p>
<h2>2 Related Work</h2>
<p>Learned Evaluation Metrics Supervised metrics optimize performance by directly fine-tuning human rating data, such as COMET <em>Rei et al. (2020)</em> and BLEURT <em>Sellam et al. (2020a)</em>, as shown by <em>Rei et al. (2020)</em> and <em>Sellam et al. (2020a)</em>. However, human rating data is often unavailable. Unsupervised metrics use different learning objectives or heuristics on embeddings, such as BERT for greedy matching and coverage scoring <em>Zhang et al. (2019)</em>, or sequence-to-sequence models for probability estimation <em>Thompson and Post (2020); Yuan et al. (2021)</em>. SEScore <em>Xu et al. (2022b)</em> and SEScore2 <em>Xu et al. (2022a)</em> train a regression model by synthesizing human-like errors from raw text and using either a pre-trained natural language inference or a multilingual masked prediction model to attach error severity score. Supervised metrics can arguably attain higher correlations with human judgments <em>Freitag et al. (2021b, 2022)</em>, while unsupervised metrics, such as SEScore <em>Xu et al. (2022b)</em> and BERTScore <em>Zhang et al. (2019)</em>, exhibit greater levels of generalization. However, none of these approaches offer an explanation for the resulting scores, rendering the decision-making processes obscure and less trustworthy. In this paper, we generate a diagnostic report to provide detailed explanations to support metric's final decisions.</p>
<p>Explainable Evaluation Metric. Recent demand for explainability in evaluation metrics has grown significantly. <em>Freitag et al. (2021a)</em> introduce a multi-dimensional human evaluation (MQM) framework for machine translation, while <em>Leiter et al. (2022)</em> investigates key characteristics of explainable metrics. Several metrics derived from those frameworks enhance explainability by differentiating error severity <em>Lu et al. (2022); Xu et al. (2022b, a); Perrella et al. (2022)</em>. Other efforts focus on explanatory aspects of text generation metrics, like error locations <em>Zerva et al. (2022)</em> and multi-dimensional assessment <em>Zhong et al. (2022)</em>. Despite progress, explanations remain unclear. Researchers also explore LLMs' potential in evaluation, as demonstrated by <em>Fu et al. (2023)</em>, but suffers from a lack of explanation. <em>Kocmi and Federmann (2023)</em> and <em>Liu et al. (2023)</em> find large models like GPT-3.5 on system-level can correlate to humans and generate rationales. However, these generated rationales are free-form and may not necessarily align with human judgements <em>Zheng et al. (2023)</em>. In our work, we explicitly refine INSTRUCTSCORE to produce explanations that align with human.</p>
<h2>3 Problem Definition</h2>
<p>Our goal is to learn an explainable metric model that not only predicts the quality score of candidate text comparing to a reference but also generates a diagnostic report in natural language. Specifically, INSTRUCTSCORE assesses the quality of $x$ regarding a reference $r$ by generating an informative diagnostic report, which includes the details about error location $l$, error type $t$, severity level $s e$, and explanation $e$ that are associated with the identified error. INSTRUCTSCORE consists of a score predictor and an explanation generator (Exp-Generator) which learns a function $f:\langle\mathbf{x}, \mathbf{y}\rangle \rightarrow{\langle l, t, s e, e\rangle_{l}}_{l=1}^{n}$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Our INSTRUCTSCORE pipeline consists of three components: First, we construct synthetic data from GPT-4 and use it to fine-tune a 7B LLAMA model. Second, we sample from real-world machine-generated distribution to trigger INSTRUCTSCORE's failure modes. We query GPT-4 on each failure mode and gather automatic feedback. Third, we select explanations that are most aligned with human to further fine-tune LLaMA model. Step 2 and 3 can be repeated to iteratively refine the model output.</p>
<p>with <em>n</em> number of errors. However, such human annotated mapping data for most text generation tasks is scarce due to limited human resources and high annotation costs. To this end, we propose a data construction method to automatically generate high-quality pseudo data to learn <em>f</em>.</p>
<h2>4 The INSTRUCTSCORE Approach</h2>
<p>INSTRUCTSCORE assesses the quality of generated texts based on an explainable diagnostic report. Building upon this report, INSTRUCTSCORE provides an intuitive way to comprehend a model's generation capability, resulting in easier comparison among different models. In particular, we begin by extracting concise yet representative explainable knowledge from a large-scale instruction-following model, which is then utilized to train our Exp-Generator. After carefully analyzing the diagnostic reports produced by our Exp-Generator, we summarize common failure modes in diagnostic report and ask GPT-4 to identify them. Then we transform the GPT-4's feedback into alignment scores using our predefined criteria. Finally, we select diagnostic reports that have the highest alignment scores, and further finetune our Exp-Generator on those self-refined outputs. The overall framework is illustrated in Figure 2.</p>
<p>The quality score <em>s</em> for each candidate <em>y</em> is determined based on the number of errors and their severity labels in the diagnostic report. Minor errors are given a score of –1 and major errors are given a score of –5. These penalties for errors are weighted to calculate the final score. Similar to previous practices (Freitag et al., 2021a), our metric identifies up to five errors per sentence.</p>
<h3>4.1 Learning with Guided Error and Explanation Synthesis</h3>
<p>We leverage GPT-4 to extract representative explainable knowledge that can greatly contribute to the subsequent Exp-Generator learning process.</p>
<p>Specifically, we collected raw sentences in the target language from diverse domains and topics via GPT-4 (Details are included in Section 5.1 and Appendix A), resulting in data across diverse tasks. This corpus is used as the starting point to inject errors. Then, we prompt GPT-4 to synthesize designated generation errors, as shown in Table 1.</p>
<p>For each text, we specify the number of errors, error types, and severity labels, and ask GPT-4 to generate a candidate output with the specified error descriptions and 2) an explanation for this error annotation. If an evaluation task is multi-dimensional, error types will be separately assigned to each dimension (An example is included in the Appendix). Benefiting from the large-scale pre-training process, GPT-4 is able to generate diverse errors and meet the requirements with specified instructions. To avoid the model's over-reliance on the lexical and structural similarities between the candidate and raw text, we request GPT-4 to rephrase the raw text sentence to construct a pseudo-reference sentence. By specifying error type <em>t</em>, severity label <em>se</em>, and raw text, GPT-4 is able to generate a synthetic error sentence <em>x</em> with annotated error location <em>l</em> and a pseudo reference <em>y</em> with explanation <em>e</em>. Therefore, we can construct synthetic data that reflects</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt for GPT4:</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">The correct translation is, "The art of writing for edu-</td>
</tr>
<tr>
<td style="text-align: left;">cational publications involves striking a delicate bal-</td>
</tr>
<tr>
<td style="text-align: left;">ance between providing enough detail to be useful</td>
</tr>
<tr>
<td style="text-align: left;">and overwhelming the reader with too much informa-</td>
</tr>
<tr>
<td style="text-align: left;">tion." Please give 0 minor errors and 2 major errors.</td>
</tr>
<tr>
<td style="text-align: left;">For each error, give me an error type, a major/minor</td>
</tr>
<tr>
<td style="text-align: left;">label, an error location, and an explanation. The format</td>
</tr>
<tr>
<td style="text-align: left;">is Paraphrase correct translation:</td>
</tr>
<tr>
<td style="text-align: left;">Incorrect Translation:</td>
</tr>
<tr>
<td style="text-align: left;">Error type 1: Translation includes information not</td>
</tr>
<tr>
<td style="text-align: left;">present in the correct translation</td>
</tr>
<tr>
<td style="text-align: left;">Major/minor: Major</td>
</tr>
<tr>
<td style="text-align: left;">Error location 1:</td>
</tr>
<tr>
<td style="text-align: left;">Explanation for error 1:</td>
</tr>
<tr>
<td style="text-align: left;">Error type 2: Problems with grammar, other than</td>
</tr>
<tr>
<td style="text-align: left;">orthography</td>
</tr>
<tr>
<td style="text-align: left;">Major/minor: Major</td>
</tr>
<tr>
<td style="text-align: left;">Error location 2:</td>
</tr>
<tr>
<td style="text-align: left;">Explanation for error 2:</td>
</tr>
<tr>
<td style="text-align: left;">GPT4 Output:</td>
</tr>
<tr>
<td style="text-align: left;">Error type 1: Translation includes information not</td>
</tr>
<tr>
<td style="text-align: left;">present in the correct translation</td>
</tr>
<tr>
<td style="text-align: left;">Major/minor: Major</td>
</tr>
<tr>
<td style="text-align: left;">Error location 1: "...giving enough examples..."</td>
</tr>
<tr>
<td style="text-align: left;">Explanation for error 1: The correct translation talks</td>
</tr>
<tr>
<td style="text-align: left;">about providing enough detail, not specific examples,</td>
</tr>
<tr>
<td style="text-align: left;">which changes the meaning of the sentence.</td>
</tr>
<tr>
<td style="text-align: left;">Error type 2: Problems with grammar, other than orthog-</td>
</tr>
<tr>
<td style="text-align: left;">raphy</td>
</tr>
<tr>
<td style="text-align: left;">Major/minor: Major</td>
</tr>
<tr>
<td style="text-align: left;">Error location 2: "...making the reader bored with too</td>
</tr>
<tr>
<td style="text-align: left;">many details."</td>
</tr>
<tr>
<td style="text-align: left;">Explanation for error 2: The correct translation mentions</td>
</tr>
<tr>
<td style="text-align: left;">overwhelming the reader with too much information, not</td>
</tr>
<tr>
<td style="text-align: left;">making them bored with too many details. This change</td>
</tr>
<tr>
<td style="text-align: left;">in phrasing alters the meaning of the sentence.</td>
</tr>
</tbody>
</table>
<p>Table 1: This prompt was used to generate synthetic data to fine-tune the LLAMA model, specifically GPT-4. The model was provided with the number of errors in a sentence. For each error annotation, we pre-define the error type and a severity label. GPT-4 was then required to identify the error locations and provide explanations for each error annotation. The texts highlighted are variables for each synthesized generation. The complete example can be found in Appendix Table 29.
the relationship between $(x, y)$ and $(t, l, s e, e)$.
We train our Exp-Generator by utilizing the constructed data. Particularly, we use LLaMA as the initialization of the Exp-Generator since it is opensourced and performs well in both understanding and generation tasks. Then we train our ExpGenerator by taking the pseudo reference $y$ and candidate $x$ as the input, and the diagnostic report including the corresponding error type $t$, error location $l$, severity label $s e$ and the explanation $e$ as the output. A concrete example can be found in Figure 2. Accordingly, our training objective can be defined as follows:</p>
<p>$$
\mathcal{L}(t, l, s e, e, x, y)=-\log P(t, l, s e, e \mid y, x ; \theta)
$$</p>
<p>where $\theta$ is the trainable parameter of the ExpGenerator.</p>
<h3>4.2 Auto-Identifying Failure Modes of Metric Output</h3>
<p>The diagnostic report plays an important role in text quality explanation. However, the above trained model is not guaranteed to produce sensible explanations - those incorrect explanations are referred to as failure modes. We categorize failure modes into global and local levels. A global failure invalidates all four fields: error type, error location, major/minor and explanation. A local failure only affects a specific field, like error type.</p>
<p>In Table 2, we define six scenarios M1-M6 for local failures and four scenarios G1-G4 for global failures. We demonstrate one failure mode M4 in Table 3. Concrete examples for each failure mode are included in the Appendix Table 13, 14, $15,16,17,18,19,20,21,22$. A special case is when the method outputs an annotation containing no error. In this situation, we need to verify if this is accurate (A concrete example can be found in Appendix Table 35). If errors are present, the diagnostic report is incorrect.</p>
<p>Ideally, a human annotator can provide the most accurate judgment for detecting each failure mode. However, obtaining annotations from humans for every instance of a diagnostic report is infeasible. As an alternative, we leverage GPT-4's capabilities in information extraction, parsing, and semantic understanding (OpenAI, 2023) to convert complex requirement queries into simple Yes/No questions.</p>
<p>Specifically, we prompt GPT-4 to parse the explanation into incorrect and correct phrase pairs and extract the error span from the error location. To address hallucinations from error location (M3) and explanation (M4), we verify if our parsed error span is present in the candidate sentence. If one error annotation contains multiple incorrect-correct phrase pairs, it indicates multiple errors in one error location (G4). To address G1, we first check if the incorrect phrase is indeed an error. Additionally, we verify if the revision suggestion is in the output. For the remaining M and G aspects, we design specific prompt queries for the GPT-4. A detailed example of the prompt query for checking M1-M6 and G1-G4 can be found in Table 4.</p>
<p>After obtaining local and global failure modes from GPT-4's feedback, we can convert those feedback into alignment scores. We assign a binary</p>
<table>
<thead>
<tr>
<th>Fields</th>
<th>Explanation Failure Mode</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Local Failure Mode</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Error Type</td>
<td>Inconsistency to explanation M1: Error type descriptions are not consistent with explanation</td>
<td></td>
</tr>
<tr>
<td>Error Location</td>
<td>Inconsistency to explanation M2: Error locations are not consistent with the explanation</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Error location hallucination M3: Error locations are not referred in the output text</td>
<td></td>
</tr>
<tr>
<td>Major/Minor</td>
<td>Major/Minor disagreement M5: Major and minor labels do not correspond to the correct severity levels</td>
<td></td>
</tr>
<tr>
<td>Explanation</td>
<td>Error location hallucination</td>
<td>M4: Error locations can not refer to the output text</td>
</tr>
<tr>
<td></td>
<td>Explanation failure</td>
<td>M6: The explanation is wrong. However, error at a specified location does exist</td>
</tr>
<tr>
<td>Global Failure Mode</td>
<td></td>
<td></td>
</tr>
<tr>
<td>All 4 Fields</td>
<td>False negative error</td>
<td>G1: Error described in the explanation is not an error</td>
</tr>
<tr>
<td></td>
<td>Repetition</td>
<td>G2: One error is mentioned more than once among explanations</td>
</tr>
<tr>
<td></td>
<td>Phrase misalignment</td>
<td>G3: Incorrect phrase and correct phrase are not correctly aligned</td>
</tr>
<tr>
<td></td>
<td>Mention multiple errors</td>
<td>G4: One error span mentions multiple errors</td>
</tr>
</tbody>
</table>
<p>Table 2: This table shows common failure modes of the explanation output of first step Exp-Generator (Fine-tuned LLaMA on synthetic data without refinement). Local errors are field-specific, which only correspond to the error of the local field. Global errors can affect all four fields, such as error type, error location, major/minor, and explanation. The observation of the failures modes at first step Exp-Generator is the main motivation for us to perform refinement with automatic feedback (See Section 4.3)</p>
<div class="codehilite"><pre><span></span><code><span class="nx">InStrUCTSCORE</span><span class="w"> </span><span class="nx">Input</span><span class="p">:</span>
<span class="nx">Reference</span><span class="p">:</span><span class="w"> </span><span class="nx">Activities</span><span class="w"> </span><span class="nx">include</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="s">&quot;Cool Maker Loves</span>
<span class="s">Family&quot;</span><span class="w"> </span><span class="nx">carnival</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">children</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">future</span><span class="w"> </span><span class="nx">learning</span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">first</span>
<span class="nx">children</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">visual</span><span class="w"> </span><span class="nx">art</span><span class="w"> </span><span class="nx">festival</span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="s">&quot;Book Carnival&quot;</span><span class="w"> </span><span class="nx">event</span>
<span class="k">and</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="s">&quot;Whole Family Loves Reading&quot;</span><span class="w"> </span><span class="nx">Picture</span><span class="w"> </span><span class="nx">Book</span>
<span class="nx">Reading</span><span class="w"> </span><span class="nx">activity</span><span class="p">.</span>
<span class="nx">Output</span><span class="p">:</span><span class="w"> </span><span class="nx">Activities</span><span class="w"> </span><span class="nx">include</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="s">&quot;love family and create</span>
<span class="s">cool&quot;</span><span class="w"> </span><span class="nx">children</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">future</span><span class="w"> </span><span class="nx">learning</span><span class="w"> </span><span class="nx">carnival</span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">first</span><span class="w"> </span><span class="nx">chil</span><span class="o">-</span>
<span class="nx">dren</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">visual</span><span class="w"> </span><span class="nx">arts</span><span class="w"> </span><span class="nx">festival</span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="s">&quot;book carnival&quot;</span><span class="w"> </span><span class="nx">activity</span><span class="p">,</span>
<span class="nx">the</span><span class="w"> </span><span class="s">&quot;family love reading&quot;</span><span class="w"> </span><span class="nx">picture</span><span class="w"> </span><span class="nx">book</span><span class="w"> </span><span class="nx">series</span><span class="w"> </span><span class="nx">activity</span><span class="p">.</span>
<span class="nx">InStrUCTSCORE</span><span class="w"> </span><span class="nx">Output</span><span class="p">:</span>
<span class="nx">Your</span><span class="w"> </span><span class="nx">Translation</span><span class="w"> </span><span class="nx">contains</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="nx">errors</span><span class="p">:</span>
<span class="nx">Error</span><span class="w"> </span><span class="k">type</span><span class="w"> </span><span class="mi">5</span><span class="p">:</span><span class="w"> </span><span class="nx">Incorrect</span><span class="w"> </span><span class="nx">translation</span><span class="w"> </span><span class="nx">includes</span><span class="w"> </span><span class="nx">information</span>
<span class="k">not</span><span class="w"> </span><span class="nx">present</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">correct</span><span class="w"> </span><span class="nx">translation</span>
<span class="nx">Major</span><span class="o">/</span><span class="nx">minor</span><span class="p">:</span><span class="w"> </span><span class="nx">Major</span>
<span class="nx">Error</span><span class="w"> </span><span class="nx">location</span><span class="w"> </span><span class="mi">5</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;whole family loves reading&quot;</span>
<span class="nx">Explanation</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">error</span><span class="w"> </span><span class="mi">5</span><span class="p">:</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">incorrect</span><span class="w"> </span><span class="nx">translation</span><span class="w"> </span><span class="nx">adds</span>
<span class="nx">the</span><span class="w"> </span><span class="nx">phrase</span><span class="w"> </span><span class="s">&quot;whole family loves reading,&quot;</span><span class="w"> </span><span class="nx">which</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span>
<span class="nx">present</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">correct</span><span class="w"> </span><span class="nx">translation</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">changes</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">focus</span>
<span class="nx">from</span><span class="w"> </span><span class="s">&quot;families&quot;</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="s">&quot;whole families.&quot;</span>
</code></pre></div>

<p>Table 3: This is the case where error locations in explanation can not refer to the output text (M4). "whole family loves reading" is never mentioned in the output text. Instead, it was mentioned in the reference text. The complete example can be found in Appendix Table 16.</p>
<h2>Prompt: Ref: ...... entered the revolutionary base area of south Jiangxi. Out: ..... entered the old revolutionary district of southern Jiangxi. Error location 1: "old revolutionary district" Error type 1: Terminology is non-standard Explanation 1: The correct term should be "new revolutionary base area" ..... Q1: For each error location, extract the incorrect error location. Q2: Parse the explanation into either one of the four forms: [incorrect phrase, correct phrase]...... Q3: If A2 is "incorrect phrase to correct phrase", is A2 a correct alignment for reference and output? Q4: According to the sentence context, is it no-error or minor-error or major-error? Q5: Is the explanation consistent with the given error type? Q6: Is error location in explanation? Q7: Do two error locations mention the same location?</h2>
<p>Table 4: Prompt for GPT4 feedback: We asked following questions to determine the correctness and consistency of the explanation. This is a simplified prompt of how we obtained GPT-4 feedback for Table 2 (Complete examples can be found in Appenfix Table 34 and 35).
corresponding alignment score will be $11 / 16$.</p>
<h3>4.3 Refinement with Meta-Feedback</h3>
<p>We then leverage the well-learned Exp-Generator to produce high-quality pseudo training data to iteratively refine the model performance, which can further benefit final quality score calculation. Specifically, we use hypothsis $h_{i}$ with reference $k_{i}$ as the model input and employ sampling strategies to generate diverse diagnostic reports. Due to discrepancies between synthetic data and real world hypothesis-reference pairs ${x, y}$ and</p>
<p>${h,k}$ (Sellam et al., 2020a; Xu et al., 2022b), we anticipate real world model hypothesis can trigger diverse failure modes from Table 2. For each input pair $\left(h_{i}, k_{i}\right)$, we use top p samping to sample $n$ possible diagnostic outputs, denotated as $\left{o_{1}, o_{2}, \ldots, o_{n}\right}$. Based on the feedback scores from GPT-4, we keep the diagnostic output that optimizes the alignment score, $o_{\text {aligned }}=\left{t_{\text {aligned }}, l_{\text {aligned }}, s e_{\text {aligned }}, e_{\text {aligned }}\right}$, and further fine-tune our Exp-Generator. This automatic critiques and self-training pipeline can further encourage our Exp-Generator to generate more accurate diagnostic reports. Based on the human evaluation, the final quality score can reduce failure modes and align outputs with humans better.</p>
<p>$$
\mathcal{L}\left(o_{\text {aligned }}, x, y\right)=-\log P\left(o_{\text {aligned }} \mid y, x ; \theta\right)
$$</p>
<h2>5 Experiments</h2>
<p>In the experiment section, we aim to answer the following research questions: 1) What is the performance across various tasks within the English language? 2) What is the performance across different domains within the same task? 3) What is the performance across different evaluation dimensions? 4) What is the performance at unseen tasks? 5) Given that LLaMA is predominantly trained in English texts, can it effectively evaluate generations in other languages? 6) Can we align the diagnostic report with human expectations without requiring extensive human efforts?</p>
<p>To answer Q1, we tested InSTructSCORE at various tasks, including WMT22 (Machine Translation) (Freitag et al., 2022), WebNLG (Table-totext) (Castro Ferreira et al., 2020), Flicker3k (Captioning) (Hodosh et al., 2013), BAGEL (Keyword-to-text) (Mairesse et al., 2010) and Commongen (Commonsense text generation) (Lin et al., 2020). To address Q2, we examed our method at four diverse domains: News, Conversation, Social, and E-Commerce at WMT22. For Q3, we evaluated our method at five evaluation dimensions at WebNLG. For Q4, we evaluated InSTructSCORE at BAGEL benchmark, a task and evaluation dimensions that are unseen in the synthetic data. For Q5, we evaluated our approach to English-to-German translations in order to investigate its multilingual evaluation capabilities. Lastly, we demonstrate our metric with automatic critique and refinement can achieve higher human ratings regarding failure modes.</p>
<h3>5.1 Experiment Setup</h3>
<p>Baseline and Benchmark We tested our InSTRUCTSCORE at 1) WMT22 shared metric task, in two target languages: English and German. WMT22 uses an MQM-based human evaluation procedure (Freitag et al., 2021a). WMT22 has 14 English, 16 German participating Machine Translation systems, with 26,250 and 21,040 humanannotated outputs respectively; 2) WebNLG20: The input of the task contains Wikipedia triples, and the output is a natural language text. This benchmark contains 16 participating WebNLG systems with 2832 human-annotated outputs; 3) Flicker3K-CF: This benchmark contains 145K binary quality judgment gathered from CrowdFlower over 48 K (image, caption) pairs with 1 K unique images; 4) Commongen: This benchmark contains 2796 model outputs from 6 participating systems. All human annotations are done in pairwise rankings for the same source input. Therefore, we compute ranking accuracy to estimate the metric's performance; 5) BAGEL: The input of this task contains keywords and the output is a fluent human conversation. This benchmark contains 202 model outputs. We included top-performing baseline metrics which joined each challenge, including n-gram based metrics: BLEU (Papineni et al., 2002), chrF (Popović, 2015), METEOR (Banerjee and Lavie, 2005) and CIDEr (Vedantam et al., 2015); Unsupervised learned metrics: PRISM (Thompson and Post, 2020), BARTScore (Yuan et al., 2021), BERTScore (Zhang et al., 2019), and SEScore2 (Xu et al., 2022a); LLM-based metrics: GPT3Dav3 and GPT4(Kocmi and Federmann, 2023); Supervised learned metrics: BLEURT-20 (Sellam et al., 2020a), MaTESe (Perrella et al., 2022), UniTE (Wan et al., 2022) and MetricX XXL.</p>
<p>Implementation We utilize GPT-4 as our implicit evaluation knowledge base and LLaMA-7B as our trained initialization ${ }^{1}$. To ensure coverage of diverse text domains, we separately collect a dataset of 10k raw sentences from 100 different domains using GPT-4 for both English and German. Details on the construction of this dataset are provided in Appendix Sec. C. To adapt to specific task domains, we separately gathered 10k sentences for the training datasets of WebNLG17 (Gardent et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th>Seen Tasks</th>
<th></th>
<th></th>
<th>Unseen Task</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>WMT22(Zh $\rightarrow$ En)</td>
<td>WebNLG20</td>
<td>Flicker3k</td>
<td>Commongen</td>
<td>BAGEL</td>
<td>Rank</td>
</tr>
<tr>
<td></td>
<td></td>
<td>$\tau / \rho$</td>
<td>$\tau / \rho$</td>
<td>$\tau / \rho$</td>
<td>Acc</td>
<td>$\tau / \rho$</td>
<td>Avg</td>
</tr>
<tr>
<td>Supervised</td>
<td>MATESE</td>
<td>38.9 / 52.8</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Metric XXL</td>
<td>42.7 / 58.1</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>COMET-22</td>
<td>42.8 / 58.5</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>BLEURT-20</td>
<td>36.1 / 43.0</td>
<td>40.2 / 63.5</td>
<td>24.3 / 35.1</td>
<td>39.5</td>
<td>22.9 / 32.3</td>
<td>2.8</td>
</tr>
<tr>
<td></td>
<td>BLEU</td>
<td>14.5 / 17.5</td>
<td>20.1 / 20.7</td>
<td>13.8 / 21.6</td>
<td>26.8</td>
<td>10.9 / 16.8</td>
<td>10.2</td>
</tr>
<tr>
<td></td>
<td>ChrF</td>
<td>15.4 / 14.7</td>
<td>26.6 / 40.0</td>
<td>13.3 / 24.5</td>
<td>33.0</td>
<td>10.8 / 16.8</td>
<td>9.0</td>
</tr>
<tr>
<td></td>
<td>METEOR</td>
<td>16.5 / 18.8</td>
<td>25.6 / 36.3</td>
<td>13.4 / 23.1</td>
<td>33.9</td>
<td>12.6 / 14.5</td>
<td>9.4</td>
</tr>
<tr>
<td></td>
<td>CIDEr</td>
<td>18.0 / 22.1</td>
<td>26.1 / 31.8</td>
<td>15.2 / 29.8</td>
<td>32.6</td>
<td>15.7 / 23.1</td>
<td>7.8</td>
</tr>
<tr>
<td>Without Supervision</td>
<td>BERTScore</td>
<td>31.6 / 37.6</td>
<td>32.8 / 50.4</td>
<td>17.4 / 24.6</td>
<td>38.8</td>
<td>17.1 / 28.2</td>
<td>5.6</td>
</tr>
<tr>
<td></td>
<td>BARTScore</td>
<td>22.2 / 24.9</td>
<td>33.1 / 56.8</td>
<td>17.9 / 22.2</td>
<td>37.0</td>
<td>20.3 / 20.7</td>
<td>6.2</td>
</tr>
<tr>
<td></td>
<td>PRISM</td>
<td>25.0 / 27.9</td>
<td>37.6 / 59.4</td>
<td>16.1 / 23.8</td>
<td>38.0</td>
<td>21.7 / 30.7</td>
<td>5.1</td>
</tr>
<tr>
<td></td>
<td>GEMBA-GPT4</td>
<td>38.2 / 37.4</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>SEScore2</td>
<td>33.0 / 46.4</td>
<td>36.8 / 48.4</td>
<td>16.7 / 22.2</td>
<td>34.4</td>
<td>23.3 / 32.5</td>
<td>4.9</td>
</tr>
<tr>
<td></td>
<td>InSTRUCTSCORE</td>
<td>40.3 / 51.9</td>
<td>39.5 / 59.0</td>
<td>30.1 / 34.6</td>
<td>58.2</td>
<td>25.6 / 34.2</td>
<td>2.0</td>
</tr>
</tbody>
</table>
<p>Table 5: We applied segment-level Kendall and Pearson correlation on WMT22, WebNLG20, Flicker3k, and BAGEL, and ranking accuracy for Commongen. InSTRUCTSCORE significantly outperforms all unsupervised metrics in 8/9 directions using William's pairwise significance test ( $\mathrm{p}&lt;0.05$ ). The top supervised and unsupervised metrics are bolded. Pearson and Kendall correlations are ranked per task, with overall performance being the average rank across tasks. Appendix Table 9 contains ranking details for each correlation and task.
2017), CoCo Captioning (Chen et al., 2015), and CommonGen (Liu et al., 2022). We apply the respective prompts defined in Appendix Tables 29, 30, 31, and 32 to generate synthetic data.</p>
<p>We define four evaluation scenarios: 1) evaluation with reference only; 2) evaluation with reference and additional data; 3) evaluation with reference where the source has different modalities; 4) evaluation with reference and world knowledge. For each scenario, we obtain 10k candidatereference pairs as input and structured diagnostic reports as output. We train a separate checkpoint for each evaluation scenario, resulting in four checkpoints in total. All models are fine-tuned with language modeling loss with 10k synthetic data. Each model is trained for three epochs, with a learning rate, batch size, and weight decay of $2 \mathrm{e}-5$, 128 , and 0 , respectively. During the evaluation of each model, we set the temperature to 0 for greedy decoding. Details of our automatic critique and self-training are included in the Appendix Sec E.</p>
<p>Meta-evaluation We assess the performance of InSTRUCTSCORE using Segment-level Kendall and Pearson correlations between human and metric output. Kendall Tau-b might favor tie pairs, possibly giving an unfair advantage to certain systems (Deutsch et al., 2023). Pearson, on the other hand, measures linear association. By reporting both complementary results, we can comprehensively understand the metric's performance. We employed three human annotators to assess the alignment of our model before and after refinement. In particular, the human raters ${ }^{2}$ will estimate a binary score based on M1 to M6 and G1 to G4 criteria for each field in the diagnostic report. The total score of a diagnostic report is calculated as $\frac{# \text { correct fields }}{# \text { total fields }}$. The final score is averaged among raters. Details of human evaluation procedures are included in the Appendix Sec F and Table 8.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Segment-level Kendall $(\tau)$ correlation on Zh-En for different domains of WMT22. We connect points to highlight InSTRUCTSCORE with two other top performing metrics in the figure.</p>
<h3>5.2 Main Results</h3>
<p>Robust Performance across Tasks We assess the primary performance of InSTRUCTSCORE for five diverse NLG tasks. As shown in Table 5, InSTRUCTSCORE significantly outperforms all other unsupervised metrics in 8 out of 9 directions, achieving the best overall ranking. All improve-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ments are statistically significant by William's pairwise significant test Graham and Baldwin, 2014 with $p&lt;0.05$. Surprisingly, InSTRUCTSCORE even outperforms prior supervised learned metrics that trained over direct assessment data (DA), leading BLEURT20 in 6 out of 9 directions. Compared to GPT4 baseline, InSTRUCTSCORE outperforms GEMBA-GPT4 with 0.021 in Kendall and 0.145 in Pearson correlation. The larger gap in Pearson correlation can be explained by a large set of ties that GEMBA-GPT4 is producing. This will lead to false positive in Kendall correlation. Lastly, we demonstrate that InSTRUCTSCORE can achieve close performance to the supervised learned metrics, MATESE, COMET22 and Metric XXL, that have trained over comprehensive human rating data (DA and MQM), with average 0.012 gap in Kendall correlation and 0.045 in Pearson correlation.</p>
<p>Robust Performance across Domains In Figure 3, we further evaluate the performance of InSTRUCTSCORE across different domains. From Kendall correlation analysis, InSTRUCTSCORE surpasses all unsupervised metrics in four domains except GPT-3.5 and GPT-4 baselines at Chat. It achieves comparable performance in Ecommerce, Chat, and Social domains when compared to the leading supervised metrics COMET22 and Metric-XXL. However, its performance is noticeably worse in the News domain compared to SOTA COMET22 and Metric-XXL. This gap can be primarily attributed to their supervised data distribution at News domain DA and MQM data.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Segment-level Kendall Correlation on WebNLG Data-to-Text generation. Cor, Cov, Flu, Rel, and Str represent Correctness, Coverage, Fluency, Relevance, and Text Structure respectively. We connect points to highlight INSTRUCTSCORE with two other top performing metrics in the figure.</p>
<p>Robust Performance across Dimensions Unlike most of metrics which only output a single score, InSTRUCTSCORE can output a score in each evaluation dimension. In Figure 4, we demonstrate that InSTRUCTSCORE outperforms all unsupervised learned metrics across five different dimensions. Compared to BLEURT, which trained over WebNLG human rating data, InSTRUCTSCORE outperforms best performed BLEURT in three out of five evaluation dimensions. This signifies that InSTRUCTSCORE can be extended to assign a single quality score but extending into multidimensional NLG evaluation.</p>
<p>Generalization over Unseen Task Since each NLG task has distinct evaluation criteria, one natural question to ask: Is InSTRUCTSCORE generalizable to the task with unseen data format and evaluation criteria? To verify this question, we use BAGEL benchmark as unseen task since it contains distinct data formats from our training data and contains distinct criteria. From Table 5 and Figure 5, we demonstrate that despite never seen the evaluation criteria of keywords to text, InSTRUCTSCORE achieves the higher Kendall and Pearson correlation compared to BLEURT as well as two of three unseen evaluation dimensions.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Segment-level Kendall Correlation on different dimensions of BAGEL dialogue generation. We connect points to highlight InSTRUCTSCORE with two other top performing metrics in the figure.</p>
<h3>5.3 Quantitative Analysis</h3>
<p>Performance at Non-English Language. In Figure 6, InSTRUCTSCORE outperforms most unsupervised metrics, but not 175B GPT3.5 models at WMT22 English-to-German or supervised counterparts like COMET22 and BLEURT20. We hypothesize that multiple factors contribute to the observed LLaMA performance on non-English texts: (1) limited pretraining data, resulting in weaker pretrained knowledge for other languages, and (2) the task setup requiring alignment between languages due to mixed code text generation (see Appendix Sec D and Table 24). Future research could explore multilingual alignment warmup methods before training on evaluation data.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Segment-level Kendall correlation of InSTRUCTSCORE at WMT22 English-to-German. Purple indicates unsupervised learned metrics, while Blue indicates supervised learned metrics.</p>
<h3>Automatic critique and Self-training can improve human Alignment</h3>
<p>We conduct human evaluation to assess our metric's alignment before and after self-training. From Figure 7, we demonstrate that INSTRUCTSCORE after automatic critique and refinement can significantly reduce most of global and local failure modes. In particular, all global failures have more than 50% decreases in occurrences. This signifies that INSTRUCTSCORE has improved over its phrase alignment, error identification, and error formats. Moreover, consistency between four fields has all improved, demonstrated by improvements over all M occurrences. We observed that M6 has slight increase. This is due to some of the conversions from global failures into the local failures. In Table 6, we demonstrated that INSTRUCTSCORE can achieve 0.106 absolute human score gains with on-par performance at Kendall and Pearson correlation. The detailed human evaluation procedure, individual rater scores, and a case study are included in the Appendix Sec F, 10, and 23.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Failure mode occurrence per sentence before and after refinement. Results are averaged by three human raters.</p>
<table>
<thead>
<tr>
<th>INSTRUCTSCORE</th>
<th>Kendall</th>
<th>Pearson</th>
<th>Human</th>
</tr>
</thead>
<tbody>
<tr>
<td>Fine-tune</td>
<td>0.404</td>
<td>0.515</td>
<td>0.773</td>
</tr>
<tr>
<td>Fine-tune+Refinement</td>
<td>0.403</td>
<td>0.519</td>
<td>0.879</td>
</tr>
</tbody>
</table>
<p>Table 6: We report Segment-level Kendall, Pearson correlation, and human score before and after refinement.</p>
<h3>Automatic critique and Self-training can improve precision and recall of INSTRUCTSCORE</h3>
<p>We conduct a human evaluation of the quality of INSTRUCTSCORE's annotations (with three annotators). We calculated the precision and recall of the annotations before and after refinement. <em>Precision = # of correctly annotated error fields / # of INSTRUCTSCORE's labeled error fields. Recall = # of correctly annotated error fields / (# of correctly annotated error fields + the number of error fields that INSTRUCTSCORE missed).</em> Error fields consist of error type, severity label, error location, and explanations. In Table 7, INSTRUCTSCORE can achieve 77.8% precision and 82.4% recall before refinement. After refinement, INSTRUCTSCORE can improve precision and recall by 11.6% and 3.2%, respectively. In addition, we study the field of explanation in particular, the refinement step can improve the precision of explanation from 75.6% to 86.1% and improve the recall of explanation from 81.9% to 85.0%. <strong>Overall, our finding suggests that 89.4% of InstructScore's output after refinement is correct and it can identify 85.6% of errors produced by the translation system.</strong></p>
<table>
<thead>
<tr>
<th>Human Evaluation</th>
<th>PAll</th>
<th>RAll</th>
<th>PExp</th>
<th>RExp</th>
</tr>
</thead>
<tbody>
<tr>
<td>Before refinement</td>
<td>0.778</td>
<td>0.824</td>
<td>0.756</td>
<td>0.819</td>
</tr>
<tr>
<td>After refinement</td>
<td>0.894</td>
<td>0.856</td>
<td>0.861</td>
<td>0.850</td>
</tr>
</tbody>
</table>
<p>Table 7: We report precision and recall before and after refinement on all annotation fields (error type, location, severity label, and explanation) by INSTRUCTSCORE, annotated by PAll and RAll. In addition, we include precision and recall on the explanation field only, annotated by PExp and RExp.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we present a novel framework for explainable text generation evaluation, addressing the existing black-box limitations associated with learned metrics. We define a set of failure modes to regularize the explanations. We empirically demonstrate that INSTRUCTSCORE can be generalized to different domains, tasks, and evaluation dimensions, achieving the best ranking compared to other general text generation metrics. Lastly, our refinement from automatic feedback can further improve human alignment score, precision, and recall, by 13.7%, 11.6%, and 3.2%, respectively, leading to a more accurate alignment with human requirements. We released the INSTRUCTSCORE model for public use and open-source the data and codes.</p>
<h2>Limitations</h2>
<p>While we have not yet been able to test INSTRUCTSCORE in a multilingual setting due to the limited availability of human annotation and significant label costs, it has shown promising results in leveraging smaller synthetic and feedback data to fine-tune and refine its performance. In fact, InSTRUCTSCORE has demonstrated superior performance compared to unsupervised baselines, such as BERTScore, BARTScore, and PRISM in high-resource non-English language, such as German. Going forward, we aim to assess INSTRUCTSCORE's multilingual evaluation capabilities across high, medium, and low-resource languages. As our instructions are in English and the evaluation target is in other language, we plan to enhance InSTRUCTSCORE's mixed code generation and multilingual word alignment abilities by exploring more pretraining and warm-up techniques.</p>
<p>Although our current computing resources restrict our ability to confirm the impacts of model size on performance, future research should investigate model size utilizing scaling law (Kaplan et al., 2020) to uncover potential improvements in failure modes related to larger model sizes.</p>
<p>In the present framework, we introduce a straightforward but efficient refinement process to enhance the alignment of our metric with human judgements. Future research can investigate more advanced techniques, such as incorporating human feedback through reinforcement (Ouyang et al., 2022), for more effective integration of feedback into the training pipeline. More sophisticated approach holds promising potential to further boost the performance of this pipeline.</p>
<h2>Ethics Statement</h2>
<p>InSTRUCTSCORE, as an open-source and explainable evaluation metric for text generation, emphasizes transparency and accountability in the evaluation of natural language processing systems. By generating interpretable evaluations and diagnostic reports, it fosters trust among developers and end-users. Moreover, its introduction could propel further innovation in the field of explainable evaluation metrics and make high-quality evaluation tools more accessible. However, it is crucial to ascertain that the interpretations provided by InstructScore do not harbor biases present in the training data, and data privacy and security measures are observed.</p>
<p>The quality improvements that may stem from using InstructScore could be instrumental in diverse applications such as translation services, chatbots, and content creation. Nonetheless, it is vital to monitor these advancements to ensure that they do not inadvertently suppress linguistic diversity. Additionally, the biases that may have been passed on to InstructScore from pre-existing models like GPT4 should be critically examined, and efforts must be made to alleviate biases that could impact language, dialect, or cultural representation.</p>
<p>Finally, the impact of InstructScore on educational and professional writing practices should not be overlooked. As writers and educators might adapt their styles based on algorithmic evaluations, it is essential to balance the quest for higher scores with the preservation of human creativity and the diversity of expression. InstructScore has the potential to be a powerful tool in the evaluation of text generation, but it is imperative that ethical considerations surrounding transparency, accessibility, bias, and societal impact are vigilantly monitored and addressed.</p>
<p>We hired three human raters to annotate InSTRUCTSCORE's diagnostic reports. We randomly sampled 100 candidate-reference pairs from WMT22 Chinese-to-English direction. All evaluated data does not contain sensitive or explicit languages. There is no risk of exposing raters' identities and they have full knowledge of data usage. All annotators are well-trained with evaluation protocols and are proficient in English. The salary rate is above minimum wage at the local region. All human rating data will be released upon paper acceptance. The detailed human evaluation process is included in the Appendix Sec F.</p>
<h2>7 Acknowledgement</h2>
<p>This work was supported by the National Science Foundation award #2048122. The views expressed are those of the author and do not reflect the official policy or position of the US government.</p>
<h2>References</h2>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65-72, Ann Arbor, Michigan. Association for Computational Linguistics.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Thiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh, Chris van der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina. 2020. The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+ 2020). In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 55-76, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server.</p>
<p>Daniel Deutsch, George Foster, and Markus Freitag. 2023. Ties matter: Modifying kendall's tau for modern metric meta-evaluation.</p>
<p>Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460-1474.</p>
<p>Markus Freitag, David Grangier, and Isaac Caswell. 2020. Bleu might be guilty but references are not innocent. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 61-71.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022. Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 46-68, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondřej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. In Proceedings of the Sixth Conference on Machine Translation, pages 733-774, Online. Association for Computational Linguistics.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on</p>
<p>Natural Language Generation, pages 124-133, Santiago de Compostela, Spain. Association for Computational Linguistics.</p>
<p>Yvette Graham and Timothy Baldwin. 2014. Testing for significance of increased correlation with human judgment. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172-176, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, 47:853-899.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models.</p>
<p>Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520.</p>
<p>Christoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, and Steffen Eger. 2022. Towards explainable evaluation metrics for natural language generation. arXiv preprint arXiv:2203.11131.</p>
<p>Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1823-1840, Online. Association for Computational Linguistics.</p>
<p>Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. 2022. Generated knowledge prompting for commonsense reasoning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3154-3169, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment.</p>
<p>Qingyu Lu, Liang Ding, Liping Xie, Kanjian Zhang, Derek F Wong, and Dacheng Tao. 2022. Toward human-like evaluation for natural language generation with error analysis. arXiv preprint arXiv:2212.10179.</p>
<p>François Mairesse, Milica Gašić, Filip Jurčíček, Simon Keizer, Blaise Thomson, Kai Yu, and Steve Young. 2010. Phrase-based statistical language generation using graphical models and active learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 15521561, Uppsala, Sweden. Association for Computational Linguistics.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Stefano Perrella, Lorenzo Proietti, Alessandro Scirè, Niccolò Campolungo, and Roberto Navigli. 2022. MaTESe: Machine translation evaluation as a sequence tagging problem. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 569-577, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Maja Popović. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578-585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702, Online. Association for Computational Linguistics.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020a. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Thibault Sellam, Amy Pu, Hyung Won Chung, Sebastian Gehrmann, Qijun Tan, Markus Freitag, Dipanjan Das, and Ankur Parikh. 2020b. Learning to evaluate translation beyond english: Bleurt submissions to the wmt metrics 2020 shared task. In Proceedings of the Fifth Conference on Machine Translation, pages 921-927, Online. Association for Computational Linguistics.</p>
<p>Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In</p>
<p>Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, pages 223-231, Cambridge, Massachusetts, USA. Association for Machine Translation in the Americas.</p>
<p>Brian Thompson and Matt Post. 2020. Automatic machine translation evaluation in many languages via zero-shot paraphrasing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90-121, Online. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation.</p>
<p>Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang, Boxing Chen, Derek Wong, and Lidia Chao. 2022. UniTE: Unified translation evaluation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8117-8127, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Wenda Xu, Xian Qian, Mingxuan Wang, Lei Li, and William Yang Wang. 2022a. Sescore2: Retrieval augmented pretraining for text generation evaluation. arXiv preprint arXiv:2212.09305.</p>
<p>Wenda Xu, Yi-Lin Tuan, Yujie Lu, Michael Saxon, Lei Li, and William Yang Wang. 2022b. Not all errors are equal: Learning text generation metrics using stratified error synthesis. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6559-6574, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems, volume 34, pages 27263-27277. Curran Associates, Inc.</p>
<p>Chrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat Lertvittayakumjorn, José G. C. de Souza, Steffen Eger, Diptesh Kanojia, Duarte Alves, Constantin Orâsan, Marina Fomicheva, André F. T. Martins, and Lucia Specia. 2022. Findings of the WMT 2022 shared task on quality estimation. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 69-99, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena.
Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. Towards a unified multidimensional evaluator for text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 20232038, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<h2>A Prompting for Data Generation</h2>
<p>In this section, we will discuss the data generation processes using prompts for GPT-4 API. We start from the seed domains, such as News, Technical, Legal and Medical, etc. We query GPT-4 to augment seed domains to 100 domains (See a prompt example in Table 25). For each domain, we query GPT-4 to generate 100 topics (See a prompt example in Table 26). Therefore, we have obtained 10,000 different topics for this process. For each topic, we generate 5 distinct sentences, each with a different length and structure (See a prompt example in Table 27). For each topic, we randomly select one sentence out of five candidates, yielding 10,000 raw text sentences from distinct topics. We start from each raw text to synthesize sentence errors.</p>
<p>Based on the guidelines provided by (Freitag et al., 2021a), we arbitrarily decide the range of errors from 1 to 5 . For each raw text, we randomly select a number of errors from 1 to 5 . Each synthesized error will have error types that are predefined from MQM guidelines (Freitag et al., 2021a). For each synthesized error, we randomly select one error type that is defined in Table 28 and randomly select major or minor severity labels. Therefore, based on the given raw text, error type, and severity label, GPT-4 needs to generate the location of the error and explanation for the error annotation (See a prompt example in Table 29). To disentangle the model's reliance on sentence structure and lexical overlap, we paraphrase the raw text to yield the pseudo reference. The generated synthetic erroneous sentence will become pseudo model generated text. This completes our synthetic data construction. During the fine-tuning process on the LLAMA model, we input the model with pseudo reference and pseudo candidate text, LLAMA is optimized to generate error type, error location, severity label, and explanation.</p>
<h2>B Prompting for LLM Feedback</h2>
<p>To obtain GPT-4 feedback for InStrUCTSCORE's generated diagnostic report, we developed two different prompts. If our diagnostic report contains error annotations, we asked 7 questions listed in Table 34 to determine the correctness and consistency of the explanation output. This is how we collected responses for failure modes that we defined in Table 2. if our diagnostic report does not contain error annotation, we directly query GPT-4</p>
<table>
<thead>
<tr>
<th>Human Evaluation Instructions:</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>M1: Error type descriptions are not consistent with explanation</td>
<td></td>
</tr>
<tr>
<td>M2: Error locations are not consistent with the explanation</td>
<td></td>
</tr>
<tr>
<td>M3: Error locations are not referred in the output text</td>
<td></td>
</tr>
<tr>
<td>M4: Error locations can not refer to the output text</td>
<td></td>
</tr>
<tr>
<td>M5: Major and minor labels do not correspond to the correct severity levels</td>
<td></td>
</tr>
<tr>
<td>M6: The explanation is wrong. However, error at a specified location does exist.</td>
<td></td>
</tr>
<tr>
<td>G1: Error described in the explanation is not an error</td>
<td></td>
</tr>
<tr>
<td>G2: One error is mentioned more than once among explanations</td>
<td></td>
</tr>
<tr>
<td>G3: Incorrect phrase and correct phrase are not correctly aligned</td>
<td></td>
</tr>
<tr>
<td>G4: One error span mentions multiple errors</td>
<td></td>
</tr>
<tr>
<td>The rule is following: local failure mode will only impact local field in the error annotation, like error type, error</td>
<td></td>
</tr>
<tr>
<td>location or explanation. However, global failure mode is between different error annotations.</td>
<td></td>
</tr>
<tr>
<td>For example, sentence 1 contains 2 errors. You will have 8 fields in total</td>
<td></td>
</tr>
<tr>
<td>Error type 1:</td>
<td></td>
</tr>
<tr>
<td>Major/minor:</td>
<td></td>
</tr>
<tr>
<td>Error location 1:</td>
<td></td>
</tr>
<tr>
<td>Explanation for error 1:</td>
<td></td>
</tr>
<tr>
<td>Error location 2:</td>
<td></td>
</tr>
<tr>
<td>Major/minor:</td>
<td></td>
</tr>
<tr>
<td>Error location 2:</td>
<td></td>
</tr>
<tr>
<td>Explanation for error 2:</td>
<td></td>
</tr>
<tr>
<td>If one M1 and one M2 occur, you will receive score 6/8. If G1 occurs, the entire error annotation 1 or 2 (like error type2,</td>
<td></td>
</tr>
<tr>
<td>major/minor, location and explanation all become invalid). You will receive score 4/8. If a global failure has an overlap</td>
<td></td>
</tr>
<tr>
<td>with local failure, you will choose global failure as annotation.</td>
<td></td>
</tr>
<tr>
<td>Your annotations will begin from here:</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 8: This is the human evaluation instruction for human raters.
to validate this claim. If GPT-4 reconfirms with InSTRUCTSCORE 's claim, the feedback score is 1. Otherwise, 0 .</p>
<h2>C Raw Sentence Generation from GPT-4</h2>
<p>For the data generation pipeline, we start by prompting GPT-4 with 12 seed domains, such as medical, technology News, etc. We used GPT-4 to augment them to 100 distinct domains. For each domain, we further used GPT-4 to generate 100 topics. In the end, we generate 10k sentences with distinct topics, lengths, and sentence structures (See Appendix Table 25).</p>
<h2>D Implementation of InSTRUCTSCORE at Non-English Language</h2>
<p>We trained a German reference only metric to investigate InSTRUCTSCORE's multilingual capability. In this case, our metric will output explanations in English but quote error locations in German. See an example in Appendix Table 33.</p>
<h2>E Implementation on Refinement pipeline</h2>
<p>We used 18 participating system outputs at WMT20 (Sellam et al., 2020b) to approximate the distribution of the model-generated output. We use 2,000</p>
<p>Chinese-to-English parallel sentences. We randomly select one of the 20 MT systems to translate each source sentence and obtain 2,000 translation outputs in the end. We pair each translation with reference as input to InSTRUCTSCORE and use top p sampling with temperature 0.8 and $p=0.9$ to generate 8 candidate outputs for each input. We obtained GPT-4's feedback on those 16,000 candidate outputs and formed 35,932 ranking pairs ${ }^{3}$. We selected 4,777 diagnostic outputs which achieved the highest alignments scores to further fine-tune InSTRUCTSCORE.</p>
<h2>F Human Evaluation Procedure</h2>
<p>We conduct a human evaluation on WMT22 Zh-En testing set. We randomly select 100 system outputs from 14 participating systems. Following the prior practice (Freitag et al., 2021a), we hired three graduate students who are proficient in English language annotations. Each rater is trained with our annotation procedure for two hours before the annotations. The detailed human evaluation instruction is included in Table 8. Each rater will give a binary score for each field of error type, error location, major/minor label and explanation. If a global failure</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MT(Zh $\rightarrow$ En) <br> Rank( $\tau / \rho)$</th>
<th style="text-align: center;">Seen Tasks <br> Flicker3k <br> Rank( $\tau / \rho)$</th>
<th style="text-align: center;">Commgen <br> Rank(Acc)</th>
<th style="text-align: center;">BAGEL <br> Rank( $\tau / \rho)$</th>
<th style="text-align: center;">Unseen Task <br> Rank <br> Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MATESE</td>
<td style="text-align: center;">$4 / 3$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Metric XXL</td>
<td style="text-align: center;">$2 / 2$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMET-22</td>
<td style="text-align: center;">$1 / 1$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEURT-20</td>
<td style="text-align: center;">$6 / 6$</td>
<td style="text-align: center;">$1 / 1$</td>
<td style="text-align: center;">$2 / 1$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$3 / 3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">$14 / 13$</td>
<td style="text-align: center;">$10 / 10$</td>
<td style="text-align: center;">$8 / 10$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$9 / 8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChrF</td>
<td style="text-align: center;">$13 / 14$</td>
<td style="text-align: center;">$7 / 7$</td>
<td style="text-align: center;">$10 / 4$</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">$10 / 8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">$12 / 12$</td>
<td style="text-align: center;">$9 / 9$</td>
<td style="text-align: center;">$9 / 9$</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$8 / 10$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CIDEr</td>
<td style="text-align: center;">$11 / 11$</td>
<td style="text-align: center;">$8 / 8$</td>
<td style="text-align: center;">$7 / 3$</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">$7 / 6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">$8 / 7$</td>
<td style="text-align: center;">$6 / 5$</td>
<td style="text-align: center;">$4 / 6$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$6 / 5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BARTScore</td>
<td style="text-align: center;">$10 / 10$</td>
<td style="text-align: center;">$5 / 4$</td>
<td style="text-align: center;">$3 / 7$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$5 / 7$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PRISM</td>
<td style="text-align: center;">$9 / 9$</td>
<td style="text-align: center;">$3 / 2$</td>
<td style="text-align: center;">$6 / 5$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$4 / 4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GEMBA-GPT4</td>
<td style="text-align: center;">$5 / 8$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SEScore2</td>
<td style="text-align: center;">$7 / 5$</td>
<td style="text-align: center;">$4 / 6$</td>
<td style="text-align: center;">$5 / 7$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$2 / 2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">INSTRUCTSCORE</td>
<td style="text-align: center;">$3 / 4$</td>
<td style="text-align: center;">$2 / 3$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$1 / 1$</td>
</tr>
</tbody>
</table>
<p>Table 9: We rank metrics based on Meta evaluation such as Kendall correlation $(\tau)$, Pearson correlation $(\rho)$ and Ranking accuracy (Acc) for each task. The final ranking is the average rankings of all Meta-evaluations across five tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">INSTRUCTSCORE</th>
<th style="text-align: center;">Rater1</th>
<th style="text-align: center;">Rater2</th>
<th style="text-align: center;">Rater3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Fine-tune</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.698</td>
<td style="text-align: center;">0.804</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune+Refinement</td>
<td style="text-align: center;">0.849</td>
<td style="text-align: center;">0.887</td>
<td style="text-align: center;">0.902</td>
</tr>
</tbody>
</table>
<p>Table 10: The human scores from three different raters before and after refinement.
has an overlap with local failure, rater will choose global failure as annotation. For each annotated example, rater will count the number of correct fields and divide the total number of fields to obtain a alignment score. The final score of 100 examples is the average of 100 alignment scores. We obtained Kappa index among three raters, 0.388 , for inter-rater agreement. In Table 10, it is evident that all raters agree on the improvement in alignment after the refinement process.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Fields</th>
<th style="text-align: center;">Explanation Failure Mode</th>
<th style="text-align: center;">Percent\%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Local Failure Mode</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Error Type</td>
<td style="text-align: center;">M1: Consistency to explanation</td>
<td style="text-align: center;">$5.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Error Location</td>
<td style="text-align: center;">M2: Consistency to explanation</td>
<td style="text-align: center;">$1.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">M3: Error location hallucination</td>
<td style="text-align: center;">$8.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Explanation</td>
<td style="text-align: center;">M4: Error location hallucination</td>
<td style="text-align: center;">$4.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">M5: Major/Minor disagreement</td>
<td style="text-align: center;">$27.6 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Global Failure Mode</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">G1: No-error</td>
<td style="text-align: center;">$2.3 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Error Type,</td>
<td style="text-align: center;">G2: Repetition</td>
<td style="text-align: center;">$1.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Location and Explanation</td>
<td style="text-align: center;">G3: Phrase misalignment</td>
<td style="text-align: center;">$1.4 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">G4: Mention multiple errors</td>
<td style="text-align: center;">$0.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">G5: Phrase inconsistency</td>
<td style="text-align: center;">$2.2 \%$</td>
</tr>
</tbody>
</table>
<p>Table 11: This table shows explanation failure modes and their corresponding failure occurrence ratios.</p>
<h2>G Qualitative Analysis</h2>
<p>In this section, we will display a case study of our generated explanations from InSTRUCTSCORE.</p>
<h2>G. 1 Case study of InSTRUCTSCORE's Output Failure Modes of InSTRUCTSCORE's Output</h2>
<p>Please check ten failure modes (M1-M6, G1-G4) for InSTRUCTSCORE without refinement (See Table $13,14,15,16,17,18,19,20,21,22$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Seen Tasks</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Unseen Task <br> BAGEL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">WMT22(Zh $\rightarrow$ En) <br> $\tau / \rho$</td>
<td style="text-align: center;">WebNLG20 <br> $\tau / \rho$</td>
<td style="text-align: center;">Flicker3k <br> $\tau / \rho$</td>
<td style="text-align: center;">Commongen <br> Acc</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">INSTRUCTSCORE (LLaMA)</td>
<td style="text-align: center;">$40.3 / 51.9$</td>
<td style="text-align: center;">$39.5 / 59.0$</td>
<td style="text-align: center;">$30.1 / 34.6$</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">$25.6 / 34.2$</td>
</tr>
<tr>
<td style="text-align: center;">INSTRUCTSCORE (LLaMA2)</td>
<td style="text-align: center;">$39.6 / 48.9$</td>
<td style="text-align: center;">$39.0 / 54.1$</td>
<td style="text-align: center;">$21.1 / 21.8$</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">$31.0 / 46.2$</td>
</tr>
</tbody>
</table>
<p>Table 12: We applied segment-level Kendall and Pearson correlation on WMT22, WebNLG20, Flicker3k, and BAGEL, and ranking accuracy for Commongen. INSTRUCTSCORE with LLaMA initialization outperforms LLaMA2 initialization at Translation, data-to-text and captioning. However, INSTRUCTSCORE based on LLaMA2 initialization significantly outperforms LLaMA initialization at Commonsense generation and unseen task text-to-dialogue (BAGEL).</p>
<h1>INSTRUCTSCORE Input:</h1>
<p>Reference: Recommendation: Swiss legacy; wear time and art on your wrist; OMEGA is elegant and simple, low-key but attractive, suitable for work and formal situations.
Output: Recommended reason: Inheriting Switzerland, the art of time is in the wrist, Omega is dignified and elegant, simple and simple, not ostentatious but charming, and can be worn on commuting and formal occasions.</p>
<h2>INSTRUCTSCORE Output:</h2>
<p>Your Translation contains 5 errors:
Error type 1: Incorrect translation does not accurately represent the correct translation
Major/minor: Major
Error location 1: "Inheriting Switzerland"
Explanation for error 1: The phrase "Inheriting Switzerland" does not accurately convey the meaning of "Swiss heritage."
Error type 2: Incorrect translation includes information not present in the correct translation
Major/minor: Major
Error location 2: "the art of time is in the wrist"
Explanation for error 2: The phrase "the art of time is in the wrist" introduces information that is not present in the correct translation, which is about the art of time being on the wrist.
Error type 3: Incorrect translation is missing content from the correct translation
Major/minor: Major
Error location 3: "simple and simple"
Explanation for error 3: The phrase "simple and simple" is missing the word "elegant" from the correct translation, which should be "simple and elegant."
Error type 4: Incorrect translation has stylistic problems
Major/minor: Major
Error location 4: "not ostentatious but charming"
Explanation for error 4: The phrase "not ostentatious but charming" has stylistic problems, as it should be "understated but charming" to match the correct translation.
Error type 5: Wrong grammatical register (eg, inappropriately informal pronouns). Internal inconsistency (not related to terminology)
Major/minor: Major
Error location 5: "commuting"
Explanation for error 5: The word "commuting" is not consistent with the correct translation's use of "work" and changes the meaning of the sentence.</p>
<p>Table 13: This is the case where error type descriptions are not consistent with explanation (M1). Error type 5 should be a word choice error. However, model predicts that this is related to grammatical register.</p>
<h1>InSTRACTSCORE Input:</h1>
<p>Reference: The advantage of building inspection shafts using concrete modules is that the four sides of the modules are of concave groove structures, which are built to form chain locks.
Output: The advantage of using concrete modules to build inspection wells is that the upper, lower, left and right sides of the modules are all concave-convex groove structure, and the combined masonry forms a chain lock.</p>
<h2>InSTRACTSCORE Output:</h2>
<h2>Your Translation contains 4 errors:</h2>
<p>Error type 1: Incorrect translation does not accurately represent the correct translation
Major/minor: Major
Error location 1: "inspection wells" instead of "inspection shafts"
Explanation for error 1: The term "inspection wells" is not the same as "inspection shafts" and may confuse the reader.
Error type 2: Incorrect translation includes information not present in the correct translation
Major/minor: Major
Error location 2: "inspection wells" instead of "inspection shafts"
Explanation for error 2: The addition of "convex" changes the meaning of the structure, which is not present in the correct translation.
Error type 3: Incorrect translation is missing content from the correct translation
Major/minor: Major
Error location 3: "combined masonry" instead of "combined masonry work"
Explanation for error 3: The phrase "combined masonry work" is missing, which changes the meaning of the sentence.
Error type 4: Incorrect translation has stylistic problems
Major/minor: Minor
Error location 4: "The advantage of using concrete modules to build inspection wells is that the upper, lower, left and right sides of the modules are all concave-convex groove structure,"
Explanation for error 4: The sentence structure is awkward and not as clear as the correct translation.
Table 14: This is the case where error locations are not consistent with the explanation (M2).</p>
<h2>InSTRACTSCORE Input:</h2>
<p>Reference: 1. [Baijiu Distillery Direct Sales] Ningcheng Balihan Aged Cellar 42 Degrees 500 ML, Rich Flavor Sorghum Original Baijiu, Aged Spirit Tasting Mellow, Four Bottles in One Box
Output: 1. [Directly run by winery] Ningcheng Balihan Laojiao 42 degree 500ML lush-flavored sorghum puree liquor with a soft and refreshing taste, four bottles in the whole box</p>
<h2>InSTRACTSCORE Output:</h2>
<h2>Your Translation contains 5 errors:</h2>
<p>Error type 1: Incorrect translation does not accurately represent the correct translation
Major/minor: Major
Error location 1: "winery" instead of "distillery"
Explanation for error 1: The translation incorrectly states that the business is a winery instead of a distillery, which changes the meaning of the sentence.
Error type 2: Incorrect translation includes information not present in the correct translation
Major/minor: Major
Error location 2: "lush-flavored" instead of "rich-flavored"
Explanation for error 2: The translation adds the word "lush," which is not present in the correct translation and changes the meaning of the description.
Error type 3: Incorrect translation is missing content from the correct translation
Major/minor: Major
Error location 3: "aged spirit" instead of "aged baijiu"
Explanation for error 3: The translation omits the specific type of alcohol being described, which is important for understanding the product being sold.
Error type 4: Incorrect translation has stylistic problems
Major/minor: Major
Error location 4: "whole box" instead of "box" Explanation for error 4: The translation uses the phrase "whole box," which is not a standard way to describe a box and may confuse the reader.
Error type 5: Incorrect translation does not accurately represent the correct translation
Major/minor: Major
Error location 5: "soft and refreshing taste" instead of "smooth and refreshing taste"
Explanation for error 5: The translation changes the adjective "smooth" to "soft," which alters the description of the taste of the alcohol.</p>
<p>Table 15: This is the case where error locations are not referred in the output text (M3). "aged spirit" is not never mentioned in the output text.</p>
<h1>InSTRUCTSCORE Input:</h1>
<p>Reference: It is understood that the activities held during the "Children's Day" by China Children's Center this year include the "Cool Maker Loves Family" carnival of children's future learning, the first children's visual art festival, the "Book Carnival" event and the "Whole Family Loves Reading" Picture Book Reading activity.
Output: It is understood that the activities held by the China children's center during this year's "June 1" children's day include the "love family and create cool" children's future learning carnival, the first children's visual arts festival, the "book carnival" activity, the "family love reading" picture book series activity, etc.</p>
<h2>InSTRUCTSCORE Output:</h2>
<h2>Your Translation contains 5 errors:</h2>
<p>Error type 1: Incorrect translation includes information not present in the correct translation
Major/minor: Major
Error location 1: "June 1"
Explanation for error 1: The incorrect translation specifies the date of Children's Day as "June 1," while the correct translation says "Children's Day," which does not specify the date.
Error type 2: Incorrect translation has stylistic problems
Major/minor: Major
Error location 2: "love family and create cool"
Explanation for error 2: The phrase "love family and create cool" is awkward and does not convey the intended meaning of "celebrating families and creativity."
Error type 3: Incorrect translation is missing content from the correct translation
Major/minor: Major
Error location 3: "Children's Day"
Explanation for error 3: The incorrect translation omits the term "Children's Day," which is an important aspect of the correct translation.
Error type 4: Incorrect translation does not accurately represent the correct translation
Major/minor: Major
Error location 4: "picture book series activity"
Explanation for error 4: The incorrect translation changes the meaning of the "picture book reading activity" to imply a series of books rather than a single event.
Error type 5: Incorrect translation includes information not present in the correct translation
Major/minor: Major
Error location 5: "whole family loves reading"
Explanation for error 5: The incorrect translation adds the phrase "whole family loves reading," which is not present in the correct translation and changes the focus from "families" to "whole families."</p>
<p>Table 16: This is the case where error locations in explanation can not refer to the output text (M4). "whole family loves reading" is never mentioned in the output text. Instead, it was mentioned in the reference text.</p>
<h2>InSTRUCTSCORE Input:</h2>
<p>Reference: Therefore, Zhang Zhongmou thinks it is ironic for Intel to work on wafer foundry.
Output: So, Zhang Zhongmou feels ironic about Intel's entry into wafer foundry.</p>
<h2>InSTRUCTSCORE Output:</h2>
<h2>Your Translation contains 2 errors:</h2>
<p>Error type 1: Wrong grammatical register (eg, inappropriately informal pronouns). Internal inconsistency (not related to terminology)
Major/minor: Major
Error location 1: "So"
Explanation for error 1: The use of "So" is informal and inconsistent with the formal tone of the rest of the sentence.
Error type 2: Incorrect translation is missing content from the correct translation
Major/minor: Minor
Error location 2: "entry" instead of "entry into"
Explanation for error 2: The incorrect translation omits the word "into," which is necessary to convey the meaning of Intel's entry into the wafer foundry market.</p>
<p>Table 17: This is the case where major and minor labels do not correspond to the correct severity levels (M5). The use of "So" is not a severe error.</p>
<h1>InSTRUCTSCORE Input:</h1>
<p>Reference: Washing hands with air, diagnosing disease through breath, and accurately seeking for source of smog; last night, a series of creative academic achievements were awarded at the opening ceremony of the 2016 Annual Academic Conference of Zhejiang University.
Output: Wash hands with air, judge diseases, accurately search for the source of fog Yesterday evening, at the opening ceremony of the 2016 academic annual meeting of Zhejiang University, a series of academic achievements were recognized.</p>
<h2>InSTRUCTSCORE Output:</h2>
<p>Your Translation contains 3 errors:
Error type 1: Incorrect translation is missing content from the correct translation
Major/minor: Major
Error location 1: "judge diseases"
Explanation for error 1: The incorrect translation is missing the word "diagnose" and changes the meaning from accurately determining the cause of a disease to simply judging diseases.
Error type 2: Incorrect translation has stylistic problems
Major/minor: Major
Error location 2: "accurately search for the source of fog"
Explanation for error 2: The incorrect translation uses "fog" instead of "smog," which changes the meaning and makes the sentence less clear.
Error type 3: Incorrect translation includes information not present in the correct translation
Major/minor: Minor
Error location 3: "academic annual meeting"
Explanation for error 3: The incorrect translation adds the word "annual" to the phrase "academic meeting," which is not present in the correct translation. However, this does not significantly change the meaning of the sentence.</p>
<p>Table 18: This is the case where the explanation is wrong. However, error at a specified location does exist (M6). "academic annual meeting" is a minor error. However, explanation is wrong because the word "annual" is also present in the reference.</p>
<h2>InSTRUCTSCORE Input:</h2>
<p>Reference: This set of biological sensor uses the new material of graphene; its target is chemical elements, and its keen "sense of smell" enables it to reflect the health status of the body more in-depth and accurately.
Output: This biosensor uses graphene, a new material, and its target is chemical elements. Its keen sense of smell enables it to reflect the health status more deeply and accurately.</p>
<h2>InSTRUCTSCORE Output:</h2>
<p>Your Translation contains 2 errors:
Error type 1: Incorrect translation is missing content from the correct translation
Major/minor: Major
Error location 1: "and its target is chemical elements"
Explanation for error 1: The incorrect translation omits the information about the biosensor's target being "chemical elements" and instead only mentions "chemical elements."
Error type 2: Incorrect translation has stylistic problems
Major/minor: Minor
Error location 2: "biosensor" instead of "biological sensor"
Explanation for error 2: The incorrect translation uses "biosensor" instead of "biological sensor," which is a minor stylistic error that does not significantly change the meaning but may be noticed by the reader.</p>
<p>Table 19: Error described in the explanation is not an error (G1). The incorrect phrase and suggested revision are exactly the same.</p>
<h1>InSTRUCTSCORE Input:</h1>
<p>Reference: And how much longer does it take for the restaurant to prepare it? Could could help me ask them?
Output: Or how long before the restaurant is ready? Can you ask for me?</p>
<h2>InSTRACTSCORE Output:</h2>
<h2>Your Translation contains 3 errors:</h2>
<p>Error type 1: Incorrect translation is missing content from the correct translation
Major/minor: Major
Error location 1: "Or how long before the restaurant is ready?"
Explanation for error 1: The incorrect translation omits the information about the wait time for the food, which is an important detail in the correct translation.
Error type 2: Incorrect translation has stylistic problems
Major/minor: Major
Error location 2: "Can you ask for me?"
Explanation for error 2: The incorrect translation changes the meaning of the sentence by implying that the person is asking the question on behalf of someone else, rather than themselves.
Error type 3: Problems with grammar, other than orthography
Major/minor: Minor
Error location 3: "could help me ask them?"
Explanation for error 3: The incorrect translation uses "could help me ask them" instead of "could you ask them for me?" The grammar is slightly off, but the meaning is still clear.</p>
<p>Table 20: One error is mentioned more than once among explanations (G2).</p>
<h2>InSTRACTSCORE Input:</h2>
<p>Reference: Normally, people wash hands by flushing hands with water, with a little soap and sanitizer at most; however, Li Qizhang and his classmates invented a set of "equipment to wash hands with air".
Output: Ordinary people wash their hands with water, at most, wipe some soap and squeeze some hand sanitizer, while Li Qizhang and other students invented a set of "air hand washing device".</p>
<h2>InSTRACTSCORE Output:</h2>
<h2>Your Translation contains 4 errors:</h2>
<p>Error type 1: Incorrect translation is missing content from the correct translation
Major/minor: Major
Error location 1: "with air"
Explanation for error 1: The incorrect translation changes the method of hand washing from "with water" to "with air," which significantly alters the meaning of the sentence.
Error type 2: Incorrect translation has stylistic problems
Major/minor: Major
Error location 2: "Ordinary people wash their hands with water, at most, wipe some soap and squeeze some hand sanitizer,"
Explanation for error 2: The incorrect translation has a choppy and awkward sentence structure, making it difficult to understand the intended meaning.
Error type 3: Incorrect translation includes information not present in the correct translation
Major/minor: Major
Error location 3: "other students"
Explanation for error 3: The incorrect translation adds information about other students, which is not present in the correct translation.
Error type 4: Incorrect translation does not accurately represent the correct translation
Major/minor: Minor
Error location 4: "air hand washing device"
Explanation for error 4: The incorrect translation uses "air hand washing device" instead of "air hand washing equipment," which is a less accurate representation of the correct translation.</p>
<p>Table 21: This is the case where two phrases are misaligned in the explanation (G3). "With water" and "With air" are incorrectly aligned. "with air" is the correct generation output and is not an error.</p>
<h1>INSTRUCTSCORE Input:</h1>
<p>Reference: Wang Cheng pointed out that hosting the talent summit was an important measure to implement the spirit of the talent working conference of the central government and the spirit of the 12th Party Congress of Hunan, showing the sincerity of Hunan in terms of putting emphasis on talents, being eager for talents and making full use of talents.
Output: Wang Cheng pointed out that the talent summit is an important initiative to implement the central talent work conference and the spirit of the 12th Party Congress of the province, which shows the sincere sincerity of Hunan in attaching importance to talents, thirsting for talents and using talents well.</p>
<h2>InSTRUCTSCORE Output:</h2>
<h2>Your Translation contains 2 errors:</h2>
<p>Error type 1: Incorrect translation does not accurately represent the correct translation
Major/minor: Major
Error location 1: "the central talent work conference" and "the 12th Party Congress of the province"
Explanation for error 1: The incorrect translation changes the meaning by referring to different events and locations than the correct translation.
Error type 2: Incorrect translation has stylistic problems
Major/minor: Minor
Error location 2: "thirsting for talents"
Explanation for error 2: The phrase "thirsting for talents" is awkward and not commonly used, which makes it a stylistic issue.</p>
<p>Table 22: One error span mentions multiple errors (G4)</p>
<h2>InSTRUCTSCORE Input:</h2>
<p>Reference: Washing hands with air, diagnosing disease through breath, and accurately seeking for source of smog; last night, a series of creative academic achievements were awarded at the opening ceremony of the 2016 Annual Academic Conference of Zhejiang University.
Output: Wash hands with air, judge diseases, accurately search for the source of fog Yesterday evening, at the opening ceremony of the 2016 academic annual meeting of Zhejiang University, a series of academic achievements were recognized.</p>
<h2>InSTRUCTSCORE before refinement's output:</h2>
<p>Your Translation contains 3 errors:
Error type 1: Incorrect translation is missing content from the correct translation
Major/minor: Major
Error location 1: "judge diseases" Explanation for error 1: The incorrect translation is missing the word "diagnose" and changes the meaning from accurately determining the cause of a disease to simply judging diseases.
Error type 2: Incorrect translation has stylistic problems
Major/minor: Major
Error location 2: "accurately search for the source of fog"
Explanation for error 2: The incorrect translation uses "fog" instead of "smog," which changes the meaning and makes the sentence less clear.
Error type 3: Incorrect translation includes information not present in the correct translation
Major/minor: Minor
Error location 3: "academic annual meeting"
Explanation for error 3: The incorrect translation adds the word "annual" to the phrase "academic meeting," which is not present in the correct translation. However, this does not significantly change the meaning of the sentence.</p>
<h2>InSTRUCTSCORE after refinement's output:</h2>
<p>Your Translation contains 3 errors:
Error type 1: Incorrect translation is missing content from the correct translation
Major/minor: Major
Error location 1: "judge diseases" Explanation for error 1: The incorrect translation is missing the information about "diagnosing diseases" from the correct translation.
Error type 2: Incorrect translation has stylistic problems
Major/minor: Major
Error location 2: "accurately search for the source of fog"
Explanation for error 2: The incorrect translation uses "fog" instead of "smog," which changes the meaning and makes it less clear.
Error type 3: Incorrect translation does not accurately represent the correct translation
Major/minor: Minor
Error location 3: "academic annual meeting"
Explanation for error 3: The incorrect translation uses "academic annual meeting" instead of "academic annual conference," which is a minor error that does not significantly change the meaning but will be noticed.</p>
<p>Table 23: In this case, InSTRUCTSCORE before refinement is able to correctly identify the error location, type and major/minor label. However, its explanation over the error is incorrect. INSTRUCTSCORE after refinement improves model's rationable and output the correct explanation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We removed tie ranking pairs&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
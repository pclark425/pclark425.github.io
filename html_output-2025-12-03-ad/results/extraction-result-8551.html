<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8551 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8551</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8551</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-8e8e761be9087be8499f9ac641e35727344fd556</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8e8e761be9087be8499f9ac641e35727344fd556" target="_blank">EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> EnigmaEval, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning, is introduced.</p>
                <p><strong>Paper Abstract:</strong> As language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities, making them a unique testbed for evaluating frontier language models. We introduce EnigmaEval, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution paths. The benchmark comprises 1184 puzzles of varying complexity -- each typically requiring teams of skilled solvers hours to days to complete -- with unambiguous, verifiable solutions that enable efficient evaluation. State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8551.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8551.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ENIGMAEVAL-O1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI O1 (checkpoint o1-2024-12-17) evaluated on ENIGMAEVAL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI vision-language checkpoint (o1-2024-12-17) evaluated as part of the ENIGMAEVAL benchmark of multimodal puzzle-hunt problems; prompted to produce step-by-step solutions and a final answer in a standardized format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>O1 (o1-2024-12-17)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A frontier vision-language model checkpoint from OpenAI listed in Appendix B.1 and evaluated on multimodal puzzles in ENIGMAEVAL; prompted with a system template asking for step-by-step solutions and a final Answer line.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>ENIGMAEVAL (mixed puzzle-hunt problems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Multimodal puzzle-hunt problems (includes grid-based puzzles, crosswords, diagrams, images and layout-dependent puzzles requiring spatial/layout reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Models received either human-transcribed text+image representations or raw PDF/page images. A standardized system prompt required a step-by-step solution and a final 'Answer: <answer>' line; outputs were string-matched to ground truth at default temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompting to generate chain-of-thought style step-by-step solutions via the provided system + user prompt template; no external symbolic solver or explicit spatial module was used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Normal split accuracy 7.0%; Hard split accuracy 0.0%. Raw image (PDF) Normal accuracy 6.1%. Meta-puzzle solve rate when provided component answers: 8.7% (Table C.1).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No concrete evidence that O1 uses explicit spatial reasoning strategies; successes were rare and no ablation or probing demonstrated constructive spatial reasoning. The paper highlights preprocessing/OCR as a bottleneck for some inputs rather than showing robust spatial manipulation capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly to other frontier vision-language models in Table 2 and meta-puzzle rates in Table C.1; O1 was the best-performing model on the normal split but still far below human puzzle-hunters. Performance sometimes dropped between transcribed and raw image formats, indicating preprocessing differences across models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failed completely on hard split (0%). Performance sensitive to raw vs transcribed input (possible OCR/parsing failures). Auditing of correct answers flagged hallucinated plagiarism (false positives) when checking for copying from official solutions. Overall low pass rate suggests inability to perform the multi-step, layout-sensitive reasoning required by many puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8551.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8551.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ENIGMAEVAL-GEMINI-FLASH-THINKING</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Gemini 2.0 FLASH THINKING (gemini-2.0-flash-thinking-exp-01-21) evaluated on ENIGMAEVAL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Google multimodal checkpoint (Gemini 2.0 Flash Thinking) evaluated on ENIGMAEVAL; prompted to produce stepwise reasoning and answers, measured on transcribed and raw image inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GEMINI 2.0 FLASH THINKING (gemini-2.0-flash-thinking-exp-01-21)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A Google multimodal/vision-language model checkpoint listed in Appendix B.1, evaluated with the ENIGMAEVAL system prompt that requests step-by-step solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>ENIGMAEVAL (mixed puzzle-hunt problems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Multimodal puzzle-hunt problems (including grid-based puzzles and other layout/visual puzzles requiring spatial interpretation).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same standardized prompt pipeline as other models: either transcribed text+image inputs or raw PDF/page images; step-by-step solution required; final answer string-matched to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Asked to provide step-by-step solutions (chain-of-thought style) via the system prompt; no additional spatial-specific modules or symbolic reasoning tools were used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Normal split accuracy 1.4%; Hard split accuracy 0.0%. Raw image Normal accuracy 1.3%. Meta-puzzle solve rate 0.6% (Table C.1).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No explicit evidence of spatial reasoning; failure rates and lack of successful structured intermediate steps indicate limited spatial problem solving on these puzzle-hunt tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to other frontier models (Table 2); substantially worse than O1 on ENIGMAEVAL. Performance difference across raw vs transcribed inputs suggests OCR/visual parsing impacts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Zero accuracy on hard subset; low success on normal subset. Performance appears constrained by ability to synthesize multimodal layout information and perform multi-step deduction required by puzzle hunts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8551.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8551.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ENIGMAEVAL-CLAUDE-3.5-SONNET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude 3.5 Sonnet (claude-3-5-sonnet-20241022) evaluated on ENIGMAEVAL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic Claude 3.5 Sonnet checkpoint evaluated on the ENIGMAEVAL dataset using a prompt requiring stepwise solutions and a final answer; performance measured on text+image and raw image inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLAUDE 3.5 SONNET (claude-3-5-sonnet-20241022)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An Anthropic multimodal/vision-language checkpoint listed in Appendix B.1, evaluated with the benchmark's standardized prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>ENIGMAEVAL (mixed puzzle-hunt problems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Multimodal puzzle-hunt problems with grid elements, diagrams and images where spatial/layout interpretation contributes to solution finding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Models given either human-transcribed text+image format or raw PDF/page images; required to output step-by-step reasoning followed by an explicit 'Answer: ...' line; evaluation via exact string matching.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Produced stepwise textual reasoning as prompted; no puzzle-specific spatial modules were invoked.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Normal split accuracy 1.1%; Hard split accuracy 0.0%. Raw image Normal accuracy 1.0%. Meta-puzzle solve rate 1.0% (Table C.1).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>The paper reports no diagnostic evidence of successful spatial reasoning by this model; low solve rates imply limited ability to combine visual layout and multi-step deduction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Ranked low compared to O1 and similar to other models like Pixtral and GPT-40 in Table 2. Performance on transcribed vs raw inputs similar for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>0% accuracy on hard split. Limited success indicates failures on puzzles requiring long, cross-modal reasoning and spatial/layout-aware deductions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8551.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8551.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ENIGMAEVAL-PIXTRAL-LARGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral Pixtral Large Instruct (pixtral-large-instruct-2411) evaluated on ENIGMAEVAL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mistral's Pixtral Large Instruct vision-enabled checkpoint evaluated on ENIGMAEVAL; produced stepwise solutions under a standardized prompt and measured for accuracy on transcribed and raw image puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PIXTRAL LARGE (pixtral-large-instruct-2411)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal/vision-enabled checkpoint from Mistral listed in Appendix B.1, evaluated with ENIGMAEVAL's prompting and scoring setup.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>ENIGMAEVAL (mixed puzzle-hunt problems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Multimodal puzzles including grid-based and diagrammatic puzzles that require interpreting spatial/layout information.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Input: either human-transcribed text+image representation or raw images (PDF/screenshots). Prompt: request for step-by-step solution and final 'Answer: ...' output. Evaluation via string matching.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Asked to generate stepwise reasoning (chain-of-thought style) per the system prompt; no dedicated symbolic spatial solver.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Normal split accuracy 1.0%; Hard split accuracy 0.0%. Raw image Normal accuracy 0.3%. Meta-puzzle solve rate 2.0% (Table C.1).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No firm evidence of structured spatial reasoning; low and inconsistent performance across formats suggests limited capability on layout-dependent puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Underperforms O1 and is comparable to other frontier models in Table 2; slightly better meta-puzzle rate than some models but still far from human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failing entirely on hard split; raw image performance is often lower, suggesting challenges with OCR/visual parsing or with downstream spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8551.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8551.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ENIGMAEVAL-CLAUDE-3-OPUS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude 3 Opus (claude-3-opus-20240229) evaluated on ENIGMAEVAL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic Claude 3 Opus checkpoint evaluated on ENIGMAEVAL's text+image and raw image puzzle formats using standardized prompts requiring stepwise solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLAUDE 3 OPUS (claude-3-opus-20240229)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An Anthropic multimodal model checkpoint listed in Appendix B.1 used as part of the ENIGMAEVAL evaluation suite.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>ENIGMAEVAL (mixed multimodal puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Puzzle-hunt style multimodal problems with grids, diagrams and layout-sensitive visual clues.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Human-transcribed text+image representation and raw image inputs; models prompted for a step-by-step solution followed by an explicit final answer; answers compared by string match.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Requested to output chain-of-thought style solutions via the system prompt; no separate spatial reasoning engine.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Normal split accuracy 1.0%; Hard split accuracy 0.0%. Raw image Normal accuracy 0.3%. Meta-puzzle solve rate 0.3% (Table C.1).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Paper provides no positive evidence that Claude 3 Opus performed meaningful spatial reasoning on these puzzles; low solve rates indicate failure on layout-dependent tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Per Table 2, performance is near many other frontier models and well below human performance on puzzle hunts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Zero accuracy on hard puzzles; struggles on transcribed and raw inputs similarly, indicating inability to synthesize multi-step and spatial clues required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8551.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8551.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ENIGMAEVAL-GPT-40</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-40 / gpt-4o-2024-11-20 evaluated on ENIGMAEVAL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI model (listed as GPT-40 / gpt-4o) included in ENIGMAEVAL evaluation; prompted to produce stepwise reasoning and final answers for transcribed and raw image puzzle inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-40 (gpt-4o-2024-11-20)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A frontier OpenAI multimodal checkpoint listed in Appendix B.1; evaluated using ENIGMAEVAL's standardized prompts and scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>ENIGMAEVAL (mixed puzzle-hunt problems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Multimodal puzzle-hunt problems including grid-based and spatial-layout puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Standardized system prompt requesting step-by-step solution and final 'Answer:' line; both transcribed text+image and raw image inputs were evaluated; string matching for scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Models were asked to provide chain-of-thought style steps; no explicit specialized spatial solving technique was used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Normal split accuracy 1.0% (text+image); Raw image Normal accuracy 1.1%. Hard split accuracy 0.0%. Meta-puzzle solve rate 0.6% (Table C.1 under the name GPT-4O where listed).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No evidence of robust spatial reasoning; low solve rates and lack of ablation/probing mean no claim that the model performed layout-aware reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared in Table 2 to other frontier models; similar low performance to many other evaluated checkpoints and much worse than human solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>0% on hard puzzles; makes few correct solutions overall; possible sensitivity to visual parsing but no strong evidence it can reliably solve layout-dependent puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8551.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8551.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ENIGMAEVAL-GEMINI-2-PRO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Gemini 2.0 PRO (gemini-2.0-pro-exp-02-05) evaluated on ENIGMAEVAL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google Gemini 2.0 Pro checkpoint evaluated on ENIGMAEVAL using the benchmark's prompts requiring stepwise solutions; success rates were very low across puzzle types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GEMINI 2.0 PRO (gemini-2.0-pro-exp-02-05)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A Google multimodal/vision-language model checkpoint listed in Appendix B.1; evaluated on ENIGMAEVAL's mixed-format puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>ENIGMAEVAL (multimodal puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Puzzle-hunt style problems including grid- and layout-based puzzles requiring spatial interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Inputs: human-transcribed text+image format and raw images. Prompt required step-by-step reasoning and an 'Answer:' line; evaluation done by string matching.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompt-driven chain-of-thought style reasoning; no external symbolic spatial solver employed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Normal split accuracy 0.9%; Hard split accuracy 0.0%. Raw image Normal accuracy 0.5%. Meta-puzzle solve rate 0.0% (Table C.1 lists GEMINI 2.0 PRO as 0.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No explicit evidence of successful spatial reasoning mechanisms; near-zero meta-puzzle performance suggests difficulty attending to provided component answers as well.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Ranks below O1 and many others in Table 2; underperforms on both transcribed and raw formats relative to the best model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Complete failure on hard split; often unable to use multimodal clues to complete multi-step spatial reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8551.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8551.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ENIGMAEVAL-GEMINI-2-FLASH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Gemini 2.0 FLASH (gemini-2.0-flash-001) evaluated on ENIGMAEVAL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google Gemini 2.0 Flash checkpoint evaluated on ENIGMAEVAL; produced stepwise solutions under standardized prompting with very low solve rates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GEMINI 2.0 FLASH (gemini-2.0-flash-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A Google multimodal model checkpoint listed in Appendix B.1 and evaluated on the ENIGMAEVAL benchmark with the provided prompts and answer extraction format.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>ENIGMAEVAL (mixed multimodal puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Puzzle-hunt style multimodal problems including grid-based layouts, images and diagrams requiring spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Standardized prompt requiring chain-of-thought style step-by-step solutions and a final 'Answer:' line; evaluated on transcribed and raw image inputs, scored by exact string match.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Prompted step-by-step textual reasoning; no explicit spatial reasoning modules introduced.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Normal split accuracy 0.8%; Hard split accuracy 0.0%. Raw image Normal accuracy 0.3%. Meta-puzzle solve rate 1.3% (Table C.1 lists GEMINI 2.0 FLASH at 1.3% in some columns).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Paper reports no strong evidence that this model performs consistent spatial reasoning; low accuracy suggests limited capability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Lower performance compared to O1 and several others; similar low absolute performance to many frontier models in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>0% on hard split; difficulties with multi-step spatial and layout-based reasoning persist.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8551.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8551.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ENIGMAEVAL-LLAMA-3.2-90B-VISION</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama 3.2 90B Vision Instruct (llama-3.2-90b-vision-instruct) evaluated on ENIGMAEVAL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta's Llama 3.2 90B vision-instruct model evaluated on ENIGMAEVAL; included in the suite of frontier vision-language models and subjected to the benchmark's standardized prompts and scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLAMA 3.2 90B VISION (llama-3.2-90b-vision-instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal vision-enabled variant of Meta's Llama 3.2 at 90B parameters (size provided in Appendix B.1) evaluated on ENIGMAEVAL using the standard prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>90B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>ENIGMAEVAL (mixed puzzle-hunt problems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Multimodal puzzle-hunt problems including grid puzzles and layout-sensitive images requiring spatial interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Human-transcribed text+image inputs and raw PDF/page images; system prompt required step-by-step reasoning and final answer; evaluation via string matching at default temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Chain-of-thought style step-by-step reasoning induced via prompt; no additional spatial or symbolic solvers used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Normal split accuracy 0.5%; Hard split accuracy 0.0%. Raw image Normal accuracy 0.1%. Meta-puzzle solve rate 0.0% (Table C.1).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>No positive evidence that the 90B vision model did reliable spatial reasoning on these tasks; near-zero success indicates failure on layout- and spatially-dependent steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Lowest-performing model among those listed in Table 2 in some settings; substantially worse than O1 and others on the normal split and equal (0%) on the hard split.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles with both parsing raw inputs and reasoning across multi-step, layout-dependent clues; 0% on hard puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PUZZLES: A Benchmark for Neural Algorithmic Reasoning <em>(Rating: 2)</em></li>
                <li>PuzzlePlex: A Benchmark to Evaluate the Reasoning and Planning of Large Language Models on Puzzles <em>(Rating: 2)</em></li>
                <li>PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems? <em>(Rating: 2)</em></li>
                <li>Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter? <em>(Rating: 2)</em></li>
                <li>Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8551",
    "paper_id": "paper-8e8e761be9087be8499f9ac641e35727344fd556",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "ENIGMAEVAL-O1",
            "name_full": "OpenAI O1 (checkpoint o1-2024-12-17) evaluated on ENIGMAEVAL",
            "brief_description": "OpenAI vision-language checkpoint (o1-2024-12-17) evaluated as part of the ENIGMAEVAL benchmark of multimodal puzzle-hunt problems; prompted to produce step-by-step solutions and a final answer in a standardized format.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "O1 (o1-2024-12-17)",
            "model_description": "A frontier vision-language model checkpoint from OpenAI listed in Appendix B.1 and evaluated on multimodal puzzles in ENIGMAEVAL; prompted with a system template asking for step-by-step solutions and a final Answer line.",
            "model_size": null,
            "puzzle_name": "ENIGMAEVAL (mixed puzzle-hunt problems)",
            "puzzle_type": "Multimodal puzzle-hunt problems (includes grid-based puzzles, crosswords, diagrams, images and layout-dependent puzzles requiring spatial/layout reasoning).",
            "task_setup": "Models received either human-transcribed text+image representations or raw PDF/page images. A standardized system prompt required a step-by-step solution and a final 'Answer: &lt;answer&gt;' line; outputs were string-matched to ground truth at default temperature.",
            "mechanisms_or_strategies": "Prompting to generate chain-of-thought style step-by-step solutions via the provided system + user prompt template; no external symbolic solver or explicit spatial module was used.",
            "performance_metrics": "Normal split accuracy 7.0%; Hard split accuracy 0.0%. Raw image (PDF) Normal accuracy 6.1%. Meta-puzzle solve rate when provided component answers: 8.7% (Table C.1).",
            "evidence_of_spatial_reasoning": "No concrete evidence that O1 uses explicit spatial reasoning strategies; successes were rare and no ablation or probing demonstrated constructive spatial reasoning. The paper highlights preprocessing/OCR as a bottleneck for some inputs rather than showing robust spatial manipulation capabilities.",
            "comparisons": "Compared directly to other frontier vision-language models in Table 2 and meta-puzzle rates in Table C.1; O1 was the best-performing model on the normal split but still far below human puzzle-hunters. Performance sometimes dropped between transcribed and raw image formats, indicating preprocessing differences across models.",
            "limitations_or_failure_cases": "Failed completely on hard split (0%). Performance sensitive to raw vs transcribed input (possible OCR/parsing failures). Auditing of correct answers flagged hallucinated plagiarism (false positives) when checking for copying from official solutions. Overall low pass rate suggests inability to perform the multi-step, layout-sensitive reasoning required by many puzzles.",
            "uuid": "e8551.0",
            "source_info": {
                "paper_title": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ENIGMAEVAL-GEMINI-FLASH-THINKING",
            "name_full": "Google Gemini 2.0 FLASH THINKING (gemini-2.0-flash-thinking-exp-01-21) evaluated on ENIGMAEVAL",
            "brief_description": "A Google multimodal checkpoint (Gemini 2.0 Flash Thinking) evaluated on ENIGMAEVAL; prompted to produce stepwise reasoning and answers, measured on transcribed and raw image inputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GEMINI 2.0 FLASH THINKING (gemini-2.0-flash-thinking-exp-01-21)",
            "model_description": "A Google multimodal/vision-language model checkpoint listed in Appendix B.1, evaluated with the ENIGMAEVAL system prompt that requests step-by-step solutions.",
            "model_size": null,
            "puzzle_name": "ENIGMAEVAL (mixed puzzle-hunt problems)",
            "puzzle_type": "Multimodal puzzle-hunt problems (including grid-based puzzles and other layout/visual puzzles requiring spatial interpretation).",
            "task_setup": "Same standardized prompt pipeline as other models: either transcribed text+image inputs or raw PDF/page images; step-by-step solution required; final answer string-matched to ground truth.",
            "mechanisms_or_strategies": "Asked to provide step-by-step solutions (chain-of-thought style) via the system prompt; no additional spatial-specific modules or symbolic reasoning tools were used.",
            "performance_metrics": "Normal split accuracy 1.4%; Hard split accuracy 0.0%. Raw image Normal accuracy 1.3%. Meta-puzzle solve rate 0.6% (Table C.1).",
            "evidence_of_spatial_reasoning": "No explicit evidence of spatial reasoning; failure rates and lack of successful structured intermediate steps indicate limited spatial problem solving on these puzzle-hunt tasks.",
            "comparisons": "Compared to other frontier models (Table 2); substantially worse than O1 on ENIGMAEVAL. Performance difference across raw vs transcribed inputs suggests OCR/visual parsing impacts.",
            "limitations_or_failure_cases": "Zero accuracy on hard subset; low success on normal subset. Performance appears constrained by ability to synthesize multimodal layout information and perform multi-step deduction required by puzzle hunts.",
            "uuid": "e8551.1",
            "source_info": {
                "paper_title": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ENIGMAEVAL-CLAUDE-3.5-SONNET",
            "name_full": "Anthropic Claude 3.5 Sonnet (claude-3-5-sonnet-20241022) evaluated on ENIGMAEVAL",
            "brief_description": "Anthropic Claude 3.5 Sonnet checkpoint evaluated on the ENIGMAEVAL dataset using a prompt requiring stepwise solutions and a final answer; performance measured on text+image and raw image inputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CLAUDE 3.5 SONNET (claude-3-5-sonnet-20241022)",
            "model_description": "An Anthropic multimodal/vision-language checkpoint listed in Appendix B.1, evaluated with the benchmark's standardized prompt templates.",
            "model_size": null,
            "puzzle_name": "ENIGMAEVAL (mixed puzzle-hunt problems)",
            "puzzle_type": "Multimodal puzzle-hunt problems with grid elements, diagrams and images where spatial/layout interpretation contributes to solution finding.",
            "task_setup": "Models given either human-transcribed text+image format or raw PDF/page images; required to output step-by-step reasoning followed by an explicit 'Answer: ...' line; evaluation via exact string matching.",
            "mechanisms_or_strategies": "Produced stepwise textual reasoning as prompted; no puzzle-specific spatial modules were invoked.",
            "performance_metrics": "Normal split accuracy 1.1%; Hard split accuracy 0.0%. Raw image Normal accuracy 1.0%. Meta-puzzle solve rate 1.0% (Table C.1).",
            "evidence_of_spatial_reasoning": "The paper reports no diagnostic evidence of successful spatial reasoning by this model; low solve rates imply limited ability to combine visual layout and multi-step deduction.",
            "comparisons": "Ranked low compared to O1 and similar to other models like Pixtral and GPT-40 in Table 2. Performance on transcribed vs raw inputs similar for this model.",
            "limitations_or_failure_cases": "0% accuracy on hard split. Limited success indicates failures on puzzles requiring long, cross-modal reasoning and spatial/layout-aware deductions.",
            "uuid": "e8551.2",
            "source_info": {
                "paper_title": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ENIGMAEVAL-PIXTRAL-LARGE",
            "name_full": "Mistral Pixtral Large Instruct (pixtral-large-instruct-2411) evaluated on ENIGMAEVAL",
            "brief_description": "Mistral's Pixtral Large Instruct vision-enabled checkpoint evaluated on ENIGMAEVAL; produced stepwise solutions under a standardized prompt and measured for accuracy on transcribed and raw image puzzles.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PIXTRAL LARGE (pixtral-large-instruct-2411)",
            "model_description": "A multimodal/vision-enabled checkpoint from Mistral listed in Appendix B.1, evaluated with ENIGMAEVAL's prompting and scoring setup.",
            "model_size": null,
            "puzzle_name": "ENIGMAEVAL (mixed puzzle-hunt problems)",
            "puzzle_type": "Multimodal puzzles including grid-based and diagrammatic puzzles that require interpreting spatial/layout information.",
            "task_setup": "Input: either human-transcribed text+image representation or raw images (PDF/screenshots). Prompt: request for step-by-step solution and final 'Answer: ...' output. Evaluation via string matching.",
            "mechanisms_or_strategies": "Asked to generate stepwise reasoning (chain-of-thought style) per the system prompt; no dedicated symbolic spatial solver.",
            "performance_metrics": "Normal split accuracy 1.0%; Hard split accuracy 0.0%. Raw image Normal accuracy 0.3%. Meta-puzzle solve rate 2.0% (Table C.1).",
            "evidence_of_spatial_reasoning": "No firm evidence of structured spatial reasoning; low and inconsistent performance across formats suggests limited capability on layout-dependent puzzles.",
            "comparisons": "Underperforms O1 and is comparable to other frontier models in Table 2; slightly better meta-puzzle rate than some models but still far from human performance.",
            "limitations_or_failure_cases": "Failing entirely on hard split; raw image performance is often lower, suggesting challenges with OCR/visual parsing or with downstream spatial reasoning.",
            "uuid": "e8551.3",
            "source_info": {
                "paper_title": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ENIGMAEVAL-CLAUDE-3-OPUS",
            "name_full": "Anthropic Claude 3 Opus (claude-3-opus-20240229) evaluated on ENIGMAEVAL",
            "brief_description": "Anthropic Claude 3 Opus checkpoint evaluated on ENIGMAEVAL's text+image and raw image puzzle formats using standardized prompts requiring stepwise solutions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CLAUDE 3 OPUS (claude-3-opus-20240229)",
            "model_description": "An Anthropic multimodal model checkpoint listed in Appendix B.1 used as part of the ENIGMAEVAL evaluation suite.",
            "model_size": null,
            "puzzle_name": "ENIGMAEVAL (mixed multimodal puzzles)",
            "puzzle_type": "Puzzle-hunt style multimodal problems with grids, diagrams and layout-sensitive visual clues.",
            "task_setup": "Human-transcribed text+image representation and raw image inputs; models prompted for a step-by-step solution followed by an explicit final answer; answers compared by string match.",
            "mechanisms_or_strategies": "Requested to output chain-of-thought style solutions via the system prompt; no separate spatial reasoning engine.",
            "performance_metrics": "Normal split accuracy 1.0%; Hard split accuracy 0.0%. Raw image Normal accuracy 0.3%. Meta-puzzle solve rate 0.3% (Table C.1).",
            "evidence_of_spatial_reasoning": "Paper provides no positive evidence that Claude 3 Opus performed meaningful spatial reasoning on these puzzles; low solve rates indicate failure on layout-dependent tasks.",
            "comparisons": "Per Table 2, performance is near many other frontier models and well below human performance on puzzle hunts.",
            "limitations_or_failure_cases": "Zero accuracy on hard puzzles; struggles on transcribed and raw inputs similarly, indicating inability to synthesize multi-step and spatial clues required.",
            "uuid": "e8551.4",
            "source_info": {
                "paper_title": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ENIGMAEVAL-GPT-40",
            "name_full": "OpenAI GPT-40 / gpt-4o-2024-11-20 evaluated on ENIGMAEVAL",
            "brief_description": "OpenAI model (listed as GPT-40 / gpt-4o) included in ENIGMAEVAL evaluation; prompted to produce stepwise reasoning and final answers for transcribed and raw image puzzle inputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-40 (gpt-4o-2024-11-20)",
            "model_description": "A frontier OpenAI multimodal checkpoint listed in Appendix B.1; evaluated using ENIGMAEVAL's standardized prompts and scoring.",
            "model_size": null,
            "puzzle_name": "ENIGMAEVAL (mixed puzzle-hunt problems)",
            "puzzle_type": "Multimodal puzzle-hunt problems including grid-based and spatial-layout puzzles.",
            "task_setup": "Standardized system prompt requesting step-by-step solution and final 'Answer:' line; both transcribed text+image and raw image inputs were evaluated; string matching for scoring.",
            "mechanisms_or_strategies": "Models were asked to provide chain-of-thought style steps; no explicit specialized spatial solving technique was used in experiments.",
            "performance_metrics": "Normal split accuracy 1.0% (text+image); Raw image Normal accuracy 1.1%. Hard split accuracy 0.0%. Meta-puzzle solve rate 0.6% (Table C.1 under the name GPT-4O where listed).",
            "evidence_of_spatial_reasoning": "No evidence of robust spatial reasoning; low solve rates and lack of ablation/probing mean no claim that the model performed layout-aware reasoning.",
            "comparisons": "Compared in Table 2 to other frontier models; similar low performance to many other evaluated checkpoints and much worse than human solvers.",
            "limitations_or_failure_cases": "0% on hard puzzles; makes few correct solutions overall; possible sensitivity to visual parsing but no strong evidence it can reliably solve layout-dependent puzzles.",
            "uuid": "e8551.5",
            "source_info": {
                "paper_title": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ENIGMAEVAL-GEMINI-2-PRO",
            "name_full": "Google Gemini 2.0 PRO (gemini-2.0-pro-exp-02-05) evaluated on ENIGMAEVAL",
            "brief_description": "Google Gemini 2.0 Pro checkpoint evaluated on ENIGMAEVAL using the benchmark's prompts requiring stepwise solutions; success rates were very low across puzzle types.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GEMINI 2.0 PRO (gemini-2.0-pro-exp-02-05)",
            "model_description": "A Google multimodal/vision-language model checkpoint listed in Appendix B.1; evaluated on ENIGMAEVAL's mixed-format puzzles.",
            "model_size": null,
            "puzzle_name": "ENIGMAEVAL (multimodal puzzles)",
            "puzzle_type": "Puzzle-hunt style problems including grid- and layout-based puzzles requiring spatial interpretation.",
            "task_setup": "Inputs: human-transcribed text+image format and raw images. Prompt required step-by-step reasoning and an 'Answer:' line; evaluation done by string matching.",
            "mechanisms_or_strategies": "Prompt-driven chain-of-thought style reasoning; no external symbolic spatial solver employed.",
            "performance_metrics": "Normal split accuracy 0.9%; Hard split accuracy 0.0%. Raw image Normal accuracy 0.5%. Meta-puzzle solve rate 0.0% (Table C.1 lists GEMINI 2.0 PRO as 0.0%).",
            "evidence_of_spatial_reasoning": "No explicit evidence of successful spatial reasoning mechanisms; near-zero meta-puzzle performance suggests difficulty attending to provided component answers as well.",
            "comparisons": "Ranks below O1 and many others in Table 2; underperforms on both transcribed and raw formats relative to the best model.",
            "limitations_or_failure_cases": "Complete failure on hard split; often unable to use multimodal clues to complete multi-step spatial reasoning tasks.",
            "uuid": "e8551.6",
            "source_info": {
                "paper_title": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ENIGMAEVAL-GEMINI-2-FLASH",
            "name_full": "Google Gemini 2.0 FLASH (gemini-2.0-flash-001) evaluated on ENIGMAEVAL",
            "brief_description": "Google Gemini 2.0 Flash checkpoint evaluated on ENIGMAEVAL; produced stepwise solutions under standardized prompting with very low solve rates.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GEMINI 2.0 FLASH (gemini-2.0-flash-001)",
            "model_description": "A Google multimodal model checkpoint listed in Appendix B.1 and evaluated on the ENIGMAEVAL benchmark with the provided prompts and answer extraction format.",
            "model_size": null,
            "puzzle_name": "ENIGMAEVAL (mixed multimodal puzzles)",
            "puzzle_type": "Puzzle-hunt style multimodal problems including grid-based layouts, images and diagrams requiring spatial reasoning.",
            "task_setup": "Standardized prompt requiring chain-of-thought style step-by-step solutions and a final 'Answer:' line; evaluated on transcribed and raw image inputs, scored by exact string match.",
            "mechanisms_or_strategies": "Prompted step-by-step textual reasoning; no explicit spatial reasoning modules introduced.",
            "performance_metrics": "Normal split accuracy 0.8%; Hard split accuracy 0.0%. Raw image Normal accuracy 0.3%. Meta-puzzle solve rate 1.3% (Table C.1 lists GEMINI 2.0 FLASH at 1.3% in some columns).",
            "evidence_of_spatial_reasoning": "Paper reports no strong evidence that this model performs consistent spatial reasoning; low accuracy suggests limited capability.",
            "comparisons": "Lower performance compared to O1 and several others; similar low absolute performance to many frontier models in Table 2.",
            "limitations_or_failure_cases": "0% on hard split; difficulties with multi-step spatial and layout-based reasoning persist.",
            "uuid": "e8551.7",
            "source_info": {
                "paper_title": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ENIGMAEVAL-LLAMA-3.2-90B-VISION",
            "name_full": "Meta Llama 3.2 90B Vision Instruct (llama-3.2-90b-vision-instruct) evaluated on ENIGMAEVAL",
            "brief_description": "Meta's Llama 3.2 90B vision-instruct model evaluated on ENIGMAEVAL; included in the suite of frontier vision-language models and subjected to the benchmark's standardized prompts and scoring.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLAMA 3.2 90B VISION (llama-3.2-90b-vision-instruct)",
            "model_description": "A multimodal vision-enabled variant of Meta's Llama 3.2 at 90B parameters (size provided in Appendix B.1) evaluated on ENIGMAEVAL using the standard prompt format.",
            "model_size": "90B",
            "puzzle_name": "ENIGMAEVAL (mixed puzzle-hunt problems)",
            "puzzle_type": "Multimodal puzzle-hunt problems including grid puzzles and layout-sensitive images requiring spatial interpretation.",
            "task_setup": "Human-transcribed text+image inputs and raw PDF/page images; system prompt required step-by-step reasoning and final answer; evaluation via string matching at default temperature.",
            "mechanisms_or_strategies": "Chain-of-thought style step-by-step reasoning induced via prompt; no additional spatial or symbolic solvers used.",
            "performance_metrics": "Normal split accuracy 0.5%; Hard split accuracy 0.0%. Raw image Normal accuracy 0.1%. Meta-puzzle solve rate 0.0% (Table C.1).",
            "evidence_of_spatial_reasoning": "No positive evidence that the 90B vision model did reliable spatial reasoning on these tasks; near-zero success indicates failure on layout- and spatially-dependent steps.",
            "comparisons": "Lowest-performing model among those listed in Table 2 in some settings; substantially worse than O1 and others on the normal split and equal (0%) on the hard split.",
            "limitations_or_failure_cases": "Struggles with both parsing raw inputs and reasoning across multi-step, layout-dependent clues; 0% on hard puzzles.",
            "uuid": "e8551.8",
            "source_info": {
                "paper_title": "EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PUZZLES: A Benchmark for Neural Algorithmic Reasoning",
            "rating": 2
        },
        {
            "paper_title": "PuzzlePlex: A Benchmark to Evaluate the Reasoning and Planning of Large Language Models on Puzzles",
            "rating": 2
        },
        {
            "paper_title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
            "rating": 2
        },
        {
            "paper_title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
            "rating": 2
        },
        {
            "paper_title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?",
            "rating": 1
        }
    ],
    "cost": 0.01674125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ENIGMAEVAL:</h1>
<h2>A Benchmark of Long Multimodal Reasoning Challenges</h2>
<p>Clinton J. Wang ${ }^{1}$, Dean Lee ${ }^{1}$, Cristina Menghini ${ }^{1}$, Johannes Mols ${ }^{1}$, Jack Doughty ${ }^{1}$, Adam Khoja ${ }^{2}$, Jayson Lynch ${ }^{3}$, Sean Hendryx ${ }^{1}$, Summer Yue ${ }^{1}$, Dan Hendrycks ${ }^{2}$<br>${ }^{1}$ Scale AI, ${ }^{2}$ Center for AI Safety, ${ }^{3}$ MIT<br> {clinton.wang, summer.yue}@scale.com, dan@safe.ai (D) https://scale.com/leaderboard/enigma_eval</p>
<h4>Abstract</h4>
<p>As language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities, making them a unique testbed for evaluating frontier language models. We introduce ENIGMAEVAL, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution paths. The benchmark comprises 1184 puzzles of varying complexity - each typically requiring teams of skilled solvers hours to days to complete - with unambiguous, verifiable solutions that enable efficient evaluation. State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning.</p>
<h2>1. Introduction</h2>
<p>Recent advances in Large Language Models (LLMs), evident in their saturation of existing benchmarks, call for a shift in how we evaluate their capabilities. We need challenging benchmarks that expose current limitations and probe unexplored abilities at the frontiers of LLMs reasoning. Puzzle-solving events represent a promising direction: they are unique expressions of human ingenuity that combine diverse knowledge domains with creative reasoning. These puzzles demand intricate chains of deductive reasoning, cleverly interweaving logic, wordplay, mathematics, coding, and cultural references. Notably, they come without explicit instructions, forcing solvers to explore multiple creative approaches at each step - a particularly challenging aspect for state-of-the-art LLMs that typically excel in well-structured tasks with clear objectives.
Despite this rich potential of puzzle-solving as an evaluation framework, current puzzle-based benchmarks remain narrow in scope. Most focus on specific specific puzzle types with consistent formats (e.g., sudokus or crosswords) or restrict themselves to text-only challenges, falling short of assessing the advanced reasoning capabilities that modern models are beginning to demonstrate. While challenging reasoning benchmarks exist - MMLU, GPQA and Olympiads test multi-step problem-solving through domain-specific challenges - they still operate within well-defined problem spaces, and are rapidly being saturated by frontier models (Figure 1). This reflects a broader evaluation gap in assessing models' capacity for creative reasoning on unstructured multimodal challenges. With models now demonstrating capabilities in multimodal understanding and long-context processing, the time is ripe to evaluate how these abilities come together in complex problem-solving scenarios.
We introduce ENIGMAEVAL, a benchmark of highly difficult problems with diverse unstructured formats spanning text and images drawn from puzzle hunts - a rich, untapped repository of puzzles created and shared by a vibrant global puzzle-solving community. We release both the original multimodal problems and high-quality human transcriptions, allowing us to evaluate models' end-to-end capabilities as well as their reasoning abilities</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: While many existing reasoning and/or multimodal benchmarks are largely solved by frontier models, all models struggle on ENIGMAEVAL, demonstrating the benchmark's effectiveness for measuring continuing progress on multimodal reasoning. More frontier model results are shown in Table 2.</p>
<p>in isolation. This design choice enables us to distinguish between performance limitations we observe stem from models' reasoning capabilities rather than their ability to parse complex documents or process different modalities.</p>
<p>We evaluate frontier language models that have demonstrated strong performance on existing multimodal and reasoning benchmarks. Our evaluation shows that state-of-the-art models achieve only 7.0% accuracy on normal ENIGMAEVAL puzzles, dropping to 0% on hard problems, falling far short of experienced human puzzle hunters' capabilities (Table 2). Interestingly, model performance could drop dramatically from the transcribed puzzles to their original PDF versions, suggesting that some frontier models are still constrained by OCR and parsing ability. While we initially hypothesized that raw puzzle formats might pose additional difficulties, detailed analysis of cases where models succeeded suggests that they have been well-optimized for processing complex documents, making the transcribed version equally challenging.</p>
<p>ENIGMAEVAL joins Humanity's Last Exam (HLE) in establishing a new class of LLM benchmarks characterized by extremely challenging tasks that expose current models' limitations. While HLE probes structured academic knowledge, ENIGMAEVAL tests the integration of reasoning capabilities in creative problem-solving scenarios. We hope these complementary benchmarks represent just the beginning of a broader shift toward evaluations that reveal the true boundaries of models' capabilities, particularly in complex tasks requiring flexible thinking and knowledge synthesis. Although puzzle solutions are hosted online, their current scattered and unstructured format makes direct memorization unlikely. However, our structured dataset release could enable solution memorization. To prevent such data leakage, as well as to respect some authors' wishes not to distribute their puzzles widely, we keep the benchmark private and will continuously update it with the leading frontier models. To request that your model be evaluated on ENIGMAEVAL, please fill out this form.</p>
<h2>2. ENIGMAEVAL</h2>
<p>ENIGMAEVAL comprises 1184 puzzles, represented in two formats: (1) PNGs of the original source PDFs (or for webpage puzzles, an automated full-page screenshot), testing end-to-end performance, and (2) a structured text-image representation that preserves semantic relationships and visual elements, for targeted evaluation of multimodal reasoning with fewer distractors and reduced preprocessing load. The solution to each puzzle is typically a word or short phrase.</p>
<h3>2.1 Data Collection</h3>
<p>Our benchmark draws from eight diverse puzzle events archived online (Table 1), ranging from PuzzledPint's beginner-friendly puzzles to advanced competitions for experienced solvers. This variety enables benchmark</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor Day Extravaganza (link, Mark Halpin, all rights reserved), (c) MIT Mystery Hunt (link, Jeck Lim, all rights reserved), and (d) CRUMS (link, CC BY-NC 3.0).</p>
<p>Table 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty split. $N$ is the number of puzzles.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Split ( $N$ )</th>
<th style="text-align: center;">Puzzle Source ( $N$ )</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Normal } \ &amp; \text { (949) } \end{aligned}$</td>
<td style="text-align: center;">PuzzledPint (838) [1]</td>
<td style="text-align: center;">Monthly beginner-friendly puzzle-solving event typically consisting of around seven puzzles including a meta-puzzle. Together, these puzzles should "be solvable in under two hours by a team of inexperienced puzzlers who are socializing, drinking, and eating in a pub or restaurant"1.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CS50x Puzzle Day (41) [2]</td>
<td style="text-align: center;">Annual puzzle set designed for small teams of beginner problem-solvers.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Puzzle Potluck (34) [3]</td>
<td style="text-align: center;">Public puzzle hunt hosted four times, designed to be accessible while more competitive than PuzzledPint.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cryptic Crosswords (30) [4]</td>
<td style="text-align: center;">Collection of crosswords by Mark Halpin featuring unique mechanics and non-standard layouts. Final answers require combining filled grid elements.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CRUMS (6) [5]</td>
<td style="text-align: center;">A short puzzle hunt hosted in 2020.</td>
</tr>
<tr>
<td style="text-align: center;">Hard (235)</td>
<td style="text-align: center;">MIT Mystery Hunt (72) [6]</td>
<td style="text-align: center;">Massive annual event at MIT with hundreds of puzzles. Our benchmark includes selected puzzles from various hunts.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Labor Day Extravaganza (153) [4]</td>
<td style="text-align: center;">Annual online hunt by Mark Halpin with multiple puzzles and a metapuzzle, designed for experienced solvers over several days.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Grandmaster Puzzles (10) [7]</td>
<td style="text-align: center;">A small collection of puzzle hunt-style puzzles published in a blog.</td>
</tr>
</tbody>
</table>
<p>stratification into normal and hard subsets. Hard puzzles typically require five or more non-trivial steps with minimal verification, where intermediate answers might only be thematically hinted by flavor text. All sources combine textual and visual elements, including grids, pictures, diagrams, and their meaningful arrangements.</p>
<p>Puzzle sourcing We scraped puzzles from online archives in their original format, either as PDFs (from CRUMS, Cryptic Crossword, PuzzledPint, Labor Day Extravaganza, CS50x, and Grandmaster Puzzles) or HTML webpages (from MIT Mystery Hunt and Puzzle Potluck). During the collection process, we applied the following filters:</p>
<ul>
<li>Advanced Meta-Puzzles. A meta-puzzle is a puzzle that combines answers or elements from multiple previous puzzles to reveal a final solution. Since these may rely on hidden clues across puzzles, we excluded them unless the answer and title of previous puzzles were sufficient to solve them independently. After filtering, our dataset contains 77 meta-puzzles, all in the normal split.</li>
<li>Audio/Video and Interactive Elements. At the time of writing, only a few frontier models can process audio and video clues effectively. As a result, we excluded puzzles that rely on these modalities, as well as those requiring interaction with a web application.</li>
<li>Licensing and Permissions. We restricted our benchmark to puzzles for which we obtained explicit consent from the authors or that were released under Creative Commons (CC) licenses.</li>
</ul>
<p>Along with the puzzles, we collected their corresponding solution documents, also available as PDFs or HTML pages.</p>
<p>Human annotation Human annotators transcribed each puzzle into a standardized text-image format. This created two evaluation paths: one using the original untranscribed puzzles (PDFs or automated webpage screenshots) and another using our standardized format. This dual approach allows us to separate a model's reasoning capabilities from its ability to parse complex documents - addressing the challenge that models may fail due to OCR or parsing issues rather than lack of problem-solving ability. Automation of this transcription process with LLMs proved impractical due to the complexity of parsing diverse puzzle formats, necessitating human</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>intervention. The standardization addressed several key challenges: (a) removing source-identifying headers and footers to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where structural elements like lines and dots are separate from textual content), and (c) ensuring accurate text extraction from non-standard formatting. A similar curation process was applied to puzzle solutions: after initial automated extraction from PDF and HTML pages, each solution underwent manual validation and was tagged with its answer type (i.e., word or short phrase, pair of words/phrases, or comma-separated list of words/phrases). This human curation was essential for maintaining semantic relationships and creating consistent representations across different source formats. To ensure annotation quality, transcriptions were subject to a rigorous review process by separate human reviewers who verified the accuracy and completeness of both puzzle and solution transcriptions. Full annotation instructions are detailed in Appendix A. $2^{2}$.</p>
<h1>3. Experiments</h1>
<p>We test state-of-the-art LLMs' deep reasoning capabilities on our benchmark. Section 3.1 describes our experimental setup, followed by quantitative results and performance analysis in Section 3.2.</p>
<h3>3.1 Evaluation Setup</h3>
<p>We run the evaluation on a range of leading LLMs with multimodal capabilities. We evaluate models by comparing their answers to ground-truth solutions through string matching. The models generate responses using formatspecific system prompt templates (Appendix B.2), that require both a step-by-step solution and a final answer in a standardized format, enabling consistent answer extraction (Appendix B.4).</p>
<p>Metrics Model performance is measured using accuracy at the default model temperature. For meta-puzzles, we evaluate accuracy by providing the model with correct component solutions, allowing us to isolate its metareasoning capabilities from its performance on individual puzzles (Appendix C).</p>
<h3>3.2 Results</h3>
<p>Models show very limited success in solving puzzles. All frontier vision-language models achieve notably low accuracy on this evaluation, with even the leading model ( O 1 ) only reaching $7.0 \%$ on the normal split and $0 \%$ on the hard split (Table 2). These results highlight the substantial gap between current multimodal LLMs and expert-level reasoning capabilities on complex long multimodal reasoning tasks. The low performance stems from the inherent complexity of the puzzle dataset - these are challenging problems that were not adversarially curated on models' weaknesses. The difficulties these models face emerge organically from puzzles requiring sophisticated reasoning, strategic thinking, and structured problem-solving approaches that current models have not yet mastered. The complete failure of all tested models on the hard split ( $0 \%$ accuracy) is particularly noteworthy, underscoring the significant challenges these models face when confronted with more complex puzzle variations.</p>
<p>The best frontier models are sometimes bottlenecked by preprocessing capabilities. The human-transcribed text-image versions of the puzzles described in Section 2 help isolate reasoning capabilities from visual parsing skills. Table 2 shows that on some models, providing raw images rather than the transcription results in similar performance, while for others it seems to drastically compromise performance, suggesting that these latter models have relatively poor OCR abilities.</p>
<p>No evidence of data leakage. The puzzles in this benchmark have publicly accessible step-by-step solutions online. Many frontier models may have seen these during pretraining. Models with web search capabilities could also solve the puzzle by searching the name of the puzzle. Still, the low pass rate across all benchmarks suggests that data contamination is not prominent in any of the frontier models, indicating their performance genuinely reflects their problem-solving capabilities, despite the possibility of puzzle solutions being present in their training data.
We used O1 to audit its own correct model-generated answers to check for plagiarism or nonsensical reasoning chains, when comparing the step-by-step solutions generated by the frontier model with the official puzzle</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Accuracy rates on puzzles for frontier models in text+image format (left) and raw image format (right). Model checkpoint details in Appendix B.1. All standard deviations $&lt;0.6 \%$ on 3 evaluation runs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Accuracy (\%)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Normal</td>
<td style="text-align: center;">Hard</td>
</tr>
<tr>
<td style="text-align: left;">O1</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">GEMINI 2.0 FLASH THINKING</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">CLAUDE 3.5 SONNET</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">PIXTRAL LARGE</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">CLAUDE 3 OPUS</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-40</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">GEMINI 2.0 PRO</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">GEMINI 2.0 FLASH</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA 3.2 90B VISION</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.0</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Normal Acc. on <br> Raw Format (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">O1</td>
<td style="text-align: center;">6.1</td>
</tr>
<tr>
<td style="text-align: left;">GEMINI 2.0 FLASH THINKING</td>
<td style="text-align: center;">1.3</td>
</tr>
<tr>
<td style="text-align: left;">GPT-40</td>
<td style="text-align: center;">1.1</td>
</tr>
<tr>
<td style="text-align: left;">CLAUDE 3.5 SONNET</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">GEMINI 2.0 PRO</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">GEMINI 2.0 FLASH</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: left;">PIXTRAL LARGE</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: left;">CLAUDE 3 OPUS</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA 3.2 90B VISION</td>
<td style="text-align: center;">0.1</td>
</tr>
</tbody>
</table>
<p>walkthroughs (Appendix B.3). Each case flagged by O1 was manually inspected, and was found to be a false positive (the auditing model hallucinated evidence for plagiarism), suggesting that the model independently arrived at the correct answer.</p>
<h1>4. Related Work</h1>
<p>Reasoning benchmarks Recent years have seen the development of increasingly sophisticated benchmarks to evaluate models' reasoning capabilities across different domains and modalities. MATH [8], GPQA [9], FrontierMath [10], and OlympiadBench [11] focus on advanced mathematical reasoning requiring both precise technical knowledge and creative problem-solving strategies. Humanity's Last Exam [12], and MMLU [13] evaluate domain expertise across academic and professional fields. MMMU [14], MathVista [15], and VISTA [16] evaluate general vision-language capabilities and reasoning across fields, while ARC-AGI [17] approaches intelligence evaluation through abstract pattern recognition tasks that assess core reasoning capabilities independent of domain knowledge. ENIGMAEVAL builds upon these foundations by requiring not just multi-step reasoning, visual-language understanding or pattern recognition, but the ability to synthesize disparate clues and discover hidden solution paths within seemingly unstructured multimodal information.</p>
<p>Puzzle solving benchmarks Several benchmarks have emerged to evaluate different aspects of puzzle-solving capabilities of LLMs. PUZZLES [18] contains 40 types of visual logic puzzles focused on assessing algorithmic and logical reasoning in reinforcement learning settings. PuzzlePlex [19] extends this with 24 diverse puzzles spanning deterministic and stochastic games that require strategic reasoning and opponent modeling. PuzzleBench [20] introduces 31 challenging first-order combinatorial reasoning problems, from graph coloring to cryptarithmetic, while GridPuzzle [21] provides 274 grid-based puzzles designed to evaluate step-by-step reasoning chains beyond simple answer correctness. RiddleSense [22] takes a different approach with testing linguistic creativity and commonsense knowledge. These benchmarks are valuable for evaluating specific types of reasoning, but they rely on consistent formats. ENIGMAEVAL challenges models with puzzles that vary dramatically in both format and modality, requiring flexible reasoning approaches and cross-domain knowledge synthesis.</p>
<p>Puzzle hunts The broader puzzle hunt ecosystem includes numerous high-quality collections: university competitions (MUMS [23], SUMS [24], Harvard Mystery Hunt[25]), corporate events (CISRA [26], Jane Street Puzzles [27], Googol Conglomerate [28]), and community-organized hunts (DASH [29], BAPHL [30], Galactic Puzzle Hunt [31]). While these events represent rich sources of creative and challenging puzzles, their decentralized authorship and varied licensing terms present significant challenges for dataset creation. To ensure both legal compliance and reproducible evaluation, ENIGMAEVAL focuses on puzzle collections with clear Creative Commons licenses and centralized organizational structures.</p>
<h1>Acknowledgments</h1>
<p>We would like to thank all the authors of the puzzles that made this benchmark possible. We are especially grateful to Mark Halpin for giving us permission to include his large collection of puzzles including Labor Day Extravaganzas, cryptic crosswords, and MIT Mystery Hunt puzzles. Thank you as well to Dave Shukan, Seth Bisen-Hersh, Brian Chen (betaveros), Evan Chen, Jeck Lim and Sami Casanova for giving us permission to include their puzzles from the MIT Mystery Hunt.
Thank you to Rajeev Nayak, Bradley Wu, Curtis Liu, Darren Yin, Julz Huang, Lindsey Shi, and Stephanie Chang for creating Puzzle Potluck and for giving us permission to include it in this benchmark.
Thank you to the many organizers, puzzle writers and game masters who make PuzzledPint possible and for generously making the puzzles available under a Creative Commons license. We are grateful to the many puzzle contributors: Bob Becker, Robert Becker, Stephanie Yang, Sara Goodchild, Andrea Blumberg, Tomer Reiter, Bill Gardner, Sam Webster, Eric Berlin, Jen Dumont, Jonah Ostroff, William Gardner, Cathy and Tom Saxton, Larry Hosken, Neal Tibrewala, Adam Levav, Aaron Kugler, Jonah Ostroff and Kell Pogue, Clare Crawford, David Palmer, and many others.
Thank you to David Malan, Meta and the CS50x staff for writing the CS50x puzzles and making them available under a Creative Commons license. Thank you to Thomas Snyder (drsudoku) for writing many of the Grandmaster Puzzles and making them available under a Creative Commons license. We are grateful to Zach Barnett, Alex Walker, and Sara Walker for creating CRUMS and making it available under a Creative Commons license.</p>
<h1>References</h1>
<p>[1] Puzzled Pint. https://puzzledpint.org/. CC BY-NC-SA Intl. 4.0.
[2] David J. Malan. CS50x Puzzle Day. https://cs50.harvard.edu/x/2025/puzzles/.
[3] Bradley Wu, Curtis Liu, Darren Yin, Julz Huang, Lindsey Shi, Rajeev Nayak, and Stephanie Chang. Puzzle Potluck. https://puzzlepotluck.com/.
[4] Mark Halpin. SOME PUZZLES by Mark Halpin. https://www.markhalpin.com/puzzles/puzzles.html.
[5] Zach Barnett, Alex Walker, and Sara Walker. CRUMS. https://crumspuzzlehunt.com/. CC BY-NC 3.0.
[6] MIT Mystery Hunt. https://puzzles.mit.edu/.
[7] Thomas Snyder. Grandmaster Puzzles. https://www.gmpuzzles.com/about/.
[8] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021.
[9] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level google-proof q\&amp;a benchmark. In First Conference on Language Modeling, 2024.
[10] Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli Jrviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, and Mark Wildon. Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai, 2024.
[11] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024.
[12] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Summer Yue, Alexandr Wang, and Dan Hendrycks. Humanity's Last Exam, 2025.
[13] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021.
[14] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024.
[15] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024.
[16] Cristina Menghini, Diego Mares, Ernesto Hernandez, Dean Lee, Mike Lunati, and Summer Yue. Vista: A rubricbased visual task assessment. https://scale.com/leaderboard/visual_language_understanding, 2024.
[17] Franois Chollet. On the measure of intelligence, 2019.
[18] Benjamin Estermann, Luca A. Lanzendrfer, Yannick Niedermayr, and Roger Wattenhofer. PUZZLES: A Benchmark for Neural Algorithmic Reasoning, 2024.
[19] Anonymous. PuzzlePlex: A Benchmark to Evaluate the Reasoning and Planning of Large Language Models on Puzzles, 2025.</p>
<p>[20] Chinmay Mittal, Krishna Kartik, Parag Singla, et al. PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems? arXiv preprint arXiv:2402.02611, 2024.
[21] Nemika Tyagi, Mihir Parmar, Mohith Kulkarni, Aswin Rrv, Nisarg Patel, Mutsumi Nakamura, Arindam Mitra, and Chitta Baral. Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter? arXiv preprint arXiv:2407.14790, 2024.
[22] Bill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee, and Xiang Ren. Riddlesense: Answering riddle questions as commonsense reasoning. CoRR, abs/2101.00376, 2021.
[23] Melburne Uni Maths and Stats Society. https://www.melbunimathsstats.org/puzzlehunt.
[24] Sydney University Mathematics Society. https://web.archive.org/web/20210725192741/https://www. maths.usyd.edu.au/ub/sums/puzzlehunt/2018/index.
[25] Harvard Mystery Hunt. https://harvardpuzzles.github.io/.
[26] CISRA. https://www.mezzacotta.net/puzzle/cisra/.
[27] Jane Street Puzzles. https://www.janestreet.com/puzzles/archive/index.html.
[28] Googol Conglomerate. https://gooooogol.theburninators.org/puzzles/.
[29] Different Area, Same Hunt. https://playdash.org/.
[30] Boston Area Puzzle Hunt League. https://www.baph1.org/.
[31] Galactic Puzzle Hunt. https://2024.galacticpuzzlehunt.com/.</p>
<h1>Appendix</h1>
<h2>A. Dataset Details</h2>
<h2>A. 1 Number of Images per Puzzle</h2>
<p>Text-only puzzles comprise a small minority of the dataset. While most puzzles have a single key visual component, there is a significant number of puzzles with many graphics.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure A.1: Image distribution across puzzles. Left shows number of images in human transcriptions of the puzzles. Right shows number of pages in the PDF version of the puzzle.</p>
<h2>A. 2 Annotator Instructions</h2>
<p>Annotators were presented with instructions based on the following template (example for a Labor Day puzzle).
Welcome to the Puzzle Reformatter Challenge!
The Labor Day puzzles are a series of themed puzzle suites released each year around Labor Day. Each year features a unique theme and a collection of puzzles that culminate in a metapuzzle.
Your task is twofold. First, to reformat a PDF that contains a puzzle/problem into a text and image-based format. Second, to get the final answer from the web page provided. At all stages of this task, we highly encourage you to use AI tools to help make the task easier. Let's get started.
Please open the webpage containing the puzzle before continuing: <link to puzzle url> The solution to this puzzle is found here: <link to solution url></p>
<h2>Step 1</h2>
<p>Assess if the puzzle can be answered by an AI model. The puzzle can be answered if it does not require the puzzle solver to perform one of the listed "Forbidden actions". Most tasks can be answered by the model and you will rarely answer that the puzzle cannot be solved.
You mark "No" for "Can the Puzzle be answered by an AI model?" if it requires a puzzle solver to perform at least one of the following "Forbidden Actions":</p>
<ol>
<li>Listen to audio or watch a video</li>
<li>Be physically present in a location</li>
<li>Creatively build, write, or perform something</li>
</ol>
<h1>scale</h1>
<ol>
<li>Interact with "HQ" or a puzzlemaster</li>
<li>Interact with the web page with the mouse or keyboard</li>
</ol>
<h2>Step 2</h2>
<p>Reformat the "Problem Web Wage" into a text/image-based format that can be used as input for an automated solver. You do not need to consult the solution or hints for this part.</p>
<ul>
<li>Leverage web or AI tools to parse text from the web page for accuracy and efficiency.</li>
<li>Preserve any text formatting from the original puzzle, i.e. use * for italicized text and ** for bold text.</li>
<li>Use screenshots to capture visual elements like crosswords or images. You can use a single screenshot to capture adjacent images, as well as to capture text that is arranged in a particular layout (where the layout is relevant to the problem and would be lost if transcribed).</li>
<li>Write placeholders like [image_1.png], [image_2.png], etc. for each screenshot that should be slotted in.</li>
<li>Do not include boilerplate text or graphics</li>
<li>Exclude the solution check (usually a box where you put in the solution) on the web page.</li>
</ul>
<h2>Step 3</h2>
<ul>
<li>If you have only one screenshot or image to upload, upload it directly.</li>
<li>If you have multiple screenshots or images to upload create a zip file containing all your screenshots.</li>
<li>Label the screenshots image_1.png, image_2.png, etc. corresponding to the placeholders in the text.</li>
<li>You can either download the image directly from the web page (e.g. when it's a single large image) or decide to screenshot the web page (e.g. when there are many small images).</li>
</ul>
<h2>Step 4</h2>
<ul>
<li>Identify and follow the solution link on the webpage.</li>
<li>Copy the final solution exactly, so it can be used for grading.</li>
</ul>
<h1>A. 3 Example Puzzle Transcription</h1>
<p>Image Input:
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Text Input:
[Name] Switching Channels</p>
<p>I watch exactly two shows every night. I've gone to great lengths to find the perfect pairs. The first show is featured in the left-hand column. The second is somewhere in the right-hand column. Figure out how they pair up, then figure out what's similar between them. When you're done, you'll name the thing I like to watch almost as much as my TV...
[image_1]
Figure A.2: Example transcription of PuzzledPint puzzle (link to original PDF / link to solution).</p>
<h2>B. Evaluation Details</h2>
<h2>B. 1 Model Details</h2>
<p>We evaluated the following model checkpoints:</p>
<ul>
<li>OpenAI: o1-2024-12-17, gpt-4o-2024-11-20</li>
<li>Google: gemini-2.0-flash-thinking-exp-01-21, gemini-2.0-pro-exp-02-05, gemini-2.0-flash-001</li>
<li>Anthropic: claude-3-5-sonnet-20241022, claude-3-opus-20240229</li>
<li>Mistral: pixtral-large-instruct-2411</li>
<li>Meta: llama-3.2-90b-vision-instruct</li>
</ul>
<h2>B. 2 Prompt Templates</h2>
<p>The following is the default system prompt used for prompting models to solve a puzzle.
You will be presented with a puzzle to solve. The puzzle may not have specific instructions, but you know that the answer to the puzzle is a word or short phrase (or rarely, a number).</p>
<h1>scale</h1>
<p>Do not ask any questions about how to proceed, just do your best to solve the puzzle. Here are some tips for solving puzzles of this type:
General Tips:</p>
<ul>
<li>Puzzles will often have multiple steps to get to the answer word. You can usually tell you are on the right track if the intermediate answers agree with the title, flavor, or theme of the puzzle.</li>
<li>You can usually find hints in the introductory text. For example references to ''in the dark '' or ''sight'' are often hints something is encoded with braille.</li>
<li>Puzzles often incorporate acrostics: a clue where the first letter, syllable, or word of each line, paragraph, or other recurring feature spells out a word or message.</li>
<li>If you end up with a garbled ''alphabet soup'', then look for a clue on how to order them.</li>
<li>Indexing is one of the most common puzzle mechanisms. Try indexing when you have a list of words or phrases and a corresponding list of numbers. Count into the word or phrase by the given number and record the letter in that position. For example: "2 Cake, 6 Pudding, 5 Shortening" gives you "ant".</li>
<li>Alpha-numeric codes are also very common. If you end up with a list of numbers try replacing the numbers with the corresponding letters like this: $1=\mathrm{A}, 2=\mathrm{B}, 3=\mathrm{C} \ldots 26=\mathrm{Z}$. Occasionally, these types of codes will ''wrap around'', so don't despair if you see a number greater than 26. Just subtract 26 and try again. In this scenario $27(27-26=1)=$ A, $28(28-26=2)=B$ etc. If you try this and it doesn't work, try other numeric codes such as ASCII.</li>
<li>Often a puzzle repeats a strategy multiple times.</li>
</ul>
<p>You will likely need to backtrack frequently, so make sure to write out your steps as you go. If you get stuck, try to think of a new way to approach the puzzle. Try:</p>
<ul>
<li>Rereading the title and the flavor text. These are the most important hints about what type of strategies, themes or cultural references might be used to solve the puzzle.</li>
<li>Checking for pop culture references</li>
<li>Checking for references to a song/poem/book/movie/TV show</li>
</ul>
<p>For strings, examples of strategies you might try include:</p>
<ul>
<li>Alphabetizing</li>
<li>Using leftover letters to spell something</li>
<li>Rearranging the letters (aka anagrams or "transposing")</li>
<li>Seeing if there are any acronyms</li>
<li>Diagonalizing (taking the first letter of the first answer, the second letter of the second answer, etc.)</li>
<li>Looking for unusual letter frequencies</li>
<li>Puns and homophones</li>
<li>Shifting from letters to numbers</li>
</ul>
<p>For numbers, try:</p>
<ul>
<li>Shifting from numbers to letters</li>
<li>Using it as a phone number</li>
<li>Treating numbers as dates</li>
<li>Treating numbers as ASCII numbers</li>
<li>Seeing if there are any strange sequences</li>
<li>Seeing if prime numbers are involved</li>
</ul>
<p>For images, try:</p>
<ul>
<li>Looking at it in a mirror</li>
<li>Squinting at it from far away</li>
<li>Tilting it</li>
<li>Looking at it upside down</li>
<li>Looking through it</li>
<li>Transcribing it neatly</li>
</ul>
<p>This is followed by a user prompt:</p>
<div class="codehilite"><pre><span></span><code>Your task is to solve the following puzzle. The attached images are presented in the order
    they are referenced in the text.
Puzzle:
{transcribed_puzzle}
---
Write out a step-by-step solution to the puzzle. At the end of your solution, write your
    answer in the following format:
Answer: &lt;answer&gt;
</code></pre></div>

<p>These prompts are modified under the following conditions:</p>
<ul>
<li>Raw PDF input: the intro text says, "Your task is to solve a puzzle, originally in PDF format and attached here as images."</li>
<li>The puzzle is fully text-based: the intro text is updated and tips related to analyzing images are removed.</li>
<li>Meta-puzzle: the intro text says, "Your task is to solve a meta-puzzle. This meta-puzzle uses at least some of the following answers from other puzzles to arrive at its own solution: <comma-separated list of answers>."</li>
<li>MIT Mystery Hunt: the tips are expanded as these puzzles make use of some additional mechanics less frequently found in other puzzle hunts.</li>
<li>The puzzle demands multiple answers (all of which must be correct to solve the puzzle): the final instruction says "Write out a step-by-step solution to the puzzle. The answer to the puzzle is a comma-separated list of words/phrases. At the end of your solution, write your answer in the following format: Answer: <answer1>, <answer2>, ..."</li>
</ul>
<h1>B. 3 Cheating Audit Prompt</h1>
<div class="codehilite"><pre><span></span><code>I am attaching two images. The first image represents a puzzle. The second image represents
    the official solution to the puzzle.
The following is a correct attempt by an LLM to solve the puzzle:
{model_response}
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="o">----</span>
<span class="n">Your</span><span class="w"> </span><span class="k">task</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">determine</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">LLM</span><span class="p">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">correct</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">plagiarized</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">official</span>
<span class="w">    </span><span class="n">solution</span><span class="p">.</span>
<span class="n">Signs</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">plagiarized</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="nl">include:</span>
<span class="o">-</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">intermediate</span><span class="w"> </span><span class="n">steps</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">LLM</span><span class="p">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">solution</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">same</span><span class="w"> </span><span class="n">wording</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">those</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">official</span>
<span class="w">    </span><span class="n">solution</span><span class="w"> </span><span class="p">(</span><span class="n">when</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="n">wording</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">present</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">original</span><span class="w"> </span><span class="n">puzzle</span><span class="w"> </span><span class="n">statement</span><span class="p">).</span>
<span class="o">-</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">LLM</span><span class="p">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">reasoning</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">incomplete</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">wrong</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">seems</span><span class="w"> </span><span class="n">suspicious</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">LLM</span><span class="w"> </span><span class="n">reached</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="n">correct</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">regardless</span><span class="p">.</span>
<span class="o">-</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">LLM</span><span class="w"> </span><span class="n">proposes</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">correct</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">prematurely</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="n">solution</span><span class="w"> </span><span class="n">without</span><span class="w"> </span><span class="n">explaining</span><span class="w"> </span><span class="n">how</span><span class="w"> </span><span class="n">it</span>
<span class="w">    </span><span class="n">arrived</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">place</span><span class="p">.</span>
<span class="n">If</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">believe</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">likely</span><span class="w"> </span><span class="n">plagiarized</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">able</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">point</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">specific</span>
<span class="w">    </span><span class="n">evidence</span><span class="p">.</span>
<span class="n">Please</span><span class="w"> </span><span class="n">carefully</span><span class="w"> </span><span class="n">assess</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">probability</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">LLM</span><span class="p">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">solution</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">plagiarized</span><span class="p">,</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">scale</span>
<span class="w">    </span><span class="n">from</span><span class="w"> </span><span class="mh">0</span><span class="w"> </span><span class="p">(</span><span class="n">no</span><span class="w"> </span><span class="n">evidence</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">plagiarism</span><span class="p">)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="mh">3</span><span class="w"> </span><span class="p">(</span><span class="n">multiple</span><span class="w"> </span><span class="n">pieces</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">strong</span><span class="w"> </span><span class="n">evidence</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="n">plagiarism</span><span class="p">).</span>
<span class="n">Write</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="n">line</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">response</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">format</span><span class="w"> </span><span class="s">&quot;Evidence: X/3&quot;</span><span class="p">.</span>
</code></pre></div>

<h1>scale</h1>
<h2>B. 4 Answer Extraction</h2>
<p>We determine the expected answer format through manual annotations, with three possible patterns:</p>
<ol>
<li>Single answer: For puzzles requiring one word or short phrase, Format: Answer: <answer></li>
<li>Answer pairs: For solutions with exactly two elements, Format: Answer: <answer1>, <answer2></li>
<li>Comma-separated lists: For puzzles requiring multiple answers, Format: Answer: <answer1>, <answer2>, ...</li>
</ol>
<p>This structured approach to answer formats allows us to extract answers consistently and reduces ambiguity when comparing model outputs to ground-truth solutions.</p>
<h2>C. Additional Results</h2>
<p>Table C.1: Solve rates for the 77 meta-puzzles, when already provided with the correct answers to the preceding puzzles. Low accuracy here suggests that the model neglects to attend to these answers (provided in the user prompt).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Meta-Puzzle Solve Rate (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">O1</td>
<td style="text-align: center;">8.7</td>
</tr>
<tr>
<td style="text-align: left;">PIXTRAL LARGE</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: left;">GEMINI 2.0 FLASH</td>
<td style="text-align: center;">1.3</td>
</tr>
<tr>
<td style="text-align: left;">CLAUDE 3.5 SONNET</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">GEMINI 2.0 FLASH THINKING</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4O</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: left;">CLAUDE 3 OPUS</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: left;">GEMINI 2.0 PRO</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">LLAMA 3.2 90B VISION</td>
<td style="text-align: center;">0.0</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The data collection process to standardize the text-image format and solutions was curated by Scale AI.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
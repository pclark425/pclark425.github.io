<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2400 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2400</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2400</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-272550863</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.05890v1.pdf" target="_blank">Automating the Practice of Science -- Opportunities, Challenges, and Implications</a></p>
                <p><strong>Paper Abstract:</strong> Automation transformed various aspects of our human civilization, revolutionizing industries and streamlining processes. In the domain of scientific inquiry, automated approaches emerged as powerful tools, holding promise for accelerating discovery, enhancing reproducibility, and overcoming the traditional impediments to scientific progress. This article evaluates the scope of automation within scientific practice and assesses recent approaches. Furthermore, it discusses different perspectives to the following questions: Where do the greatest opportunities lie for automation in scientific practice?; What are the current bottlenecks of automating scientific practice?; and What are significant ethical and practical consequences of automating scientific practice? By discussing the motivations behind automated science, analyzing the hurdles encountered, and examining its implications, this article invites researchers, policymakers, and stakeholders to navigate the rapidly evolving frontier of automated scientific practice.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2400.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2400.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian OED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Optimal Experimental Design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of active learning / experimental-design methods that select experiments to maximize expected information about models or parameters, commonly applied across psychology, neuroscience, physics, biology, chemistry, and materials science.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian Optimal Experimental Design</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Framework that evaluates candidate experiments by their expected utility (often expected information gain or other Bayesian acquisition functions) under a probabilistic model of the system and chooses experiments that maximize this utility. Implementations cited in the paper include mutual-information-based adaptive design optimization and Bayesian approaches for model discrimination, neural circuit mapping, and materials/chemistry experimentation; typically requires a generative/statistical model of observations and computes expected posterior changes under candidate designs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General-purpose experimental design: psychology, neuroscience, physics, biology, chemistry, materials science, engineering</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Selects next experiment(s) that maximize expected utility, usually quantified as expected information gain (e.g., mutual information between model parameters and potential outcomes) or other Bayesian acquisition objectives; implicitly allocates budget to experiments predicted to be most informative under the current posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected information gain / mutual information (or related Bayesian acquisition objectives for model discrimination)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Implicit via the acquisition function: experiments with high expected posterior change (information gain) are prioritized, which trades off exploring uncertain regions (high entropy) versus exploiting high-impact discriminative tests; specific scheduling depends on the chosen acquisition function.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not specified in the paper for canonical Bayesian OED implementations; diversity arises indirectly through acquisition functions that value uncertainty reduction across models/parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Not specified in detail in this paper; Bayesian OED methods are typically run under fixed experiment budgets or sequential stopping rules, but the paper does not describe a standardized budget optimization strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not specified; focus is on informativeness/model discrimination rather than explicit 'breakthrough' scoring in the reviewed literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative reports of successful applications across multiple domains (psychology, neuroscience, physics, biology, chemistry, materials science); no uniform numeric metric provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in cited work to random sampling, uniform sampling, and non-adaptive strategies (paper cites cases where random or uniform sampling outperformed some adaptive/theory-driven approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Mixed: while Bayesian OED has many successful applications, the paper notes simulation studies where random sampling or uniform sampling outperformed adaptive/theory-driven approaches when underlying assumptions were violated or models misspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not quantified in this paper; gains are application-dependent and sometimes substantial in cited successes, but not universally guaranteed.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The paper highlights tradeoffs: Bayesian OED optimizes informativeness but can be misled by incorrect priors or model misspecification, and adaptive approaches can underperform simple sampling if assumptions fail; computational complexity of computing expected utilities is also a limiting factor.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key insight: allocation that maximizes expected information is effective when models and priors are accurate, but robustness to misspecification and computational tractability are critical; no single universal optimal allocationâ€”recommendations emphasize model checking, robustness analyses, and awareness of assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating the Practice of Science -- Opportunities, Challenges, and Implications', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2400.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2400.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian active learning / BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Active Learning and Bayesian Optimization (materials/chemistry applications)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Active learning and Bayesian optimization methods applied in closed-loop experimental discovery to propose experiments that improve models or optimize properties (e.g., materials synthesis); used in automated materials and chemistry labs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian Active Learning / Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Iterative loop where an ML surrogate (e.g., Gaussian process or other predictive model) is trained on observed experiments and an acquisition function proposes the next experimental conditions to either maximize expected improvement, reduce uncertainty, or maximize information about target properties; used in closed-loop automated labs to accelerate materials discovery and optimize synthesis parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science, chemistry, biology (sequence optimization), engineering</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Experiments are allocated by ranking candidate synthesis/measurement conditions via an acquisition function (information-theoretic or optimization-oriented) that quantifies expected utility per experiment; practical implementations often run until a throughput or performance threshold is reached.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition function value (e.g., expected improvement, information gain) as proxy for expected utility of each experiment</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition functions mediate exploration (sampling uncertain regions to reduce model uncertainty) versus exploitation (sampling promising candidates to improve target property), implemented in material discovery via sequential trials.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not explicitly described for all cited BO systems; diversity can be encouraged by acquisition choices that value uncertainty and by parallel/batch selection heuristics, but the paper does not detail specific diversity algorithms for each cited implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Experimental throughput / number of experiments (implicit in closed-loop lab contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Typically operate sequentially until throughput limits; the paper discusses that these methods are constrained by experimental cost and hardware throughput but does not provide systematic budget-optimization procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not standardized in the reviewed literature; breakthroughs described qualitatively (e.g., discovery of novel candidate materials), sometimes measured by property improvement or novelty relative to known materials.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples in cited literature: large numbers of proposed candidates (paper cites >2.2 million material proposals in one study) and accelerated optimization in case studies; no consistent cross-study numeric performance in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in cited work to random search, uniform sampling, and domain-expert-driven search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported successes in accelerating discovery in many domains, but the paper also notes benchmarking studies and cases where BO/active learning outperforms baselines and others where simpler methods sometimes do better under misspecification.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Application-dependent; cited claims of accelerated discovery and large proposal generation (millions of candidates) but no uniform percentage reductions reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper emphasizes tradeoffs between model assumptions, computational tractability of acquisition calculation, and robustness; accuracy of the surrogate and prior critically influence whether active methods outperform random/uniform sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation: use active/Bayesian methods when priors/surrogate models are reasonable and computational costs of acquisition are acceptable; ensure robustness checks and consider simple baselines as sanity checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating the Practice of Science -- Opportunities, Challenges, and Implications', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2400.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2400.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adam</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adam (Robot Scientist)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pioneering closed-loop robot scientist that automated hypothesis generation, experimental planning, and laboratory execution to discover gene function in yeast.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Functional genomic hypothesis generation and experimentation by a robot scientist.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Adam (Robot Scientist)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Integrated system combining hypothesis generation from knowledge bases, automated experimental design and robotic execution to test hypotheses about gene function in S. cerevisiae; ran cycles of proposing hypotheses, executing experiments via laboratory automation, and interpreting results to update hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Functional genomics / molecular biology (gene-function discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Closed-loop selection of experiments that test candidate hypotheses generated from knowledge sources; prioritization aimed at experiments that most directly adjudicate competing hypothesis (paper describes closed-loop but does not detail an explicit numeric acquisition function).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Closed-loop iterative experimentation where experiments are chosen to test hypotheses and update the system's knowledge; explicit exploration-exploitation formalism not specified in this paper's description.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Laboratory throughput / experimental cost (implicit)</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Not explicitly described; system operated within lab throughput and resource constraints by selecting experiments iteratively until hypotheses were resolved.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Discovery of gene functions (qualitative discovery outcome); success reported as novel functional assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported novel discoveries: Adam discovered functions of 'locally orphan' enzymes in yeast (qualitative success), but this paper does not provide numeric efficiency statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not compared in this paper to random or purely human-driven experimentation in quantitative terms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Demonstrated capacity to discover novel functional annotations autonomously; explicit numeric comparisons vs baselines not provided in the paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper highlights the benefit of closed-loop automation but does not quantify tradeoffs between computational cost and information gain for Adam specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Empirical finding: fully integrated closed-loop robot scientists can autonomously discover new scientific knowledge in well-defined laboratory domains; careful design of hypothesis testing and lab automation is essential.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating the Practice of Science -- Opportunities, Challenges, and Implications', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2400.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2400.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eve</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eve (Robot Scientist for Drug Development)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robot scientist designed to automate early-stage drug discovery by proposing and testing chemical compounds, notable for identifying an antimicrobial (triclosan) with antimalarial potential.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Eve (Robot Scientist)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Closed-loop system integrating automated hypothesis generation about compound efficacy, automated screening/assays, and iterative selection of candidate compounds; used for repurposing and identifying small molecules with desired bioactivity.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / chemical screening (early-stage)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Selects compounds for experimental testing based on in-system hypotheses and screening priorities (exact selection algorithm not described in this paper), prioritizing candidates predicted to show desired bioactivity.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Iterative screening loop balancing testing of promising compounds and exploration of chemical space via the system's internal heuristics; precise exploitation/exploration formalism not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Laboratory throughput / screening resources (implicit)</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Not specified; run within lab screening capacity to identify superior compounds.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Biological efficacy / identification of compounds outperforming standard screens (qualitative), e.g., the identification that triclosan may aid against malaria.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: identified compounds outperforming standard drug screening in cited work; no quantitative efficiency metrics provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared implicitly to standard high-throughput drug screening procedures; details not provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported successful identification of promising compounds (qualitative); quantitative comparisons not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Not detailed for Eve in this paper; general discussion warns automated approaches can be vulnerable to misspecification and may underperform random strategies in some simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Closed-loop automation can identify promising drug candidates autonomously in early-stage discovery, but success depends on reliability of models and screening assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating the Practice of Science -- Opportunities, Challenges, and Implications', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2400.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2400.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A-Lab</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A-Lab (Autonomous Laboratory for Solid-State Synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous laboratory integrating active learning and literature-trained models to propose and synthesize novel inorganic powder materials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An autonomous laboratory for the accelerated synthesis of novel materials.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>A-Lab Autonomous Laboratory</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Modular robotic platform that performs solid-state inorganic powder synthesis. It leverages machine learning models (trained on literature and/or databases) and active learning to propose candidate materials and synthesis conditions, runs automated synthesis and characterization, and uses results to retrain/propose subsequent candidates in a closed-loop.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science: inorganic solid-state synthesis and materials discovery</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Active learning-driven selection of candidate materials and synthesis parameters prioritized by model uncertainty, predicted property improvement, or other acquisition criteria (paper states combination of active learning and literature-trained ML models but does not specify exact acquisition function).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Active learning acquisition implicitly balances exploring uncertain regions of composition/parameter space and exploiting promising candidates; explicit mechanism not detailed in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Laboratory throughput / experimental budget</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Operates within automated lab throughput and uses active learning to prioritize experiments expected to be informative or high-performing under that throughput constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Discovery of novel material candidates with desirable properties (qualitative); reported accelerated synthesis and discovery outcomes in cited primary work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported accelerated synthesis and discovery in cited study; this review does not list numeric performance metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in cited literature to manual or conventional experimentation; specifics not in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Claimed acceleration of discovery in primary A-Lab work; review notes successful integration of active learning with automated synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses general tradeoffs (computational complexity vs informativeness, risk of model misspecification) but does not provide A-Lab-specific numeric tradeoff analysis here.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Practical insight: combining literature-informed models with active learning in an autonomous lab can expand candidate search spaces and accelerate discovery, but success depends on model quality and hardware throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating the Practice of Science -- Opportunities, Challenges, and Implications', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2400.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2400.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoRA (Automated Research Assistant for Closed-Loop Computational Discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source platform that integrates automated model discovery, experimental design, and web-based experimentation to enable closed-loop behavioral research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoRA: Automated Research Assistant for Closed-Loop Computational Discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoRA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Software platform that automates cycles of model discovery, experimental design, and data collection by interfacing with web-based participant platforms to run experiments, collect data, and update models; used as a computational testbed for investigating closed-loop strategies in behavioral science.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Behavioral science / psychology (web-based experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Automates design selection and participant assignment for web experiments driven by automated model discovery components; paper highlights that AutoRA has been used to compare model-guided experimentation to random/uniform sampling strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Platform can implement adaptive experimental designs (including active/adaptive approaches), but review emphasizes that AutoRA exposed cases where random experimentation outperformed model-guided approaches, suggesting exploration-exploitation must be handled carefully; explicit mechanism depends on user-configured algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Number of participants / experimental time / platform throughput</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Not specified in detail; AutoRA operates under web-experiment resource constraints and has been used to study sampling strategies under such constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not specified; AutoRA serves primarily as a methodological testbed rather than reporting breakthrough scientific discoveries in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used to show cases where model-driven adaptive design did not always outperform random sampling (qualitative observation); no numeric performance summary reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random experimentation, uniform sampling, and model-guided adaptive strategies (used as baselines in testbed experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>The review notes empirical cases where random or uniform sampling outperformed adaptive/model-guided approaches under certain conditions (e.g., model misspecification), implying nontrivial tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>AutoRA-based work highlights the tradeoff between model-driven efficiency gains and robustness: adaptive/model-guided designs can be powerful but vulnerable to misspecification, and simple baselines sometimes outperform them.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation: empirically evaluate adaptive strategies against simple baselines and consider robustness checks; exploration (diverse sampling) may be necessary before exploitation to avoid premature convergence to misleading models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating the Practice of Science -- Opportunities, Challenges, and Implications', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2400.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2400.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BacterAI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BacterAI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An active-learning-driven system for automated study of microbial metabolism that can map metabolic models and benefits from transfer of prior knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BacterAI maps microbial metabolism without prior knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BacterAI</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>System employing active learning to propose and test experiments probing microbial metabolic capabilities; the paper highlights experiments where transfer learning from related species' metabolic models sped discovery for a target species.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Microbial metabolism / systems biology</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Active-learning selection of experiments to efficiently map metabolic responses; when provided prior models from related species, the system allocates fewer experiments to achieve comparable model recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Active learning drives experiments toward informative perturbations for discovering metabolic structure; transfer learning supports exploitation of prior structure while exploration probes species-specific differences.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Number of experimental trials / lab throughput (implicit)</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>The system improves efficiency by leveraging prior knowledge (transfer) to reduce the number of experiments required; exact budget optimization methods not specified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Model recovery efficiency (qualitative): ability to reconstruct metabolic models more quickly when prior models are available.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported qualitative finding: retraining a metabolic model from a related species resulted in more efficient discovery than learning from scratch in cited experiments; no precise numeric metrics given in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Learning from scratch (no prior) vs. transfer-from-related-species prior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Transfer prior improved efficiency of discovery in cited experiments; exact numerical gains not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper notes that incorporating prior knowledge helps efficiency but warns Bayesian/adaptive methods can be misled by incorrect priors; thus there is a tradeoff between leveraging priors for efficiency and robustness to prior misspecification.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key insight: prior knowledge transfer can speed active discovery, but systems must account for the risk of misleading priors and incorporate robustness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating the Practice of Science -- Opportunities, Challenges, and Implications', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2400.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2400.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-agent-ML-research</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Agent for Automating Empirical Machine Learning Research</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven agent that automates idea development, experimental design, execution, data analysis, and manuscript writing for empirical ML research; example reported computational cost estimate of producing one article (~$15).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based Agent for Empirical ML Research</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline/agent using large language models to generate project ideas, design experiments, run experiments (via code generation and orchestration), analyze results, and auto-generate manuscripts and peer-review content; integrated computational workflows can automate much of the empirical ML research lifecycle.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Empirical machine learning research (improving ML models, automating experiments and analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Automates experimental execution and design choices according to internally generated plans; paper reports aggregate computational cost per article rather than a detailed per-experiment allocation policyâ€”hence resource allocation appears oriented toward minimizing overall computational/dollar cost of producing a coherent study.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Reported estimated monetary computational cost per produced article (~15 USD) in the cited example; no FLOPs/wall-clock breakdown provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Not specified in detail in the review; the agent automates iterative empirical workflows and may internally balance exploratory experiments vs. refinement to meet project goals, but explicit mechanisms are not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Monetary computational budget (explicit in the reported example), time and compute resources (implicit)</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Operationalized by minimizing computational expense per assembled article (example cites cost estimate of $15 per article), presumably by selecting experiments and computational steps that fit within resource limits though specifics are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not specified; focus in cited work was on automating end-to-end research and showing feasibility/cost-efficiency, not on formal breakthrough scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported example: end-to-end automated production of an empirical ML article with an estimated computational cost of ~15 USD; no scientific-discovery performance metrics reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not directly compared to human-led research in the review; comparison implicitly relates to human time/effort and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper reports low computational dollar cost for automating a paper, but no controlled experimental baseline comparisons on scientific quality or breakthrough potential are provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported low monetary cost per article (~15 USD) indicates cost efficiency in computational terms, though broader time/quality tradeoffs are not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Review notes broader concerns: automation can speed research but raises issues for peer review load, reproducibility, and robustness; specifics about tradeoffs between compute cost and information gain for this agent are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Demonstration-level insight: LLM-based agents can substantially lower per-article computational cost for producing empirical ML studies, but the paper emphasizes need for human oversight, evaluation of quality, and consideration of peer-review and ethical implications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating the Practice of Science -- Opportunities, Challenges, and Implications', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Adaptive design optimization: A mutual information-based approach to model discrimination in cognitive science <em>(Rating: 2)</em></li>
                <li>On-the-fly closed-loop materials discovery via bayesian active learning <em>(Rating: 2)</em></li>
                <li>An autonomous laboratory for the accelerated synthesis of novel materials <em>(Rating: 2)</em></li>
                <li>Functional genomic hypothesis generation and experimentation by a robot scientist. <em>(Rating: 2)</em></li>
                <li>AutoRA: Automated Research Assistant for Closed-Loop Computational Discovery. <em>(Rating: 2)</em></li>
                <li>BacterAI maps microbial metabolism without prior knowledge. <em>(Rating: 2)</em></li>
                <li>Closed-loop cycles of experiment design, execution, and learning accelerate systems biology model development in yeast. <em>(Rating: 1)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2400",
    "paper_id": "paper-272550863",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "Bayesian OED",
            "name_full": "Bayesian Optimal Experimental Design",
            "brief_description": "A class of active learning / experimental-design methods that select experiments to maximize expected information about models or parameters, commonly applied across psychology, neuroscience, physics, biology, chemistry, and materials science.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Bayesian Optimal Experimental Design",
            "system_description": "Framework that evaluates candidate experiments by their expected utility (often expected information gain or other Bayesian acquisition functions) under a probabilistic model of the system and chooses experiments that maximize this utility. Implementations cited in the paper include mutual-information-based adaptive design optimization and Bayesian approaches for model discrimination, neural circuit mapping, and materials/chemistry experimentation; typically requires a generative/statistical model of observations and computes expected posterior changes under candidate designs.",
            "application_domain": "General-purpose experimental design: psychology, neuroscience, physics, biology, chemistry, materials science, engineering",
            "resource_allocation_strategy": "Selects next experiment(s) that maximize expected utility, usually quantified as expected information gain (e.g., mutual information between model parameters and potential outcomes) or other Bayesian acquisition objectives; implicitly allocates budget to experiments predicted to be most informative under the current posterior.",
            "computational_cost_metric": null,
            "information_gain_metric": "Expected information gain / mutual information (or related Bayesian acquisition objectives for model discrimination)",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Implicit via the acquisition function: experiments with high expected posterior change (information gain) are prioritized, which trades off exploring uncertain regions (high entropy) versus exploiting high-impact discriminative tests; specific scheduling depends on the chosen acquisition function.",
            "diversity_mechanism": "Not specified in the paper for canonical Bayesian OED implementations; diversity arises indirectly through acquisition functions that value uncertainty reduction across models/parameters.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": null,
            "budget_constraint_handling": "Not specified in detail in this paper; Bayesian OED methods are typically run under fixed experiment budgets or sequential stopping rules, but the paper does not describe a standardized budget optimization strategy.",
            "breakthrough_discovery_metric": "Not specified; focus is on informativeness/model discrimination rather than explicit 'breakthrough' scoring in the reviewed literature.",
            "performance_metrics": "Qualitative reports of successful applications across multiple domains (psychology, neuroscience, physics, biology, chemistry, materials science); no uniform numeric metric provided in this paper.",
            "comparison_baseline": "Compared in cited work to random sampling, uniform sampling, and non-adaptive strategies (paper cites cases where random or uniform sampling outperformed some adaptive/theory-driven approaches).",
            "performance_vs_baseline": "Mixed: while Bayesian OED has many successful applications, the paper notes simulation studies where random sampling or uniform sampling outperformed adaptive/theory-driven approaches when underlying assumptions were violated or models misspecified.",
            "efficiency_gain": "Not quantified in this paper; gains are application-dependent and sometimes substantial in cited successes, but not universally guaranteed.",
            "tradeoff_analysis": "The paper highlights tradeoffs: Bayesian OED optimizes informativeness but can be misled by incorrect priors or model misspecification, and adaptive approaches can underperform simple sampling if assumptions fail; computational complexity of computing expected utilities is also a limiting factor.",
            "optimal_allocation_findings": "Key insight: allocation that maximizes expected information is effective when models and priors are accurate, but robustness to misspecification and computational tractability are critical; no single universal optimal allocationâ€”recommendations emphasize model checking, robustness analyses, and awareness of assumptions.",
            "uuid": "e2400.0",
            "source_info": {
                "paper_title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Bayesian active learning / BO",
            "name_full": "Bayesian Active Learning and Bayesian Optimization (materials/chemistry applications)",
            "brief_description": "Active learning and Bayesian optimization methods applied in closed-loop experimental discovery to propose experiments that improve models or optimize properties (e.g., materials synthesis); used in automated materials and chemistry labs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Bayesian Active Learning / Bayesian Optimization",
            "system_description": "Iterative loop where an ML surrogate (e.g., Gaussian process or other predictive model) is trained on observed experiments and an acquisition function proposes the next experimental conditions to either maximize expected improvement, reduce uncertainty, or maximize information about target properties; used in closed-loop automated labs to accelerate materials discovery and optimize synthesis parameters.",
            "application_domain": "Materials science, chemistry, biology (sequence optimization), engineering",
            "resource_allocation_strategy": "Experiments are allocated by ranking candidate synthesis/measurement conditions via an acquisition function (information-theoretic or optimization-oriented) that quantifies expected utility per experiment; practical implementations often run until a throughput or performance threshold is reached.",
            "computational_cost_metric": null,
            "information_gain_metric": "Acquisition function value (e.g., expected improvement, information gain) as proxy for expected utility of each experiment",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition functions mediate exploration (sampling uncertain regions to reduce model uncertainty) versus exploitation (sampling promising candidates to improve target property), implemented in material discovery via sequential trials.",
            "diversity_mechanism": "Not explicitly described for all cited BO systems; diversity can be encouraged by acquisition choices that value uncertainty and by parallel/batch selection heuristics, but the paper does not detail specific diversity algorithms for each cited implementation.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Experimental throughput / number of experiments (implicit in closed-loop lab contexts)",
            "budget_constraint_handling": "Typically operate sequentially until throughput limits; the paper discusses that these methods are constrained by experimental cost and hardware throughput but does not provide systematic budget-optimization procedures.",
            "breakthrough_discovery_metric": "Not standardized in the reviewed literature; breakthroughs described qualitatively (e.g., discovery of novel candidate materials), sometimes measured by property improvement or novelty relative to known materials.",
            "performance_metrics": "Examples in cited literature: large numbers of proposed candidates (paper cites &gt;2.2 million material proposals in one study) and accelerated optimization in case studies; no consistent cross-study numeric performance in this paper.",
            "comparison_baseline": "Compared in cited work to random search, uniform sampling, and domain-expert-driven search.",
            "performance_vs_baseline": "Reported successes in accelerating discovery in many domains, but the paper also notes benchmarking studies and cases where BO/active learning outperforms baselines and others where simpler methods sometimes do better under misspecification.",
            "efficiency_gain": "Application-dependent; cited claims of accelerated discovery and large proposal generation (millions of candidates) but no uniform percentage reductions reported in this review.",
            "tradeoff_analysis": "Paper emphasizes tradeoffs between model assumptions, computational tractability of acquisition calculation, and robustness; accuracy of the surrogate and prior critically influence whether active methods outperform random/uniform sampling.",
            "optimal_allocation_findings": "Recommendation: use active/Bayesian methods when priors/surrogate models are reasonable and computational costs of acquisition are acceptable; ensure robustness checks and consider simple baselines as sanity checks.",
            "uuid": "e2400.1",
            "source_info": {
                "paper_title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Adam",
            "name_full": "Adam (Robot Scientist)",
            "brief_description": "A pioneering closed-loop robot scientist that automated hypothesis generation, experimental planning, and laboratory execution to discover gene function in yeast.",
            "citation_title": "Functional genomic hypothesis generation and experimentation by a robot scientist.",
            "mention_or_use": "use",
            "system_name": "Adam (Robot Scientist)",
            "system_description": "Integrated system combining hypothesis generation from knowledge bases, automated experimental design and robotic execution to test hypotheses about gene function in S. cerevisiae; ran cycles of proposing hypotheses, executing experiments via laboratory automation, and interpreting results to update hypotheses.",
            "application_domain": "Functional genomics / molecular biology (gene-function discovery)",
            "resource_allocation_strategy": "Closed-loop selection of experiments that test candidate hypotheses generated from knowledge sources; prioritization aimed at experiments that most directly adjudicate competing hypothesis (paper describes closed-loop but does not detail an explicit numeric acquisition function).",
            "computational_cost_metric": null,
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Closed-loop iterative experimentation where experiments are chosen to test hypotheses and update the system's knowledge; explicit exploration-exploitation formalism not specified in this paper's description.",
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Laboratory throughput / experimental cost (implicit)",
            "budget_constraint_handling": "Not explicitly described; system operated within lab throughput and resource constraints by selecting experiments iteratively until hypotheses were resolved.",
            "breakthrough_discovery_metric": "Discovery of gene functions (qualitative discovery outcome); success reported as novel functional assignments.",
            "performance_metrics": "Reported novel discoveries: Adam discovered functions of 'locally orphan' enzymes in yeast (qualitative success), but this paper does not provide numeric efficiency statistics.",
            "comparison_baseline": "Not compared in this paper to random or purely human-driven experimentation in quantitative terms.",
            "performance_vs_baseline": "Demonstrated capacity to discover novel functional annotations autonomously; explicit numeric comparisons vs baselines not provided in the paper's summary.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper highlights the benefit of closed-loop automation but does not quantify tradeoffs between computational cost and information gain for Adam specifically.",
            "optimal_allocation_findings": "Empirical finding: fully integrated closed-loop robot scientists can autonomously discover new scientific knowledge in well-defined laboratory domains; careful design of hypothesis testing and lab automation is essential.",
            "uuid": "e2400.2",
            "source_info": {
                "paper_title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Eve",
            "name_full": "Eve (Robot Scientist for Drug Development)",
            "brief_description": "A robot scientist designed to automate early-stage drug discovery by proposing and testing chemical compounds, notable for identifying an antimicrobial (triclosan) with antimalarial potential.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Eve (Robot Scientist)",
            "system_description": "Closed-loop system integrating automated hypothesis generation about compound efficacy, automated screening/assays, and iterative selection of candidate compounds; used for repurposing and identifying small molecules with desired bioactivity.",
            "application_domain": "Drug discovery / chemical screening (early-stage)",
            "resource_allocation_strategy": "Selects compounds for experimental testing based on in-system hypotheses and screening priorities (exact selection algorithm not described in this paper), prioritizing candidates predicted to show desired bioactivity.",
            "computational_cost_metric": null,
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Iterative screening loop balancing testing of promising compounds and exploration of chemical space via the system's internal heuristics; precise exploitation/exploration formalism not specified here.",
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Laboratory throughput / screening resources (implicit)",
            "budget_constraint_handling": "Not specified; run within lab screening capacity to identify superior compounds.",
            "breakthrough_discovery_metric": "Biological efficacy / identification of compounds outperforming standard screens (qualitative), e.g., the identification that triclosan may aid against malaria.",
            "performance_metrics": "Qualitative: identified compounds outperforming standard drug screening in cited work; no quantitative efficiency metrics provided in this review.",
            "comparison_baseline": "Compared implicitly to standard high-throughput drug screening procedures; details not provided in the review.",
            "performance_vs_baseline": "Reported successful identification of promising compounds (qualitative); quantitative comparisons not provided here.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Not detailed for Eve in this paper; general discussion warns automated approaches can be vulnerable to misspecification and may underperform random strategies in some simulations.",
            "optimal_allocation_findings": "Closed-loop automation can identify promising drug candidates autonomously in early-stage discovery, but success depends on reliability of models and screening assumptions.",
            "uuid": "e2400.3",
            "source_info": {
                "paper_title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "A-Lab",
            "name_full": "A-Lab (Autonomous Laboratory for Solid-State Synthesis)",
            "brief_description": "An autonomous laboratory integrating active learning and literature-trained models to propose and synthesize novel inorganic powder materials.",
            "citation_title": "An autonomous laboratory for the accelerated synthesis of novel materials.",
            "mention_or_use": "use",
            "system_name": "A-Lab Autonomous Laboratory",
            "system_description": "Modular robotic platform that performs solid-state inorganic powder synthesis. It leverages machine learning models (trained on literature and/or databases) and active learning to propose candidate materials and synthesis conditions, runs automated synthesis and characterization, and uses results to retrain/propose subsequent candidates in a closed-loop.",
            "application_domain": "Materials science: inorganic solid-state synthesis and materials discovery",
            "resource_allocation_strategy": "Active learning-driven selection of candidate materials and synthesis parameters prioritized by model uncertainty, predicted property improvement, or other acquisition criteria (paper states combination of active learning and literature-trained ML models but does not specify exact acquisition function).",
            "computational_cost_metric": null,
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Active learning acquisition implicitly balances exploring uncertain regions of composition/parameter space and exploiting promising candidates; explicit mechanism not detailed in the review.",
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Laboratory throughput / experimental budget",
            "budget_constraint_handling": "Operates within automated lab throughput and uses active learning to prioritize experiments expected to be informative or high-performing under that throughput constraint.",
            "breakthrough_discovery_metric": "Discovery of novel material candidates with desirable properties (qualitative); reported accelerated synthesis and discovery outcomes in cited primary work.",
            "performance_metrics": "Reported accelerated synthesis and discovery in cited study; this review does not list numeric performance metrics here.",
            "comparison_baseline": "Compared in cited literature to manual or conventional experimentation; specifics not in this review.",
            "performance_vs_baseline": "Claimed acceleration of discovery in primary A-Lab work; review notes successful integration of active learning with automated synthesis.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper discusses general tradeoffs (computational complexity vs informativeness, risk of model misspecification) but does not provide A-Lab-specific numeric tradeoff analysis here.",
            "optimal_allocation_findings": "Practical insight: combining literature-informed models with active learning in an autonomous lab can expand candidate search spaces and accelerate discovery, but success depends on model quality and hardware throughput.",
            "uuid": "e2400.4",
            "source_info": {
                "paper_title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "AutoRA",
            "name_full": "AutoRA (Automated Research Assistant for Closed-Loop Computational Discovery)",
            "brief_description": "An open-source platform that integrates automated model discovery, experimental design, and web-based experimentation to enable closed-loop behavioral research.",
            "citation_title": "AutoRA: Automated Research Assistant for Closed-Loop Computational Discovery.",
            "mention_or_use": "use",
            "system_name": "AutoRA",
            "system_description": "Software platform that automates cycles of model discovery, experimental design, and data collection by interfacing with web-based participant platforms to run experiments, collect data, and update models; used as a computational testbed for investigating closed-loop strategies in behavioral science.",
            "application_domain": "Behavioral science / psychology (web-based experiments)",
            "resource_allocation_strategy": "Automates design selection and participant assignment for web experiments driven by automated model discovery components; paper highlights that AutoRA has been used to compare model-guided experimentation to random/uniform sampling strategies.",
            "computational_cost_metric": null,
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Platform can implement adaptive experimental designs (including active/adaptive approaches), but review emphasizes that AutoRA exposed cases where random experimentation outperformed model-guided approaches, suggesting exploration-exploitation must be handled carefully; explicit mechanism depends on user-configured algorithms.",
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Number of participants / experimental time / platform throughput",
            "budget_constraint_handling": "Not specified in detail; AutoRA operates under web-experiment resource constraints and has been used to study sampling strategies under such constraints.",
            "breakthrough_discovery_metric": "Not specified; AutoRA serves primarily as a methodological testbed rather than reporting breakthrough scientific discoveries in this review.",
            "performance_metrics": "Used to show cases where model-driven adaptive design did not always outperform random sampling (qualitative observation); no numeric performance summary reported here.",
            "comparison_baseline": "Random experimentation, uniform sampling, and model-guided adaptive strategies (used as baselines in testbed experiments).",
            "performance_vs_baseline": "The review notes empirical cases where random or uniform sampling outperformed adaptive/model-guided approaches under certain conditions (e.g., model misspecification), implying nontrivial tradeoffs.",
            "efficiency_gain": null,
            "tradeoff_analysis": "AutoRA-based work highlights the tradeoff between model-driven efficiency gains and robustness: adaptive/model-guided designs can be powerful but vulnerable to misspecification, and simple baselines sometimes outperform them.",
            "optimal_allocation_findings": "Recommendation: empirically evaluate adaptive strategies against simple baselines and consider robustness checks; exploration (diverse sampling) may be necessary before exploitation to avoid premature convergence to misleading models.",
            "uuid": "e2400.5",
            "source_info": {
                "paper_title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "BacterAI",
            "name_full": "BacterAI",
            "brief_description": "An active-learning-driven system for automated study of microbial metabolism that can map metabolic models and benefits from transfer of prior knowledge.",
            "citation_title": "BacterAI maps microbial metabolism without prior knowledge.",
            "mention_or_use": "mention",
            "system_name": "BacterAI",
            "system_description": "System employing active learning to propose and test experiments probing microbial metabolic capabilities; the paper highlights experiments where transfer learning from related species' metabolic models sped discovery for a target species.",
            "application_domain": "Microbial metabolism / systems biology",
            "resource_allocation_strategy": "Active-learning selection of experiments to efficiently map metabolic responses; when provided prior models from related species, the system allocates fewer experiments to achieve comparable model recovery.",
            "computational_cost_metric": null,
            "information_gain_metric": null,
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Active learning drives experiments toward informative perturbations for discovering metabolic structure; transfer learning supports exploitation of prior structure while exploration probes species-specific differences.",
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Number of experimental trials / lab throughput (implicit)",
            "budget_constraint_handling": "The system improves efficiency by leveraging prior knowledge (transfer) to reduce the number of experiments required; exact budget optimization methods not specified in the review.",
            "breakthrough_discovery_metric": "Model recovery efficiency (qualitative): ability to reconstruct metabolic models more quickly when prior models are available.",
            "performance_metrics": "Reported qualitative finding: retraining a metabolic model from a related species resulted in more efficient discovery than learning from scratch in cited experiments; no precise numeric metrics given in this review.",
            "comparison_baseline": "Learning from scratch (no prior) vs. transfer-from-related-species prior.",
            "performance_vs_baseline": "Transfer prior improved efficiency of discovery in cited experiments; exact numerical gains not provided in this review.",
            "efficiency_gain": null,
            "tradeoff_analysis": "Paper notes that incorporating prior knowledge helps efficiency but warns Bayesian/adaptive methods can be misled by incorrect priors; thus there is a tradeoff between leveraging priors for efficiency and robustness to prior misspecification.",
            "optimal_allocation_findings": "Key insight: prior knowledge transfer can speed active discovery, but systems must account for the risk of misleading priors and incorporate robustness checks.",
            "uuid": "e2400.6",
            "source_info": {
                "paper_title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "LLM-agent-ML-research",
            "name_full": "LLM-based Agent for Automating Empirical Machine Learning Research",
            "brief_description": "An LLM-driven agent that automates idea development, experimental design, execution, data analysis, and manuscript writing for empirical ML research; example reported computational cost estimate of producing one article (~$15).",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM-based Agent for Empirical ML Research",
            "system_description": "Pipeline/agent using large language models to generate project ideas, design experiments, run experiments (via code generation and orchestration), analyze results, and auto-generate manuscripts and peer-review content; integrated computational workflows can automate much of the empirical ML research lifecycle.",
            "application_domain": "Empirical machine learning research (improving ML models, automating experiments and analyses)",
            "resource_allocation_strategy": "Automates experimental execution and design choices according to internally generated plans; paper reports aggregate computational cost per article rather than a detailed per-experiment allocation policyâ€”hence resource allocation appears oriented toward minimizing overall computational/dollar cost of producing a coherent study.",
            "computational_cost_metric": "Reported estimated monetary computational cost per produced article (~15 USD) in the cited example; no FLOPs/wall-clock breakdown provided in the review.",
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Not specified in detail in the review; the agent automates iterative empirical workflows and may internally balance exploratory experiments vs. refinement to meet project goals, but explicit mechanisms are not described here.",
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Monetary computational budget (explicit in the reported example), time and compute resources (implicit)",
            "budget_constraint_handling": "Operationalized by minimizing computational expense per assembled article (example cites cost estimate of $15 per article), presumably by selecting experiments and computational steps that fit within resource limits though specifics are not provided.",
            "breakthrough_discovery_metric": "Not specified; focus in cited work was on automating end-to-end research and showing feasibility/cost-efficiency, not on formal breakthrough scoring.",
            "performance_metrics": "Reported example: end-to-end automated production of an empirical ML article with an estimated computational cost of ~15 USD; no scientific-discovery performance metrics reported in this review.",
            "comparison_baseline": "Not directly compared to human-led research in the review; comparison implicitly relates to human time/effort and cost.",
            "performance_vs_baseline": "Paper reports low computational dollar cost for automating a paper, but no controlled experimental baseline comparisons on scientific quality or breakthrough potential are provided in this review.",
            "efficiency_gain": "Reported low monetary cost per article (~15 USD) indicates cost efficiency in computational terms, though broader time/quality tradeoffs are not quantified here.",
            "tradeoff_analysis": "Review notes broader concerns: automation can speed research but raises issues for peer review load, reproducibility, and robustness; specifics about tradeoffs between compute cost and information gain for this agent are not detailed in this paper.",
            "optimal_allocation_findings": "Demonstration-level insight: LLM-based agents can substantially lower per-article computational cost for producing empirical ML studies, but the paper emphasizes need for human oversight, evaluation of quality, and consideration of peer-review and ethical implications.",
            "uuid": "e2400.7",
            "source_info": {
                "paper_title": "Automating the Practice of Science -- Opportunities, Challenges, and Implications",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Adaptive design optimization: A mutual information-based approach to model discrimination in cognitive science",
            "rating": 2,
            "sanitized_title": "adaptive_design_optimization_a_mutual_informationbased_approach_to_model_discrimination_in_cognitive_science"
        },
        {
            "paper_title": "On-the-fly closed-loop materials discovery via bayesian active learning",
            "rating": 2,
            "sanitized_title": "onthefly_closedloop_materials_discovery_via_bayesian_active_learning"
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "rating": 2,
            "sanitized_title": "an_autonomous_laboratory_for_the_accelerated_synthesis_of_novel_materials"
        },
        {
            "paper_title": "Functional genomic hypothesis generation and experimentation by a robot scientist.",
            "rating": 2,
            "sanitized_title": "functional_genomic_hypothesis_generation_and_experimentation_by_a_robot_scientist"
        },
        {
            "paper_title": "AutoRA: Automated Research Assistant for Closed-Loop Computational Discovery.",
            "rating": 2,
            "sanitized_title": "autora_automated_research_assistant_for_closedloop_computational_discovery"
        },
        {
            "paper_title": "BacterAI maps microbial metabolism without prior knowledge.",
            "rating": 2,
            "sanitized_title": "bacterai_maps_microbial_metabolism_without_prior_knowledge"
        },
        {
            "paper_title": "Closed-loop cycles of experiment design, execution, and learning accelerate systems biology model development in yeast.",
            "rating": 1,
            "sanitized_title": "closedloop_cycles_of_experiment_design_execution_and_learning_accelerate_systems_biology_model_development_in_yeast"
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 1,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        }
    ],
    "cost": 0.01899,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>D R A F T Automating the Practice of Science -Opportunities, Challenges, and Implications
September 27, 2024</p>
<p>Sebastian Musslick sebastian.musslick@uos.de 
Institute of Cognitive Science
Department of Cognitive, Linguistic, &amp; Psychological Sciences
Osnabr Ã¼ck University
49090 Osnabr Ã¼ckGermany</p>
<p>Brown University
02912ProvidenceRI, ORCIDUSA</p>
<p>Laura K Bartlett 
Centre for Philosophy of Natural and Social Science
London School of Economics
Lakatos Building, Houghton StreetWC2A 2AELondonUK</p>
<p>ORCID</p>
<p>Suyog H Chandramouli 
Department of Information and Communications Engineering
Department of Computing Science
Aalto University
P.O. Box 110001B) FI-00076Otakaari, AALTOFinland</p>
<p>University of Alberta
8900 114 St NWT6G 2S4EdmontonABCanada</p>
<p>ORCID; d Cognitive Science Program
Indiana University
1101 E 10th St47405BloomingtonINUSA</p>
<p>ORCID</p>
<p>Marina Dubova 
Fernand Gobet 
Centre for Philosophy of Natural and Social Science
London School of Economics
Lakatos Building, Houghton StreetWC2A 2AELondonUK</p>
<p>ORCID</p>
<p>School of Psychology
University of Roehampton
SW15 4JDLondonUK</p>
<p>ORCID</p>
<p>Thomas L Griffiths 
Departments of Psychology and Computer Science
Princeton University
PrincetonNJUSA</p>
<p>ORCID</p>
<p>Jessica Hullman 
Department of Computer Science
Northwestern University
ILUSA</p>
<p>ORCID</p>
<p>Ross D King 
Department of Chemical Engineering and Biotechnology
Department of Computer Science and Engineering
University of Cambridge
CB3 0ASCambridgeUK</p>
<p>Chalmers University of Technology
412 96GothenburgSweden; ORCID</p>
<p>J Nathan Kutz 
Department of Applied Mathematics and Electrical and Computer Engineering
University of Washington
98195SeattleUSA</p>
<p>ORCID</p>
<p>Christopher G Lucas 
School of Informatics
University of Edinburgh
10 Crichton StEH8 9ABUnited Kingdom</p>
<p>ORCID</p>
<p>Suhas Mahesh 
Department of Materials Science and Engineering
University of Toronto
Canada</p>
<p>ORCID</p>
<p>Franco Pestilli 
Department of Psychology and Department of Neuroscience
The University of Texas
AustinTXUSA</p>
<p>ORCID</p>
<p>Sabina J Sloman 
Department of Computer Science
University of Manchester
M13 9PLUK ORCID</p>
<p>William R Holmes </p>
<p>In-stitute of Cognitive Science
Wachsbleiche 27, 49090 Osnabr Ã¼ckGermany</p>
<p>D R A F T Automating the Practice of Science -Opportunities, Challenges, and Implications
September 27, 202498411D25EBD3D0346E02ECC8F32238E910.1073/pnas.XXXXXXXXXXarXiv:2409.05890v1[cs.CY]
Automation transformed various aspects of our human civilization, revolutionizing industries and streamlining processes.In the domain of scientific inquiry, automated approaches emerged as powerful tools, holding promise for accelerating discovery, enhancing reproducibility, and overcoming the traditional impediments to scientific progress.This article evaluates the scope of automation within scientific practice and assesses recent approaches.Furthermore, it discusses different perspectives to the following questions: Where do the greatest opportunities lie for automation in scientific practice?;What are the current bottlenecks of automating scientific practice?; and What are significant ethical and practical consequences of automating scientific practice?By discussing the motivations behind automated science, analyzing the hurdles encountered, and examining its implications, this article invites researchers, policymakers, and stakeholders to navigate the rapidly evolving frontier of automated scientific practice.Automation| Computational Scientific Discovery | Metascience | AI for Science "Though the world does not change with a change of paradigm, the scientist afterward works in a different world."-Thomas S. Kuhn, The Structure of Scientific Revolutions</p>
<p>Automation is transforming every domain of scientific inquiry, from the study of functional genomics in biology (1,2) to the derivation of conjectures in mathematics (3,4).Recent advances in automation are accelerating hypothesis generation in chemistry (5)(6)(7)(8), material discovery in materials science (9,10), and theory development in psychology (11).These breakthroughs are not only garnering attention but also an uptick in funding and prizes dedicated to the automation of scientific practice (12)(13)(14).Furthermore, concurrent advancements in artificial intelligence, software, and computing hardware are setting the stage for even more extensive automation within the scientific process (15)(16)(17).</p>
<p>The impact of automation in industry serves as a parallel to its potential in science.In the early 20th century, industrial automation began with mechanized assembly lines, revolutionizing manufacturing efficiency and output.The introduction of robotics and computer-aided manufacturing marked another leap, enabling precision and consistency previously unattainable by human labor.Today, industry-wide automation facilitates not just cost-efficient mass production, but also customized, adaptable, and intelligent manufacturing processes.This evolution demonstrates the capacity of automation to radically redefine operational paradigms.</p>
<p>Drawing parallels to scientific practice, one can anticipate a similar trajectory of profound change, where automation could accelerate discovery, reshape research methodologies, and redefine the very nature of scientific inquiry.At the same time, automation in industry had significant impacts on workers and the kind of products that dominate the marketplace.It is thus important to consider parallel impacts in the scientific setting which may have negative consequences for science and society.</p>
<p>In this perspective, we evaluate what automation should and can achieve for scientific practice.In doing so, we outline the current state of science automation, drawing on recent examples from different domains of science.Furthermore, we examine technological advancements that open new avenues for automation in science, and discuss current bottlenecks.Finally, we highlight a selection of practical and ethical considerations, and discuss how automation may lead scientists to work in a different world, one where traditional methodologies are redefined and new metaparadigms for science emerge.</p>
<p>What are the bounds of automating scientific practice?</p>
<p>Scientific practice can be defined as the set of methods and processes used by scientists to acquire knowledge about the natural world.Automation, in its broadest sense, refers to the use of technology to perform tasks with minimal human intervention.In the context of scientific practice, automation specifically denotes the use of technological tools and systems to carry out scientific tasks or processes traditionally performed by human scientists.</p>
<p>The bounds of automation within scientific practice hinge on at least two questions: First, is there a desire and justification for automating a given scientific practice?This question touches upon goal-related bounds-the alignment of automation with the overarching goals of science.Second, what factors characterizing D R A F T a scientific practice influence the feasibility of automating that practice?This aspect focuses on the technological bounds, assessing the practicality and potential constraints of applying automation in science.</p>
<p>Goal-related bounds: what automation should (not) achieve.</p>
<p>Science is driven by normative and epistemic goals.Here, we discuss arguments for and against automation serving these goals.</p>
<p>The normative goals of science involve ethical, moral, and societal values guiding both basic and applied science.One such goal may be to enable cheap and fast discoveries that advance human health.Along these lines, automation can serve to yield faster scientific discoveries with fewer resources.This is particularly desirable in the applied sciences, e.g., for identifying novel drugs or treatments.Thus, automation can aid scientific practice if societal needs are clear and research questions are well defined.However, the process of identifying a research question itself requires considering societal needs or the interests of the scientific community.As noted in the Opportunities section below, generative artificial intelligence (AI) can integrate large bodies of literature to identify societally and scientifcally important gaps in our knowledge that are worth filling.However, since the relevant normative considerations inherently depend on evolving human contexts, it can be argued that humans ought to always be involved in and monitor the degree to which scientific practices achieve these objectives (18).Consequently, full automation in these areas might not only be impractical but also undesirable, underscoring the indispensable role of human scientists in addressing the normative dimensions of science.</p>
<p>The epistemic goal of science is to understand the natural world through description, prediction, explanation, and control.As discussed in the sections that follow, advances in machine learning can aid in automating the description or explanation of natural phenomena.Such automation can help reduce human errors and biases, leading to more accurate predictions and better control of natural phenomena.Even more so, automation may help bypass or augment the cognitive capacities of human researchers (19), enabling degrees of prediction and control unachievable for human cognition alone.For example, machine learning models can generate millions of proposals for novel materials that lie beyond human intuition (9).Yet, the increase in precision achieved through automation presents an epistemic dilemma, as automation can limit human understanding.In the basic sciences, advancement of human understanding may be more desirable than merely improving predictability through automation.The complexity of a machine learning model, for example, might enhance its ability to accurately predict new stable materials, but concurrently obscure the process by which these predictions are made for human scientists.This scenario illustrates a potential conflict between the scientific objectives of enhancing prediction, on the one hand, and enabling human understanding, on the other (see Practical Implications).This suggests keeping human scientists involved in the scientific process rather than minimizing their involvement.Meanwhile, in applied sciences and engineering, the focus might shift towards maximizing prediction and control, providing a stronger case for automation of scientific practice.</p>
<p>Technological bounds: what automation can (not) achieve.</p>
<p>The technological bounds of automation hinge on the difficulty of automating scientific tasks.Here, we discuss four factors characterizing this difficulty (Figure 1).opportunities and barriers to automation, thereby guiding the identification of areas within scientific practice where automation can be most effectively implemented or where it may face challenges.</p>
<p>The first factor concerns the availability and quality of inputs that a scientific task requires.Some tasks, such as identifying a research question, rely on diverse and sometimes subjective inputs, including peer opinions, news articles, or funding announcements.Such inputs may not be trustworthy, widely accessible or structured for machine processing, posing a challenge to automation.</p>
<p>Another limiting factor for automation is the computational complexity of algorithms available to perform a scientific task.For example, identifying an appropriate experiment for testing a research question may require taking into account numerous decision variables (e.g., internal validity, resources needed, novelty) and searching an exponentially increasing space of possible experimental paradigms, which can be computationally intractable.</p>
<p>A related, yet often overlooked, factor influencing the automation of scientific tasks is the complexity of required hardware engineering.As stated in Moravec's paradox, sensorimotor tasks, like executing invasive brain recordings or social experiments, require advanced solutions in robotics to facilitate automation, which can pose more significant challenges to automation compared to cognitive tasks (20).</p>
<p>Finally, some tasks are difficult to automate because of the subjectivity of the task goal.Some scientific goals cannot be easily turned into a well-defined objective, which is required to communicate it to a machine.For instance, choosing between scientific models can be a matter of personal preference (21).</p>
<p>While the four factors collectively dictate the automatability of scientific tasks, they can be considered interdependent.For example, the automated discovery of scientific equations long relied on search methods with high computational complexity, such as evolutionary computation or brute force search, to identify a set of equations that best describes a given data set (22,23).However, the ability to collect large datasets cheaply, paired with improvements in computing hardware, enables the application of "data-hungry" but computationally tractable machine learning algorithms for equation discovery (24)(25)(26)(27).This approach reduces computational complexity, illustrating how enhancements in one factor can compensate for limitations in another.</p>
<p>Automation in current scientific practice</p>
<p>Existing approaches to automating science target tasks with readily available inputs, computational complexity and hardware demands that align well with current technological capabilities, and clear task goals.Accordingly, efforts at automatization in science have mostly been confined to tasks characterized by clearly specified objectives and well-defined subtasks, which include instances of quantitative hypothesis generation, experimental design, data collection, and quantitative analysis and inference.While covering all advances D R A F T is out of the scope of this article, we highlight a subset of these approaches, focusing on cases that facilitated novel discoveries.</p>
<p>Hypothesis generation.</p>
<p>Hypothesis generation is the development of testable statements that are based on observations, existing knowledge, or theory.Advances in automated hypothesis generation were primarily driven by two factors: improvements in computer algorithms, and the availability of large datasets.</p>
<p>Initial automated hypothesis formation approaches relied on symbolic reasoning systems.For example, in organic chemistry, logical deduction based on existing knowledge was employed to formulate hypotheses about the chemical constituents of body fluids (28).Furthermore, quantum simulations, facilitated through cloud computing, became the backbone of hypothesis generation for materials properties (29,30).The development of efficient search algorithms further expanded the scope of automated hypothesis formation to areas with large hypothesis spaces (3).For instance, hypothesis generation in mathematics leveraged efficient machine learning algorithms to identify novel conjectures about fundamental constants (3).Finally, deep learning enabled more breakthroughs in chemistry.A landmark achievement in this area is AlphaFold, which predicts 3D protein structures from amino acid sequences, facilitating the development of drugs (6).</p>
<p>The availability of large data sets led to further advances in automated hypothesis formation.One example is the field of biomedicine, where large gene databases led to a surge in hypothesis generation with computational methods, e.g., using data mining and network analysis to propose genes that may be linked to diseases (31,32).Similarly, existing materials databases provided sufficient information for machine learning methods to generate over 2.2 million proposals for novel materials that, so far, escaped human intuition (9).</p>
<p>Experimental design.</p>
<p>The problem of automated experimental design is to systematically identify the most informative experiment to address a particular hypothesis or scientific question.The informativeness of an experiment can be evaluated in various ways.Some automated experimental design methods are geared towards identifying the experimental conditions that minimize the influence of nuisance variables--experimental variables that are not of interest but can pollute the informativeness of intended experimental manipulations (33,34).Other methods aim to find experimental conditions that are well suited to identify a scientific model of interest (35)(36)(37).This problem of experimental design is closely related to the problem of active learning in machine learning research (2,(38)(39)(40), which seeks to identify data points that can best inform a machine learning model when included as training data.A prominent active learning method used for scientific practice is Bayesian optimal experimental design, which has been successfully applied in various fields, including psychology (36,37,41,42), neuroscience (43), physics (44,45), biology (46,47), chemistry (48,49), materials science (50)(51)(52), and engineering (53).For example, in the domain of psychology, Bayesian optimal experimental design led to the discovery of novel models of how humans discount the future relative to the present (54).</p>
<p>While automated experimental design methods can facilitate efficient data collection and strong inferences, their efficacy can be compromised if the underlying assumptions are violated or if the scientific model is incorrectly specified (55)(56)(57).This limitation led to unexpected findings in simulation studies, where random sampling of experimental conditions outperformed automated theory-driven approaches to experimental design (38,58), and where uniform sampling outperformed adaptive approaches in learning continuous relationships (59).</p>
<p>Another limitation of current approaches to automated experimental design pertains to their scope, as they focus on navigating a pre-defined space of experimental manipulations.Exploring novel research directions, however, often involves identifying completely new experimental manipulations (60).</p>
<p>Data collection.Data collection, often a time-consuming and costly aspect of empirical research, is a significant bottleneck in scientific discovery.Accordingly, automated tools for data collection emerged as some of the most impactful innovations in accelerating the pace of science.These tools span a wide range of applications and fields: fitness trackers revolutionized public health studies (61), continuous glucose monitors are providing critical insights into nutrition and diabetes research (62), and automated weather stations enhanced meteorological predictions (63).In addition to providing streams of real-time data for ongoing analysis, these automated systems can minimize human observation and experimenter biases.Experimenter bias occurs when the beliefs, expectations, or preferences of the researcher unconsciously influence the conduct or outcome of an experiment.Automating data collection in animal studies helped to eliminate experimenter bias, resulting in refutations of previous results, such as the evidence for statistical learning ability in newborn chicks (64).A particularly noteworthy advancement in the behavioral sciences was the adoption of web-based experiments, especially during the COVID-19 pandemic.Online platforms and interfaces for recruiting and conducting experiments did not only facilitate the collection of behavioral data at a time when traditional lab-based studies were impractical, but they also broadened the scope and diversity of participants (65)(66)(67).Automating data collection also generated opportunities for automating other elements of behavioral science, such as adopting adaptive experimental designs that change based on the responses of participants (68) or collecting larger datasets that can support the use of machine-learning algorithms (11).</p>
<p>Statistical inference.The automation of statistical inference transformed dramatically from the era of manual computations, a reality echoed in old statistical textbooks filled with computationsimplifying shortcuts.The introduction of computers altered statistical methodologies, sometimes even leading to their replacement by machine learning techniques.For example, modern statistical inference engines, like Stan, leverage techniques such as Markov Chain Monte Carlo (MCMC) for efficient sampling of model parameters (69).Tools for likelihood-free inference enable the analysis of statistical models that are not mathematically tractable.Furthermore, frameworks such as Bayesian Workflow (70) and platforms such as the Automatic Statistician (71) are streamlining complex processes like Bayesian inference and the construction of traditional statistical models.The automation of statistical inference, however, is mostly confined to the deduction of new knowledge based on pre-specified statistical models.</p>
<p>Scientific inference and model discovery.Scientific inference, unlike statistical inference, involves generating hypotheses about observations (abduction) and generalizing from observations to laws or broader theories (induction).The automation of scientific inference is termed computational scientific discovery and has so far centered on identifying models or laws that elucidate specific phenomena (22,23,72).One instance of computational scientific D R A F T discovery involves the identification of equations ("symbolic regression") to uncover quantitative laws governing a given data set.Early efforts relied on heuristic search techniques to rediscover insights from mathematics (73,74) or physics (75).Advances in machine learning and high-performance computing facilitated equation discovery, building on reinforcement learning (26), genetic algorithms (25,76,77), MCMC sampling (78), mixed-integer nonlinear programming (79), or gradient-based search techniques (24,27,80,81).However, most forms of computational model discovery are limited to the rediscovery of existing knowledge.Possible exceptions include the discovery of scaling laws and boundary equations in plasma physics (82) and novel models of human decision-making (11).</p>
<p>Closed-loop automation spanning multiple scientific practices.Demonstrations of successful closed-loop automation in empirical research-implementing iterations between experimental design, data collection and model discovery-mark a significant progression for automated scientific practice.One pioneering example is the robot scientist Adam (Figure 2A), which was the first fully automated machine to discover novel scientific knowledge (2).Adam investigated the functional genomics of the yeast S. cerevisiae, and discovered the function of locally orphan enzymesenzymes known to be in yeast but for which the gene(s) encoding them were unknown.The successor of Adam, Eve, is a robot scientist designed for early-stage drug development (39), which identified chemical compounds that outperformed standard drug screening.Eve's most significant discovery is that triclosan (an antimicrobial compound commonly used in toothpastes) may aid against malaria (39,83,84).Another example of a closed-loop discovery system in biology is Wormbot-AI, a platform designed to autonomously conduct experiments on the longevity of worms, capable of testing thousands of interventions annually (85,86).</p>
<p>Complete automation also gained momentum in materials science and chemistry, where efforts are focused on integrating hypothesis generation, decentralized experimentation, and cloudbased decision-making.For instance, modular robotic platforms, driven by machine learning algorithms, were used to optimize material properties by varying synthesis conditions (87-89).One notable example is A-Lab (Figure 2B), an autonomous laboratory for the solid-state synthesis of inorganic powders, which leverages a combination of active learning and machine learning models trained on the literature, to propose novel material candidates (10).</p>
<p>Additionally, behavioral research became amenable to closedloop automation with the ability to collect data via online experiments.Open-source tools like AutoRA (90) facilitate closed-loop research by integrating automated model discovery, experimental design, and experimentation in empirical research.AutoRA effectively interfaces with web-based platforms for automated data collection, integrating the acquisition of behavioral data from human participants.While the potential to yield novel discoveries stands to test, AutoRA served as a computational testbed for philosophy of science, exposing cases where random experimentation outperforms model-guided experimentation (38).</p>
<p>Finally, researchers introduced an LLM-based agent for automating empirical machine learning research, from idea development and experimental design to execution and data analysis, e.g., for improving existing machine learning models (91).Notably, this system also leveraged LLMs to automate the writing and peer review of the resulting research manuscript, with the computational cost of one article estimated to be just 15 USD.</p>
<p>Future opportunities</p>
<p>Existing approaches for automating scientific practice primarily target tasks for which (a) high-quality data is available, (b) the computational complexity can be addressed by current algorithms, and (c) hardware complexity is manageable.The most promising prospects for future automation in scientific practice are found in tasks traditionally limited by human cognitive capacities.This includes areas requiring the processing of large volumes of highdimensional data or exhaustive literature searches.In this section, we highlight a few technological trends that promise to push the boundaries of science automation along these lines.</p>
<p>Data collection, standardization, and sharing.Advancements in cost-effective data collection, standardization, and sharing significantly boost the automatability of scientific practices, particularly those dependent on empirical data.For example, in the behavioral sciences, the utilization of crowd-sourced experimentation platforms like Amazon Mechanical Turk and Prolific revolutionized the efficiency of behavioral data collection.Additionally, LLMs that can mimic human behavior were proposed as proxies for participants, aiding in the acquisition of large-scale datasets (92).Once acquired, such large-yet cost-efficient-datasets can empower datahungry machine learning algorithms, enabling them to uncover novel, and more precise models of human behavior (93)(94)(95)(96).Largescale data collection, however, still bears significant hardware</p>
<p>D R A F T</p>
<p>challenges, e.g., for collecting biological samples from a large number of participants (see Future challenges).Nevertheless, the data quality needed for automated analysis techniques should be complemented by data standardization and sharing.</p>
<p>Scientific data sharing platforms, such as the Open Science Framework, facilitated the availability and accessibility of data needed for automated analyses and computational discovery.The potential of data sharing and standardization is perhaps best illustrated in materials science, where databases for stable materials enabled the prediction of large quantities of new materials (9).Other scientific domains profit from similar efforts.For example, in neuroscience, archives like DANDI, OpenNeuro, DABI and BossDB allow researchers to share data using community standards (97), such as BIDS for neural data (98).</p>
<p>Combining data-driven and knowledge-driven discovery.</p>
<p>A particularly promising approach to automating scientific discovery is the integration of pre-existing human knowledge into the discovery process.Traditionally, data-driven discovery methods operated with minimal prior knowledge about the specific domain of scientific inquiry.This pure data-driven approach makes such methods particularly susceptible to noisy data.However, recent work demonstrates that incorporating prior theoretical knowledge can significantly aid in recovering scientific models from noisy datasets.For example, Bayesian symbolic regression exhibits greater efficacy in recovering equations from noisy data when given priors about scientific equations extracted from Wikipedia (78,99).Similarly, embedding prior knowledge in the form of general logical axioms proved instrumental in rediscovering complex scientific laws, including Kepler's third law of planetary motion and Einstein's relativistic time-dilation law (79,100).Furthermore, experiments with the BacterAI, which uses active learning for the automated study of microbial metabolisms, have demonstrated the advantage of leveraging relevant prior knowledge (101).Specifically, when the metabolic model trained on one bacterial species was retrained for the species of interest, it more efficiently discovered its metabolic model compared to starting the learning process from scratch, despite the two species differing in their metabolic capabilities.These examples highlight the benefits of combining data-driven and knowledge-driven approaches for automated model discovery.</p>
<p>The benefits of knowledge-driven discovery are, however, fundamentally limited by the quality of prior knowledge.For example, Bayesian adaptive experimentation can be misled if prior knowledge mischaracterizes the data (102,103).Thus, data-driven approaches to computational model discovery become particularly beneficial when dominant scientific models in the empirical sciences are more informed by (wrong) theory versus data.This is evident in computational models of human reinforcement learning, which predominantly rely on classic machine learning algorithms (104).Recent work demonstrated that a data-driven model discovery can uncover novel reinforcement learning models that better explain human learning than traditional models (95).</p>
<p>Finally, a notable area of progress in automated model discovery is the analysis of high-dimensional datasets, such as fluid dynamics captured in video format, through reduced-order modeling.This process involves learning a low-dimensional representation of the dynamics inherent in complex data and then decoding the governing equations of these latent dynamics (105)(106)(107)(108). Similar approaches were developed to automate the discovery of neural data embeddings correlating with behavioral dynamics (109).These approaches promise to extend the reach of automated model discovery to high-dimensional naturalistic datasets.beyond experimental control.</p>
<p>Generative AI and LLMs.Generative AI and LLMs offer paths towards automating scientific practices that have historically been challenging due to their computational complexity and qualitative nature (8,16,91,110).Among these are the synthesis and integration of literature, and documentation of findings.</p>
<p>Researchers argued that LLMs show promise in enhancing literature reviews, a task currently limited by the cognitive constraints and language barriers of human scientists (111,112).Whereas humans may only be able to parse and integrate a few hundred articles into a literature review-the scope of which is heavily influenced by the expertise and biases of the researcher-LLMs may accomplish literature synthesis in the order of thousands or millions of articles.Critically, LLMs can take into account articles written in different languages, thus helping to counter the dominance of Western perspectives in scientific literature.Thus, LLMs can assist in extending or even bypassing human researchers' cognitive limitations.A notable application of LLMs for the purpose of literature synthesis is Elicit, which utilizes LLMs trained on paper abstracts to support and help researchers extract relevant information from the scientific literature (112).Another instance of such assistance is an LLM-based "coscientist" for chemical research, which improved the planning of chemical syntheses based on extensive information available on the internet, and aided in the navigation of extensive hardware documentation (8).Additionally, BrainGPT-an LLM fine-tuned to the neuroscience literature-demonstrated the capability to outperform human experts in predicting the results of neuroscience experiments (113).</p>
<p>Combined with their capability for literature synthesis, LLMs can foster the discovery of new research directions and hypotheses (91).Along these lines, LLMs have the potential to expand experimental design spaces, addressing a common bottleneck in automated scientific practice.While traditional automated experimentation is confined to researcher-defined variables (cf. Figure 2), LLMs could identify novel experimental variables of interest, thus broadening the scope of scientific inquiry.However, it can be argued that LLMs risk rediscovering already known hypotheses and experiments (18).</p>
<p>Once experiments are designed, LLMs may aid in the balanced documentation and communication of the research study, including the automated documentation of research code (114,115).Apart from aiding in the construction of research articles, LLMs can enable automated translation into multiple languages.This advancement is particularly beneficial for non-native English speakers and is an example of how automation and AI can address ethical challenges in science.Nevertheless, literature reviews conducted by human scientists serve not only to synthesize knowledge but also to build and refine the conceptual frameworks of evolving scientists-a process that is critical to scientific training and that is challenged by the overuse of LLMs for literature synthesis.</p>
<p>Future challenges</p>
<p>Despite recent advances and opportunities for the automation of science, there remain substantial obstacles.This section examines technological bounds rooted in four bottlenecks (cf. Figure 1): limited availability and quality of data, intractable computational complexity of certain scientific tasks, lack of required hardware, and subjectivity in assessing the outputs of scientific tasks.These</p>
<p>D R A F T</p>
<p>bottlenecks highlight why barriers to automation remain difficult to surmount in the basic sciences (as opposed to engineering), at least with the technologies and methodologies currently at our disposal.Addressing these challenges will require significant interdisciplinary efforts to identify solutions that enable automation beyond a few selected domains of scientific inquiry.</p>
<p>Limited availability and quality of inputs.Prior applications of computational discovery, such as in chemistry (5,7,116) and materials science (9,10), relied on standardized formats for both data and scientific hypotheses that are easily parsed by machine learning algorithms.However, most tasks of scientific practice rely on a diversity of representations for scientific knowledge.For example, computational models in the natural sciences are expressed in various formats, such as equations embedded in scientific articles or computer code written in different programming languages.Without standardization across disciplines, automated systems face significant challenges in drawing parallels or applying concepts from one domain to another.Efforts to standardize the representation of scientific models and other forms of scientific knowledge promise to ease the automation of scientific practices relying on such knowledge (117).However, even if data is standardized and widely available, ensuring its quality remains critical.For instance, literature synthesis enabled by LLMs may be unfruitful or even misleading if fraudulent or unreproducible papers are included as inputs to these models.Therefore, robust quality control measures must accompany standardization efforts to maintain the integrity and usefulness of automated systems.</p>
<p>Computational complexity.One of the fundamental bottlenecks in the automation of scientific practice lies in the computational complexity of many scientific tasks.For example, complexity analyses within the realm of cognitive science indicate that scientific discovery in cognitive science may be computationally intractable in principle, even with unlimited availability of data (118).These theoretical results suggest that uncovering a definitive "ground-truth" theory may be beyond the reach of computation.</p>
<p>One potential critique of leveraging computational methods for scientific discovery hinges on the incomplete comprehension of the cognitive processes, and the concomitant computational complexity underlying it.One may argue that without a full grasp of how humans tackle scientific inquiries, designing algorithms capable of similar feats seems implausible.However, at least two counterarguments challenge this perspective.First, replicating natural processes is not a prerequisite for solving problems.For instance, modern airplanes achieve superior lift not by emulating the flapping motion of birds but through aerodynamically efficient designs.Second, a deep understanding of cognitive phenomena is not a strict requirement for automation, as evidenced by the capabilities of LLMs to produce coherent natural language sequences without humans having a complete scientific understanding of language generation.Nonetheless, this gap in understanding underscores the importance of implementing robust evaluation methods to ensure the accuracy and mitigate any potential negative impacts of automating scientific processes.</p>
<p>Hardware engineering.The advancement of automated science is significantly hindered by current limitations in laboratory robotics and hardware engineering.For instance, executing complex biological or physics experiments remains challenging.Moreover, while robotic automation has been successfully implemented in certain areas, such as with the robot scientist concept (1,2,101,119), its application is primarily limited to clearly defined engineering problems.Yet, even well-defined engineering problems must manage the noise and variability inherent in the data collected by sensors, which can dramatically affect the reliability of scientific outcomes.Therefore, while progress has been made in automating scientific practice, developing more sophisticated robotics to handle complex, noisy data is crucial for its broader adoption and effectiveness.</p>
<p>The automation of hardware tasks in scientific practice is also hindered by the need for highly specialized equipment, leading to significant capital expenditures, often exceeding millions of dollars.Such custom-built hardware is typically field-specific and lacks versatility for reuse in other scientific domains.This challenge is evident in the limited cross-utilization of hardware between disciplines, as seen in the relatively small amount of equipment that materials scientists have been able to adapt from the more heavily automated field of drug discovery.Addressing this issue requires a strategic approach where, for each scientific field, scientists identify and develop a core set of automated hardware that can deliver the greatest impact.This not only involves designing equipment that meets the unique needs of each field but also balancing specificity with adaptability, to maximize utility and cost-effectiveness.</p>
<p>Subjective goals of scientific tasks.More than in engineering, practices in basic science are inherently subjective in how the outcomes of those practices are evaluated.This challenge is particularly evident in developing AI capable of generating novel and impactful scientific ideas.Novelty and impact involve a high degree of subjectivity and variability, making it difficult for these systems to replicate human judgment in the space of scientific inquiry (16).This issue is compounded by the personal aspect of scientific practice.The selection of scientific projects is guided by the personal experience and perspective of human scientists.Diversity in such perspectives paired with interdisciplinary exchange can lead to a greater diversity of ideas in human scientific systems (120)-a dimension that AI currently cannot emulate without explicit instruction.Furthermore, the lack of standardized solutions in many scientific areas means that automating these tasks risks constraining exploration, which is vital for scientific advancement.</p>
<p>Moreover, interpretation of data patterns and hypothesis generation often necessitates human judgment to translate statistical regularities into meaningful scientific interpretations.Techniques like topic modeling, while effective in identifying text co-occurrence patterns, require human insight to align these patterns with relevant scientific constructs (121).The role of human judgment is perhaps best exemplified in serendipitous discovery, often stemming from unexpected failures or results.For example, Alexander Fleming's discovery of penicillin began with the accidental contamination of a Petri dish.Instead of discarding it, his observation of the bacteria being killed by the mold led to the development of the first antibiotic.These aspects highlight the crucial role of human judgment in scientific discovery.</p>
<p>Implications</p>
<p>Although the automation of science currently faces significant limitations, the extent to which it will evolve in the mid-to longterm remains an open empirical question.As advancements in hardware and algorithms continue, the range of practices subject to automation is likely to expand.In this section, we explore the practical and ethical consequences of this trend.</p>
<p>D R A F T</p>
<p>Practical implications.</p>
<p>The role of human scientists and the paradox of automation.The advancement of automation in scientific practice raises considerations regarding the future role of human scientists.On the one hand, it can be argued that automation reduces the need for human involvement.Scientific discovery systems may become able to monitor themselves and tune themselves to optimal performancepotentially excluding humans from the scientific discovery loop.On the other hand, it can argued that the greater the efficiency of an automated system, the more vital the role of human oversight (122).A critical assumption underlying this "paradox of automation" is that automation is not perfect; the potential for accumulating errors necessitates human intervention.If automation were flawless, human oversight would be unnecessary, and the paradox would not exist.However, for tasks with sufficient complexity and uncertainty, this paradox suggests that, in highly automated environments, human contributions, though less frequent, are more critical.This may specifically apply to tasks that demand subjective assessment or the synthesis of complex data, such as reviewing scientific literature, as well as high-level responsibilities such as strategic allocation of funds for scientific inquiry.</p>
<p>Even in the absence of subjective assessment, there are inherent risks associated with automation.For instance, an error within an automated system can lead to a cascade of compounded errors, persisting and potentially amplifying until the system is either corrected or deactivated.This may be particularly problematic for automation methods whose decisionmaking processes are not completely predictable, as is the case for many machine learning algorithms.This unpredictability raises the issue of responsibility for unintended consequences such as injuries.Given the potential severe legal and financial implications of compounding errors in automation, the involvement of human scientists, even in areas where automation is technically feasible, may prove to be more efficient, practical, and safe in the near future.Thus, the paradox of automation underscores the lasting importance of human expertise and the need for a balanced approach that combines automated systems with human judgment.</p>
<p>Research training.With increased automation of science, there arises a need to reevaluate and adapt scientific education.This new landscape calls for training that encompasses not only traditional scientific knowledge but also skills for effectively working alongside automated scientific discovery systems.For instance, obtaining valuable outputs from LLMs is becoming an essential skill.Moreover, scientists will need to develop competencies in understanding and evaluating the functioning and outputs of automated systems, as is already demanded for statistical software (47).This shift implies a growing demand for engineers, scientists, and technicians proficient in advanced STEM skills.</p>
<p>Research evaluation.The current pace of science is primarily determined by our capacity to carry out the research itself.Laboratory studies in fields like biology and chemistry can take years, contrasting with the relatively quick peer review process.However, if advancements in automation enable research to be conducted and documented several magnitudes faster (91), this could lead to a substantial increase in the rate of research article submissions.Such a scenario would further strain the already pressured peer review system.One potential solution could be the automation of peer review, possibly through the use of LLMs; however, this approach has already faced restrictions and bans in certain contexts due to concerns about its efficacy, reliability, and confidentiality (123).Another potential solution is for journals to require that articles generated by automated systems be accompanied by critical evaluations from corresponding human authors.This ensures that human researchers retain comprehension and oversight of what is being submitted while also serving as initial reviewers of the work generated by their automated systems.Either way, this shift would necessitate a reevaluation of the peer review process, ensuring it remains rigorous and effective in the face of increased scientific productivity.</p>
<p>Scientific methods.The automation of scientific practice has the potential to bring about a shift in scientific methods that goes beyond mere acceleration of scientific discovery.As discussed above, the use of machines for scientific discovery allows us to move beyond the cognitive and physical constraints inherent to human scientists (19).Consider, for example, the principle of parsimony in the construction of scientific models.Traditionally, parsimonious models have been favored for their superior generalization, ease of interpretation and communicability among human scientists.However, as discussed in (21), recent studies suggest that highly complex models can, under certain conditions, surpass the generalization capabilities of simpler ones (124), leading to unprecedented advances in scientific research (e.g., for 3D protein folding (6) or material discovery ( 9)).Moreover, as explored in ( 21), the development of such complex models is often a prerequisite for discovering successful parsimonious models (e.g., (125)(126)(127)).This ability of machines to explore and develop models with a level of complexity beyond what is readily interpretable by humans opens up new avenues for scientific progress, less constrained by human cognitive limitations.However, as discussed above, for basic science, there is epistemic value in human understanding that may outweigh the predictive power of AI scientists.</p>
<p>Another consequence of automation concerns the ways in which empirical research is conducted.For example, automated systems can hypothesize and experiment in design spaces far beyond the reach of human cognitive capabilities (9,119).Furthermore, the ability to collect large amounts of data cheaply may obviate frequent iterations between hypothesis generation, experimental design, and data collection.Instead, with the availability of large data sets, the problem of scientific discovery can be transformed into a model discovery problem more amenable to machine learning (11,94,128).However, it is important to recognize that the success of a one-time large-scale data collection hinges on a well-defined experimental design space and the stability of the system under study, as constant changes in the system can undermine the effectiveness of this approach.Accordingly, adaptive experimental design may be needed to identify suitable design spaces (58).</p>
<p>Ethical implications.</p>
<p>Biases.While human biases influence every aspect of scientific work, automated systems are not immune to bias.They can inherit biases from their creators, the construction process, the data they use, and their training format (129).Examples include discriminatory biases in facial recognition technology (130), unrepresentative sampling in psychological experiments (116), and discrimination in automated participant recruitment processes (131).Moreover, automated literature reviews don't escape the biases inherent to the existing literature.These biases can be democratized and exacerbated by the pace of these systems, especially when D R A F T they are uninterpretable or operate as "black boxes."However, a potential advantage is that biases in automated systems may be easier to correct than in humans, such as by using more diverse data, or by aligning automated systems with societal norms.</p>
<p>Value alignment and responsibility.The risk of harmful biases and outcomes of automated processes call for their value alignment with broader societal norms.This is particularly crucial as automation could potentially ease the path for malevolent entities to conduct research detrimental to society, such as developing chemical or biological weapons.Such outcomes underscore the necessity of ethics dedicated to addressing these issues, ensuring that automated scientific advancements align with human values.</p>
<p>Consequences of automation also bring about the issue of responsibility: If a scientific discovery that affects the wider society is based on an automated process, who is responsible?The accountability for effects arising from harmful scientific practice remains ambiguous-whether it lies with the system's creator, its user, or the implementer of societal changes based on the system's output.This issue parallels broader debates in AI, such as liability in self-driving car accidents or the creation of automated artwork.Additionally, the potential misuse of powerful systems (e.g., a system suggesting harmful drug treatments) necessitates robust safeguards.The same applies to potential violations of data privacy.When automated systems generate contentious theories or design ethically questionable experiments, human oversight and responsibility are imperative.Importantly, ethical guidelines are often formulated by the institutions developing the systems (132), highlighting the need for an external framework that can hold institutions accountable.</p>
<p>Conclusion</p>
<p>While the automation of scientific practice is currently confined mostly to well-defined engineering and discovery problems, there is the potential for automation to pervade a large part of scientific practice.We suggest that this trend represents not merely a series of quantitative changes, such as increased efficiency or precision in science, but brings about a fundamental shift in the conduct of science.The integration of AI into scientific practice has the potential to overcome human cognitive limitations, thereby expanding our capabilities for discovery.Yet, this advance is not without challenges-data availability, computational complexity, engineering demands, and subjectivity of scientific task goals mark the technical boundaries of current automatability.Furthermore, normative goals of science-anchored on societal valuespotentially make complete automation of scientific practice neither desirable nor feasible.Finally, this qualitative shift comes with practical and ethical challenges that call for interdisciplinary and collective efforts from researchers, policymakers, and the broader community to navigate the future of science.</p>
<p>D R A F T</p>
<p>Fig. 1 .
1
Fig. 1.Factors determining the technological reach of automation in scientific practice.</p>
<p>Fig. 2 .
2
Fig. 2. Closed-loop automation systems.(A) Adam for functional genomics.(B) A-Lab for materials science.(C) AutoRA for behavioral science.Dashed boxes list knowledge and processes provided by human researchers.</p>
<p>of 10 -www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX Musslick et al.
PNAS -September 27, 2024 -vol. XXX -no. XX -3
of 10 -www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX Musslick et al.
of 10 -www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX Musslick et al.
PNAS -September 27, 2024 -vol. XXX -no. XX -7
of 10 -www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX Musslick et al.
PNAS -September 27, 2024 -vol. XXX -no. XX -9
of 10 -www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX Musslick et al.
AcknowledgmentsS. Musslick  and S. Mahesh were supported by Schmidt Science Fellows, in partnership with the Rhodes Trust.S. Musslick was also supported by the Carney BRAINSTORM program at Brown University and the National Science Foundation (2318549).S. Mahesh also acknowledges the support of the Acceleration Consortium fellowship.S.J. Sloman acknowledges support from the UKRI Turing AI World-Leading Researcher Fellowship, [EP/W002973/1].S. Chandramouli was supported by the Finnish Center for Artificial Intelligence, and Academy of Finland (328813); he also acknowledges the support from the Jorma Ollila Mobility Grant by Nokia Foundation.L. Bartlett and F. Gobet were supported by European Research Council Grant ERC-ADG-835002-GEMS. T. L. Griffiths was supported by a grant from the NOMIS Foundation.R. D. King was supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation, by Chalmers Artificial Intelligence Research Centre (CHAIR), and by the UK EPSRC grants EP/R022925/2 and EP/W004801/1.The authors thank Solomon Oyakhire for valuable feedback.DisclosuresThe authors have no competing interests to report.
Functional genomic hypothesis generation and experimentation by a robot scientist. Rd King, Nature. 4272004Nature Publishing Group UK London</p>
<p>The automation of science. Rd King, Science. 3242009American Association for the Advancement of Science</p>
<p>Generating conjectures on fundamental constants with the Ramanujan Machine. Raayoni, Nature. 5902021Nature Publishing Group UK London</p>
<p>Advancing mathematics by guiding human intuition with AI. Davies, Nature. 6002021Nature Publishing Group UK London</p>
<p>Synthetic organic chemistry driven by artificial intelligence. R Af De Almeida, Moreira, Rodrigues, Nat. Rev. Chem. 32019Nature Publishing Group UK London</p>
<p>Highly accurate protein structure prediction with AlphaFold. Jumper, Nature. 5962021Nature Publishing Group</p>
<p>Applications of artificial intelligence for organic chemistry: the DENDRAL project. Lindsay Rk, No Title)1980</p>
<p>Autonomous chemical research with large language models. Boiko, Macknight, Kline, Nature. 6242023Nature Publishing Group</p>
<p>Scaling deep learning for materials discovery. Merchant, Nature. 2023</p>
<p>An autonomous laboratory for the accelerated synthesis of novel materials. Nj, Szymanski, Nature. 2023</p>
<p>Using large-scale experiments and machine learning to discover theories of human decision-making. Jc Peterson, Dd Bourgin, Agrawal, Reichman, Griffiths, Science. 3722021American Association for the Advancement of Science</p>
<p>Foundation Models for Scientific Discovery (FoundSci). Velasquez, Def. Adv. Res. Proj. Agency (DARPA) Program Solicitation. 2023</p>
<p>Nobel Turing Challenge: creating the engine for scientific discovery. Kitano, Syst. Biol. Appl. 7292021Nature Publishing Group UK London</p>
<p>The future of fundamental science led by generative closed-loop artificial intelligence. Zenil, arXiv:2307.075222023arXiv preprint</p>
<p>Science in the age of large language models. Birhane, Kasirzadeh, S Leslie, Wachter, Nat. Rev. Phys. pp. 2023Nature Publishing Group UK London</p>
<p>Scientific discovery in the age of artificial intelligence. Wang, Nature. 6202023Nature Publishing Group UK London</p>
<p>How should the advent of large language models affect the practice of science?. Binz, arXiv:2312.037592023arXiv preprint</p>
<p>Cognitive science of augmented intelligence. Dubova, Galesic, Rl Goldstone, Cogn. Sci. 46e132292022Wiley Online Library</p>
<p>Mind children: The future of robot and human intelligence. Moravec, Harv. UP. 1988</p>
<p>Is ockham's razor losing its edge? new perspectives on the principle of model parsimony. Dubova, 10.31222/osf.io/bs5xe2024MetaArXiv preprint</p>
<p>Scientific discovery: Computational explorations of the creative processes. Langley, 1987MIT press</p>
<p>Computational discovery of scientific knowledge in Computational discovery of scientific knowledge: introduction, techniques, and applications in environmental and life sciences. DÅ¾eroski, L Langley, Todorovski, 2007Springer</p>
<p>AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. Sm Udrescu, Adv. Neural Inf. Process. Syst. 332020</p>
<p>Interpretable machine learning for science with PySR and SymbolicRegression. Cranmer, arXiv:2305.015822023jl. arXiv preprint</p>
<p>Discovering symbolic policies with deep reinforcement learning. Landajuela, 2021</p>
<p>GFN-SR: Symbolic Regression with Generative Flow Networks. Li, Marinescu, Musslick, 2023</p>
<p>DENDRAL: a case study of the first expert system for scientific hypothesis formation. Rk Lindsay, Buchanan, Feigenbaum, Lederberg, Artif. intelligence. 611993Elsevier</p>
<p>Materials design and discovery with high-throughput density functional theory: the open quantum materials database (OQMD). Je Saal, Kirklin, Aykol, C Meredig, Wolverton, Jom. 652013Springer</p>
<p>Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. Jain, APL materials. 12013AIP Publishing</p>
<p>BioGraph: unsupervised biomedical knowledge discovery via automated hypothesis generation. Am Liekens, Publisher: BioMed Central. 122011Genome biology</p>
<p>Automated cognome construction and semi-automated hypothesis generation. Jb, Voytek, Voytek, J. neuroscience methods. 2082012Elsevier</p>
<p>SweetPea: A standard language for factorial experimental design. Musslick, Behav. Res. Methods pp. 2020Springer</p>
<p>Mix, a program for pseudorandomization. Van Casteren, Davis, Behav. research methods. 382006Springer</p>
<p>Assessing the distinguishability of models and the informativeness of data. M A Dj Navarro, Pitt, Myung Ij, Cogn. psychology. 492004Elsevier</p>
<p>Optimal experimental design for model discrimination. M A Ji Myung, Pitt, Psychol. review. 1162009American Psychological Association</p>
<p>Adaptive design optimization: A mutual information-based approach to model discrimination in cognitive science. Dr Cavagnaro, M A Myung, Pitt, Kujala, Neural computation. 222010MIT Press</p>
<p>An evaluation of experimental sampling strategies for autonomous empirical research in cognitive science. Musslick, 20234545</p>
<p>Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases. Williams, J. Royal society Interface. 1220141289. 2015The Royal Society</p>
<p>Closed-loop cycles of experiment design, execution, and learning accelerate systems biology model development in yeast. Coutant, Proc. Natl. Acad. Sci. Natl. Acad. SciNational Acad Sciences2019116</p>
<p>QUEST+: A general multidimensional Bayesian adaptive psychometric method. Ab Watson, J. Vis. 172017The Association for Research in Vision and Ophthalmology</p>
<p>Valentin, arXiv:2305.07721Designing Optimal Behavioral Experiments Using Machine Learning. 2023arXiv preprint</p>
<p>Bayesian inference and online experimental design for mapping neural microcircuits. Shababo, Paige, L Pakman, Paninski, Adv. Neural Inf. Process. Syst. 262013</p>
<p>Sequential Bayesian experiment design for optically detected magnetic resonance of nitrogen-vacancy centers. Dushenko, Ambal, Mcmichael, Phys. review applied. 14540362020APS</p>
<p>Simulation-based optimal Bayesian experimental design for nonlinear systems. Huan, Marzouk, J. Comput. Phys. 2322013Elsevier</p>
<p>Robotic search for optimal cell culture in regenerative medicine. Gn Kanda, Elife. 11e770072022Publisher: eLife Sciences Publications Limited</p>
<p>Accelerating bayesian optimization for biological sequence design with denoising autoencoders. Stanton, 2022</p>
<p>Chembo: Bayesian optimization of small organic molecules with synthesizable recommendations. Korovina, 2020</p>
<p>Constrained Bayesian optimization for automatic chemical design using variational autoencoders. Rr Griffiths, Jm Hern Ãndez-Lobato, Chem. science. 112020Royal Society of Chemistry</p>
<p>Bayesian optimization for materials design with mixed quantitative and qualitative variables. Zhang, Apley, Chen, Sci. reports. 102020Nature Publishing Group UK London</p>
<p>On-the-fly closed-loop materials discovery via bayesian active learning. Ag Kusne, Nat. communications. 1159662020</p>
<p>Benchmarking the performance of Bayesian optimization across multiple experimental materials science domains. Liang, Comput. Mater. 71882021Nature Publishing Group UK London</p>
<p>Optimal sensor placement methodology for parametric identification of structural systems. Papadimitriou, J. sound vibration. 2782004Elsevier</p>
<p>Data-driven experimental design and model development using Gaussian process with active learning. Chang, Kim, M A Zhang, J I Pitt, Myung, Cogn. Psychol. 1251013602021Elsevier</p>
<p>Inconsistency of Bayesian Inference for Misspecified Linear Models, and a Proposal for Repairing It. T P Gr Ãœnwald, Van Ommen, Bayesian Analysis. 122017</p>
<p>Rainforth, Foster, Ivanova, Smith, arXiv:2302.14545Modern bayesian experimental design. 2023arXiv preprint</p>
<p>Towards Robust Bayesian Adaptive Design Methods for the Study of Human Behavior. Sj Sloman, 2022Carnegie Mellon UniversityPhD thesis</p>
<p>Against theory-motivated experimentation in science. Dubova, Moskvichev, Zollman, MetaArXiv. June. 242022</p>
<p>Sampling heuristics for active function. Gelpi, Saxena, Lifchits, Buchsbaum, Lucas, Proceedings of the 43rd Annual Meeting of the Cognitive Science Society. (cognitivesciencesociety.org). the 43rd Annual Meeting of the Cognitive Science Society. (cognitivesciencesociety.org)2021</p>
<p>Explore your experimental designs and theories before you exploit them! Behav. Dubova, Sj Sloman, Andrew, Nassar, Musslick, Brain Sci. 472024Cambridge University Press</p>
<p>Wearable activity trackers, accuracy, adoption, acceptance and health impact: A systematic literature review. Shin, J. biomedical informatics. 932019Elsevier</p>
<p>Juvenile Diabetes Research Foundation Continuous Glucose Monitoring Study Group, Effectiveness of continuous glucose monitoring in a clinical care environment: evidence from the Juvenile Diabetes Research Foundation continuous glucose monitoring (JDRF-CGM) trial. Diabetes care. 332010Publisher: Am Diabetes Assoc</p>
<p>Antarctic automatic weather station program: 30 years of polar observation. Ma Lazzara, Weidner, Keller, Thom, Cassano, Bull. Am. Meteorol. Soc. 932012American Meteorological Society</p>
<p>Automated study challenges the existence of a foundational statistical-learning ability in newborn chicks. Sm Wood, Johnson, Wood, Psychol. Sci. 302019Sage Publications Sage CA</p>
<p>psiTurk: An open-source framework for conducting replicable behavioral experiments online. Tm Gureckis, Behav. research methods. 482016Springer</p>
<p>Conducting behavioral research on Amazon's Mechanical Turk. Mason, Suri, Behav. research methods. 442012Springer</p>
<p>Prolific. ac-A subject pool for online experiments. S Palan, Schitter, J. Behav. Exp. Finance. 172018Elsevier</p>
<p>Complex cognitive algorithms preserved by selective social learning in experimental populations. Thompson, Van Opheusden, Sumers, Griffiths, Science. 3762022</p>
<p>Stan: A probabilistic programming language. Carpenter, J. statistical software. 762017NIH Public Access</p>
<p>. Gelman, arXiv:2011.018082020Bayesian workflow. arXiv preprint</p>
<p>The automatic statistician. Autom. machine learning: Methods, systems, challenges pp. Steinruecken, Smith, Janz, Z Lloyd, Ghahramani, 2019Springer International Publishing</p>
<p>Introduction: Scientific discovery in the social sciences. Gobet, Addis, Lane, Sozou, Sci. discovery social sciences. 2019Springer</p>
<p>Integrating quantitative and qualitative discovery: the ABACUS system. Bc Falkenhainer, Michalski, Mach. Learn. 11986Springer</p>
<p>The ubiquity of discovery. Db Lenat, Artif. Intell. 91977Elsevier</p>
<p>Data-driven discovery of physical laws. Langley, Cogn. Sci. 51981Elsevier</p>
<p>Genetic programming for developing simple cognitive models. Bartlett, Pirrone, Javed, Lane, Gobet, 20234545</p>
<p>Automatic generation of cognitive theories using genetic programming. Minds Mach. F Frias-Martinez, Gobet, 2007Springer17</p>
<p>A Bayesian machine scientist to aid in the solution of challenging scientific problems. Guimer Ã€, Sci. advances. 669712020American Association for the Advancement of Science</p>
<p>Combining data and theory for derivable scientific discovery with AI-Descartes. Cornelio, Nat. Commun. 1417772023Nature Publishing Group UK London</p>
<p>A unified framework for deep symbolic regression. Landajuela, Adv. Neural Inf. Process. Syst. 352022</p>
<p>Musslick, Recovering Quantitative Models of Human Information Processing with Differentiable Architecture Search. 20214345</p>
<p>Data driven theory for knowledge discovery in the exact sciences with applications to thermonuclear fusion. Murari, Sci. Reports. 1019858. 2020Nature Publishing Group UK London</p>
<p>Yeast-based automated high-throughput screens to identify anti-parasitic lead compounds. Bilsland, Open Biol. 31201582013The Royal Society</p>
<p>Plasmodium dihydrofolate reductase is a second enzyme target for the antimalarial action of triclosan. Bilsland, Sci. Reports. 810382018Nature Publishing Group UK London</p>
<p>The million-molecule challenge: a moonshot project to rapidly advance longevity intervention discovery. Mb Lee, Blue, M Muir, Kaeberlein, GeroScience pp. 2023Springer</p>
<p>WormBot, an open-source robotics platform for survival and behavior analysis in C. elegans. Jn Pitt, GeroScience. 412019Springer</p>
<p>Machine learning on a robotic platform for the design of polymer-protein hybrids. Mj Tamasi, Adv. Mater. 3422018092022Wiley Online Library</p>
<p>Self-driving laboratory for accelerated discovery of thin-film materials. Bp Macleod, Sci. Adv. 688672020American Association for the Advancement of Science</p>
<p>Closed-loop superconducting materials discovery. Ea Pogue, Comput. Mater. 91812023</p>
<p>AutoRA: Automated Research Assistant for Closed-Loop Computational Discovery. Musslick, Strittmatter, Holland, 2023</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Lu, arXiv:2408.062922024arXiv preprint</p>
<p>Can ai language models replace human participants?. D Dillion, Tandon, K Gu, Gray, Trends Cogn. Sci. 272023</p>
<p>Bk Petersen, Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients in International Conference on Learning Representations. 2021</p>
<p>Beyond playing 20 questions with nature: Integrative experiment design in the social and behavioral sciences. Almaatouq, Behav. Brain Sci. pp. 2022Cambridge University Press</p>
<p>Predictive and Interpretable: Combining Artificial Neural Networks and Classic Cognitive Models to Understand Human Learning and Decision Making. Mk Eckstein, Summerfield, Daw, Miller, 2023Publisher: Cold Spring Harbor Laboratory</p>
<p>Automatic Discovery of Cognitive Strategies with Tiny Recurrent Neural Networks. Ji-An, Benna, Mattar, 2023Publisher: Cold Spring Harbor Laboratory</p>
<p>A comparison of neuroelectrophysiology databases. Subash, Sci. Data. 107192023</p>
<p>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. Kj Gorgolewski, Sci. data. 32016Nature Publishing Group</p>
<p>Bayesian Machine Scientist for Model Discovery in Psychology. Hewson, Strittmatter, Marinescu, S Williams, Musslick, 2023</p>
<p>Evolving scientific discovery by unifying data and background knowledge with ai hilbert. C Cory-Wright, Cornelio, Dash, L El Khadir, Horesh, Nat. Commun. 1559222024</p>
<p>Bacterai maps microbial metabolism without prior knowledge. Ac Dama, Nat. Microbiol. 82023</p>
<p>Characterizing the robustness of Bayesian adaptive experimental designs to active learning bias. Sj Sloman, Oppenheimer, Broomell, Shalizi, arXiv:2205.136982022arXiv preprint</p>
<p>Knowing what to know: Implications of the choice of prior distribution on the behavior of adaptive design optimization. Sj Sloman, Cavagnaro, Broomell, Behav. Res. Methods pp. 2024</p>
<p>Reinforcement learning: An introduction. A G Rs Sutton, Barto, 2018MIT press</p>
<p>Machine Learning Methods for Reduced Order Modeling in Model Order Reduction and Applications: Cetraro, Italy 2021. Jn Kutz, 2023Springer</p>
<p>Reduced order modeling of parametrized systems through autoencoders and SINDy approach: continuation of periodic solutions. Conti, Gobat, Fresca, Manzoni, Frangi, Comput. Methods Appl. Mech. Eng. 4111160722023Elsevier</p>
<p>Dimensionality reduction and reduced-order modeling for traveling wave physics. Mendible, Brunton, Ay Aravkin, Lowrie, Kutz, Theor. Comput. Fluid Dyn. 342020Springer</p>
<p>Automated discovery of fundamental variables hidden in experimental data. Chen, Nat. Comput. Sci. 22022Nature Publishing Group</p>
<p>Learnable latent embeddings for joint behavioural and neural analysis. Schneider, Lee, Mathis, Nature pp. 2023Nature Publishing Group UK London</p>
<p>Large language models for scientific synthesis, inference and explanation. Zheng, arXiv:2310.079842023arXiv preprint</p>
<p>Artificial Intelligence Research in Business and Management: A Literature Review Leveraging Machine Learning and Large Language Models. Guler, Kirshner, Vidgen, Available at SSRN. 45408342023</p>
<p>Elicit: AI literature review research assistant. M A Whitfield, Hofmann, Public Serv. Q. 192023Taylor &amp; Francis</p>
<p>Large language models surpass human experts in predicting neuroscience results. Luo, 2024</p>
<p>Chen, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>HotGPT: How to Make Software Documentation More Useful with a Large Language Model. Su, 2023</p>
<p>The weirdest people in the world?. Henrich, Sj Heine, Norenzayan, Behav. brain sciences. 332010Cambridge University Press</p>
<p>Integrating model development across computational neuroscience, cognitive science, and machine learning. P Gleeson, Neuron. 1112023Elsevier</p>
<p>How hard is cognitive science?. Rich, De Haan, I Wareham, Van Rooij, 20214343</p>
<p>A mobile robotic chemist. Burger, Nature. 5832020Nature Publishing Group UK London</p>
<p>Artificial intelligence and illusions of understanding in scientific research. Messeri, Crockett, Nature. 6272024</p>
<p>Reading tea leaves: How humans interpret topic models. Chang, Gerrish, J Wang, D Boyd-Graber, Blei, Adv. neural information processing systems. 222009</p>
<p>Ironies of automation in Analysis, design and evaluation of man-machine systems. Bainbridge, 1983Elsevier</p>
<p>The Use of Generative Artificial Intelligence Technologies is Prohibited for the NIH Peer Review Process (2023) Published: Available from NIH Grants. National Institutes of Health</p>
<p>Reconciling modern machine-learning practice and the classical bias-variance trade-off. Belkin, Hsu, S Ma, Mandal, Proc. Natl. Acad. Sci. Natl. Acad. Sci2019116</p>
<p>The lottery ticket hypothesis: Finding sparse, trainable neural networks. Frankle, Carbin, arXiv:1803.036352018arXiv preprint</p>
<p>Li, Train big, then compress: Rethinking model size for efficient training and inference of transformers in International Conference on machine learning. 2020</p>
<p>Scaling up psychology via scientific regret minimization. Agrawal, Peterson, Griffiths, Proc. Natl. Acad. Sci. Natl. Acad. SciNational Acad Sciences2020117</p>
<p>Manifesto for a new (computational) cognitive revolution. Griffiths, Cognition. 1352015Elsevier</p>
<p>. L Daston, Galison, Objectivity, 2021Princeton University Press</p>
<p>Gender shades: Intersectional accuracy disparities in commercial gender classification. Buolamwini, Gebru, 2018</p>
<p>Algorithmic equity in the hiring of underrepresented IT job candidates. Cobb Yarger, B Payton, Neupane, Online information review. 442020Emerald Publishing Limited</p>
<p>The ethics of AI ethics: An evaluation of guidelines. Minds machines. Hagendorff, 2020Springer30</p>            </div>
        </div>

    </div>
</body>
</html>
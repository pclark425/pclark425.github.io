<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7208 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7208</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7208</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-259202736</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.10062v1.pdf" target="_blank">Revealing the structure of language model capabilities</a></p>
                <p><strong>Paper Abstract:</strong> Building a theoretical understanding of the capabilities of large language models (LLMs) is vital for our ability to predict and explain the behavior of these systems. Here, we investigate the structure of LLM capabilities by extracting latent capabilities from patterns of individual differences across a varied population of LLMs. Using a combination of Bayesian and frequentist factor analysis, we analyzed data from 29 different LLMs across 27 cognitive tasks. We found evidence that LLM capabilities are not monolithic. Instead, they are better explained by three well-delineated factors that represent reasoning, comprehension and core language modeling. Moreover, we found that these three factors can explain a high proportion of the variance in model performance. These results reveal a consistent structure in the capabilities of different LLMs and demonstrate the multifaceted nature of these capabilities. We also found that the three abilities show different relationships to model properties such as model size and instruction tuning. These patterns help refine our understanding of scaling laws and indicate that changes to a model that improve one ability might simultaneously impair others. Based on these findings, we suggest that benchmarks could be streamlined by focusing on tasks that tap into each broad model ability.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Holistic Evaluation of Language Models <em>(Rating: 2)</em></li>
                <li>Measuring Massive Multitask Language Understanding <em>(Rating: 2)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>Solving Quantitative Reasoning Problems with Language Models <em>(Rating: 2)</em></li>
                <li>Emergent Abilities of Large Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7208",
    "paper_id": "paper-259202736",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Holistic Evaluation of Language Models",
            "rating": 2,
            "sanitized_title": "holistic_evaluation_of_language_models"
        },
        {
            "paper_title": "Measuring Massive Multitask Language Understanding",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Solving Quantitative Reasoning Problems with Language Models",
            "rating": 2,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        },
        {
            "paper_title": "Emergent Abilities of Large Language Models",
            "rating": 1,
            "sanitized_title": "emergent_abilities_of_large_language_models"
        }
    ],
    "cost": 0.005152499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Revealing the structure of language model capabilities</p>
<p>Ryan Burnell rburnell@turing.ac.uk 
Leverhulme Centre for the Future of Intelligence
Universitat Politecnica de Valencia Leverhulme Centre for the Future of Intelligence, University of Cambridge Centre for the Study of Existential Risk
University of Cambridge
The Alan Turing Institute
New Mexico State University
New Mexico State University
University of Cambridge</p>
<p>Han Hao 
Leverhulme Centre for the Future of Intelligence
Universitat Politecnica de Valencia Leverhulme Centre for the Future of Intelligence, University of Cambridge Centre for the Study of Existential Risk
University of Cambridge
The Alan Turing Institute
New Mexico State University
New Mexico State University
University of Cambridge</p>
<p>Andrew R A Conway 
Leverhulme Centre for the Future of Intelligence
Universitat Politecnica de Valencia Leverhulme Centre for the Future of Intelligence, University of Cambridge Centre for the Study of Existential Risk
University of Cambridge
The Alan Turing Institute
New Mexico State University
New Mexico State University
University of Cambridge</p>
<p>Jose Hernandez Orallo 
Leverhulme Centre for the Future of Intelligence
Universitat Politecnica de Valencia Leverhulme Centre for the Future of Intelligence, University of Cambridge Centre for the Study of Existential Risk
University of Cambridge
The Alan Turing Institute
New Mexico State University
New Mexico State University
University of Cambridge</p>
<p>Revealing the structure of language model capabilities</p>
<p>Building a theoretical understanding of the capabilities of large language models (LLMs) is vital for our ability to predict and explain the behavior of these systems. Here, we investigate the structure of LLM capabilities by extracting latent capabilities from patterns of individual differences across a varied population of LLMs. Using a combination of Bayesian and frequentist factor analysis, we analyzed data from 29 different LLMs across 27 cognitive tasks. We found evidence that LLM capabilities are not monolithic. Instead, they are better explained by three well-delineated factors that represent reasoning, comprehension and core language modeling. Moreover, we found that these three factors can explain a high proportion of the variance in model performance. These results reveal a consistent structure in the capabilities of different LLMs and demonstrate the multifaceted nature of these capabilities. We also found that the three abilities show different relationships to model properties such as model size and instruction tuning. These patterns help refine our understanding of scaling laws and indicate that changes to a model that improve one ability might simultaneously impair others. Based on these findings, we suggest that benchmarks could be streamlined by focusing on tasks that tap into each broad model ability.Preprint. Under review.</p>
<p>Introduction</p>
<p>The capabilities of Large language models (LLMs) are growing broader every year. Yet our understanding of the these capabilities remains remarkably narrow. Because these systems are so general, it has proved incredibly challenging to create benchmarks that can provide an informative account of their capabilities and limitations. In an attempt to do so, researchers have developed large benchmarks such as Big-BENCH and HELM [30,19] that test the performance of LLMs on a variety of different tasks. These benchmarks are an important step in the right direction, but they currently lack both explanatory and predictive value-although they might be able to tell us that a model performs poorly on a particular set of tasks, they cannot tell us why the model struggled nor accurately predict how it will behave for a new, untested task.</p>
<p>Part of the problem is that the underlying abilities that enable LLMs to perform well across so many tasks are poorly understood [32,2,3]. We do not yet have a clear idea of how to meaningfully characterize these abilities, how they are structured, or how they are related to one another. We also lack a full understanding of how the properties of a model affect specific aspects of its abilities. Addressing these gaps is vital for our ability to explain and predict the performance of LLMs across diverse deployment scenarios-after all, if we do not understand the capabilities of these models, it is difficult to anticipate their behavior [30]. Moreover, a strong understanding of LLM capabilities would provide more theoretically grounded ways of creating evaluation benchmarks, potentially making these benchmarks both more robust and more efficient. For all these reasons, we aim in this paper to build a deeper understanding of the capabilities of LLMs by drawing on empirical techniques from cognitive science.</p>
<p>Understanding the nature of intelligence has long been a focus of cognitive science. Over the past century, psychometric theories of human intelligence, such as the Cattell-Horn-Carroll theory [5,4,23], have demonstrated that a great deal of the variance among people's cognitive performance on different tasks can be explained by a relatively small set of latent factors that represent broad cognitive abilities. These separable but interrelated abilities are considered to be essential components of human cognition [31,8]. These abilities include the ability to solve complex novel problems, known as fluid intelligence, the ability to comprehend and produce written language, known as reading and writing ability, and the breadth and depth of one's acquired knowledge about the world, known as domain knowledge.</p>
<p>Research into the structure of human cognition has provided a number of benefits. At a high level, this work has allowed us to build a theoretical understanding of the mechanisms that underpin human behavior. At a more practical level, this understanding as allowed us to build robust tests of cognitive abilities [16] that enable the accurate prediction of a wide range of life outcomes, including academic achievement and job performance [8,11]. This understanding also makes it possible to identify and explain cognitive deficits or learning disabilities in specific individuals, which in turn helps inform interventions and treatments [12,25].</p>
<p>In a similar way, there are likely to be many benefits to building an understanding of the structure of large language model capabilities. At a high level, such an understanding would help us figure out how these "black box" models function and how they process information [32]. More practically, this understanding would help inform the construction of evaluation benchmarks that can identify the strengths and limitations of these models, enabling more robust predictions about their behavior on a wide range of tasks. Finally, if we can identify deficiencies in the capabilities of specific models, we can better target efforts to address those deficiencies.</p>
<p>One useful way to build this understanding would be to utilize factor analysis techniques, which can be used to extract latent factors that explain variance across different tasks. The data-driven, "bottomup" nature of these techniques makes them an excellent starting point for empirical investigations into the structure of language model capabilities. This approach also fits with research in cognitive science, which has long used factor analysis to build an understanding of the structure of intelligence in humans and animals.</p>
<p>Until recently, the relatively small number of language models capable of complex behavior had rendered this approach infeasible. However, the recent explosion in the number of LLMs of various scales, architectures, and training regimes now makes it possible to collect data from a varied population of models. In addition, over the past few years, large test benchmarks designed to evaluate the broad capabilities of LLMs have been developed, such as BIG-bench and HELM [30,19]. These benchmarks provide a standardized way of testing LLMs across a variety of tasks, many of which were designed to test specific cognitive abilities. With data from this new population of LLMs across a range of these benchmark tasks, it is now becoming feasible to use an individual differences approach to investigate the structure of language model capabilities.</p>
<p>What hypotheses, though, might we make about the structure of language model capabilities? It seems likely that the capabilities of LLMs are multifaceted. After all, we can find this multifaceted pattern across a number of different species, including humans, pigeons, and chimpanzees [10,29]. This idea also fits with the existing literature on LLMs-for example, there is evidence that new abilities can suddenly "emerge" at particular scales [34,30], which is consistent with the idea that certain abilities can be dissociated from one another. What is more uncertain is how these abilities are structured. For example, can the ability of language models to comprehend language be largely explained by a single broad ability, as is the case in humans [29]? Or are there multiple distinct abilities involved? Similarly, a range of studies have investigated the ability of LLMs to perform various kinds of reasoning, including quantitative reasoning, deductive reasoning, and commonsense reasoning [26,6,18]. But it remains unclear whether these different kinds of tasks rely on a set of distinct abilities or rather on a single, underlying reasoning ability.</p>
<p>Once we have identified the broad abilities of LLMs, we can begin to examine how model properties affect these different abilities. For example, a great deal of work has already been done to examine how the scale of a model affects its performance. In general, there is evidence increasing model size tends to improve performance, a pattern often referred to as "scaling laws" [30,15]. This pattern mirrors findings from cognitive science showing relationships between brain size and intelligence [13,20]. But at the level of specific tasks, the findings from the LLM literature are more complex and difficult to interpret. For example, data from BIG-bench suggests that model scale is closely related to performance on some tasks, but bears no relationship to performance on other tasks [30]. What are we to make of this pattern? One interpretation is that these two sets of tasks rely on different abilities, and that these abilities have differing relationships with model size. In order to disentangle these complex patterns, we need to identify the underlying abilities involved in different tasks and examine the relationships between these abilities and various model properties. If we can do that, we may be better able to predict how changes to model properties might affect performance in specific domains.</p>
<p>Methods</p>
<p>To investigate the structure of large language model capabilities, we drew on a large dataset released by the creators of the HELM benchmark [19] that includes performance data from a variety of LLMs. We used this dataset to extract latent factors representing the broad abilities of LLMs, and then examined the relationships between those capabilities and model properties, such as model size and instruction tuning. The full dataset and analysis code are available on Github https://github.com/RyanBurnell/revealing-LLM-capabilities/.</p>
<p>HELM data</p>
<p>Here, we analysed benchmark data from 29 LLMs tested on the HELM benchmark [19]. The models vary in size, release date, training data, and architecture. These include variants of GPT-3 and InstructGPT as well as models from Cohere, Anthropic, Google, Microsoft/NVIDIA, and AI21. Models were only included in the dataset if they contained missing data on fewer than 3 of the included tasks. A full list of the models and their characteristics can be found in Table 1. These characteristics were drawn from a survey of LLMs [35] as well as the papers introducing each model. Instruction tuning (IT) refers to the process of fine tuning the model to respond to instructions and answer questions. Reinforcement learning from human feedback (RLHF) refers to the process of fine tuning the model to provide responses that are liked by humans.</p>
<p>Task selection</p>
<p>The HELM dataset contains performance data from 34 different tasks. However, several of these tasks contained missing data for a number of models. Several other tasks lacked a clear cognitive basis (e.g., the data imputation task). In addition, the NaturalQuestions task includes two very similar variations (open book and closed book). Because we were limited by the number of models included in the HELM dataset, we removed these tasks to minimize the number of parameters to be estimated by the factor analysis models. The full list of included tasks can be found in Figure 1, and more details about each task can be found in the HELM benchmark paper [19]. Note that we included two versions of the synthetic reasoning task-one based on abstract symbols (A), and one based on natural language (NL).</p>
<p>Task demand coding</p>
<p>To assist with the interpretation of any extracted factors, a cognitive science expert (the first author) annotated each task according to the primary cognitive ability tested by the task. The task annotations can be found in Figure 1. Note that MMLU was annotated as mixed because it includes a range of sub-tasks, some of which rely primarily on domain knowledge, and others that rely primarily on fluid reasoning [14].  Table 1: The 29 models included in the analysis, the model parameter count (billions), the total number of tokens trained on, the release data of the model and whether the model underwent instruction tuning (IT) and/or reinforcement learning from human feedback (RLHF). "?" denotes that the characteristic could not be determined.</p>
<p>Results</p>
<p>Task correlations</p>
<p>The simplest way to investigate how performance varies across different tasks is to examine the correlations between the tasks across the population of LLMs. Doing so, we find a largely positive correlation manifold, with a mean correlation of r = 0.56, (Med = 0.6, see Appendix A for the full correlation matrix). In general, the correlations were strongest between the tasks that were annotated as relying on the same cognitive abilities, which provides some initial evidence for the idea that these tasks fit together conceptually. But given the complexity of the correlation matrix, and because each task might require more than one ability, looking at the raw correlations does not necessarily tell the full story.</p>
<p>Factor analysis</p>
<p>To build a better model of the abilities that can explain performance across a range of tasks, we draw on factor analytic techniques commonly used to understand the structure of cognition in humans and non-human animals [9,28]. These techniques are designed to extract latent variables based on individual differences in performance across a range of different tasks. Studies using frequentist factor analytic typically employ larger samples than it would be possible to obtain from modern LLMs [22], so we also employed Bayesian factor analysis as a robustness check.</p>
<p>Because we lacked a clear a priori theoretical model of the structure of LLMs, we first aimed to determine the number of factors that best explain the data. To do so, we used the Hull method [21], which aims to balance model fit and model simplicity. This method suggests that a 3-factor solution is most appropriate (see Appendix B for more details).</p>
<p>Based on this result, we conducted a frequentist maximum likelihood exploratory factor analysis (EFA) assuming 3 factors and using the oblimin rotation method (an oblique rotation method that allows the factors to be correlated) [28]. 1 . As expected given the small sample, the model fit statistics were somewhat poor (CFI = 0.70, TLI = 0.61, RMSEA = 0.26). Nonetheless, the analysis produced three clear factors, each of which explains a substantial proportion of the variance across the different LLMs (see Table 2).</p>
<p>F1</p>
<p>F2 F3</p>
<p>Proportion var. explained 0.33 0.31 0.17 Cumulative var. explained 0.33 0.64 0.82 Table 2: Variance explained by each of the three factors from the frequentist EFA Based on the task annotations, a relatively clear picture emerged of the three extracted factors. The right-most three columns in Fig. 1 display the factor loadings for each task on each of the three factors, which can be broadly interpreted as capturing comprehension, language modelling, and reasoning, respectively.</p>
<p>We found that tasks testing comprehension generally loaded strongly on factor 1. These include NaturalQuestions (which involves answering questions about a passage from from wikipedia provided in the context window), XSUM (which involves summarizing articles from the BBC provided in 1 Using an orthogonal rotation method such as varimax produces very similar results. Factor loadings (Bayesian) Factor loadings (Freq.) Figure 1: Task annotations and factor loadings for each task from the Bayesian factor analysis (left) and the frequentist factor analysis (right). Darker greens represent stronger positive loadings, darker reds represent stronger negative loadings. Note that the Bayesian method proposed by Conti et al. [7] only calculates factor loadings with the assigned factor. the context window), and NarrativeQA (which involves answering questions about a book or movie script provided in the context window).</p>
<p>Tasks explicitly testing the language modeling ability of the model through next-token prediction of the model (as measured by bits-per-byte) tended to load strongly on factor 2. These include ICE (which tests next-token prediction on the The International Corpus of English), TwitterAAE (which tests next-token prediction on African-American-aligned and White-aligned English tweets), and The Pile (which tests next-token prediction on an array of English texts from The Pile dataset).</p>
<p>Reasoning tasks generally loaded strongly on factor 3. These include GSM8K (a mathematical reasoning task), both synthetic reasoning tasks (pattern matching tasks designed to measure fluid reasoning), and BBQ (an inductive reasoning task that is used to test for bias in a system).</p>
<p>Several other notable patterns emerged from the loadings. First, the two tasks designed to measure cognitive biases (BBQ and TruthfulQA) loaded strongly on the reasoning factor. This could be because in order to perform well in these tasks, models need to reason about the correct answer "in the moment" and avoid relying on stored knowledge that may lead to an incorrect response. Second, the two deductive reasoning tasks (bAbI and Dyck) loaded only weakly on the reasoning factor, while the commonsense reasoning task (HellaSwag) loaded on the comprehension factor. This may indicate that the reasoning factor primarily represents the model's ability to engage in complex forms of inductive reasoning-a possibility worthy of further investigation.</p>
<p>Finally, we found that the three extracted factors were moderately correlated with one another (see the top 2 rows of Table 3). This pattern fits with findings from cognitive science demonstrating that human cognitive abilities tend to be inter-correlated, despite the fact that they can be dissociated from one another-this is thought to be because complex tasks require tend to require multiple abilities working in combination. [17].  Table 3: Pearson correlations (with 95% confidence intervals) between extracted abilities and model characteristics. Log model size was used due to the exponential distribution of the model sizes in the data.</p>
<p>Bayesian factor analysis</p>
<p>Because of the relatively small population of subjects in our dataset and the somewhat poor fit of the model, some caution is needed when interpreting the results from this analysis. Therefore, to test the robustness of our findings, we next conducted a Bayesian Factor analysis using the method proposed by Conti and colleagues [7] because Bayesian methods are much more robust to small sample sizes [24]. This analysis method determines the optimal factor solution as part of the modeling process, then assigns each task to a single factor and calculates the factor loadings with that factor. In general, Bayesian EFA is relatively more conservative and data-driven than frequentist EFA techniques.</p>
<p>The results of the Bayesian factor analysis align closely with the results from the frequentist analysis, which gives us confidence in the reliability of the findings. As Fig. 1 shows, a clear 3-factor solution emerged, with factor loadings that largely match those from the frequentist analysis. The most notable exception was the LSAT task, which was not assigned to any of the three factors by the Bayesian analysis but loaded moderately with the reasoning factor in the frequentist analysis (which makes sense, given that it was designed as a fluid reasoning task).</p>
<p>Model factor scores</p>
<p>Having identified three clear factors representing different cognitive abilities, we next examined the scores for each model across each factor. The results are displayed in Fig. 2. We find that the InstructGPT davinci models are far and away the most capable when it comes to reasoning, and also score highly on comprehension-second only to the largest Cohere Command beta model. This pattern fits with the excellent scores the InstructGPT models achieve on the HELM benchmark in general [19], but the cause of this high level of ability is worth further investigation-perhaps it is because the InstructGPT models are the largest models in the dataset to have undergone instruction tuning and RLHF. In terms of language modelling, BLOOM and GPT-NeoX scored the highest, in contrast to their mediocre comprehension and reasoning scores. It is also striking that the InstructGPT ada and babbage models had by far the lowest language modeling score of any model, which could indicate these models are too small to effectively fine-tune without disrupting the models' latent representations.  </p>
<p>Model</p>
<p>Relationships with model characteristics</p>
<p>Now that we have identified these three latent abilities, we can start to examine how these abilities are related to different characteristics of the model-namely, model size, instruction tuning, and training length. There were no models that underwent RLHF without instruction tuning, so we did not analyse the relationships with this property. The bottom three rows of Table 3 display the correlations between the models' factor scores and various model characteristics.</p>
<p>We found that model size was positively correlated with all three factors (as a model gets bigger, it gets better at comprehension, language modeling, and reasoning). However, model size was more strongly correlated with comprehension than with either language modeling or reasoning. Plots of the relationship between model size and each factor can be found in Figure 3.</p>
<p>We also found divergence between the three abilities in terms of their relationships to instruction tuning. Instruction tuning was negatively correlated with language modeling, but positively correlated with reasoning. This pattern is logical because instruction tuning explicitly shifts a model away from the optimisation goal of modeling language and instead towards providing responses that fit the goals of the task provided by the user. The relationship with comprehension was not significantly different from zero.</p>
<p>We did not find evidence of clear relationships between any of the factors and the total number of tokens the models were trained on. But this could be due to the fact that there was relatively little variation in the training length across the dataset (see Table 1). To properly test these relationships, we would need data from models that vary substantially in training length, such as those used in other studies of scaling laws [15,33]. Figure 3: Plots of the relationships between log model size (billions) and the extracted factor scores for each factor. Lines represent the linear relationship between the variables with 95% confidence bands.</p>
<p>Discussion</p>
<p>Here, we investigated the structure of language model capabilities using performance data from a range of models on the HELM benchmark tasks. We found evidence that language model capabilities are multidimensional, and identified three factors that appear to correspond to abilities capturing comprehension, language modeling, and reasoning.</p>
<p>To the best of our knowledge, this is the first empirical investigation into the structure of language model capabilities. Our findings suggest that a large proportion of the variance in language model performance can be explained by a small number of latent capabilities. The fact that three factors alone explained so much of the variance in performance across the wide variety of models in the dataset is striking-the included tasks test everything from question answering and summarization to sentiment analysis and various kinds of reasoning [19]. This pattern indicates that although LLMs vary greatly in their capabilities, they might share a common underlying cognitive structure. If this is indeed the case, we may be able to learn a lot about the capabilities of future models by examining current and past systems-a fact that might provide some hope for efforts to understand these systems in the face of breakneck progress.</p>
<p>The data presented here provide useful insights into the underlying abilities needed to complete different kinds of tasks. For instance, the results suggest that various tasks involving comprehension, including question answering, summarization, and sentiment analysis all appear to rely on the same underlying ability, despite having quite different formats. Similarly, our data suggest that mathematical reasoning tasks and other inductive reasoning tasks might rely on a single underlying reasoning ability.</p>
<p>These findings also have a number of implications for benchmark construction. If the performance of LLMs on a wide range of tasks can be largely explained by a small number of broad capabilities, it should be possible to make benchmarks more efficient. Instead of requiring dozens or even hundreds of tasks, we may be able to obtain a good sense of a model's capabilities with only a small number of tasks that provide estimates of a model's reasoning, comprehension, and language modeling abilities. This approach would provide a much more cost-efficient way of gathering useful data on a wide range of models. Such an outcome would be in-keeping with wider efforts to make more streamlined, "lite" benchmarks to combat the prohibitive costs involved in benchmarking LLMs [19,30]. However, many of these streamlined benchmarks, including BIG-bench lite, take a somewhat arbitrary approach to selecting which tasks to include [30]. By contrast, in line with calls to make benchmarks more systematic [3,27], our approach provides an empirically-driven way of selecting benchmark tasks based on the underlying abilities those tasks measure and the unique variance they explain. Moreover, because this approach centres around high-level cognitive abilities that are not tied to any specific task, we can use estimates of these abilities to predict performance on untested tasks, provided we know what abilities the untested tasks are likely to require.</p>
<p>This study is an important first step on the road to understanding language model capabilities, but it should not be thought of as a complete accounting of every possible ability. Although the HELM tasks included in this analysis cover a wide range of the key tasks LLMs are used for, there may be other important abilities that were not tested here (e.g., abilities involved in creative tasks such as writing poetry or novels). The results of any factor analysis depend on the tasks included in the analysis, so it is important that future work examines how the factor structure changes (if at all) when other kinds of tasks are added to the mix. Indeed, another way of looking at these findings is that, despite the fact that the HELM tasks were intended to test a variety of abilities, they really only tap into the three main abilities identified here. From that perspective, our findings suggest that new task paradigms may be needed if we want to tap into abilities beyond comprehension, reasoning, and language modeling.</p>
<p>These findings also further our understanding of how the properties of a model affect its capabilities.</p>
<p>In general, we found support for scaling laws-all three of the extracted factors were positively correlated with model size. However, we also found evidence that scale is not uniformly related to the three abilities. More specifically, we found that model size was more strongly related to comprehension than it was to either reasoning or language modeling, which could partly explain some of the inconsistencies in the findings of studies investigating the relationships between model size and task performance [30,1]. We also found other evidence of dissociations between the different abilities. For example, instruction tuning was negatively related to language modeling ability, but positively related to reasoning. These findings demonstrate that changes to the properties of a model can have differential effects on the different capabilities of the model-changes that improve one ability might actually impair one or more other abilities. These divergences also highlight the importance of considering the specific abilities involved in a set of tasks when attempting to draw conclusions about the effects of particular model properties on task performance.</p>
<p>Finally, our findings demonstrate the broader value of making evaluation data publicly available [3]. Datasets containing evaluation results, such as the one released by the creators of HELM [19], provide an incredibly useful tool for building a deeper understanding of AI systems, and we encourage researchers and organizations to publicly release their own benchmark data wherever possible.</p>
<p>There are, naturally, several limitations to our conclusions. First, given the relatively small sample of LLMs in this study, it is important to replicate these findings in a larger sample of models. To the best of our knowledge, the HELM dataset is currently the most comprehensive public dataset of benchmark data from LLMs, but given the speed at which models are being released, it should soon be possible to obtain larger samples. Additional data could also be obtained by performing ablations of different models and testing each version on the same benchmark. Second, the interpretations of the factors in any factor analysis are inherently somewhat subjective. Although there was a relatively clear divide between the reasoning and comprehension tasks, there were some tasks that did not fit neatly, suggesting that future work is needed to confirm the validity of these interpretations. Third, the tasks were annotated according to the main cognitive ability being tested by the task. But many of the tasks in HELM were designed based on the format of the task (e.g., question answering), rather than based on the cognitive demands of the task (e.g., fluid reasoning). To more cleanly separate out the capabilities of LLMs, it is important that future benchmarks include tasks specifically designed to test for particular cognitive abilities.</p>
<p>Despite these limitations, our findings provide a valuable first step on the path to developing a theoretical understanding the capabilities of large language models. We hope that future work can build on this foundation, using a variety of empirical methods to identify the fundamental building blocks of language model cognition. Given the rapid advancement and widespread deployment of LLMs, we believe a full understanding of these building blocks will be crucial for our ability to predict the complex behavior of these complex prediction machines. </p>
<p>task</p>
<p>BBQ TQA MML GSM MAT SR-A SR-N CC</p>
<p>B Determining Factor Structure</p>
<p>We conducted a number of steps to determine the factor structure of the data. First, we analyzed the scree plot and examined the eigenvalues of each extracted factor (see Figure 5). The patterns suggest a 3 or 4-factor solution is most appropriate Figure 5: Scree plot of eigenvalues associated with each factor. The red line represents the standard cutoff (eigenvalue = 1).</p>
<p>Next, we employed a more systematic method of determining the factor structure-the Hull method [21], which aims to balance the goodness of fit against the degrees of freedom of the model. As shown in Figure 6, this method suggests a 3-factor solution is most appropriate (determining that the potential fourth factor would not improve goodness of fit sufficiently to warrant a more complex model).</p>
<p>These findings align with the Bayesian factor analysis, which determines the most likely number of factors as part of the modelling process. As shown in the posterior distribution displayed in Figure 7, the analysis determined that it is most likely that the data are explained by three underlying factors. Based on this converging evidence, we conducted the final frequentist exploratory factor analysis with 3 factors.  </p>
<p>Figure 2 :
2Factor scores for each model on the three factors based on the frequentist analysis. Darker greens represent higher factor scores (greater levels of ability), while darker reds represent lower factor scores (lower levels of ability). Models are sorted by reasoning score.</p>
<p>Figure 4 :
4LSu NQ HS OQA XSU WF Qu BoQ RAF bAb NQA Dy CNN IMD ICE TP Tw BLi LSA Pearson correlations between tasks (r). Darker greens represent stronger positive correlations, darker reds represent stronger negative correlations. Lines separate tasks according to the assigned factors in the Bayesian factor analysis.</p>
<p>Figure 6 :
6Hull method plot of goodness of fit (f; calculated as 1 -RMSEA), against model degrees of freedom.</p>
<p>Figure 7 :
7Posterior distribution of the number of factors based on the Bayesian Factor Analysis.</p>
<p>ModelSize (B) Total Tokens Release Date IT RLHFAnthropic-LM v4-s3 
52 
4.00E+11 
01/12/2021 
1 
1 
BLOOM 
176.00 
3.66E+11 
01/07/2022 
0 
0 
Cohere Command beta 
52.4 
? 
03/01/2023 
1 
? 
Cohere Command beta 
6.1 
? 
03/01/2023 
1 
? 
Cohere large v20220720 
13.1 
? 
20/07/2022 
0 
0 
Cohere medium v20220720 
6.1 
? 
20/07/2022 
0 
0 
Cohere medium v20221108 
6.1 
? 
08/11/2022 
0 
0 
Cohere small v20220720 
0.41 
? 
20/07/2022 
0 
0 
Cohere xlarge v20220609 
52.4 
? 
09/06/2022 
0 
0 
Cohere xlarge v20221108 
52.4 
? 
08/11/2022 
0 
0 
GPT-3-ada 
2.7 
3.00E+11 
01/05/2020 
0 
0 
GPT-3-babbage 
6.7 
3.00E+11 
01/05/2020 
0 
0 
GPT-3-curie 
13 
3.00E+11 
01/05/2020 
0 
0 
GPT-3-davinci 
175 
3.00E+11 
01/05/2020 
0 
0 
GPT-J 
6 
4.00E+11 
04/06/2021 
0 
0 
GPT-NeoX 
20 
4.00E+11 
14/04/2022 
0 
0 
InstructGPT-text-ada-001 
2.7 
3.00E+11 
27/01/2022 
1 
1 
InstructGPT-text-babbage-001 
6.7 
3.00E+11 
27/01/2022 
1 
1 
InstructGPT-text-curie-001 
13 
3.00E+11 
27/01/2022 
1 
1 
InstructGPT-text-davinci-002 
175 
3.00E+11 
27/01/2022 
1 
1 
InstructGPT-text-davinci-003 
175 
3.00E+11 
28/11/2022 
1 
1 
J1-Grande v1 
17 
? 
01/08/2021 
0 
0 
J1-Grande v2 beta 
17 
? 
01/08/2021 
0 
0 
J1-Jumbo v1 
178 
3.00E+11 
01/08/2021 
0 
0 
J1-Large v1 
7 
3.00E+11 
01/08/2021 
0 
0 
OPT 
175 
1.80E+11 
22/12/2022 
0 
0 
OPT 
66 
1.80E+11 
22/12/2022 
0 
0 
TNLG v2 
530 
3.39E+11 
28/01/2022 
0 
0 
TNLG v2 
6.7 
3.39E+11 
28/01/2022 
0 
0 </p>
<p>Appendix A Correlations between tasksAs an initial effort to examine how the HELM tasks are related to one another, we calculated Pearson correlations between each task. These correlations are displayed inFigure 4. Note that column names are abbreviated to save space-the order corresponds to the row order.
. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel MTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.</p>
<p>Jeffrey Ziegler, Clemens Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Scott Litwin, Gray, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners.</p>
<p>Not a Number: Identifying Instance Features for Capability-Oriented Evaluation. Ryan Burnell, John Burden, Danaja Rutar, Konstantinos Voudouris, Lucy Cheke, José Hernández-Orallo, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence. the Thirty-First International Joint Conference on Artificial IntelligenceRyan Burnell, John Burden, Danaja Rutar, Konstantinos Voudouris, Lucy Cheke, and José Hernández-Orallo. Not a Number: Identifying Instance Features for Capability-Oriented Evalua- tion. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, pages 2827-2835. International Joint Conferences on Artificial Intelligence Organization.</p>
<p>. Ryan Burnell, Wout Schellaert, John Burden, D Tomer, Fernando Ullman, Joshua B Martinez-Plumed, Danaja Tenenbaum, Lucy G Rutar, Jascha Cheke, Melanie Sohl-Dickstein, Douwe Mitchell, Murray Kiela, Ellen M Shanahan, Anthony G Voorhees, Joel Z Cohn, Jose Leibo, Hernandez-Orallo, Rethink reporting of evaluation results in AI. 380(6641Ryan Burnell, Wout Schellaert, John Burden, Tomer D. Ullman, Fernando Martinez-Plumed, Joshua B. Tenenbaum, Danaja Rutar, Lucy G. Cheke, Jascha Sohl-Dickstein, Melanie Mitchell, Douwe Kiela, Murray Shanahan, Ellen M. Voorhees, Anthony G. Cohn, Joel Z. Leibo, and Jose Hernandez-Orallo. Rethink reporting of evaluation results in AI. 380(6641):136-138.</p>
<p>Psychometrics, intelligence, and public perception. John B Carroll, 24John B. Carroll. Psychometrics, intelligence, and public perception. 24(1):25-52.</p>
<p>A check on the theory of fluid and crystallized intelligence with description of new subtest designs. B Raymond, John L Cattell, Horn, 15Raymond B. Cattell and John L. Horn. A check on the theory of fluid and crystallized intelligence with description of new subtest designs. 15(3):139-164.</p>
<p>Transformers as Soft Reasoners over Language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as Soft Reasoners over Language.</p>
<p>Bayesian exploratory factor analysis. Gabriella Conti, Sylvia Frühwirth-Schnatter, James J Heckman, Rémi Piatek, 183Gabriella Conti, Sylvia Frühwirth-Schnatter, James J. Heckman, and Rémi Piatek. Bayesian exploratory factor analysis. 183(1):31-57.</p>
<p>New and emerging models of human intelligence. R A Andrew, Kristof Conway, Kovacs, 6Andrew R. A. Conway and Kristof Kovacs. New and emerging models of human intelligence. 6(5):419-426.</p>
<p>Exploratory Factor Analysis. Exploratory Factor Analysis. R Leandre, Duane T Fabrigar, Wegener, Oxford University PressLeandre R. Fabrigar and Duane T. Wegener. Exploratory Factor Analysis. Exploratory Factor Analysis. Oxford University Press.</p>
<p>The comparative analysis of intelligence. Mary Flaim, Aaron P Blaisdell, 146Mary Flaim and Aaron P. Blaisdell. The comparative analysis of intelligence. 146(12):1174- 1199.</p>
<p>Broad and Narrow CHC Abilities Measured and Not Measured by the Wechsler Scales: Moving Beyond Within-Battery Factor Analysis. Dawn P Flanagan, Vincent C Alfonso, Matthew R Reynolds, 31Dawn P. Flanagan, Vincent C. Alfonso, and Matthew R. Reynolds. Broad and Narrow CHC Abilities Measured and Not Measured by the Wechsler Scales: Moving Beyond Within-Battery Factor Analysis. 31(2):202-223.</p>
<p>Enhancing practice through application of Cattell-Horn-Carroll theory and research: A "third method" approach to specific learning disability identification. Dawn P Flanagan, Catherine A Fiorello, Samuel O Ortiz, 47Dawn P. Flanagan, Catherine A. Fiorello, and Samuel O. Ortiz. Enhancing practice through application of Cattell-Horn-Carroll theory and research: A "third method" approach to specific learning disability identification. 47(7):739-760.</p>
<p>A critique of comparative studies of brain size. D Susan, Candy Healy, Rowe, 274Susan D Healy and Candy Rowe. A critique of comparative studies of brain size. 274(1609):453- 464.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Simonyan, Oriol Vinyals, and Laurent Sifre. Training Compute-Optimal Large Language Models. Erich Elsen, Jack W. RaeJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training Compute-Optimal Large Language Models.</p>
<p>Kaufman Brief Intelligence Test, Second Edition. Alan S Kaufman, Nadeen L Kaufman, Encyclopedia of Special Education. John Wiley &amp; Sons, LtdAlan S. Kaufman and Nadeen L. Kaufman. Kaufman Brief Intelligence Test, Second Edition. In Encyclopedia of Special Education. John Wiley &amp; Sons, Ltd.</p>
<p>Process Overlap Theory: A Unified Account of the General Factor of Intelligence. Kristof Kovacs, Andrew R A Conway, 27Kristof Kovacs and Andrew R. A. Conway. Process Overlap Theory: A Unified Account of the General Factor of Intelligence. 27(3):151-177.</p>
<p>Solving Quantitative Reasoning Problems with Language Models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant MisraAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving Quantitative Reasoning Problems with Language Models.</p>
<p>. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D Manning, Christopher Ré, Diana Acosta-Navas, Drew A Hudson, Eric Zelikman, Esin Durmus, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta KoreedaFaisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan KimHolistic Evaluation of Language ModelsPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic Evaluation of Language Models.</p>
<p>Beyond brain size: Uncovering the neural correlates of behavioral and cognitive specialization. Corina J Logan, Shahar Avin, Neeltje Boogert, Andrew Buskell, Fiona R Cross, Adrian Currie, Sarah Jelbert, Dieter Lukas, Rafael Mares, Ana F Navarrete, Shuichi Shigeno, Stephen H Montgomery, 13Corina J. Logan, Shahar Avin, Neeltje Boogert, Andrew Buskell, Fiona R. Cross, Adrian Currie, Sarah Jelbert, Dieter Lukas, Rafael Mares, Ana F. Navarrete, Shuichi Shigeno, and Stephen H. Montgomery. Beyond brain size: Uncovering the neural correlates of behavioral and cognitive specialization. 13:55-89.</p>
<p>The Hull Method for Selecting the Number of Common Factors. Urbano Lorenzo-Seva, Marieke E Timmerman, A L Henk, Kiers, 46Urbano Lorenzo-Seva, Marieke E. Timmerman, and Henk A. L. Kiers. The Hull Method for Selecting the Number of Common Factors. 46(2):340-364.</p>
<p>Sample size in factor analysis. C Robert, Keith F Maccallum, Shaobo Widaman, Sehee Zhang, Hong, 4Robert C. MacCallum, Keith F. Widaman, Shaobo Zhang, and Sehee Hong. Sample size in factor analysis. 4:84-99.</p>
<p>CHC theory and the human cognitive abilities project: Standing on the shoulders of the giants of psychometric intelligence research. Kevin S Mcgrew, 37Kevin S. McGrew. CHC theory and the human cognitive abilities project: Standing on the shoulders of the giants of psychometric intelligence research. 37(1):1-10.</p>
<p>On Using Bayesian Methods to Address Small Sample Problems. Daniel Mcneish, 23Daniel McNeish. On Using Bayesian Methods to Address Small Sample Problems. 23(5):750- 773.</p>
<p>Cattell-Horn-Carroll (CHC) Theory-Based Assessment With Deaf and Hard of Hearing Children in the School Setting. D Bryan, Miller, 152Bryan D. Miller. Cattell-Horn-Carroll (CHC) Theory-Based Assessment With Deaf and Hard of Hearing Children in the School Setting. 152(5):459-466.</p>
<p>Explain Yourself! Leveraging Language Models for Commonsense Reasoning. Bryan Nazneen Fatema Rajani, Caiming Mccann, Richard Xiong, Socher, Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain Yourself! Leveraging Language Models for Commonsense Reasoning.</p>
<p>. Emily M Inioluwa Deborah Raji, Amandalynne Bender, Emily Paullada, Alex Denton, Hanna, AI and the Everything in the Whole Wide World BenchmarkInioluwa Deborah Raji, Emily M. Bender, Amandalynne Paullada, Emily Denton, and Alex Hanna. AI and the Everything in the Whole Wide World Benchmark.</p>
<p>Lavaan : An R Package for Structural Equation Modeling. Yves Rosseel, 48Yves Rosseel. Lavaan : An R Package for Structural Equation Modeling. 48(2).</p>
<p>The Cattell-Horn-Carroll model of intelligence. W , Joel Schneider, Kevin S Mcgrew, Contemporary Intellectual Assessment: Theories, Tests, and Issues. The Guilford Press3rd EdW. Joel Schneider and Kevin S. McGrew. The Cattell-Horn-Carroll model of intelligence. In Contemporary Intellectual Assessment: Theories, Tests, and Issues, 3rd Ed, pages 99-144. The Guilford Press.</p>
<p>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Aarohi Srivastava et al. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models.</p>
<p>Contemporary theories of intelligence. Robert J Sternberg, Handbook of Psychology: Educational Psychology. John Wiley &amp; Sons, Inc72nd EdRobert J. Sternberg. Contemporary theories of intelligence. In Handbook of Psychology: Educational Psychology, Vol. 7, 2nd Ed, pages 23-44. John Wiley &amp; Sons, Inc.</p>
<p>Artificial cognition: How experimental psychology can help generate explainable artificial intelligence. J , Eric T Taylor, Graham W Taylor, 28J. Eric T. Taylor and Graham W. Taylor. Artificial cognition: How experimental psychology can help generate explainable artificial intelligence. 28(2):454-475.</p>
<p>LLaMA: Open and Efficient Foundation Language Models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Naman Baptiste Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models.</p>
<p>. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Emergent Abilities of Large Language ModelsJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent Abilities of Large Language Models.</p>
<p>. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, A Survey of Large Language ModelsWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A Survey of Large Language Models.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-345 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-345</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-345</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-264426397</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.14540v3.pdf" target="_blank">Evaluating Spatial Understanding of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) show remarkable capabilities across a variety of tasks. Despite the models only seeing text in training, several recent studies suggest that LLM representations implicitly capture aspects of the underlying grounded concepts. Here, we explore LLM representations of a particularly salient kind of grounded knowledge -- spatial relationships. We design natural-language navigation tasks and evaluate the ability of LLMs, in particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and reason about spatial structures. These tasks reveal substantial variability in LLM performance across different spatial structures, including square, hexagonal, and triangular grids, rings, and trees. In extensive error analysis, we find that LLMs' mistakes reflect both spatial and non-spatial factors. These findings suggest that LLMs appear to capture certain aspects of spatial structure implicitly, but room for improvement remains.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e345.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e345.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large transformer-based chat model evaluated for implicit spatial, procedural, and object-relational reasoning using text-only navigation/relational tasks; shows substantial but structure-dependent spatial competence learned from text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Spatial Understanding of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained transformer chat model (exact training details not specified in paper). Operates purely on text input in experiments (no visual or proprioceptive inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-only spatial navigation and relational memory (square/triangle/hexagon/ring/tree)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot prompts describe a map (either incrementally as a local walk or fully as a global map) over discrete graph topologies (square grids, triangular grids, hexagonal grids, rings, and trees). The agent is given a starting vertex and a sequence of navigation actions; object names encountered at visited nodes are provided in text. The model is asked to report the object at a target location after following the specified steps (tests loop closure, path integration, relational queries like 'cousin of X', and grid-size inference from navigation traces).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation; instruction following; multi-step spatial reasoning; relational queries</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational (locations/layouts, action sequences, and relations between labeled objects on a graph)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora (implicit), plus zero-shot textual prompts in evaluation; occasional in-context chain-of-thought/few-shot tests were explored but main results are zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting (main); ablations with local vs global map presentation; ordering manipulations (row-by-row, random, snake, snake+coord); limited chain-of-thought few-shot and program-of-thought/code prompting evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit in model weights and activations; evidence suggests models sometimes use key-value-like encodings when input order affords this (e.g., row-by-row or address-based listing), and can recover 2D structure from sequence (spatial-topology bias in errors). No explicit symbolic map is given by model outputs; internal representation inferred via error analyses (spatial vs temporal biases).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>exact-match accuracy on predicted object(s) (binary correct/incorrect); comparison to random guessing baseline (uniform over visited nodes) and human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Zero-shot aggregate accuracy varies by structure: reported GPT-4 accuracies from experiments: Square ≈ 0.69, Ring ≈ 0.22, Hexagon ≈ 0.05, Triangle ≈ 0.20 (aggregate ≈ 0.29). On grid-size inference: 3x4/4x3: 0.612; 2x6/6x2: 0.103; 4x6/6x4: 0.162; 3x8/8x3: 0.016; 2x12/12x2: 0.005 (all zero-shot). Local (incremental) presentation usually yields higher accuracy than global presentation (example: local > global for 3x3 square and 12-node ring under 8 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Performs well on square-grid tasks (loop closure, short-path integration) and shows spatial-topology bias in errors (predicted objects tend to be topologically near correct location). Can use address-like global listings (row-by-row) to recover 2D structure. Performs better when map is fed in naturally-structured order (row-by-row) or when explicit coordinates are provided (snake+coord). Small-grid grid-size inference works (e.g., 3x4), especially when object tokens accompany directions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Struggles on less common topologies (hexagonal and triangular grids) and for longer navigation sequences; shows non-spatial biases (temporal bias—predicting items proximate in text sequence, starting-position bias), often predicts an object seen nearby in the prompt rather than the correct topological neighbor. Grid-size inference fails for elongated or large rectangles. Global-map prompts can be harder than local incremental prompts. Chain-of-thought and program-of-thought gave limited gains; hexagon/triangle spatial structure often not recovered leading to near-random or systematically biased errors.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Random guessing baseline = 1/8 (0.125) for 8-step local paths; human baseline aggregated ≈ 0.67 (humans outperform GPT-4 overall); GPT-4 outperforms random for many square and some triangle/ring conditions but is worse than humans and fails on hexagon conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Order-of-presentation ablation: row-by-row (baseline) > random > snake; adding explicit coordinates to snake (snake+coord) increased GPT-4 accuracy from 0.44 to 0.525 (for the tested 3x3). Local vs global: local presentation yields higher accuracy than global for square/ring. Chain-of-thought few-shot improved GPT-3.5 somewhat but plateaued; program-of-thought code prompts for CodeLlama did not reliably improve results. Model-size ablation: larger models (GPT-4) substantially outperform smaller variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 implicitly encodes aspects of 2D spatial topology from purely textual sequential inputs (evidenced by spatial-topology bias in error patterns and ability to do loop-closure), but this encoding is structure-dependent (better for square/grid-like layouts seen commonly in text/code) and brittle: non-spatial temporal biases and input-ordering strongly influence behavior. Explicit coordinate signals and structured input orders help, while uncommon topologies (hexagon/triangle) and longer sequences reveal limits of ungrounded LLM spatial representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Spatial Understanding of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e345.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e345.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Predecessor chat model evaluated on the same text-only spatial navigation/relational tasks; generally performs poorly on zero-shot spatial mapping compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Spatial Understanding of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based chat model trained on large text corpora; evaluated zero-shot on text-only navigation tasks without sensory inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-only spatial navigation and relational memory (same experimental suite as GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot prompts that describe walks, global maps, and relational tree queries; model must report the object at a specified place after following textual navigation instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation; instruction following; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora; zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting; some few-shot/chain-of-thought tested (improved performance for GPT-3.5 in preliminary tests but plateaued beyond 3-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit in weights/activations; appears not to reliably form robust spatial/topological maps from sequential text (errors often not spatially local).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>exact-match accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall poor performance across structures; often performs at or below random-guess baseline (random=0.125 for 8-step tasks). For some tree-structured relational queries GPT-3.5 sometimes outperforms its square/ring performance, but absolute values are low (no large numbers reported).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Occasionally succeeds on simpler relational tree queries and very short navigation sequences under favorable input orders or with chain-of-thought few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Failed to maintain accurate local maps over multi-step sequences in most zero-shot spatial tasks; frequently worse than random guessing on square/triangle/hexagon structures; fails to infer rectangle sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to GPT-4: substantially worse across nearly all spatial structures; random baseline often matches or exceeds GPT-3.5 performance in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Preliminary chain-of-thought prompting increased performance for GPT-3.5 in low-shot settings but gains plateaued beyond 3-shot; otherwise no ablation produced robust competence.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Smaller/earlier chat models like GPT-3.5 have limited implicit spatial map formation from text-only sequences; some improvement possible with chain-of-thought prompting but insufficient to close gap with larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Spatial Understanding of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e345.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e345.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 - 70B Chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source large language model (70B parameters) evaluated on text-only navigation and relational tasks; shows similar qualitative patterns as GPT-4 but notably lower absolute accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Spatial Understanding of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama 2 chat-finetuned variant (70B parameters) used in zero-shot experiments; operates on text-only prompts without sensory grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-only spatial navigation and relational memory (square/ring/tree/triangle/hexagon)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same suite of map-following and relational queries as for GPT-4/GPT-3.5; includes local vs global presentation and order-of-presentation manipulations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation; relational reasoning; instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training and chat fine-tuning on text corpora; zero-shot prompts</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting; evaluation includes different map presentation orders and traversal styles (depth-first vs breadth-first for trees).</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit representations in model weights; in many conditions performance is very low suggesting failure to form robust maps from sequential text alone for this model size/fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>exact-match accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Generally poor; e.g., for a 3x3 row-by-row square prompt Llama2-70B accuracy reported ≈ 0.04 in one evaluated condition (while GPT-4 achieved ≈ 0.55 for that condition). Patterns track GPT-4 qualitatively (square > ring/triangle/hexagon) but at lower absolute levels.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Occasional correct responses on simpler/local tasks or when input ordering strongly supports a key-value/address-style mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Often fails across most structures (especially rings, hexagons, triangles) and under global presentation; smaller Llama models (7B/13B) were near-zero accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Performs worse than GPT-4 and often at or below random baseline for many tasks; human baseline superior.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Performance degrades severely under snake ordering; adding explicit coordinates can help but absolute performance remains low. Breadth-first vs depth-first traversal descriptions for tree tasks produced similar (low) results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large open models like Llama2-70B can exhibit the same qualitative sensitivities to input ordering and topology as GPT-4 but require larger model capacity or other changes to reach comparable spatial understanding when operating without sensory grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Spatial Understanding of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e345.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e345.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-7B/13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 - 7B / 13B Chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller Llama 2 chat models (7B and 13B parameters) evaluated on the spatial suite and found to have near-zero performance in zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Spatial Understanding of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B / Llama2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller Llama 2 chat variants; evaluated zero-shot on text-only navigation/relational tasks without sensory grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-only spatial navigation and relational memory (same tasks described above)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot map-following, loop-closure, relational tree queries and grid-size inference using purely textual navigation traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation; instruction following; relational reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora; zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit in model weights but capacity appears insufficient to form reliable spatial/topological maps in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>exact-match accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Near-zero accuracy across most structures and tasks in zero-shot evaluation (authors report 'zero (or very close to zero) accuracy' for these models).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Essentially none in the tested zero-shot settings for multi-step spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Failed to track object locations across multi-step navigation; cannot reliably answer loop-closure or grid-size inference queries.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially worse than GPT-4 and Llama2-70B; often at floor performance below random baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No ablation produced notable competence; increasing model size to 70B improved results qualitatively but remained far below GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model capacity is critical: small chat models here do not implicitly form usable spatial maps from sequential text in zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Spatial Understanding of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e345.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e345.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeLlama-34B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeLlama 34B (Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-optimized LLM (34B) tested for spatial navigation tasks both by direct prompting and by program-of-thought (PoT) code generation; produced modest/variable results and PoT did not reliably improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Spatial Understanding of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLlama-34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Code-oriented large language model (34B) used in instruct/chat mode for experiments; authors also prompted it to generate Python code (Program-of-Thought) to solve navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-only spatial navigation and relational memory (square grid size 3 and other structures)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same evaluation tasks: follow navigation instructions on textual maps and return object at specified location. For PoT experiments, model was asked to write code that simulates navigation and returns the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation; instruction following; programmatic planning (when using PoT)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text and code corpora; zero-shot prompting and program-of-thought prompts</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot prompting; Program-of-Thought code generation (PoT) for some experiments</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Implicit in weights; PoT attempts to use explicit procedural code (programmatic simulation) but generated programs were often incorrect for navigation and did not systematically improve accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>exact-match accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported accuracies for 3x3 square: CodeLlama-34b-Instruct (no PoT) 0.25; CodeLlama-34b (PoT) 0.20 in one tested setting (Table 6). Overall pattern similar to GPT-4 but lower absolute accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>When generated code was correct, PoT provided an interpretable plan and answer; otherwise PoT failures led to wrong navigation and incorrect answers. Works modestly on small/simpler grids.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Generated code often contained incorrect navigation logic, limiting gains from PoT. Struggles with complex topologies (hexagon/triangle) and longer sequences; PoT did not generalize improvements beyond simple grids.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Underperforms GPT-4; small improvement over random in some settings but not consistently. PoT did not improve over non-PoT prompting in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>PoT vs non-PoT: no consistent improvement; CodeLlama-13b/34b PoT experiments showed similar or slightly worse accuracies compared to direct prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Programmatic prompting (PoT) can make the model's reasoning explicit, but generated programs are brittle for spatial navigation and do not reliably improve performance; code-specialized models still depend on input order and model capacity to recover spatial structure from text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Spatial Understanding of Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mapping Language Models to Grounded Conceptual Spaces <em>(Rating: 2)</em></li>
                <li>Inner Monologue: Embodied Reasoning through Planning with Language Models <em>(Rating: 2)</em></li>
                <li>Code as Policies: Language Model Programs for Embodied Control <em>(Rating: 2)</em></li>
                <li>Evaluating Cognitive Maps and Planning in Large Language Models with CogEval <em>(Rating: 2)</em></li>
                <li>Sparks of Artificial General Intelligence: Early experiments with GPT-4 <em>(Rating: 2)</em></li>
                <li>Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color <em>(Rating: 1)</em></li>
                <li>SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments <em>(Rating: 2)</em></li>
                <li>StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-345",
    "paper_id": "paper-264426397",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A large transformer-based chat model evaluated for implicit spatial, procedural, and object-relational reasoning using text-only navigation/relational tasks; shows substantial but structure-dependent spatial competence learned from text.",
            "citation_title": "Evaluating Spatial Understanding of Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_description": "Large pre-trained transformer chat model (exact training details not specified in paper). Operates purely on text input in experiments (no visual or proprioceptive inputs).",
            "task_name": "Text-only spatial navigation and relational memory (square/triangle/hexagon/ring/tree)",
            "task_description": "Zero-shot prompts describe a map (either incrementally as a local walk or fully as a global map) over discrete graph topologies (square grids, triangular grids, hexagonal grids, rings, and trees). The agent is given a starting vertex and a sequence of navigation actions; object names encountered at visited nodes are provided in text. The model is asked to report the object at a target location after following the specified steps (tests loop closure, path integration, relational queries like 'cousin of X', and grid-size inference from navigation traces).",
            "task_type": "navigation; instruction following; multi-step spatial reasoning; relational queries",
            "knowledge_type": "spatial+procedural+object-relational (locations/layouts, action sequences, and relations between labeled objects on a graph)",
            "knowledge_source": "pre-training on text corpora (implicit), plus zero-shot textual prompts in evaluation; occasional in-context chain-of-thought/few-shot tests were explored but main results are zero-shot.",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot prompting (main); ablations with local vs global map presentation; ordering manipulations (row-by-row, random, snake, snake+coord); limited chain-of-thought few-shot and program-of-thought/code prompting evaluated.",
            "knowledge_representation": "Implicit in model weights and activations; evidence suggests models sometimes use key-value-like encodings when input order affords this (e.g., row-by-row or address-based listing), and can recover 2D structure from sequence (spatial-topology bias in errors). No explicit symbolic map is given by model outputs; internal representation inferred via error analyses (spatial vs temporal biases).",
            "performance_metric": "exact-match accuracy on predicted object(s) (binary correct/incorrect); comparison to random guessing baseline (uniform over visited nodes) and human baseline.",
            "performance_result": "Zero-shot aggregate accuracy varies by structure: reported GPT-4 accuracies from experiments: Square ≈ 0.69, Ring ≈ 0.22, Hexagon ≈ 0.05, Triangle ≈ 0.20 (aggregate ≈ 0.29). On grid-size inference: 3x4/4x3: 0.612; 2x6/6x2: 0.103; 4x6/6x4: 0.162; 3x8/8x3: 0.016; 2x12/12x2: 0.005 (all zero-shot). Local (incremental) presentation usually yields higher accuracy than global presentation (example: local &gt; global for 3x3 square and 12-node ring under 8 steps).",
            "success_patterns": "Performs well on square-grid tasks (loop closure, short-path integration) and shows spatial-topology bias in errors (predicted objects tend to be topologically near correct location). Can use address-like global listings (row-by-row) to recover 2D structure. Performs better when map is fed in naturally-structured order (row-by-row) or when explicit coordinates are provided (snake+coord). Small-grid grid-size inference works (e.g., 3x4), especially when object tokens accompany directions.",
            "failure_patterns": "Struggles on less common topologies (hexagonal and triangular grids) and for longer navigation sequences; shows non-spatial biases (temporal bias—predicting items proximate in text sequence, starting-position bias), often predicts an object seen nearby in the prompt rather than the correct topological neighbor. Grid-size inference fails for elongated or large rectangles. Global-map prompts can be harder than local incremental prompts. Chain-of-thought and program-of-thought gave limited gains; hexagon/triangle spatial structure often not recovered leading to near-random or systematically biased errors.",
            "baseline_comparison": "Random guessing baseline = 1/8 (0.125) for 8-step local paths; human baseline aggregated ≈ 0.67 (humans outperform GPT-4 overall); GPT-4 outperforms random for many square and some triangle/ring conditions but is worse than humans and fails on hexagon conditions.",
            "ablation_results": "Order-of-presentation ablation: row-by-row (baseline) &gt; random &gt; snake; adding explicit coordinates to snake (snake+coord) increased GPT-4 accuracy from 0.44 to 0.525 (for the tested 3x3). Local vs global: local presentation yields higher accuracy than global for square/ring. Chain-of-thought few-shot improved GPT-3.5 somewhat but plateaued; program-of-thought code prompts for CodeLlama did not reliably improve results. Model-size ablation: larger models (GPT-4) substantially outperform smaller variants.",
            "key_findings": "GPT-4 implicitly encodes aspects of 2D spatial topology from purely textual sequential inputs (evidenced by spatial-topology bias in error patterns and ability to do loop-closure), but this encoding is structure-dependent (better for square/grid-like layouts seen commonly in text/code) and brittle: non-spatial temporal biases and input-ordering strongly influence behavior. Explicit coordinate signals and structured input orders help, while uncommon topologies (hexagon/triangle) and longer sequences reveal limits of ungrounded LLM spatial representations.",
            "uuid": "e345.0",
            "source_info": {
                "paper_title": "Evaluating Spatial Understanding of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5 Turbo",
            "brief_description": "Predecessor chat model evaluated on the same text-only spatial navigation/relational tasks; generally performs poorly on zero-shot spatial mapping compared to GPT-4.",
            "citation_title": "Evaluating Spatial Understanding of Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": null,
            "model_description": "Transformer-based chat model trained on large text corpora; evaluated zero-shot on text-only navigation tasks without sensory inputs.",
            "task_name": "Text-only spatial navigation and relational memory (same experimental suite as GPT-4)",
            "task_description": "Zero-shot prompts that describe walks, global maps, and relational tree queries; model must report the object at a specified place after following textual navigation instructions.",
            "task_type": "navigation; instruction following; multi-step reasoning",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pre-training on text corpora; zero-shot prompting",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot prompting; some few-shot/chain-of-thought tested (improved performance for GPT-3.5 in preliminary tests but plateaued beyond 3-shot).",
            "knowledge_representation": "Implicit in weights/activations; appears not to reliably form robust spatial/topological maps from sequential text (errors often not spatially local).",
            "performance_metric": "exact-match accuracy",
            "performance_result": "Overall poor performance across structures; often performs at or below random-guess baseline (random=0.125 for 8-step tasks). For some tree-structured relational queries GPT-3.5 sometimes outperforms its square/ring performance, but absolute values are low (no large numbers reported).",
            "success_patterns": "Occasionally succeeds on simpler relational tree queries and very short navigation sequences under favorable input orders or with chain-of-thought few-shot prompting.",
            "failure_patterns": "Failed to maintain accurate local maps over multi-step sequences in most zero-shot spatial tasks; frequently worse than random guessing on square/triangle/hexagon structures; fails to infer rectangle sizes.",
            "baseline_comparison": "Compared to GPT-4: substantially worse across nearly all spatial structures; random baseline often matches or exceeds GPT-3.5 performance in these tasks.",
            "ablation_results": "Preliminary chain-of-thought prompting increased performance for GPT-3.5 in low-shot settings but gains plateaued beyond 3-shot; otherwise no ablation produced robust competence.",
            "key_findings": "Smaller/earlier chat models like GPT-3.5 have limited implicit spatial map formation from text-only sequences; some improvement possible with chain-of-thought prompting but insufficient to close gap with larger models.",
            "uuid": "e345.1",
            "source_info": {
                "paper_title": "Evaluating Spatial Understanding of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Llama2-70B",
            "name_full": "Llama 2 - 70B Chat",
            "brief_description": "Open-source large language model (70B parameters) evaluated on text-only navigation and relational tasks; shows similar qualitative patterns as GPT-4 but notably lower absolute accuracy.",
            "citation_title": "Evaluating Spatial Understanding of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Llama2-70B",
            "model_size": "70B",
            "model_description": "Llama 2 chat-finetuned variant (70B parameters) used in zero-shot experiments; operates on text-only prompts without sensory grounding.",
            "task_name": "Text-only spatial navigation and relational memory (square/ring/tree/triangle/hexagon)",
            "task_description": "Same suite of map-following and relational queries as for GPT-4/GPT-3.5; includes local vs global presentation and order-of-presentation manipulations.",
            "task_type": "navigation; relational reasoning; instruction following",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pre-training and chat fine-tuning on text corpora; zero-shot prompts",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot prompting; evaluation includes different map presentation orders and traversal styles (depth-first vs breadth-first for trees).",
            "knowledge_representation": "Implicit representations in model weights; in many conditions performance is very low suggesting failure to form robust maps from sequential text alone for this model size/fine-tuning.",
            "performance_metric": "exact-match accuracy",
            "performance_result": "Generally poor; e.g., for a 3x3 row-by-row square prompt Llama2-70B accuracy reported ≈ 0.04 in one evaluated condition (while GPT-4 achieved ≈ 0.55 for that condition). Patterns track GPT-4 qualitatively (square &gt; ring/triangle/hexagon) but at lower absolute levels.",
            "success_patterns": "Occasional correct responses on simpler/local tasks or when input ordering strongly supports a key-value/address-style mapping.",
            "failure_patterns": "Often fails across most structures (especially rings, hexagons, triangles) and under global presentation; smaller Llama models (7B/13B) were near-zero accuracy.",
            "baseline_comparison": "Performs worse than GPT-4 and often at or below random baseline for many tasks; human baseline superior.",
            "ablation_results": "Performance degrades severely under snake ordering; adding explicit coordinates can help but absolute performance remains low. Breadth-first vs depth-first traversal descriptions for tree tasks produced similar (low) results.",
            "key_findings": "Large open models like Llama2-70B can exhibit the same qualitative sensitivities to input ordering and topology as GPT-4 but require larger model capacity or other changes to reach comparable spatial understanding when operating without sensory grounding.",
            "uuid": "e345.2",
            "source_info": {
                "paper_title": "Evaluating Spatial Understanding of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Llama2-7B/13B",
            "name_full": "Llama 2 - 7B / 13B Chat",
            "brief_description": "Smaller Llama 2 chat models (7B and 13B parameters) evaluated on the spatial suite and found to have near-zero performance in zero-shot settings.",
            "citation_title": "Evaluating Spatial Understanding of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Llama2-7B / Llama2-13B",
            "model_size": "7B / 13B",
            "model_description": "Smaller Llama 2 chat variants; evaluated zero-shot on text-only navigation/relational tasks without sensory grounding.",
            "task_name": "Text-only spatial navigation and relational memory (same tasks described above)",
            "task_description": "Zero-shot map-following, loop-closure, relational tree queries and grid-size inference using purely textual navigation traces.",
            "task_type": "navigation; instruction following; relational reasoning",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pre-training on text corpora; zero-shot prompting",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot prompting",
            "knowledge_representation": "Implicit in model weights but capacity appears insufficient to form reliable spatial/topological maps in these tasks.",
            "performance_metric": "exact-match accuracy",
            "performance_result": "Near-zero accuracy across most structures and tasks in zero-shot evaluation (authors report 'zero (or very close to zero) accuracy' for these models).",
            "success_patterns": "Essentially none in the tested zero-shot settings for multi-step spatial tasks.",
            "failure_patterns": "Failed to track object locations across multi-step navigation; cannot reliably answer loop-closure or grid-size inference queries.",
            "baseline_comparison": "Substantially worse than GPT-4 and Llama2-70B; often at floor performance below random baseline.",
            "ablation_results": "No ablation produced notable competence; increasing model size to 70B improved results qualitatively but remained far below GPT-4.",
            "key_findings": "Model capacity is critical: small chat models here do not implicitly form usable spatial maps from sequential text in zero-shot settings.",
            "uuid": "e345.3",
            "source_info": {
                "paper_title": "Evaluating Spatial Understanding of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CodeLlama-34B",
            "name_full": "CodeLlama 34B (Instruct)",
            "brief_description": "A code-optimized LLM (34B) tested for spatial navigation tasks both by direct prompting and by program-of-thought (PoT) code generation; produced modest/variable results and PoT did not reliably improve performance.",
            "citation_title": "Evaluating Spatial Understanding of Large Language Models",
            "mention_or_use": "use",
            "model_name": "CodeLlama-34B",
            "model_size": "34B",
            "model_description": "Code-oriented large language model (34B) used in instruct/chat mode for experiments; authors also prompted it to generate Python code (Program-of-Thought) to solve navigation tasks.",
            "task_name": "Text-only spatial navigation and relational memory (square grid size 3 and other structures)",
            "task_description": "Same evaluation tasks: follow navigation instructions on textual maps and return object at specified location. For PoT experiments, model was asked to write code that simulates navigation and returns the answer.",
            "task_type": "navigation; instruction following; programmatic planning (when using PoT)",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pre-training on text and code corpora; zero-shot prompting and program-of-thought prompts",
            "has_direct_sensory_input": false,
            "elicitation_method": "zero-shot prompting; Program-of-Thought code generation (PoT) for some experiments",
            "knowledge_representation": "Implicit in weights; PoT attempts to use explicit procedural code (programmatic simulation) but generated programs were often incorrect for navigation and did not systematically improve accuracy.",
            "performance_metric": "exact-match accuracy",
            "performance_result": "Reported accuracies for 3x3 square: CodeLlama-34b-Instruct (no PoT) 0.25; CodeLlama-34b (PoT) 0.20 in one tested setting (Table 6). Overall pattern similar to GPT-4 but lower absolute accuracy.",
            "success_patterns": "When generated code was correct, PoT provided an interpretable plan and answer; otherwise PoT failures led to wrong navigation and incorrect answers. Works modestly on small/simpler grids.",
            "failure_patterns": "Generated code often contained incorrect navigation logic, limiting gains from PoT. Struggles with complex topologies (hexagon/triangle) and longer sequences; PoT did not generalize improvements beyond simple grids.",
            "baseline_comparison": "Underperforms GPT-4; small improvement over random in some settings but not consistently. PoT did not improve over non-PoT prompting in reported experiments.",
            "ablation_results": "PoT vs non-PoT: no consistent improvement; CodeLlama-13b/34b PoT experiments showed similar or slightly worse accuracies compared to direct prompting.",
            "key_findings": "Programmatic prompting (PoT) can make the model's reasoning explicit, but generated programs are brittle for spatial navigation and do not reliably improve performance; code-specialized models still depend on input order and model capacity to recover spatial structure from text.",
            "uuid": "e345.4",
            "source_info": {
                "paper_title": "Evaluating Spatial Understanding of Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mapping Language Models to Grounded Conceptual Spaces",
            "rating": 2,
            "sanitized_title": "mapping_language_models_to_grounded_conceptual_spaces"
        },
        {
            "paper_title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
            "rating": 2,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        },
        {
            "paper_title": "Code as Policies: Language Model Programs for Embodied Control",
            "rating": 2,
            "sanitized_title": "code_as_policies_language_model_programs_for_embodied_control"
        },
        {
            "paper_title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
            "rating": 2,
            "sanitized_title": "evaluating_cognitive_maps_and_planning_in_large_language_models_with_cogeval"
        },
        {
            "paper_title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
            "rating": 2,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color",
            "rating": 1,
            "sanitized_title": "can_language_models_encode_perceptual_structure_without_grounding_a_case_study_in_color"
        },
        {
            "paper_title": "SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments",
            "rating": 2,
            "sanitized_title": "saynav_grounding_large_language_models_for_dynamic_planning_to_navigation_in_new_environments"
        },
        {
            "paper_title": "StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts",
            "rating": 1,
            "sanitized_title": "stepgame_a_new_benchmark_for_robust_multihop_spatial_reasoning_in_texts"
        }
    ],
    "cost": 0.0174795,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Spatial Understanding of Large Language Models
13 Apr 2024</p>
<p>Yutaro Yamada yutaro.yamada@yale.edu 
Yale University</p>
<p>Yihan Bao yihan.bao@yale.edu 
Yale University</p>
<p>Andrew K Lampinen 
Jungo Kasai 
Toyota Technological Institute at Chicago</p>
<p>Ilker Yildirim ilker.yildirim@yale.edu 
Yale University</p>
<p>Evaluating Spatial Understanding of Large Language Models
13 Apr 202463F411977AEE4F988E4603D7FC2CC11BarXiv:2310.14540v3[cs.CL]
Large language models (LLMs) show remarkable capabilities across a variety of tasks.Despite the models only seeing text in training, several recent studies suggest that LLM representations implicitly capture aspects of the underlying grounded concepts.Here, we explore LLM representations of a particularly salient kind of grounded knowledge -spatial relationships.We design natural-language navigation tasks and evaluate the ability of LLMs, in particular GPT-3.5-turbo,GPT-4, and Llama2 series models, to represent and reason about spatial structures.These tasks reveal substantial variability in LLM performance across different spatial structures, including square, hexagonal, and triangular grids, rings, and trees.In extensive error analysis, we find that LLMs' mistakes reflect both spatial and non-spatial factors.These findings suggest that LLMs appear to capture certain aspects of spatial structure implicitly, but room for improvement remains. 1 * Equal contribution.</p>
<p>Introduction</p>
<p>Large language models (LLMs) show remarkable capabilities in language, and also hints of implicitly learning about the grounded concepts beyond language.For example, language models can develop semanticallyorganized internal representations for basic concepts like color and direction (Abdou et al., 2021;Patel &amp; Pavlick, 2022) -which can allow grounding the models with only a few examples.Furthermore Li et al. (2021) demonstrate that internal representations of language models can dynamically track the states of entities and their relations during discourse.Human language use manifests the semantics of the world from which it originates, and thereby might allow LLMs to implicitly learn something about the entities and processes that exist in the physical world.</p>
<p>Natural intelligences extract and use such knowledge of the physical world -often referred to as world models.A particularly salient example is the ability of humans and animals to create and manipulate mental maps, which serves as a fundamental prerequisite for flexibly navigating and interacting with their environments.Cognitive maps (Tolman, 1948) were suggested as a metaphor for mental representations that enable adaptable behavior such as planning routes or finding shortcuts.The quest to uncover how the brain represents such maps has led to significant discoveries about the neural mechanisms underlying such maps, such as place cells (O'Keefe &amp; Dostrovsky, 1971), grid cells (Hafting et al., 2005), and boundary cells (Lever et al., 2009).While navigation generally involves active, grounded experience, some studies suggest that humans use similar representational structures for abstract knowledge as well (e.g.Whittington et al., 2020;Constantinescu et al., 2016).Furthermore, cognitive and neural evidence suggests that humans and animals can learn spatial structure solely from sequences of observations (Whittington et al., 2022;Garvert et al., 2017).This raises an intriguing possibility -that LLMs might also be capable of inferring sophisticated spatial relations from their sequential, text-based inputs.</p>
<p>For example, if a model comprehends a square map's structure, it should be able to answer the following question: "You start at a spot where you find an apple.You move up and find a banana.Then you move right and find an orange.Next, you move down and find a grape.Now, you move left.What do you find?" Answering this question correctly demonstrates an understanding of loop closure, which is a fundamental aspect of this spatial structure.That is, if we have a square grid and an initial location, we can allow the model to take a random walk until it reaches a location it has already visited before.At each newly visited location, we inform the model about the objects it perceives, and then we ask the model which object it would have seen just before reaching the already visited location.By generating such questions synthetically, we can systematically evaluate the spatial understanding of LLMs.For each question, we randomly select the object names from the ImageNet-1k labels to fill every location of the spatial grid to create the underlying map.</p>
<p>Models and evaluation metrics</p>
<p>We test , , Llama2-7B, Llama2-13B, Llama2-70B, and CodeLlama-34B.The decoding parameters we use are: frequency penalty = 0.0, presence penalty = 0.0, temperature = 1.0, top p = 1.0.All Llama models are Llama-Chat models and the CodeLlama model is the Instruct variant.The context window sizes for these models are 4,096 tokens except for which has 8,192 tokens.We focus on zero-shot experiments, where we used the following system prompt: "You are given a task to solve.Make sure to output an answer after "Answer:" without any explanation."</p>
<p>To ensure consistent evaluation, we utilize the following protocol: We first check if the generated text contains the keyword "Answer:".If present, we consider the subsequent text as the model's prediction.In situations where there are multiple ground-truth answers, we store the answers as a set using "," as the separator.We consider the prediction correct only when the generated set of answers matched the ground-truth set exactly.</p>
<p>Figure 1: The spatial structures we examine for the underlying maps include squares, triangles, hexagons and rings.Additionally, we analyze a tree structure to explore its relational nature.</p>
<p>Question: "You have been given a 2 by 2 square grid.Starting from a vertex, you will move along the edges of the grid.Initially, you are positioned at the bottom left corner of the grid, where you find a box turtle.You move right by one step, where you find a table lamp.You move up by one step, where you find an American black bear.You move left by one step, where you find a hand plane.You move down by one step.What will you find?" Answer: "box turtle" Question: "You have been given a circular grid consisting of 4 connected dots.Starting from a vertex, you will move along the edges of the circular grid.Initially, you are positioned on the dot that's located at the top of the grid, where you find a palace.You move around the ring by 1 step in a clockwise direction, where you find a gong.You move around the ring by 2 steps in a clockwise direction, where you find a shopping basket.You move around the ring by 2 steps in a clockwise direction.What will you find?" Answer: "gong" Question: "You have been given a pointy-topped regular hexagonal tile map consisting of 1 tile.Starting from a vertex, you will move along the edges of the tile.Initially, you are positioned at the top corner of the map, where you find an ice pop.You move downright by one step, where you find a Boxer.You move down by one step, where you find a poke bonnet.You move down-left by one step, where you find a combination lock.You move up-left by one step, where you find a spotlight.You move up by one step, where you find a gibbon.You move up-right by one step.What will you find?" Answer: "ice pop"</p>
<p>Results</p>
<p>Do different spatial structure features affect model performance?</p>
<p>In Section 2, we provide an example that utilizes a square grid to assess the understanding of spatial structures in LLMs.In the example, we exploit the concept of loop closure within the square grid for this purpose.Since loop closure also exists in other spatial structures, we also included triangles, hexagons and rings to explore the spatial understanding ability of LLMs on less common 2D structures.Example prompt is in Figure 2. In this section, we are interested in analyzing how different factors contributes to the difficulty of a problem instance such as the number of navigation steps2 in the prompts, which is independent of the specific graph structures; and graph-related features, which includes high-level structure features, like the structure type, and low-level structure features, like the number of edges and vertices.</p>
<p>With these goals, we run a logistic regression analysis to examine what factors influence the prediction accuracy.Specifically, using the square type as the reference level for graph types, we analyze the prediction outcomes via the following model: Prediction correctness ∼ intercept + 1(graph type == hexagon) + 1(graph type == triangle) + 1(graph type == ring) + number of edges + number of navigation steps.</p>
<p>Note that we only pick the number of edges as our low-level graph feature because the correlation between the number of edges and the number of vertices is very high (correlation coefficient was 0.998).We chose logistic regression as our model because the Prediction correctness is a binary variable (for every prompt, the prediction is either correct (Prediction correctness = 1) or wrong (Prediction correctness = 0)).We collect a total of 6,100 prediction results of different prompts using GPT-4 varying the structure type, the number of edges, and the number of navigation steps (i.e.hexagon: 1400 samples, ring: 1500 samples, square: 1800 samples, and triangle: 1400 samples).The summarized results are presented in Table 1 1: Logistic regression results.We see that the number of edges (an example of lower-level features) is not a significant predictor variable for prediction correctness (p value = 0.288).However, the higher-level graph structure (e.g.square or hexagon) is a significant predictor of correctness (all p values &lt; 2e-16).</p>
<p>We observe that the prediction accuracy is not significantly influenced by lower-level structure features (the number of edges).However, it does depend on the structure type.Furthermore, the accuracy is heavily influenced by the number of navigation steps (p value &lt; 2e-16).This finding aligns with our intuition, as longer navigation poses greater challenges in tracking objects in a spatial map.</p>
<p>In the above, we model the difficulty of navigation tasks as a function of several attributes.Now we evaluate the difficulty of the task as a function only of the graph structure, separately from the number of navigation steps.To achieve this, we condition on the number of navigation steps (to be 8) in order to get a measure of the difficulty that stems from graph structures.We use a 3 by 3 square grid, size 2 hexagonal grid, size 3 triangular grid, and size 12 ring grid as the underlying maps.The prompts are generated in a similar manner as Figure 2. We test on several LLMs.The results is in Figure 3.</p>
<p>When comparing GPT-3.5-turbo and GPT-4, we find that GPT-3.5-turboperforms poorly across all spatial structures, while GPT-4 shows higher variation in performance.GPT-4 excels on the Square structure, ranks second best on the Ring and Triangle structures, and performs the worst on the Hexagon structure.3: We compare the accuracy of the models across the different spatial structures.The random guessing accuracy is 1/8 since the predictions from random guessing are uniformly selected from the nodes encountered by the models, which corresponds to the local path with 8 navigation steps.GPT-4 have higher prediction accuracy than random guessing in square, ring and triangle structures, but worse in hexagon.ChatGPT exhibits lower prediction accuracy than random guessing across all of these structures.Llama2-70B and CodeLlama-34B shows a similar pattern to GPT-4.The error bars indicate 1.96 times standard error across 5 runs.</p>
<p>Llama2-70B and CodeLlama-34B, while generally performing worse than GPT-4, exhibit a similar pattern of performance to GPT-4.We omit Llama2-7B and 13B from our discussion because they achieve zero (or very close to zero) accuracy across all structures.This indicates that tackling zero-shot spatial reasoning tasks may necessitate larger models.</p>
<p>The relative ease of the square grid in comparison to other grid types could be attributed to factors such as the prevalence of tabular data and city grid navigation within the model's training data.In addition, coding problems related to maze exploration often involve navigating a two-dimensional square grid, while triangular and hexagonal grids are less commonly encountered.Thus, it is conceivable that such exposure during pre-training makes GPT-4 possess an enhanced understanding of 2D square grids.Analogously, for humans, individuals who grow up in cities with a more grid-like structure may exhibit greater difficulty in navigating through less organized environments, such as older European cities, and vice versa (Coutrot et al., 2022).Additionally, we perform an experiment using the rhombus grid, which was achieved by rotating the square grid 45 degrees.Under the same experimental condition, we find that GPT-4 maintains an accuracy of 0.66, which is slightly lower than the original square-grid accuracy of 0.71 but still significantly higher than other structures.This outcome provides further confirmation that the specific grid structure with two axes contributes to its strong performance.</p>
<p>Is building a local map more difficult than building a full map and retrieving a path?</p>
<p>In the previous section, LLMs were tasked with constructing a local map gradually as they received new information, one step at a time, which we refer to as the "local" setting.Alternatively, we can provide LLMs with the complete map from the start and instruct them to begin exploration from a randomly selected initial location for a specific number of steps, which we refer to as the "global" setting.On one hand, local exploration may be deemed easier as it requires retaining less information.On the other hand, presenting the global map upfront could potentially aid in more accurate map navigation.To address this question, we compare the local and global settings in our spatial understanding task.For this comparison, we use square and ring structures since there are widely accepted methods of specifying global coordinates, making it easier to specify paths from a randomly selected initial position.In particular, we provide the global map information in the following manner: For the square structure, we list the object names row by row.As for the ring structure, we list the object names starting from the top and proceed clockwise.We then have the model follow a fixed number of navigation instructions, just like the local setting.Example prompts for the square and ring structure under the global setting are in Figure 4.</p>
<p>Question: "You have been given a 3 by 3 square grid.In the 1st row, from left to right, we have tiger, rifle, and English Setter.In the 2nd row, from left to right, we have soup bowl, mongoose, and ski.In the 3rd row, from left to right, we have stopwatch, bow, and wolf spider.You start at soup bowl, then you go down by one step, then you go right by one step, then you go right by one step, and then you go up by one step.What will you find?" Answer: "ski."</p>
<p>Question: "You have been given a circular path consisting of 12 connected dots.At the start, you are positioned on the dot that is located at the top of the path, where you find a mop.Moving in a clockwise direction from the mop, the elements on the path are a home theater, a limousine, a vacuum cleaner, a Bedlington Terrier, a fireboat, a red panda, a projector, a seashore, a Redbone Coonhound, a Scottish Terrier, and a laptop computer.Starting from the laptop computer, you move around the ring by 12 steps in a clockwise direction.What will you find?" Answer: "laptop computer."The results in Figure 5 show that the global setting is slightly harder than the local setting for both 3 by 3 square and size-12 ring structures (all under 8 navigation steps), except when the performance is already low for the ring structure for Llama2 models, which shows a less clear pattern as such.</p>
<p>The results presented in Figure 5 show that, in general, the global setting is more challenging than the local setting for both the square and the ring structures.However, this trend becomes less evident when considering the already low performance of Llama2 models on the ring structure, leading to a less clear pattern in this case.</p>
<p>Figure 5: Performance is evaluated on GPT-4, Llama2-70B, and CodeLlama-34B.For both square and ring structures, we observe that the prediction accuracy of GPT-4 using the local map is higher compared to the global map.Llama2-70B and CodeLlama-34B show a similar pattern for the square, while the pattern is less clear for the ring.The error bars indicate 1.96 times standard error across 5 runs.</p>
<p>The order of presenting the map impacts spatial understanding</p>
<p>In the previous section, our approach to providing complete map information upfront to the model has involved a specific method.We describe the items in the map row by row, indicating their positions from left to right.For example, in the first row, we have item A, item B, and item C. In the second row, we have item D, item E, and item F from left to right, and so on.However, there exist multiple ways to convey the same information.Here, we explore different approaches to feeding data into the model.In addition to the aforementioned method, we examine two alternative techniques: random and snake order.The random approach involves placing items in the map at random positions using the global coordinate system.On the other hand, the snake order method follows a specific pattern.In the first row, items are fed from left to right as before.However, when transitioning to the second row, we introduce the instruction "you move down by one step" to indicate the change in row.In the second row, items are then fed from right to left.By investigating these alternative data feeding methods, we aim to understand their implications and assess their impact on the model's performance.Example prompts using random and snake order are shown in Figure 6.</p>
<p>Question: "You have been given a 3 by 3 square grid with various items located at different indices: a restaurant is at index (1, 2), a half-track is at index (2, 2), a sleeping bag is at index (2, 3), a Scottish Terrier is at index (2, 1), a wok is at index (1, 3), a balance beam is at index (3, 3), a military uniform is at index (3, 1), a marimba is at index (1, 1), and a radio telescope is at index (3, 2).You start at the position where the half-track is located, then you go left by one step, then you go up by one step, then you go right by one step, then you go right by one step, then you go down by one step, then you go down by one step, then you go left by one step, and then you go up by one step.What will you find?" Answer: "half-track."</p>
<p>Question: "You have been given a 3 by 3 square grid.Starting from a vertex, you will move along the edges of the grid.Initially, you are positioned at the bottom-left corner of the grid, where you will find a toilet paper, then you go right, where you will find a stethoscope, then you go right, where you will find a screw.Then you go up, where you will find a modem, then you go left, where you will find a pretzel, then you go left, where you will find an Otterhound.Then you go up, where you will find an Asian elephant, then you go right, where you will find a tile roof, then you go right, where you will find a slip-on shoe.Now you have all the information on the map.You start at the position where the pretzel is located, then you go right by one step, then you go down by one step, and then you go up by one step.What will you find?" Answer: "modem."</p>
<p>(a) Random order (b) Snake order</p>
<p>Figure 6: Example question and its answer for 3 by 3 square grid using random and snake order.</p>
<p>The results are shown in Table 2.Although the GPT-4's accuracy degrades for Random and Snake, it is noteworthy that Random is better than Snake.We note that we omit the results for Llama-2 models because even Llama2-70B's performance was already very low (e.g.row-by-row's accuracy is 0.04 for Llama2-70B whereas GPT-4 achieves 0.55.)</p>
<p>Row-by-row Random Snake Snake+Coord GPT-4 Acc 0.572 (0.019) 0.495 (0.007) 0.440 (0.057) 0.525 (0.049) Table 2: The order of presenting the map impacts spatial understanding accuracy.Snake+Coord refers to the setting where we append the global coordinates of the location after each step.The mean and the standard deviation (shown in parentheses) are calculated over 5 different runs.</p>
<p>What could potentially explain these phenomena?It is reasonable to speculate that different methods of inputting data could influence how LLMs internally represent spatial relations.For instance, when utilizing the row-by-row approach, the LLM can register these items in a key-value dictionary, where the row id serves as the key and a list of objects represents the corresponding value.The 'random' approach also enables the LLM to store a key-value dictionary where the key denotes the location address and the value denotes a single item, which can be leveraged for navigation purposes later on.On the other hand, the 'snake' order approach necessitates the LLM to simultaneously handle the storage of object item information and spatial relational understanding.This added complexity potentially complicates the task.</p>
<p>To investigate whether or not such a key-value data structure plays a role, we perform an additional experiment where we add the global coordinate information to the snake order approach.We see that this approach indeed increases the accuracy from 0.44 to 0.525, corroborating our hypothesis.</p>
<p>Relational structure: Tree</p>
<p>In addition to spatial structures, relational structures can also be represented using connected graphs.In this section, we examine a tree structure.Unlike spatial structures, the hierarchical structure of a tree is more naturally presented in a global setting, where all objects are provided at the beginning, and relational questions can be asked subsequently.</p>
<p>Published in Transactions on Machine Learning Research (02/2024)</p>
<p>To ensure comparability with the square and ring structures, we set the number of nodes in each tree to be 9.For comparison, we also included a 3 by 3 square grid and a 9-node ring in this experiment.The exploration steps are set to 4 for all structures.For the tree structure, we utilize the same ImageNet object labels, but focus on relational questions that involve 4 steps, such as "What is the cousin of A?", "What is the great-great-grandparent of A?", and "What is/are the great-great-grandchild/children of A?".An illustrative example question and its corresponding answer can be found in Figure 7.</p>
<p>Question: "You have been given a tree structure with 9 nodes.The root node is a great white shark.The great white shark has 2 children: a garter snake and a Gila monster.The garter snake has 2 children: a jigsaw puzzle and a moped.The jigsaw puzzle has a child: a Tibetan Terrier.The Tibetan Terrier has no children.The moped has a child: an umbrella.The umbrella has no children.The Gila monster has 2 children: a Christmas stocking and a horse-drawn vehicle.The Christmas stocking has no children.The horse-drawn vehicle has no children.What is the cousin of the moped?" Answer: "Christmas stocking, horse-drawn vehicle" The evaluation results are depicted in Figure 8.We observe that for GPT-4, while the tree structure performs worse than the square structure, it outperforms the ring structure.On the other hand, for GPT-3.5-turbo, the tree structure exhibits better performance compared to the square structure.We observe that just like GPT-3.5-turbo,Tree performs better than Square for all Llama-2 models.The only exception is GPT-4; this further demonstrates that GPT-4's exception ability to comprehend the square structure.We also note that Ring is harder than Square for all Llama models and GPT-4.Further investigation into how relational structure, spatial structure, and model size impact performance would be an intriguing topic for future research.We evaluate the prediction accuracy of the models on a 9-node tree, a 3 by 3 square, and a 9-node ring structure with 4 exploration steps in the global setting.Comparing the performance of GPT-4 and random guessing, GPT-4 outperforms random guessing with higher prediction accuracy, with the order of accuracy being square &gt; tree &gt; ring.GPT-3.5-turbo also performs better than random guessing on the tree structure, but worse on the square and ring structures, with the order of accuracy being tree &gt; ring &gt; square.Just like GPT-3.5-turbo,Tree performs better than Square for all Llama-2 models.The error bars indicate 1.96 times standard error across 5 runs.</p>
<p>Grid size inference from sequences of navigational instructions</p>
<p>In the preceding sections, we examined the capability of LLMs to understand the spatial and relational structure of a map.In this section, our focus shifts to investigating whether LLMs can infer the global size of a map based solely on a sequence of local navigational actions.Simultaneously inferring the extent of map while navigating is an important component of navigation as a cognitive task.Inferring the extent of a map is a non-trivial task which requires integrating information about the history of actions and reasoning about what it implies spatially.Indeed, humans often have a sense of the size of the places they visit, not only how to navigate from point A to point B. Specifically, we provide navigational instructions that guide the exploration of all locations within a rectangle.As before, we also provide what item the agent finds at each step.Then we ask LLMs about the height and width of the rectangle.This task necessitates LLMs to maintain the entire path in order to accurately deduce the overall dimensions of the rectangle.</p>
<p>Table 3 illustrates the accuracy comparison of GPT-4 for different size configurations of the same area (e.g., 2 by 6, 3 by 4 for an area of 12).We prepare 200 samples for each area.We observe a general trend where accuracy decreases as the length of the sides increases and as the area size of the rectangular grid increases.We omit the results for GPT-3.5-turbo,Llama-2-70B and CodeLlama-34B because these models were not able to infer the size of rectangle.3x4 or 4x3 2x6 or 6x2 4x6 or 6x4 3x8 or 8x3 2x12 or 12x2 GPT-4 Acc 0.612 (0.209) 0.103 (0.058) 0.162 (0.071) 0.016 (0.017) 0.005 (0.007) Table 3: Grid size inference performance of GPT-4.The mean and the standard deviation of prediction accuracy (shown in parentheses) are calculated over 10 different runs.</p>
<p>Additionally, we evaluate the same setup but exclude object item information during navigation.In this case, GPT-4 relies solely on directional information (e.g., "you go up by one step, then you go down by one step, etc.").Shown in Figure 9 are the mean accuracy and standard deviation across 10 different runs, grouped by the shape of the rectangles.We see variations in accuracy across based on the shape of the rectangles.</p>
<p>[ [ [ [ [ [ [ [ [ [ [ 6WUXFWXUH $FFXUDF\PHDQ 3URPSW 'LUHFWLRQ,WHP 'LUHFWLRQ</p>
<p>Figure 9: For grid size inference, we see variations in accuracy based on the shape of the rectangles, specifically whether they were 'tall' or 'fat'.(e.g."4x3" is consistently better than "3x4" for prompts that combines both directional and object item information at each step, while "3x4" is consistently better than "4x3" for the approach that only uses directional information.)The error bars indicate 1.96 times standard error across all runs.</p>
<p>Error analysis</p>
<p>In this section, we perform a detailed analysis of the errors produced by GPT-4, to assess whether it is modelling the correct topology.In our error analysis, we focus on GPT-4 because of its relatively strong performance, which reveals intriguing error patterns.</p>
<p>To study the extent to which LLM understands the topology of a given spatial structure, we examine what types of mistakes it makes.In our spatial understanding task, when LLM makes a mistake, in the overwhelming majority of cases, it provides the name of an object at a different location (rather than naming an object that did not appear at all in the prompt).Therefore, we can measure the distance between the correct location and the location of the predicted object with respect to the underlying topology.If the LLM represents the spatial structure of the map, the distribution of these distances will tend to cluster at small values, which we call spatial-topology bias.That is, if the LLM represents spatial structure, we should expect more mistakes for objects topologically close to the correct location, and fewer mistakes for objects farther away.A natural choice for measuring distances in grids is the shortest distance between two vertices in the grid.LLMs may also show non-topological biases in their predictions, an instance of which is the temporal bias -an inclination to predict objects that are observed in the textual vicinity of the ground truth item in the given prompt.To investigate the presence of this bias, we measure the temporal distance as the number of objects between the first occurrence of the ground truth in the prompt and the predicted item (including the predicted item).An illustrative example of the temporal and spatial distances in the local setting is shown in Figure 10.</p>
<p>In the following, we examine the error distributions of GPT-4 in square, triangular, and hexagonal grids.</p>
<p>For each experiment, we collect 1000 predictions from GPT-4, and analyze them along with their corresponding prompts and ground-truth correct answers.Out of the 1000 prompts, we only consider the subset of predictions in which the model was wrong.We compare GPT-4's errors with the random distributions of spatial and temporal distances with respect to a uniform baseline.To do this, we randomly pick one of the prompts the model is tested with.Next, we record the location of the ground truth object based on the prompt.Then, for the global setting prompts, we at random select a location from the entire grid; for the local setting prompts, we at random select one of the visited nodes along the path.We then calculate both the spatial or temporal distances between this randomly selected location and the ground truth location.</p>
<p>We repeat this procedure for 100,000 times to generate the error distribution for the uniform baselines.</p>
<p>Comparing error distributions of square, triangular, and hexagonal grids</p>
<p>In our analysis, we used a 3 by 3 square grid, a triangular grid with a size of 3, and a hexagonal grid with a size of 2. We chose these grid configurations to ensure a fair comparison across different grid structures, allowing for prompts conducting 8 navigation steps in each grid.The precise shape of the size-3 triangular grid is shown in Figure 15 in Appendix, and the shape of size-2 hexagonal grid is shown in Figure 1.The results for the local setting are shown in Figure 11.</p>
<p>For the square grid, GPT-4 tends to make more errors at spatial distances of 1 relative to random baseline, indicating a spatial-topology bias (Fig. 11a, left).However, temporal distance also shows a stronger peak at the value of 1 (relative to the uniform baseline; Fig. 11a, right) indicating that having two items more closely located in the prompt is also an effective predictor of GPT-4's errors.In Appendix Figure 19, we also generated a plot for the conditional distribution of TD when SD=1 to further validate the temporal bias.</p>
<p>In the case of the hexagonal grid (Fig. 11b), we see a lack of spatial-topology bias, with the distribution of distances peaking at 2 (instead of 1).We also do not see a temporal bias in GPT-4's behavior, again with a distribution peaked at the temporal distance of 2. This indicates the presence of some other source of non-spatial bias besides the temporal bias.Closer inspection revealed that whenever GPT-4 makes an error in hexagonal grids, these errors are often due to the model predicting the very first object on the path, which often ends up having spatial and temporal distances of 2 from the ground truth correct answer.</p>
<p>For the triangular grid (Fig. 11c), the distribution of SD from GPT-4 and random guessing is almost the same, suggesting that there is almost no spatial bias.However, there is a spike when TD = 1.We find  that among all the instances where TD equals 1, the proportion of predicting the starting position is 0.416.</p>
<p>Hence, it appears that the temporal bias accounts for more than half of the bias observed in the triangular grid.In terms of the starting position bias of the square grid, see Appendix Section C. We see a spike when the temporal distance (TD) is 3 and 6, indicating an effect of spatial distance -these two TD values correspond to the spatial distance of 1 and 2.</p>
<p>In Figure 12, we present analyses for the global structure.3When dealing with a square grid, we provide GPT-4 with the complete map upfront by listing each object row by row.For instance, in prompts for the square grid structure in the global setting, such as "In the first row, we have item A, B, and C. In the second row, we have item D, E, and F, ..." the temporal distance between A and D would be 3.If GPT-4 represents the square grid as a one-dimensional array, we expect that the frequency of temporal distance would decrease steadily.However, we observe spikes at 3 and 6 in the temporal distance, which correspond to spatial distances of 1 and 2, respectively.This finding suggests that GPT-4 makes more errors when objects are closer to the ground truth in terms of spatial structure, rather than temporal distance.It supports the notion that GPT-4 actually models some aspect of the two-dimensional structure.Furthermore, we investigate whether there are any discrepancies in error distributions when the distance is calculated using either row-major order or column-major order.Figure 13 demonstrates that there is an almost symmetrical distribution between rows and columns.This provides additional evidence that GPT-4 recovers the structure of the two-dimensional array.</p>
<p>Frequency</p>
<p>Figure 13: The row-wise and column-wise error histograms for the 3 by 3 square grid with 8 navigation steps under the global setting.We see that both row and column-wise histograms are almost identical, which suggests GPT-4 does not have bias for row or column.</p>
<p>Comparison to human baseline</p>
<p>Finally, we have conducted human experiments to assess the average human baseline performance.These experiments focused on local navigation tasks using the structures shown in Figure 3, which include a 3 by 3 square grid, a size 2 hexagonal grid, a size 3 triangular grid, and a size 12 ring grid.For each of these structures, we randomly selected 20 prompts from the previously used dataset, resulting in a total of 80 candidate prompts.Then, for each participant in the experiment, we randomly chose 10 prompts from this pool of candidate prompts.Participants were asked to provide textual answers to these questions.Additionally, we included 4 attention check questions.These questions were intentionally designed to be easy so that we can assess whether participants were providing meaningful answers, and were used as an exclusion criterion in our analysis.These attention check questions were drawn from the set of questions associated with very simple structures, such as a 2 by 2 square grid, a size 1 hexagonal grid, a size 2 triangular grid, and a size 5 ring grid.These attention check questions were distributed randomly throughout the survey.</p>
<p>We established a criterion to measure participants' engagement in the experiment: if a participant made more than one mistake in the attention check problems, we would exclude their response from the analysis.Specifically, each participant had 30 minutes to solve 14 problems, which consisted of 10 regular problems to evaluate human performance and 4 attention check problems.The participants were not informed whether the given problem was a regular task or an attention check.</p>
<p>In this experiment, we received completed responses from a total of 23 participants.Details about the participant recruitment process and the selection criteria are discussed in Appendix Section F. Among them, 5 participants did not meet the attention check criterion mentioned earlier.Their responses were excluded, leaving us with the responses of the remaining 18 participants for analysis.In total, we collected 180 questionanswer pairs, distributed as follows: 48 pairs for the 3 by 3 square grid, 41 for the size 12 ring grid, 48 for the size 2 hexagon grid, and 43 for the size 3 triangle grid.We converted all the responses to lowercase and removed all English articles (a, an, the) from both the human responses and the ground truth.A response was considered "correct" only if it matched the ground truth exactly.The results can be found in Table 4. Similar analysis under a different criterion of participants' engagement measurement is conducted with results in Section F. The aggregated accuracy across all the structures is 0.67, and the accuracy for each structure is 0.90 for 3 by 3 square grid, 0.78 for size 12 ring grid, 0.41 for size 2 hexagon and 0.58 for size 3 triangle grid respectively.For GPT-4, the accuracies for each structure are from Figure 3.Although human responses are not perfect, they still outperform GPT-4 by a significant margin.It is also interesting to note that, like GPT-4, non-expert humans struggle with non-square grid shapes.The differences in human performance across different structures may be attributable to familiarity with such structures in every-day life and how well these structures can be described in natural language.For example, in contrast to the square and ring structures, hexagonal and triangular grids are much less frequently encountered in everyday life.Moreover, these structures are intrinsically more difficult to describe in natural language.</p>
<p>Related Work</p>
<p>Some research suggests that language models have the ability to acquire implicit world models (Abdou et al., 2021;Patel &amp; Pavlick, 2022;Li et al., 2021).Spatial understanding is particularly intriguing because it might seem counterintuitive that a language model, which lacks visual or sensorimotor input, can comprehend spatial structures.</p>
<p>Patel &amp; Pavlick (2022) provide evidence that GPT-3 is capable of grounding spatial and cardinal direction terms in a text-based grid world.They present contextual examples of cardinal directions (e.g., north, east, northeast) and evaluate whether the model can generalize to a different subset (e.g., south, west, southwest).</p>
<p>Our work expands upon these findings by assessing spatial understanding that necessitates the accurate construction and retention of representations of spatial structure in more challenging tasks.</p>
<p>Previous studies have employed text-based navigation tasks to evaluate language models.Bubeck et al. (2023) evaluate GPT-4 across various domains, including mathematics, coding, vision, medicine, law, and psychology.In one task involving embodied interaction, they create a simple map and prompt GPT-4 to explore it interactively using actions such as left, right, up, and down.A human provides feedback during the exploration.They demonstrate that GPT-4 successfully tracks all the locations and visualizes them using a generated program.However, their study only examines a single instance of a square-grid map and does not thoroughly investigate the extent of GPT-4's spatial understanding.Another similar task is present in (Whittington et al., 2020) but the goal is to test structural generalization.BIG-bench collaboration (2023) has a task where the agent is required to determine whether it would return to its original starting position based on a set of navigation instructions.However, this task only involves providing "yes/no" answers.In the NLP community, more complex spatial reasoning tasks have been introduced, including those involving multi-hop reasoning (Shi et al., 2022) and tasks that involve understanding of textual descriptions for intricate visual scenes (Mirzaee et al., 2021).In contrast, our study investigates various spatial structures, such as rings, trees, hexagons, and triangles, while also requiring the language model to remember and track object names.</p>
<p>Moreover, LLMs increasingly underlie embodied agents, and exhibit impressive spatial reasoning capability (Singh et al., 2023;Liang et al., 2023;Huang et al., 2022;Rajvanshi et al., 2023).Our work explores the basic spatial reasoning skills of these models via controlled experiments using synthetic grid data.This approach allows us to identify both their strengths and weaknesses.</p>
<p>A concurrent work (Momennejad et al., 2023) also evaluates LLMs in terms of cognitive mapping and planning abilities.While they examine a wide array of tasks, their focus is on planning (e.g.generating path between given vertices), and their graph structures are limited to trees, linear paths, and social graphs.In contrast, our work is centered on path integration for sensory prediction and perform a systematic evaluation of basic topological spatial structures, coupled with in-depth error analysis with experiments on human participants.</p>
<p>Conclusion</p>
<p>In this work, we investigated whether LLMs could build representations of spatial structure implicitly from their sequential inputs.We found that LLMs are able to answer questions about spatial relationships, and tend to make errors that reflect spatial proximity.However, the details of their performance depends on the task and structure -square grids are easier than other structures, and local presentation is easier than global.Overall, our results suggest that LLMs implicitly learn to represent aspects of spatial structure, though their performance is far from perfect.These findings contribute to the growing literature on the aspects of world knowledge that LLMs implicitly acquire from their language-only training.</p>
<p>Limitations</p>
<p>In our investigation into the spatial understanding of LLM, our focus lies on the zero-shot setting.We have conducted a preliminary study to examine the impact of chain-of-thought style prompting, and we observe an increase in performance for GPT-3.5.However, beyond the 3-shot setting, the performance improvement reached a plateau.Details of these studies can be found in Fig 14 in Appendix, and we encourage future research to explore the effects of additional variations of chain-of-thought prompting on LLM's spatial understanding.We note that the lack of complete detail about the training of GPT-4 makes understanding the origins of its strong spatial performance somewhat challenging.Investigating how smaller models fine-tuned on spatial tasks exhibit spatial understanding would be an intriguing subject for further study.</p>
<p>Broader Impact Statement</p>
<p>Before the human experiment starts, all participants are provided with an online form with information about the experiment, including the purpose, a brief description, the procedure, confidentiality, and voluntary participation.The participants are informed that their responses will be confidential and they are free to decline or end participation at any time.In the "agreement to participate" section, we need participants to confirm that by clicking the "start" button, they acknowledge that they have read the above information, and agree to participate in the study.The experiment only starts after they click the "start" button.</p>
<p>The participants were recruited from an online crowdsourcing platform (Prolific.com),it is a platform for researchers to post behavioral studies and recruit participants.We set the only participants' criteria to be "fluent in English" as all our prompts and questions are written in English.Our study is evenly distributed to all available participants.The demographic data of the 23 participants that took part in our study is as follows: age range (min: 21, max: 62, median: 36, mean: 41.9), gender (Male: 12, Female: 10, No data: 1), employment status (Full-time: 9, Not-in-paid-work: 5, part-time: 3, unemployed: 2, No data: 4).Although we seek to mitigate the bias by setting as few criteria as possible, there is still a bias toward English-speaker with computers and internet connection, as well as some characteristics of the WEIRD (Western, Educated, Industrial, Rich, Democracies) population.</p>
<p>As large language models continue to advance and find applications in real-world scenarios, it becomes increasingly crucial to assess the risks and unintended consequences associated with such LLM-based applications.Although our study on the spatial understanding of LLMs may not directly address the reduction of harm and bias in these models, we believe it is important to comprehend their inner workings.We hope that our work contributes to the ongoing effort of understanding and exploring the mechanisms at play within LLMs.</p>
<p>Question: "You have been given an equilateral triangular tile map consisting of 3 rows, where the first row has one tile, the second row has three tiles, and so on, so that the i th row has 2*i-1 tiles.Starting from a vertex, you will move along the edges of these tiles.Initially, you are positioned at the top corner of the map, where you find an ear.</p>
<p>E Conditional distribution of temporal Distance</p>
<p>Condition on spatial distance = 1, we can see that the temporal distance has a clear bias towards smaller values in Figure 19 .</p>
<p>F Human experiments</p>
<p>Recruitment Process</p>
<p>We recruited participants online through an online crowdsourcing platform (Prolific.com),it is a platform for researchers to post behavioral studies and recruit participants.The way it works is that we first create a web interface for our study and upload it to Prolific with estimated finish time, hourly payment and participants criteria, then the platform will send out the study invitation to participants who match the criteria.Participants take part in the study online and remotely using their own laptop.</p>
<p>Impact of the exclusion criteria</p>
<p>In our experiment, we set the participants' criteria to be fluent in English as answering the prompts required the understanding of English, and the study to be evenly distributed to all available participants.The demographic data of the 23 participants that took part in our study is as follows: age range (min: 21, max: 62, median: 36, mean: 41.9), gender (Male: 12, Female: 10, No data: 1), employment status (Full-time: 9, Not-in-paid-work: 5, part-time:3, unemployed: 2, No data: 4), student status (Yes: 3, No: 17, No data: 3).In the experiment, we seek to mitigate the bias by setting as few criteria as possible, but it is inevitably skewed to people with computers and internet connection, and some characteristics of the WEIRD (Western, Educated, Industrial, Rich, Democracies) population, which is unfortunately still a common practice across much of cognitive science and psychology research.</p>
<p>Different criterion for measuring participants' engagement</p>
<p>As one of the reviewer suggests, given the difficulty of non-square grid questions, we conducted similar analysis under the criterion that only the participants that gets the square attention check wrong will be excluded.Among the 23 participants, only one participant failed the attention check for the rectangular grid, and this participant got 3 out of 4 attention checks wrong.If we exclude this participant' response and calculate the prediction accuracy for each structure among the rest of the participants, we have 0.91 for square, 0.76 for ring, 0.36 for hexagon and 0.48 for triangle.The overall accuracy is 0.62.The accuracy are generally lower (except for the square) compared to using our old criterion where we exclude participants that get more than attention checks wrong.However, the performance is still higher than that of GPT-4.</p>
<p>G Variation on prompts</p>
<p>Hexagonal and triangular grid</p>
<p>In this variation, we gave a more detailed description of the hexagonal and triangular grid.Example of prompts are shown in Figure 20.A similar analysis was conducted on the same dataset as Figure 3.Given that the performance of the other models other than GPT-4 are below random guessing, we only include the result from GPT-4 here.The prediction accuracy for triangle is 0.238 (with std = 0.016) and for hexagon is 0.088 (with std = 0.029), both are still within the margin of error as the results (for triangle is 0.204 with std = 0.015 and for hexagon is 0.046 with std = 0.018) from the original prompting.</p>
<p>Question: "In a hexagonal tiling (or hexagonal tessellation), equilateral hexagonal tiles are arranged to fill 2-dimensional space with no overlaps and no gaps."Pointy-topped" refers to the orientation of the hexagon, indicating that it has a pointed or acute angle at the top.Since equilateral hexagons have equal sides and 120 degree internal angles, each vertex has 3 directions to go along the edges.Now, consider the graph given by the vertices of two rows of a Pointy-topped hexagonal tiling.The top row contains one hexagonal tile, while the bottom row has two.Starting from a vertex, you will move between vertices along the edges of these tiles.Initially, you are positioned at the bottom left corner of the map, where you find a drilling rig.You move up by one step, where you find a carousel.You move up-right by one step, where you find a loupe.You move up by one step, where you find a gown.You move up-right by one step, where you find a printer.You move down-right by one step, where you find a black-footed ferret.You move down by one step, where you find a station wagon.You move down-left by one step, where you find a bath towel.You move up-left by one step.What will you find?" Answer: "loupe."</p>
<p>Question: "In a triangular tiling (or triangular tessellation), equilateral triangular tiles are arranged to fill 2-dimensional space with no overlaps and no gaps.Since equilateral triangles have equal sides and 60 degree internal angles, each vertex has 6 directions to go along the edges: left/right, down left/right, and up left/right.Now, consider the graph given by the vertices of two rows of a triangular tiling.The top row contains one triangular tile, the bottom row has three.Hence, there are three rows of vertices: the first row has one vertex, the second has two, and the third has three.Starting from a vertex, you will move between vertices along the edges of these tiles.Initially, you are positioned at the bottom left corner of the map, where you find a box turtle.You move right by one step, where you find a hand plane.You move up-right by one step, where you find a guacamole.You move down-right by one step, where you find a table lamp.You move left by one step.What will you find?" Answer: "hand plane."</p>
<p>Different tree traversals</p>
<p>In this variation, we used breadth-first traversal method (instead of depthfirst traversal in the main text) to generate the prompt, which gives a layer-wise description of the tree structure.An example of the prompt are shown in Figure 21.The prediction accuracy under both breadthfirst traversal and depth-first traversal method is in Table 5.We can see they are similar (within margin of error) across different LLMs.GPT-4 Llama-2-70b-chat-hf CodeLlama-34b-Instruct-hf depth-first traversal (Original) 0.250 (std=0.037)0.506 (std=0.017)0.350 (std=0.025)0.264 (std=0.029)breadth-first traversal 0.262 (std=0.032)0.556 (std=0.020)0.316 (std=0.018)0.264 (std=0.025)</p>
<p>Table 5: Prediction accuracy under depth-first traversal vs. breadth-first traversal for tree structure.The mean accuracy and standard deviation are calculated over 5 runs on the same dataset.</p>
<p>H Prompting CodeLlama to write code</p>
<p>For CodeLlama-34b, we performed an additional experiment using Program-of-Thought (PoT) prompting, where we prompt it to write a python code.An example is shown in Figure 23.</p>
<p>We tested CodeLlama-34b and CodeLlama-13b (which is more tailored for autocompletion) for the square grid of size 3:</p>
<p>CodeLlama-13b (PoT) CodeLlama-34b (PoT) CodeLlama-34b-Instruct-hf (No PoT) Accuracy 0.25 0.20 0.25 Table 6: Prediction accuracy of CodeLlama models with code prompting.</p>
<p>As we can see in Table 6, there wasn't any improvement in accuracy.Upon examining the outputs, we noticed that the generated code often suggest incorrect navigation, resulting in wrong answers.This implies that while using code prompts works well for simple arithmetic, it struggles with spatial navigation tasks.Additionally, while this method might be suitable for simple grids like squares, it's not yet clear how to effectively apply it to more complex grid patterns, such as those involving hexagons and triangles.</p>
<p>You have been given an equilateral triangular tile map consisting of 2 rows, where the first row has one tile and the second row has three tiles.Starting from a vertex, you will move along the edges of these tiles.Initially, you are positioned at the bottom left corner of the map, where you find a box turtle.You move right by one step, where you find a hand plane.You move up-right by one step, where you find a guacamole.You move down-right by one step, where you find a table lamp.You move left by one step.What will you find?" Answer: "hand plane"</p>
<p>Figure 2 :
2
Figure 2: Example question and its answer for square, triangle, hexagon and ring structure.</p>
<p>Figure 4 :
4
Figure 4: Example question and its answer for square and ring structure under the global setting.</p>
<p>Figure 7 :
7
Figure 7: Example prompt for tree structure.</p>
<p>Figure8: We evaluate the prediction accuracy of the models on a 9-node tree, a 3 by 3 square, and a 9-node ring structure with 4 exploration steps in the global setting.Comparing the performance of GPT-4 and random guessing, GPT-4 outperforms random guessing with higher prediction accuracy, with the order of accuracy being square &gt; tree &gt; ring.GPT-3.5-turbo also performs better than random guessing on the tree structure, but worse on the square and ring structures, with the order of accuracy being tree &gt; ring &gt; square.Just like GPT-3.5-turbo,Tree performs better than Square for all Llama-2 models.The error bars indicate 1.96 times standard error across 5 runs.</p>
<p>Figure 10 :
10
Figure 10: An example of two distance metrics in 3 × 3 square grid in the local setting.If the path in the local setting starts at node A and follows through B, C, F, I, H, E, and finally ends at B, and if the LLM predicts F instead of B, then the temporal distance between B and F is 4, while the spatial distance is 2.</p>
<p>Figure 11 :
11
Figure 11: The spatial and temporal distance (SD, TD, respectively) histograms for square, hexagonal, and triangular grids under the local setting.Blue histograms show random baselines.Orange histograms show the observed distribution of errors as the spatial (left) and temporal (right) distances between the ground truth and GPT-4's predicted locations.(a) In grids with square topology, GPT-4 makes more errors both when SD is 1 and TD is 1, compared to the uniform baseline, meaning that both spatial and temporal biases contribute to GPT-4's errors.(b) In grids with hexagonal topology, we do not observe spatial nor temporal bias.(c) The simultaneous lack of spatial bias and the presence of temporal bias indicate that GPT-4 was not able to accurately construct the triangular grid.</p>
<p>FrequencyFigure 12 :
12
Figure12: The spatial and temporal error histograms for the 3 by 3 square grid under the global setting.We see a spike when the temporal distance (TD) is 3 and 6, indicating an effect of spatial distance -these two TD values correspond to the spatial distance of 1 and 2.</p>
<p>Figure 16 :Figure 17 :Figure 18 :
161718
Figure 16: An example of Chain-of-thought style prompting for the 2 by 2 square grid.</p>
<p>Figure 19 :
19
Figure 19: The temporal distance histogram conditioned on spatial distance (SD) = 1 for the square, SD = 2 for the hexagonal, and SD = 1 for the triangular grid under the local setting.These spatial distance conditions are selected because of its maximum frequency in Fig 11.</p>
<p>Figure 20 :
20
Figure 20: Example question and its answer for size-2 triangular grid and size-2 hexagonal grid with more detailed description.</p>
<p>.
Estimate Std. Error z valuePr(&gt;|z|)(Intercept)3.4480.14224.273 &lt;2e-16 (<strong><em>)type is hexagon-2.3270.091 -25.595 &lt;2e-16 (</em></strong>)type is triangle-1.8200.082 -22.199 &lt;2e-16 (<strong><em>)type is ring-2.1170.103 -20.616 &lt;2e-16 (</em></strong>)number of edges-0.0020.002-1.0620.288number of navigation steps-0.3450.018 -18.924 &lt;2e-16 (***)Table</p>
<p>Table 4 :
4
Human baseline performance compared against GPT-4.The values of the GPT-4 accuracy are taken from Figure3.
Square Ring Hexagon Triangle AggregatedHuman0.900.780.410.580.67GPT-40.690.220.050.200.29</p>
<p>You move down-left by one step, where you find a sundial.You move down-left by one step, where you find a flagpole.You move down-right by one step, where you find a West Highland White Terrier.You move right by one step, where you find a Sussex Spaniel.You move up-right by one step, where you find a howler monkey.You move left by one step, where you find a Basset Hound.You move up-right by one step, where you find an ice pop.You move up-left by one step.What will you find?" Answer: "ear" Question: "You have been given a 3 by 3 square grid.Starting from a vertex, you will move along the edges of the grid.Initially, you are positioned at the top right corner of the grid, where you find a breastplate.You move down by one step, where you find a hummingbird.You move down by one step, where you find an eastern diamondback rattlesnake.You move left by one step, where you find a basketball.You move up by one step, where you find a pulled rickshaw.You move left by one step, where you find a Black and Tan Coonhound.You move up by one step, where you find a breakwater.You move right by one step, where you find a sea cucumber.You move right by one step.What will you find?" "You have been given an equilateral triangular tile map consisting of 3 rows, where the first row has one tile, the second row has three tiles, and so on, so that the i th row has 2*i-1 tiles.Starting from a vertex, you will move along the edges of these tiles.Initially, you are positioned at the bottom right corner of the map, where you find a measuring cup.You move left by one step, where you find a binoculars.You move left by one step, where you find a rotary dial telephone.You move left by one step, where you find a space heater.You move up-right by one step, where you find a bolo tie.You move right by one step, where you find an African bush elephant.You move up-right by one step, where you find a ladle.You move left by one step, where you find an English Setter.You move down-left by one step.What will you find?" Answer: "bolo tie" Question: "You have been given a 3 by 3 square grid.Starting from a vertex, you will move along the edges of the grid.Initially, you are positioned at the top left corner of the grid, where you find a cock.You move down by one step, where you find a geyser.You move right by one step, where you find a jellyfish.You move up by one step, where you find an impala.You move right by one step, where you find a box turtle.You move down by one step, where you find an espresso machine.You move down by one step, where you find a bib.You move left by one step, where you find a megalith.You move up by one step.What will you find?" "You have been given a pointy-topped regular hexagonal tile map consisting of 2 rows, where the first row has one tile and the second row has two tiles.Starting from a vertex, you will move along the edges of these tiles.Initially, you are positioned at the bottom left corner of the map, where you find a drilling rig.You move up by one step, where you find a carousel.You move upright by one step, where you find a loupe.You move up by one step, where you find a gown.You move up-right by one step, where you find a printer.You move down-right by one step, where you find a black-footed ferret.You move down by one step, where you find a station wagon.You move down-left by one step, where you find a bath towel.You move up-left by one step.What will you find?" Question: "You have been given a pointy-topped regular hexagonal tile map consisting of 2 rows, where the first row has one tile and the second row has two tiles.Starting from a vertex, you will move along the edges of these tiles.Initially, you are positioned at the top corner of the map, where you find a bucket.You move down-left by one step, where you find a gown.You move down by one step, where you find a racket.You move down-right by one step, where you find an amphibious vehicle.You move down by one step, where you find a CD player.You move down-left by one step, where you find a T-shirt.You move up-left by one step, where you find a library.You move up by one step, where you find a moped.You move up-right by one step.What will you find?" "You have been given a circular grid consisting of 12 connected dots.Starting from a vertex, you will move along the edges of the circular grid.Initially, you are positioned on the dot that's located at the top of the grid, where you find a milk can.You move around the ring by 4 steps in a counter-clockwise direction, where you find a mushroom.You move around the ring by 9 steps in a counter-clockwise direction, where you find a spotlight.You move around the ring by 1 step in a counter-clockwise direction, where you find a shopping basket.You move around the ring by 5 steps in a counter-clockwise direction, where you find a safety pin.You move around the ring by 8 steps in a counterclockwise direction, where you find a poke bonnet.You move around the ring by 10 steps in a clockwise direction, where you find a Standard Schnauzer.You move around the ring by 6 steps in a counter-clockwise direction, where you find a shovel.You move around the ring by 4 steps in a clockwise direction.Question: "You have been given a circular grid consisting of 12 connected dots.Starting from a vertex, you will move along the edges of the circular grid.Initially, you are positioned on the dot that's located at the top of the grid, where you find a giant panda.You move around the ring by 9 steps in a clockwise direction, where you find a pickup truck.You move around the ring by 6 steps in a counterclockwise direction, where you find a car wheel.You move around the ring by 5 steps in a clockwise direction, where you find a vulture.You move around the ring by 3 steps in a clockwise direction, where you find a quill.You move around the ring by 5 steps in a counter-clockwise direction, where you find a fountain pen.You move around the ring by 11 steps in a counter-clockwise direction, where you find a snoek.You move around the ring by 6 steps in a counter-clockwise direction, where you find a military uniform.You move around the ring by 7 steps in a clockwise direction.What will you find?" "You are at the top left corner of a 2 by 2 grid, where you find box turtle.You move right by one step, where you find table lamp.You move down by one step, where you find American black bear.You move left by one step, where you find hand plane.You move up by one step.
size-3 triangular gridAnswer: "breastplate"Answer: "jellyfish"Figure 15: Size-3 triangular grid.(a) 4 by 4 square grid with 8 navigation steps(b) Size-3 triangular grid with 8 navigation stepsAnswer: "loupe"Answer: "racket"(c) Size-2 hexagonal grid with 8 navigation stepsWhat willyou find?"Answer: "vulture"Answer: "safety pin"(d) 12-node ring with 8 navigation stepsFigure 14: Example question and its answer for square, triangle, hexagon and ring structures under localsetting.
Question:Question: Question:Question: What do you find?" CoT: "We can describe our movements in the 2 by 2 grid starting from the the top left corner as follows:\n-Move right from (1,1) to (2,1)\n-Move down from (2,1) to (2,2)\n-Move left from (2,2) to (1,2)\n-Move up from (1,2) to (1,1)\nAs a result, we reach the coordinate (1,1) where we find the box turtle.Therefore, the answer is box turtle."Answer: "box turtle",</p>
<p>Navigation step refers to the movements asked to be performed in the prompt. In the example prompts in Figure2, the number of navigation step is
for square,
for triangle, 6 for hexagon, and 3 for ring.
We omit the ring structure because each movement can involve many steps around the ring (e.g. "move by 3 steps clockwise"), which effectively introduces edges between all graph nodes, and thus renders analysis based on spatial distance less meaningful.
A Additional prompt examplesWe provide additional examples of prompts we used with bigger size for different structures in Figure14.B Temporal distance in the global settingFor instance, in prompts for the 3 by 3 square grid structure in the global setting, such as "In the first row, we have item A, B, and C. In the second row, we have item D, E, and F, ..." the temporal distance between A and D would be 3, while the spatial distance between A and D is 1.C Starting position bias of the square gridIn the 3 by 3 square grid, the ground truth is only present at the 1st, 3rd, 5th, and 7th nodes of the path.To qualify as a starting position bias when Temporal Distance (TD) equals 1, the ground truth would need to occur at the 2nd node of the path.Consequently, there is no contribution from the starting position bias when TD = 1.D Chain-of-thought promptingIn the case of in-context learning experiments, we used the following system prompt: "You are given a task to solve.Make sure to output a final answer after "Answer:"."We include in-context examples in the user prompt in the following format: "Question:\n [question] \n Explanation:\n [explanation] \n Answer:\n[answer]."An example prompt is given in Figure16.Question: "You have been given a tree structure with 9 nodes.The root node is a great white shark.The great white shark has 2 children: a garter snake and a Gila monster.The garter snake has 2 children: a jigsaw puzzle and a moped.The Gila monster has 2 children: a Christmas stocking and a horse-drawn vehicle.The jigsaw puzzle has a child: a Tibetan Terrier.The moped has a child: an umbrella.The Christmas stocking has no children.The horse-drawn vehicle has no children.The Tibetan Terrier has no children.The umbrella has no children.What is the cousin of the moped?" Answer: "Christmas stocking, horse-drawn vehicle."
Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color. Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, Anders Søgaard, 10.18653/v1/2021.conll-1.9Proceedings of the 25th Conference on Computational Natural Language Learning. the 25th Conference on Computational Natural Language LearningAssociation for Computational LinguisticsNovember 2021</p>
<p>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research. 2835-8856May 2023BIG-bench collaboration</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of Artificial General Intelligence: Early experiments with GPT-4. April 2023</p>
<p>Organizing conceptual knowledge in humans with a gridlike code. Alexandra O Constantinescu, Jill X O'reilly, Timothy E J Behrens, 10.1126/science.aaf0941Science. 3526292June 2016</p>
<p>Entropy of city street networks linked to future spatial navigation ability. A Coutrot, E Manley, S Goodroe, C Gahnstrom, G Filomena, D Yesiltepe, R C Dalton, J M Wiener, C Hölscher, M Hornberger, H J Spiers, 10.1038/s41586-022-04486-7Nature. 1476-46876047904April 2022</p>
<p>A map of abstract relational knowledge in the human hippocampal-entorhinal cortex. eLife, 6:e17086. Mona M Garvert, Raymond J Dolan, Timothy Ej Behrens, 10.7554/eLife.17086April 2017</p>
<p>Microstructure of a spatial map in the entorhinal cortex. Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, Edvard I Moser, 10.1038/nature03721Nature. 1476-46874367052August 2005</p>
<p>Inner Monologue: Embodied Reasoning through Planning with Language Models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter, 10.48550/ARXIV.2207.05608Conference on Robot Learning. 2022</p>
<p>Boundary Vector Cells in the Subiculum of the Hippocampal Formation. Colin Lever, Stephen Burton, Ali Jeewajee, O' John, Neil Keefe, Burgess, 10.1523/JNEUROSCI.1319-09.2009Journal of Neuroscience. 0270-64742931August 2009</p>
<p>Implicit Representations of Meaning in Neural Language Models. Belinda Z Li, Maxwell Nye, Jacob Andreas, 10.18653/v1/2021.acl-long.143Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 20211</p>
<p>Code as Policies: Language Model Programs for Embodied Control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 10.1109/ICRA48891.2023.10160591IEEE International Conference on Robotics and Automation (ICRA). 2023. May 2023</p>
<p>SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning. Roshanak Mirzaee, Rajaby Hossein, Qiang Faghihi, Parisa Ning, Kordjamshidi, doi: 10.18653Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterAssociation for Computational LinguisticsJune 2021</p>
<p>Evaluating Cognitive Maps and Planning in Large Language Models with CogEval. Ida Momennejad, Hosein Hasanbeig, Felipe Vieira, Hiteshi Sharma, Robert Osazuwa Ness, Nebojsa Jojic, Hamid Palangi, Jonathan Larson, September 2023</p>
<p>The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat. J O'keefe, J Dostrovsky, 10.1016/0006-8993Brain Research. 0006-8993341November 1971</p>
<p>Mapping Language Models to Grounded Conceptual Spaces. Roma Patel, Ellie Pavlick, International Conference on Learning Representations. January 2022</p>
<p>SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments. Abhinav Rajvanshi, Karan Sikka, Xiao Lin, Bhoram Lee, Han-Pang Chiu, Alvaro Velasquez, September 2023</p>
<p>StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts. Zhengxiang Shi, Qiang Zhang, Aldo Lipani, 10.1609/aaai.v36i10.21383Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceJune 202236</p>
<p>ProgPrompt: Program generation for situated robot task planning using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, 10.1007/s10514-023-10135-3Autonomous Robots. 0929-5593478December 2023</p>
<p>Cognitive maps in rats and men. C Edward, Tolman, 10.1037/h0061626Psychological Review. 1939- 1471551948</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, July 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models</p>
<p>The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation. C R James, Timothy H Whittington, Shirley Muller, Guifen Mark, Caswell Chen, Neil Barry, Timothy E J Burgess, Behrens, 10.1016/j.cell.2020.10.024Cell. 0092-86741835November 2020</p>
<p>How to build a cognitive map. C R James, David Whittington, Mccaffary, J W Jacob, Timothy E J Bakermans, Behrens, 10.1038/s41593-022-01153-yNature Neuroscience. 1546-17262510October 2022</p>            </div>
        </div>

    </div>
</body>
</html>
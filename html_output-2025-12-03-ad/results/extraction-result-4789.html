<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4789 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4789</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4789</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-c0e36c42bb7c70d64e4fe3b736ae1944210accdc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c0e36c42bb7c70d64e4fe3b736ae1944210accdc" target="_blank">Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement</a></p>
                <p><strong>Paper Venue:</strong> Conference of the European Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A novel framework that leverages commonsense-based persona expansion to address issues in long-term conversation by transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies.</p>
                <p><strong>Paper Abstract:</strong> Memorizing and utilizing speakers’ personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation.While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4789.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4789.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAFFEINE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context-Aware reFinement Framework for contradictory pErsonas IN long-tErm conversations (CAFFEINE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses a large language model to iteratively refine and store speaker persona sentences (including commonsense expansions) into a long-term memory for multi-session dialogue; it resolves or disambiguates contradictory persona pairs using context-aware strategies (resolution, disambiguation, preservation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CAFFEINE (LLM-based persona-refinement + memory-enabled dialogue system)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory-management/dialogue agent that uses an LLM (ChatGPT in this paper) to (1) refine contradictory persona sentences based on their originating dialogue context using three strategies (Resolution, Disambiguation, Preservation) and (2) write the refined persona sentences into a long-term memory used for response generation. The pipeline also uses an NLI model to detect contradictions and an iterative graph algorithm to schedule refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented long-term memory (persona store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>A long-term memory M that stores refined persona sentences (both human-authored personas and COMET-generated commonsense expansions). At response time, a dense retriever (Contriever) fetches top-k relevant persona sentences from M to condition an LLM (ChatGPT) for zero-shot response generation. The refinement process updates M by removing original contradictory pairs and inserting the refined persona(s).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Persona-grounded long-term conversational response generation (multi-session dialogue)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate appropriate next-turn responses in multi-session open-domain dialogues where the model must recall and use persona information from prior sessions; personas may be human-authored (GOLD) and/or expanded with commonsense (COMET-EXP). The task is evaluated across sessions 2–5, testing accumulation and management of memory across sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multi-Session Chat (MSC)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Using CAFFEINE to refine and store personas then retrieving top-k (k=20) for response generation yields (BLEU-1, ROUGE-1, ROUGE-L): With GOLD+CAFFEINE — Session2: (20.93, 20.18, 15.47); Session3: (21.41, 20.72, 15.86); Session4: (21.67, 21.00, 16.15); Session5: (21.92, 21.23, 16.31). With COMET-EXP+CAFFEINE — Session2: (20.97, 20.06, 15.32); Session3: (21.63, 20.73, 15.86); Session4: (21.97, 21.10, 16.18); Session5: (22.26, 21.32, 16.37). (Metrics = percentages where applicable; reported as BLEU-1 and ROUGE scores from the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No Memory (no persona retrieval) baseline (same metric format): Session2: (20.75, 19.38, 15.16); Session3: (20.42, 19.53, 15.09); Session4: (19.88, 19.56, 14.98); Session5: (19.87, 20.16, 15.33).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>CAFFEINE's context-aware refinement + retrieval consistently improves automatic metrics (BLEU-1, ROUGE-1, ROUGE-L) versus the No Memory baseline and versus memory-management baselines (NLI-remove and NLI-recent). Improvements grow larger as the number of previous sessions increases (greater benefit in later sessions). CAFFEINE also outperforms NLI-based strategies that remove contradictory personas (NLI-remove) or keep only the most recent persona (NLI-recent), and yields higher human-evaluation winning rates on several criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reported limitations include sensitivity to the quality of commonsense models (COMET) and underlying knowledge graphs; reliance on an NLI model to detect contradiction can miss persona pairs that need refinement (false negatives); current framework refines one speaker's persona at a time (ignores cross-speaker persona interactions); longer refined personas may exceed or be under-utilized by the LLM input context (LLM context-utilization limits). Also cost/latency for LLM API calls is a practical issue (addressed partially by iterative graph removal).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Context-aware refinement of contradictory persona sentences (rather than removing them) produces richer, more specific memory content that improves long-term conversational response quality. Contradictions are often resolvable when contextual dialogue fragments are considered (the paper found ~65% of high-confidence contradictory pairs could be judged consistent when context is included). Iterative graph-based scheduling of refinements yields substantial API cost/time savings without harming performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4789.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4789.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (Instruct-tuned LLM, used via API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned large language model (used as the LLM in CAFFEINE) applied for both persona refinement (selecting strategy and generating refined personas) and zero-shot response generation conditioned on retrieved personas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatGPT (OpenAI, used for refinement and response generation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A 175B-parameter instruction-tuned LLM (InstructGPT family) used in two roles: (1) to choose between resolution/disambiguation/preservation strategies and generate refined persona sentences given persona pairs and their dialogue contexts; (2) to generate dialogue responses conditioned on current conversation context plus retrieved persona memory (zero-shot prompts). ChatGPT is accessed via API (LangChain used to manage calls).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>uses external retrieved memory (top-k persona sentences retrieved from a long-term persona store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>ChatGPT consumes retrieved persona sentences (from Contriever + long-term memory M) concatenated with the dialogue context as input prompts for zero-shot response generation. Refined personas are written back to M by the CAFFEINE pipeline; ChatGPT itself does not implement the persistent memory store.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Persona refinement (generation) and persona-conditioned response generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Two sub-tasks: (a) Given two contradictory persona sentences and their originating dialogue fragments, select an appropriate strategy and generate refined persona(s); (b) Given dialogue context and retrieved persona sentences, generate the next response (1-3 sentences) using persona memory.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multi-Session Chat (MSC) (used to evaluate downstream response generation quality)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>ChatGPT is the LLM used to consume retrieved memory and the refined persona store; the paper reports system-level comparisons (CAFFEINE vs baselines) rather than isolated ChatGPT-only ablations. Improvements in response metrics are reported when ChatGPT is provided with CAFFEINE-refined memories versus no memory or other memory management strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Operational costs of API usage (paper reports $35.52 for CAFFEINE refinement and $27.09 for response generation on their experiments); potential under-utilization of long refined persona inputs by ChatGPT due to LLM long-context effectiveness limits (cites 'Lost in the middle' phenomenon).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Large LLMs can be effective for both generating refinements to memory items and for conditioning responses on retrieved memories, but prompts/format and memory management (pruning/refinement) materially affect downstream performance and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4789.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4789.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contriever Retriever</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contriever (unsupervised dense retrieval model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dense information retrieval model (Contrastive unsupervised training) used to retrieve top-k relevant persona sentences from the long-term memory store to provide context for response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Contriever (memory retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An unsupervised dense retriever employed to fetch top-k relevant persona sentences from the long-term persona memory given the current conversation context. The paper uses Contriever (k=20 by default) to construct the set of persona memories provided to the LLM for response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dense retrieval from a stored persona database (vector store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Contriever encodes dialogue context and candidate persona sentences into vector embeddings and returns the top-k persona sentences (k set to 20 in main experiments) from the persona memory store; retrieved personas are then fed to ChatGPT to condition response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Memory retrieval for persona-grounded response generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a dialogue context, retrieve the most relevant persona sentences (from refined/persona-expanded long-term memory) to supply the LLM for next-turn generation; evaluated as part of end-to-end response generation on the MSC dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multi-Session Chat (MSC)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Contriever is used as the retrieval component across all memory-enabled conditions. The paper reports results for multiple k values (k=12,20,30) and shows CAFFEINE improvements are robust across k; specific retrieval-level ablations are not the focus, but retrieval is a required component to enable memory-augmented response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No retrieval-specific failure analysis detailed beyond noting retrieval is a component; overall system performance depends on retrieval quality and the size/quality of the persona memory store.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Using a dense retriever to obtain a focused set of persona sentences is an effective way to present LLMs with long-term memory items; tuning k and ensuring high-quality stored persona items (via CAFFEINE refinement) improves downstream response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Keep me updated! memory management in long-term conversations <em>(Rating: 2)</em></li>
                <li>Beyond goldfish memory: Long-term open-domain conversation <em>(Rating: 2)</em></li>
                <li>Like hiking? you probably enjoy nature: Persona-grounded dialog with commonsense expansions <em>(Rating: 2)</em></li>
                <li>Persona expansion with commonsense knowledge for diverse and consistent response generation <em>(Rating: 2)</em></li>
                <li>Dialogue natural language inference <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4789",
    "paper_id": "paper-c0e36c42bb7c70d64e4fe3b736ae1944210accdc",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "CAFFEINE",
            "name_full": "Context-Aware reFinement Framework for contradictory pErsonas IN long-tErm conversations (CAFFEINE)",
            "brief_description": "A framework that uses a large language model to iteratively refine and store speaker persona sentences (including commonsense expansions) into a long-term memory for multi-session dialogue; it resolves or disambiguates contradictory persona pairs using context-aware strategies (resolution, disambiguation, preservation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CAFFEINE (LLM-based persona-refinement + memory-enabled dialogue system)",
            "agent_description": "A memory-management/dialogue agent that uses an LLM (ChatGPT in this paper) to (1) refine contradictory persona sentences based on their originating dialogue context using three strategies (Resolution, Disambiguation, Preservation) and (2) write the refined persona sentences into a long-term memory used for response generation. The pipeline also uses an NLI model to detect contradictions and an iterative graph algorithm to schedule refinements.",
            "memory_type": "retrieval-augmented long-term memory (persona store)",
            "memory_description": "A long-term memory M that stores refined persona sentences (both human-authored personas and COMET-generated commonsense expansions). At response time, a dense retriever (Contriever) fetches top-k relevant persona sentences from M to condition an LLM (ChatGPT) for zero-shot response generation. The refinement process updates M by removing original contradictory pairs and inserting the refined persona(s).",
            "task_name": "Persona-grounded long-term conversational response generation (multi-session dialogue)",
            "task_description": "Generate appropriate next-turn responses in multi-session open-domain dialogues where the model must recall and use persona information from prior sessions; personas may be human-authored (GOLD) and/or expanded with commonsense (COMET-EXP). The task is evaluated across sessions 2–5, testing accumulation and management of memory across sessions.",
            "benchmark_name": "Multi-Session Chat (MSC)",
            "performance_with_memory": "Using CAFFEINE to refine and store personas then retrieving top-k (k=20) for response generation yields (BLEU-1, ROUGE-1, ROUGE-L): With GOLD+CAFFEINE — Session2: (20.93, 20.18, 15.47); Session3: (21.41, 20.72, 15.86); Session4: (21.67, 21.00, 16.15); Session5: (21.92, 21.23, 16.31). With COMET-EXP+CAFFEINE — Session2: (20.97, 20.06, 15.32); Session3: (21.63, 20.73, 15.86); Session4: (21.97, 21.10, 16.18); Session5: (22.26, 21.32, 16.37). (Metrics = percentages where applicable; reported as BLEU-1 and ROUGE scores from the paper.)",
            "performance_without_memory": "No Memory (no persona retrieval) baseline (same metric format): Session2: (20.75, 19.38, 15.16); Session3: (20.42, 19.53, 15.09); Session4: (19.88, 19.56, 14.98); Session5: (19.87, 20.16, 15.33).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "CAFFEINE's context-aware refinement + retrieval consistently improves automatic metrics (BLEU-1, ROUGE-1, ROUGE-L) versus the No Memory baseline and versus memory-management baselines (NLI-remove and NLI-recent). Improvements grow larger as the number of previous sessions increases (greater benefit in later sessions). CAFFEINE also outperforms NLI-based strategies that remove contradictory personas (NLI-remove) or keep only the most recent persona (NLI-recent), and yields higher human-evaluation winning rates on several criteria.",
            "limitations_or_challenges": "Reported limitations include sensitivity to the quality of commonsense models (COMET) and underlying knowledge graphs; reliance on an NLI model to detect contradiction can miss persona pairs that need refinement (false negatives); current framework refines one speaker's persona at a time (ignores cross-speaker persona interactions); longer refined personas may exceed or be under-utilized by the LLM input context (LLM context-utilization limits). Also cost/latency for LLM API calls is a practical issue (addressed partially by iterative graph removal).",
            "key_insights": "Context-aware refinement of contradictory persona sentences (rather than removing them) produces richer, more specific memory content that improves long-term conversational response quality. Contradictions are often resolvable when contextual dialogue fragments are considered (the paper found ~65% of high-confidence contradictory pairs could be judged consistent when context is included). Iterative graph-based scheduling of refinements yields substantial API cost/time savings without harming performance.",
            "uuid": "e4789.0",
            "source_info": {
                "paper_title": "Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ChatGPT (used)",
            "name_full": "ChatGPT (Instruct-tuned LLM, used via API)",
            "brief_description": "An instruction-tuned large language model (used as the LLM in CAFFEINE) applied for both persona refinement (selecting strategy and generating refined personas) and zero-shot response generation conditioned on retrieved personas.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "ChatGPT (OpenAI, used for refinement and response generation)",
            "agent_description": "A 175B-parameter instruction-tuned LLM (InstructGPT family) used in two roles: (1) to choose between resolution/disambiguation/preservation strategies and generate refined persona sentences given persona pairs and their dialogue contexts; (2) to generate dialogue responses conditioned on current conversation context plus retrieved persona memory (zero-shot prompts). ChatGPT is accessed via API (LangChain used to manage calls).",
            "memory_type": "uses external retrieved memory (top-k persona sentences retrieved from a long-term persona store)",
            "memory_description": "ChatGPT consumes retrieved persona sentences (from Contriever + long-term memory M) concatenated with the dialogue context as input prompts for zero-shot response generation. Refined personas are written back to M by the CAFFEINE pipeline; ChatGPT itself does not implement the persistent memory store.",
            "task_name": "Persona refinement (generation) and persona-conditioned response generation",
            "task_description": "Two sub-tasks: (a) Given two contradictory persona sentences and their originating dialogue fragments, select an appropriate strategy and generate refined persona(s); (b) Given dialogue context and retrieved persona sentences, generate the next response (1-3 sentences) using persona memory.",
            "benchmark_name": "Multi-Session Chat (MSC) (used to evaluate downstream response generation quality)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "ChatGPT is the LLM used to consume retrieved memory and the refined persona store; the paper reports system-level comparisons (CAFFEINE vs baselines) rather than isolated ChatGPT-only ablations. Improvements in response metrics are reported when ChatGPT is provided with CAFFEINE-refined memories versus no memory or other memory management strategies.",
            "limitations_or_challenges": "Operational costs of API usage (paper reports $35.52 for CAFFEINE refinement and $27.09 for response generation on their experiments); potential under-utilization of long refined persona inputs by ChatGPT due to LLM long-context effectiveness limits (cites 'Lost in the middle' phenomenon).",
            "key_insights": "Large LLMs can be effective for both generating refinements to memory items and for conditioning responses on retrieved memories, but prompts/format and memory management (pruning/refinement) materially affect downstream performance and cost.",
            "uuid": "e4789.1",
            "source_info": {
                "paper_title": "Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Contriever Retriever",
            "name_full": "Contriever (unsupervised dense retrieval model)",
            "brief_description": "A dense information retrieval model (Contrastive unsupervised training) used to retrieve top-k relevant persona sentences from the long-term memory store to provide context for response generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Contriever (memory retriever)",
            "agent_description": "An unsupervised dense retriever employed to fetch top-k relevant persona sentences from the long-term persona memory given the current conversation context. The paper uses Contriever (k=20 by default) to construct the set of persona memories provided to the LLM for response generation.",
            "memory_type": "dense retrieval from a stored persona database (vector store)",
            "memory_description": "Contriever encodes dialogue context and candidate persona sentences into vector embeddings and returns the top-k persona sentences (k set to 20 in main experiments) from the persona memory store; retrieved personas are then fed to ChatGPT to condition response generation.",
            "task_name": "Memory retrieval for persona-grounded response generation",
            "task_description": "Given a dialogue context, retrieve the most relevant persona sentences (from refined/persona-expanded long-term memory) to supply the LLM for next-turn generation; evaluated as part of end-to-end response generation on the MSC dataset.",
            "benchmark_name": "Multi-Session Chat (MSC)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Contriever is used as the retrieval component across all memory-enabled conditions. The paper reports results for multiple k values (k=12,20,30) and shows CAFFEINE improvements are robust across k; specific retrieval-level ablations are not the focus, but retrieval is a required component to enable memory-augmented response generation.",
            "limitations_or_challenges": "No retrieval-specific failure analysis detailed beyond noting retrieval is a component; overall system performance depends on retrieval quality and the size/quality of the persona memory store.",
            "key_insights": "Using a dense retriever to obtain a focused set of persona sentences is an effective way to present LLMs with long-term memory items; tuning k and ensuring high-quality stored persona items (via CAFFEINE refinement) improves downstream response generation.",
            "uuid": "e4789.2",
            "source_info": {
                "paper_title": "Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Keep me updated! memory management in long-term conversations",
            "rating": 2
        },
        {
            "paper_title": "Beyond goldfish memory: Long-term open-domain conversation",
            "rating": 2
        },
        {
            "paper_title": "Like hiking? you probably enjoy nature: Persona-grounded dialog with commonsense expansions",
            "rating": 2
        },
        {
            "paper_title": "Persona expansion with commonsense knowledge for diverse and consistent response generation",
            "rating": 2
        },
        {
            "paper_title": "Dialogue natural language inference",
            "rating": 1
        }
    ],
    "cost": 0.013669499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement</h1>
<p>Hana Kim ${ }^{1}$ Kai Tzu-iunn Ong ${ }^{2}$ Seoyeon Kim ${ }^{2}$ Dongha Lee ${ }^{2}$ Jinyoung Yeo ${ }^{2}$<br>Department of Computer Science ${ }^{1}$, Artificial Intelligence ${ }^{2}$, Yonsei University<br>{hana.kim,ktio89, emseoyk, donalee, jinyeo}@yonsei.ac.kr</p>
<h4>Abstract</h4>
<p>Memorizing and utilizing speakers' personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation. While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/.</p>
<h2>1 Introduction</h2>
<p>Memorizing participants' personal information and conversing accordingly is important for dialogue systems to maintain long-term intimacy with users (Adiwardana et al., 2020). For that, studies have proposed datasets of long-term conversations, which require dialogue systems to memorize and utilize speakers' personas from past dialogue sessions to generate proper responses (Xu et al., 2022; Bae et al., 2022). Regardless, human-authored personas can be generic and over-simplified, limiting the generation of diverse and engaging responses.</p>
<p>Intuitively, this can be addressed by expanding existing personas with commonsense expansion (Majumder et al., 2020). However, such a naive remedy can raise contradiction between personas (e.g., "I am lazy" and "I clean my room every day"), especially as sessions accumulate (Figure 7), hindering consistent response generation. While we can simply get rid of contradictory personas utilizing external modules such as models for natural language inference (NLI), it yields sub-optimal
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Contradictory personas can co-exist and provide rich speaker information for the conversation when their contexts are considered (an empirical example).
results (Section 3.2). Also, avoiding contradictory personas (Bae et al., 2022; Kim et al., 2023) does not align with human personality traits. Since human personality is context-dependent (van Oers et al., 2005), we naturally exhibit different personalities and behaviors in different contexts, allowing personas with contradictory interpretations to coexist as one's personas, as shown in Figure 1.</p>
<p>Motivated by these, in this paper, we tackle such bottleneck of persona expansion in longterm conversations. Specifically, we focus on transforming contradictory personas into sentences that contain richer speaker information. To this end, we present Caffeine, a Context-Aware reFinement Framework for contradictory pErsonas IN long-tErm conversations. Caffeine leverages large language models (LLMs) to iteratively refine the contradictory personas within/across the session(s) based on their contextual background with designed strategies. Our contributions are two-fold: (i) To the best of our knowledge, we are the first to explore commonsense-based persona expansion in multi-session settings; (ii) CAFFEINE enables better response generation in long-term conversations in both automatic and human evaluations. Also, it refines contradictory personas in a human-like manner, eliciting persona sentences that are superior in</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: At the end of each dialogue session, CAFFEINE refines contradictory personas within/across the session(s) and saves the refined version to the dialogue model's memory for response generation in the next session.
various criteria while being cost- and time-efficient.</p>
<h2>2 Approach</h2>
<p>Long-term conversations involve multiple dialogue sessions. At the end of each session, we perform:</p>
<h3>2.1 Commonsense-based Persona Expansion</h3>
<p>Following Majumder et al. (2020), we perform commonsense expansion on personas derived from the conversation using COMET (Hwang et al., 2020). COMET generates commonsense knowledge based on cause-effect relation types (e.g., XNEED and XWANT). For example, "I drink coffee" $\rightarrow$ "I want to stay awake". Implementation details on COMET expansion are in Appendix A.1.</p>
<h3>2.2 CAFFEINE</h3>
<p>We present the overview of CAFFEINE in Figure 2.</p>
<h3>2.2.1 Preparation: Graph Construction for Iterative Persona Refinement</h3>
<p>After expansion, we identify contradictory personas by computing the probability of contradiction $\delta$ between all personas with an external NLI model. To refine contradictory personas cost- and time-efficiently, we adopt iterative refinement with a graph structure: Contradictory pairs with $\delta$ larger than a threshold $\mu$ are added as nodes $V$ (edges $E=\left(\delta_{1}, \delta_{2}, \cdots, \delta_{| E \mid}\right)$ ) to the refinement graph $G .{ }^{1}$ Then, we locate the node (persona) $p_{1}$ with the largest $\Sigma \delta$ within its neighborhood. We select $p_{1}$ and the adjacent node $p_{2}$ with the highest $\delta$ with $p_{1}$ for the first refinement iteration (Algorithm 1).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>2.2.2 Context-aware Persona Refinement</h3>
<p>As shown in Figure 1, personas causing contradiction can be logically acceptable and beneficial for conversations if contextual cues from their origin context are appended via commonsense reasoning. For that, we propose the following refinement strategies for the LLM to choose from:</p>
<p>Strategy I: Resolution. Inspired by entity resolution (Benjelloun et al., 2009), persona resolution resolves the contradiction between personas by seamlessly merging them into one informative sentence based on the contextual background from where they are derived (Figure 3 (a), Figure 1 is also an example of persona resolution).</p>
<p>Strategy II: Disambiguation. Contradiction between two statements can stem from the lack of contexts, known as pragmatic ambiguity (Macagno and Bigi, 2018). Drawn from entity disambiguation (Dredze et al., 2010), persona disambiguation specifies each persona with relevant information from their contextual backgrounds (Figure 3 (b)).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Empirical demonstration of our strategies. Top: relevant contexts; Mid: contradictory personas; Bottom: refined persona(s).</p>
<p>Strategy III: Preservation. Due to the limitation of NLI models, personas predicted as contradictory</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Session 2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Session 3</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Session 4</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Session 5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Settings</td>
<td style="text-align: center;">B-1</td>
<td style="text-align: center;">R-1</td>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">B-1</td>
<td style="text-align: center;">R-1</td>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">B-1</td>
<td style="text-align: center;">R-1</td>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">B-1</td>
<td style="text-align: center;">R-1</td>
<td style="text-align: center;">R-L</td>
</tr>
<tr>
<td style="text-align: center;">No Memory</td>
<td style="text-align: center;">20.75</td>
<td style="text-align: center;">19.38</td>
<td style="text-align: center;">15.16</td>
<td style="text-align: center;">20.42</td>
<td style="text-align: center;">19.53</td>
<td style="text-align: center;">15.09</td>
<td style="text-align: center;">19.88</td>
<td style="text-align: center;">19.56</td>
<td style="text-align: center;">14.98</td>
<td style="text-align: center;">19.87</td>
<td style="text-align: center;">20.16</td>
<td style="text-align: center;">15.33</td>
</tr>
<tr>
<td style="text-align: center;">GOLD</td>
<td style="text-align: center;">21.19</td>
<td style="text-align: center;">19.86</td>
<td style="text-align: center;">15.50</td>
<td style="text-align: center;">21.24</td>
<td style="text-align: center;">20.16</td>
<td style="text-align: center;">15.47</td>
<td style="text-align: center;">20.57</td>
<td style="text-align: center;">19.94</td>
<td style="text-align: center;">15.16</td>
<td style="text-align: center;">20.49</td>
<td style="text-align: center;">20.53</td>
<td style="text-align: center;">15.55</td>
</tr>
<tr>
<td style="text-align: center;">+ NLI-remove</td>
<td style="text-align: center;">20.81</td>
<td style="text-align: center;">19.98</td>
<td style="text-align: center;">15.26</td>
<td style="text-align: center;">21.04</td>
<td style="text-align: center;">20.28</td>
<td style="text-align: center;">15.52</td>
<td style="text-align: center;">21.33</td>
<td style="text-align: center;">20.69</td>
<td style="text-align: center;">15.91</td>
<td style="text-align: center;">21.43</td>
<td style="text-align: center;">20.75</td>
<td style="text-align: center;">15.95</td>
</tr>
<tr>
<td style="text-align: center;">+ NLI-recent</td>
<td style="text-align: center;">20.87</td>
<td style="text-align: center;">20.09</td>
<td style="text-align: center;">15.39</td>
<td style="text-align: center;">21.14</td>
<td style="text-align: center;">20.52</td>
<td style="text-align: center;">15.71</td>
<td style="text-align: center;">21.46</td>
<td style="text-align: center;">20.79</td>
<td style="text-align: center;">15.97</td>
<td style="text-align: center;">21.60</td>
<td style="text-align: center;">20.97</td>
<td style="text-align: center;">16.11</td>
</tr>
<tr>
<td style="text-align: center;">+ CAFFEINE</td>
<td style="text-align: center;">20.93</td>
<td style="text-align: center;">20.18</td>
<td style="text-align: center;">15.47</td>
<td style="text-align: center;">21.41</td>
<td style="text-align: center;">20.72</td>
<td style="text-align: center;">15.86</td>
<td style="text-align: center;">21.67</td>
<td style="text-align: center;">21.00</td>
<td style="text-align: center;">16.15</td>
<td style="text-align: center;">21.92</td>
<td style="text-align: center;">21.23</td>
<td style="text-align: center;">16.31</td>
</tr>
<tr>
<td style="text-align: center;">COMET-EXP</td>
<td style="text-align: center;">21.23</td>
<td style="text-align: center;">19.82</td>
<td style="text-align: center;">15.44</td>
<td style="text-align: center;">20.95</td>
<td style="text-align: center;">19.90</td>
<td style="text-align: center;">15.38</td>
<td style="text-align: center;">20.33</td>
<td style="text-align: center;">20.02</td>
<td style="text-align: center;">15.18</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">20.27</td>
<td style="text-align: center;">15.37</td>
</tr>
<tr>
<td style="text-align: center;">+ NLI-remove</td>
<td style="text-align: center;">20.72</td>
<td style="text-align: center;">19.96</td>
<td style="text-align: center;">15.27</td>
<td style="text-align: center;">21.12</td>
<td style="text-align: center;">20.40</td>
<td style="text-align: center;">15.56</td>
<td style="text-align: center;">21.66</td>
<td style="text-align: center;">20.77</td>
<td style="text-align: center;">15.88</td>
<td style="text-align: center;">21.77</td>
<td style="text-align: center;">20.91</td>
<td style="text-align: center;">16.01</td>
</tr>
<tr>
<td style="text-align: center;">+ NLI-recent</td>
<td style="text-align: center;">20.73</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">15.33</td>
<td style="text-align: center;">21.16</td>
<td style="text-align: center;">20.40</td>
<td style="text-align: center;">15.64</td>
<td style="text-align: center;">21.57</td>
<td style="text-align: center;">20.77</td>
<td style="text-align: center;">15.89</td>
<td style="text-align: center;">21.78</td>
<td style="text-align: center;">20.99</td>
<td style="text-align: center;">16.09</td>
</tr>
<tr>
<td style="text-align: center;">+ CAFFEINE</td>
<td style="text-align: center;">20.97</td>
<td style="text-align: center;">20.06</td>
<td style="text-align: center;">15.32</td>
<td style="text-align: center;">21.63</td>
<td style="text-align: center;">20.73</td>
<td style="text-align: center;">15.86</td>
<td style="text-align: center;">21.97</td>
<td style="text-align: center;">21.10</td>
<td style="text-align: center;">16.18</td>
<td style="text-align: center;">22.26</td>
<td style="text-align: center;">21.32</td>
<td style="text-align: center;">16.37</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance in response generation. Bold and underline show the best and second-highest in each column.
may be consistent and may not require refinement. Thus, we allow the LLM to preserve personas as they are when their contexts suggest so.</p>
<p>In practice, with contradictory personas $\mathcal{P}=$ $\left(p_{1}, p_{2}\right)$ and relevant dialogue contexts $\mathcal{D}=$ $\left(d_{1}, d_{2}\right)$ from where $\mathcal{P}$ are derived, we prompt the LLM to choose one out of the three strategies $S$ with rationale and generate the refinement $\mathcal{R}$ :</p>
<p>$$
\begin{gathered}
\mathcal{S}^{<em>}=\underset{\mathcal{S}}{\operatorname{argmax}} P_{\mathrm{LLM}}(\mathcal{S} \mid \mathcal{P}, \mathcal{D}) \
\Rightarrow \mathcal{R}^{</em>}=\underset{\mathcal{R}}{\operatorname{argmax}} P_{\mathrm{LLM}}(\mathcal{R} \mid \mathcal{P}, \mathcal{D}, \mathcal{S}^{*})
\end{gathered}
$$</p>
<p>where $\Rightarrow$ denotes a sequential generation of tokens. $D$ consists of $w$ consecutive utterances. ${ }^{2}$ When $p$ is a persona generated by COMET, we use $D$ of its original persona and concatenate the original persona with $D$. After refinement, we save $\mathcal{R}^{*}$ to long-term memory $\mathcal{M}$ and remove $\mathcal{P}$ from the graph $G$, and start the next iteration (Algorithm 1).</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Iterative Graph Refinement
Require: Refinement graph \(G(V, E)\)
Ensure: The dialogue model&#39;s long-term memory \(\mathcal{M}\)
    \(\mathcal{M} \leftarrow \mathcal{M} \backslash V\)
    while \(G \neq \emptyset\) do
        Select \(p_{1}\) in \(V\) with the highest \(\Sigma \delta\)
        Select \(p_{2}\), a neighbor of \(p_{1}\) with the highest \(\delta\)
        \(\left(\mathcal{S}^{*}, \mathcal{R}^{*}\right) \leftarrow \operatorname{Refine}\left(p_{1}, p_{2}\right)\)
        \(\mathcal{M} \leftarrow \mathcal{M} \cup \mathcal{R}^{*}\)
        Remove \(p_{1}, p_{2}\) from \(G\)
        Remove isolated nodes from \(G\)
    return \(\mathcal{M}\)
</code></pre></div>

<h2>3 Experiments</h2>
<h3>3.1 Experimental Settings</h3>
<p>Dataset. We use Multi-Session Chat (MSC) (Xu et al., 2022) to conduct experiments. MSC takes</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the dialogues from Persona-Chat (Zhang et al., 2018) and extends their follow-up conversations throughout several sessions. Each session comes with speakers' personas authored by humans.</p>
<p>Models and baselines. In this work, we use ChatGPT (OpenAI, 2023) for CAFFEINE and response generation (RG), ${ }^{3}$ and Contriever (Izacard et al., 2021) to retrieve top- $k$ relevant personas from longterm memory. ${ }^{4}$ As for the NLI model, we use RoBERTa (Liu et al., 2019) fine-tuned on the MNLI dataset (Williams et al., 2017). To evaluate the effectiveness of CAFFEINE in RG, we apply it to: (i) COMET-EXP, human-authored personas with COMET expansion; (ii) GOLD, human-authored personas. We include this setting as a contradiction can also exist among un-expanded personas. ${ }^{5}$ Also, to justify our choice to refine rather than remove, we compare CAFFEINE with two baselines: NLI-remove and NLI-recent. ${ }^{6}$ The NLI-remove approach filters out personas that contradict at least one other persona with $\delta \geq 0.8$ via the NLI model. Similarly, the NLI-recent approach also uses the NLI model, but it differs by keeping the most recent persona in contradictory persona pairs and removing the older one (Bae et al., 2022), thereby prioritizing updated personas over time.</p>
<h3>3.2 Results and Discussion</h3>
<p>We present the empirical findings of the following research questions guiding our experiments:</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>CAFFEINE vs.</th>
<th>GOLD</th>
<th>COMET-EXP</th>
<th>NLI-remove</th>
</tr>
</thead>
<tbody>
<tr>
<td>Naturalness</td>
<td>$73 \%{ }^{*}$</td>
<td>$71 \%{ }^{*}$</td>
<td>$79 \%{ }^{*}$</td>
</tr>
<tr>
<td>Consistency</td>
<td>$66 \%{ }^{*}$</td>
<td>$62 \%{ }^{*}$</td>
<td>$67 \%{ }^{*}$</td>
</tr>
<tr>
<td>Specificity</td>
<td>$55 \%$</td>
<td>$53 \%$</td>
<td>$51 \%$</td>
</tr>
<tr>
<td>Engagingness</td>
<td>$63 \%{ }^{*}$</td>
<td>$64 \%{ }^{*}$</td>
<td>$66 \%{ }^{*}$</td>
</tr>
<tr>
<td>Overall</td>
<td>$62 \%{ }^{*}$</td>
<td>$63 \%{ }^{*}$</td>
<td>$67 \%{ }^{*}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of generated responses. We report CAFFEINE's winning rate. (*: p-value $&lt;0.05$ )</p>
<p>RQ1: Does CAFFEINE benefit response generation in long-term conversations?
RQ2: Does CAFFEINE refine personas in a way that aligns with human judgment?
RQ3: Is CAFFEINE cost- and time-efficient?
CAFFEINE improves response generation (RQ1). To evaluate the efficacy of CAFFEINE, we conduct experiments on response generation (RG) using sessions 2 to 5 of each dialogue from MSC. Table 1 shows the results of RG in MSC with BLEU-1 (B-1), ROUGE-1 (R-1), and ROUGE-L (R-L) (Papineni et al., 2002; Lin, 2004). Applying CAFFEINE yields performance gains, which are more significant as sessions increase. Also, CAFFEINE consistently outperforms NLI-remove and NLI-recent, showing that leveraging contradictory personas elicits a more informative memory for RG than removing them. Compared to NLIremove, the improved efficacy of NLI-recent is attributed to its focus on the recency of personas. By eliminating outdated personas from contradictory pairs, NLI-recent enhances RG, yielding responses more aligned with the current dialogue context. However, despite the enhancements in NLIrecent performance, CAFFEINE still exhibits superior performance. Furthermore, the performance brought by CAFFEINE exhibits a continuously rising trend as the number of previous sessions increases, while baselines yield a flat or downward tendency. These demonstrate the effectiveness of CAFFEINE in multi-session conversations. Table 2 shows the human evaluation results of randomly sampled 50 responses conducted by 3 judges from Amazon Mechanical Turk (Appendix F). CAFFEINE yields responses that are better (i.e., winning) in several criteria. We provide examples of RG in Appendix E.</p>
<p>CAFFEINE elicits personas that align with human preference (RQ2). We sample 100 persona pairs refined with "resolution" or "disambiguation" and ask 3 judges " whether they are contradictory
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Human evaluation results on (i) refined personas and (ii) the refinement process (p-value $&lt;0.05$ ).
before refinement from a human standpoint". 89 samples that receive "yes" from all judges are used for the evaluation. Judges compare the refined version with its un-refined version and vote if they agree: it is less contradictory (Consistency); it provides more speaker information (Specificity); it is more useful when having a conversation with this person (Helpfulness); it has better quality (Overall); the refinement process is reasonable (Humanlikeness). Figure 4 shows that personas refined by CAFFEINE are greater in all criteria, especially helpfulness. This supports our argument that contradictory personas become sentences with rich speaker information for the conversation if cues from their relevant contexts are included, and explains the performance gain in RG. Also, a $69 \%$ agreement on human-likeness demonstrates that CAFFEINE's refinement is in line with human judgment. Refinement examples are presented in Appendix E.</p>
<p>CAFFEINE refines personas in a cost- and timeefficient manner (RQ3). In CAFFEINE, we remove refined $\left(p_{1}, p_{2}\right)$ from $G$ after refinement. Figure 5 compares this with a setting without such removal, i.e., all $|E|$ contradictory persona pairs in $G$ are all refined (denoted as ALL). While yielding similar RG performance (Session 2-5), ours requires significantly fewer API calls per dialogue per session, especially as the sessions accumulate (9-fold $\rightarrow 21$-fold more cost- and time-efficient).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Cost and time efficiency of our algorithm.</p>
<h2>4 Related Work</h2>
<p>Many studies have utilized commonsense knowledge for response generation. For instance: leveraging knowledge from a general-purpose knowledge model (Zhou et al., 2022b; Wu et al., 2022;</p>
<p>Liu et al., 2022; Li et al., 2023); training commonsense generators for dialogues via humanannotated dataset (Ghosal et al., 2022); formulating commonsense-linking between knowledge graphs and dialogues (Gao et al., 2022); modeling speakers' mutual beliefs before a response (Zhou et al., 2022a); integrating implicit information in dialogues into rationale for more effective response generation (Chae et al., 2023). While most work focuses on speaker utterances, we leverage speaker personas to address commonsense knowledge in response generation.</p>
<h2>5 Conclusion</h2>
<p>This work pioneers commonsense-based persona expansion in multi-session settings and presents a context-aware refinement framework that leverages contradictory personas to elicit a memory with richer speaker details. C AFFEINE improves response generation in long-term conversations and demonstrates human-like refinement of contradictory personas while being cost- and time-efficient.</p>
<h2>6 Limitations</h2>
<p>Our study has the following limitations: (1) Apart from the proposed C AFFEINE, our results can be affected by the quality of commonsense models and the knowledge graph on which they are trained. As future work, we plan to leverage LLM for persona expansion; (2) Our refinement graph stores contradictory personas that are predicted as contradiction with a probability higher than a pre-defined threshold by the NLI model. Our framework may miss personas that actually need a refinement due to the limitation of the NLI model; (3) While we pioneer the commonsense-based persona expansion in multi-session settings, we only consider one speaker's persona at a time in our refinement framework. Since different people can demonstrate different personality traits and behaviors in the same commonly experienced event (e.g., discussed topic), we acknowledge there can be potential performance gain in response generation if such modeling is included; (4) In this work, we employ LLMs to generate responses based on the dialogue context and retrieved memories (i.e., both speakers' personas) in a zero-shot setting. However, since the refined personas tend to be longer and contain more information, it is possible that the LLM can not fully utilize the presented personas in its inputs as they get longer (Liu et al., 2023). We
plan to address a better utilization of LLM's input texts for response generation in future work.</p>
<h2>7 Ethical Statement</h2>
<p>LLMs and COMET can generate sensual, harmful, biased, offensive, or violent content. Authors avoid such content from appearing in the main text, figure, and appendix. We guarantee fair compensation for workers we hire on Amazon Mechanical Turk. We ensure an effective pay rate higher than $\$ 18$ per hour based on the estimated time required to complete the tasks.</p>
<h2>8 Acknowledgements</h2>
<p>This work was supported by Institute of Information \&amp; Communications Technology Planning \&amp; Evaluation (IITP) grant funded by the Korean government (MSIT)(No.2020-0-01361, Artificial Intelligence Graduate School Program (Yonsei University)) and (No.2021-0-02068, Artificial Intelligence Innovation Hub) and (No.2022-0-00077, AI Technology Development for Commonsense Extraction, Reasoning, and Inference from Heterogeneous Data). Jinyoung Yeo is a corresponding author.</p>
<h2>References</h2>
<p>Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. 2020. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977.</p>
<p>Sanghwan Bae, Donghyun Kwak, Soyoung Kang, Min Young Lee, Sungdong Kim, Yuin Jeong, Hyeri Kim, Sang-Woo Lee, Woomyoung Park, and Nako Sung. 2022. Keep me updated! memory management in long-term conversations. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3769-3787, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Omar Benjelloun, Hector Garcia-Molina, David Menestrina, Qi Su, Steven Euijong Whang, and Jennifer Widom. 2009. Swoosh: a generic approach to entity resolution. The VLDB Journal, 18:255-276.</p>
<p>Hyungjoo Chae, Yongho Song, Kai Ong, Taeyoon Kwon, Minjin Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, and Jinyoung Yeo. 2023. Dialogue chain-of-thought distillation for commonsense-aware conversational agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5606-5632, Singapore. Association for Computational Linguistics.</p>
<p>Mark Dredze, Paul McNamee, Delip Rao, Adam Gerber, and Tim Finin. 2010. Entity disambiguation for knowledge base population. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 277-285, Beijing, China. Coling 2010 Organizing Committee.</p>
<p>Silin Gao, Jena D. Hwang, Saya Kanno, Hiromi Wakaki, Yuki Mitsufuji, and Antoine Bosselut. 2022. ComFact: A benchmark for linking contextual commonsense knowledge. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1656-1675, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Deepanway Ghosal, Siqi Shen, Navonil Majumder, Rada Mihalcea, and Soujanya Poria. 2022. CICERO: A dataset for contextualized commonsense inference in dialogues. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5010-5028, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi. 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In AAAI Conference on Artificial Intelligence.</p>
<p>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118.</p>
<p>Donghyun Kim, Youbin Ahn, Wongyu Kim, Chanhee Lee, Kyungchan Lee, Kyong-Ho Lee, Jeonguk Kim, Donghoon Shin, and Yeonsoo Lee. 2023. Persona expansion with commonsense knowledge for diverse and consistent response generation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1139-1149, Dubrovnik, Croatia. Association for Computational Linguistics.</p>
<p>Minju Kim, Beong-woo Kwak, Youngwook Kim, Hong-in Lee, Seung-won Hwang, and Jinyoung Yeo. 2022. Dual task framework for improving persona-grounded dialogue dataset. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10912-10920.</p>
<p>Siheng Li, Wangjie Jiang, Pengda Si, Cheng Yang, Qiu Yao, Jinchao Zhang, Jie Zhou, and Yujiu Yang. 2023. Enhancing dialogue generation with conversational concept flows. In Findings of the Association for Computational Linguistics: EACL 2023, pages 15141525, Dubrovnik, Croatia. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Yiting Liu, Liang Li, Beichen Zhang, and Qingming Huang. 2022. Think beyond words: Exploring context-relevant visual commonsense for diverse dialogue generation. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3106-3117, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Fabrizio Macagno and Sarah Bigi. 2018. Types of dialogue and pragmatic ambiguity. Argumentation and Language-Linguistic, Cognitive and Discursive Explorations, pages 191-218.</p>
<p>Bodhisattwa Prasad Majumder, Harsh Jhamtani, Taylor Berg-Kirkpatrick, and Julian McAuley. 2020. Like hiking? you probably enjoy nature: Personagrounded dialog with commonsense expansions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9194-9206, Online. Association for Computational Linguistics.</p>
<p>OpenAI. 2023. Chatgpt. https://openai.com/blog/ chatgpt.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Kees van Oers, Margreet Klunder, and Piet J Drent. 2005. Context dependence of personalities: risktaking behavior in a social and a nonsocial situation. Behavioral Ecology, 16(4):716-723.</p>
<p>Sean Welleck, Jason Weston, Arthur Szlam, and Kyunghyun Cho. 2019. Dialogue natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3731-3741, Florence, Italy. Association for Computational Linguistics.</p>
<p>Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426.</p>
<p>Sixing Wu, Ying Li, Ping Xue, Dawei Zhang, and Zhonghai Wu. 2022. Section-aware commonsense knowledge-grounded dialogue generation with pretrained language model. In Proceedings of the 29th International Conference on Computational Linguistics, pages 521-531, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.</p>
<p>Jing Xu, Arthur Szlam, and Jason Weston. 2022. Beyond goldfish memory: Long-term open-domain conversation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5180-5197, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing dialogue agents: I have a dog, do you have pets too? In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2204-2213, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Pei Zhou, Hyundong Cho, Pegah Jandaghi, Dong-Ho Lee, Bill Yuchen Lin, Jay Pujara, and Xiang Ren. 2022a. Reflect, not reflex: Inference-based common ground improves dialogue response quality. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1045010468, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Pei Zhou, Karthik Gopalakrishnan, Behnam Hedayatnia, Seokhwan Kim, Jay Pujara, Xiang Ren, Yang Liu, and Dilek Hakkani-Tur. 2022b. Think before you speak: Explicitly generating implicit commonsense knowledge for response generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1237-1252, Dublin, Ireland. Association for Computational Linguistics.</p>
<h2>A Implementation Details</h2>
<h2>A. 1 Commonsense Expansion with COMET</h2>
<p>At the end of each dialogue session, we augment personas derived from the current session with new personas via COMET (Hwang et al., 2020), a widely used commonsense model generating rich and diverse commonsense expansions of a given statement based on cause-effect relation. Among the 23 possible candidate relation types, following prior works on commonsense-based persona expansion (Majumder et al., 2020; Kim et al., 2022), we choose 9 relation types: xAttr, xEffect, xIntent, xNeed, xReact, xWant, oEffect, OREACT, and OWANT for our expansion, where the prefix ' $x$ ' indicates an effect or cause on that person and ' $o$ ' denotes others. After persona expansion via COMET, we leverage an external NLI model to initially filter out improper expansion. Specifically, when a new persona $p^{n}$ is generated based on an original persona $p^{o}$ ( 1 original persona yields nine 9 personas), we filter it out if the NLI model predicts the logical relationship between $p^{n}$ and $p^{o}$ is contradiction with $\delta&gt;0.33$. Note that this is different from the NLI-remove baseline, as here we solely address a one-to-one relationship between a generated persona and its corresponding original persona, while the latter addresses the contradiction among all possible combinations of personas within/across the dialogue session(s). We report the statistics of this initial filtering in Table 3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">At the End of</th>
<th style="text-align: left;">Filtered (\%)</th>
<th style="text-align: left;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Session #1</td>
<td style="text-align: left;">$2830(6.84 \%)$</td>
<td style="text-align: left;">41391</td>
</tr>
<tr>
<td style="text-align: left;">Session #2</td>
<td style="text-align: left;">$2715(7.39 \%)$</td>
<td style="text-align: left;">36718</td>
</tr>
<tr>
<td style="text-align: left;">Session #3</td>
<td style="text-align: left;">$2935(7.43 \%)$</td>
<td style="text-align: left;">39523</td>
</tr>
<tr>
<td style="text-align: left;">Session #4</td>
<td style="text-align: left;">$2971(7.58 \%)$</td>
<td style="text-align: left;">39198</td>
</tr>
</tbody>
</table>
<p>Table 3: Initial filtering of improper expansion.</p>
<h2>A. 2 Contriever</h2>
<p>In our experiments on persona-grounded response generation (RG), we adopt Contriever (Izacard et al., 2021) as the memory retriever to retrieve top- $k$ relevant personas from long-term memory based on the current conversation. Contriever is a dense information retriever trained with unsupervised contrastive learning. Even without supervision, it has shown remarkable capabilities in information retrieval tasks, particularly in demonstrating competitiveness with BM25 in Recall at
$100(\mathrm{R} @ 100)$ on the benchmark for zero-shot retrieval.</p>
<h2>A. 3 Large language model</h2>
<p>In this work, we employ ChatGPT for the proposed CAFFEINE and response generation. ChatGPT is an LLM with 175B parameters based on InstructGPT (Ouyang et al., 2022) ${ }^{7}$. ChatGPT is trained to follow instructions given by users and return requested information in a conversational manner. We use LangChain ${ }^{8}$ to send API calls to OpenAI API. The prompt used in CAFFEINE and response generation are in Table 6 and Table 7, respectively.</p>
<h2>A. 4 Linking Personas to their Contextual Backgrounds</h2>
<p>In the adopted MSC dataset, human annotators summarize information in a speaker's utterance and use it to derive a persona sentence. As demonstrated in Figure 6, since not every utterance contains enough information to conclude a persona for that speaker, some utterances are not paired with a persona sentence. In our experiment for context-aware persona refinement, we utilize contradictory personas $\mathcal{P}=\left(p_{1}, p_{2}\right)$ and their contextual backgrounds, i.e., relevant dialogue contexts $\mathcal{D}=\left(d_{1}, d_{2}\right)$ from where they are derived. $d$ consists of $w$ consecutive sentences. In practice, $w$ can differ, as we link each persona with their relevant dialogue context by separating the past conversation into dialogue fragments based on utterances that have corresponding persona sentences. For instance, the $d_{i}$ for persona $p_{i}$ will be $d_{i}=\left(u_{1}, u_{2}\right)$, and $d_{i+1}=\left(u_{3}, u_{4}, \cdots, u_{6}\right)$ for $p_{i+1}$.</p>
<p>Dialogue fragments
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Demonstration of personas and their contextual backgrounds in the MSC dataset.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>A.5 Computational Resources and API Cost</h3>
<p>We run Contriever and the NLI model on eight NVIDIA RTX A5000 GPUs. For ChatGPT API usage, we use $\$ 35.52$ on CAFFEINE's refinement, and $\$ 27.09$ on response generation.</p>
<h2>B Performance in Response Generation</h2>
<p>In response generation, top- $k$ relevant persona sentences are retrieved from the long-term memory to assist response generation. In the main text, we report the mode performance in response generation with $k=20$, the results with $k=12$ and $k=30$ are presented in Table 4.</p>
<h2>C Contradictory Personas in Multi-session Conversations</h2>
<p>As human personalities are context-independent, we display different personalities in different contexts and adapt to new situations. This naturally leads to personas with contradictory literal interpretations to co-exist as one's persona. Such a phenomenon does not harm human conversations. However, contradictions between personas can lead to inconsistent response generation, hindering user interest in the dialogue systems.</p>
<p>In our study on the Multi-session Chat dataset, we first find that contradictory personas exist in human-authored personas (Figure 7 (a)). Then, we show that expanding existing human-authored personas via commonsence expansion can lead to orders-of-magnitude more contradictory personas that hinder user interest in the conversation (Figure 7 (b)) (Kim et al., 2023).</p>
<p>Personas can contradict other personas from the same sessions (intra-session) and from the previous sessions (inter-session). When comparing COMETEXP with the human-authored personas (GOLD), we observe that as the number of previous sessions increases, the intra-session contradiction slightly increases, whereas the inter-session contradiction skyrockets significantly. Although such a rising trend appears similarly in GOLD and COMETEXP, the total count in COMET-EXP is order-ofmagnitude larger. This supports the necessity of CAFFEINE, which refines the contradictory personas in the long-term memory of dialogue models in multi-session settings.</p>
<h2>D CAFFEINE vs. NLI models</h2>
<p>Noteworthily, Figure 8 shows that CAFFEINE determines that $65.45 \%$ of contradictory personas
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Contradiction among human-authored original personas (GOLD) and expanded personas (COMETEXP). The blue, red, and green lines represent the intrasession, inter-session, and total contradictory persona pairs, respectively.
(with $\delta \geq 0.80$ ) can be consistent without requiring any refinement when their contextual backgrounds are taken into account, indicating that our contextaware refinement can address the simplification of NLI models where they often solely compare the semantic representation of two statements without reasoning over their contexts. We employ two NLI models: the MNLI model (referred to as 'NLI model') and the DNLI model. Results with the DNLI model are presented in Table 5.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Proportion of selected strategies highlights the limitation of the NLI model.</p>
<h2>E Examples of Refinement and Response Generation</h2>
<p>We provide examples of response generation in Figure 9 and Figure 10. We can observe that while baselines provide short personas and yield inconsistent or unconstructive responses (green underlines), CAFFEINE offers informative personas (color red) and leads to a response that provides constructive suggestion for Persona A's Spanish learning (Figure 9) and a response that reflects Person B's situation and what B is looking for in a car (Figure 10).</p>
<p>Apart from the empirical examples demonstrated in figures in the main text, we have provided more examples for CAFFEINE's refinement in Table 8, 9, and 10 .</p>
<h1>F Details on Human Evaluation.</h1>
<h2>F. 1 Response Quality</h2>
<p>We outsource a human evaluation comparing the generated responses from our setting and those from the baselines via Amazon Mechanical Turk (AMT). We show the interface for the evaluation in Figure 11. We ask the human judges to compare the responses based on the following criteria:</p>
<ul>
<li>Naturalness: Which response is more humanlike?</li>
<li>Consistency: Which response is more consistent (aligned) with the dialogue context?</li>
<li>Specificity: Which response contains more speaker information?</li>
<li>Engagingness: Which response is more interesting?</li>
</ul>
<h2>F. 2 Refinement Quality</h2>
<p>We outsource a human evaluation comparing the personas before/after CAFFEINE via Amazon Mechanical Turk (AMT). We show the interface for this evaluation in Figure 12.</p>
<p>We ask the human judges to compare the persona before and after refinement based on the following criteria:</p>
<ul>
<li>Consistency: Is the refined version less contradictory or more reasonable than before?</li>
<li>Specificity: Does the refined version describe a person more specifically?</li>
<li>Helpfulness: Can the refined version be more helpful if you are having a conversation with this person?</li>
<li>Overall: Overall, do you prefer the refined version?</li>
<li>Human-likeness: Is the refinement process (generated rationales) reasonable?</li>
</ul>
<p>Note that before assessing CAFFEINE's refinement, we ask workers to determine whether the personas before refinement are actually contradictory from human standpoints. The assessment of the above criteria only begins if the answer is positive. Therefore, the reported human evaluation results are based on 89 out of 100 samples we provided.</p>
<h1>Dialogue Context</h1>
<p>A: Do you know where to get a copy of the The Fugitive movies?
B: Sure! if you go to justwatch.com it will tell you where to buy a digital copy or you can go to walmart.com or amazon. How's your Spanish learning going?
A: Perfect, thank you! I will look at all those places. I am still learning, and it is slow but I am trying!</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Speaker A's Personas (draw from retrieved top-20 memories)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Generated Response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GOLD</td>
<td style="text-align: center;">I am trying to learn Spanish better. <br> I'm learning Spanish. <br> The Fugitive is my all time favorite movie. <br> My Spanish language progress is not much I'd expect. <br> I like to speak in Spanish. <br> I like to watch that movie.</td>
<td style="text-align: center;">B: That's great that you're putting in the effort to learn Spanish! The Fugitive is a fantastic movie, by the way. How's your progress in Spanish been so far?</td>
</tr>
<tr>
<td style="text-align: center;">COMET-EXP</td>
<td style="text-align: center;">I am trying to learn Spanish better. <br> I'm learning Spanish. <br> The Fugitive is my all time favorite movie. <br> I want to learn Spanish. <br> I need to learn Spanish.</td>
<td style="text-align: center;">B: That's great to hear! Learning a new language can take time, but as long as you're putting in the effort, you'll make progress. Keep it up!</td>
</tr>
<tr>
<td style="text-align: center;">COMET-EXP <br> +CAFFEINE</td>
<td style="text-align: center;">I am trying to learn Spanish better. <br> I am trying to learn Spanish better by watching shows and movies with Spanish subtitles. <br> The Fugitive is my all time favorite movie. <br> I'm learning Spanish. <br> I'm learning Spanish by watching movies. <br> I need to read books to learn Spanish faster. <br> I need to go to the library to learn Spanish, and my girlfriend might be interested in learning too.</td>
<td style="text-align: center;">B: That's great to hear! Learning a new language can take time, but as long as you're putting in the effort, you'll get there. <br> Have you tried watching any Spanish movies with subtitles to practice?</td>
</tr>
</tbody>
</table>
<p>Figure 9: Example of Response Generation #1. While baselines yield short personas and inconsistent or unconstructive responses (green underlines), CAFFEINE offers informative personas (color red) and leads to a response that provides constructive suggestion for Person A's Spanish learning.</p>
<h1>Dialogue Context</h1>
<p>A: I always think used is the way to go; new cars lose so much value the second they leave the lot. I am all for them! If I didn't live in a tiny town with no charging stations I would've definitely gotten an electric car. Were you thinking about getting one?
B: I totally agree! I thought about getting electric because I've been thinking about the environment and the earth lately. I think I might change my major.
A: I really think you should do it! You can even get used ones, especially hybrids, I've seen more and more popping up for sale. Oh really, to what?</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Speaker B's Personas (draw from retrieved top-20 memories)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Generated Response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GOLD</td>
<td style="text-align: center;">I prefer imported cars. <br> I want to buy a car. <br> I am looking for a car. <br> I cannot afford a hyundai genesis. <br> I used to have a Ford Edge.</td>
<td style="text-align: center;">B: I cannot afford a hybrid at the moment, but maybe I'll consider it in the future. I was actually considering a pre-owned luxury car. Maybe something like a used BMW or Mercedes-Benz. What do you think?</td>
</tr>
<tr>
<td style="text-align: center;">COMET-EXP</td>
<td style="text-align: center;">I get a new car. <br> I want to have a good car. <br> I want to have a car. <br> I want to get a car. <br> I need to have a car. <br> I prefer imported cars. <br> I want to buy a car. <br> I am looking for a car. <br> I want to buy a new car.</td>
<td style="text-align: center;">B: I want to get a new car, preferably an imported one. I'm looking for something that combines good value and performance. Maybe a new BMW or Mercedes-Benz.</td>
</tr>
<tr>
<td style="text-align: center;">COMET-EXP <br> +CAFFEINE</td>
<td style="text-align: center;">I want to have a car. <br> As I work on my PhD, I'm considering buying a used car like a Honda Fit for its affordability and spaciousness. <br> I want to get a car. <br> I prefer imported cars. <br> Since I've been laid off from my job at Cisco, I'm hoping I can still buy a car. <br> I cannot afford a hyundai genesis. <br> I want to buy a car.</td>
<td style="text-align: center;">B: I am considering sticking with a used car for now, especially given my recent employment situation. A Honda Fit seems like a practical choice, given its affordability and spaciousness. It might be a sensible option considering my current circumstances.</td>
</tr>
</tbody>
</table>
<p>Figure 10: Example of Response Generation #2. While baselines yield short personas, CAFFEINE offers informative personas (color red) and leads to a response that reflects Person B's situation and what B is looking for in a car.</p>
<p>We are surveying qualities for responses from dialogue.
Specifically, you'll be given a piece of dialogue context, and a response to follow the dialogue. You'll be asked to compare which response is better in terms of different aspects, and specify which aspect was most important for judging.</p>
<h1>Guidelines:</h1>
<ol>
<li>
<p>[Q1-5] First, choose which response is better regarding the given aspect.</p>
</li>
<li>
<p>Try to focus on quality over quantity. Contentful/high-quality response doesn't need to be lengthy.</p>
</li>
<li>
<p>[Q6] Second, choose which aspect influenced you the most when judging the overall quality.</p>
</li>
<li>
<p>If some factor other than the ones in Question 1-6 had the biggest influence, please select "Other" and specify.</p>
</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;">Dialogue Context <br> \$(context)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Response candidate 1 <br> \$(response_ours)</td>
<td style="text-align: center;">Response candidate 2 <br> \$(response_other)</td>
</tr>
<tr>
<td style="text-align: center;">Question 1. Which response is more natural (human-like)?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Question 2. Which response is more interesting?</p>
<p>Question 3. Which response is more consistent (well aligned) with the dialogue context?</p>
<p>Question 4. Which response is more specific(contains more information about speakers)?</p>
<p>Question 5. Which response do you like more overall?</p>
<p>Question 6. Which aspect affected you the most when judging the overall quality?
$\square$ Naturalness $\square$ Interesting $\square$ Consistency $\square$ Specificity $\square$ Other:
Optional feedback? (expand/collapse)
Submit
Figure 11: Interface for human evaluation on response quality.</p>
<p>We are surveying qualities of personas from dialogues.
Specifically, you'll be given Persona 1 and 2 that are potentially contradictory, its dialogue context, and its refined persona. Each persona is expanded from Source persona. Refined persona is either a single merged persona, or two individually specified persona of Persona 1 and 2. You'll be asked to compare which persona(s) is better in terms of different aspects, and assess rationale that was used for refinement.</p>
<h1>Guidelines:</h1>
<p>There are four choices: Definitely /Disagree/Agree and Slightly /Disagree/Agree
Please trust your instincts and choose Definitely if you would feel more confident giving one response, versus the other one.</p>
<ol>
<li>[Q1] First, answer if Persona 1 and 2 contradict each other.</li>
<li>[Q2-6] Second, assess refined persona in terms of different aspects.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;">Persona 1 (Before refinement)</th>
<th style="text-align: left;">Persona 2 (Before refinement)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\${$ persona_1)</td>
<td style="text-align: left;">$\${$persona_2)</td>
</tr>
</tbody>
</table>
<p>Question 1. Are Persona 1 and Persona 2 contradictory?</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Definitely Disagree</th>
<th style="text-align: left;">Slightly Disagree</th>
<th style="text-align: left;">Slightly Agree</th>
<th style="text-align: left;">Definitely Agree</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Refined Persona
$\$($ refined $)$
Question 2.
(If there is one refined persona) Is refined persona reasonable?
(If there are two refined personas) Are refined personas less contradictory?</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Definitely Disagree</th>
<th style="text-align: left;">Slightly Disagree</th>
<th style="text-align: left;">Slightly Agree</th>
<th style="text-align: left;">Definitely Agree</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Question 3. Is refined persona more specific to decribe a person?</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Definitely Disagree</th>
<th style="text-align: center;">Slightly Disagree</th>
<th style="text-align: center;">Slightly Agree</th>
<th style="text-align: center;">Definitely Agree</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dialog Context of Persona 1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dialog Context of Persona 2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\${$ context_1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\${$ (context_2)</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Question 4. Is refined persona more useful when having a dialogue with this person?</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Definitely Disagree</th>
<th style="text-align: center;">Slightly Disagree</th>
<th style="text-align: center;">Slightly Agree</th>
<th style="text-align: center;">Definitely Agree</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Rationale for refinement</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\$($ rationale)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Question 5. Do you think this rationale is appropriate for refining contradiction in Persona 1 and 2?</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Definitely Disagree</th>
<th style="text-align: center;">Slightly Disagree</th>
<th style="text-align: center;">Slightly Agree</th>
<th style="text-align: center;">Definitely Agree</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Question 6. Is Refined persona more preferrable overall than personas before refinement?</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Definitely Disagree</th>
<th style="text-align: center;">Slightly Disagree</th>
<th style="text-align: center;">Slightly Agree</th>
<th style="text-align: center;">Definitely Agree</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Optional feedback? (expand/collapse)</p>
<h2>Submit</h2>
<p>Figure 12: Interface for human evaluation on refinement quality.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Memory</th>
<th style="text-align: center;">Session 2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Session 3</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Session 4</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Session 5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">B-1</td>
<td style="text-align: center;">R-1</td>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">B-1</td>
<td style="text-align: center;">R-1</td>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">B-1</td>
<td style="text-align: center;">R-1</td>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">B-1</td>
<td style="text-align: center;">R-1</td>
<td style="text-align: center;">R-L</td>
</tr>
<tr>
<td style="text-align: center;">None</td>
<td style="text-align: center;">20.75</td>
<td style="text-align: center;">19.38</td>
<td style="text-align: center;">15.16</td>
<td style="text-align: center;">20.42</td>
<td style="text-align: center;">19.53</td>
<td style="text-align: center;">15.09</td>
<td style="text-align: center;">19.88</td>
<td style="text-align: center;">19.56</td>
<td style="text-align: center;">14.98</td>
<td style="text-align: center;">19.87</td>
<td style="text-align: center;">20.16</td>
<td style="text-align: center;">15.33</td>
</tr>
<tr>
<td style="text-align: center;">$k=12$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GOLD</td>
<td style="text-align: center;">21.18</td>
<td style="text-align: center;">19.78</td>
<td style="text-align: center;">15.46</td>
<td style="text-align: center;">21.26</td>
<td style="text-align: center;">20.11</td>
<td style="text-align: center;">15.43</td>
<td style="text-align: center;">20.58</td>
<td style="text-align: center;">19.97</td>
<td style="text-align: center;">15.17</td>
<td style="text-align: center;">20.38</td>
<td style="text-align: center;">20.40</td>
<td style="text-align: center;">15.42</td>
</tr>
<tr>
<td style="text-align: center;">+ NLI-remove</td>
<td style="text-align: center;">20.74</td>
<td style="text-align: center;">19.83</td>
<td style="text-align: center;">15.19</td>
<td style="text-align: center;">21.05</td>
<td style="text-align: center;">20.27</td>
<td style="text-align: center;">15.51</td>
<td style="text-align: center;">21.09</td>
<td style="text-align: center;">20.35</td>
<td style="text-align: center;">15.62</td>
<td style="text-align: center;">21.22</td>
<td style="text-align: center;">20.56</td>
<td style="text-align: center;">15.78</td>
</tr>
<tr>
<td style="text-align: center;">+ Caffeine</td>
<td style="text-align: center;">20.91</td>
<td style="text-align: center;">20.03</td>
<td style="text-align: center;">15.33</td>
<td style="text-align: center;">21.20</td>
<td style="text-align: center;">20.52</td>
<td style="text-align: center;">15.74</td>
<td style="text-align: center;">21.46</td>
<td style="text-align: center;">20.77</td>
<td style="text-align: center;">15.94</td>
<td style="text-align: center;">21.62</td>
<td style="text-align: center;">20.97</td>
<td style="text-align: center;">16.11</td>
</tr>
<tr>
<td style="text-align: center;">COMET-EXP</td>
<td style="text-align: center;">21.04</td>
<td style="text-align: center;">19.63</td>
<td style="text-align: center;">15.32</td>
<td style="text-align: center;">20.89</td>
<td style="text-align: center;">19.88</td>
<td style="text-align: center;">15.27</td>
<td style="text-align: center;">20.20</td>
<td style="text-align: center;">19.84</td>
<td style="text-align: center;">15.14</td>
<td style="text-align: center;">20.12</td>
<td style="text-align: center;">20.43</td>
<td style="text-align: center;">15.50</td>
</tr>
<tr>
<td style="text-align: center;">+ NLI-remove</td>
<td style="text-align: center;">20.68</td>
<td style="text-align: center;">19.89</td>
<td style="text-align: center;">15.19</td>
<td style="text-align: center;">21.04</td>
<td style="text-align: center;">20.21</td>
<td style="text-align: center;">15.42</td>
<td style="text-align: center;">21.49</td>
<td style="text-align: center;">20.70</td>
<td style="text-align: center;">15.81</td>
<td style="text-align: center;">21.57</td>
<td style="text-align: center;">20.73</td>
<td style="text-align: center;">15.88</td>
</tr>
<tr>
<td style="text-align: center;">+ Caffeine</td>
<td style="text-align: center;">20.99</td>
<td style="text-align: center;">20.05</td>
<td style="text-align: center;">15.32</td>
<td style="text-align: center;">21.41</td>
<td style="text-align: center;">20.55</td>
<td style="text-align: center;">15.71</td>
<td style="text-align: center;">21.66</td>
<td style="text-align: center;">20.83</td>
<td style="text-align: center;">15.93</td>
<td style="text-align: center;">21.86</td>
<td style="text-align: center;">20.96</td>
<td style="text-align: center;">16.07</td>
</tr>
<tr>
<td style="text-align: center;">$k=20$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GOLD</td>
<td style="text-align: center;">21.19</td>
<td style="text-align: center;">19.86</td>
<td style="text-align: center;">15.50</td>
<td style="text-align: center;">21.24</td>
<td style="text-align: center;">20.16</td>
<td style="text-align: center;">15.47</td>
<td style="text-align: center;">20.57</td>
<td style="text-align: center;">19.94</td>
<td style="text-align: center;">15.16</td>
<td style="text-align: center;">20.49</td>
<td style="text-align: center;">20.53</td>
<td style="text-align: center;">15.55</td>
</tr>
<tr>
<td style="text-align: center;">+ NLI-remove</td>
<td style="text-align: center;">20.81</td>
<td style="text-align: center;">19.98</td>
<td style="text-align: center;">15.26</td>
<td style="text-align: center;">21.04</td>
<td style="text-align: center;">20.28</td>
<td style="text-align: center;">15.52</td>
<td style="text-align: center;">21.33</td>
<td style="text-align: center;">20.69</td>
<td style="text-align: center;">15.91</td>
<td style="text-align: center;">21.43</td>
<td style="text-align: center;">20.75</td>
<td style="text-align: center;">15.95</td>
</tr>
<tr>
<td style="text-align: center;">+ Caffeine</td>
<td style="text-align: center;">20.93</td>
<td style="text-align: center;">20.18</td>
<td style="text-align: center;">15.47</td>
<td style="text-align: center;">21.41</td>
<td style="text-align: center;">20.72</td>
<td style="text-align: center;">15.86</td>
<td style="text-align: center;">21.67</td>
<td style="text-align: center;">21.00</td>
<td style="text-align: center;">16.15</td>
<td style="text-align: center;">21.92</td>
<td style="text-align: center;">21.23</td>
<td style="text-align: center;">16.31</td>
</tr>
<tr>
<td style="text-align: center;">COMET-EXP</td>
<td style="text-align: center;">21.23</td>
<td style="text-align: center;">19.82</td>
<td style="text-align: center;">15.44</td>
<td style="text-align: center;">20.95</td>
<td style="text-align: center;">19.90</td>
<td style="text-align: center;">15.38</td>
<td style="text-align: center;">20.33</td>
<td style="text-align: center;">20.02</td>
<td style="text-align: center;">15.18</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">20.27</td>
<td style="text-align: center;">15.37</td>
</tr>
<tr>
<td style="text-align: center;">+ NLI-remove</td>
<td style="text-align: center;">20.72</td>
<td style="text-align: center;">19.96</td>
<td style="text-align: center;">15.27</td>
<td style="text-align: center;">21.12</td>
<td style="text-align: center;">20.40</td>
<td style="text-align: center;">15.56</td>
<td style="text-align: center;">21.66</td>
<td style="text-align: center;">20.77</td>
<td style="text-align: center;">15.88</td>
<td style="text-align: center;">21.77</td>
<td style="text-align: center;">20.91</td>
<td style="text-align: center;">16.01</td>
</tr>
<tr>
<td style="text-align: center;">+ Caffeine</td>
<td style="text-align: center;">20.97</td>
<td style="text-align: center;">20.06</td>
<td style="text-align: center;">15.32</td>
<td style="text-align: center;">21.63</td>
<td style="text-align: center;">20.73</td>
<td style="text-align: center;">15.86</td>
<td style="text-align: center;">21.97</td>
<td style="text-align: center;">21.10</td>
<td style="text-align: center;">16.18</td>
<td style="text-align: center;">22.26</td>
<td style="text-align: center;">21.32</td>
<td style="text-align: center;">16.37</td>
</tr>
<tr>
<td style="text-align: center;">$k=30$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GOLD</td>
<td style="text-align: center;">20.88</td>
<td style="text-align: center;">19.65</td>
<td style="text-align: center;">15.45</td>
<td style="text-align: center;">21.09</td>
<td style="text-align: center;">20.18</td>
<td style="text-align: center;">15.56</td>
<td style="text-align: center;">20.50</td>
<td style="text-align: center;">19.89</td>
<td style="text-align: center;">15.09</td>
<td style="text-align: center;">20.41</td>
<td style="text-align: center;">20.47</td>
<td style="text-align: center;">15.46</td>
</tr>
<tr>
<td style="text-align: center;">+ NLI-remove</td>
<td style="text-align: center;">20.65</td>
<td style="text-align: center;">19.85</td>
<td style="text-align: center;">15.14</td>
<td style="text-align: center;">21.09</td>
<td style="text-align: center;">20.43</td>
<td style="text-align: center;">15.68</td>
<td style="text-align: center;">21.50</td>
<td style="text-align: center;">20.83</td>
<td style="text-align: center;">15.98</td>
<td style="text-align: center;">21.59</td>
<td style="text-align: center;">20.93</td>
<td style="text-align: center;">16.07</td>
</tr>
<tr>
<td style="text-align: center;">+ Caffeine</td>
<td style="text-align: center;">20.89</td>
<td style="text-align: center;">20.13</td>
<td style="text-align: center;">15.43</td>
<td style="text-align: center;">21.42</td>
<td style="text-align: center;">20.77</td>
<td style="text-align: center;">15.96</td>
<td style="text-align: center;">21.73</td>
<td style="text-align: center;">21.07</td>
<td style="text-align: center;">16.19</td>
<td style="text-align: center;">22.01</td>
<td style="text-align: center;">21.29</td>
<td style="text-align: center;">16.32</td>
</tr>
<tr>
<td style="text-align: center;">COMET-EXP</td>
<td style="text-align: center;">21.40</td>
<td style="text-align: center;">19.89</td>
<td style="text-align: center;">15.52</td>
<td style="text-align: center;">21.06</td>
<td style="text-align: center;">20.10</td>
<td style="text-align: center;">15.40</td>
<td style="text-align: center;">20.38</td>
<td style="text-align: center;">20.03</td>
<td style="text-align: center;">15.27</td>
<td style="text-align: center;">20.06</td>
<td style="text-align: center;">20.50</td>
<td style="text-align: center;">15.59</td>
</tr>
<tr>
<td style="text-align: center;">+ NLI-remove</td>
<td style="text-align: center;">20.60</td>
<td style="text-align: center;">19.86</td>
<td style="text-align: center;">15.20</td>
<td style="text-align: center;">21.02</td>
<td style="text-align: center;">20.33</td>
<td style="text-align: center;">15.57</td>
<td style="text-align: center;">21.35</td>
<td style="text-align: center;">20.62</td>
<td style="text-align: center;">15.83</td>
<td style="text-align: center;">21.71</td>
<td style="text-align: center;">20.97</td>
<td style="text-align: center;">16.05</td>
</tr>
<tr>
<td style="text-align: center;">+ Caffeine</td>
<td style="text-align: center;">20.96</td>
<td style="text-align: center;">20.11</td>
<td style="text-align: center;">15.37</td>
<td style="text-align: center;">21.73</td>
<td style="text-align: center;">20.85</td>
<td style="text-align: center;">15.99</td>
<td style="text-align: center;">22.17</td>
<td style="text-align: center;">21.24</td>
<td style="text-align: center;">16.27</td>
<td style="text-align: center;">22.57</td>
<td style="text-align: center;">21.58</td>
<td style="text-align: center;">16.54</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance in response generation. Bold and underline show the best and second-highest in each column.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Settings</th>
<th style="text-align: center;">Session 2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Session 3</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Session 4</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Session 5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">B-1</td>
<td style="text-align: center;">R-1</td>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">B-1</td>
<td style="text-align: center;">R-1</td>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">B-1</td>
<td style="text-align: center;">R-1</td>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">B-1</td>
<td style="text-align: center;">R-1</td>
<td style="text-align: center;">R-L</td>
</tr>
<tr>
<td style="text-align: center;">No Memory</td>
<td style="text-align: center;">20.75</td>
<td style="text-align: center;">19.38</td>
<td style="text-align: center;">15.16</td>
<td style="text-align: center;">20.42</td>
<td style="text-align: center;">19.53</td>
<td style="text-align: center;">15.09</td>
<td style="text-align: center;">19.88</td>
<td style="text-align: center;">19.56</td>
<td style="text-align: center;">14.98</td>
<td style="text-align: center;">19.87</td>
<td style="text-align: center;">20.16</td>
<td style="text-align: center;">15.33</td>
</tr>
<tr>
<td style="text-align: center;">GOLD</td>
<td style="text-align: center;">21.19</td>
<td style="text-align: center;">19.86</td>
<td style="text-align: center;">15.50</td>
<td style="text-align: center;">21.24</td>
<td style="text-align: center;">20.16</td>
<td style="text-align: center;">15.47</td>
<td style="text-align: center;">20.57</td>
<td style="text-align: center;">19.94</td>
<td style="text-align: center;">15.16</td>
<td style="text-align: center;">20.49</td>
<td style="text-align: center;">20.53</td>
<td style="text-align: center;">15.55</td>
</tr>
<tr>
<td style="text-align: center;">+ DNLI-remove</td>
<td style="text-align: center;">20.87</td>
<td style="text-align: center;">20.07</td>
<td style="text-align: center;">15.31</td>
<td style="text-align: center;">21.15</td>
<td style="text-align: center;">20.50</td>
<td style="text-align: center;">15.70</td>
<td style="text-align: center;">21.37</td>
<td style="text-align: center;">20.82</td>
<td style="text-align: center;">15.97</td>
<td style="text-align: center;">21.52</td>
<td style="text-align: center;">20.95</td>
<td style="text-align: center;">16.08</td>
</tr>
<tr>
<td style="text-align: center;">+ DNLI-recent</td>
<td style="text-align: center;">20.92</td>
<td style="text-align: center;">20.09</td>
<td style="text-align: center;">15.36</td>
<td style="text-align: center;">21.16</td>
<td style="text-align: center;">20.58</td>
<td style="text-align: center;">15.80</td>
<td style="text-align: center;">21.36</td>
<td style="text-align: center;">20.83</td>
<td style="text-align: center;">16.01</td>
<td style="text-align: center;">21.60</td>
<td style="text-align: center;">21.08</td>
<td style="text-align: center;">16.20</td>
</tr>
<tr>
<td style="text-align: center;">+ Caffeine</td>
<td style="text-align: center;">20.94</td>
<td style="text-align: center;">20.15</td>
<td style="text-align: center;">15.41</td>
<td style="text-align: center;">21.33</td>
<td style="text-align: center;">20.69</td>
<td style="text-align: center;">15.89</td>
<td style="text-align: center;">21.54</td>
<td style="text-align: center;">21.01</td>
<td style="text-align: center;">16.17</td>
<td style="text-align: center;">21.75</td>
<td style="text-align: center;">21.18</td>
<td style="text-align: center;">16.30</td>
</tr>
<tr>
<td style="text-align: center;">COMET-EXP</td>
<td style="text-align: center;">21.23</td>
<td style="text-align: center;">19.82</td>
<td style="text-align: center;">15.44</td>
<td style="text-align: center;">20.95</td>
<td style="text-align: center;">19.90</td>
<td style="text-align: center;">15.38</td>
<td style="text-align: center;">20.33</td>
<td style="text-align: center;">20.02</td>
<td style="text-align: center;">15.18</td>
<td style="text-align: center;">20.00</td>
<td style="text-align: center;">20.27</td>
<td style="text-align: center;">15.37</td>
</tr>
<tr>
<td style="text-align: center;">+ DNLI-remove</td>
<td style="text-align: center;">20.81</td>
<td style="text-align: center;">20.01</td>
<td style="text-align: center;">15.26</td>
<td style="text-align: center;">21.13</td>
<td style="text-align: center;">20.46</td>
<td style="text-align: center;">15.73</td>
<td style="text-align: center;">21.53</td>
<td style="text-align: center;">20.96</td>
<td style="text-align: center;">16.12</td>
<td style="text-align: center;">21.66</td>
<td style="text-align: center;">21.06</td>
<td style="text-align: center;">16.18</td>
</tr>
<tr>
<td style="text-align: center;">+ DNLI-recent</td>
<td style="text-align: center;">20.92</td>
<td style="text-align: center;">20.10</td>
<td style="text-align: center;">15.41</td>
<td style="text-align: center;">21.35</td>
<td style="text-align: center;">20.69</td>
<td style="text-align: center;">15.91</td>
<td style="text-align: center;">21.51</td>
<td style="text-align: center;">20.93</td>
<td style="text-align: center;">16.11</td>
<td style="text-align: center;">21.72</td>
<td style="text-align: center;">21.20</td>
<td style="text-align: center;">16.34</td>
</tr>
<tr>
<td style="text-align: center;">+ Caffeine</td>
<td style="text-align: center;">20.89</td>
<td style="text-align: center;">20.10</td>
<td style="text-align: center;">15.40</td>
<td style="text-align: center;">21.37</td>
<td style="text-align: center;">20.62</td>
<td style="text-align: center;">15.81</td>
<td style="text-align: center;">21.82</td>
<td style="text-align: center;">21.06</td>
<td style="text-align: center;">16.19</td>
<td style="text-align: center;">22.07</td>
<td style="text-align: center;">21.21</td>
<td style="text-align: center;">16.30</td>
</tr>
</tbody>
</table>
<p>Table 5: Performance in response generation with DNLI model. Bold and underline show the best and secondhighest in each column.</p>
<h1>Prompt</h1>
<p>You will be provided with two contradictory persona sentences, along with their source personas and the dialogue fragments from which these persona sentences were derived.
Your task is to resolve the contradiction between the two persona sentences based on the dialogue fragments and the source persona of each contradictory persona. You can use these two strategies:
[Resolution]: If the two personas are based on the same event but change over time (possibly due to a temporal difference or other events in between), adjust and aggregate them into one new persona sentence.
[Disambiguation]: If the contradiction between them isn't due to time changes or they are derived from unrelated events in the first place, utilize dialogue fragments to rewrite (clarify/specify) each persona.
First, provide a rationale for your choice ([Resolution] or [Disambiguation]). Then, based on this rationale, generate refined persona sentence(s).
If the two personas are not contradictory, generate [NO_CONFLICT].
Example 1:
Persona 1: I am a programmer.
Dialogue fragment of Persona 1:
A: As a computer programmer, I spend a lot of time writing and debugging code. It's rewarding to see my work contribute to the development of functional and efficient software.
Source Persona: I am a programmer.
Persona 2: I get fire.
Dialogue fragment of Persona 2:
A: I got fired. It was quite unexpected, and I'm still processing everything.
B: I'm really sorry to hear that. Do you want to talk about what happened?
Source Persona: I don't have a job right now.
Rationale: There is a temporal connection between the two personas. Persona 1 is about being a programmer, whereas Persona 2 is about having been fired. Both personas can exist over time with Persona 2 occurring after Persona 1.
[Resolution]: I am a programmer who has recently been fired.</p>
<h2>Example 2:</h2>
<p>Persona 1: I feel happy.
Dialogue fragment of Persona 1:
A: My favorite baseball team won this season! I'm so excited!
B: Wow, really? That's great news!
Source Persona: My team won the league.
Persona 2: I have been feeling quite stressed.
Dialogue fragment of Persona 2:
A: Work has been overwhelming with tight deadlines and high expectations. It's been hard to find time for relaxation and self-care.
B: With stress piling up, it's important to find ways to unwind and destress. Maybe I'll try meditation or yoga to help manage my anxiety.
Source Persona: I work very hard.
Rationale: The two personas do not reflect changes over time but rather different emotional states in response to separate circumstances; one, a moment of happiness due to a favorite team winning, and the other, underlying stress caused by work pressures.
[Disambiguation]:</p>
<ul>
<li>Persona 1: I feel happy when my favorite baseball team wins.</li>
<li>Persona 2: I am a person dealing with work-related stress and looking for ways to manage anxiety.</li>
</ul>
<p>Example 3:
Persona 1: I am a vegetarian.
Dialogue fragment of Persona 1:
A: I've been a vegetarian for about 5 years now. It started for health reasons, but now it's also about sustainability and animal welfare.
Source Persona: I am a vegetarian.
Persona 2: I enjoy reading fiction books.
Dialogue fragments of Persona 2:
A: One of my favorite pastimes is reading fiction. I love getting lost in different worlds and lives through the pages of a good novel.
Source Persona: I enjoy reading fiction books.
Rationale: The two persona sentences do not contradict each other as they pertain to different aspects of the speaker's identity. One persona is about dietary preference (being a vegetarian), and the other is about a hobby or interest (enjoying reading fiction books). There is no inherent conflict between being a vegetarian and enjoying reading fiction, so the two persona sentences can coexist without the need for resolution or disambiguation. [NO_CONFLICT]</p>
<p>Example 4: ...
Table 6: The prompt for CAFFEINE (Five-shot setting, Examples 4 and 5 are omitted in this table). The "preservation" strategy is represented as [NO_CONFLICT] in our prompt.</p>
<h1>Prompt</h1>
<p>You will be generating the next turn of a given dialogue context between Speaker A and Speaker B. Alongside the dialogue context, you'll be given persona statements about both speakers. Your response should be 1-2 sentences, utilizing the persona statements as guidance to create an appropriate reply. Generate appropriate answers using given persona statements as memory.
What is the most appropriate next utterance ( 3 sentences max)?
Persona Statements of A: {A's personas within the top- $k$ retrieved personas from long-term memory }
Persona Statements of B: {B's personas within the top- $k$ retrieved personas from long-term memory }
Dialogue: {dialogue context}
Response: ${$ generation result $}$
Table 7: The prompt for response generation (zero-shot).</p>
<h1>Example 1 - [Resolution]</h1>
<h2>Before refinement:</h2>
<p>B's Persona 1: I feel happy.
B's Persona 2: I feel sad.
Contextual Backgrounds (relevant dialogue contexts):
In this example, both personas are derived from the same dialogue context.
A: It's really bad from the first of the year until tax deadline day but it keeps me pretty busy the rest of the year as well. I will need to think it over. It does get lonely sometimes.
B: I can only imagine. That was another thing about being a vet that was hard. People expected you to be on call all the time, so I didn't have a chance for any sort of social life. That's gotten a lot better since I quit. I've found a good group of friends to hang out with at least a few times a week at a local cafe.</p>
<h2>CAFFEINE's output:</h2>
<p>Explanation: The two persona sentences reflect contradictory emotional states. Persona 1 expresses happiness due to having a good group of friends to hang out with, whereas Persona 2 expresses sadness and loneliness from lacking a social life. Both personas are based on the same events and experiences, indicating a change in emotional state over time.
After refinement: I used to feel sad and lonely when I was a vet, but now I feel happy because I have a good group of friends to hang out with at a cafe every week.</p>
<h2>Example 2 - [Disambiguation]</h2>
<h2>Before refinement:</h2>
<p>B's Persona 1: I want to play with the dogs.
B's Persona 2: I want to eat them.
Contextual Backgrounds (relevant dialogue contexts):
A: Yeah definitely, that's great to hear you're enjoying your new vegan diet. Oh, cute! I love labs. Yes, my puppies keep me busy as well. The training phase is definitely a lot of work. How old are your labs now?
B: Almost 4. Still act like puppies running around like crazy. I think I wouldn't get any exercise if it weren't for them though, so that is a good thing for me.</p>
<p>A: That sounds delicious!!! I agree, a recipe exchange would be great. True, vegan cheese does help with that. Have you found any favorite brands for vegan cheese? I found a couple that I like, and some that I didn't enjoy as much.
B: I have tried a couple, none that I've enjoyed so far. the next on my list to try is cashew cheese, which I have heard is superior, so I am excited to try it. I hear it makes great nachos, and I miss nachos so much!</p>
<h2>CAFFEINE's output:</h2>
<p>Explanation: The two personas do not reflect changes over time but rather different topics of discussion. Persona 1 is about the speaker's dogs and their age, while Persona 1 is about discussing vegan cheese and favorite brands.
After refinement:
B's Persona 1: My 4-year-old labs (labrador) keep me active and it's a good thing for me.
B's Persona 2: I enjoy trying different brands of vegan cheese, although I haven't found a favorite yet.</p>
<p>Table 8: More examples of persona refinement done by CAFFEINE. Color magenta highlights the contextual cues from the relevant dialogue contexts.</p>
<h1>Example 1 - [Resolution]</h1>
<h2>Before refinement:</h2>
<p>A's Persona 1: I feel excited.
A's Persona 2: I get tired.
Contextual Backgrounds (relevant dialogue contexts):
In this example, both personas are derived from the same dialogue context.
B: I am sure you feel so small and at peace up there. When will your mission make its trip? I imagine training for space is long and strenuous.
A: It absolutely is! The trip is scheduled for around this time next year. I'm going to have to get on a pretty strict diet and exercise routine to make sure that I am fit enough for the trip.</p>
<h2>CAFFEINE's output:</h2>
<p>Explanation: The two persona sentences contradict each other in terms of the emotional state of the speaker. One persona expresses excitement, while the other expresses tiredness. However, both personas are based on the same event or circumstance of training for a space mission. Thus, there is a temporal aspect to this contradiction.
After refinement: I am currently feeling tired due to the strenuous training for my upcoming space mission, but I am also excited about the opportunity.</p>
<h2>Example 2 - [Disambiguation]</h2>
<h2>Before refinement:</h2>
<p>A's Persona 1: I feel relaxed.
A's Persona 2: I feel tired.
Contextual Backgrounds (relevant dialogue contexts):
B: That sounds like so much fun. I have never been fishing in my life. Is it difficult to learn?
A: There's a bit of a knack to the rod, but once you get that down, it's pretty simple. It's a lot of just sitting around and waiting. It's very peaceful and relaxing.
-     -         - 
B: We are always on the lookout for a new restaurant. Have you tried a great place recently?
A: I have been so busy with work, that I haven't! I was hoping you might have a recommendation. We definitely need to go somewhere with great crab legs, or else a nice sushi joint. Somewhere with delicious cocktails as well!</p>
<h2>CAFFEINE's output:</h2>
<p>Explanation: The two persona sentences do not reflect changes over time but rather different emotional states and interests. Persona 1 expresses feeling happy, while Persona 2 expresses being bored. The contradiction lies in the difference in emotional experiences and preferences.
After refinement:
A's Persona 1: I feel relaxed when I go fishing.
A's Persona 2: I feel tired because I spend a lot of time at work.
Table 9: More examples of persona refinement done by CAFFEINE. Color magenta highlights the contextual cues from the relevant dialogue contexts.</p>
<h1>Example 1 - [Preservation]</h1>
<h2>Before refinement:</h2>
<p>B's Persona 1: I love punk music.
B's Persona 2: I enjoy romantic comedies and would like to watch some cop shows.
Contextual Backgrounds (relevant dialogue contexts):
A: I am great. Residing in Arizona, skydiving is awesome, Barnes \&amp; Noble is my favorite, you?
B: I like movies over books, love punk music and just love to cruise around!</p>
<hr />
<p>A: I read the Tori Spelling novel you told me about!
B: Awesome! What did you think of it?
A: I thought it was really good. Thanks for recommending it. You also said you watch a lot of movies. Can you recommend a good movie to me too?
B: Well, I've been watching a lot of romantic comedies lately. We should watch a couple the next time we have a movie night! I also wanted to ask, are there any cop shows you'd recommend? I'd like to try watching them too.</p>
<h2>CAFFEINE's output:</h2>
<p>Explanation: The two persona sentences do not contradict each other as they pertain to different areas of interest. Persona 1 expresses a love for punk music, while Persona 2 expresses a liking for romantic comedies and an interest in watching cop shows. There is no inherent conflict between loving punk music and enjoying romantic comedies or cop shows, so the two persona sentences can coexist without the need for resolution or disambiguation.</p>
<h2>Example 2 - [Preservation]</h2>
<h2>Before refinement:</h2>
<p>B's Persona 1: I got a new car.
B's Persona 2: I like to drink water.
Contextual Backgrounds (relevant dialogue contexts):
A: How was the drive up to San Diego? Did you bring your kids or are you flying solo? I hear the beaches are beautiful there.
B: The drive was long, but the scenery was gorgeous! Took a lot longer to make the trip up than I expected because with the kids I had to make more stops to stretch our legs, feed them, and give them a little time to burn off some energy. I think I always took for granted the luxury of traveling sans kids in the past, but honestly I don't mind the company! It was the first time traveling with my kids in my new car. How's it going for you?</p>
<ul>
<li>
<ul>
<li>
<ul>
<li>A: I have not been to San Diego, but I have always wanted to go. You will have to tell me all about it.
B: I will. I will be on the lookout for the best iced tea! Haha. A: You know it is one of my favorites to drink. Besides tea I only really drink water
B: I love water, too. I wish I could get my kids to drink more water.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>CAFFEINE's output:</h2>
<p>Explanation: The two personas do not contradict each other as they pertain to different aspects of the speaker's life. Persona 1 is about getting a new car and taking a trip to San Diego, while Persona 2 is about the speaker's preference for drinking water. The two persona sentences can coexist without the need for resolution or disambiguation.</p>
<p>Table 10: Examples of "preservation" addressing the sub-optimal performance of NLI models that solely rely on the persona sentences without contextual backgrounds. Color magenta highlights the contextual cues.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://openai.com/blog/chatgpt
${ }^{8}$ https://github.com/hwchase17/langchain&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ Prompts for RG and CAFFEINE are in Appendix A.3.
${ }^{4}$ We set $k=20$. Results with other $k$ are in Appendix B.
${ }^{5}$ We report the statistics of contradiction in Appendix D. As our focus is persona expansion, extracting personas from conversations is out of the scope of this work.
${ }^{6}$ We test with MNLI and DNLI (Welleck et al., 2019) and report results using MNLI as it shows better performance. Results with DNLI are in Appendix D.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-434 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-434</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-434</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-272738091</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.13153v2.pdf" target="_blank">Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture</a></p>
                <p><strong>Paper Abstract:</strong> The remarkable advancements in artificial intelligence (AI), primarily driven by deep neural networks, are facing challenges surrounding unsustainable computational trajectories, limited robustness, and a lack of explainability. To develop next-generation cognitive AI systems, neuro-symbolic AI emerges as a promising paradigm, fusing neural and symbolic approaches to enhance interpretability, robustness, and trustworthiness, while facilitating learning from much less data. Recent neuro-symbolic systems have demonstrated great potential in collaborative human-AI scenarios with reasoning and cognitive capabilities. In this paper, we aim to understand the workload characteristics and potential architectures for neuro-symbolic AI. We first systematically categorize neuro-symbolic AI algorithms, and then experimentally evaluate and analyze them in terms of runtime, memory, computational operators, sparsity, and system characteristics on CPUs, GPUs, and edge SoCs. Our studies reveal that neuro-symbolic models suffer from inefficiencies on off-the-shelf hardware, due to the memory-bound nature of vector-symbolic and logical operations, complex flow control, data dependencies, sparsity variations, and limited scalability. Based on profiling insights, we suggest cross-layer optimization solutions and present a hardware acceleration case study for vector-symbolic architecture to improve the performance, efficiency, and scalability of neuro-symbolic computing. Finally, we discuss the challenges and potential future directions of neuro-symbolic AI from both system and architectural perspectives.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e434.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e434.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NVSA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuro-Vector-Symbolic Architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid neuro-symbolic system that combines a neural perception frontend with vector-symbolic probabilistic reasoning (holographic distributed representations) to perform abductive and compositional reasoning for abstract visual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neuro-Vector-Symbolic Architecture (NVSA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NVSA uses a neural perception module (CNN / perception backbone) to extract object-level features from images and projects them into high-dimensional hypervector codebooks; a vector-symbolic symbolic reasoning backend (binding, bundling, permutation, similarity/cleanup memory, probabilistic abduction) performs rule detection, factorization, and probabilistic inference to produce final answers. The pipeline is modular: perception -> projection into VSA hypervectors -> resonator/cleanup/nearest-neighbor-based symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Vector-symbolic representations (hypervectors / holographic distributed representations) combined with probabilistic abductive reasoning; symbolic rules and codebooks (VSA codebooks), nearest-neighbor cleanup memory, and explicit reasoning operators (binding, bundling, permutation, similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural perception backbone (convolutional networks / MatMul and Conv dominated) for semantic parsing and feature extraction; standard NN forward inference used to produce inputs for symbolic stage.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Pipeline (Neuro|Symbolic): neural frontend produces features projected into VSA hypervectors (codebook lookup/encoding) that become inputs to symbolic VSA reasoning. The symbolic stage depends on neural outputs and lies on the critical path; NVSA uses probabilistic abduction operating on VSA vectors (resonator network + cleanup memory + argmax similarity). Integration is modular, not end-to-end differentiable in the described implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>High accuracy and systematic abstract reasoning (e.g., solving Raven-style RPM problems) exceeding pure neural baselines and human baselines cited; ability to bind and represent structured compositional relations, robust factorization of composite perceptual vectors, and explicit interpretable symbolic hypotheses (abductions) about scene structure that enable better data-efficiency and explainability than perception alone.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Raven's Progressive Matrices (RPM) / spatial-temporal reasoning / abstract visual reasoning datasets (RAVEN, I-RAVEN, PGM).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Reported 98.8% accuracy on spatial-temporal reasoning tasks (NVSA) in the paper's overview; runtime on RTX 2080Ti for RPM task: 380 s; runtime on Jetson TX2: 7507 s. Symbolic stage accounts for ~92.1% of NVSA inference time while contributing ~19% of FLOPS (inefficient on GPU).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>ResNet baseline cited in paper: 53.4% (on comparable reasoning benchmark mentioned in overview); GPT-4 cited at 89.0% (comparison context in overview).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>NVSA exhibits superior compositional/abstract reasoning and improved data efficiency compared to pure neural models; paper claims NVSA outperforms neural-only models and humans on the specific RPM/spatial-temporal tasks, implying better systematic generalization for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: symbolic VSA stage yields explicit symbolic hypotheses and factorization results (abductions) and uses explicit codebooks and nearest-neighbor clean-up that provide interpretable reasoning traces compared to black-box NN outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Symbolic VSA stage is memory-bound and sequential; extremely high latency on current CPUs/GPUs (hundreds to thousands of seconds for RPM tasks), low ALU and cache utilization on GPUs, scalability issues as task size grows (quadratic runtime increase observed), and heavy memory footprint for codebooks (>90% of NVSA model storage in some cases).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Complementary-strengths framework: neural perception for feature extraction and VSA/probabilistic symbolic stage for structured, interpretable reasoning; resonator networks and VSA algebra (binding/bundling/permutation + similarity/cleanup) form the mathematical backbone for factorization and abduction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e434.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e434.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logical Neural Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic framework that maps neurons to logical formula elements and parameterizes logical connectives (e.g., ∧, ∨) to combine neural learning with symbolic logic for direct interpretability and theorem-proving style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logical Neural Network (LNN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LNN encodes facts and symbolic rules into neural-like modules where neurons correspond to logical formula components; logical connectives are represented by parameterized differentiable functions constrained to preserve logical semantics (e.g., via Łukasiewicz or fuzzy logic), enabling the network to perform logical deduction while remaining amenable to learning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>First-order/fuzzy logic rules and symbolic facts represented explicitly; logical connectives modeled (Łukasiewicz logic) and used as constraints; symbolic rule base and bidirectional reasoning/inference.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural parameterized modules (differentiable functions) that implement logical connectives and enable gradient-based learning; the network structure resembles neural networks but is keyed to logical structure.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Compilation of symbolic rules into parameterized neural modules (Neuro:Symbolic→Neuro style): symbolic knowledge is encoded into neural structure and constraints (end-to-end neural system governed by logic-derived modules); training uses differentiable learning subject to logical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines interpretability of symbolic rules with learning-based tolerance to noisy/incomplete data; supports theorem-proving-like reasoning within a learnable neural framework and robustness to incomplete knowledge; modular/compositional deductions via structured neural wiring.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Logic program tasks (e.g., theorem proving / TPTP-style logical reasoning benchmarks and synthetic logic tasks cited in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Claims improved tolerance to incomplete knowledge and better general applicability for logical tasks relative to pure NNs; paper notes LNNs offer compositional and modular generalization advantages though specific numeric OOD metrics are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: neurons map to logical formula elements and outputs adhere to logic semantics, enabling traceable logical deductions and explanations derived from the symbolic rule encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Execution is data-movement- and control-flow-heavy (bidirectional inference causes data movement overhead); symbolic operations and sparse, irregular memory access lead to low hardware utilization on CPU/GPU; potential scalability limits for large rule bases and tree-structured deductions.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Fuzzy-first-order logic mapping into parameterized neural modules (Łukasiewicz logic); perspective that symbolic rules can be preserved as constraints within differentiable modules to retain logical semantics while enabling learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e434.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e434.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LTN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic Tensor Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic framework that grounds fuzzy first-order logic in neural tensor representations, transforming logical connectives and quantifiers into differentiable tensor operations for querying and reasoning with data and knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logic Tensor Network (LTN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LTN represents logical predicates and terms in vector/tensor spaces and interprets fuzzy first-order logic connectives and quantifiers with differentiable tensor operations; neural graph/MLP components ground symbols in data and compute truth degrees for formulas, enabling learning and reasoning with symbolic axioms.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Fuzzy first-order logic axioms and logical formulas (FOL) represented as constraints over embedded tensor representations; quantifiers approximated through aggregations.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural graphs / MLPs that ground logical variables and compute truth degrees (tensorized embeddings used as neural inputs), enabling gradient-based learning of the embedding and predicate parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Neuro:Symbolic→Neuro / Neuro Symbolic style: symbolic FOL constraints are converted into differentiable losses/constraints on neural tensor representations (soft constraints), and learning optimizes neural parameters subject to those logic-derived objectives (end-to-end differentiable coupling).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved interpretability via explicit logical axioms, increased data efficiency, and better out-of-distribution generalization in knowledge-completion-like tasks compared to purely neural systems; logical constraints act as regularizers guiding the learned embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Querying and reasoning tasks including knowledge graph completion and UCI-style datasets (as referenced); general logical querying/learning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>LTN's neural components dominated by MatMul/MLP; no single-number baseline provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Paper states LTNs improve out-of-distribution generalization and data efficiency by encoding knowledge as logical axioms over data; specific numeric OOD metrics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate-to-high: logic axioms and truth-degree outputs provide interpretable constraints and explanations for predictions, though inference is still governed by learned embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Symbolic constraints are softened into losses which may compromise strict interpretability; dense computation patterns (LTN is noted to be dense) and potential trade-offs between expressivity and scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Fuzzy first-order logic grounded in tensor embeddings; logical formulas map to differentiable aggregations and constraints that guide neural training (principle: encoding symbolic knowledge as soft differentiable constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e434.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e434.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Logic Machine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic architecture that approximates logic operations with neural modules wired to implement logical quantifiers and structured relational reasoning through a multi-layer architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural Logic Machine (NLM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NLM constructs multi-layer neural modules that approximate logic operators and quantifiers; it wires local neural function approximators to implement relational induction and logical deduction across object sets, producing higher-level abstractions across layers for relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Logic-like structure approximated (propositional and relational logic semantics expressed by wiring and module organization); explicit relational rules are approximated via learned modules rather than symbolic rule sets.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural network modules approximating logical operators; the network architecture encodes quantification via carefully constructed neural wiring and aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Neuro[Symbolic] style where neural modules implement symbolic-like operations: the symbolic logic is embedded/approximated inside the neural architecture (end-to-end neural system that implements symbolic processing internally).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Good relational induction and generalization from small to larger-scale tasks; preserves relational/compositional reasoning capability while staying within the neural training framework, allowing generalization to larger numbers of objects (size-generalization) not seen in purely connectionist baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Relational reasoning and decision-making tasks, e.g., family graph reasoning, sorting, path finding benchmarks and synthetic relational tasks referenced in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Claims strong generalization from small-scale training to larger-scale tasks due to the structured multi-layer design; improves relational generalization over standard neural baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate: the architecture is designed to emulate logic operations, which helps interpretability of the learned relational computations, but internal approximations mean exact symbolic explanations are implicit rather than explicit.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Symbolic-like computations implemented as neural approximations can be compute- and data-movement-intensive (vector/element-wise heavy), leading to low hardware utilization; irregular control and sparse syntax-tree structures make CPU/GPU acceleration challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Design principle: approximate logical quantifiers and operators with structured neural modules and wiring to achieve relational inductive biases and compositionality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e434.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e434.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VSAIT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vector Symbolic Architecture Image-to-Image Translation (VSAIT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An image-to-image translation system that leverages vector-symbolic architecture to map images into hypervector space, learn invertible hypervector mappings, and perform robust unpaired translation across domains with large distribution gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Vector Symbolic Architecture Image-to-Image Translation (VSAIT)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>VSAIT encodes images/features into random high-dimensional hypervectors using VSA operations (binding/bundling/permutation), learns invertible mappings in hypervector space to translate between domains, and decodes back to images, aiming to preserve semantics and avoid hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Vector-symbolic codebooks and algebraic symbolic operations (binding, bundling, permutation) used to represent semantic attributes and ensure consistency across translations.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks (perception and decoding networks / ConvNets) for encoding images to features and decoding translated hypervectors back to image space; neural modules learn mappings between hypervector representations.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Pipeline hybrid where neural encoders/decoders interface with a VSA latent symbolic space. The VSA latent operations act as structured symbolic transformations enforced between neural perception and generation stages (Neuro|Symbolic pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved robustness to semantic flipping and reduced image hallucination in unpaired domain translation; preservations of semantic consistency across large domain gaps because symbolic hypervector operations enforce structure in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Unpaired image-to-image translation benchmarks with large distribution gaps (e.g., GTA, Cityscapes, Google Maps dataset referenced in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Claims greater robustness in translation and better semantic consistency across domains compared to purely neural unpaired translation methods (reduced hallucination), though no single-number metrics provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate: VSA latent representations provide structured symbolic encodings which make certain transformations and semantic correspondences more explicit than purely neural latent spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>VSA symbolic computations are memory- and data-movement-intensive and are inefficient on GPUs/CPUs; large hypervector codebooks and high-dimensional operations create storage and intermediate caching demands.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Using VSA algebra (binding/bundling/permutation) as a latent symbolic calculus that composes and preserves semantic structure across neural encoder/decoder stages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e434.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e434.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZeroC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-Shot Concept Recognition and Acquisition (ZeroC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic model that uses graph structures and energy-based models to represent hierarchical concept models, enabling zero-shot recognition and acquisition of novel concepts during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ZeroC (Zero-shot Concept Recognition and Acquisition)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ZeroC represents constituent concept models as nodes in a symbolic graph with edges encoding relations, and uses energy-based/neural models to score hypotheses; symbolic graph structure guides inference and generalization so that new concepts can be recognized/acquired at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic concept graphs (hierarchical relational graphs) that encode concept relations and structure used to guide inference.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural energy-based models and neural classifiers that score/instantiate concept nodes and perform perception-level processing.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Neuro[Symbolic] style where neural models are integrated into a symbolic graph representation (neural components implement scoring within an explicit symbolic graph) enabling selective attention to pertinent symbolic information during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables zero-shot recognition and acquisition of novel concepts by leveraging symbolic graph relations combined with neural scoring; hierarchical concept transfer across domains at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Cross-domain classification and detection tasks; hierarchical-concept corpus and generalization benchmarks referenced in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Designed to generalize across domains for novel concept acquisition at inference time; outperforms neural-only baselines in zero-shot concept recognition tasks according to the paper summary.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: symbolic graph nodes and relations provide interpretable structure showing how concepts relate and how novel concepts are inferred via graph relations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Symbolic and ensemble computations can be memory-intensive; neural scoring integrated into symbolic graph may still suffer hardware inefficiencies; limited numeric results reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Combines energy-based neural scoring with explicit symbolic graph structure to allow inference-time concept composition and transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e434.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e434.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PrAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic Abduction and Execution (PrAE) Learner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic learner that combines neural visual perception with probabilistic abduction and execution of scene representations to perform spatial-temporal cognitive reasoning and infer hidden rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Probabilistic Abduction and Execution (PrAE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PrAE uses neural perception to extract scene primitives and attributes, then constructs probabilistic symbolic scene representations and performs abductive inference (generate latent rules/hypotheses) followed by symbolic execution to predict future events/attributes; final inference is probabilistic and interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Probabilistic symbolic scene representations and abductive reasoning machinery (probabilistic abduction, symbolic execution of inferred rules).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural perception modules (CNNs / feature extractors) to extract object attributes and scene information that seed the symbolic probabilistic reasoning pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Neuro|Symbolic pipeline where perception outputs are transformed into symbolic probabilistic representations used by an abductive symbolic reasoner; the symbolic phase depends on neural outputs and is on the end-to-end critical path.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved spatial-temporal reasoning and human-level generalizability with transparent, interpretable latent rules; better systematic generalization than pure neural models on spatial-temporal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Spatial-temporal reasoning tasks (CLEVRER-like / RAVEN-like datasets cited); cognitive reasoning benchmarks for event prediction and rule inference.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Paper claims human-level generalizability and transparency; symbolic abduction helps infer hidden causal rules enabling systematic generalization, though numeric generalization metrics are not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: explicit abductive hypotheses and executed symbolic scene representations provide interpretable reasoning traces and explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Symbolic stage dominates runtime (~80.5% of PrAE inference time), is memory- and data-movement-bounded, causing low hardware utilization and scalability challenges on standard CPUs/GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Probabilistic abduction + execution: the hybrid uses probabilistic symbolic representations to handle uncertainty, exploiting neural perception for grounding and symbolic probabilistic inference for structured reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural logic machines. <em>(Rating: 2)</em></li>
                <li>Logical neural networks. <em>(Rating: 2)</em></li>
                <li>Logic tensor networks. <em>(Rating: 2)</em></li>
                <li>Unpaired image translation via vector symbolic architectures. <em>(Rating: 2)</em></li>
                <li>Abstract spatial-temporal reasoning via probabilistic abduction and execution. <em>(Rating: 2)</em></li>
                <li>Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. <em>(Rating: 2)</em></li>
                <li>DeepProbLog: Neural probabilistic logic programming. <em>(Rating: 2)</em></li>
                <li>Neural probabilistic logic programming in deepproblog. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-434",
    "paper_id": "paper-272738091",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "NVSA",
            "name_full": "Neuro-Vector-Symbolic Architecture",
            "brief_description": "A hybrid neuro-symbolic system that combines a neural perception frontend with vector-symbolic probabilistic reasoning (holographic distributed representations) to perform abductive and compositional reasoning for abstract visual tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Neuro-Vector-Symbolic Architecture (NVSA)",
            "system_description": "NVSA uses a neural perception module (CNN / perception backbone) to extract object-level features from images and projects them into high-dimensional hypervector codebooks; a vector-symbolic symbolic reasoning backend (binding, bundling, permutation, similarity/cleanup memory, probabilistic abduction) performs rule detection, factorization, and probabilistic inference to produce final answers. The pipeline is modular: perception -&gt; projection into VSA hypervectors -&gt; resonator/cleanup/nearest-neighbor-based symbolic reasoning.",
            "declarative_component": "Vector-symbolic representations (hypervectors / holographic distributed representations) combined with probabilistic abductive reasoning; symbolic rules and codebooks (VSA codebooks), nearest-neighbor cleanup memory, and explicit reasoning operators (binding, bundling, permutation, similarity).",
            "imperative_component": "Neural perception backbone (convolutional networks / MatMul and Conv dominated) for semantic parsing and feature extraction; standard NN forward inference used to produce inputs for symbolic stage.",
            "integration_method": "Pipeline (Neuro|Symbolic): neural frontend produces features projected into VSA hypervectors (codebook lookup/encoding) that become inputs to symbolic VSA reasoning. The symbolic stage depends on neural outputs and lies on the critical path; NVSA uses probabilistic abduction operating on VSA vectors (resonator network + cleanup memory + argmax similarity). Integration is modular, not end-to-end differentiable in the described implementation.",
            "emergent_properties": "High accuracy and systematic abstract reasoning (e.g., solving Raven-style RPM problems) exceeding pure neural baselines and human baselines cited; ability to bind and represent structured compositional relations, robust factorization of composite perceptual vectors, and explicit interpretable symbolic hypotheses (abductions) about scene structure that enable better data-efficiency and explainability than perception alone.",
            "task_or_benchmark": "Raven's Progressive Matrices (RPM) / spatial-temporal reasoning / abstract visual reasoning datasets (RAVEN, I-RAVEN, PGM).",
            "hybrid_performance": "Reported 98.8% accuracy on spatial-temporal reasoning tasks (NVSA) in the paper's overview; runtime on RTX 2080Ti for RPM task: 380 s; runtime on Jetson TX2: 7507 s. Symbolic stage accounts for ~92.1% of NVSA inference time while contributing ~19% of FLOPS (inefficient on GPU).",
            "declarative_only_performance": null,
            "imperative_only_performance": "ResNet baseline cited in paper: 53.4% (on comparable reasoning benchmark mentioned in overview); GPT-4 cited at 89.0% (comparison context in overview).",
            "has_comparative_results": true,
            "generalization_properties": "NVSA exhibits superior compositional/abstract reasoning and improved data efficiency compared to pure neural models; paper claims NVSA outperforms neural-only models and humans on the specific RPM/spatial-temporal tasks, implying better systematic generalization for these tasks.",
            "interpretability_properties": "High: symbolic VSA stage yields explicit symbolic hypotheses and factorization results (abductions) and uses explicit codebooks and nearest-neighbor clean-up that provide interpretable reasoning traces compared to black-box NN outputs.",
            "limitations_or_failures": "Symbolic VSA stage is memory-bound and sequential; extremely high latency on current CPUs/GPUs (hundreds to thousands of seconds for RPM tasks), low ALU and cache utilization on GPUs, scalability issues as task size grows (quadratic runtime increase observed), and heavy memory footprint for codebooks (&gt;90% of NVSA model storage in some cases).",
            "theoretical_framework": "Complementary-strengths framework: neural perception for feature extraction and VSA/probabilistic symbolic stage for structured, interpretable reasoning; resonator networks and VSA algebra (binding/bundling/permutation + similarity/cleanup) form the mathematical backbone for factorization and abduction.",
            "uuid": "e434.0",
            "source_info": {
                "paper_title": "Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LNN",
            "name_full": "Logical Neural Network",
            "brief_description": "A neuro-symbolic framework that maps neurons to logical formula elements and parameterizes logical connectives (e.g., ∧, ∨) to combine neural learning with symbolic logic for direct interpretability and theorem-proving style tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Logical Neural Network (LNN)",
            "system_description": "LNN encodes facts and symbolic rules into neural-like modules where neurons correspond to logical formula components; logical connectives are represented by parameterized differentiable functions constrained to preserve logical semantics (e.g., via Łukasiewicz or fuzzy logic), enabling the network to perform logical deduction while remaining amenable to learning.",
            "declarative_component": "First-order/fuzzy logic rules and symbolic facts represented explicitly; logical connectives modeled (Łukasiewicz logic) and used as constraints; symbolic rule base and bidirectional reasoning/inference.",
            "imperative_component": "Neural parameterized modules (differentiable functions) that implement logical connectives and enable gradient-based learning; the network structure resembles neural networks but is keyed to logical structure.",
            "integration_method": "Compilation of symbolic rules into parameterized neural modules (Neuro:Symbolic→Neuro style): symbolic knowledge is encoded into neural structure and constraints (end-to-end neural system governed by logic-derived modules); training uses differentiable learning subject to logical constraints.",
            "emergent_properties": "Combines interpretability of symbolic rules with learning-based tolerance to noisy/incomplete data; supports theorem-proving-like reasoning within a learnable neural framework and robustness to incomplete knowledge; modular/compositional deductions via structured neural wiring.",
            "task_or_benchmark": "Logic program tasks (e.g., theorem proving / TPTP-style logical reasoning benchmarks and synthetic logic tasks cited in paper).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Claims improved tolerance to incomplete knowledge and better general applicability for logical tasks relative to pure NNs; paper notes LNNs offer compositional and modular generalization advantages though specific numeric OOD metrics are not provided here.",
            "interpretability_properties": "High: neurons map to logical formula elements and outputs adhere to logic semantics, enabling traceable logical deductions and explanations derived from the symbolic rule encodings.",
            "limitations_or_failures": "Execution is data-movement- and control-flow-heavy (bidirectional inference causes data movement overhead); symbolic operations and sparse, irregular memory access lead to low hardware utilization on CPU/GPU; potential scalability limits for large rule bases and tree-structured deductions.",
            "theoretical_framework": "Fuzzy-first-order logic mapping into parameterized neural modules (Łukasiewicz logic); perspective that symbolic rules can be preserved as constraints within differentiable modules to retain logical semantics while enabling learning.",
            "uuid": "e434.1",
            "source_info": {
                "paper_title": "Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LTN",
            "name_full": "Logic Tensor Network",
            "brief_description": "A neuro-symbolic framework that grounds fuzzy first-order logic in neural tensor representations, transforming logical connectives and quantifiers into differentiable tensor operations for querying and reasoning with data and knowledge.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Logic Tensor Network (LTN)",
            "system_description": "LTN represents logical predicates and terms in vector/tensor spaces and interprets fuzzy first-order logic connectives and quantifiers with differentiable tensor operations; neural graph/MLP components ground symbols in data and compute truth degrees for formulas, enabling learning and reasoning with symbolic axioms.",
            "declarative_component": "Fuzzy first-order logic axioms and logical formulas (FOL) represented as constraints over embedded tensor representations; quantifiers approximated through aggregations.",
            "imperative_component": "Neural graphs / MLPs that ground logical variables and compute truth degrees (tensorized embeddings used as neural inputs), enabling gradient-based learning of the embedding and predicate parameters.",
            "integration_method": "Neuro:Symbolic→Neuro / Neuro Symbolic style: symbolic FOL constraints are converted into differentiable losses/constraints on neural tensor representations (soft constraints), and learning optimizes neural parameters subject to those logic-derived objectives (end-to-end differentiable coupling).",
            "emergent_properties": "Improved interpretability via explicit logical axioms, increased data efficiency, and better out-of-distribution generalization in knowledge-completion-like tasks compared to purely neural systems; logical constraints act as regularizers guiding the learned embeddings.",
            "task_or_benchmark": "Querying and reasoning tasks including knowledge graph completion and UCI-style datasets (as referenced); general logical querying/learning benchmarks.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": "LTN's neural components dominated by MatMul/MLP; no single-number baseline provided in this paper.",
            "has_comparative_results": false,
            "generalization_properties": "Paper states LTNs improve out-of-distribution generalization and data efficiency by encoding knowledge as logical axioms over data; specific numeric OOD metrics not provided here.",
            "interpretability_properties": "Moderate-to-high: logic axioms and truth-degree outputs provide interpretable constraints and explanations for predictions, though inference is still governed by learned embeddings.",
            "limitations_or_failures": "Symbolic constraints are softened into losses which may compromise strict interpretability; dense computation patterns (LTN is noted to be dense) and potential trade-offs between expressivity and scalability.",
            "theoretical_framework": "Fuzzy first-order logic grounded in tensor embeddings; logical formulas map to differentiable aggregations and constraints that guide neural training (principle: encoding symbolic knowledge as soft differentiable constraints).",
            "uuid": "e434.2",
            "source_info": {
                "paper_title": "Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "NLM",
            "name_full": "Neural Logic Machine",
            "brief_description": "A neuro-symbolic architecture that approximates logic operations with neural modules wired to implement logical quantifiers and structured relational reasoning through a multi-layer architecture.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Neural Logic Machine (NLM)",
            "system_description": "NLM constructs multi-layer neural modules that approximate logic operators and quantifiers; it wires local neural function approximators to implement relational induction and logical deduction across object sets, producing higher-level abstractions across layers for relational reasoning.",
            "declarative_component": "Logic-like structure approximated (propositional and relational logic semantics expressed by wiring and module organization); explicit relational rules are approximated via learned modules rather than symbolic rule sets.",
            "imperative_component": "Neural network modules approximating logical operators; the network architecture encodes quantification via carefully constructed neural wiring and aggregation.",
            "integration_method": "Neuro[Symbolic] style where neural modules implement symbolic-like operations: the symbolic logic is embedded/approximated inside the neural architecture (end-to-end neural system that implements symbolic processing internally).",
            "emergent_properties": "Good relational induction and generalization from small to larger-scale tasks; preserves relational/compositional reasoning capability while staying within the neural training framework, allowing generalization to larger numbers of objects (size-generalization) not seen in purely connectionist baselines.",
            "task_or_benchmark": "Relational reasoning and decision-making tasks, e.g., family graph reasoning, sorting, path finding benchmarks and synthetic relational tasks referenced in the paper.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Claims strong generalization from small-scale training to larger-scale tasks due to the structured multi-layer design; improves relational generalization over standard neural baselines.",
            "interpretability_properties": "Moderate: the architecture is designed to emulate logic operations, which helps interpretability of the learned relational computations, but internal approximations mean exact symbolic explanations are implicit rather than explicit.",
            "limitations_or_failures": "Symbolic-like computations implemented as neural approximations can be compute- and data-movement-intensive (vector/element-wise heavy), leading to low hardware utilization; irregular control and sparse syntax-tree structures make CPU/GPU acceleration challenging.",
            "theoretical_framework": "Design principle: approximate logical quantifiers and operators with structured neural modules and wiring to achieve relational inductive biases and compositionality.",
            "uuid": "e434.3",
            "source_info": {
                "paper_title": "Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "VSAIT",
            "name_full": "Vector Symbolic Architecture Image-to-Image Translation (VSAIT)",
            "brief_description": "An image-to-image translation system that leverages vector-symbolic architecture to map images into hypervector space, learn invertible hypervector mappings, and perform robust unpaired translation across domains with large distribution gaps.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Vector Symbolic Architecture Image-to-Image Translation (VSAIT)",
            "system_description": "VSAIT encodes images/features into random high-dimensional hypervectors using VSA operations (binding/bundling/permutation), learns invertible mappings in hypervector space to translate between domains, and decodes back to images, aiming to preserve semantics and avoid hallucinations.",
            "declarative_component": "Vector-symbolic codebooks and algebraic symbolic operations (binding, bundling, permutation) used to represent semantic attributes and ensure consistency across translations.",
            "imperative_component": "Neural networks (perception and decoding networks / ConvNets) for encoding images to features and decoding translated hypervectors back to image space; neural modules learn mappings between hypervector representations.",
            "integration_method": "Pipeline hybrid where neural encoders/decoders interface with a VSA latent symbolic space. The VSA latent operations act as structured symbolic transformations enforced between neural perception and generation stages (Neuro|Symbolic pipeline).",
            "emergent_properties": "Improved robustness to semantic flipping and reduced image hallucination in unpaired domain translation; preservations of semantic consistency across large domain gaps because symbolic hypervector operations enforce structure in latent space.",
            "task_or_benchmark": "Unpaired image-to-image translation benchmarks with large distribution gaps (e.g., GTA, Cityscapes, Google Maps dataset referenced in the paper).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Claims greater robustness in translation and better semantic consistency across domains compared to purely neural unpaired translation methods (reduced hallucination), though no single-number metrics provided here.",
            "interpretability_properties": "Moderate: VSA latent representations provide structured symbolic encodings which make certain transformations and semantic correspondences more explicit than purely neural latent spaces.",
            "limitations_or_failures": "VSA symbolic computations are memory- and data-movement-intensive and are inefficient on GPUs/CPUs; large hypervector codebooks and high-dimensional operations create storage and intermediate caching demands.",
            "theoretical_framework": "Using VSA algebra (binding/bundling/permutation) as a latent symbolic calculus that composes and preserves semantic structure across neural encoder/decoder stages.",
            "uuid": "e434.4",
            "source_info": {
                "paper_title": "Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ZeroC",
            "name_full": "Zero-Shot Concept Recognition and Acquisition (ZeroC)",
            "brief_description": "A neuro-symbolic model that uses graph structures and energy-based models to represent hierarchical concept models, enabling zero-shot recognition and acquisition of novel concepts during inference.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ZeroC (Zero-shot Concept Recognition and Acquisition)",
            "system_description": "ZeroC represents constituent concept models as nodes in a symbolic graph with edges encoding relations, and uses energy-based/neural models to score hypotheses; symbolic graph structure guides inference and generalization so that new concepts can be recognized/acquired at test time.",
            "declarative_component": "Symbolic concept graphs (hierarchical relational graphs) that encode concept relations and structure used to guide inference.",
            "imperative_component": "Neural energy-based models and neural classifiers that score/instantiate concept nodes and perform perception-level processing.",
            "integration_method": "Neuro[Symbolic] style where neural models are integrated into a symbolic graph representation (neural components implement scoring within an explicit symbolic graph) enabling selective attention to pertinent symbolic information during inference.",
            "emergent_properties": "Enables zero-shot recognition and acquisition of novel concepts by leveraging symbolic graph relations combined with neural scoring; hierarchical concept transfer across domains at inference time.",
            "task_or_benchmark": "Cross-domain classification and detection tasks; hierarchical-concept corpus and generalization benchmarks referenced in the paper.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Designed to generalize across domains for novel concept acquisition at inference time; outperforms neural-only baselines in zero-shot concept recognition tasks according to the paper summary.",
            "interpretability_properties": "High: symbolic graph nodes and relations provide interpretable structure showing how concepts relate and how novel concepts are inferred via graph relations.",
            "limitations_or_failures": "Symbolic and ensemble computations can be memory-intensive; neural scoring integrated into symbolic graph may still suffer hardware inefficiencies; limited numeric results reported in this paper.",
            "theoretical_framework": "Combines energy-based neural scoring with explicit symbolic graph structure to allow inference-time concept composition and transfer.",
            "uuid": "e434.5",
            "source_info": {
                "paper_title": "Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "PrAE",
            "name_full": "Probabilistic Abduction and Execution (PrAE) Learner",
            "brief_description": "A neuro-symbolic learner that combines neural visual perception with probabilistic abduction and execution of scene representations to perform spatial-temporal cognitive reasoning and infer hidden rules.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Probabilistic Abduction and Execution (PrAE)",
            "system_description": "PrAE uses neural perception to extract scene primitives and attributes, then constructs probabilistic symbolic scene representations and performs abductive inference (generate latent rules/hypotheses) followed by symbolic execution to predict future events/attributes; final inference is probabilistic and interpretable.",
            "declarative_component": "Probabilistic symbolic scene representations and abductive reasoning machinery (probabilistic abduction, symbolic execution of inferred rules).",
            "imperative_component": "Neural perception modules (CNNs / feature extractors) to extract object attributes and scene information that seed the symbolic probabilistic reasoning pipeline.",
            "integration_method": "Neuro|Symbolic pipeline where perception outputs are transformed into symbolic probabilistic representations used by an abductive symbolic reasoner; the symbolic phase depends on neural outputs and is on the end-to-end critical path.",
            "emergent_properties": "Improved spatial-temporal reasoning and human-level generalizability with transparent, interpretable latent rules; better systematic generalization than pure neural models on spatial-temporal tasks.",
            "task_or_benchmark": "Spatial-temporal reasoning tasks (CLEVRER-like / RAVEN-like datasets cited); cognitive reasoning benchmarks for event prediction and rule inference.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Paper claims human-level generalizability and transparency; symbolic abduction helps infer hidden causal rules enabling systematic generalization, though numeric generalization metrics are not reported here.",
            "interpretability_properties": "High: explicit abductive hypotheses and executed symbolic scene representations provide interpretable reasoning traces and explanations.",
            "limitations_or_failures": "Symbolic stage dominates runtime (~80.5% of PrAE inference time), is memory- and data-movement-bounded, causing low hardware utilization and scalability challenges on standard CPUs/GPUs.",
            "theoretical_framework": "Probabilistic abduction + execution: the hybrid uses probabilistic symbolic representations to handle uncertainty, exploiting neural perception for grounding and symbolic probabilistic inference for structured reasoning.",
            "uuid": "e434.6",
            "source_info": {
                "paper_title": "Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural logic machines.",
            "rating": 2,
            "sanitized_title": "neural_logic_machines"
        },
        {
            "paper_title": "Logical neural networks.",
            "rating": 2,
            "sanitized_title": "logical_neural_networks"
        },
        {
            "paper_title": "Logic tensor networks.",
            "rating": 2,
            "sanitized_title": "logic_tensor_networks"
        },
        {
            "paper_title": "Unpaired image translation via vector symbolic architectures.",
            "rating": 2,
            "sanitized_title": "unpaired_image_translation_via_vector_symbolic_architectures"
        },
        {
            "paper_title": "Abstract spatial-temporal reasoning via probabilistic abduction and execution.",
            "rating": 2,
            "sanitized_title": "abstract_spatialtemporal_reasoning_via_probabilistic_abduction_and_execution"
        },
        {
            "paper_title": "Neural-symbolic vqa: Disentangling reasoning from vision and language understanding.",
            "rating": 2,
            "sanitized_title": "neuralsymbolic_vqa_disentangling_reasoning_from_vision_and_language_understanding"
        },
        {
            "paper_title": "DeepProbLog: Neural probabilistic logic programming.",
            "rating": 2,
            "sanitized_title": "deepproblog_neural_probabilistic_logic_programming"
        },
        {
            "paper_title": "Neural probabilistic logic programming in deepproblog.",
            "rating": 1,
            "sanitized_title": "neural_probabilistic_logic_programming_in_deepproblog"
        }
    ],
    "cost": 0.01781375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture
23 Sep 2024</p>
<p>Zishen Wan 
Georgia Institute of Technology</p>
<p>Che-Kai Liu 
Georgia Institute of Technology</p>
<p>Hanchen Yang 
Georgia Institute of Technology</p>
<p>Ritik Raj 
Georgia Institute of Technology</p>
<p>Chaojian Li 
Haoran You 
Georgia Institute of Technology</p>
<p>Yonggan Fu 
Georgia Institute of Technology</p>
<p>Cheng Wan 
Georgia Institute of Technology</p>
<p>Sixu Li 
Youbin Kim 
Georgia Institute of Technology</p>
<p>University of California
Berkeley</p>
<p>Ananda Samajdar 
Georgia Institute of Technology</p>
<p>IBM Research</p>
<p>Celine Lin 
Georgia Institute of Technology</p>
<p>Mohamed Ibrahim 
Georgia Institute of Technology</p>
<p>University of California
Berkeley</p>
<p>Jan M Rabaey 
University of California
Berkeley</p>
<p>Tushar Krishna 
Georgia Institute of Technology</p>
<p>Arijit Raychowdhury 
Georgia Institute of Technology</p>
<p>Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture
23 Sep 2024B02378E1219334F60776FCBE52D0BC07arXiv:2409.13153v2[cs.AR]cognitive AIneuro-symbolic AIworkload characterizationperformance analysisdomain-specific architecture
The remarkable advancements in artificial intelligence (AI), primarily driven by deep neural networks, are facing challenges surrounding unsustainable computational trajectories, limited robustness, and a lack of explainability.To develop next-generation cognitive AI systems, neuro-symbolic AI emerges as a promising paradigm, fusing neural and symbolic approaches to enhance interpretability, robustness, and trustworthiness, while facilitating learning from much less data.Recent neuro-symbolic systems have demonstrated great potential in collaborative human-AI scenarios with reasoning and cognitive capabilities.In this paper, we aim to understand the workload characteristics and potential architectures for neuro-symbolic AI.We first systematically categorize neuro-symbolic AI algorithms, and then experimentally evaluate and analyze them in terms of runtime, memory, computational operators, sparsity, and system characteristics on CPUs, GPUs, and edge SoCs.Our studies reveal that neuro-symbolic models suffer from inefficiencies on off-the-shelf hardware, due to the memory-bound nature of vector-symbolic and logical operations, complex flow control, data dependencies, sparsity variations, and limited scalability.Based on profiling insights, we suggest cross-layer optimization solutions and present a hardware acceleration case study for vectorsymbolic architecture to improve the performance, efficiency, and scalability of neuro-symbolic computing.Finally, we discuss the challenges and potential future directions of neuro-symbolic AI from both system and architectural perspectives.</p>
<p>I. INTRODUCTION</p>
<p>The remarkable advancements in AI have had a profound impact on our society.These advancements are primarily driven by deep neural networks and a virtuous cycle involving large networks, extensive datasets, and augmented computing power.As we reap the benefits of this success, there is growing evidence that continuing our current trajectory may not be viable for realizing AI's full potential.First, the escalating computational requirements and energy consumption associated with AI are on an unsustainable trajectory [1], threatening to reach a level that could stifle innovation by restricting it to fewer organizations.Second, the lack of robustness and explainability remains a significant challenge, likely due to inherent limitations in current learning methodologies [2], [3].Third, contemporary AI systems often operate in isolation with limited collaboration among humans and other AI agents.Hence, it is imperative to develop next-generation AI paradigms that address the growing demand for enhanced efficiency, explainability, and trust in AI systems.</p>
<p>Neuro-symbolic AI [4] represents an emerging AI paradigm that integrates the neural and symbolic approaches with prob- abilistic representations to enhance explainability, robustness and facilitates learning from much less data in AI (Fig. 1).Neural methods are highly effective in extracting complex features from data for vision and language tasks.On the other hand, symbolic methods enhance explainability and reduce the dependence on extensive training data by incorporating established models of the physical world, and probabilistic representations enable cognitive systems to more effectively handle uncertainty, resulting in improved robustness under unstructured conditions.The synergistic fusion of neural and symbolic methods positions neuro-symbolic AI as a promising paradigm capable of ushering in the third wave of AI [5], [6].</p>
<p>Neuro-symbolic AI promises possibilities for systems that acquire human-like communication and reasoning capabilities, enabling them to recognize, classify, and adapt to new situations autonomously.For example, neuro-vector-symbolic architecture [7] is able to reach 98.8% accuracy on spatialtemporal reasoning tasks, greatly surpassing human performance (84.4%), neuro-only ResNet (53.4%) and GPT-4 performance (89.0%).In addition to its superior performance in vision and language [8], neuro-symbolic AI holds significant potential for enhancing explainability and trustworthiness of collaborative human-AI applications [9].These applications include collaborative robotics, mixed-reality systems, and human-AI interactions, where robots can seamlessly interact with humans in environments, agents can reason and make decisions in an explainable manner, and intelligence is pervasively embedded and untethered from the cloud.</p>
<p>Despite the promising algorithmic performance, the higher memory intensity, greater kernel heterogeneity, and access pattern irregularity of neuro-symbolic computing lead to an increasing divergence from the current hardware roadmap that largely optimizes for matrix multiplication and convolution [10]- [14] and leads to severe inefficiencies and underutilization of hardware.Therefore, understanding its computational and memory demands is essential for efficient processing on both general-purpose and custom hardware.</p>
<p>Our goal in this work is to quantify the workload characteristics and potential system architecture for neuro-symbolic AI.Built on our work [4], [15], we first conduct a systematic review and categorize state-of-the-art neuro-symbolic AI workloads in a structured manner (Sec.II).We then characterize seven representative neuro-symbolic workloads on general-purpose and edge platforms, analyzing their runtime, memory, compute operators, operation graph, hardware utilization, and sparsity characteristics (Secs.III, IV, V).Our workload characterization reveals several key observations and insights, including the following:</p>
<p>• Neuro-symbolic AI models typically exhibit high latency compared to neural models, prohibiting them from realtime applications.• The neural components mainly consist of MatMul and Convs, while the symbolic components are dominated by vector/element-wise and logical operations.The low ALU utilization, low cache hit rates, and high volume of data movement of symbolic operations make them inefficient on CPUs/GPUs and may result in system bottlenecks.• The neural workloads are compute-bounded while the symbolic workloads are typically memory-bounded and face potential scalability issues.• The symbolic operations may depend on neural results or need to compile into the neural structure, thus lying on the critical path of end-to-end neuro-symbolic systems.• Some neural and vector-symbolic components demonstrate a high level of unstructured sparsity with variations under different task scenarios and attributes.Inspired by our workload profiling insights, we recommend several cross-layer software and hardware optimization solutions to improve the efficiency and scalability of neurosymbolic systems (Sec.V).Specifically, we leverage vectorsymbolic architecture as a case study and present a hardware acceleration methodology, including kernel formulation, microarchitecture, dataflow, and control schemes (Sec.VI).Finally, we explore the research opportunities in neuro-symbolic computing and share our outlook on the road ahead (Sec.VII).</p>
<p>To the best of our knowledge, this is one of the first works to characterize neuro-symbolic computing from both system and architectural perspectives, and enable its efficient and scalable execution.We aim to inspire the design of nextgeneration cognitive computing systems through synergistic advancements in neuro-symbolic algorithms, systems, architecture, and algorithm-hardware co-design.</p>
<p>II. NEURO-SYMBOLIC AI ALGORITHMS</p>
<p>In this section, we systematically review and categorize the recent research progress in neuro-symbolic AI algorithms.</p>
<p>Overview.Neuro-symbolic AI represents an interdisciplinary approach that synergistically combines symbolic reasoning with neural network (NN) learning to create intelligent systems, leveraging the complementary strengths of both to enhance the accuracy and interpretability of the resulting models.Given that neuro-symbolic algorithms incorporate symbolic and neural components, various paradigms can be categorized based on how these components are integrated into a cohesive system.Inspired by Henry Kautz's taxonomy [31], we systematically categorize these algorithms into five paradigms (Tab.I).We elaborate on each of these paradigms below.Additionally, Tab.II provides examples of several underlying operations based on the categorization in Tab.I.</p>
<p>Symbolic [Neuro] refers to an intelligent system that empowers symbolic reasoning with the statistical learning capabilities of NNs.These systems typically consist of a comprehensive symbolic problem solver that includes loosely-coupled neural subroutines for statistical learning.Examples include DeepMind's AlphaGo [16] and AlphaZero [32], which use Monte-Carlo Tree Search (MCTS) as the symbolic solver and NN state estimators for learning statistical patterns.</p>
<p>Neuro|Symbolic refers to a hybrid system that combines neural and symbolic components in a pipeline, where each component typically specializes in complementary tasks.To the best of our knowledge, the majority of neuro-symbolic algorithms fall into this category.For example, IBM's neurovector-symbolic architecture (NVSA) [7] uses an NN as the perception frontend for semantic parsing and a symbolic reasoner as the backend for probabilistic abductive reasoning on the RAVEN [33] and I-RAVEN [34] datasets.Probabilistic abduction and execution (PrAE) learner [22] adopts a similar approach where the difference lies in features are first projected to high-dimensional vectors in NVSA, whereas PrAE utilizes the original features directly as the NN's input.Other examples include vector symbolic architecture-based image-to-image translation (VSAIT) [21], neuro-probabilistic soft logic (NeuPSL) [17], neural probabilistic logic programming (DeepProbLog) [35], neuro-answer set programming (NeurASP) [18], neural symbolic dynamic reasoning [36], neural symbolic concept learner (NSCL) [8], abductive learning (ABL) [19], and neuro-symbolic visual question answering (NSVQA) [20] on the CLEVRER dataset [36].</p>
<p>Neuro:Symbolic→Neuro approach incorporates symbolic rules into NNs to guide the learning process, where symbolic knowledge is compiled into the structure of neural models for enhancing the model interpretability.For instance, logical NNs (LNNs) [23] encode knowledge or domain expertise as symbolic rules (first-order logic or fuzzy logic) that act as constraints on the NN output.Other examples include the application of deep learning for symbolic mathematics [24] and differentiable inductive logic programming (ILP) [25].</p>
<p>Neuro Symbolic is a type of hybrid approach that combines symbolic logic rules with NNs.It involves mapping symbolic logic rules onto embeddings that serve as soft constraints or End-to-end symbolic system that uses neural models internally as a subroutine AlphaGo [16] NN, MCTS Vector</p>
<p>Neuro|Symbolic</p>
<p>Pipelined system that integrates neural and symbolic components where each component specializes in complementary tasks within the whole system NVSA [7] NN, mul, add, circular conv.Vector NeuPSL [17] NN, fuzzy logic Vector NSCL [8] NN, add, mul, div, log Vector NeurASP [18] NN, logic rules Non-Vector ABL [19] NN, logic rules Non-Vector NSVQA [20] NN, pre-defined objects Non-Vector VSAIT [21] NN, binding/unbinding Vector PrAE [22] NN, logic rules, prob.abduction Vector</p>
<p>Neuro:Symbolic→Neuro End-to-end neural system that compiles symbolic knowledge externally LNN [23] NN, fuzzy logic Vector Symbolic Math [24] NN Vector Differentiable ILP [25] NN, fuzzy logic Vector</p>
<p>Neuro Symbolic</p>
<p>Pipelined system that maps symbolic first-order logic onto embeddings serving as soft constraints or regularizers for neural model LTN [26] NN, fuzzy logic Vector DON [27] NN Vector</p>
<p>Neuro[Symbolic]</p>
<p>End-to-end neural system that uses symbolic models internally as a subroutine</p>
<p>GNN+attention [28] NN, SpMM, SDDMM Vector ZeroC [29] NN (energy-based model, graph) Vector NLM [30] NN, permutation Vector    [26], for instance, use logical formulas to define constraints on the tensor representations, which have proven successful in knowledge graph completion tasks.These tasks aim to predict missing facts or relationships between entities.Other examples of this approach include deep ontology networks (DONs) [27] and tensorization methods [37].As inference is still governed by NNs, it remains a research question whether this approach will compromise interpretability.
F = ∀x(isCarnivor(s)) → (isM ammal(x)) {isCarnivor(s):[0, 1], isM ammal(x):[1, 0]} → F = [
Neuro[Symbolic] refers to a system that empowers NNs with the explainability and robustness of symbolic reasoning.Unlike Symbolic [Neuro], where symbolic reasoning is used to guide the neural model learning process, in Neuro[Symbolic], the neural model incorporates symbolic reasoning by paying attention to a specific symbolic at certain conditions.For instance, graph neural networks (GNNs) are adopted for representing symbolic expressions when endowed with attention mechanisms [28].In particular, this attention mechanism can be leveraged to incorporate symbolic rules into GNN models, enabling selective attention to pertinent symbolic information in the graph.Other examples include neural logic machines (NLM) [30] and Zero-shot concept recognition and acquisition (ZeroC) [29].ZeroC leverages the graph representation where the constituent concept models are represented as nodes and their relations are represented by edges.</p>
<p>Each neuro-symbolic category reflects different kernel operators and data dependencies.Therefore, this paper takes one of the first steps towards understanding its computing characteristics and aims to serve as a cornerstone for the design and deployment of future neuro-symbolic systems.</p>
<p>III. REPRESENTATIVE NEURO-SYMBOLIC MODELS</p>
<p>This section presents selected widely-used neuro-symbolic AI workloads as representative ones for our analysis.We consider them representative because they are diverse in terms of applications, model structures, and computational patterns.</p>
<p>A. Model Overview.</p>
<p>We select seven neuro-symbolic AI models for profiling analysis (Tab.III): LNN on logic program tasks [23], LTN on querying and reasoning tasks [26], NVSA [7] on the Raven's Progressive Matrices task [33], NLM on relational reasoning and decision making tasks [30], VSAIT on unpaired image-toimage translation tasks [21], ZeroC on cross-domain classification and detection tasks [29], and PrAE on spatial-temporal reasoning tasks [22].These selected workloads represent Neuro:Symbolic→Neuro, Neuro Symbolic , Neuro|Symbolic, and Neuro[Symbolic] systems (Sec.II), respectively.Interested readers could refer to their references for more details.</p>
<p>B. Logical Neural Network (LNN)</p>
<p>LNN is a neuro-symbolic framework that integrates neural learning with symbolic logic, enabling direct interpretability, domain knowledge utilization, and robust problemsolving [23].LNNs map neurons to logical formula elements, using parameterized functions to represent logical connectives (e.g., ∧, ∨) with constraints to preserve logical behavior.By combining facts and rules within a neural framework, LNNs use weighted real-valued logics via Łukasiewicz logic [26].Compared to neural models, LNNs offer superior logical expressivity, tolerance to incomplete knowledge, and general task applicability, excelling in theorem proving with compositional, modular structures.</p>
<p>C. Logical Tensor Network (LTN)</p>
<p>LTN is a neuro-symbolic framework for querying, learning, and reasoning with data and abstract knowledge using fuzzy first-order logic (FOL) [26].LTN grounds FOL elements in data using neural graphs and fuzzy logic, transforming Representative Neuro-Symbolic AI Workloads Logic Neural Network [23] Logic Tensor Network [26] Neuro-Vector-Symbolic Architecture [7] Neural Logic Machine [30] Vector Symbolic Architecture Image2Image Translation [21] Zero-shot Concept Recognition and Acquisition [29] Probabilistic Abduction and Execution [ TPTP benchmark [39] UCI [40], Leptograpsus crabs [41], DeepProbLog [42] RAVEN [33], I-RAVEN [34], PGM [43] Family graph reasoning, sorting, path finding [44] GTA [45], Cityscapes [46], Google Maps dataset [47] Abstraction reasoning [48], Hierarchical-concept corpus [49] RAVEN [33], I-RAVEN [34], PGM [43] Computation Pattern  connectives into real values and interpreting quantifiers via approximate aggregations [26].The network computes truth degrees using embedded tensor representations.Compared to neural models, LTN enhances explainability, data efficiency, and out-of-distribution generalization by expressing knowledge through logical axioms over data.
Datatype FP32 FP32 FP32 FP32 FP32 INT64 FP32 Neural Graph MLP ConvNet</p>
<p>D. Neuro-Vector-Symbolic Architecture (NVSA)</p>
<p>NVSA is a neuro-symbolic architecture for abstract reasoning, combining neural visual perception and vector-symbolic probabilistic reasoning to improve abduction reasoning efficiency [7].NVSA uses holographic distributed representations to co-design visual perception and probabilistic reasoning, enabling perceptual representations and symbolic rule processing for accurate Raven's progressive matrices (RPM) [50], [51] test performance.Compared to neural models, NVSA overcomes the binding problem and superposition catastrophe, achieving superior accuracy in RPM tests and even surpassing human performance.</p>
<p>E. Neural Logic Machine (NLM)</p>
<p>NLM is a neuro-symbolic architecture for inductive learning and logical reasoning, combining neural networks as function approximators with logic programming for symbolic processing [30].NLM approximates logic operations using neural networks and implements logic quantifiers through neural module wiring.Its multi-layer structure deduces object relations, forming higher abstractions with increased layers.Compared to neural models, NLM excels in relational reasoning and decision-making, generalizing well from small-scale to large-scale tasks, outperforming traditional neural networks and logic programming.</p>
<p>F. Vector Symbolic Architecture-Based Image-to-Image Translation (VSAIT)</p>
<p>VSAIT addresses semantic flipping in image translation between domains with large distribution gaps, leveraging vector-symbolic architecture for photorealism and robustness [21].VSAIT learns invertible mappings in hypervector space, ensuring consistency between source and translated images while encoding features into random vector-symbolic hyperspace.Compared to neural models, VSAIT ensures robustness to semantic flipping and significantly reduces image hallucinations observed for unpaired image translation between domains with large gaps.</p>
<p>G. Zero-Shot Concept Recognition and Acquisition (ZeroC)</p>
<p>ZeroC is a neuro-symbolic architecture that recognizes and acquires novel concepts in a zero-shot manner by leveraging symbolic graph structures [29].ZeroC uses graphs and energybased models to represent concepts and relations, allowing hierarchical concept models to generalize across domains during inference.Compared to neural models, ZeroC excels in zero-shot concept recognition, surpassing neural models in tasks requiring novel concept learning without extensive examples.</p>
<p>H. Probabilistic Abduction and Execution (PrAE) Learner</p>
<p>PrAE is a neuro-symbolic learner for spatial-temporal cognitive reasoning, centered on probabilistic abduction and execution of scene representations [22].PrAE combines neural visual perception with symbolic reasoning to predict object attributes and generate probabilistic scene representations, inferring hidden rules for systematic generalization.Compared to neural models, PrAE outperforms them in spatial-temporal reasoning, offering transparency, interpretability, and humanlevel generalizability.</p>
<p>IV. WORKLOAD CHARACTERIZATION METHODOLOGY</p>
<p>This section presents our neuro-symbolic AI workload profiling methodology (Sec.IV-A) and operator characterization taxonomy (Sec.IV-B) that will be leveraged in Sec.V.</p>
<p>A. Workload Profiling Methodology</p>
<p>We first conduct function-level profiling to capture statistics such as runtime, memory, invocation counts, tensor sizes, and sparsity of each model, by leveraging the built-in PyTorch Profiler.We also perform post-processing to partition the characterization results into various operation categories.The experiments are conducted on a system with Intel Xeon Silver 4114 CPU and Nvidia RTX 2080 Ti GPU (250W), as well as edge SoCs such as Xavier NX (20W) and Jetson TX2 (15W).</p>
<p>B. Workload Characterization Taxonomy</p>
<p>On top of function-level profiling, we further conduct compute operator-level profiling for further analysis.We classify each neural and symbolic workload of the LNN, LTN, NVSA, NLM, VSAIT, ZeroC, and PrAE neuro-symbolic models into six operator categories: convolution, matrix multiplication (MatMul), vector/element-wise tensor operation, data transformation, data movement, and others [52].</p>
<p>Convolution: refers to operations involving overlaying a matrix (kernel) onto another matrix (input) and computing the sum of element-wise products.This process is slid across the entire matrix and transforms the data.Convolution is common in neural networks and leads to high operational intensity.</p>
<p>Matrix Multiplication: refers to general matrix multiplication (GEMM) with two matrices, either dense or sparse.Fully-connected layers in neural networks use GEMM as their primary mathematical operation.Multiplication of large, dense matrices is typically computationally intensive but highly parallelizable.There is typically a trade-off between the generality of the sparsity and the overhead of hardware optimization.Sparse matrix multiplication requires efficient mechanisms to perform lookups into the tables of non-zero values.</p>
<p>Vector/Element-wise Tensor Operation: refers to operations performed element-wise on tensors (generalized matrices, vectors, and higher-dimensional arrays), including addition, subtraction, multiplication, and division, applied between two tensors element by element, as well as activation, normalization, and relational operations in neuron models.</p>
<p>Data Transformation: refers to operations that reshape or subsample data, including matrix transposes, tensor reordering, masked selection, and coalescing which is a process in which duplicate entries for the same coordinates in a sparse matrix are eliminated by summing their associated values.</p>
<p>Data Movement: refers to data transferring from memoryto-compute, host-to-device, and device-to-host, as well as operations such as tensor duplication and assignment.</p>
<p>Others: refers to operations such as fuzzy first of logic and logical rules that are utilized in some symbolic AI workloads.</p>
<p>V. WORKLOAD CHARACTERIZATION RESULTS</p>
<p>This section analyzes the performance characteristics of representative neuro-symbolic workloads and discusses their runtime and scalability (Sec.V-A), compute operators (Sec.V-B), memory usage (Sec.V-C), operation graph (Sec.V-D), hardware utilization (Sec.V-E), and sparsity (Sec.V-F).</p>
<p>A. Compute Latency Analysis</p>
<p>End-to-end latency breakdown.We first characterize the end-to-end latency of representative neuro-symbolic AI workloads (Fig. 2).We can observe that (1) Compared to neural workloads, symbolic workloads are not negligible in computing latency and may become a system bottleneck.For example, the neural (symbolic) workloads account for 54.6% (45.4%), 48.0% (52.0%), 7.9% (92.1%), 39.4% (60.6%), 16.3% (83.7%), 73.2% (26.8%), and 19.5% (80.5%) runtime of LNN, LTN, NVSA, NLM, VSAIT, ZeroC, and PrAE models, respectively (Fig. 2a).Notably, the symbolic workload dominates the NVSA's runtime, predominately due to the sequential and computational-intensive rule detection during the involved reasoning procedure.(2) The real-time performance cannot be satisfied, e.g., RTX 2080Ti GPU takes 380 s and TX2 takes 7507 s for RPM task in NVSA (Fig. 2b).Even if more computing resources are available to reduce neural inference time, the significant overhead of vector-symbolicbased reasoning still prohibits real-time execution.(3) The symbolic operations may not be well accelerated by GPU.For example, symbolic counts for 92.1% of total NVSA inference time while its floating-point operations (FLOPS) count for only 19% of total FLOPS, indicating inefficient computation.</p>
<p>Takeaway 1: Neuro-symbolic AI models typically exhibit high latency compared to neural models, prohibiting them from real-time applications.Symbolic operations are processed inefficiently on CPU/GPUs and may result in system bottlenecks.</p>
<p>End-to-end latency scalability.We evaluate the end-toend runtime across various task sizes and complexities, as shown in Fig. 2c of RPM task for NVSA.We can observe that (1) The neural vs. symbolic runtime proportion remains relatively stable across various task sizes.For example, when task size increases from 2×2 to 3×3, the symbolic runtime slightly changes from 91.59% to 87.35%.(2) The total runtime increases quadratically with task size evolving.For example, the total runtime increases 5.02× in the above case, indicating the potential scalability bottleneck of neuro-symbolic models.</p>
<p>Takeaway 2: The neural and symbolic components runtime ratio remains relatively stable while total latency explodes with the task complexity evolving.The potential scalability bottleneck calls for highly scalable and efficient architecture.</p>
<p>Recommendation 1: Optimization on neuro-symbolic workloads from algorithm-system-hardware cross-layer perspectives is highly desirable for achieving real-time, efficient and scalable cognitive systems.</p>
<p>B. Compute Operator Analysis</p>
<p>Fig. 3a partitions the neural and symbolic workloads of the LNN, LTN, NVSA, NLM, VSAIT, ZeroC, and PrAE workloads into six operator categories (Sec.IV-B) with runtime latency breakdown.We make the following observations:</p>
<p>Neural Workload Analysis.The neural workload is dominated by the MatMul and activation operations.LTN (neuro) is dominated by MatMul due to its heavy MLP components, while NVSA, VSAIT, and PrAE's (neuro) majority runtime is on MatMul and convolution because they adopt the neural network as the perception backbone for feature extraction.By    Vector/Element wise 0.00% 0.00% 0.00% 0.00% 30.7% 35.7% 0.51% 0.00% 62.5% 0.00% 0.00% 59.5% 0.00% 31.6%0.00% 28.6% 28.0% 0.00% 34.8% 0.52% 24.5% 0.00% 30.0%0.00% 28.2% 0.00% 36.0%0.91% 39.5% 39.4% 3.48% 6.36% 9.40% 7.12% 0.00% 24.0% 0.00% 24.9% 14.36% 0.84% 13.87% 2.52% 22.9% 10.6% 6.69% 18.1% 0.00% 0.00% 0.00% 58.9% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% contrast, a large portion of LNN and NLM's (neuro) runtime is on vector and element-wise tensor operations due to the sparse syntax tree structure composed of proposition logic and the sequential logic deduction computations on multigroup architecture.Notably, data movement also takes up a significant amount of LNN (neuro) runtime because of its unique bidirectional dataflow during reasoning inference.</p>
<p>Symbolic Workload Analysis.The symbolic workload is dominated by vector and scalar operations that exhibit low operational intensities and complex control flows.Both LNN, LTN, and NLM's (symbolic) have a large number of logic operations, posing parallelism optimization opportunities in their database queries and arithmetic operations, especially for larger symbolic models.Meanwhile, LNN (symbolic) is severally data movement-bounded due to its sparse and irregular memory accesses and bidirectional inference, where model-aware dataflow architecture would likely be beneficial for alleviating this bottleneck.NVSA, VSAIT, and PrAE's (symbolic) are composed of vectors for vector-symbolic operations.Notably, these operations usually stem from highdimensional distributed vector computations (e.g., binding, bundling) for symbolic representation, which are difficult to process efficiently on GPUs.Therefore, the challenges of accelerating these computations will become increasingly important as the task and feature complexities further grow.We leverage VSA kernels as a case study and present a cross-layer optimization solution in Sec.VI to improve system efficiency.</p>
<p>Takeaway 3: The neural components mainly consist of MatMul and Convs, while the symbolic components are dominated by vector/element-wise tensor and logical operations.</p>
<p>The data transfer overhead arising from the separate neural and symbolic execution on GPUs and CPUs poses efficient hardware design challenges.</p>
<p>Recommendation 2: From the architecture level, custom processing units can be built for efficient symbolic operations (e.g., high-dimensional distributed vectors, logical operation, graph, etc).For non-overlap neural and symbolic components, reconfigurable processing units supporting both neural and symbolic operations are recommended.</p>
<p>C. Memory and System Analysis</p>
<p>Memory Usage Analysis.Fig. 3b characterizes the memory usage of the LNN, LTN, NVSA, NLM, VSAIT, ZeroC, and PrAE workloads during computation.We can observe that (1) PrAE (symbolic) consumes a high ratio of memory due to its large number of vector operations depending on intermediate results and exhaustive symbolic search.NVSA (symbolic) slightly alleviates the vector-symbolic operation memory by leveraging probabilistic abduction reasoning.ZeroC (neuro) contains energy-based models and process images in a large ensemble thus taking much memory.(2) In terms of storage footprint, neural weights and symbolic codebooks typically consume more storage.For example, neural network and holographic vector-inspired codebook account for &gt;90% memory footprint in NVSA, because NVSA neural frontend enables the expression of more object combinations than vector space dimensions, requiring the codebook to be large enough to contain all object combinations and ensure quasi-orthogonality.</p>
<p>System Roofline Analysis.Fig. 3c  GPU versions of the selected workloads.We observe that the symbolic components are in the memory-bound area while neural components are in the compute-bound area.For example, NVSA and PrAE symbolic operations require streaming vector elements to circular convolution computing units, increasing the memory bandwidth pressure.Optimizing the compute dataflow and leveraging the scalable and reconfigurable processing element can help provide this bandwidth.Takeaway 4: Symbolic operations are memory-bounded due to large element streaming for vector-symbolic operations.Neural operations are compute-bounded due to computationalintensive MatMul/Convs.Neural weights and vector codebooks typically account for most storage while symbolic components require large intermediate caching during computation.</p>
<p>Recommendation 3: From the algorithm level, model compression (e.g., quantization and pruning) and efficient factorization of neural and symbolic components can be used to reduce memory and data movement overhead without sacrificing cognitive reasoning accuracy.</p>
<p>Recommendation 4: From the technology level, emerging memories and in/near-memory computing can alleviate the memory-bounded symbolic operations and improve scalability, performance, and efficiency of neuro-symbolic systems.</p>
<p>D. Operation and Dataflow</p>
<p>Fig. 4 analyzes the operation dependency in representative neuro-symbolic workloads.We can observe that the reasoning computation of NVSA, VSAIT, and PrAE depends on the result of the frontend neural workload and thus lies on the critical path during inference.LNN, LTN, NLM, and ZeroC need to compile the symbolic knowledge in neural representation or input embeddings.The complex control results in inefficiency in CPU and GPU, and the vector-symbolic computation period results in low hardware utilization.There are opportunities for data pre-processing, parallel rule query, and heterogeneous and reconfigurable hardware design to reduce this bottleneck.</p>
<p>Takeaway 5: The symbolic operations depend on the neural module results or need to compile into the neural structure, thus lying on the critical path of end-to-end neuro-symbolic systems.The vector-symbolic computation phase and complex control of neuro-symbolic components bring low hardware resource utilization and inefficiency in CPU/GPU.</p>
<p>Recommendation 5: From the system level, adaptive workload scheduling with parallelism processing of neural and symbolic components can be leveraged to alleviate resource underutilization and improve runtime efficiency.</p>
<p>E. Hardware Inefficiency Analysis</p>
<p>The hardware inefficiencies of executing neuro-symbolic workloads mainly come from ALU underutilization, low cache hit rate, and massive data transfer.We leverage Nsight Systems/Compute tools to further characterize the GPU behavior of executing selected neuro-symbolic workloads.Tab.IV lists the compute, memory, and data movement characteristics of representative neural and symbolic kernels in NVSA as an example.We observe that typically in symbolic operations, the ALU utilization is &lt;10%, the L1 cache hit rate is around 20%, the L2 cache hit rate is around 40%, and DRAM bandwidth utilization is around 90% with several memory-bounded.The data transfer memory operations account for around 50% of total latency, where &gt;80% is from host CPU to GPU.Additionally, the synchronization overhead and waiting for GPU operations to complete results in CPU underutilization.</p>
<p>Takeaway 6: While neural kernels exhibit high compute utilization and memory efficiency in GPUs, symbolic operations suffer from low ALU utilization, low L1 cache hit rates, and high memory transactions, resulting in low efficiency.</p>
<p>Recommendation 6: From the architecture level, heterogeneous or reconfigurable neural/symbolic architecture with efficient vector-symbolic units and high-bandwidth NoC can be optimized to improve ALU utilization and reduce data movement, thus improving system performance.</p>
<p>F. Sparsity Analysis</p>
<p>Neuro-symbolic workloads also exhibit sparsity features.For example, Fig. 5 characterizes the sparsity of NVSA symbolic modules, including probabilistic mass function (PMF)to-VSA transform, probability computation, and VSA-to-PMF transform, under different reasoning rule attributes.We can observe that NVSA has a high sparsity ratio (&gt;95%) with variations for specific attributes and unstructured patterns.Similarly, ZeroC and LNN also demonstrate &gt;90% sparsity ratio, while LTN features a dense computation pattern.</p>
<p>Takeaway 7: Some neural and vector-symbolic components demonstrate a high level of unstructured sparsity with variations under different task scenarios and attributes.</p>
<p>Recommendation 7: From the algorithm and architecture level, sparsity-aware neural and symbolic algorithm and architecture design can benefit memory footprint, communication overhead, and computation FLOPS reduction.</p>
<p>G. Uniqueness of Neuro-Symbolic vs. Neural Networks</p>
<p>To summarize, based on above analysis, neuro-symbolic AI workloads differ from neural networks mainly in three aspects:  Compute kernels.Neuro-symbolic workloads consist of heterogeneous neural and symbolic kernels.The symbolic operators (e.g., vector, graph, logic) are processed inefficiently on off-the-shelf CPUs/GPUs with low hardware utilization and cache hit and may result in runtime latency bottleneck.</p>
<p>Memory.Symbolic operations are memory-bounded due to large element streaming for vector-symbolic operations.Symbolic codebooks typically account for large memory footprints and require large intermediate caching during computation.</p>
<p>Dataflow and scalability.Neuro-symbolic workloads exhibit more complex control than NNs.Symbolic operations either critically depend on or compile in neural kernels.Their irregular dataflow, data dependency, and sequential processing bring low parallelism scalability and inefficiency in CPU/GPU.</p>
<p>VI. CASE STUDY: HARDWARE ACCELERATION OF VECTOR-SYMBOLIC ARCHITECTURE</p>
<p>This section presents a cross-layer acceleration case study for vector-symbolic architecture (VSA), which is a powerful model in many neuro-symbolic tasks [7], [21], [53], [54].We develop a design method consisting of accelerated vectorsymbolic kernel formulation (Sec.VI-A, VI-B), architecture and dataflow (Sec.VI-C), and programming method (Sec.VI-D), that overcomes computational inefficiencies from executing VSA components on CPUs and GPUs (Sec.VI-E).</p>
<p>Our proposed hardware design is inspired by neurosymbolic workload insights from the characterization study in Sec.V. Specifically, as shown in Tab.V, it features (1) an energy-efficient dataflow with heterogeneous arithmetic units that can flexibly execute key vector-symbolic operations, (2) a distributed memory system employing near-memory computing to enhance scalability and memory performance, (3) compressed storage of symbolic operators to reduce the memory footprint of vector codebooks, and (4) a tiled design for vector-symbolic units to minimize data movement and optimize computational efficiency.These features collectively enable a highly efficient and scalable vector-symbolic hardware accelerator that significantly outperforms traditional platforms.</p>
<p>A. Vector-Symbolic Operations</p>
<p>In the vector-symbolic kernel, computational elements, such as scalars and objects, are represented with hypervectors which can be manipulated by a set of algebraic operations [15], [55], specifically, (1) binding, or element-wise multiplication, which creates a new hypervector that is quasi-orthogonal (dissimilar) to its constituents; (2) bundling, or element-wise addition, which combines hypervectors using element-wise majority count; (3) permutation, which rearranges the elements of a hypervector to preserve its order within a sequence; (4) scalar multiplication, which scales hypervector elements with a scalar weight The similarity between vectors is measured using a variety of distance metrics, such as the dot product, Hamming distance, L1, and L2 [56], [57].These operations collectively form a mathematical framework for implementing various cognitive functions tailored for VSA operations [58].</p>
<p>B. Vector-Symbolic Kernel Formulation</p>
<p>We present a description of operations and programmability features of our proposed hardware accelerator using a formal representation, i.e., kernel function.We express this kernel function as O := F (y, s), where F (•) integrates an array of kernel sub-functions f i that together cover the whole domain of accelerator operations, and y = {y 1 , y 2 , . ..} represents an array combining all item and prototype vectors used in computation.The argument s is defined by a group of conditional variables s = (s 1 , s 2 , . ..), which together are used to draw the sub-domains associated with the sub-functions f i .</p>
<p>The kernel functionality integrates computations for encoding and decoding, memory, and reasoning.Next, we formulate sub-functions f i to describe these computations.</p>
<p>Encoding and Decoding Kernel.To facilitate the encoding and decoding, the kernel function needs to allow for flexible configuration of hypervector operations (binding, bundling, permutation).We take into account that binding can be distributed over bundling [59], and propose the kernel function:
a(y, (s 1 , s 2 )) := b y, (s 2 ) ; s 1 = 0 i b y i , (s 2 ) ; s 1 = 1 ∀{i, j} ⊂ N b y, (s 2 ) :=          y; s 2 = 0 j (y j ); s 2 = 1 ρ j (y j ); s 2 = 2 j ρ (j−1) (y j ); s 2 = 3 ∀{i, j} ⊂ N
where ρ j means that the permutation operation (ρ) is repeated j times, i.e., ρ 3 (x) = ρ(ρ(ρ(x))).Likewise, when j = 3, the term j (x j ) becomes equivalent to (x 1 ⊗ x 2 ⊗ x 3 ), and also j ρ (j−1) (x j ) becomes equivalent to (x 1 ⊗ρ(x 2 )⊗ρ(ρ(x 3 ))).</p>
<p>Resonator-Network Kernel.This is a template VSA kernel for reasoning functions.Specifically, it takes as input a composed vector (which may represent a visual scene involving multiple objects as in the RPM problem) and seeks to factorize the vector into its constituent factors.The operation of the resonator network involves iterative steps for similarity evaluation and projection [54].The kernel function used for projection can be defined as follows: c(y)
:= i [n i × y i ]; ∀i ∈ N; n i ∈ Z.
Here, c(y) calculates a weighted sum of the vectors in y.
1 𝑠 $ ← ∑ % 𝑜𝑏𝑠 % 2 𝑚 $ ← ∑ &amp; 𝑎 &amp; ⊗ 𝑣 &amp; ' 3 𝑏 $ ← ∑ ' 𝑙𝑐 ' 4 𝑥 ← ∑ $ (𝑠 $ ⊗ 𝑚 $ ⊗ 𝑏 $ ) 5 # 𝑣 ! ← 𝑥 ⊗ (𝑠 " ⊗ 𝑏 " ⊗ 𝑎 # ) 6 argmax ' 𝑑(𝑣 &amp; ' , &lt; 𝑣 ( )
• Superpose state information • Binding motor value with ID ( &amp; )</p>
<p>• Gather environment data (labels)</p>
<p>• Learning reactive behavior model • Decoding of a motor value
• Clean-up memory 1, 0, 0 1, 1, 0 1, 0, 0 1, 1, 0 0, 1, 0 −, −, 2
Factoring -Single iteration [46] 1  ←  ⊗ ( ?
𝑏 ⊗ ĉ ⊗ A 𝑑) 2 B 𝑎 ← ∑ % 𝑑(𝑎 % , 𝑥)×𝑎 % 3 argmax % 𝑑(𝑎 % , D 𝑎)
• Decoding step for factor  • Similarity against codebook of "" and weighted bundling of vectors • Finding the right item for factor ""</p>
<p>(following the last iteration)
0, 1, 0 1, 0, 1 −, −, 2
Algorithm Legend: Encoding; Decoding; Clean-up/Associative Memory; Resonator Network Fig. 6: Compact VSA Kernel Formulation.Illustration of how the VSA kernel is programmed to implement workloads.</p>
<p>The weight n i is given by a function d(y i , ȳ), which measures the similarity between items y i and an estimate vector ȳ ∈ y.</p>
<p>Nearest-Neighbor Search Kernel.The similarity function d(y i , ȳ) serves as the basis for identifying the closest vector to a query ȳ ∈ y among an array of vectors y = {y 1 , y 2 , • • • }.The array y represents item vectors when performing a cleanup memory search, and prototype vectors when performing an associative memory search.To describe this operation, we use a kernel function defined as follows: e(y) := argmax i d(y i , ȳ).</p>
<p>Kernel Support for Extended Vector Dimensions via Time-Multiplexing: A particular advantage of element-wise vector operations (binding, bundling, and permutation) is that they can process full-scale vectors and time-multiplexed folds similarly.In contrast, similarity operations in d(y i , ȳ) require the time-multiplexed folds to be collapsed into a single vector representation.When a similarity quantity is computed using only a single fold, it represents a partial quantity.Therefore, to obtain the total similarity value, d(y i , ȳ) needs to aggregate these partial quantities.We express this condition as follows:
d(y i , ȳ) := k y ik • ȳk ∀ i ∈ N; k ∈ {1, 2, . . . , L}.
Here, L is the number of folds, and ȳk and y ik are the k-th folds of the vectors ȳ and y i , respectively.The dot product measures the similarity between these folds, and the sum over all k aggregates the similarities computed for all the folds.</p>
<p>Compact Kernel Formalism.Considering the information presented above, we present a compact and formal description of VSA hardware accelerator's kernel functionality as follows:
F y, (s 1 , s 2 , s 3 ) :=      a y, (s 1 , s 2 ) ; s 3 = 0 c(y); s 3 = 1 e(y); s 3 = 2
In this definition, the control variables (s 1 , s 2 , s 3 ) are used to dynamically adjust the behavior of the kernel during runtime.Fig. 6 demonstrates how the kernel is adjusted to execute VSA workloads.Performance results based on the mapping of these workloads and others are shown in Sec.VI-E.</p>
<p>C. Hardware Architecture and Dataflow</p>
<p>We present a method for constructing architecture dataflow informed by the derived VSA kernels.Fig. 7 shows the overall architecture, consisting of three subsystems: (1) memory and codebook-generation subsystem (MCG), (2) vector-symbolic operations subsystem (VOP), and (3) distance computation subsystem (DC).A control unit is used to decode instructions and determine control configurations.A description of these subsystems and their internal operations are presented below.MCG Subsystem.This subsystem is distributed across multiple tiles, with each tile comprising four functional modules: a local memory (SRAM), a logic unit implementing cellular automata with rule 90 (CA-90) [60], a register file (CA-90 RF), and a query register (QRY).Vectors loaded from the local SRAM are processed exclusively within the tile's logic to leverage near-memory computing.The SRAMs are initialized with randomly generated atomic vectors (i.e., codebooks) used for symbolic encoding.The dimension of these vectors is constrained by the size of the physical datapth; therefore, we utilize a folding mechanism to support extended vector dimensions.CA-90 is integral to this mechanism, utilizing XOR and shift operations to generate new random vectors on-the-fly [60].This design significantly reduces the memory footprint, as only seed folds need to be stored in the local SRAMs.CA-90 RF is a register file that temporarily stores newly generated folds to minimize redundant activations of CA-90.The QRY register holds query data required for similarity computation, an essential component of VSA.</p>
<p>VOP Subsystem.The VOP subsystem implements key VSA operations, used to construct distributed perceptual representations and perform symbolic reasoning computations.It consists of five logic units: a binding unit (BIND), a multiplying unit (MULT), a bundling unit (BND), a register file (BND RF), and a sign unit (SGN).BIND connects to a local buffer storing vectors and is to used to execute the binding operations over these vectors.The superposition of binded vectors is implemented in BND through element-wise addition (bundling).BIND and BND utilize different data representations, with BIND using binary and BND using integer formats.MULT manages the conversion from binary to integer formats and also performs element-wise scalar multiplication, an essential operation for neuro-symbolic encoding.Integer folds outputted from BND can be temporarily stored in BND RF for continuous superposition or converted to binary through SGN for transfer over the global vector-symbolic datapath.</p>
<p>DC Subsystem.This subsystem handles operations for distance computation and nearest neighbor search, and it comprises three critical logic units: POPCNT, DSUM RF, and ARGMAX.POPCNT evaluates the popcount of the difference between two vectors; it executes element-wise XOR operations followed by an addition operation to compute the difference between the number of 1's and the number of 0's in the difference vector.As POPCNT operates on partial vectors due QRY REG MEMORY READ Fig. 8: VSA accelerator's pipeline stages and operation types.</p>
<p>to vector folding, its output also represents a partial distance quantity.Hence, DSUM RF facilitates distance accumulation over multiple partial vectors, distributing distance computations across multiple independently controllable registers.The resulting distance data is then communicated to ARGMAX, which manages the search for the nearest neighbor vector based on the transferred distance values.Parameterized Multi-Tile Architecture.The integration of the above modules leads to a "single-tile" architecture, which includes a single instance of MCG and DC.We also propose a "multi-tile" architecture, which allows memory-bounded vector loading and similarity computations to be distributed across multiple tiles and exploits a SIMD implementation to speed up the execution.This approach, therefore, enables parallel, near-memory processing of symbolic computations, and hence improves the utilization of compute units.A multi-tile architecture also extends the storage capabilities, providing a means to accommodate larger models.Tiles are also equipped with configuration registers, which allow tiles to be selectively activated (or deactivated) before issuing instructions.</p>
<p>D. Accelerator Control Methods</p>
<p>The configuration of the different modules as described above exhibits a pipelined architecture that consists of seven pipeline stages, with each stage associated with a certain type of operation (Fig. 8).Such a pipelined configuration motivates a streamlined integration of dataflow and control-flow primitives, allowing different control methods to be applied without hazard.To perform this study, we particularly examine two control methods for this accelerator: single-operation-percycle (SOPC) and multiple-operations-per-cycle (MOPC).</p>
<p>SOPC and MOPC.SOPC simplifies programming and reduces power consumption since only one pipeline stage switches during each cycle.However, this approach increases runtime, making it unsuitable for high-throughput applications.Conversely, MOPC enables pipeline stages to perform operations simultaneously, thus increasing the number of operations per cycle.However, MOPC leads to increased power consumption and requires a complex mapping framework to analyze program dependencies and optimize control activities.MOPC is better suited for high-throughput applications that require a balance between runtime and power consumption.</p>
<p>Control Methods Comparison.We compare SOPC and MOPC by implementing factorization using the resonator network kernel.Fig. 9 compares the runtime and power consumption of SOPC and MOPC when executing at various complexity levels (number of factors).We observe that MOPC achieves lower runtime in comparison with SOPC, and that the speed-up gained by using MOPC increases from 1.8 to  Fig. 10: The Instruction Word format adopted in the proposed design.</p>
<p>2.3.However, using MOPC also increases power consumption by 44% to 57% as the complexity increases.We adopt MOPC in our design because its better speedup capability is especially important when multiple heterogeneous tasks need to be executed simultaneously.Moreover, the speed-up gain of MOPC can be flexibly configured based on powerconsumption constraints for low-power purposes.</p>
<p>Accelerator Instruction Format.To realize the MOPC control method, we design an instruction-set architecture that employs a wide-word macro format, referred to as Instruction Word.Similar to a Very-Large Instruction Word (VLIW), a single Word consists of multiple operations, except that these operations are sequential in the pipelined dataflow and not parallel like VLIW architectures.As shown in Fig. 10, the Word format consists of seven Type fields, used to specify the operations to be executed in seven pipelined stages, and an OP_PARAM field, used to configure Type operations.This approach offers a high degree of flexibility and is commonly used with domain-specific processors.Details on the instruction fields and compiler optimization are omitted due to space.</p>
<p>E. Evaluation Results</p>
<p>Experimental Setup.The design was implemented in Sys-temVerilog and synthesized with Synopsys Design Compiler using foundry 28nm library.Tab.VI lists the architectural parameters.The energy is measured using Synopsys PrimeTime PX.VSA workloads are also simulated on NVIDIA V100 GPU as the baseline, and GPU power was measured using the nvidia-smi utility.The algorithms listed in Tab.VII are used for evaluation, facilitating a comprehensive assessment of multi-layer cognition systems.</p>
<p>Latency.We first evaluate the impact of varying VSA accelerator (Acc) size on latency.Fig. 11a shows that Acc 4 provides speed-up of 1.3-1.8×compared to Acc 2 , highlighting resource underprovisioning in Acc 2 .However, we observe that the benefits of scaling up the design from Acc 4 to Acc 8 are not equally realized by all algorithms.Specifically, only 1.16× speed-up is achieved by MULT.This is because MULT typically performs VOP-intensive computations for sequence encoding and thus its response to further increase in design size is minimal.On the other hand, REACT achieves 1.69× speed-up when Acc 8 is used.This result is attributed to the fact that REACT performs extensive clean-up memory operations, which can be efficiently distributed across all tiles.Energy Consumption.Fig. 11a shows that the energy efficiency does not exhibit systematic behavior as the accelerator size varies.The reasons are twofold: (1) The leakage power becomes increasingly significant when Acc size is increased.Our analysis shows that the leakage power increases from 1.7 mW to 5.2 mW (i.e., 3× increase) when the design is scaled up from Acc 2 to Acc 8 .(2) Each of the instructions has a unique effect on energy consumption, especially because instructions trigger circuit activity at different hardware modules.</p>
<p>Comparison with GPU.We also compare all VSA accelerator instances with GPU in terms of latency and energy consumption.Fig. 11b shows that Acc is up to three orders of magnitude faster in executing VSA workloads than GPU, despite using batch processing in our GPU implementation.This result consolidates suggests GPU-memory interface is not optimized for VSA data transfer.In addition, Acc operation is up to six orders of magnitude more energy efficient than GPU processing.This performance gap is attributed to GPU's scalar architecture, which relies on complex SIMD arithmetic units to perform simple vector operations.</p>
<p>VII. OUTLOOK AND RESEARCH OPPORTUNITIES</p>
<p>In this section, we discuss the challenges and opportunities for neuro-symbolic systems, and outline our vision for the future, focusing on the system and architecture perspectives.</p>
<p>Building ImageNet-like neuro-symbolic datasets.Neurosymbolic systems hold great potential in achieving humanlike performance [63].However, their current applications are still limited to basic decision-making and reasoning problems, falling short of the broader vision of human cognitive abilities, such as deductive reasoning, compositionality, and counterfactual thinking.It is still an open question of how perception learned from other domains can be transferred to abstract reasoning tasks.To significantly advance the metacognitive capabilities of neuro-symbolic systems, more challenging and suitable datasets are highly desirable to unleash its potential.</p>
<p>Unifying neuro-symbolic models.Integrating neural, symbolic, and probabilistic approaches offers promise to improve AI models' explainability and robustness.However, the current attempts to combine these complementary approaches are still in a nascent manner -how to integrate them in a principled manner remains an open challenge.Particularly, symbolic components can be combined with Large Language Models (LLMs) to improve their planning and reasoning capabilities [64].We envision a unified framework to design algorithms that opportunistically combine neural and symbolic with probabilistic representations, and for quantifying scaling laws for neuro-symbolic inference versus large neural models.</p>
<p>Developing efficient software frameworks.Neurosymbolic AI systems typically utilize underlying logic, such as fuzzy logic, parameterization, and differentiable structures, to support learning and reasoning capabilities.However, most system implementations create custom software for deduction for the particular logic, which limits modularity and extensibility.Thus, new software frameworks are needed that can encompass a broad set of reasoning logical capabilities and provide practical syntactic and semantic extensions while being fast and memory-efficient.Moreover, new programming models and compilers that can facilitate the ease and efficient realization of the neuro-symbolic models are of significance to realize the full promise of neuro-symbolic AI paradigms.</p>
<p>Benchmarking diverse neuro-symbolic workloads.Given the proliferation of neuro-symbolic algorithms and the rapid hardware advancements, it is crucial to benchmark neurosymbolic AI systems in a comparable and validated manner.To achieve this, from the system aspect, we need representative benchmarks that capture the essential workload characteristics (e.g., compute kernels, access patterns, and sparsity) of neural and symbolic models, and that can be quantitatively tested in human-AI applications.From an architectural and hardware perspective, we need modeling-simulation frameworks to enable the development of novel architectures for these workloads and build optimized modular blocks as libraries by leveraging workload characteristics.Benchmarking neurosymbolic computing will guide ML researchers and system architects in investigating the trade-offs in accuracy, performance, and efficiency of various neuro-symbolic algorithms, and in implementing systems in a performance-portable way.</p>
<p>Designing cognitive hardware architectures.Neurosymbolic workloads that combine neural, symbolic, and probabilistic methods feature much greater heterogeneity in compute kernels, sparsity, irregularity in access patterns, and higher memory intensity than DNNs.This leads to an increasing divergence with the current hardware roadmap that largely focuses on matrix multiplication and regular dataflow.Therefore, we need novel architectures with dedicated processing units, memory hierarchies, and NoCs that can handle the additional complexities in computations and communications.Additionally, the architecture needs to provide flexibility with both configurable interconnects and full addressable memories to keep pace with neuro-symbolic AI algorithmic innovations.</p>
<p>VIII. CONCLUSION</p>
<p>Neuro-symbolic AI is an emerging paradigm for developing efficient, robust, explainable, and cognitively advanced AI systems.This paper provides a systematic characterization of neuro-symbolic system performance and analyzes their operational components.Leveraging insights from profiling, we propose cross-layer optimization techniques and present a case study of a hardware architecture designed to enhance their performance and efficiency.We believe this research will address key challenges and highlight opportunities essential for advancing next-generation neuro-symbolic AI systems.</p>
<p>Fig. 1 :
1
Fig. 1: Overview of neuro-symbolic AI systems, workload characterizations, optimization solutions, challenges, and research opportunities in improving the performance of next-generation cognitive AI.</p>
<p>→(</p>
<p>Whiskers ⨂ Tail ⨂ (Laser point → Chases)) → Cat (Cat ⊕ Dog) → Pet ⨂ , , ,  +  , , ,  +  ,   ,   ,   + ,  , ,  +,,</p>
<p>Fig. 2 :
2
Fig. 2: Neural and symbolic runtime latency characterization.(a) Benchmark seven representative neuro-symbolic workloads (LNN, LTN, NVSA, NLM, VSAIT, ZeroC, PrAE) on the CPU+GPU system, showing symbolic may serve as system bottleneck.(b) Benchmark NVSA and NLM workloads on Jetson TX2, Xavier NX, and RTX GPU, showing that real-time performance cannot be satisfied.(c) Benchmark NVSA workload on various RPM task sizes on RTX GPU, indicating the potential scalability problem and consistent symbolic bottleneck.</p>
<p>Fig. 3 :
3
Fig. 3: Compute operators, memory and roofline characterization.(a) Compute operator runtime ratio of representative neuro-symbolic workloads, indicating neural operations mainly consisting of MatMul and Conv, while symbolic operations with vector/tensors.(b) Benchmark memory usage during computation and (c) roofline analysis on RTX 2080Ti GPU, showing typically neural operations are compute-bounded and symbolic operations are memory-bounded.</p>
<p>Fig. 5 :
5
Fig.5: Sparsity analysis.The Sparsity ratio of NVSA symbolic operations, shows a high degree of sparsity with variations in attributes.</p>
<p>Fig. 7 :
7
Fig. 7: Hardware Architecture and Dataflow.The proposed multitile architecture for VSA consists of MCG, DC, and VOP subsystems.</p>
<p>Fig. 9 :
9
Fig. 9: Accelerator Control Methods Comparison.Runtime and power consumption results under two control methods (SOPC and MOPC) when executing the resonator network VSA algorithm.OP_PARAM (57 bits) Type_7 (3 bits) Type_6 (3 bits) Type_5 (3 bits)Type_4(2 bits)Type_3(3 bits)Type_2(3 bits)Type_1(2 bits)</p>
<p>Fig. 11 :
11
Fig. 11: VSA Accelerator (Acc) Efficiency.(a) Comparison between Acc2, Acc4, and Acc8 in terms of latency and energy consumption across workloads.(b) Comparison between Acc and GPU (baseline).</p>
<p>TABLE I :
I
Review of recent neuro-symbolic AI algorithms into five categories, with their underlying operations and vector formats.</p>
<p>TABLE II :
II
Enumeration of the underlying operations based on Tab.I.
Underlying OperationsExamplesFuzzy logic(LTN)</p>
<p>TABLE III :
III
Selected neuro-symbolic AI workloads for analysis, representing a diverse of categories, applications, and computational patterns.</p>
<p>43.6% 19.3% 26.8% 73.1% 22.0% 49.9% 16.4% 17.3% 7.20% 34.6% 22.9% 6.75% 65.3% 33.7% 74.9% 20.1% 56.3% 2.40% 3.11% 6.82% 16.0% 3.85% 2.94% 20.8% 3.96% 2.13% 4.72% 8.11%</p>
<p>employs the roofline model to quantify the memory boundedness of RTX 2080Ti
InputInputInputSymbolicSymbolic Neural NetworkNetwork NeuralStructure SymbolicNetwork NeuralKnowledgeOutputOutputOutputNVSA, VSAIT, PrAENLM, ZeroC, LTNLNNFig. 4: Operator graph analysis. Symbolic operation depends onneural results or needs to compile in neural structure as the criticalpath. Complex control and symbolic-only phase operation result ininefficiency and low hardware resource utilization.</p>
<p>TABLE IV :
IV
Hardware inefficiency analysis.The compute, memory, and communication characteristics of representative neural and symbolic kernels in NVSA workload executed on CPU/GPU platform.
Neural KernelSymbolic Kernelsgemm nn relu nn vectorized elem elementwiseCompute Throughput (%)95.192.93.02.3ALU Utilization (%)90.148.35.94.5L1 Cache Throughput (%)79.782.628.410.8L2 Cache Throughput (%)19.217.529.822.8L1 Cache Hit Rate (%)1.651.629.533.3L2 Cache Hit Rate (%)86.865.548.634.3DRAM BW Utilization (%)14.924.290.978.4</p>
<p>TABLE V :
V
Design Features.Features of the proposed VSA processor and their association with design recommendations from Sec. V.
VSA Processor FeatureFulfilled RecommendationCompressed Storage of SymbolsRecommendation 3 (Sec. V-C)Distributed Memory SystemRecommendation 4 (Sec. V-C)
SIMD Multi-Tile Dataflow Recommendation 5 (Sec.V-D) Heterogeneous Arithmetic Processing Recommendation 6 (Sec.V-E)</p>
<p>TABLE VI :
VI
Hardware Setup.VSA accelerator (Acc) configurations.
InstanceBus Width#Tiles#CA-90-RF#BND-RF#DSUMDistance bitBND bit-MemoryName(W)(K)register (R)register (B)register (D)-width (C)width (H)CapacityAcc25122222128128 KBAcc45124444128256 KBAcc85128888128512 KB</p>
<p>TABLE VII :
VII
Algorithm Setup.VSA workloads used in evaluation.
Workload LayerApplicationProblem Size (Complexity)MULT PerceptionMulti-modal learning and Inference [61]300 samples, 120 item vectors, 16 prototype vectors (classes), 100 queriesTREETree encoding and search [53] 70 tree structures, 9 items, 400 queriesFACTReasoningFactorization of data sets [54]60 iterations, 120 item vectors, 13 prototype vectorsREACTControl Motor learning and recall [62] 500 samples, 55 item vectors, 160 recallsAcc 2Acc 4Acc 8Energy (uJ) Latency (ms)0 2 4 0 50 100MULT MULTTREE TREEFACT FACTREACT REACTEnergy (uJ)Latency (ms) 10 0 10 1 10 2 10 610 3
ACKNOWLEDGEMENTSThis work was supported in part by CoCoSys, one of seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.Dr. Rabaey has made high-impact contributions to a number of fields, including low power integrated circuits, advanced wireless systems, mobile devices, sensor networks, and ubiquitous computing.Some of the systems he helped envision include the infoPad (a forerunner of the iPad), PicoNets and PicoRadios (IoT avant-la-lettre), the Swarm (IoT on steroids), Brain-Machine interfaces and the Human Intranet.His current interests include the conception of the next-generation distributed systems, as well as the exploration of the interaction between the cyber and the biological worlds.He is the primary author of the influential Digital Integrated Circuits: A Design Perspective textbook that has served to educate hundreds of thousands of students all over the world.He is the recipient of numerous awards, is a Life Fellow of the IEEE, and has been involved in a broad variety of start-up ventures.Tushar
Sustainable ai: Environmental implications, challenges and opportunities. C.-J Wu, R Raghavendra, U Gupta, B Acun, N Ardalani, K Maeng, G Chang, F Aga, J Huang, C Bai, Proceedings of Machine Learning and Systems (MLSys). Machine Learning and Systems (MLSys)20224</p>
<p>Analyzing and improving fault tolerance of learning-based navigation systems. Z Wan, A Anwar, Y.-S Hsiao, T Jia, V J Reddi, A Raychowdhury, 2021 58th ACM/IEEE Design Automation Conference (DAC). IEEE2021</p>
<p>Scaling compute is not all you need for adversarial robustness. E Debenedetti, Z Wan, M Andriushchenko, V Sehwag, K Bhardwaj, B Kailkhura, arXiv:2312.131312023arXiv preprint</p>
<p>Towards cognitive ai systems: Workload and characterization of neuro-symbolic ai. Z Wan, C.-K Liu, R Raj, C Li, H You, Y Fu, C Wan, A Samajdar, C Lin, T Krishna, A Raychowdhury, Proceedings of IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)2024</p>
<p>Neurosymbolic ai: The 3 rd wave. A D Garcez, L C Lamb, Artificial Intelligence Review. 2023</p>
<p>Towards cognitive ai systems: a survey and prospective on neuro-symbolic ai. Z Wan, C.-K Liu, H Yang, C Li, H You, Y Fu, C Wan, T Krishna, Y Lin, A Raychowdhury, arXiv:2401.010402024arXiv preprint</p>
<p>. M Hersche, M Zeqiri, L Benini, A Sebastian, A Rahimi, Nature Machine Intelligence. 2023</p>
<p>The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. J Mao, C Gan, P Kohli, J B Tenenbaum, J Wu, International Conference on Learning Representations (ICLR). 2019</p>
<p>Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. Q Gu, A Kuwajerwala, S Morin, K M Jatavallabhula, B Sen, A Agarwal, C Rivera, W Paul, K Ellis, R Chellappa, arXiv:2309.166502023arXiv preprint</p>
<p>A systematic methodology for characterizing scalability of dnn accelerators using scale-sim. A Samajdar, J M Joseph, Y Zhu, P Whatmough, M Mattina, T Krishna, 2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE2020</p>
<p>Heterogeneous dataflow accelerators for multi-dnn workloads. H Kwon, L Lai, M Pellauer, T Krishna, Y.-H Chen, V Chandra, 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE2021</p>
<p>Highlight: Efficient and flexible dnn acceleration with hierarchical structured sparsity. Y N Wu, P.-A Tsai, S Muralidharan, A Parashar, V Sze, J Emer, Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). the 56th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)2023</p>
<p>Algorithm-hardware co-design of distribution-aware logarithmic-posit encodings for efficient dnn inference. A Ramachandran, Z Wan, G Jeong, J Gustafson, T Krishna, arXiv:2403.054652024arXiv preprint</p>
<p>Benchmarking test-time dnn adaptation at edge with compute-inmemory. Z Fan, Z Wan, C.-K Liu, A Lu, K Bhardwaj, A Raychowdhury, Journal on Autonomous Transportation Systems. 2024</p>
<p>Efficient design of a hyperdimensional processing unit for multi-layer cognition. M Ibrahim, Y Kim, J M Rabaey, 2024 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE). IEEE2024</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, M Lanctot, L Sifre, D Kumaran, T Graepel, arXiv:1712.018152017arXiv preprint</p>
<p>Neupsl: Neural probabilistic soft logic. C Pryor, C Dickens, E Augustine, A Albalak, W Wang, L Getoor, arXiv:2205.142682022arXiv preprint</p>
<p>Neurasp: Embracing neural networks into answer set programming. Z Yang, A Ishay, J Lee, 29th International Joint Conference on Artificial Intelligence (IJCAI). 2020</p>
<p>Bridging machine learning and logical reasoning by abductive learning. W.-Z Dai, Q Xu, Y Yu, Z.-H Zhou, Advances in Neural Information Processing Systems (NeurIPS). 201932</p>
<p>Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. K Yi, J Wu, C Gan, A Torralba, P Kohli, J Tenenbaum, Advances in Neural Information Processing Systems (NeurIPS). 201831</p>
<p>Unpaired image translation via vector symbolic architectures. J Theiss, J Leverett, D Kim, A Prakash, European Conference on Computer Vision (ECCV). Springer2022</p>
<p>Abstract spatial-temporal reasoning via probabilistic abduction and execution. C Zhang, B Jia, S.-C Zhu, Y Zhu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2021</p>
<p>R Riegel, A Gray, F Luus, N Khan, N Makondo, I Y Akhalwaya, H Qian, R Fagin, F Barahona, U Sharma, arXiv:2006.13155Logical neural networks. 2020arXiv preprint</p>
<p>Deep learning for symbolic mathematics. G Lample, F Charton, International Conference on Learning Representations (ICLR). 2019</p>
<p>Learning explanatory rules from noisy data. R Evans, E Grefenstette, Journal of Artificial Intelligence Research. 612018</p>
<p>Logic tensor networks. S Badreddine, A D Garcez, L Serafini, M Spranger, Artificial Intelligence. 3031036492022</p>
<p>Ontology reasoning with deep neural networks. P Hohenecker, T Lukas, Journal of Artificial Intelligence Research. 682020</p>
<p>Graph neural networks meet neural-symbolic computing: A survey and perspective. L C Lamb, A Garcez, M Gori, M Prates, P Avelar, M Vardi, IJCAI 2020-29th International Joint Conference on Artificial Intelligence. 2020</p>
<p>Zeroc: A neuro-symbolic model for zero-shot concept recognition and acquisition at inference time. T Wu, M Tjandrasuwita, Z Wu, X Yang, K Liu, R Sosic, J Leskovec, Advances in Neural Information Processing Systems (NeurIPS). 202235</p>
<p>Neural logic machines. H Dong, J Mao, T Lin, C Wang, L Li, D Zhou, International Conference on Learning Representations (ICLR). 2019</p>
<p>Robert s. engelmore memorial lecture at aaai 2020. H Kaut, 2020</p>
<p>Alphazero. H Zhang, T Yu, Deep Reinforcement Learning: Fundamentals, Research and Applications. 2020</p>
<p>Raven: A dataset for relational and analogical visual reasoning. C Zhang, F Gao, B Jia, Y Zhu, S.-C Zhu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2019</p>
<p>Stratified rule-aware network for abstract visual reasoning. S Hu, Y Ma, X Liu, Y Wei, S Bai, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)202135</p>
<p>Neural probabilistic logic programming in deepproblog. R Manhaeve, S Dumančić, A Kimmig, T Demeester, L De Raedt, Artificial Intelligence. 2981035042021</p>
<p>Clevrer: Collision events for video representation and reasoning. K Yi, C Gan, Y Li, P Kohli, J Wu, A Torralba, J B Tenenbaum, International Conference on Learning Representations (ICLR). 2020</p>
<p>Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning. A D Garcez, M Gori, L C Lamb, L Serafini, M Spranger, S N Tran, arXiv:1905.060882019arXiv preprint</p>
<p>Lubm: A benchmark for owl knowledge base systems. Y Guo, Z Pan, J Heflin, Journal of Web Semantics. 32-32005</p>
<p>The tptp problem library and associated infrastructure. G Sutcliffe, Journal of Automated Reasoning. 5942017</p>
<p>Uci machine learning repository. A Asuncion, D Newman, 2007</p>
<p>The research about morphometric characteristics on leptograpsus crabs. Ö Gencer, 2023</p>
<p>Deepproblog: Neural probabilistic logic programming. R Manhaeve, S Dumancic, A Kimmig, T Demeester, L De Raedt, Advances in neural information processing systems (NeurIPS). 201831</p>
<p>Measuring abstract reasoning in neural networks. D Barrett, F Hill, A Santoro, A Morcos, T Lillicrap, International conference on machine learning (ICML). PMLR2018</p>
<p>Hybrid computing using a neural network with dynamic external memory. A Graves, G Wayne, M Reynolds, T Harley, I Danihelka, A Grabska-Barwińska, S G Colmenarejo, E Grefenstette, T Ramalho, J Agapiou, Nature. 53876262016</p>
<p>Playing for data: Ground truth from computer games. S R Richter, V Vineet, S Roth, V Koltun, European Conference on Computer Vision (ECCV). Springer2016</p>
<p>The cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Image-to-image translation with conditional adversarial networks. P Isola, J.-Y Zhu, T Zhou, A A Efros, Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR). the IEEE conference on computer vision and pattern recognition (CVPR)2017</p>
<p>On the measure of intelligence. F Chollet, arXiv:1911.015472019arXiv preprint</p>
<p>An explicitly relational neural network architecture. M Shanahan, K Nikiforou, A Creswell, C Kaplanis, D Barrett, M Garnelo, International Conference on Machine Learning (ICML). PMLR2020</p>
<p>Raven: A dataset for relational and analogical visual reasoning. C Zhang, F Gao, B Jia, Y Zhu, S.-C Zhu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR). the IEEE/CVF conference on computer vision and pattern recognition (CVPR)2019</p>
<p>Stratified rule-aware network for abstract visual reasoning. S Hu, Y Ma, X Liu, Y Wei, S Bai, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)202135</p>
<p>Neuro-symbolic ai: An emerging class of ai workloads and their characterization. Z Susskind, B Arden, L K John, P Stockton, E B John, arXiv:2109.061332021arXiv preprint</p>
<p>Vector Symbolic Architectures as a Computing Framework for Emerging Hardware. D Kleyko, Proceedings of the IEEE. the IEEE2022110</p>
<p>Resonator Networks, 1: An Efficient Solution for Factoring High-Dimensional, Distributed Representations of Data Structures. E P Frady, Neural Computation. 32122020</p>
<p>H3dfact: Heterogeneous 3d integrated cim for factorization with holographic perceptual representations. Z Wan, C.-K Liu, M Ibrahim, H Yang, S Spetalnick, T Krishna, A Raychowdhury, 2024 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE). IEEE2024</p>
<p>Ferex: A reconfigurable design of multibit ferroelectric compute-in-memory for nearest neighbor search. Z Xu, C.-K Liu, C Li, R Mao, J Yang, T Kämpfe, M Imani, C Li, C Zhuo, X Yin, Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE). IEEE2024. 2024</p>
<p>See-mcam: Scalable multi-bit fefet content addressable memories for energy efficient associative search. S Shou, C.-K Liu, S Yun, Z Wan, K Ni, M Imani, X S Hu, J Yang, C Zhuo, X Yin, 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD). IEEE2023</p>
<p>Vector symbolic architectures as a computing framework for emerging hardware. D Kleyko, M Davies, E P Frady, P Kanerva, S J Kent, B A Olshausen, E Osipov, J M Rabaey, D A Rachkovskij, A Rahimi, Proceedings of the IEEE. 110102022</p>
<p>Prototypes and Mapping in Concept Space. P Kanerva, Proceedings of AAAI Fall Symposium: Quantum Informatics for Cognitive Social and Semantic Processes. AAAI Fall Symposium: Quantum Informatics for Cognitive Social and Semantic Processes2010</p>
<p>Cellular automata can reduce memory requirements of collective-state computing. D Kleyko, E P Frady, F T Sommer, IEEE Transactions on Neural Networks and Learning Systems. 3362021</p>
<p>A Programmable Hyper-Dimensional Processor Architecture for Human-Centric IoT. S Datta, IEEE JETCAS. 932019</p>
<p>Shared Control of Assistive Robots through Userintent Prediction and Hyperdimensional Recall of Reactive Behavior. A Menon, Proc. IEEE ICRA. IEEE ICRA2023</p>
<p>Thinking fast and slow in ai. G Booch, F Fabiano, L Horesh, K Kate, J Lenchner, N Linck, A Loreggia, K Murgesan, N Mattei, F Rossi, Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)202135</p>
<p>His research interests include computer architecture, neural-symbolic AI, and hardware-software codesign for novel ML applications. Ritik Raj (Student Member, IEEE) received the B.Tech. degree in electronics and communication engineering from Indian Institute of Technology. S Kambhampati, K Valmeekam, L Guan, K Stechly, M Verma, S Bhambri, L Saldyt, A Murthy, arXiv:2402.01817Zishen Wan (Student Member, IEEE) received the B.E. degree in electrical engineering and automation from Harbin Institute of Technology, Harbin, China, in 2018, and the M.S. degree in electrical engineering from Harvard University. Cambridge, MA, USA; Atlanta, GA, USA; Hangzhou, China; San Jose, USA; Beijing, China; Pittsburgh, PA, USA; Atlanta, GA, USA; Roorkee, India; Atlanta, GA, USA2024. 2020. 2023. 2020Georgia Institute of Technology ; Electronic Engineering from Zhejiang University ; Carnegie Mellon University ; Georgia Institute of Technology ; Georgia Institute of TechnologyarXiv preprintHis research interests include computer architecture, VLSI, and embedded systems, with a focus on designing efficient and reliable hardware and systems for autonomous machines and cognitive intelligence. Che-Kai Liu (Student Member, IEEE) received the B.Eng. degree in. in 2023. Currently, he is pursuing Ph.D. degree with the School of Electrical and Computer Engineering. His research interests include computer architecture, specifically AI and domain-specific accelerators, FPGA, and architecture simulator design</p>
<p>His research interests include deep learning and computer architecture, with a focus on 3D reconstruction and rendering in an algorithm-hardware codesign approach and deep learning on edge devices. Haoran You (Student Member, IEEE) received the B.E. degree in electronic information and communication from. Chaojian Li, Currently, he is pursuing Ph.D. degree with the School of Computer Science. 2019. 2019instrument from Tsinghua University ; Huazhong University of Science and TechnologyCurrently, he is pursuing Ph.D. degree with the School of Computer Science. His research interests are efficient and automated ML/AI systems through algorithm-hardware co-design</p>
<p>His research interests are developing efficient and robust AI algorithms and co-designing the corresponding hardware accelerators towards a triple-win in accuracy, efficiency, and robustness. Cheng Wan (Student Member, IEEE) received the B.E. degree in computer science at. Yonggan Fu, Georgia Institute of Technology. 2019. 2018University of Science and Technology of China ; Shanghai Jiao Tong UniversityHis research interests include algorithm-system co-design for machine learning systems. with a special focus on distributed training</p>
<p>His research interests include digital integrated circuits, computer architecture, and the design of hardware accelerators for high-dimensional computing and machine learning algorithms. Ananda Samajdar (Member, IEEE) received the Ph.D. degree in electrical and computer engineering. Sixu Li, Student Member, IEEE) received the B.E. degree in communication engineering from the University of Electronic Science and Technology of China. Chengdu, China; Atlanta, GA, USA; Cambridge, MA, USA; Berkeley, Berkeley, CA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Yorktown Heights, NY, USA; Urbana-Champaign, Champaign, IL2018. 2022. 2017School of Computer Science, Georgia Institute of Technology ; from Harvard University ; University of CaliforniaHis research interests include computer archtitecture, VLSI design, computer systems design, machine learning algorithm development. Yingyan (Celine) Lin (Member, IEEE) received her Ph.D. degree in Electrical and Computer Engineering from the University of Illinois. Currently, she is an Associate Professor at the School of Com</p>            </div>
        </div>

    </div>
</body>
</html>
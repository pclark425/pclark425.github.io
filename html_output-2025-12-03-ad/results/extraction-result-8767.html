<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8767 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8767</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8767</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-277596537</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.02902v1.pdf" target="_blank">Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable self-improvement capabilities, whereby models iteratively revise their outputs through self-generated feedback. While this reflective mechanism has shown promise in enhancing task performance, recent studies suggest that it may also introduce undesirable biases-most notably, self-bias, or the tendency of LLMs to favor their own prior outputs. In this work, we extend this line of inquiry by investigating the impact on confidence estimation. We evaluate three representative self-improvement paradigms-basic prompting, Chain-of-Thought (CoT) prompting, and tuning-based methods and find that iterative self-improvement can lead to systematic overconfidence, as evidenced by a steadily increasing Expected Calibration Error (ECE) and lower accuracy with high confidence. We then further explore the integration of confidence calibration techniques with self-improvement. Specifically, we compare three strategies: (1) applying calibration after multiple rounds of self-improvement, (2) calibrating before self-improvement, and (3) applying calibration iteratively at each self-improvement step. Our results show that iterative calibration is most effective in reducing ECE, yielding improved calibration. Our work pioneers the study of self-improving LLMs from a calibration perspective, offering valuable insights into balancing model performance and reliability.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8767.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8767.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BasicPrompting_Llama-DeepSeek</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Basic prompting iterative self-improvement on DeepSeek-R1-Distill-Llama-8B (Llama-deepseek)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative intrinsic self-improvement where the model generates an answer, produces feedback on that answer, and refines the answer using the same model in successive rounds; applied to Llama-deepseek and evaluated on MMLU.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Llama-8B (denoted Llama-deepseek)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source deep-reasoning tuned variant (DeepSeek-R1-Distill-Llama-8B). Exact pretraining details not provided in this paper beyond the name and that it is a stronger inherent reasoning model relative to Llama-2-7b-chathf.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Basic prompting iterative self-improvement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>At each round t: produce answer a^(t)=LLM(q); produce feedback f^(t)=LLM(q,a^(t)); then produce refined answer a^(t+1)=LLM(q,a^(t),f^(t)). Iterative loop applied for multiple rounds (experiments report up to 5 rounds).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Massive Multitask Language Understanding benchmark covering 57 domains (STEM, humanities, social science, etc.) used to evaluate knowledge and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative: Llama-deepseek demonstrated progressive self-improvement over multiple prompting rounds (accuracy increased across rounds); however calibration (ECE) was high and generally remained high or worsened during iterative prompting. No precise numeric ACC/ECE values reported for each round in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Qualitative: Baseline (initial, round 0) accuracy lower than the multi-round improved accuracy for Llama-deepseek; initial ECE was already substantially higher than Llama and remained high. No exact numeric baseline given.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering / intrinsic self-feedback: the same model generates feedback on its own output and then re-prompts itself with that feedback to produce refined answers (no external verifier used).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Reported progressive accuracy improvement in Llama-deepseek across multiple rounds of basic prompting self-improvement (text states 'Llama-deepseek demonstrating a clear trend of progressive self-improvement over multiple rounds').</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Calibration degraded: Llama-deepseek had a substantially higher initial ECE and maintained high ECE across iterative improvement, indicating increasing or persistent overconfidence; some confidence bins (e.g., 0.2-0.3) showed dips in confidence; fine-tuning on certain datasets worsened ACC and ECE likely due to dataset-model mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against CoT prompting and SFT: basic prompting enabled progressive ACC improvement in Llama-deepseek, but CoT and SFT had mixed effects depending on model — CoT sometimes gave lower ACC than basic prompting for Llama-deepseek; fine-tuning sometimes degraded deepseek's performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8767.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8767.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BasicPrompting_Llama-2-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Basic prompting iterative self-improvement on Llama-2-7b-chat-hf (Llama)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same iterative self-improvement loop as above, applied to Llama-2-7b-chat-hf and evaluated on MMLU; results differ from Llama-deepseek due to weaker inherent reasoning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7b-chat-hf (denoted Llama)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 7B parameter chat/tuned Llama-2 variant used as a standard baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Basic prompting iterative self-improvement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same loop: a^(t)=LLM(q), f^(t)=LLM(q,a^(t)), a^(t+1)=LLM(q,a^(t),f^(t)); evaluated across rounds (up to 5).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>57-subject multi-domain benchmark assessing knowledge and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative: For Llama, accuracy generally declined over multiple prompting rounds for basic prompting (ACC decreased as number of prompting rounds increased); ECE increased, and basic prompting ultimately yielded the lowest ACC among tested self-improvement strategies for Llama.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Qualitative: Initial round showed better alignment of confidence and accuracy; over rounds performance degraded relative to initial. No numeric ACC/ECE values are provided in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering intrinsic self-feedback as above.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>No consistent improvement: Llama did not exhibit progressive gains with basic prompting; textual evidence indicates deterioration in ACC across rounds, demonstrating that intrinsic self-improvement can fail for weaker models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance degradation with iterative basic prompting (ACC decline); amplification of overconfidence in high-confidence bins by the fifth round; limited context and weaker reasoning capacity led to negative effects.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared with CoT prompting and SFT: CoT methods yielded better ACC in Llama than basic prompting in some settings; fine-tuning improved both ACC and ECE for Llama (unlike for deepseek), suggesting SFT helps weaker models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8767.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8767.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT_Llama-DeepSeek</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting iterative self-improvement on Llama-deepseek</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of Chain-of-Thought prompting to generate intermediate reasoning steps (CoT) which are then used as context during iterative self-improvement and answer refinement; applied to Llama-deepseek on MMLU.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Llama-8B (Llama-deepseek)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DeepSeek reasoning-specialized 8B variant used in experiments; described as having stronger inherent reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) iterative self-improvement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate a CoT c = LLM_CoT(q); then use (q,c,a^(t)) to produce feedback f^(t)=LLM(q,c,a^(t)) and refined answer a^(t+1)=LLM(q,c,a^(t),f^(t)). Experiments compare different CoT lengths (128 vs 512 tokens) and multiple rounds (up to 5).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-domain benchmark for knowledge and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative: Longer CoT (512 tokens) generally enhanced accuracy across both models; Llama-deepseek showed progressive self-improvement trends with CoT but sometimes CoT methods (128/512) exhibited lower ACC than basic prompting or fine-tuning depending on the configuration; calibration (ECE) tended to be lower when CoT reasoning was applied, particularly for Llama.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Qualitative: Baseline without iterative CoT was lower ACC in some settings; exact baseline numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering with structured intermediate reasoning (CoT) used as persistent context across reflection iterations; CoT length varied (128 vs 512 tokens) and affected results.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Text reports that longer CoT reliably produced higher ACC compared to shorter CoT and that Llama-deepseek benefited from longer CoT across rounds (qualitative evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>CoT length subject to context window limits (e.g., 4096-token context causing late drop in Llama when CoT 512 tokens); CoT effectiveness depends on model's intrinsic reasoning—shorter CoT sometimes better for weaker models; occasionally CoT gave lower ACC than other methods for deepseek due to dataset or method mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared with basic prompting and SFT: CoT often improved ACC and sometimes reduced ECE relative to basic prompting, but results varied by model and CoT length; CoT was not uniformly superior.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8767.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8767.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT_Llama-2-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting iterative self-improvement on Llama-2-7b-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CoT prompting applied to Llama-2-7b where the model produces intermediate reasoning and uses it as context to iteratively refine answers; evaluated on MMLU with varied CoT lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7b-chat-hf (Llama)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B parameter Llama-2 chat variant used as standard baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) iterative self-improvement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate CoT and use it as context for feedback and refinement across rounds; CoT length variations (128 vs 512 tokens) evaluated; iterative rounds up to 5.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Comprehensive 57-domain benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative: CoT methods tended to lower ECE and improve ACC for Llama relative to some prompting baselines; longer CoT (512) produced higher ACC than shorter CoT (128) for Llama, though some sudden changes in accuracy appear in particular confidence bins.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Qualitative: Initial performance without CoT was lower in some settings; no numeric values provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering with explicit generation of intermediate reasoning steps used repeatedly during refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Reported lower ECE values and ACC improvements for Llama when CoT prompting applied, and qualitative statements that CoT reduces calibration error for some setups.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Weaker models benefit from shorter CoT sometimes; CoT effectiveness limited by context window (4096 tokens) causing late drops in ACC when CoT too long; iterative CoT sometimes exacerbates overconfidence in certain bins by later rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared with basic prompting and SFT: CoT often outperforms basic prompting for Llama and yields better calibration; SFT also helped Llama in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8767.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8767.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT_LoRA_Llama-DeepSeek</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning (LoRA) self-improvement on Llama-deepseek</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning the base model using supervised self-correction dataset with LoRA (rank=32) to enable self-improvement via learned parameter updates rather than only prompting; applied to Llama-deepseek.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Llama-8B (Llama-deepseek)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DeepSeek reasoning-specialized 8B model; fine-tuned using LoRA (r=32, alpha=16), batch size 8, 5 epochs, LR 2e-4, gradient clipping, BF16, on one A100.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Supervised Fine-Tuning (SFT) with LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train model parameters (LoRA adapters) on a refined self-correction dataset to make the model better at producing corrected outputs; after SFT the model is evaluated; SFT used as a form of self-improvement via learned updates rather than purely iterative prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (evaluation); fine-tune dataset per Zhang et al. 2024b</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MMLU for evaluation; SFT dataset focused on self-correction abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative: For Llama-deepseek, fine-tuned basic prompting resulted in the poorest ACC among conditions and one of the highest ECEs, indicating SFT degraded performance for this stronger model (likely dataset-model mismatch).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Qualitative: Baseline (pre-finetune) basic prompting showed better ACC for Llama-deepseek than the fine-tuned variant in this setup; no numeric values provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Parameter-efficient fine-tuning (LoRA) using a supervised dataset of self-correction examples; subsequent evaluation measures effect of learned self-improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Negative evidence for this model: fine-tuning worsened ACC and calibration for Llama-deepseek, which the authors attribute to mismatch between fine-tuning dataset and reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SFT can harm stronger models if dataset mismatches reasoning distribution; led to higher ECE and lower ACC for Llama-deepseek in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to prompting-based iterative methods: SFT improved weaker Llama but degraded Llama-deepseek; demonstrates SFT's effect depends on base model and dataset alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8767.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8767.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT_LoRA_Llama-2-7b</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning (LoRA) self-improvement on Llama-2-7b-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LoRA fine-tuning using a self-correction-focused dataset to enable self-improvement via learned adjustments; evaluated on Llama-2-7b.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7b-chat-hf (Llama)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B Llama-2 chat model fine-tuned via LoRA with the same hyperparameters as above (r=32, alpha=16, 5 epochs, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Supervised Fine-Tuning (SFT) with LoRA</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>LoRA-based supervised fine-tuning on a refined dataset to teach self-correction behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>57-domain knowledge and reasoning benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative: For Llama, fine-tuning produced improvements: fine-tuned methods surpassed original basic prompting in ACC and improved ECE, indicating better calibration and accuracy after SFT for the weaker model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Qualitative: Baseline (pre-finetune) basic prompting had lower ACC and worse ECE for Llama compared to the fine-tuned variant; no numeric values reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>LoRA parameter adaptation with supervised self-correction data.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Textual reporting that SFT improved both ACC and ECE for Llama (weaker model), contrasting with the effect on deepseek.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Benefit observed primarily for weaker models; strong models may be harmed by poor dataset alignment during SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>SFT helped Llama outperform original basic prompting and in some cases CoT; SFT effect is model-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8767.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8767.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calibration+SI_Strategies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Calibration and Self-Improvement Combined Strategies (Iterative, Calibration-then-SI, Multi-SI-then-Calibration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three strategies to combine temperature-scaling-based calibration (logits-based neural temperature mapping) with iterative self-improvement: (1) iterative calibration each round, (2) calibrate once before iterative self-improvement, (3) perform multiple self-improvement rounds then calibrate once.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7b-chat-hf (Llama); DeepSeek-R1-Distill-Llama-8B (Llama-deepseek)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Both models from the experiments; calibration strategies were evaluated on both.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Iterative calibration / calibration-then-self-improvement / multi-self-improvement-then-calibration</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Calibration implemented via a logits-based temperature-scaling neural mapping (learned auxiliary network to infer task-specific latent temperatures τ). Strategies evaluated: (a) Iterative: calibrate after each self-improvement round; (b) Calibration then SI: calibrate initial response once then perform SI rounds; (c) Multi SI then Calibration: perform T SI rounds then calibrate final output.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-domain benchmark used to measure effect of calibration strategies on ACC and ECE when combined with self-improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Qualitative: Multi self-improvement then calibration yielded the lowest ECE overall (best calibration), while iterative calibration produced relatively high ECE; 'calibration then multi self-improvement' often provided steady ACC improvements for stronger Llama-deepseek and avoided initial ACC drops. Llama (weaker model) saw the iterative method achieve highest ACC across rounds but still with overall decline and higher ECE in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Qualitative: Without any calibration, iterative self-improvement tended to increase ECE (worse calibration) and could either improve or degrade ACC depending on model. No numeric values provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Calibration is logits-based temperature scaling via a learned auxiliary neural mapping that produces task-specific temperatures; combined with prompt-based SI loops as described previously.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical comparisons across the three strategies reported: multi SI then calibration produced markedly lower ECE than other strategies; calibration-then-SI gave steady ACC gains on the stronger model; iterative calibration did not reduce ECE as effectively as post-hoc calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Iterative calibration can be diluted by alternating self-improvement steps and may not reduce ECE effectively; calibration performed at beginning can be swamped by later SI-induced shifts in confidence distribution; effects are model-dependent (strategy that helps deepseek may hurt or not help Llama).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared the three calibration+SI strategies against uncalibrated SI and across models; multi SI then calibration best for ECE reduction, calibration-then-SI best for stable ACC in stronger model, iterative calibration less effective.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8767.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8767.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine_mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related-work iterative self-refinement method where the model generates feedback on its own outputs and refines answers iteratively (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Described in related work as prompting-style iterative refinement where the model generates critique/feedback and uses it to improve answers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-driven self-feedback and iterative correction (mentioned but not experimentally used here).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited as part of broader literature noting that intrinsic self-improvement can be limited and sometimes degrades performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned among prompting-based self-improvement approaches alongside RCI prompting and reflexion.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8767.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8767.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion_mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited method that uses verbal reinforcement learning for language agents to improve via verbalized feedback loops (related work reference).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion (verbal RL-based iterative improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A framework where agents use verbalized reinforcement learning signals to iteratively improve behavior; mentioned in related work but not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Verbal reinforcement learning / internal verbal feedback loop (mentioned only).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Listed among intrinsic improvement and agentic self-improvement approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8767.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8767.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL4f_mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RL4f: Generating natural language feedback with reinforcement learning for repairing model outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work using reinforcement learning to generate natural-language feedback to repair model outputs (appears in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>RL4f (RL-generated feedback for repair)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use RL to generate feedback in natural language that helps repair model outputs; mentioned in related work but not experimentally used here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Reinforcement-learning-generated feedback loop (external training method).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8767.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8767.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Critic_mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Critic: Large language models can self-correct with tool-interactive critiquing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited method where LLMs use external tool-interactive critiquing to self-correct outputs (related work mention).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Critic: Large language models can self-correct with tool-interactive critiquing.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Tool-interactive critiquing (Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Approach combining LLMs with tool-interactive critique to enable self-correction; mentioned in related work but not applied in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External tool-assisted critique loop.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned as an external-information variant of self-improvement, in contrast to intrinsic prompting-only methods studied in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. <em>(Rating: 2)</em></li>
                <li>Critic: Large language models can self-correct with tool-interactive critiquing. <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug. <em>(Rating: 1)</em></li>
                <li>Teaching language models to critique via reinforcement learning. <em>(Rating: 1)</em></li>
                <li>Pride and prejudice: Llm amplifies self-bias in self-refinement. <em>(Rating: 2)</em></li>
                <li>Multiple choice questions: Reasoning makes large language models (llms) more self-confident even when they are wrong. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8767",
    "paper_id": "paper-277596537",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "BasicPrompting_Llama-DeepSeek",
            "name_full": "Basic prompting iterative self-improvement on DeepSeek-R1-Distill-Llama-8B (Llama-deepseek)",
            "brief_description": "Iterative intrinsic self-improvement where the model generates an answer, produces feedback on that answer, and refines the answer using the same model in successive rounds; applied to Llama-deepseek and evaluated on MMLU.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Llama-8B (denoted Llama-deepseek)",
            "model_description": "Open-source deep-reasoning tuned variant (DeepSeek-R1-Distill-Llama-8B). Exact pretraining details not provided in this paper beyond the name and that it is a stronger inherent reasoning model relative to Llama-2-7b-chathf.",
            "reflection_method_name": "Basic prompting iterative self-improvement",
            "reflection_method_description": "At each round t: produce answer a^(t)=LLM(q); produce feedback f^(t)=LLM(q,a^(t)); then produce refined answer a^(t+1)=LLM(q,a^(t),f^(t)). Iterative loop applied for multiple rounds (experiments report up to 5 rounds).",
            "task_name": "MMLU",
            "task_description": "Massive Multitask Language Understanding benchmark covering 57 domains (STEM, humanities, social science, etc.) used to evaluate knowledge and reasoning.",
            "performance_with_reflection": "Qualitative: Llama-deepseek demonstrated progressive self-improvement over multiple prompting rounds (accuracy increased across rounds); however calibration (ECE) was high and generally remained high or worsened during iterative prompting. No precise numeric ACC/ECE values reported for each round in the text.",
            "performance_without_reflection": "Qualitative: Baseline (initial, round 0) accuracy lower than the multi-round improved accuracy for Llama-deepseek; initial ECE was already substantially higher than Llama and remained high. No exact numeric baseline given.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering / intrinsic self-feedback: the same model generates feedback on its own output and then re-prompts itself with that feedback to produce refined answers (no external verifier used).",
            "number_of_iterations": 5,
            "evidence_for_improvement": "Reported progressive accuracy improvement in Llama-deepseek across multiple rounds of basic prompting self-improvement (text states 'Llama-deepseek demonstrating a clear trend of progressive self-improvement over multiple rounds').",
            "limitations_or_failure_cases": "Calibration degraded: Llama-deepseek had a substantially higher initial ECE and maintained high ECE across iterative improvement, indicating increasing or persistent overconfidence; some confidence bins (e.g., 0.2-0.3) showed dips in confidence; fine-tuning on certain datasets worsened ACC and ECE likely due to dataset-model mismatch.",
            "comparison_to_other_methods": "Compared against CoT prompting and SFT: basic prompting enabled progressive ACC improvement in Llama-deepseek, but CoT and SFT had mixed effects depending on model — CoT sometimes gave lower ACC than basic prompting for Llama-deepseek; fine-tuning sometimes degraded deepseek's performance.",
            "ablation_study_results": null,
            "uuid": "e8767.0",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "BasicPrompting_Llama-2-7b",
            "name_full": "Basic prompting iterative self-improvement on Llama-2-7b-chat-hf (Llama)",
            "brief_description": "Same iterative self-improvement loop as above, applied to Llama-2-7b-chat-hf and evaluated on MMLU; results differ from Llama-deepseek due to weaker inherent reasoning ability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7b-chat-hf (denoted Llama)",
            "model_description": "Open-source 7B parameter chat/tuned Llama-2 variant used as a standard baseline in experiments.",
            "reflection_method_name": "Basic prompting iterative self-improvement",
            "reflection_method_description": "Same loop: a^(t)=LLM(q), f^(t)=LLM(q,a^(t)), a^(t+1)=LLM(q,a^(t),f^(t)); evaluated across rounds (up to 5).",
            "task_name": "MMLU",
            "task_description": "57-subject multi-domain benchmark assessing knowledge and reasoning.",
            "performance_with_reflection": "Qualitative: For Llama, accuracy generally declined over multiple prompting rounds for basic prompting (ACC decreased as number of prompting rounds increased); ECE increased, and basic prompting ultimately yielded the lowest ACC among tested self-improvement strategies for Llama.",
            "performance_without_reflection": "Qualitative: Initial round showed better alignment of confidence and accuracy; over rounds performance degraded relative to initial. No numeric ACC/ECE values are provided in the text.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering intrinsic self-feedback as above.",
            "number_of_iterations": 5,
            "evidence_for_improvement": "No consistent improvement: Llama did not exhibit progressive gains with basic prompting; textual evidence indicates deterioration in ACC across rounds, demonstrating that intrinsic self-improvement can fail for weaker models.",
            "limitations_or_failure_cases": "Performance degradation with iterative basic prompting (ACC decline); amplification of overconfidence in high-confidence bins by the fifth round; limited context and weaker reasoning capacity led to negative effects.",
            "comparison_to_other_methods": "Compared with CoT prompting and SFT: CoT methods yielded better ACC in Llama than basic prompting in some settings; fine-tuning improved both ACC and ECE for Llama (unlike for deepseek), suggesting SFT helps weaker models.",
            "ablation_study_results": null,
            "uuid": "e8767.1",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CoT_Llama-DeepSeek",
            "name_full": "Chain-of-Thought (CoT) prompting iterative self-improvement on Llama-deepseek",
            "brief_description": "Use of Chain-of-Thought prompting to generate intermediate reasoning steps (CoT) which are then used as context during iterative self-improvement and answer refinement; applied to Llama-deepseek on MMLU.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Llama-8B (Llama-deepseek)",
            "model_description": "DeepSeek reasoning-specialized 8B variant used in experiments; described as having stronger inherent reasoning.",
            "reflection_method_name": "Chain-of-Thought (CoT) iterative self-improvement",
            "reflection_method_description": "Generate a CoT c = LLM_CoT(q); then use (q,c,a^(t)) to produce feedback f^(t)=LLM(q,c,a^(t)) and refined answer a^(t+1)=LLM(q,c,a^(t),f^(t)). Experiments compare different CoT lengths (128 vs 512 tokens) and multiple rounds (up to 5).",
            "task_name": "MMLU",
            "task_description": "Multi-domain benchmark for knowledge and reasoning.",
            "performance_with_reflection": "Qualitative: Longer CoT (512 tokens) generally enhanced accuracy across both models; Llama-deepseek showed progressive self-improvement trends with CoT but sometimes CoT methods (128/512) exhibited lower ACC than basic prompting or fine-tuning depending on the configuration; calibration (ECE) tended to be lower when CoT reasoning was applied, particularly for Llama.",
            "performance_without_reflection": "Qualitative: Baseline without iterative CoT was lower ACC in some settings; exact baseline numbers not provided.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering with structured intermediate reasoning (CoT) used as persistent context across reflection iterations; CoT length varied (128 vs 512 tokens) and affected results.",
            "number_of_iterations": 5,
            "evidence_for_improvement": "Text reports that longer CoT reliably produced higher ACC compared to shorter CoT and that Llama-deepseek benefited from longer CoT across rounds (qualitative evidence).",
            "limitations_or_failure_cases": "CoT length subject to context window limits (e.g., 4096-token context causing late drop in Llama when CoT 512 tokens); CoT effectiveness depends on model's intrinsic reasoning—shorter CoT sometimes better for weaker models; occasionally CoT gave lower ACC than other methods for deepseek due to dataset or method mismatch.",
            "comparison_to_other_methods": "Compared with basic prompting and SFT: CoT often improved ACC and sometimes reduced ECE relative to basic prompting, but results varied by model and CoT length; CoT was not uniformly superior.",
            "ablation_study_results": null,
            "uuid": "e8767.2",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CoT_Llama-2-7b",
            "name_full": "Chain-of-Thought (CoT) prompting iterative self-improvement on Llama-2-7b-chat-hf",
            "brief_description": "CoT prompting applied to Llama-2-7b where the model produces intermediate reasoning and uses it as context to iteratively refine answers; evaluated on MMLU with varied CoT lengths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7b-chat-hf (Llama)",
            "model_description": "7B parameter Llama-2 chat variant used as standard baseline.",
            "reflection_method_name": "Chain-of-Thought (CoT) iterative self-improvement",
            "reflection_method_description": "Generate CoT and use it as context for feedback and refinement across rounds; CoT length variations (128 vs 512 tokens) evaluated; iterative rounds up to 5.",
            "task_name": "MMLU",
            "task_description": "Comprehensive 57-domain benchmark.",
            "performance_with_reflection": "Qualitative: CoT methods tended to lower ECE and improve ACC for Llama relative to some prompting baselines; longer CoT (512) produced higher ACC than shorter CoT (128) for Llama, though some sudden changes in accuracy appear in particular confidence bins.",
            "performance_without_reflection": "Qualitative: Initial performance without CoT was lower in some settings; no numeric values provided.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering with explicit generation of intermediate reasoning steps used repeatedly during refinement.",
            "number_of_iterations": 5,
            "evidence_for_improvement": "Reported lower ECE values and ACC improvements for Llama when CoT prompting applied, and qualitative statements that CoT reduces calibration error for some setups.",
            "limitations_or_failure_cases": "Weaker models benefit from shorter CoT sometimes; CoT effectiveness limited by context window (4096 tokens) causing late drops in ACC when CoT too long; iterative CoT sometimes exacerbates overconfidence in certain bins by later rounds.",
            "comparison_to_other_methods": "Compared with basic prompting and SFT: CoT often outperforms basic prompting for Llama and yields better calibration; SFT also helped Llama in some cases.",
            "ablation_study_results": null,
            "uuid": "e8767.3",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "SFT_LoRA_Llama-DeepSeek",
            "name_full": "Supervised Fine-Tuning (LoRA) self-improvement on Llama-deepseek",
            "brief_description": "Fine-tuning the base model using supervised self-correction dataset with LoRA (rank=32) to enable self-improvement via learned parameter updates rather than only prompting; applied to Llama-deepseek.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Llama-8B (Llama-deepseek)",
            "model_description": "DeepSeek reasoning-specialized 8B model; fine-tuned using LoRA (r=32, alpha=16), batch size 8, 5 epochs, LR 2e-4, gradient clipping, BF16, on one A100.",
            "reflection_method_name": "Supervised Fine-Tuning (SFT) with LoRA",
            "reflection_method_description": "Train model parameters (LoRA adapters) on a refined self-correction dataset to make the model better at producing corrected outputs; after SFT the model is evaluated; SFT used as a form of self-improvement via learned updates rather than purely iterative prompting.",
            "task_name": "MMLU (evaluation); fine-tune dataset per Zhang et al. 2024b",
            "task_description": "MMLU for evaluation; SFT dataset focused on self-correction abilities.",
            "performance_with_reflection": "Qualitative: For Llama-deepseek, fine-tuned basic prompting resulted in the poorest ACC among conditions and one of the highest ECEs, indicating SFT degraded performance for this stronger model (likely dataset-model mismatch).",
            "performance_without_reflection": "Qualitative: Baseline (pre-finetune) basic prompting showed better ACC for Llama-deepseek than the fine-tuned variant in this setup; no numeric values provided.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Parameter-efficient fine-tuning (LoRA) using a supervised dataset of self-correction examples; subsequent evaluation measures effect of learned self-improvement.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Negative evidence for this model: fine-tuning worsened ACC and calibration for Llama-deepseek, which the authors attribute to mismatch between fine-tuning dataset and reasoning tasks.",
            "limitations_or_failure_cases": "SFT can harm stronger models if dataset mismatches reasoning distribution; led to higher ECE and lower ACC for Llama-deepseek in these experiments.",
            "comparison_to_other_methods": "Compared to prompting-based iterative methods: SFT improved weaker Llama but degraded Llama-deepseek; demonstrates SFT's effect depends on base model and dataset alignment.",
            "ablation_study_results": null,
            "uuid": "e8767.4",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "SFT_LoRA_Llama-2-7b",
            "name_full": "Supervised Fine-Tuning (LoRA) self-improvement on Llama-2-7b-chat-hf",
            "brief_description": "LoRA fine-tuning using a self-correction-focused dataset to enable self-improvement via learned adjustments; evaluated on Llama-2-7b.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7b-chat-hf (Llama)",
            "model_description": "7B Llama-2 chat model fine-tuned via LoRA with the same hyperparameters as above (r=32, alpha=16, 5 epochs, etc.).",
            "reflection_method_name": "Supervised Fine-Tuning (SFT) with LoRA",
            "reflection_method_description": "LoRA-based supervised fine-tuning on a refined dataset to teach self-correction behavior.",
            "task_name": "MMLU",
            "task_description": "57-domain knowledge and reasoning benchmark.",
            "performance_with_reflection": "Qualitative: For Llama, fine-tuning produced improvements: fine-tuned methods surpassed original basic prompting in ACC and improved ECE, indicating better calibration and accuracy after SFT for the weaker model.",
            "performance_without_reflection": "Qualitative: Baseline (pre-finetune) basic prompting had lower ACC and worse ECE for Llama compared to the fine-tuned variant; no numeric values reported.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "LoRA parameter adaptation with supervised self-correction data.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Textual reporting that SFT improved both ACC and ECE for Llama (weaker model), contrasting with the effect on deepseek.",
            "limitations_or_failure_cases": "Benefit observed primarily for weaker models; strong models may be harmed by poor dataset alignment during SFT.",
            "comparison_to_other_methods": "SFT helped Llama outperform original basic prompting and in some cases CoT; SFT effect is model-dependent.",
            "ablation_study_results": null,
            "uuid": "e8767.5",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Calibration+SI_Strategies",
            "name_full": "Calibration and Self-Improvement Combined Strategies (Iterative, Calibration-then-SI, Multi-SI-then-Calibration)",
            "brief_description": "Three strategies to combine temperature-scaling-based calibration (logits-based neural temperature mapping) with iterative self-improvement: (1) iterative calibration each round, (2) calibrate once before iterative self-improvement, (3) perform multiple self-improvement rounds then calibrate once.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7b-chat-hf (Llama); DeepSeek-R1-Distill-Llama-8B (Llama-deepseek)",
            "model_description": "Both models from the experiments; calibration strategies were evaluated on both.",
            "reflection_method_name": "Iterative calibration / calibration-then-self-improvement / multi-self-improvement-then-calibration",
            "reflection_method_description": "Calibration implemented via a logits-based temperature-scaling neural mapping (learned auxiliary network to infer task-specific latent temperatures τ). Strategies evaluated: (a) Iterative: calibrate after each self-improvement round; (b) Calibration then SI: calibrate initial response once then perform SI rounds; (c) Multi SI then Calibration: perform T SI rounds then calibrate final output.",
            "task_name": "MMLU",
            "task_description": "Multi-domain benchmark used to measure effect of calibration strategies on ACC and ECE when combined with self-improvement.",
            "performance_with_reflection": "Qualitative: Multi self-improvement then calibration yielded the lowest ECE overall (best calibration), while iterative calibration produced relatively high ECE; 'calibration then multi self-improvement' often provided steady ACC improvements for stronger Llama-deepseek and avoided initial ACC drops. Llama (weaker model) saw the iterative method achieve highest ACC across rounds but still with overall decline and higher ECE in some cases.",
            "performance_without_reflection": "Qualitative: Without any calibration, iterative self-improvement tended to increase ECE (worse calibration) and could either improve or degrade ACC depending on model. No numeric values provided in text.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Calibration is logits-based temperature scaling via a learned auxiliary neural mapping that produces task-specific temperatures; combined with prompt-based SI loops as described previously.",
            "number_of_iterations": 5,
            "evidence_for_improvement": "Empirical comparisons across the three strategies reported: multi SI then calibration produced markedly lower ECE than other strategies; calibration-then-SI gave steady ACC gains on the stronger model; iterative calibration did not reduce ECE as effectively as post-hoc calibration.",
            "limitations_or_failure_cases": "Iterative calibration can be diluted by alternating self-improvement steps and may not reduce ECE effectively; calibration performed at beginning can be swamped by later SI-induced shifts in confidence distribution; effects are model-dependent (strategy that helps deepseek may hurt or not help Llama).",
            "comparison_to_other_methods": "Compared the three calibration+SI strategies against uncalibrated SI and across models; multi SI then calibration best for ECE reduction, calibration-then-SI best for stable ACC in stronger model, iterative calibration less effective.",
            "ablation_study_results": null,
            "uuid": "e8767.6",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Self-Refine_mention",
            "name_full": "Self-refine: Iterative refinement with self-feedback",
            "brief_description": "Related-work iterative self-refinement method where the model generates feedback on its own outputs and refines answers iteratively (cited in related work).",
            "citation_title": "Self-refine: Iterative refinement with self-feedback.",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Self-refine (iterative self-feedback)",
            "reflection_method_description": "Described in related work as prompting-style iterative refinement where the model generates critique/feedback and uses it to improve answers.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompt-driven self-feedback and iterative correction (mentioned but not experimentally used here).",
            "number_of_iterations": null,
            "evidence_for_improvement": "",
            "limitations_or_failure_cases": "Cited as part of broader literature noting that intrinsic self-improvement can be limited and sometimes degrades performance.",
            "comparison_to_other_methods": "Mentioned among prompting-based self-improvement approaches alongside RCI prompting and reflexion.",
            "ablation_study_results": null,
            "uuid": "e8767.7",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Reflexion_mention",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A cited method that uses verbal reinforcement learning for language agents to improve via verbalized feedback loops (related work reference).",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Reflexion (verbal RL-based iterative improvement)",
            "reflection_method_description": "A framework where agents use verbalized reinforcement learning signals to iteratively improve behavior; mentioned in related work but not used in experiments here.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Verbal reinforcement learning / internal verbal feedback loop (mentioned only).",
            "number_of_iterations": null,
            "evidence_for_improvement": "",
            "limitations_or_failure_cases": "",
            "comparison_to_other_methods": "Listed among intrinsic improvement and agentic self-improvement approaches.",
            "ablation_study_results": null,
            "uuid": "e8767.8",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "RL4f_mention",
            "name_full": "RL4f: Generating natural language feedback with reinforcement learning for repairing model outputs",
            "brief_description": "Cited work using reinforcement learning to generate natural-language feedback to repair model outputs (appears in related work).",
            "citation_title": "Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs.",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "RL4f (RL-generated feedback for repair)",
            "reflection_method_description": "Use RL to generate feedback in natural language that helps repair model outputs; mentioned in related work but not experimentally used here.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Reinforcement-learning-generated feedback loop (external training method).",
            "number_of_iterations": null,
            "evidence_for_improvement": "",
            "limitations_or_failure_cases": "",
            "comparison_to_other_methods": "",
            "ablation_study_results": null,
            "uuid": "e8767.9",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Critic_mention",
            "name_full": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "brief_description": "Cited method where LLMs use external tool-interactive critiquing to self-correct outputs (related work mention).",
            "citation_title": "Critic: Large language models can self-correct with tool-interactive critiquing.",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Tool-interactive critiquing (Critic)",
            "reflection_method_description": "Approach combining LLMs with tool-interactive critique to enable self-correction; mentioned in related work but not applied in this paper's experiments.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "External tool-assisted critique loop.",
            "number_of_iterations": null,
            "evidence_for_improvement": "",
            "limitations_or_failure_cases": "",
            "comparison_to_other_methods": "Positioned as an external-information variant of self-improvement, in contrast to intrinsic prompting-only methods studied in this paper.",
            "ablation_study_results": null,
            "uuid": "e8767.10",
            "source_info": {
                "paper_title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs.",
            "rating": 2,
            "sanitized_title": "rl4f_generating_natural_language_feedback_with_reinforcement_learning_for_repairing_model_outputs"
        },
        {
            "paper_title": "Critic: Large language models can self-correct with tool-interactive critiquing.",
            "rating": 2,
            "sanitized_title": "critic_large_language_models_can_selfcorrect_with_toolinteractive_critiquing"
        },
        {
            "paper_title": "Teaching large language models to self-debug.",
            "rating": 1,
            "sanitized_title": "teaching_large_language_models_to_selfdebug"
        },
        {
            "paper_title": "Teaching language models to critique via reinforcement learning.",
            "rating": 1,
            "sanitized_title": "teaching_language_models_to_critique_via_reinforcement_learning"
        },
        {
            "paper_title": "Pride and prejudice: Llm amplifies self-bias in self-refinement.",
            "rating": 2,
            "sanitized_title": "pride_and_prejudice_llm_amplifies_selfbias_in_selfrefinement"
        },
        {
            "paper_title": "Multiple choice questions: Reasoning makes large language models (llms) more self-confident even when they are wrong.",
            "rating": 1,
            "sanitized_title": "multiple_choice_questions_reasoning_makes_large_language_models_llms_more_selfconfident_even_when_they_are_wrong"
        }
    ],
    "cost": 0.01778275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models
3 Apr 2025</p>
<p>Liangjie Huang 
University of Illinois Chicago</p>
<p>Dawei Li daweili5@asu.edu 
Arizona State University</p>
<p>Huan Liu huanliu@asu.edu 
Arizona State University</p>
<p>Lu Cheng lucheng@uic.edu 
University of Illinois Chicago</p>
<p>Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models
3 Apr 2025F4659B157D7A52EC5615072A42CE9082arXiv:2504.02902v1[cs.CL]Will self-improve lead to bias in confidence estimation?
Large Language Models (LLMs) have demonstrated remarkable selfimprovement capabilities, whereby models iteratively revise their outputs through self-generated feedback.While this reflective mechanism has shown promise in enhancing task performance, recent studies suggest that it may also introduce undesirable biases-most notably, self-bias, or the tendency of LLMs to favor their own prior outputs.In this work, we extend this line of inquiry by investigating the impact on confidence estimation.We evaluate three representative self-improvement paradigms-basic prompting, Chain-of-Thought (CoT) prompting, and tuning-based methods-and find that iterative self-improvement can lead to systematic overconfidence, as evidenced by a steadily increasing Expected Calibration Error (ECE) and lower accuracy with high confidence.We then further explore the integration of confidence calibration techniques with self-improvement.Specifically, we compare three strategies: (1) applying calibration after multiple rounds of self-improvement, (2) calibrating before self-improvement, and (3) applying calibration iteratively at each self-improvement step.Our results show that iterative calibration is most effective in reducing ECE, yielding improved calibration.Our work pioneers the study of selfimproving LLMs from a calibration perspective, offering valuable insights into balancing model performance and reliability.</p>
<p>Introduction</p>
<p>The development of Large Language Models (LLMs) has catalyzed transformative changes across numerous domains, from natural language understanding and generation (Storks et al., 2019;Weld et al., 2022) to assisting in complex question-answering and decisionmaking processes (Li et al., 2025b;2024a;Tan et al., 2024).To handle this, one of the emerging techniques for LLMs is self-improvement (Bai et al., 2022;Kim et al., 2023), wherein LLMs iteratively review their own responses and refine their outputs based on self-generated feedback to enhancing the performance.This process fosters human-like reflective thinking and has proven effective across a range of tasks and applications (Tong et al., 2024;Pan et al., 2024;Li et al., 2024b).</p>
<p>However, some recent studies also report cases where LLM-based self-improvement does not bring a significant boost and can even degrade the model's performance (Zhang et al., 2024a;Wu et al., 2024).One contributing factor to this counterintuitive outcome is selfbias (Xu et al., 2024b;Wataoka et al., 2024;Li et al., 2025a)-the tendency of LLMs to favor their own generated content.This cognitive bias impedes LLMs from providing impartial feedback on their outputs, thereby hindering effective self-correction and self-improvement.</p>
<p>Borrowing this insight, we propose our first research question: Will self-improvement also lead to bias in confidence estimation?As LLMs become increasingly integral to both research Figure 1: The two research questions and overview of our exploration process in this work.and industry applications (Zhu et al., 2025), the ability to accurately express confidence or uncertainty in their outputs is crucial (Su et al., 2024), particularly in high-risk scenarios (Thirunavukarasu et al., 2023;Li et al., 2024c).If self-improvement methods introduce self-bias in confidence estimation, this could pose a significant threat to LLM safety and reliability, creating substantial challenges in the pursuit of trustworthy AI (Sun et al., 2024;Huang et al., 2025).To investigate this, we examine three types of self-improvement methods in our experiments: Basic prompting, Chain-of-Thought (CoT) prompting, and Tuningbased approaches (First et al., 2023;Han et al., 2024;Zhang et al., 2024b;Aky ürek et al., 2023;Xie et al., 2025).We implement each method and analyze its impact on LLMs' confidence estimation performance.Our results reveal a clear trend of increasing overconfidence as self-improvement iterations progress, leading to a continuously rising Expected Calibration Error (ECE) score (Guo et al., 2017).</p>
<p>As calibration Guo et al. (2017); Geng et al. (2023); Xie et al. (2024b) serves as an effective technique to align a model's confidence with its correctness and thus improve models confidence estimation, we pose our second research question: How to combine calibration with the self-improvement method to mitigate the confidence estimation bias?To explore this, we examine the compounded effects of calibration and self-improvement.Specifically, we evaluate three experimental settings to analyze their interaction: (1) multiple self-improvement iterations followed by calibration, (2) calibration applied before multiple self-improvement iterations, and (3) iterative calibration and self-improvement at each step.Our results indicate that applying calibration before self-improvement leads to sustained improvements over time.Meanwhile, self-improvement then calibration achieves the best ECE score, resulting in better-calibrated confidence estimates.</p>
<p>To summarize, our contribution in this paper is in two-fold:</p>
<p>• From a novel perspective of calibration, we first propose to explore selfimprovement's impact on LLMs' confidence estimation and reveal a significant overconfidence issue caused by iterative self-improvement.</p>
<p>• We explore several self-improvement paradigms to showcase the compounded effect when combining self-improvement with calibration, producing LLMs that are both effective and reliable in real-world applications.</p>
<p>Related Work</p>
<p>Self-Improvement generally refers to the way that LLMs try to review and correct their own mistakes to achieve performance improvement on their own.Broader view on this topic can be categorized into three types of methods (Kamoi et al., 2024): Intrinsic Improvement, External Information and Fine-tuning.Intrinsic Improvement means LLMs generate feedbacks to their own responses and correct themselves (Kim et al., 2023;Dhuliawala et al.).Recently, some researchers found that intrinsic improvement can be affected by the prompting mechanism.Specifically, prompting with CoT and self-refinement style have gained effective results (Shinn et al., 2023;Madaan et al., 2023;Fu et al., 2025).An iterative and intrinsic self-improvement process where LLMs generates a response, receives feedback via a feedback model, and refines its output using the same model as a refinement model.</p>
<p>External information will introduce some extra tools to help check the responses from LLMs.These include many scopes, such as code executors (Chen et al., 2023), search engines (Zhao et al., 2023), human feedback (Chen et al., 2024) and so on.Fine-tuning for self-improvement generates feedback and then refines its responses, so that it achieves self-improvement via learning from these corpus.Popular methods in this branch are supervised fine-tuning (First et al., 2023;Han et al., 2024;Zhang et al., 2024b) and reinforcement learning (RL) (Aky ürek et al., 2023;Xie et al., 2025).</p>
<p>In this study, we focus on intrinsic self-improvement, a concept that has attracted considerable debate in recent years.On the one hand, studies such as Bai et al. (2022) suggest that prompting LLMs can enable them to self-correct harmful outputs.Other work, including self-refine approaches (Madaan et al., 2023) and RCI Prompting (Kim et al., 2023), demonstrates how LLMs can iteratively refine their own responses in reasoning tasks.On the other hand, Huang et al. (2023a) indicates that LLMs may struggle to enhance their performance without external feedback, and that their performance can even degrade after self-improvement attempts.Further research similarly reports that achieving selfimprovement by solely relying on prompts remains challenging (Gou et al., 2023;Olausson et al., 2023).These findings motivate us to investigate the underlying mechanisms and conditions under which intrinsic self-improvement can be most effectively realized.Additionally, we also adopt supervised fine-tuning method for self-improvement, considering its efficiency and effectiveness compared with the RL-based one.</p>
<p>Calibration.Popular methods for calibrating language models can be broadly classified into five categories: verbalization-based, self-consistency-based, logit-based, internal state-based, and surrogate approaches (Geng et al., 2023;Xie et al., 2024b).Verbalization-based methods leverage an LLM to explicitly express uncertainty about its answers.For instance, Xu et al. (2024a) fine-tune its language models and then prompt the LLMs to indicate the confidence of its response by generating a probability scaler.Self-consistency-based methods rely on the intuition that confident models produce consistent outputs.Consequently, these methods sample multiple responses and estimate confidence by clustering outputs based on similarity (Huang et al., 2024).Internal state-based examines how the model's internal layers (like attention heads or hidden states) respond during generation (Azaria &amp; Mitchell, 2023;Li et al., 2023).And surrogate models are used to mimic or approximate a black-box LLM in order to estimate confidence or uncertainty (Shrivastava et al., 2023).</p>
<p>However, both verbalization-based and self-consistency-based methods may be constrained by the LM's ability to follow instructions accurately.Appropriate layers or heads in internal state-based methods vary a lot and thus are hard to unify for comparison.And the surrogate model is not the same as the target model.For a better fit to our research goal, logit-based methods directly utilize predicted token probabilities to evaluate response confidence (Huang et al., 2023b).Typically, logits are transformed or calculated to represent the forecasted confidence.The logits are believed to have the capacity to offer a more nuanced understanding of confidence knowledge (Widmann et al., 2021;Kuhn et al.;Jang et al., 2024).Notably, as a logits-based method, temperature scaling has been widely applied in LLMs for answering questions.By adjusting the temperature parameter, it influences the model's probability distribution over possible answers, thereby enhancing its performance in selecting the correct option (Peeperkorn et al., 2024;Xie et al., 2024a;Shen et al., 2024).</p>
<p>In this work, we thus use logits-based calibration approach to discover the relationship between self-improvement and calibration in multi domains.</p>
<p>Methods</p>
<p>In this section, we introduce the three backbone techniques for self-improvement, as well as various manners to marry self-improvement methods with calibration.The overall framework can be found in Figure 1.</p>
<p>Self-Improvement</p>
<p>Basic Prompting.Basic prompting in this work refers to clearly and directly prompt LLMs to answer questions without guiding the LLM to output its CoT.As shown in left panel of Figure 1, an LLM generates an answer a (0) = LLM( q ) given query q .Subsequently, this initial answer undergoes an evaluation phase, where feedback f (t) in round t is produced.This feedback is then in a subsequent step "Answer Refining," to revise the answer given the feedback to get a (t+1) .
f (t) = LLM(q, a (t) ), t ≥ 0; a (t+1) = LLM(q, a (t) , f (t) ).(1)
This iterative loop above involving answer generation, feedback provision, and refinement contributes to enhancing the LLM's performance (Madaan et al., 2023).</p>
<p>Chain-of-Thought (CoT) Prompting.CoT Prompting (Wei et al., 2022) involves guiding LLMs through step-by-step reasoning to solve complex problems, generate detailed explanation or feedback.Recent advancement, including the emerging DeepSeek-R1 models (Guo et al., 2025), leverage CoT by explicitly generating structured intermediate steps, significantly boosting the reasoning capabilities.We also introduce this technique as one of our self-improvement methods.Instead of directly outputting answers from LLMs, we first guide the language models to generate a CoT response c = LLM CoT ( q ), explicitly articulating the reasoning steps involved in answering the query.After getting the CoT for a specific question, we then use this generated CoT as new context to guide the LLM to provide the answer a = LLM(q, c):
f (t) = LLM(q, c, a (t) ), a (t+1) = LLM(q, c, a (t) , f (t) ).(2)
Supervised Fine-Tuning (SFT).Apart from prompting, we also utilize SFT (Dong et al., 2024a;b) with specific datasets to investigate the resoning ability in LLMs, thereby exploring its role in self-improvement and calibration.A SFT loss is typically defined as
L SFT (θ) = − ∑ (q,y)∈D I ∑ i=1 log p θ (y i |q, y &lt;i ). (3)
Noted that I is the total number of tokens in the target output sequence y.And p θ (y i |q, y &lt;i ) is the probability assigned by the model to token y i .Query q is input sequence, which means the question prompt.i is to locate output sequence position.θ is the model parameters.D is the finetuning dataset, a collection of question query and the according answer pairs.</p>
<p>Calibration</p>
<p>In the context of LLMs, calibration refers to how well an LLM's predicted confidence aligns with its actual accuracy.As one of the most common calibration approaches, temperature scaling (Guo et al., 2017) is a post-hoc calibration strategy that aligns model predictions with observed probabilities.We adapt the method from (Shen et al., 2024), a temperature scaling calibration approach tailored to LLMs that learns an auxiliary model to map the outputs of the LLM to better-calibrated probabilities.The calibration formula is shown below:
p(y n |q n , τ k ; W) = exp(w T y ϕ(q n ; W)/τ k ) ∑ v ′ exp(w T v ′ ϕ(q n ; W)/τ k ) . (4)
The key idea is to train an neuro network to fit the logits distribution and then use the network to infer task-specific latent temperatures τ, allowing the model to adapt to new questions with learned parameters.ϕ(q n ; W) is the feature that the language model produces for the input token sequence q n .∑ v ′ exp(w T v ′ ϕ(q n ; W)/τ k ) is the sum of exponential over all possible tokens v ′ in the vocabulary.W and w are model parameters and the logit vector transformation, respectively.The method is computationally efficient, to preserve the accuracy of the LLM, and takes a step towards being universal among different tasks.</p>
<p>Marrying Self-improvement with Calibration</p>
<p>We propose three methods to answer the second research question, via marrying selfimprovement with calibration, as shown in Figure 1.</p>
<p>The Iterative Method refers to a process where each round consists of basic-promptingbased self-improvement followed by calibration.It facilitates a direct observation of how selfimprovement and calibration mutually enhance or constrain each other during successive iterations.
a (t+1) = Calibrate Self-Improve(a (t) ) , t = 0, 1, 2, . . .(5)
In the equation, a (0) is the inital response from LLM for query q and a (t+1) is the result after self-improvement and calibration in each round.</p>
<p>Calibration then Self-improvement performs calibration only once at the beginning, and in the subsequent rounds, only self-improvement is conducted.</p>
<p>a (0) = Calibrate a (0) ; a (t+1) = Self-Improve a (t) , t = 0, 1, 2, . . .</p>
<p>This helps determine if an initial calibration provides a stronger foundation for subsequent self-improvements, reducing the risk of deviation from the ideal state.</p>
<p>In Multi Self-Improvement then Calibration, it instructs the LLM to perform T rounds of self-improvement first, followed by a single calibration:
a (t+1) = Self-Improve a (t) , t = 0, 1, 2, . . . , T − 1. a (final) = Calibrate a (T) . (7)
This design allows the model to freely explore and maximize its potential before using calibration to correct accumulated errors and biases.It also helps to assess whether a single calibration step remains effective in correcting deviations accumulated through multiple self-improvement iterations.</p>
<p>EXPERIMENTS</p>
<p>Set Up</p>
<p>Models.In this paper, we use popular open-source LLMs.Specifically, Llama-2-7b-chathf (Touvron et al., 2023) as a standard LLM and DeepSeek-R1-Distill-Llama-8B (Guo et al., 2025) as a deep thinking model, denoted as Llama-deepseek in later section, are used to investigate the effectiveness and relationship between self-improvement and calibration.</p>
<p>Dataset.MMLU (Hendrycks et al., 2020) is utilized in our paper for evaluation.The MMLU is a comprehensive benchmark, which covers 57 sub-datasets spanning various subjects including STEM (Science, Technology, Engineering, and Mathematics), humanities, social sciences, and other specialized areas.It consists of multi-domain questions that assess both world knowledge and problem-solving abilities, making it well-suited for evaluating calibration and self-improvement.We use all 57 sub-datasets for the experiments.</p>
<p>In terms of SFT dataset, we follow (Zhang et al., 2024b), which focuses on the self-correction abilities of small, open-source LMs, exploring whether they can self-correct with minimal guidance.We adopt their refined dataset to fine-tune the LLMs.</p>
<p>Evaluation Metrics.We investigate our research using these two metrics: Accuracy (ACC) for LLM prediction accuracy and ECE for model calibration measurement.ECE is a widely adopted metric that measures the discrepancy between predicted confidence and its actual accuracy.A high ECE score reflects poor calibration, indicating a significant discrepancy between the model's predicted confidence and its empirical accuracy on the given dataset.
ECE = K ∑ k=1 |B k | N |acc(B k ) − conf(B k )| . (8)
ECE essentially computes a weighted sum of absolute differences between accuracy acc(B k ) and confidence conf(B k ) across bins.Here, we use 10 bins of width 0.1 each in [0,1], where N denotes the number of model's generations and K denotes the number of bins.</p>
<p>Supervised Fine-Tuning (SFT).We performed SFT of the base models using Low-Rank Adaptation (LoRA) (Hu et al., 2022).Specifically, the LoRA configuration employed a rank (r) of 32, along with a scaling factor (lora alpha) of 16.LoRA dropout was set to 0.05.</p>
<p>Training was conducted using a batch size of 8. To efficiently manage GPU memory, gradient checkpointing was enabled.The maximum gradient norm was clipped at 0.3 to ensure training stability.We fine-tuned the model for five epochs using a learning rate of 2e-4, coupled with a cosine learning rate scheduler and a warm-up ratio of 0.05.Additionally, training with BF16 was utilized to enhance training efficiency.One A100 GPU was used in our experiment.</p>
<p>Results and Analysis</p>
<p>In this section, we will be answering the following research questions introduced in the Introduction with the experiment results analysis.</p>
<p>• RQ 1: Will self-improvement lead to LLM self-bias in confidence estimation?</p>
<p>• RQ 2: What are the compounded effects of marrying calibration and selfimprovement on model performance?</p>
<p>RQ 1: Will self-improvement lead to bias in confidence estimation?</p>
<p>As seen in Figure 2, the two upper charts are the accuracy scores of Llama-deepseek and Llama, on the left and right respectively.Similarly, the two bottom charts are the ECE scores.</p>
<p>Longer CoT generally enhances model accuracy but model's inherent reasoning capacity can modulate its effectiveness.Longer CoT consistently yielded the highest accuracy across both Llama-deepseek and Llama, with Llama-deepseek demonstrating a clear trend of progressive self-improvement over multiple rounds (Jin et al., 2024).While Llama's accuracy eventually declined after several rounds of self-improvement using a longer CoT, it still outperformed other self-improvement methods within Llama.Notably, the CoT methods with 512 tokens in Llama experienced a late drop in accuracy due to the 4096token context limitation.Moreover, CoT length significantly influenced inference accuracy: longer CoT (512-token limit) reliably produced higher accuracy compared to shorter CoT (128-token limit).Interestingly, for Llama-the weaker of the two base models-shorter CoT sequences provide a moderate boost, suggesting limited CoT can still benefit models of relatively constrained reasoning capabilities.</p>
<p>The effectiveness of prompting-based methods appears to be strongly influenced by the model's intrinsic reasoning capabilities.In Llama-deepseek, while basic prompting Note.Basic means the basic prompting method and cot is for CoT prompting with different length of tokens.Tuned stands for the fine-tuned method experienced a slight decline in accuracy initially, it subsequently facilitated continuous error correction and progressive improvement.By contrast, Llama exhibited a general deterioration in ACC as the number of prompting rounds increased, with basic prompting ultimately yielding the lowest accuracy among all tested self-improvement strategies.</p>
<p>SFT may be more beneficial for weaker models.Our fine-tuning experiments revealed divergent outcomes in these two models.In Llama-Deepseek, both CoT with 128 and 512 tokens exhibited lower ACC than the original basic prompting method and the fine-tuning ones.Notably, fine-tuned basic prompting resulted in the poorest ACC among all conditions, with one of the highest ECE.This indicates that calibration worsened for Llama-deepseek post-fine-tuning.In Llama, however, fine-tuning produced improvements: both fine-tuned methods surpassed the original basic prompting in terms of ACC, and their ECE also improved, suggesting better calibration.The performance drop in Llama-deepseek after fine-tuning may stem from a mismatch between the fine-tuning dataset and the reasoning dataset, thereby causing noticeable degradation.As Llama-deepseek possesses stronger inherent reasoning capabilities, it may be ill-suited to the chosen dataset and approach.In contrast, Llama, with weaker intrinsic reasoning ability, appears to benefit from fine-tuning, which leads to more pronounced gains.</p>
<p>In addition, Llama-deepseek exhibited a substantially higher initial ECE than Llama and maintained an high level in all self-improvement experiments, suggesting poor calibration under iterative improvement.In contrast, although basic prompting yielded the highest ECE among Llama's self-improvement methods, its ECE does not exceed 0.7-lower than that of Llama-deepseek-indicating that Llama remains inherently better calibrated than Llama-deepseek.Moreover, ECE values tended to be lower when CoT reasoning was applied, particularly in Llama.To further investigate these findings, we propose conducting confidence distribution bias experiments to compare predicted confidence versus actual accuracy across various confidence intervals (e.g., 0.1-0.2,0.2-0.3,etc.) under both basic and CoT prompting for the two LLMs.</p>
<p>Self-Bias in Confidence Estimation.We use the longer CoT and basic prompting to illustrate self-improvement performance at the initial and intermediate stages.The x-axis in Figure 3 represents confidence levels divided into ten bins, while the y-axis denotes the corresponding accuracy.</p>
<p>We can observe that in Llama-deepseek, there is no substantial calibration improvement from the first to the fifth round; rather, the performance appears to deteriorate.In particular, the accuracy of high-confidence predictions decreases, suggesting that self-improvement might have exacerbated overconfidence in certain areas.Furthermore, during Llama-deepseek's self-improvement with basic prompting, a notable dip in confidence occurs in the 0.2-0.3interval.</p>
<p>Figure 4: Llama's accuracy and confidence distribution.</p>
<p>In Figure 4, we observe that in the initial round for Llama, the relationship between confidence and accuracy generally aligns with expectations: as confidence increases, accuracy also improves.However, in the high-confidence region, the model exhibits a tendency toward overconfidence, characterized by high confidence yet relatively lower accuracy.By the fifth round of self-improvement, this issue becomes more pronounced, exacerbating the overconfidence effect.Similarly, during the fifth round of CoT, we observe a sudden rise in accuracy within the 0.3-0.4confidence range.Based on these, we thus can conclude that prompting and fine-tuning based methods in iterative self-improvement can introduce or amplify self-biases in confidence estimation.</p>
<p>RQ 2: What are the compounded effects of marrying calibration and self-improvement on model performance?</p>
<p>As calibration serves as an effective technique to align a model's confidence with its correctness and thus improve models confidence estimation, we propose three experiments using the basic prompting approach to investigate the RQ2.The results highlight notable commonalities and distinctions between the LLMs, as shown in Figure 5.</p>
<p>ECE can be diminished when combined with self-improvement after calibration.Multi self-improvement-then-calibration methods yield reduced ECE, with the latter achieving a markedly lower ECE compared to the other two approaches.Despite performing calibration after each round, the iterative method continues to exhibit relatively high ECE, possibly because the alternating introduction of self-improvement dilutes the calibration effect and consequently compromises alignment between confidence and accuracy.Furthermore, both Note.Cms means calibration then multi self-improvement and msc is multi self-improvement then calibration.Ics stands for iterative calibration and self-improvement the "calibration then multi self-improvement" and "iterative" methods produce relatively high ECE-particularly in Llama, where ECE increases substantially compared to selfimprovement alone.One explanation for this phenomenon is that calibration is primarily intended to align the model's confidence with its actual accuracy.However, during selfimprovement, the model refines its responses based on self-generated feedback, which can shift its confidence distribution.As a result, the self-bias dominates over calibration effect when the calibration is performed at the beginning.</p>
<p>Calibration can serve as a better foundation in self-improvement for stronger LLM.</p>
<p>The "calibration then multi self-improvement" strategy in Llama-deepseek shows steady improvement of ACC, surpassing the performance of Llama-deepseek's longer CoT in pure self-improvement setting.Additionally, unlike basic prompting-based self-improvement, this method does not exhibit an initial accuracy drop.In Llama-deepseek, the multi selfimprovement-then-calibration approach effectively rectifies errors in earlier stages while maintaining a relatively stable ECE; however, it manifests some fluctuations of ACC in later stages, suggesting a pronounced impact on model reasoning ability.Meanwhile, for Llama, the iterative method achieves the highest ACC across multiple rounds, although the overall trend still declines, reinforcing the notion that calibration is beneficial for selfimprovement but Llama's comparatively weaker intrinsic reasoning limits its capacity for effective self-correction.</p>
<p>Conclusion</p>
<p>In this work, we study the effect of self-improving LLM from a calibration perspective.The first research question we propose is will self-improvement leads to self-bias in confidence estimation.Based on our experiment results on three mainstream self-improvement approaches, we reveal an obvious trend of increasing overconfidence as self-improvement iterations progress, leading to a large ECE score value after the self-improvement process.This motivates our second research question on how to marry calibration with selfimprovement to mitigate this overconfidence.With several potential solutions proposed and analyzed, we conclude that ECE can be largely diminished when applying calibration after self-improvement.In the future, we will explore the calibration of self-improving LLMs in larger sizes of LLMs, as well as use a wider spectrum of calibration methods to validate the robustness and generalization of our findings.Besides, investigating self-improvement and calibration in multilingual or multimodal settings would provide a richer understanding of how overconfidence manifests in more complex scenarios.</p>
<p>Figure 2 :
2
Figure 2: Results of Self-Improvement in Different Methods.</p>
<p>Figure 3 :
3
Figure 3: Llama-deepSeek's accuracy and confidence distribution.</p>
<p>Figure 5 :
5
Figure 5: Self-Improve and Calibration Relationship Experiment Result.</p>
<p>Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. Afra Feyza Aky Ürek, Ekin Aky Ürek, Ashwin Kalyan, Peter Clark, Derry Tanti Wijaya, Niket Tandon, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>The internal state of an llm knows when it's lying. Amos Azaria, Tom Mitchell, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Learning from natural language feedback. Chen, J A Scheurer, Campos, Korbak, Chan, Bowman, Cho, Perez, Transactions on machine learning research. 2024</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, arXiv:2304.051282023arXiv preprint</p>
<p>Chain-of-verification reduces hallucination in large language models. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason E Weston, ICLR 2024 Workshop on Reliable and Responsible Foundation Models. </p>
<p>How abilities in large language models are affected by supervised fine-tuning data composition. Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, Jingren Zhou, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics2024a1</p>
<p>Threshold filtering packing for supervised fine-tuning: Training related samples within packs. Jiancheng Dong, Lei Jiang, Wei Jin, Lu Cheng, arXiv:2408.093272024barXiv preprint</p>
<p>Baldur: Whole-proof generation and repair with large language models. Emily First, Markus N Rabe, Talia Ringer, Yuriy Brun, Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2023</p>
<p>Multiple choice questions: Reasoning makes large language models (llms) more self-confident even when they are wrong. Tairan Fu, Javier Conde, Gonzalo Martínez, María Grandury, Pedro Reviriego, arXiv:2501.097752025arXiv preprint</p>
<p>A survey of confidence estimation and calibration in large language models. Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, Iryna Gurevych, arXiv:2311.082982023arXiv preprint</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023arXiv preprint</p>
<p>On calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, International conference on machine learning. PMLR2017</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Small language model can self-correct. Haixia Han, Jiaqing Liang, Jie Shi, Qianyu He, Yanghua Xiao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, ICLR. 1232022</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, arXiv:2310.017982023aarXiv preprint</p>
<p>On the trustworthiness of generative foundation models: Guideline. Yue Huang, Chujie Gao, Siyuan Wu, Haoran Wang, Xiangqi Wang, Yujun Zhou, Yanbo Wang, Jiayi Ye, Jiawen Shi, Qihui Zhang, arXiv:2502.142962025arXiv preprint</p>
<p>Look before you leap: An exploratory study of uncertainty measurement for large language models. Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu, Lei Ma, arXiv:2307.102362023barXiv preprint</p>
<p>Arman Cohan, and Bhuwan Dhingra. Calibrating long-form generations from large language models. Yukun Huang, Yixin Liu, Raghuveer Thirukovalluru, arXiv:2402.065442024arXiv preprint</p>
<p>Calibrated decision-making through llm-assisted retrieval. Chaeyun Jang, Hyungi Lee, Seanie Lee, Juho Lee, arXiv:2411.088912024arXiv preprint</p>
<p>The impact of reasoning step length on large language models. Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, Findings of the Association for Computational Linguistics ACL 2024. 2024</p>
<p>When can llms actually correct their own mistakes? a critical survey of self-correction of llms. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, Rui Zhang, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, Advances in Neural Information Processing Systems. 202336</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, NeurIPS ML Safety Workshop. </p>
<p>From generation to judgment: Opportunities and challenges of llm-as-a-judge. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, arXiv:2411.165942024aarXiv preprint</p>
<p>Smoa: Improving multi-agent large language models with sparse mixture-of-agents. Dawei Li, Zhen Tan, Peijia Qian, Yifan Li, Satvik Kumar, Lijie Chaudhary, Jiayi Hu, Shen, arXiv:2411.032842024barXiv preprint</p>
<p>Dalk: Dynamic co-augmentation of llms and kg to answer alzheimer's disease questions with scientific literature. Dawei Li, Shu Yang, Zhen Tan, Jae Baik, Sukwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, Ying Ding, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024c</p>
<p>Preference leakage: A contamination problem in llm-as-a-judge. Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, Huan Liu, arXiv:2502.015342025aarXiv preprint</p>
<p>Inference-time intervention: Eliciting truthful answers from a language model. Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, Advances in Neural Information Processing Systems. 362023</p>
<p>Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, arXiv:2502.17419From system 1 to system 2: A survey of reasoning large language models. 2025barXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202336</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse automated correction strategies. Jeevana Theo X Olausson, Chenglong Priya Inala, Jianfeng Wang, Armando Gao, Michael Solar-Lezama ; Liangming Pan, Wenda Saxon, Deepak Xu, Xinyi Nathani, William Wang, Wang Yang, arXiv:2306.098962023. 2024Transactions of the Association for Computational Linguistics12arXiv preprintIs self-repair a silver bullet for code generation?</p>
<p>Max Peeperkorn, Tom Kouwenhoven, Dan Brown, Anna Jordanous, arXiv:2405.00492Is temperature the creativity parameter of large language models?. 2024arXiv preprint</p>
<p>Thermometer: Towards universal calibration for large language models. Maohao Shen, Subhro Das, Kristjan Greenewald, Prasanna Sattigeri, Gregory W Wornell, Soumya Ghosh, Forty-first International Conference on Machine Learning. 2024</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>Llamas know what gpts don't show: Surrogate models for confidence estimation. Vaishnavi Shrivastava, Percy Liang, Ananya Kumar, arXiv:2311.088772023arXiv preprint</p>
<p>Shane Storks, Qiaozi Gao, Joyce Y Chai, arXiv:1904.01172Commonsense reasoning for natural language understanding: A survey of benchmarks, resources, and approaches. 2019arXiv preprint</p>
<p>Api is enough: Conformal prediction for large language models without logit-access. Jiayuan Su, Jing Luo, Hongwei Wang, Lu Cheng, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, arXiv:2401.05561Trustworthiness in large language models. 20243arXiv preprint</p>
<p>Large language models for data annotation and synthesis: A survey. Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Large language models in medicine. Arun James Thirunavukarasu, Darren Shu, Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, Daniel Shu, Wei Ting, Nature medicine. 2982023</p>
<p>Can llms learn from previous mistakes? investigating llms' errors to boost for reasoning. Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, Jingbo Shang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Self-preference bias in llm-as-a-judge. Koki Wataoka, Tsubasa Takahashi, Ryokan Ri, arXiv:2410.218192024arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>A survey of joint intent detection and slot filling models in natural language understanding. Henry Weld, Xiaoqi Huang, Siqu Long, Josiah Poon, Caren Soyeon, Han, ACM Computing Surveys. 5582022</p>
<p>Calibration tests beyond classification. David Widmann, Fredrik Lindsten, Dave Zachariah, International Conference on Learning Representations, Virtual conference. May 3-May 7, 2021. 2021International Conference on Learning Representations, ICLR</p>
<p>Progress or regress? self-improvement reversal in post-training. Ting Wu, Xuefeng Li, Pengfei Liu, arXiv:2407.050132024arXiv preprint</p>
<p>Calibrating language models with adaptive temperature scaling. Johnathan Xie, Annie Chen, Yoonho Lee, Eric Mitchell, Chelsea Finn, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024a</p>
<p>A survey of calibration process for black-box llms. Liangru Xie, Hui Liu, Jingying Zeng, Xianfeng Tang, Yan Han, Chen Luo, Jing Huang, Zhen Li, Suhang Wang, Qi He, arXiv:2412.127672024barXiv preprint</p>
<p>Teaching language models to critique via reinforcement learning. Zhihui Xie, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong, arXiv:2502.034922025arXiv preprint</p>
<p>Sayself: Teaching llms to express confidence with self-reflective rationales. Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024a</p>
<p>Pride and prejudice: Llm amplifies self-bias in self-refinement. Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, William Wang, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics2024b1</p>
<p>Understanding the dark side of llms' intrinsic self-correction. Qingjie Zhang, Han Qiu, Di Wang, Haoting Qian, Yiming Li, Tianwei Zhang, Minlie Huang, arXiv:2412.149592024aarXiv preprint</p>
<p>Small language models need strong verifiers to selfcorrect reasoning. Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, Lu Wang, arXiv:2404.171402024barXiv preprint</p>
<p>Verify-andedit: A knowledge-enhanced chain-of-thought framework. Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, Lidong Bing, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, arXiv:2503.085692025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4864 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4864</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4864</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-587352c3b95c90de6d37f061c8e117f42be0b575</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/587352c3b95c90de6d37f061c8e117f42be0b575" target="_blank">Building Cooperative Embodied Agents Modularly with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication, and it is discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans.</p>
                <p><strong>Paper Abstract:</strong> In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4864.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4864.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoELA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cooperative Embodied Language Agent (CoELA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular cognitive-inspired embodied agent architecture that uses LLMs for high-level planning and free-form message generation, integrated with Perception, Memory (semantic/episodic/procedural), Communication, Planning, and Execution modules to solve long-horizon multi-agent cooperative embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CoELA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A modular embodied agent whose Communication and Planning modules are implemented by prompting large language models; Perception produces semantic maps, Memory stores semantic/episodic/procedural information, and Execution converts LLM high-level plans into low-level actions via procedural code.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (primary); experiments also with GPT-3.5, LLaMA-2-13b-chat, and a LoRA-fine-tuned CoLLAMA</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-agent cooperative embodied tasks: transport target objects using containers within time/horizon limits (TDW-MAT) and household multi-subgoal tasks represented as predicates (C-WAH); agents must perceive partial observations, plan, decide when/what to communicate (costly), and execute low-level actions to satisfy multiple subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Three-part long-term memory: Semantic Memory, Episodic Memory, Procedural Memory (explicit external memory outside the LLM context window)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Semantic Memory: top-down semantic map, task progress, self-state and others' states; Episodic Memory: action history and dialogue history (keeps last K actions and last D dialogues); Procedural Memory: code and procedures for executing high-level plans and model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Semantic memory updated whenever a new observation is perceived; Episodic memory appended whenever the agent executes an action or sends/receives a message (stores last K/D entries for efficiency); Procedural memory largely static and only updated via fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Explicit retrieval from Memory Module and conversion to textual descriptions via templates; retrieved text is concatenated into LLM prompts (Instruction Head, Goal Description, State Description, Action History, Dialogue History) used by the Communication and Planning LLMs (a retrieval-augmented prompting pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative/relative: Agents with the Memory Module substantially outperform memory-ablated agents; paper reports that removing the Memory Module nearly doubled the number of steps required to finish tasks (exact numeric metrics for this ablation are not reported). CoELA (with memory & GPT-4) achieved large efficiency improvements over baselines on TDW-MAT and C-WAH (e.g., Transport Rate improvements given in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Qualitative description: Removing the Memory Module caused severe degradation—steps to finish tasks nearly doubled; exact quantitative metrics for the memory-ablated condition are not provided in the paper (no numeric TR or step counts reported specifically for memory ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>The paper ablates key components: (1) Memory Module removal — large negative impact (nearly 2x steps); (2) Execution Module removal — caused slow inference and poor task performance; (3) Communication disabled — among AI agents this did not cause a large drop (communication is costly and hard to do efficiently), but disabling communication hurts human–agent cooperation; (4) LLM strength — replacing GPT-4 with weaker LLMs (GPT-3.5, LLaMA-2) degrades reasoning, planning, and messaging quality; fine-tuned CoLLAMA recovers some performance. These analyses show memory is critical and that memory + strong LLM produces best results.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Memory inaccuracies: semantic memory can become stale because other agents may change object states without observation, causing mismatches between memory and reality; limited incorporation of 3D spatial information into text prompts (spatial detail not effectively encoded for LLMs), leading to semantically plausible but time-inefficient plans; LLM reasoning errors (e.g., miscounting unsatisfied goals or incorrectly reasoning about others' concurrent actions) can combine with stale memory to produce failures.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Design a modular explicit memory (semantic/episodic/procedural) external to the LLM and retrieve concise, templated textual summaries to condition the LLM; update semantic memory each perception and episodic memory after each action/message; store only a bounded recent history (last K actions, last D dialogues) to balance prompt length and relevance; use seed messages and a prompt note to constrain free-form communication; offload low-level control to a procedural Execution Module (so LLMs focus on high-level planning); prefer stronger LLMs or fine-tune open models (e.g., CoLLAMA) for better reasoning and memory conditioning; use chain-of-thought prompting to encourage reasoning steps. Also, future directions: multimodal LLMs to better encode 3D spatial information into prompts and mechanisms to reconcile stale memory with incoming communications/observations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Building Cooperative Embodied Agents Modularly with Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4864.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4864.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents (Park et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents: Interactive Simulacra of Human Behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work that builds agent societies using LLMs augmented with memories to simulate human-like behavior and long-term interactions; mentioned as related work on memory-augmented LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents (Park et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-driven multi-agent simulation where agents are augmented with memory systems to store experiences and drive behavior; cited as prior work using memory augmentation with LLMs to model human behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Simulated multi-agent human-like interactions and behavior over time (not an embodied text game in this paper's experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Memory-augmented LLM agents (external memory to store experiences) — described in the referenced work but details not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Not specified in this paper (referenced work used stored experiences/memories to drive behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Not specified here (referenced as using augmented memories).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Cited as related prior work demonstrating memory augmentation for LLM-driven agents; this paper does not analyze that work's ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not discussed in detail in this paper beyond referencing that Park et al. used memory augmentation to simulate human behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>No explicit recommendations beyond noting Park et al. as an example of memory-augmented LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Building Cooperative Embodied Agents Modularly with Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4864.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4864.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inner Monologue (Huang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inner Monologue: Embodied reasoning through planning with language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned prior work that uses an internal monologue (a form of intermediate textual reasoning/memory) with environment feedback to improve planning for embodied agents driven by LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inner monologue: Embodied reasoning through planning with language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Inner Monologue agent (Huang et al. 2022b)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An approach that augments an LLM with an internal monologue (textual chain-of-thought / scratchpad) conditioned on environment feedback to improve stepwise planning in embodied tasks; cited as related work using internal textual state to assist LLM planning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Embodied instruction-following/planning tasks where the model maintains an inner monologue to incorporate environment feedback into planning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Inner monologue / scratchpad style memory (textual intermediate reasoning kept across steps)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Textual inner monologue and environment feedback summaries (as reported in the cited work; details are not expanded in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Updated with environment feedback each planning/observation step (as described in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Used as context for subsequent LLM promptings (scratchpad-style usage); exact mechanism not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Mentioned as prior evidence that internal textual state / monologues can improve planning; no experiments in this paper directly compare variants.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not discussed in detail here other than general limitations of LLM planning noted by the authors (e.g., low-level control challenges, reasoning errors).</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Paper cites inner monologue work as inspiration; recommends structured retrieval and templating of memory into prompts for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Building Cooperative Embodied Agents Modularly with Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4864.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4864.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code-as-Policies (Liang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Code as Policies: Language model programs for embodied control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior work that uses code / programmatic outputs from LLMs to produce policies for embodied control, referenced in relation to LLM-driven planning for agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Code as policies: Language model programs for embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Code-as-Policies agent (Liang et al. 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Approach where LLMs generate code-like plans or programs which are executed as policies for embodied control; referenced as an alternative planning technique based on LLM output format.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Embodied control tasks where the LLM outputs structured programs/policies to be executed.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Mentioned as related work; this paper does not analyze its memory strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Referenced as an approach for generating executable high-level plans; the current paper instead uses an explicit Procedural Memory + Execution Module to run high-level plans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Building Cooperative Embodied Agents Modularly with Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative agents: Interactive simulacra of human behavior. <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models. <em>(Rating: 2)</em></li>
                <li>Code as policies: Language model programs for embodied control. <em>(Rating: 1)</em></li>
                <li>Language models are zero-shot reasoners. <em>(Rating: 1)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4864",
    "paper_id": "paper-587352c3b95c90de6d37f061c8e117f42be0b575",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "CoELA",
            "name_full": "Cooperative Embodied Language Agent (CoELA)",
            "brief_description": "A modular cognitive-inspired embodied agent architecture that uses LLMs for high-level planning and free-form message generation, integrated with Perception, Memory (semantic/episodic/procedural), Communication, Planning, and Execution modules to solve long-horizon multi-agent cooperative embodied tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CoELA",
            "agent_description": "A modular embodied agent whose Communication and Planning modules are implemented by prompting large language models; Perception produces semantic maps, Memory stores semantic/episodic/procedural information, and Execution converts LLM high-level plans into low-level actions via procedural code.",
            "llm_model_name": "GPT-4 (primary); experiments also with GPT-3.5, LLaMA-2-13b-chat, and a LoRA-fine-tuned CoLLAMA",
            "game_or_benchmark_name": "ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH)",
            "task_description": "Multi-agent cooperative embodied tasks: transport target objects using containers within time/horizon limits (TDW-MAT) and household multi-subgoal tasks represented as predicates (C-WAH); agents must perceive partial observations, plan, decide when/what to communicate (costly), and execute low-level actions to satisfy multiple subgoals.",
            "memory_used": true,
            "memory_type": "Three-part long-term memory: Semantic Memory, Episodic Memory, Procedural Memory (explicit external memory outside the LLM context window)",
            "memory_representation": "Semantic Memory: top-down semantic map, task progress, self-state and others' states; Episodic Memory: action history and dialogue history (keeps last K actions and last D dialogues); Procedural Memory: code and procedures for executing high-level plans and model parameters.",
            "memory_update_mechanism": "Semantic memory updated whenever a new observation is perceived; Episodic memory appended whenever the agent executes an action or sends/receives a message (stores last K/D entries for efficiency); Procedural memory largely static and only updated via fine-tuning.",
            "memory_retrieval_mechanism": "Explicit retrieval from Memory Module and conversion to textual descriptions via templates; retrieved text is concatenated into LLM prompts (Instruction Head, Goal Description, State Description, Action History, Dialogue History) used by the Communication and Planning LLMs (a retrieval-augmented prompting pipeline).",
            "performance_with_memory": "Qualitative/relative: Agents with the Memory Module substantially outperform memory-ablated agents; paper reports that removing the Memory Module nearly doubled the number of steps required to finish tasks (exact numeric metrics for this ablation are not reported). CoELA (with memory & GPT-4) achieved large efficiency improvements over baselines on TDW-MAT and C-WAH (e.g., Transport Rate improvements given in Table 1).",
            "performance_without_memory": "Qualitative description: Removing the Memory Module caused severe degradation—steps to finish tasks nearly doubled; exact quantitative metrics for the memory-ablated condition are not provided in the paper (no numeric TR or step counts reported specifically for memory ablation).",
            "has_performance_comparison": true,
            "ablation_or_analysis": "The paper ablates key components: (1) Memory Module removal — large negative impact (nearly 2x steps); (2) Execution Module removal — caused slow inference and poor task performance; (3) Communication disabled — among AI agents this did not cause a large drop (communication is costly and hard to do efficiently), but disabling communication hurts human–agent cooperation; (4) LLM strength — replacing GPT-4 with weaker LLMs (GPT-3.5, LLaMA-2) degrades reasoning, planning, and messaging quality; fine-tuned CoLLAMA recovers some performance. These analyses show memory is critical and that memory + strong LLM produces best results.",
            "challenges_or_limitations": "Memory inaccuracies: semantic memory can become stale because other agents may change object states without observation, causing mismatches between memory and reality; limited incorporation of 3D spatial information into text prompts (spatial detail not effectively encoded for LLMs), leading to semantically plausible but time-inefficient plans; LLM reasoning errors (e.g., miscounting unsatisfied goals or incorrectly reasoning about others' concurrent actions) can combine with stale memory to produce failures.",
            "best_practices_or_recommendations": "Design a modular explicit memory (semantic/episodic/procedural) external to the LLM and retrieve concise, templated textual summaries to condition the LLM; update semantic memory each perception and episodic memory after each action/message; store only a bounded recent history (last K actions, last D dialogues) to balance prompt length and relevance; use seed messages and a prompt note to constrain free-form communication; offload low-level control to a procedural Execution Module (so LLMs focus on high-level planning); prefer stronger LLMs or fine-tune open models (e.g., CoLLAMA) for better reasoning and memory conditioning; use chain-of-thought prompting to encourage reasoning steps. Also, future directions: multimodal LLMs to better encode 3D spatial information into prompts and mechanisms to reconcile stale memory with incoming communications/observations.",
            "uuid": "e4864.0",
            "source_info": {
                "paper_title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Generative Agents (Park et al.)",
            "name_full": "Generative Agents: Interactive Simulacra of Human Behavior",
            "brief_description": "Referenced work that builds agent societies using LLMs augmented with memories to simulate human-like behavior and long-term interactions; mentioned as related work on memory-augmented LLM agents.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior.",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents (Park et al. 2023)",
            "agent_description": "An LLM-driven multi-agent simulation where agents are augmented with memory systems to store experiences and drive behavior; cited as prior work using memory augmentation with LLMs to model human behavior.",
            "llm_model_name": "",
            "game_or_benchmark_name": "",
            "task_description": "Simulated multi-agent human-like interactions and behavior over time (not an embodied text game in this paper's experiments).",
            "memory_used": true,
            "memory_type": "Memory-augmented LLM agents (external memory to store experiences) — described in the referenced work but details not specified in this paper.",
            "memory_representation": "Not specified in this paper (referenced work used stored experiences/memories to drive behavior).",
            "memory_update_mechanism": "Not specified here (referenced as using augmented memories).",
            "memory_retrieval_mechanism": "Not specified in this paper.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "Cited as related prior work demonstrating memory augmentation for LLM-driven agents; this paper does not analyze that work's ablations.",
            "challenges_or_limitations": "Not discussed in detail in this paper beyond referencing that Park et al. used memory augmentation to simulate human behavior.",
            "best_practices_or_recommendations": "No explicit recommendations beyond noting Park et al. as an example of memory-augmented LLM agents.",
            "uuid": "e4864.1",
            "source_info": {
                "paper_title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Inner Monologue (Huang et al.)",
            "name_full": "Inner Monologue: Embodied reasoning through planning with language models",
            "brief_description": "Mentioned prior work that uses an internal monologue (a form of intermediate textual reasoning/memory) with environment feedback to improve planning for embodied agents driven by LLMs.",
            "citation_title": "Inner monologue: Embodied reasoning through planning with language models.",
            "mention_or_use": "mention",
            "agent_name": "Inner Monologue agent (Huang et al. 2022b)",
            "agent_description": "An approach that augments an LLM with an internal monologue (textual chain-of-thought / scratchpad) conditioned on environment feedback to improve stepwise planning in embodied tasks; cited as related work using internal textual state to assist LLM planning.",
            "llm_model_name": "",
            "game_or_benchmark_name": "",
            "task_description": "Embodied instruction-following/planning tasks where the model maintains an inner monologue to incorporate environment feedback into planning.",
            "memory_used": true,
            "memory_type": "Inner monologue / scratchpad style memory (textual intermediate reasoning kept across steps)",
            "memory_representation": "Textual inner monologue and environment feedback summaries (as reported in the cited work; details are not expanded in this paper).",
            "memory_update_mechanism": "Updated with environment feedback each planning/observation step (as described in cited work).",
            "memory_retrieval_mechanism": "Used as context for subsequent LLM promptings (scratchpad-style usage); exact mechanism not detailed here.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "Mentioned as prior evidence that internal textual state / monologues can improve planning; no experiments in this paper directly compare variants.",
            "challenges_or_limitations": "Not discussed in detail here other than general limitations of LLM planning noted by the authors (e.g., low-level control challenges, reasoning errors).",
            "best_practices_or_recommendations": "Paper cites inner monologue work as inspiration; recommends structured retrieval and templating of memory into prompts for LLMs.",
            "uuid": "e4864.2",
            "source_info": {
                "paper_title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Code-as-Policies (Liang et al.)",
            "name_full": "Code as Policies: Language model programs for embodied control",
            "brief_description": "Cited prior work that uses code / programmatic outputs from LLMs to produce policies for embodied control, referenced in relation to LLM-driven planning for agents.",
            "citation_title": "Code as policies: Language model programs for embodied control.",
            "mention_or_use": "mention",
            "agent_name": "Code-as-Policies agent (Liang et al. 2022)",
            "agent_description": "Approach where LLMs generate code-like plans or programs which are executed as policies for embodied control; referenced as an alternative planning technique based on LLM output format.",
            "llm_model_name": "",
            "game_or_benchmark_name": "",
            "task_description": "Embodied control tasks where the LLM outputs structured programs/policies to be executed.",
            "memory_used": null,
            "memory_type": "",
            "memory_representation": "",
            "memory_update_mechanism": "",
            "memory_retrieval_mechanism": "",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "ablation_or_analysis": "Mentioned as related work; this paper does not analyze its memory strategies.",
            "challenges_or_limitations": "Not discussed here.",
            "best_practices_or_recommendations": "Referenced as an approach for generating executable high-level plans; the current paper instead uses an explicit Procedural Memory + Execution Module to run high-level plans.",
            "uuid": "e4864.3",
            "source_info": {
                "paper_title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior.",
            "rating": 2
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models.",
            "rating": 2
        },
        {
            "paper_title": "Code as policies: Language model programs for embodied control.",
            "rating": 1
        },
        {
            "paper_title": "Language models are zero-shot reasoners.",
            "rating": 1
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances.",
            "rating": 1
        }
    ],
    "cost": 0.015051249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Building Cooperative Embodied Agents Modularly with Large Language Models</h1>
<p>Hongxin Zhang ${ }^{1 <em>}$, Weihua Du ${ }^{2 </em>}$, Jiaming Shan ${ }^{3}$, Qinhong Zhou ${ }^{1}$<br>Yilun Du ${ }^{4}$, Joshua B. Tenenbaum ${ }^{4}$, Tianmin Shu ${ }^{4}$, Chuang Gan ${ }^{1,5}$<br>${ }^{1}$ University of Massachusetts Amherst, ${ }^{2}$ Tsinghua University,<br>${ }^{3}$ Shanghai Jiao Tong University, ${ }^{4}$ MIT, ${ }^{5}$ MIT-IBM Watson AI Lab</p>
<h4>Abstract</h4>
<p>In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multiobjective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on CWAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoLLAMA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.</p>
<h2>1 INTRODUCTION</h2>
<p>Humans are adept at cooperating and communicating with others when solving complex tasks (Woolley et al., 2010). Building embodied agents that can also engage in and assist humans in everyday life is a valuable but challenging task, considering the complexity of perception, partial observation, long-horizon planning, natural language communication, and so on (Deitke et al., 2022).
Large Language Models (LLMs) have exhibited remarkable capabilities across various domains, implying their mastery of natural language understanding, dialogue generation, rich world knowledge, and complex reasoning capability (OpenAI, 2023; Touvron et al., 2023; Brown et al., 2020; Bubeck et al., 2023). Recent research has also demonstrated that LLMs can drive embodied agents for single-agent tasks through zero-shot prompting for instruction following tasks (Huang et al., 2022a) or few-shot prompting for more complex long-horizon tasks (Song et al., 2022). However, building cooperative embodied agents to work with other agents or with humans under decentralized settings with costly communication remains challenging and rarely explored, where they also need to have strong abilities for cooperative planning and efficient communication. To date, it still remains unclear whether LLMs have such abilities necessary for distributed embodied multi-agent cooperation.</p>
<p>Therefore, this paper aims to investigate how to leverage LLMs to build cooperative embodied agents that can collaborate and efficiently communicate with other agents and humans to accomplish longhorizon multi-objective tasks in a challenging decentralized setting with costly communication. To this end, we focus on an embodied multi-agent setting as shown in Figure 1, where two decentralized</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A challenging multi-agent cooperation problem with decentralized control, raw sensory observations, costly communication, and long-horizon multi-objective tasks.
embodied agents have to cooperate to finish a multi-objective household task efficiently with complex partial observation given. Specifically, communication in our setting takes time as in real life, so the agents can't simply keep free talking with each other. To succeed in this setting, agents must i) perceive the observation to extract useful information, ii) maintain their memory about the world, the task, and the others, iii) decide what and when to communicate for the best efficiency and iv) plan collaboratively to reach the common goal.</p>
<p>Inspired by prior work in cognitive architectures (Laird, 2019), we present CoELA, a Cooperative Embodied Language Agent, a cognitive architecture with a novel modular framework that utilizes the rich world knowledge, strong reasoning ability and mastery natural language understanding and generation capability of LLMs, who plan and communicate with others to cooperatively solve complex embodied tasks. Our framework consists of five modules, each to address a critical aspect of successful multi-agent cooperation, including a Perception Module to perceive the observation and extract useful information, a Memory Module mimicking human's long-term memory to maintain the agent's understanding of both the physical environment and other agents, a Communication Module to decide what to communicate utilizing the strong dialogue generation and understanding capability of LLMs, a Planning Module to decide high-level plans including when to communicate considering all the information available, and an Execution Module to execute the plan by generating primitive actions using procedures stored in the memory module.</p>
<p>We instantiate our challenging setting and evaluate our framework on two embodied environments: ThreeDWorld Multi-Agent Transport (TDW-MAT) and Communicative Watch-And-Help (C-WAH). Our experimental results indicate that CoELA can perceive complex observations, reason about the world and others' state, communicate efficiently, and make long-horizon plans accordingly, as showcased in Figure 1 where CoELA divide the labor with its partner through natural language communication effectively. In particular, CoELA driven by GPT-4 can outperform strong planningbased baselines by achieving more than $40 \%$ efficiency improvements and exhibiting emergent efficient communication. Though Open LMs like LLAMA-2 still underperform, we utilize parameterefficient fine-tuning techniques LoRA (Hu et al., 2021) to train a CoLLAMA on few data collected with our agents and gain promising performance. In the user study, we also discover that CoELA communicating with humans in natural language can earn more trust. Our contribution includes:</p>
<ul>
<li>We formalized a challenging multi-agent embodied cooperation problem with decentralized control, complex partial observation, costly communication, and long-horizon multi-objective tasks, and instantiated it in two embodied environments: C-WAH and TDW-MAT.</li>
<li>We presented a novel cognitive-inspired modular framework that utilizes the strong planning and communication capability of the LLMs to build cooperative embodied agents CoELA, surpassing strong planning-based methods.</li>
<li>We conducted a user study to evaluate the possibility of achieving effective and trustworthy humanAI cooperation using LLMs.</li>
</ul>
<h1>2 Related Work</h1>
<p>Multi-Agent Cooperation and Communication The field of multi-agent cooperation and communication has a long-standing history (Stone \&amp; Veloso, 2000). Many platforms have been proposed for various multi-agent tasks (Lowe et al., 2017; Resnick et al., 2018; Shu \&amp; Tian, 2018; Jaderberg</p>
<p>et al., 2019; Samvelyan et al., 2019; Suarez et al., 2019; Baker et al., 2019; Bard et al., 2020). Other works focused on methods that improves communication efficiency (Jiang \&amp; Lu, 2018; Das et al., 2019; Wang et al., 2021; Wan et al., 2022), cooperation in visually rich domains (Jain et al., 2020), or grounding communications in environments (Patel et al., 2021; Mandi et al., 2023; Narayan-Chen et al., 2019). For embodied intelligence, Puig et al. (2021; 2023) explored the social perception of the agents during their cooperation. However, these platforms either neglects communication (Jaderberg et al., 2019; Samvelyan et al., 2019; Carroll et al., 2019; Puig et al., 2021; 2023), or use uninterpretable continuous vectors (Jiang \&amp; Lu, 2018; Das et al., 2019) or limited discrete symbols (Lowe et al., 2017; Jaques et al., 2019; Jain et al., 2020; Patel et al., 2021; Resnick et al., 2018) for communication. In contrast, we propose a more challenging setting where no presupposed free communication channel exists, and distributed agents need to use natural language to communicate efficiently with others, especially humans.</p>
<p>Language Agents Recently, numerous studies have explored language agents which use LLMs for sequential decision-making (Yang et al., 2023; Wang et al., 2023b; Xi et al., 2023; Sumers et al., 2023). Although LLMs still face challenges when solving complex reasoning problems (Bubeck et al., 2023), a substantial body of work demonstrates their capacity to make plans (Sharma et al., 2021; Raman et al., 2022; Pallagani et al., 2022; Gramopadhye \&amp; Szafir, 2022; Yuan et al., 2023; Li et al., 2022; Wang et al., 2023d), especially in embodied environments (Li et al., 2023a; Padmakumar et al., 2022; Kolve et al., 2017; Shridhar et al., 2020; Misra et al., 2018; Zhu et al., 2017; Brodeur et al., 2017; Xia et al., 2018; Savva et al., 2019; Xiang et al., 2020; Jain et al., 2020; 2019). Specifically, Liang et al. (2022); Song et al. (2022) used codes or few-shot prompting to directly generate plans, Huang et al. (2022b) built an inner monologue with environment feedback to improve planning, Ahn et al. (2022) combined robotic affordances and LLMs for grounded instruction following. There has also been a line of work utilizing multiple LLMs to cooperate or debate with each other "in mind" to strengthen the single agent's capability to solve complex tasks (Li et al., 2023b; Du et al., 2023; Wang et al., 2023c), different from their "free self-talk" setting, our decentralized language agents must plan about when and what to communicate carefully since it's costly in real-life. More recently, Park et al. (2023) built an agent society using LLMs augmented with memories to simulate human behavior. In contrast to the above, our work addresses a more challenging multi-agent cooperation problem, characterized by decentralized control, complex observations, costly communication, and long-horizon multi-objective tasks. We also study the capability of Open LMs like LLAMA-2 and tine-tune a CoLLAMA using LoRA with data collected by our agents in embodied environments to demonstrate their promising performance for building better cooperative embodied agents.</p>
<h1>3 Cooperative Planning under DEC-POMDP-COM</h1>
<p>Our setting can be defined as an extension of the decentralized partially observable Markov decision process (DEC-POMDP) (Bernstein et al., 2002; Spaan et al., 2006; Goldman \&amp; Zilberstein, 2003), which can be formalized by $\left(n, S,\left{\Sigma_{i}\right},\left{A_{i}\right},\left{O_{i}\right}, T, G, R, \gamma, h\right)$, where $n$ denotes the number of agents; $S$ is a finite set of states; $A_{i}=A_{i}^{W} \cup A_{i}^{C}$ is the action set for agent $i$, including a finite set of world actions $A_{i}^{W}$ and a communication action $A_{i}^{C}$ to send a message $\sigma_{i} \in \Sigma_{i}$; $O_{i}=O_{i}^{W} \times O_{i}^{C}$ is the observation set for agent $i$, including world observations $O_{i}^{W}$ the agent receives through its sensors, and $O_{i}^{C}=\Sigma_{1} \times \cdots \times \Sigma_{n}$ the set of possible messages the agent can receive from any of its teammates; $T\left(s, a, s^{\prime}\right)=p\left(s^{\prime} \mid s, a\right)$ is the joint transition model which defines the probability that after taking joint action $a \in A_{1} \times \cdots \times A_{n}$ in $s \in S$, the new state $s^{\prime} \in S$ is achieved; $G=\left{g_{1}, \cdots, g_{k}\right}$ defines the task with several sub-goals for the agents to finish; $R\left(s, a, s^{\prime}\right)=-c(a)+\sum_{i=1}^{k} \mathbb{1}\left(s^{\prime}=g_{i}\right)-\mathbb{1}\left(s=g_{i}\right)$ is the reward function to the team, where $c(a)$ is the cost for action $a$, and $\mathbb{1}(\cdot)$ checks if the sub-goal $g_{i}$ is satisfied in the world state $s ; \gamma$ is the discount rate and $h$ is the planning horizon. In the remainder of this paper, we focus on noise-free broadcast communication and limit our discussion to two agents, though our methods and experiments are generalizable to more than two agents.</p>
<p>We instantiate the problem with two decentralized intelligent embodied agents (including humans) cooperating to accomplish a long-horizon rearrangement task (Batra et al., 2020) in an indoor multiroom environment. The agents are capable of executing one of the actions from the action space $\mathcal{A}=\mathcal{A}<em _mathrm_INT="\mathrm{INT">{\mathrm{NAV}} \cup \mathcal{A}</em>}} \cup \mathcal{A<em _mathrm_NAV="\mathrm{NAV">{\mathrm{COM}}$, where $\mathcal{A}</em>}}$ includes navigation actions, $\mathcal{A<em _mathrm_COM="\mathrm{COM">{\mathrm{INT}}$ includes interaction actions and $\mathcal{A}</em>$ includes a communication action with which the agent can send a message in natural language to broadcast to others. The rearrangement task is defined with several predicates}</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An overview of CoELA. There are five key modules in our framework: (c) The Communication Module and (d) the Planning Module leverage LLMs to generate messages and make plans, (b) The Memory Module stores the agent's knowledge and experience about the world and others in semantic, episodic and procedural memory respectively, (a) The Perception Module and (e) the Execution Module interact directly with the external environment by perceiving raw observations and generating primitive actions. More design details can be found in Appendix A.
$g_{i}$ with counts to be satisfied, such as ON (plate, dinnertable) : 2 representing a sub-task of putting two plates onto the dinner table.</p>
<h1>4 Building Cooperative Embodied Agents Modularly with LLMs</h1>
<h3>4.1 FRAMEWORK OVERVIEW</h3>
<p>Inspired by the cognitive architectures (Langley et al., 2009; Laird, 2019; 2022), we build CoELA, a Cooperative Embodied Language Agent with novel modular framework integrating the strong reasoning ability and language generation capability of LLMs. As shown in Figure 2, CoELA consists of five key modules: (a) Perception, (b) Memory, (c) Communication, (d) Planning, and (e) Execution. At each interaction step, CoELA first uses (a) Perception Module to perceive the raw sensory observation received from the environment, then updates the (b) Memory Module with extracted new information, which stores its knowledge and experience of the world and others. CoELA tackles the challenge of efficient communication with a two-step method: first decide on what to send, then decide whether to send this message or choose another plan by deliberately using (c) The Communication Module to retrieve related information from (b) and utilize an LLM to generate the best message to send "in mind" beforehand, then leverages (d) the Planning Module driven by LLM with strong reasoning ability to make the decision on which plan to take given the related information retrieved from (b) and available actions proposed regarding the current state. The generated plan is then used to update (b2) the Episodic Memory. Finally, (e) the Execution Module retrieves procedural knowledge stored in (b3) to turn the high-level plan into primitive actions executable in the environment.</p>
<h3>4.2 Perception Module</h3>
<p>For embodied agents to be helpful in the real world, they have to perceive raw observations gained through sensors and extract useful information for downstream higher-order reasoning. We incorporate the Perception Module to deal directly with the complex visual observation received from the environment by training a Mask-RCNN (He et al., 2017) to predict the segmentation masks from the RGB image, then build 3D point clouds using the RGB-D image, extract useful high-level information such as the states of the key objects and build a local semantic map.</p>
<h3>4.3 MEMORY MODULE</h3>
<p>It's of vital importance for an agent to maintain a memory of the knowledge and experience it has of the world and others, we mimic human's long-term memory (Atkinson \&amp; Shiffrin, 1968; Wang \&amp;</p>
<p>Laird, 2006; Nuxoll \&amp; Laird, 2012) and design Semantic memory, Episodic Memory, and Procedural Memory for CoELA.</p>
<p>Semantic Memory stores CoELA's knowledge about the world including a semantic map, the task progress, the state of self, and the state of others. Each time a new observation is received and perceived by the Perception Model, the Semantic Memory is updated accordingly. To be noticed, CoELA's knowledge about the world may not be accurate since other agents may interact with the objects and change their states without its awareness. Dealing with imparities between the memory and the description of the world from others adds even more challenges.</p>
<p>Episodic Memory stores CoELA's experience about the past including the action history and dialogue history. Each time CoELA executes a new action including sending out a message or receiving a new message, the related information is added to the Episodic Memory.</p>
<p>Procedural Memory contains knowledge including how to carry out specific high-level plans in a specific environment implemented in code and the neural models' parameters.</p>
<h1>4.4 Communication Module</h1>
<p>To deal with the what to send problem, we deliberately design a Communication Module utilizing the strong free-form language generation capability of the LLMs to act as a message generator. To better condition the LLMs on the cooperative task and avoid inefficient casual chatting, the Communication Module first retrieves the related information from the Memory Module including the semantic map, task progress, agent state, others state, and the action and dialogue history, then convert these into text descriptions using templates, finally prompt the LLMs with the concatenation of Instruction Head, Goal Description, State Description, Action History, and Dialogue History to generate the message to send. To better constrain LLMs' generated messages, a note at the end of the prompt is added and two seed messages are appended at the beginning of the Dialogue History to elicit deserved effective communication behavior. Detailed prompt design in Appendix. A.3.</p>
<h3>4.5 Planning Module</h3>
<p>CoELA needs a strong Planning Module to make decisions on which action to take utilizing all available information gathered and stored so far to maximize cooperation efficiency. While designing such a module from scratch consumes large human expert efforts and is nearly impossible to generalize, we utilize powerful LLMs directly as the Planning Module by first retrieving the related information from the Memory Module and converting them into text descriptions as in the Communication Module, then compile an Action List of all available high-level plans proposed according to the current state and the procedural knowledge stored for the LLMs to make the choice, which formalization makes it easier for the LLMs to concentrate on the reasoning and make an executable plan without any few-shot demonstrations easily, finally prompting the LLMs with current information and the proposed Action List to generate a high-level plan. We also use the zero-shot chain-of-thought prompting technique introduced by Kojima et al. (2022) to encourage the LLMs to carry out more reasoning before giving the final answer. More details can be found in Appendeix. A.4.</p>
<h3>4.6 Execution Module</h3>
<p>As shown in (Deitke et al., 2022), solving challenging embodied tasks requires modular methods to tackle the complexity of tasks. We found that while LLMs were effective at making high-level plans, they were poor at making low-level controls, as also discussed in (Wu et al., 2023). Thus, to enable effective and generalized cooperation decision-making in different environments, we design an Execution Module to generate primitive actions to execute a given high-level plan robustly in a specific environment, allowing the Planning Module to be generalizable and focus more on solving the overall task with LLMs' rich world knowledge and strong reasoning ability. Practically, this design can also reduce the LLM inference time and is time-saving and economical. CoELA retrieves the procedures in its Memory Module regarding the plan generated by the Planning Module and then carries out the procedure with primitive actions suitable for the environment.</p>
<h2>5 EXPERIMENTS</h2>
<h3>5.1 EXPERIMENTAL SETUP</h3>
<p>ThreeDWorld Multi-Agent Transport (TDW-MAT) is a multi-agent embodied task extended from the ThreeDWorld Transport Challenge (Gan et al., 2022) with more types of objects and containers, more realistic object placements, and communication between agents supported, built on top of the TDW platform (Gan et al., 2021), which is a general-purpose virtual world simulation platform. The agents are tasked to transport as many target objects as possible to the goal position with the help</p>
<p>of containers as tools. The agents receive ego-centric $512 \times 512$ RGB-D images as observation and have an action space of low-level navigation control, interaction, and communication. We selected 6 scenes from the TDW-House dataset and sampled 2 out of the two types of tasks food and stuff in each of the scenes, making a test set of 24 episodes, and instantiate the horizon $h$ with 3000 frames.
Communicative Watch-And-Help (C-WAH) is extended from the Watch-And-Help Challenge (Puig et al., 2021) built on a realistic multi-agent simulation platform, VirtualHome-Social (Puig et al., 2018; 2021), where we focus more on cooperation ability and support communication between agents. We conduct experiments under both symbolic and visual observation settings. The task is defined as five types of common household activities and represented as various predicates with counts to be satisfied. We sampled 2 tasks from each of the five types of activities to construct a test set of 10 episodes and instantiate the horizon $h$ with 250 steps. More details can be found at Appendix. B.
Metrics We use the Transport Rate $(T R)$, the fraction of the sub-goals satisfied on TDW-MAT, and the Average Steps $L$ taken to finish the task on C-WAH as main efficiency metrics respectively and calculate Efficiency Improvement (EI) of cooperating with other agents as $\Delta M / M_{0}$, where $\Delta M$ denotes the main efficiency metric difference, and $M_{0}$ denotes the larger one of the main efficiency metric for numerical stability.</p>
<h1>5.2 BASELINES</h1>
<p>MCTS-based Hierarchical Planner(MHP) is adopted from the strongest baseline in the original Watch-And-Help Challenge, which is a Hierarchical Planner with a high-level planner based on MCTS and a low-level planner based on regression planning (Korf, 1987).
Rule-based Hierarchical Planner(RHP) is adopted from the strong performing baseline in the original ThreeDWorld Transport Challenge, which is a Hierarchical Planner with a high-level planner based on heuristics rules and a low-level A-start-based planner to navigate with semantic map, using Frontier Exploration strategy which randomly samples a way-point from an unexplored area as a sub-goal for exploration.
Multi-Agent Transformer(MAT) is a MARL baseline that applies a centralized decision transformer to generate actions from shared observations (Wen et al., 2022). To apply MAT in our setting, we make the compromise to feed the oracle semantic map and the agent states as observation and stack up to 50 frames as an RL step since TDW-MAT is too hard for it with long-horizon and sparse reward signals. We train MAT on the training set with more details in Appendix. C.1.
Implementation Details. We train a Mask-RCNN on the training set for the Perception Module and instantiate CoELA with the most powerful LLM GPT-4 from the OpenAI API ${ }^{1}$ with the default parameter of temperature 0.7 , top-p 1 , and max tokens 256 unless other stated. We also conduct experiments with Open LLM LLAMA-2-13b-chat (Touvron et al., 2023) and fine-tune a CoLLAMA with LoRA (Hu et al., 2021) on a small set of human-filtered high-quality trajectory data collected with our agents. More details are deferred to the Appendix. C.3.</p>
<h3>5.3 RESULTS</h3>
<h3>5.3.1 Collaborating with AI Agents</h3>
<p>CoELA cooperates better with baseline agent As shown in Table 1, compared with RHP doing the task alone, cooperating with CoELA leads to a higher TR and EI than cooperating with another RHP $(0.69(36 \%)$ v.s. $0.61(29 \%)$ ), even without any knowledge of the inner working mechanism of others, showing CoELA can reason about the other agent's state well without hand-designed heuristics. From Table 2, we can observe the same performance boost of cooperating with CoELA on C-WAH of $45 \%$ compared to $33 \%$ of cooperating with the same MHP.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Symbolic Obs</th>
<th style="text-align: center;">Visual Obs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MHP</td>
<td style="text-align: center;">111</td>
<td style="text-align: center;">141</td>
</tr>
<tr>
<td style="text-align: left;">MHP + MHP</td>
<td style="text-align: center;">$75(\uparrow 33 \%)$</td>
<td style="text-align: center;">$103(\uparrow 26 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">MHP + CoELA</td>
<td style="text-align: center;">$59(\uparrow 45 \%)$</td>
<td style="text-align: center;">$94(\uparrow \mathbf{3 4 \% )}$</td>
</tr>
<tr>
<td style="text-align: left;">CoELA + CoELA</td>
<td style="text-align: center;">$\mathbf{5 7}(\uparrow \mathbf{4 9 \% )}$</td>
<td style="text-align: center;">$\mathbf{9 2}(\uparrow \mathbf{3 4 \% )}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Quantitative results on C-WAH. We report the average steps(Efficiency Improvement) here over 5 runs for MHP and 1 run for CoELA due to cost constraints. The best performance is achieved when cooperating with CoELA.</p>
<p>CoLLAMA is in competence with GPT-4 to drive CoELA Two CoELA cooperate together can further boost the TR to 0.71 and 0.85 on TDW-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">RHP</th>
<th style="text-align: center;">RHP + RHP</th>
<th style="text-align: center;">RHP + CoELA</th>
<th style="text-align: center;">CoELA + CoELA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MAT*</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">LLAMA-2</td>
<td style="text-align: center;">CoLLAMA-2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TDW-MAT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Food</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">$0.67(\uparrow 25 \%)$</td>
<td style="text-align: center;">$0.79(\uparrow \mathbf{3 9 \% )}$</td>
<td style="text-align: center;">$0.82(\uparrow 38 \%)$</td>
<td style="text-align: center;">$0.57(\uparrow 9 \%)$</td>
<td style="text-align: center;">$0.73(\uparrow 33 \%)$</td>
<td style="text-align: center;">$/$</td>
</tr>
<tr>
<td style="text-align: center;">Stuff</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">$0.54(\uparrow 34 \%)$</td>
<td style="text-align: center;">$0.59(\uparrow 34 \%)$</td>
<td style="text-align: center;">$0.61(\uparrow 41 \%)$</td>
<td style="text-align: center;">$0.48(\uparrow 11 \%)$</td>
<td style="text-align: center;">$0.66(\uparrow 44 \%)$</td>
<td style="text-align: center;">$/$</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">$0.61(\uparrow 29 \%)$</td>
<td style="text-align: center;">$0.69(\uparrow 36 \%)$</td>
<td style="text-align: center;">$0.71(\uparrow 39 \%)$</td>
<td style="text-align: center;">$0.53(\uparrow 10 \%)$</td>
<td style="text-align: center;">$0.70(\uparrow 38 \%)$</td>
<td style="text-align: center;">$/$</td>
</tr>
<tr>
<td style="text-align: center;">TDW-MAT w/ Oracle Perception</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Food</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">$0.76(\uparrow 33 \%)$</td>
<td style="text-align: center;">$0.85(\uparrow 40 \%)$</td>
<td style="text-align: center;">$0.87(\uparrow 41 \%)$</td>
<td style="text-align: center;">$0.60(\downarrow 3 \%)$</td>
<td style="text-align: center;">$0.78(\uparrow 34 \%)$</td>
<td style="text-align: center;">$0.13(\downarrow)$</td>
</tr>
<tr>
<td style="text-align: center;">Stuff</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">$0.74(\uparrow 34 \%)$</td>
<td style="text-align: center;">$0.77(\uparrow 35 \%)$</td>
<td style="text-align: center;">$0.83(\uparrow 41 \%)$</td>
<td style="text-align: center;">$0.63(\uparrow 19 \%)$</td>
<td style="text-align: center;">$0.81(\uparrow 38 \%)$</td>
<td style="text-align: center;">$0.17(\downarrow)$</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">$0.75(\uparrow 34 \%)$</td>
<td style="text-align: center;">$0.81(\uparrow 37 \%)$</td>
<td style="text-align: center;">$0.85(\uparrow 41 \%)$</td>
<td style="text-align: center;">$0.62(\uparrow 8 \%)$</td>
<td style="text-align: center;">$0.80(\uparrow 36 \%)$</td>
<td style="text-align: center;">$0.15(\downarrow)$</td>
</tr>
</tbody>
</table>
<p>Table 1: Quantitative results on TDW-MAT. We report the average Transport Rate(Efficiency Improvement) here over 5 runs for RHP and 1 run for CoELA due to cost constraints. *MAT uses central observation and oracle perception. The best results are in bold. The best performance is achieved when cooperating with CoELA.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example cooperative behaviors demonstrating CoELA can communicate effectively and are good cooperators.</p>
<p>MAT without and with Oracle Perception. While replacing GPT-4 with open Model LLAMA-2 leads to a significant performance drop, our fine-tuned CoLLAMA can gain a competitive performance of 0.70 TR and even surpass GPT-4 on the subtask of Stuff where GPT-4 performs not so well, showing the promising future of fine-tuning open LLMs with our proposed framework on embodied environments for even better cooperative embodied agents.</p>
<p>CoELA exhibit efficient communication and effective cooperation behavior To better understand the essential factors for effective cooperation, we conduct a qualitative analysis of the agents' behaviors exhibited in our experiments and identified several cooperative behaviors: CoELA share progress and information with others, know when to request help and can respond to others' requests, can adapt plans considering others and knows when not to communicate, as shown in Figure 3. We discuss some here and the remaining in the Appendix. C.4.</p>
<h1>5.3.2 Collaborating with Humans</h1>
<p>It's our ultimate goal to build agents that can cooperate with humans, a user study is important. We conducted human experiments on the C-WAH where the agent Alice is controlled by real humans.</p>
<p>We recruited 8 human subjects to perform the experiments under four scenarios: cooperating with the $\mathbf{M H P}^{\overline{1}}, \boldsymbol{C o E L A}, \boldsymbol{C o E L A}$ w/o communication, and doing the task alone. Subjects have access to the same observation and action space as the agents, they can click on visible objects and select actions</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Human experiments results (a) The Average steps when collaborating with Humans and agents. (b) Subjective Rating Humans give when cooperating with different agents. Humans trust CoELA communicating in natural language more and cooperate more efficiently with them. Ablation results (c) The light-colored portions represent the number of steps used for communication. The Memory Module and a strong LLM for the Planning Module are important, while the Communication Module matters more when cooperating with humans.
to interact with them, including navigation to each room and communication through a chat box. We gave each subject a tutorial and they had the chance to get familiar with the interface in a few pilot trials. We evaluate the same 10 tasks as in previous experiments and each task was performed by at least 2 subjects, making 80 trials in total. We made sure each subject do 10 trials with at least two trials under each scenario. After each trial including a baseline to cooperate with, we asked subjects to rate the agent they just cooperated with on a 7-point Likert Scale based on three criteria adapted from Puig et al. (2021): (i) How effective do you think of your communication with the other agent Bob? Did it understand your message and/or share useful information with you? (ii) How helpful do you find the other agent Bob? Did it help you achieve the goal faster? (iii) How much do you trust the other agent Bob? Would you feel safe doing the task with it, or you rather do the task alone?
As we can see in Figure 4a, when cooperating with humans, CoELA still performs better than MHP, and when communication is unable, CoELA w/o communication encounters a performance drop. As reported in Figure 4b, we also observe that humans would trust the agents more if they can communicate with humans (trust score of 6.3 v.s. 4.7 for CoELA v.s CoELA w/o communication, $\mathrm{p}=0.0003$ over the t-test), and therefore achieves better cooperation. Compared with MHP using template language to communicate, humans prefer to collaborate with CoELA who communicates in natural language and can understand and respond to Human dialogues. We show an effective communication example in Figure 10, where the human first shares his progress with CoELA and suggests a labor division, CoELA understands and responds with its future plan as well, resulting in a perfect division of the exploration trajectory. These results imply promising futures for leveraging LLMs to build cooperative embodied agents that can successfully work with humans.</p>
<h1>5.4 ANALYSIS</h1>
<p>Do we need a strong LLM for the Planning and Communication Module? As shown in Figure 4c, when we replace GPT-4 with GPT-3.5 to drive CoELA, the agents would need more steps to finish the task. GPT-3.5 makes more reasoning errors about the state and therefore generates more implausible plans, which leads CoELA to spend more time finishing the task. GPT-3.5 also tends to generate unuseful messages more often than GPT-4. The performance gap can be attributed to more advanced reasoning and Theory of Mind abilities of GPT-4, which is also observed by Bubeck et al. (2023).
Is the communication effective? Though communication still fails in some cases, as shown in Figure 3, our agent exhibits effective communication behaviors, such as sharing information, requesting help, responding to requests, and knowing when not to communicate. More importantly, natural language communication provides us with a lens to understand the decision-making of the agents and could lead to better cooperation between humans and AI (as shown in section 5.3.2). We did not observe a significant performance drop when disabling communication among AI agents (as shown in Figure 4c), because carrying out efficient communication in our setting is extremely challenging as communication costs time, requiring agents to model others accurately and understand the ambiguity of the natural language itself, which current LLMs still can not master robustly.
Is the Memory Module and Execution Module effective? As shown in Figure 4c, the steps needed to finish the task for the agent with no Memory Module nearly double, showing the importance of the Memory Module to store and update the knowledge and experience of the scene and the others. We also tried to remove the Execution Module and let the Planning Module make low-level control</p>
<p>directly at every step. However, this slows down the inference process largely and all our trials perform poorly and strueele to finish anv task.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Failure cases in TDW-MAT. (a) The Agent fails to reason the other one is already putting the burger into the container. (b) The LLM counts the number of the remaining target objects wrong.</p>
<h1>5.5 Failure Cases and Limitations of LLM</h1>
<p>Though CoELA built with sota LLMs is effective and has achieved impressive results, we find that the agent still falls short in several essential capabilities. We provide an in-depth analysis of its limitations and share some insights on designing better cooperative embodied agents for future work.
Limited usage of 3D spatial information. CoELA did not incorporate the spatial information of objects and rooms into consideration due to the challenge of effectively introducing the spatial information to pure text language models. This may cause the agents to come up with a semantic sound exploration plan which is actually time-consuming. Work on multi-modal large models capable of both processing visual modalities effectively and generating natural language fluently (Huang et al., 2023; Driess et al., 2023; Lu et al., 2022) would help overcome this limitation and build better grounded embodied agents.
Lack of effective reasoning on low-level actions. To help LLMs better focus on solving the overall task, we abstract high-level plans for LLMs to directly reason on, reducing the potential decision space significantly, but also making it unaware of the execution of low-level actions, and impossible to reason over them, which may lead to plausible but ineffective decisions. For example in Figure 5a, Alice saw Bob holding a container and a target object in both hands and figured he may not know how to utilize the containers, so sent a message to instruct him to put the object into the container, though Bob was actually putting in the objects at the same time, which is impossible for Alice to reason over now. Developing agents that can directly make low-level controls is essential for building better cooperative agents.
Unstable performance on complex reasoning. Although LLMs make correct reasoning most of the time, they still occasionally make mistakes, including misunderstanding the environment rules specified in the prompt, and incorrect reasoning over the number of unsatisfied goals (Figure 5b). These mistakes can cause failures in planning. This calls for developing LLMs with stronger instruction following and reasoning capability.</p>
<h2>6 CONCLUSION</h2>
<p>In this work, we propose a novel modular framework integrating the Large Language Models to build cooperative embodied agents CoELA, who can plan, communicate, and collaborate efficiently with other agents and humans in a challenging multi-agent setting with decentralized control, complex partial observation, costly communication, and multi-objective long-horizon tasks. Our experiments on two extended embodied multi-agent environments show the effectiveness of our proposed framework and exhibit several cooperative behaviors. We fine-tune a CoLLAMA from LLAMA-2 using data collected with our agents in embodied environments and showcase its promising performance to build better cooperative embodied agents. We also discover that CoELA communicating in natural language can cooperate better with humans and earn more trust from them. We believe that our work indicates promising future avenues to design even stronger embodied agents with LLMs for multi-agent cooperation. We further perform an in-depth analysis of the limitations of the current LLMs and highlight several potential solutions for building better embodied cooperative agents for the future.</p>
<h1>ACKNOWLEDGEMENT</h1>
<p>We thank Zishuo Zheng and Zhiqing Sun for their insightful discussions and help with the experiments, Jeremy Schwartz and Esther Alter for setting up ThreeDWorld environments. We thank the anonymous reviewers for their helpful suggestions.</p>
<h1>REFERENCES</h1>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.</p>
<p>Richard C Atkinson and Richard M Shiffrin. Human memory: A proposed system and its control processes. In Psychology of learning and motivation, volume 2, pp. 89-195. Elsevier, 1968.</p>
<p>Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528, 2019 .</p>
<p>Nolan Bard, Jakob N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, et al. The hanabi challenge: A new frontier for ai research. Artificial Intelligence, 280:103216, 2020.</p>
<p>Dhruv Batra, Angel X Chang, Sonia Chernova, Andrew J Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, et al. Rearrangement: A challenge for embodied ai. arXiv preprint arXiv:2011.01975, 2020.</p>
<p>Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of decentralized control of markov decision processes. Mathematics of operations research, 27(4): 819-840, 2002.</p>
<p>Simon Brodeur, Ethan Perez, Ankesh Anand, Florian Golemo, Luca Celotti, Florian Strub, Jean Rouat, Hugo Larochelle, and Aaron Courville. Home: A household multimodal environment. arXiv preprint arXiv:1711.11017, 2017.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.</p>
<p>Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca Dragan. On the utility of learning about humans for human-ai coordination. Advances in neural information processing systems, 32, 2019.</p>
<p>Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle Pineau. Tarmac: Targeted multi-agent communication. In International Conference on Machine Learning, pp. 1538-1546. PMLR, 2019.</p>
<p>Matt Deitke, Dhruv Batra, Yonatan Bisk, Tommaso Campari, Angel X Chang, Devendra Singh Chaplot, Changan Chen, Claudia Pérez D'Arpino, Kiana Ehsani, Ali Farhadi, et al. Retrospectives on the embodied ai workshop. arXiv preprint arXiv:2210.06849, 2022.</p>
<p>Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In arXiv preprint arXiv:2303.03378, 2023.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.</p>
<p>Chuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, Kuno Kim, Elias Wang, Michael Lingelbach, Aidan Curtis, Kevin Tyler Feigelis, Daniel Bear, Dan Gutfreund,</p>
<p>David Daniel Cox, Antonio Torralba, James J. DiCarlo, Joshua B. Tenenbaum, Josh Mcdermott, and Daniel LK Yamins. ThreeDWorld: A platform for interactive multi-modal physical simulation. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/forum?id=dbi1nWAwW2T.</p>
<p>Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar, Dan Gutfreund, Daniel LK Yamins, James J DiCarlo, Josh McDermott, Antonio Torralba, et al. The threedworld transport challenge: A visually guided task-and-motion planning benchmark towards physically realistic embodied ai. In 2022 International Conference on Robotics and Automation (ICRA), pp. 8847-8854. IEEE, 2022.</p>
<p>Claudia V Goldman and Shlomo Zilberstein. Optimizing information exchange in cooperative multi-agent systems. In Proceedings of the second international joint conference on Autonomous agents and multiagent systems, pp. 137-144, 2003.</p>
<p>Maitrey Gramopadhye and Daniel Szafir. Generating executable action plans with environmentallyaware language models. arXiv preprint arXiv:2210.04964, 2022.</p>
<p>Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961-2969, 2017.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.</p>
<p>Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045, 2023.</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118-9147. PMLR, 2022a.</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022b.</p>
<p>Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Humanlevel performance in 3d multiplayer games with population-based reinforcement learning. Science, $364(6443): 859-865,2019$.</p>
<p>Unnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana Lazebnik, Ali Farhadi, Alexander G Schwing, and Aniruddha Kembhavi. Two body problem: Collaborative visual task completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6689-6699, 2019.</p>
<p>Unnat Jain, Luca Weihs, Eric Kolve, Ali Farhadi, Svetlana Lazebnik, Aniruddha Kembhavi, and Alexander Schwing. A cordial sync: Going beyond marginal policies for multi-agent embodied tasks. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V 16, pp. 471-490. Springer, 2020.</p>
<p>Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning. In International conference on machine learning, pp. 3040-3049. PMLR, 2019 .</p>
<p>Jiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation. Advances in neural information processing systems, 31, 2018.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017.</p>
<p>Richard E Korf. Planning as search: A quantitative approach. Artificial intelligence, 33(1):65-88, 1987.</p>
<p>John E Laird. The Soar cognitive architecture. MIT press, 2019.
John E Laird. Introduction to soar. arXiv preprint arXiv:2205.03854, 2022.
Pat Langley, John E Laird, and Seth Rogers. Cognitive architectures: Research issues and challenges. Cognitive Systems Research, 10(2):141-160, 2009.</p>
<p>Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto MartínMartín, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference on Robot Learning, pp. 80-93. PMLR, 2023a.</p>
<p>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large scale language model society. arXiv preprint arXiv:2303.17760, 2023b.</p>
<p>Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, et al. Pre-trained language models for interactive decision-making. Advances in Neural Information Processing Systems, 35:31199-31212, 2022.</p>
<p>Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022.</p>
<p>Ryan Lowe, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017.</p>
<p>Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unifiedio: A unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022.</p>
<p>Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. arXiv preprint arXiv:2307.04738, 2023.</p>
<p>Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin, and Yoav Artzi. Mapping instructions to actions in 3d environments with visual goal prediction. arXiv preprint arXiv:1809.00786, 2018.</p>
<p>Anjali Narayan-Chen, Prashant Jayannavar, and Julia Hockenmaier. Collaborative dialogue in Minecraft. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5405-5415, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1537. URL https://aclanthology.org/P19-1537.</p>
<p>Andrew M Nuxoll and John E Laird. Enhancing intelligent agents with episodic memory. Cognitive Systems Research, 17:34-48, 2012.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek Hakkani-Tur. Teach: Task-driven embodied agents that chat. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 2017-2025, 2022.</p>
<p>Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Lior Horesh, Biplav Srivastava, Francesco Fabiano, and Andrea Loreggia. Plansformer: Generating symbolic plans using transformers. arXiv preprint arXiv:2212.08681, 2022.</p>
<p>Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.</p>
<p>Shivansh Patel, Saim Wani, Unnat Jain, Alexander G Schwing, Svetlana Lazebnik, Manolis Savva, and Angel X Chang. Interpretation of emergent communication in heterogeneous collaborative embodied agents. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15953-15963, 2021.</p>
<p>Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.</p>
<p>Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua B Tenenbaum, Sanja Fidler, and Antonio Torralba. Watch-and-help: A challenge for social perception and human-ai collaboration. In International Conference on Learning Representations, 2021.</p>
<p>Xavier Puig, Tianmin Shu, Joshua B Tenenbaum, and Antonio Torralba. Nopa: Neurally-guided online probabilistic assistance for building socially intelligent home assistants. arXiv preprint arXiv:2301.05223, 2023.</p>
<p>Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius, and Stefanie Tellex. Planning with large language models via corrective re-prompting. arXiv preprint arXiv:2211.09935, 2022.</p>
<p>Cinjon Resnick, Wes Eldridge, David Ha, Denny Britz, Jakob Foerster, Julian Togelius, Kyunghyun Cho, and Joan Bruna. Pommerman: A multi-agent playground. arXiv preprint arXiv:1809.07124, 2018.</p>
<p>Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2186-2188, 2019.</p>
<p>Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9339-9347, 2019.</p>
<p>Pratyusha Sharma, Antonio Torralba, and Jacob Andreas. Skill induction and planning with latent language. arXiv preprint arXiv:2110.01517, 2021.</p>
<p>Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10740-10749, 2020.</p>
<p>Tianmin Shu and Yuandong Tian. M ${ }^{3}$ rl: Mind-aware multi-agent management reinforcement learning. arXiv preprint arXiv:1810.00147, 2018.</p>
<p>Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. arXiv preprint arXiv:2212.04088, 2022.</p>
<p>Matthijs TJ Spaan, Geoffrey J Gordon, and Nikos Vlassis. Decentralized planning under uncertainty for teams of communicating agents. In Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pp. 249-256, 2006.</p>
<p>Peter Stone and Manuela Veloso. Multiagent systems: A survey from a machine learning perspective. Autonomous Robots, 8:345-383, 2000.</p>
<p>Joseph Suarez, Yilun Du, Phillip Isola, and Igor Mordatch. Neural mmo: A massively multiagent game environment for training and evaluating intelligent agents. arXiv preprint arXiv:1903.00784, 2019 .</p>
<p>Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L Griffiths. Cognitive architectures for language agents. arXiv preprint arXiv:2309.02427, 2023.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Yanming Wan, Jiayuan Mao, and Josh Tenenbaum. Handmethat: Human-robot communication in physical and social environments. Advances in Neural Information Processing Systems, 35: $12014-12026,2022$.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023b.</p>
<p>Yongjia Wang and John E Laird. Integrating semantic memory into a cognitive architecture. Ann Arbor, MI: University of Michigan Center for Cognitive Architecture, 2006.</p>
<p>Yuanfei Wang, Jing Xu, Yizhou Wang, et al. Tom2c: Target-oriented multi-agent communication and cooperation with theory of mind. In International Conference on Learning Representations, 2021.</p>
<p>Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona selfcollaboration. arXiv preprint arXiv:2307.05300, 2023c.</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents, 2023d.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Muning Wen, Jakub Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and Yaodong Yang. Multiagent reinforcement learning is a sequence modeling problem. Advances in Neural Information Processing Systems, 35:16509-16521, 2022.</p>
<p>Anita Williams Woolley, Christopher F Chabris, Alex Pentland, Nada Hashmi, and Thomas W Malone. Evidence for a collective intelligence factor in the performance of human groups. science, 330(6004):686-688, 2010.</p>
<p>Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom Mitchell, and Shrimai Prabhumoye. Plan, eliminate, and track-language models are good teachers for embodied agents. arXiv preprint arXiv:2305.02412, 2023.</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.</p>
<p>Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9068-9079, 2018.</p>
<p>Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: A simulated part-based interactive environment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. $11097-11107,2020$.</p>
<p>Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foundation models for decision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129, 2023.</p>
<p>Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Deqing Yang, and Yanghua Xiao. Distilling script knowledge from large language models for constrained language planning. arXiv preprint arXiv:2305.05252, 2023.</p>
<p>Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, and Ali Farhadi. Visual semantic planning using deep successor representations. In Proceedings of the IEEE international conference on computer vision, pp. 483-492, 2017.</p>
<h1>A ADDITIONAL DETAILS ON THE FRAMEWORK</h1>
<h2>A. 1 PERCEPTION MODULE</h2>
<p>To deal with raw sensory observations, a well-constructed Perception Module is needed for embodied agents to extract useful information for downstream higher-order reasoning.</p>
<p>In TDW-MAT, the environment provides an observation of $512 \times 512$ first-person view RGB image and Depth image. The agent first utilizes a pre-trained Mask-RCNN (He et al., 2017) to obtain the instance segmentation mask, then combines it with the depth image and the agent's position to project each pixel into the 3D world coordinate to obtain a 3D voxel semantic map, and finally accumulates along the height dimension to build a top-down 2D semantic map of size $L \times W \times 3$, where the first channel represents semantic classes including target objects, containers, destinations, and agents, and the last two channels represent the occupied and explored area respectively. Each element in the map denotes a grid of size $0.125 \mathrm{~m} \times 0.125 \mathrm{~m}$ in the scene. The agent also extracts the relationship of the objects with the help of instance segmentation masks and updates its Semantic Memory with the new information extracted from the observation.</p>
<p>To obtain a more suitable model for instance segmentation in a TDW simulation environment, we fine-tune the MASK-RCNN model pre-trained on the MS COCO dataset in training scenes. By random sampling in the training environments, we collected 53 K $512 \times 512$ RGB images and obtained the ground truth instance segmentation mask from the environment as the training set. The fine-tuned model achieves $81.4 \%$ mAP@50 in the test set.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: A visualization of the semantic map stored in the Semantic Memory and updated with new observations at every time in the TDW-MAT environment. The destination is shown in red, target objects are in blue, containers are in green, the agent is denoted with cyan, and the other agent's position in memory is denoted in yellow.</p>
<h1>A. 2 MEMORY ModulE</h1>
<p>We mimic human's long-term memory and design Semantic memory, Episodic Memory, and Procedural Memory for CoELA to store the knowledge and experience it has of the world, other agents, and itself.</p>
<p>Semantic Memory stores CoELA's knowledge about the world including a semantic map as shown in Figure 6 built and updated with local map perceived from the Perception Module, the task progress which is initialized with all zeros and updated whenever the agent is in the range of the goal position, the state of self including positions, holding objects status, and the state of others in memory which is updated whenever the others is perceived in the observation. To be noticed, CoELA's knowledge about the world may not be accurate since other agents may interact with the objects and change their states without its awareness. Dealing with imparities between the memory and the description of the world from others adds even more challenges.</p>
<p>Episodic Memory stores CoELA's experience about the past including the action history and dialogue history. Each time CoELA executes a new action including sending out a message or receiving a new message, the related information is added to the Episodic Memory. Empirically, we only keep the last $K$ actions and $D$ dialogues for storage efficiency.</p>
<p>Procedural Memory contains knowledge including how to carry out specific high-level plans in a specific environment implemented in code and the neural models' parameters including LLMs and Mask-RCNN. In our current implementation, the Procedural Memory is never updated except for fine-tuning the model parameters, while it's interesting to design a learning mechanism for it as in (Wang et al., 2023a) as well.</p>
<h2>A. 3 COMMUNICATION ModulE</h2>
<p>It's important for cooperative embodied agents to be able to communicate effectively with others. Effective communication needs to solve two problems: what to send and when to send.</p>
<p>We deal with the what to send problem in this module by directly using the LLMs as a Message Generator with designed prompts, constructed from the components of Instruction Head, Goal Description, States Description, Action History, and Dialogue History. To better constrain LLMs' generated messages, we also add a note at the end of the prompt and append two seed messages at the beginning of the Dialogue History to elicit deserved effective communication behavior. The detailed prompt design is shown below:</p>
<p>Instruction Head This part of the prompts is fixed for an environment, mainly consisting of the task instructions and environmental constraints.</p>
<p>Goal Description For each task, the goal description is converted from $G=\left{g_{1}, g_{2}, \ldots, g_{k}\right}$ using a formal template.</p>
<p>State Description For each step, the state description is converted from task progress, state of self, state of others, and semantic map retrieved from the Memory Module through a template.</p>
<p>Action History The concatenation of the last $K$ actions (high-level plans) the agent took.
Dialogue History The Concatenation of the last $D$ dialogues between agents including the messages the agent itself has sent.</p>
<p>To constrain the message generation of the LLMs, we add a note at the end of the prompt:
Note: The generated message should be accurate, helpful, and brief. Do not generate repetitive messages.</p>
<p>And append two seed messages at the beginning of the Dialogue History to elicit deserved effective communication behavior:</p>
<p>Alice: "Hi, I'll let you know if I find any goal objects, finish any subgoals, and ask for your help when necessary."</p>
<p>Bob: "Thanks! I'll let you know if I find any goal objects, finish any subgoals, and ask for your help when necessary."</p>
<h1>A. 4 Planning Module</h1>
<p>CoELA needs a strong Planning Module to make decisions on which action to take utilizing all available information gathered and stored so far to maximize cooperation efficiency.</p>
<p>While designing such a module from scratch consumes large human expert efforts and is nearly impossible to generalize, we utilize powerful LLMs directly as the Planning Module by first retrieving the related information from the Memory Module and converting them into text descriptions as in the Communication Module, then compile an Action List of all available high-level plans proposed according to the current state and the procedural knowledge stored for the LLMs to make the choice, which formalization makes it easier for the LLMs to concentrate on the reasoning and make an executable plan without any few-shot demonstrations easily, finally prompting the LLMs with current information and the proposed Action List to generate a high-level plan. We also use the zero-shot chain-of-thought prompting technique introduced by Kojima et al. (2022) to encourage the LLMs to carry out more reasoning before giving the final answer.</p>
<p>Action List We compile all available actions regarding the current state into an Action List for the LLMs to select from. The multi-choice formalization makes it easier for the LLM to make an executable plan without any few-shot demonstrations. All available high-level plans on the TDW-MAT include</p>
<ul>
<li>go to room *</li>
<li>explore current room</li>
<li>go grasp target object/container *</li>
<li>put holding objects into the holding container</li>
<li>transport holding objects to the bed</li>
<li>send a message: "*"</li>
</ul>
<p>Answer Extraction As shown in (Wei et al., 2022), chain-of-thought prompting can unleash the strong reasoning ability of the LLMs, we use the zero-shot chain-of-thought prompting technique introduced by (Kojima et al., 2022) to encourage the LLM to carry out more reasoning before giving the final answer.</p>
<h2>A. 5 EXECUTION MODULE</h2>
<p>To enable effective and generalized cooperation decision-making in different environments, we design an Execution Module to generate primitive actions to execute a given high-level plan robustly in a specific environment, allowing the Planning Module to be generalizable and focus more on solving the overall task with LLMs' rich world knowledge and strong reasoning ability. Practically, this design can also reduce the LLM inference time and is time-saving and economical. When facing a new environment with a different action space, only the procedural knowledge needs to be rewritten for CoELA to work. For rearrangement tasks, we mainly use an A-star-based planner to find the shortest path for navigation and robustly interact with the objects according to rules.</p>
<h2>A. 6 A WORKING EXAMPLE ON TDW-MAT</h2>
<p>To better understand our method, we present A working example of CoELA on one step in the TDW-MAT in Figure 7. CoELA receives an observation of $512 \times 512$ first-person view RGB image and Depth image from the environment, first uses the Perception Module implemented with MaskRCNN to predict an instance segmentation mask, then builds 3D point clouds and extracts the states (positions, names, IDs, objects holding if agents) of the key objects including target objects, containers, and the agents, and builds a local occupancy map. The Memory Module uses the extracted states of the key objects and the local occupancy map to construct and update the semantic map, which is stored in Semantic Memory. The Memory Module also stores the task progress, the states</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: A working example on the TDW-MAT. The environment provides an observation of 512 * 512 first-person view RGB image and Depth image. The Perception Module takes these in, builds 3D point clouds, then extracts the states (positions, names, IDs, objects holding if agents) of the key objects including target objects, containers, and the agents, and builds a local occupancy map. The Memory Module uses the extracted states of the key objects and the local occupancy map to construct and update the semantic map, which is stored in Semantic Memory. The Memory Module also stores the task progress, the states of the agents in the Semantic memory, and the agent's action and dialogue history in the Episodic Memory, which are also updated when a message is received. The Communication Module converts the semantic map, task progress, and agents' states into textual State Description and concatenates it with the Instruction Head, Goal Description, Action History, and Dialogue History as the prompt to condition the LLM on current states and generate the message to be sent beforehand. The Planning Module similarly takes these inputs and converts them into a prompt with the addition of an Action List compiled with all available high-level plans including sending the message just generated, then taking advantage of the chain-of-thought prompting to decide on the high-level plan. The Execution Module first uses an A-Star-based planner to find the shortest path from the current location to the target location with the help of the semantic map if needed, then carry out the interaction required to finish the high-level plan.</p>
<p>of the agents in the Semantic memory, and the agent's action and dialogue history in the Episodic Memory, which are also updated when a message is received. The Communication Module converts the semantic map, task progress, and agents' states into textual State Description and concatenates it with the Instruction Head, Goal Description, Action History, and Dialogue History as the prompt to condition the LLM on current states and generate the message to be sent beforehand. The Planning Module similarly takes these inputs and converts them into a prompt with the addition of an Action List compiled with all available high-level plans including sending the message just generated, then taking advantage of the chain-of-thought prompting to decide on the high-level plan "explore current room <Livingroom> (4000)". The Execution Module then uses an A-Star-based planner to find the shortest path from the current location to the target location with the help of the semantic map and gives the low-level primitive action of "Move forward 0.5 m ", which is carried out in the environment and the new observation will be sent to the agents again.</p>
<h1>B ADDITIONAL DETAILS ON ENVIRONMENTS</h1>
<h2>B. 1 ThreeDWorld Multi-Agent Transport</h2>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: TDW-MAT scenes, target objects, and containers.</p>
<p>As an extension of the ThreeDWorld Transport Challenge(Gan et al., 2021), ThreeDWorld MultiAgent Transport (TDW-MAT) supports multi-agent cooperation with natural language communication and includes more types of objects with more realistic placements. In the new challenge, we use the latest replicant humanoid provided by the TDW platform as an embodiment.</p>
<p>Tasks Two tasks are available in TDW-MAT: food-transporting task and stuff-transporting task. The two tasks have different types of target objects and containers. Figure 8 shows an overview of the two tasks: We create 4 floorplans and each of them has 3 layouts, where two floorplans are for the training set and another two are for the test set. The food-transporting task has 6 types of targets (apple, banana, orange, bread, loaf bread, and burger) and 3 containers (bowl, plate, and tea tray). In contrast, the stuff-transporting task has 6 different types of targets(calculator, mouse, pen, lighter, purse, and iPhone) and 3 containers (plastic basket, wood basket, and wicker basket). In each task, there are 10 target objects and 2 to 5 containers in total. Additionally, there are 4 types of rooms: living room, office, kitchen, and bedroom, and objects are placed in these rooms consistent with common sense. For example, food is more likely to be found in kitchens, while stuff is often in offices.</p>
<p>The agents are tasked to transport as many target objects as possible to the goal position with the help of containers as tools. One container can carry most three objects, and without containers, the agent can transport only two objects at a time. Agents need to transport target objects as much as possible within 3000 frames.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: The RGB, depth, and oracle perception generated from the TDW-MAT environment.</p>
<p>Observation Space The embodied agent receives the egocentric RGB image and depth image as the main observation, as well as some auxiliary observations. Figure 9 is an example of an image generated from the TDW-MAT environment, and the detailed observation space is listed here:</p>
<ul>
<li>RGB image: the egocentric image comes from the camera facing forward, with screen size $512 \times 512$ and field of view 90 ;</li>
<li>Depth image: the depth image has the same camera intrinsic parameters as the RGB image;</li>
<li>Oracle Perception (optional): an image where each object id is mapped to a color and the camera intrinsic parameters are the same as the RGB image;</li>
<li>Agent position and rotation: the agent's position and rotation in the simulation world;</li>
<li>Messages: the messages sent by all the agents;</li>
</ul>
<p>Action Space In TDW-MAT, there are 7 types of actions for agents to interact with the environment or communicate with each other. Each action takes several frames and the detailed action space is listed here:</p>
<ul>
<li>Move forward: move forward 0.5 m ;</li>
<li>Turn left: turn left by 15 degrees;</li>
<li>Turn right: turn right by 15 degrees;</li>
<li>Grasp: grasp an object, only the agent is close to the object can he perform the action successfully. The object can be either a target or a container;</li>
<li>Put In: put the target into the container, only the agent is holding a target in one hand and a container in another hand can he perform the action.</li>
<li>Drop: drop the objects held in hand;</li>
<li>Send message: Send a message to other agents. In each frame, no more than 500 characters can be sent.</li>
</ul>
<h1>B. 2 Communicative Watch-And-Help</h1>
<p>Communicative Watch-And-Help (C-WAH) is an extension of the Watch-And-Help challenge(Puig et al., 2021), which enables agents to send messages to each other. Sending messages, alongside other actions, takes one timestep and has an upper limit on message length.</p>
<p>Tasks Five types of tasks are available in C-WAH, named Prepare afternoon tea, Wash dishes, Prepare a meal, Put groceries, and Set up a dinner table. These tasks include a range of housework, and each task contains a few subgoals, which are described by predicates. A predicate is in "ON/IN(x, $y$ )" format, that is, "Put $x$ ON/IN $y$ ". The detailed descriptions of tasks are listed in Table 3.</p>
<p>The task goal is to satisfy all the given subgoals within 250 time steps, and the number of subgoals in each task ranges from 3 to 5 .</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2} \mathrm{~A}$ template communication is used here to study humans' communication preference, details in Appendix F&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
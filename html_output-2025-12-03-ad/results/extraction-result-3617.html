<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3617 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3617</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3617</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-258762793</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.emnlp-main.36.pdf" target="_blank">Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning</a></p>
                <p><strong>Paper Abstract:</strong> Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we apply large language models (LLMs) to these benchmarks using in-context learning (ICL). We investigate whether and to what extent LLMs can be used for TKG forecasting, especially without any fine-tuning or explicit modules for capturing structural and temporal information. For our experiments, we present a framework that converts relevant historical facts into prompts and generates ranked predictions using token probabilities. Surprisingly, we observe that LLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully designed and trained for TKG forecasting. Our extensive evaluation presents performances across several models and datasets with different characteristics, compares alternative heuristics for preparing contextual information, and contrasts to prominent TKG methods and simple frequency and recency baselines. We also discover that using numerical indices instead of entity/relation names, i.e., hiding semantic information, does not significantly affect the performance ($\pm$0.4\% Hit@1). This shows that prior semantic knowledge is unnecessary; instead, LLMs can leverage the existing patterns in the context to achieve such performance. Our analysis also reveals that ICL enables LLMs to learn irregular patterns from the historical context, going beyond simple predictions based on common or recent information.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3617.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3617.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-NeoX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-NeoX (20B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source large autoregressive transformer language model (GPT-NeoX family) used in the paper as a foundation LLM to perform in-context learning for temporal knowledge graph forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-NeoX (neox-20b)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer language model; open-source implementation (GPT-NeoX) with ~20B parameters (reported in paper as GPT-NeoX 20B); GPT-2-like tokenization and decoder-only architecture. Pretrained, not fine-tuned for the task in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Temporal Knowledge Graph (TKG) forecasting: predict the missing entity in a future quadruple (?, p, o, t) or (s, p, ?, t) using historical facts presented in the prompt (in-context learning). Datasets: WIKI, YAGO, ICEWS14, ICEWS18, ACLED-CD22.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Zero-shot in-context learning: construct lexical or indexed prompts from retrieved historical facts; pass prompt to LLM and extract next-token probabilities (greedy decoding). Candidate entities are mapped to numerical labels (indirect logits); probabilities of the first generated token(s) corresponding to numeric labels are used to rank candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Standard temporal KG forecasting benchmarks: WIKI, YAGO, ICEWS14, ICEWS18, and an ACLED-derived Cabo Delgado dataset (ACLED-CD22). Each dataset provides time-stamped triples; test split is chronologically after training/validation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Link-prediction metrics Hits@k (k=1,3,10) with time-aware filtering. Reported values vary by dataset and prompt settings; overall ICL with GPT-NeoX falls within (-3.6%, +1.5%) Hits@1 relative to the median supervised baseline per dataset and outperforms simple heuristics (frequency/recency) by roughly +10% to +28% Hits@1 depending on dataset. (Exact dataset-specific H@1 values are reported in paper tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to SOTA supervised TKG models (RE-Net, RE-GCN, TANGO, xERTE, TimeTraveler, CyGNet, TLogic), GPT-NeoX via ICL achieves competitive performance: within a small margin of the median supervised approach (± a few percent H@1). It markedly outperforms simple rule baselines (frequency and recency heuristics) by double-digit relative improvements in Hits@1 on the ICEWS benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires mapping entities to numeric labels to extract probabilities reliably; sometimes model outputs no numeric token, requiring an evaluation protocol that assigns rank=100 (treated as incorrect). Multi-step forecasting compounds noise since predicted facts are added back to history. Limited to inductive setting (answers must come from observed history). Experiments constrained by compute; results reported for available model sizes. Tokenization quirks and models with only single-digit number tokens (e.g., some LLAMA variants) pose challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>GPT-NeoX can use ICL to forecast future events on TKGs without supervised fine-tuning, learn irregular temporal patterns beyond simple frequency/recency, and achieve competitive Hits@1 relative to supervised SOTA; performance improves with longer history and larger model size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3617.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3617.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J (6B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source GPT-family autoregressive language model (GPT-J) used for ablations and prompt-construction experiments in TKG forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J (gpt-j-6b)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source autoregressive transformer (approx. 6B parameters). Uses GPT-2 byte-level BPE tokenizer and a decoder-only transformer architecture. Pretrained and used without task-specific fine-tuning in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Same TKG forecasting tasks as other LLMs: predict the missing entity in future quadruples using in-context examples constructed from historical facts.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>In-context learning with prompts (Index and Lexical variants). Use token probabilities for the next token; numerical labels assigned to candidate entities used as proxies for logits (indirect logit approach).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>WIKI, YAGO, ICEWS14, ICEWS18, ACLED-CD22 temporal KG datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Hits@k (1,3,10) with time-aware filtering. Results reported in prompt-variation experiments; index vs lexical prompts show near-identical performance (mean diff ~0.004 H@1), indicating reliance on pattern learning rather than semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>GPT-J's ICL performance is comparable across index and lexical prompts and demonstrates the same qualitative advantages over simple heuristics; generally competitive but fall behind larger models like GPT-NeoX, following the scaling trend.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Same evaluation challenges as other LLMs: mapping multi-token entity names to single numeric labels, occasional failure to emit numeric tokens, and limited compute restricting experiments to smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Prompt design (index vs lexical) had minor effect on GPT-J; performance depends on history retrieval strategy (Entity vs Pair) and directionality (Unidirectional often better than Bidirectional). Findings support that ICL extracts symbolic/sequential patterns rather than relying on semantic pretraining alone.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3617.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3617.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 family (gpt2, gpt2-medium, gpt2-large, gpt2-xl)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Series of decoder-only transformer LMs of increasing size used to evaluate scaling effects on in-context TKG forecasting performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 family (gpt2:124M, gpt2-medium:355M, gpt2-large:774M, gpt2-xl:1.5B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer language models (GPT-2 family) with byte-level BPE tokenization; evaluated at multiple sizes to observe the scaling law for in-context learning in forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>TKG forecasting via in-context learning: predicting future relational facts (missing entity) given historical context in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Standard ICL prompting (index/lexical) with extraction of next-token probabilities; map candidates to numeric tokens and use their probability as a score.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>WIKI, YAGO, ICEWS14, ICEWS18, ACLED-CD22.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Hits@1 (and H@3, H@10) reported; the paper documents that performance scales with model size (larger GPT-2 variants show higher Hit@1), consistent with known scaling of ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Smaller GPT-2 variants perform worse than larger open models (GPT-J, GPT-NeoX) and supervised SOTA; however, scaling size improves performance, approaching competitive performance for larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Smaller sizes show limited ICL capability; tokenization and numeric label mapping challenges persist. Compute and dataset constraints limit deeper exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Performance improves monotonically with model size, supporting a scaling law for ICL on forecasting tasks; even relatively small models can capture some temporal/symbolic patterns but larger models are substantially better.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3617.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3617.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruct-tuned GPT-3.5 Turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instruction-tuned ChatGPT-style model (gpt-3.5-turbo) evaluated for TKG forecasting with constrained system instructions; lexical prompts provided slight advantage over indexed prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large language model (OpenAI's gpt-3.5-turbo); model size not disclosed in paper. Provides instruction-following behavior but does not expose token-level output probabilities in the evaluation API used by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>TKG forecasting via in-context learning using prompt templates and curated system instruction; model required to output a single numeric label representing the predicted entity.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Because token probabilities were not available from the API, authors evaluated gpt-3.5-turbo using deterministic outputs constrained by a system instruction; measured Hit@1 by observing the single returned numeric label rather than ranking by probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>ICEWS14 (example reported), and other TKG datasets in broader study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Hit@1 reported where available; example: on ICEWS14 single-step, gpt-3.5-turbo H@1 = 0.1615 for indexed prompts and 0.1858 for lexical prompts (lexical outperforming index by 0.024 H@1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Instruction-tuning allows better exploitation of semantic priors (lexical prompts gave a gap not seen in other foundation models). Overall, gpt-3.5-turbo competitive in some settings but direct probability-based ranking comparisons were not possible due to API limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>API did not provide token-level probabilities, so ranking across all candidate entities via probabilistic scoring was not possible. Comparisons across model sizes are also not possible because exact model size is undisclosed. The instruction-tuned model demonstrated a reliance on lexical (semantic) priors more than other foundation models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Instruction tuning can make LLMs more sensitive to lexical/semantic priors (lexical prompts outperformed index prompts), but lack of probability access limits calibration and full comparison. Despite that, gpt-3.5-turbo demonstrates reasonable single-label forecasting ability via constrained outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 2)</em></li>
                <li>Teaching models to express their uncertainty in words <em>(Rating: 2)</em></li>
                <li>Forecasting future world events with neural networks <em>(Rating: 2)</em></li>
                <li>Recurrent event network: Autoregressive structure inference over temporal knowledge graphs <em>(Rating: 2)</em></li>
                <li>TimeTraveler: Reinforcement learning for temporal knowledge graph forecasting <em>(Rating: 2)</em></li>
                <li>CyGNet: Copy and Generation networks for temporal KG forecasting (CyGNet) <em>(Rating: 1)</em></li>
                <li>TLogic: Temporal logical rules for explainable link forecasting on temporal knowledge graphs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3617",
    "paper_id": "paper-258762793",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [
        {
            "name_short": "GPT-NeoX",
            "name_full": "GPT-NeoX (20B)",
            "brief_description": "An open-source large autoregressive transformer language model (GPT-NeoX family) used in the paper as a foundation LLM to perform in-context learning for temporal knowledge graph forecasting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-NeoX (neox-20b)",
            "model_description": "Autoregressive transformer language model; open-source implementation (GPT-NeoX) with ~20B parameters (reported in paper as GPT-NeoX 20B); GPT-2-like tokenization and decoder-only architecture. Pretrained, not fine-tuned for the task in this study.",
            "prediction_task": "Temporal Knowledge Graph (TKG) forecasting: predict the missing entity in a future quadruple (?, p, o, t) or (s, p, ?, t) using historical facts presented in the prompt (in-context learning). Datasets: WIKI, YAGO, ICEWS14, ICEWS18, ACLED-CD22.",
            "method_of_probability_estimation": "Zero-shot in-context learning: construct lexical or indexed prompts from retrieved historical facts; pass prompt to LLM and extract next-token probabilities (greedy decoding). Candidate entities are mapped to numerical labels (indirect logits); probabilities of the first generated token(s) corresponding to numeric labels are used to rank candidates.",
            "dataset_or_benchmark": "Standard temporal KG forecasting benchmarks: WIKI, YAGO, ICEWS14, ICEWS18, and an ACLED-derived Cabo Delgado dataset (ACLED-CD22). Each dataset provides time-stamped triples; test split is chronologically after training/validation.",
            "performance_metrics": "Link-prediction metrics Hits@k (k=1,3,10) with time-aware filtering. Reported values vary by dataset and prompt settings; overall ICL with GPT-NeoX falls within (-3.6%, +1.5%) Hits@1 relative to the median supervised baseline per dataset and outperforms simple heuristics (frequency/recency) by roughly +10% to +28% Hits@1 depending on dataset. (Exact dataset-specific H@1 values are reported in paper tables.)",
            "comparison_to_baselines": "Compared to SOTA supervised TKG models (RE-Net, RE-GCN, TANGO, xERTE, TimeTraveler, CyGNet, TLogic), GPT-NeoX via ICL achieves competitive performance: within a small margin of the median supervised approach (± a few percent H@1). It markedly outperforms simple rule baselines (frequency and recency heuristics) by double-digit relative improvements in Hits@1 on the ICEWS benchmarks.",
            "limitations_or_challenges": "Requires mapping entities to numeric labels to extract probabilities reliably; sometimes model outputs no numeric token, requiring an evaluation protocol that assigns rank=100 (treated as incorrect). Multi-step forecasting compounds noise since predicted facts are added back to history. Limited to inductive setting (answers must come from observed history). Experiments constrained by compute; results reported for available model sizes. Tokenization quirks and models with only single-digit number tokens (e.g., some LLAMA variants) pose challenges.",
            "notable_findings": "GPT-NeoX can use ICL to forecast future events on TKGs without supervised fine-tuning, learn irregular temporal patterns beyond simple frequency/recency, and achieve competitive Hits@1 relative to supervised SOTA; performance improves with longer history and larger model size.",
            "uuid": "e3617.0",
            "source_info": {
                "paper_title": "Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-J",
            "name_full": "GPT-J (6B)",
            "brief_description": "An open-source GPT-family autoregressive language model (GPT-J) used for ablations and prompt-construction experiments in TKG forecasting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-J (gpt-j-6b)",
            "model_description": "Open-source autoregressive transformer (approx. 6B parameters). Uses GPT-2 byte-level BPE tokenizer and a decoder-only transformer architecture. Pretrained and used without task-specific fine-tuning in the experiments.",
            "prediction_task": "Same TKG forecasting tasks as other LLMs: predict the missing entity in future quadruples using in-context examples constructed from historical facts.",
            "method_of_probability_estimation": "In-context learning with prompts (Index and Lexical variants). Use token probabilities for the next token; numerical labels assigned to candidate entities used as proxies for logits (indirect logit approach).",
            "dataset_or_benchmark": "WIKI, YAGO, ICEWS14, ICEWS18, ACLED-CD22 temporal KG datasets.",
            "performance_metrics": "Hits@k (1,3,10) with time-aware filtering. Results reported in prompt-variation experiments; index vs lexical prompts show near-identical performance (mean diff ~0.004 H@1), indicating reliance on pattern learning rather than semantics.",
            "comparison_to_baselines": "GPT-J's ICL performance is comparable across index and lexical prompts and demonstrates the same qualitative advantages over simple heuristics; generally competitive but fall behind larger models like GPT-NeoX, following the scaling trend.",
            "limitations_or_challenges": "Same evaluation challenges as other LLMs: mapping multi-token entity names to single numeric labels, occasional failure to emit numeric tokens, and limited compute restricting experiments to smaller models.",
            "notable_findings": "Prompt design (index vs lexical) had minor effect on GPT-J; performance depends on history retrieval strategy (Entity vs Pair) and directionality (Unidirectional often better than Bidirectional). Findings support that ICL extracts symbolic/sequential patterns rather than relying on semantic pretraining alone.",
            "uuid": "e3617.1",
            "source_info": {
                "paper_title": "Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-2 family",
            "name_full": "GPT-2 family (gpt2, gpt2-medium, gpt2-large, gpt2-xl)",
            "brief_description": "Series of decoder-only transformer LMs of increasing size used to evaluate scaling effects on in-context TKG forecasting performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 family (gpt2:124M, gpt2-medium:355M, gpt2-large:774M, gpt2-xl:1.5B)",
            "model_description": "Decoder-only transformer language models (GPT-2 family) with byte-level BPE tokenization; evaluated at multiple sizes to observe the scaling law for in-context learning in forecasting.",
            "prediction_task": "TKG forecasting via in-context learning: predicting future relational facts (missing entity) given historical context in prompt.",
            "method_of_probability_estimation": "Standard ICL prompting (index/lexical) with extraction of next-token probabilities; map candidates to numeric tokens and use their probability as a score.",
            "dataset_or_benchmark": "WIKI, YAGO, ICEWS14, ICEWS18, ACLED-CD22.",
            "performance_metrics": "Hits@1 (and H@3, H@10) reported; the paper documents that performance scales with model size (larger GPT-2 variants show higher Hit@1), consistent with known scaling of ICL.",
            "comparison_to_baselines": "Smaller GPT-2 variants perform worse than larger open models (GPT-J, GPT-NeoX) and supervised SOTA; however, scaling size improves performance, approaching competitive performance for larger models.",
            "limitations_or_challenges": "Smaller sizes show limited ICL capability; tokenization and numeric label mapping challenges persist. Compute and dataset constraints limit deeper exploration.",
            "notable_findings": "Performance improves monotonically with model size, supporting a scaling law for ICL on forecasting tasks; even relatively small models can capture some temporal/symbolic patterns but larger models are substantially better.",
            "uuid": "e3617.2",
            "source_info": {
                "paper_title": "Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "gpt-3.5-turbo",
            "name_full": "Instruct-tuned GPT-3.5 Turbo (OpenAI)",
            "brief_description": "Instruction-tuned ChatGPT-style model (gpt-3.5-turbo) evaluated for TKG forecasting with constrained system instructions; lexical prompts provided slight advantage over indexed prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (instruction-tuned)",
            "model_description": "Instruction-tuned large language model (OpenAI's gpt-3.5-turbo); model size not disclosed in paper. Provides instruction-following behavior but does not expose token-level output probabilities in the evaluation API used by the authors.",
            "prediction_task": "TKG forecasting via in-context learning using prompt templates and curated system instruction; model required to output a single numeric label representing the predicted entity.",
            "method_of_probability_estimation": "Because token probabilities were not available from the API, authors evaluated gpt-3.5-turbo using deterministic outputs constrained by a system instruction; measured Hit@1 by observing the single returned numeric label rather than ranking by probabilities.",
            "dataset_or_benchmark": "ICEWS14 (example reported), and other TKG datasets in broader study.",
            "performance_metrics": "Hit@1 reported where available; example: on ICEWS14 single-step, gpt-3.5-turbo H@1 = 0.1615 for indexed prompts and 0.1858 for lexical prompts (lexical outperforming index by 0.024 H@1).",
            "comparison_to_baselines": "Instruction-tuning allows better exploitation of semantic priors (lexical prompts gave a gap not seen in other foundation models). Overall, gpt-3.5-turbo competitive in some settings but direct probability-based ranking comparisons were not possible due to API limitations.",
            "limitations_or_challenges": "API did not provide token-level probabilities, so ranking across all candidate entities via probabilistic scoring was not possible. Comparisons across model sizes are also not possible because exact model size is undisclosed. The instruction-tuned model demonstrated a reliance on lexical (semantic) priors more than other foundation models.",
            "notable_findings": "Instruction tuning can make LLMs more sensitive to lexical/semantic priors (lexical prompts outperformed index prompts), but lack of probability access limits calibration and full comparison. Despite that, gpt-3.5-turbo demonstrates reasonable single-label forecasting ability via constrained outputs.",
            "uuid": "e3617.3",
            "source_info": {
                "paper_title": "Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 2,
            "sanitized_title": "calibrate_before_use_improving_fewshot_performance_of_language_models"
        },
        {
            "paper_title": "Teaching models to express their uncertainty in words",
            "rating": 2,
            "sanitized_title": "teaching_models_to_express_their_uncertainty_in_words"
        },
        {
            "paper_title": "Forecasting future world events with neural networks",
            "rating": 2,
            "sanitized_title": "forecasting_future_world_events_with_neural_networks"
        },
        {
            "paper_title": "Recurrent event network: Autoregressive structure inference over temporal knowledge graphs",
            "rating": 2,
            "sanitized_title": "recurrent_event_network_autoregressive_structure_inference_over_temporal_knowledge_graphs"
        },
        {
            "paper_title": "TimeTraveler: Reinforcement learning for temporal knowledge graph forecasting",
            "rating": 2,
            "sanitized_title": "timetraveler_reinforcement_learning_for_temporal_knowledge_graph_forecasting"
        },
        {
            "paper_title": "CyGNet: Copy and Generation networks for temporal KG forecasting (CyGNet)",
            "rating": 1,
            "sanitized_title": "cygnet_copy_and_generation_networks_for_temporal_kg_forecasting_cygnet"
        },
        {
            "paper_title": "TLogic: Temporal logical rules for explainable link forecasting on temporal knowledge graphs",
            "rating": 1,
            "sanitized_title": "tlogic_temporal_logical_rules_for_explainable_link_forecasting_on_temporal_knowledge_graphs"
        }
    ],
    "cost": 0.011172749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning</p>
<p>Dong-Ho Lee dongho.lee@usc.edu 
Department of Computer Science and Information Sciences Institute
University of Southern California</p>
<p>Kian Ahrabian ahrabian@usc.edu 
Department of Computer Science and Information Sciences Institute
University of Southern California</p>
<p>Woojeong Jin woojeong.jin@usc.edu 
Department of Computer Science and Information Sciences Institute
University of Southern California</p>
<p>Fred Morstatter 
Department of Computer Science and Information Sciences Institute
University of Southern California</p>
<p>Jay Pujara jpujara@isi.edu 
Department of Computer Science and Information Sciences Institute
University of Southern California</p>
<p>Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning
729486CD8CC547B2F4C234962F9465A3
Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts.In this paper, we develop an approach to use in-context learning (ICL) with large language models (LLMs) for TKG forecasting.Our extensive evaluation compares diverse baselines, including both simple heuristics and state-of-the-art (SOTA) supervised models, against pre-trained LLMs across several popular benchmarks and experimental settings.We observe that naive LLMs perform on par with SOTA models, which employ carefully designed architectures and supervised training for the forecasting task, falling within the (-3.6%, +1.5%) Hits@1 margin relative to the median performance.To better understand the strengths of LLMs for forecasting, we explore different approaches for selecting historical facts, constructing prompts, controlling information propagation, and parsing outputs into a probability distribution.A surprising finding from our experiments is that LLM performance endures (±0.4% Hit@1) even when semantic information is removed by mapping entities/relations to arbitrary numbers, suggesting that prior semantic knowledge is unnecessary; rather, LLMs can leverage the symbolic patterns in the context to achieve such a strong performance.Our analysis also reveals that ICL enables LLMs to learn irregular patterns from the historical context, going beyond frequency and recency biases 1 .</p>
<p>Introduction</p>
<p>Knowledge Graphs (KGs) are prevalent resources for representing real-world facts in a structured way.While traditionally, KGs have been utilized for representing static snapshots of "current" knowledge, recently, temporal KGs (TKGs) have gained popularity to preserve the complex temporal dynamics proaches such as employing graph neural networks to model interrelationships among entities and relations (Jin et al., 2020;Li et al., 2021;Han et al., 2021b,a), using reinforcement learning techniques (Sun et al., 2021), and utilizing logical rules (Zhu et al., 2021;Liu et al., 2022).However, these techniques have prominent limitations, including the need for large amounts of training data that include thorough historical information for the entities.Additionally, model selection is a computationally expensive challenge as the stateof-the-art approach differs for each dataset.</p>
<p>In this paper, we develop a TKG forecasting approach by casting the task as an in-context learning (ICL) problem using large language models (LLMs).ICL refers to the capability of LLMs to learn and perform an unseen task efficiently when provided with a few examples of input-label pairs in the prompt (Brown et al., 2020).Prior works on ICL usually leverage few-shot demonstrations, where a uniform number of examples are provided for each label to solve a classification task (Min et al., 2022;Wei et al., 2023).In contrast, our work investigates what the model learns from irregular patterns of historical facts in the context.We design a three-stage pipeline to control (1) the background knowledge selected for context, (2) the prompting strategy for forecasting, and (3) decoding the output into a prediction.The first stage uses the prediction query to retrieve a set of relevant past facts from the TKG that can be used as context (Section 3.1).The second stage transforms these contextual facts into a lexical prompt representing the prediction task (Section 3.3).The third stage decodes the output of the LLM into a probability distribution over the entities and generates a response to the prediction query (Section 3.4).Our experimental evaluation performs competitively across a diverse collection of TKG benchmarks without requiring the time-consuming supervised training, or custom-designed architectures.</p>
<p>We present extensive experimental results on common TKG benchmark datasets such as WIKI (Leblay and Chekol, 2018), YAGO (Mahdisoltani et al., 2014), andICEWS (García-Durán et al., 2018;Jin et al., 2020).Our findings are as follows: (1) LLMs demonstrate the ability to make predictions about future facts using ICL without requiring any additional training.Moreover, these models show comparable performance to supervised approaches, falling within the (-3.6%, +1.5%) Hits@1 margin, relative to the median approach for each dataset; (2) LLMs perform almost identically when we replace entities' and relations' lexical names with numerically mapped indices, suggesting that the prior semantic knowledge is not a critical factor for achieving such a high performance; and (3) LLMs outperform the best heuristic rule-based baseline on each dataset (i.e., the most frequent or the most recent, given the historical context) by (+10%, +28%) Hits@1 relative margin, indicating that they do not simply select the output using frequency or recency biases in ICL (Zhao et al., 2021).</p>
<p>Problem Formulation</p>
<p>In-Context Learning.ICL is an emergent capability of LLMs that aims to induce a state in the model to perform a task by utilizing contextual input-label examples, without requiring changes to its internal parameters (Brown et al., 2020).Formally, in ICL for classification, a prompt is constructed by linearizing a few input-output pair examples (x i , y i ) from the training data.Subsequently, when a new test input text x test is provided, ICL generates the output y test ∼ P LLM (y test | x 1 , y 1 , . . ., x k , y k , x test ) where ∼ refers to decoding strategy.</p>
<p>Temporal Knowledge Graph Forecasting.Formally, a TKG, G = (V, R, E, T ), is comprised of a set of entities V, relations R, facts E, and timestamps T .Moreover, since time is sequential, G can be split into a sequence of time-stamped snapshots, G = {G 1 , G 2 , . . ., G t , . ..},where each snapshot, G t = (V, R, E t ), contains the facts at a specific point in time t.Each fact f ∈ E t is a quadruple (s, p, o, t) where s, o ∈ V, p ∈ R, and t ∈ T .The TKG forecasting task involves predicting a temporally conditioned missing entity in the future given a query quadruple, (?, p, o, t) or (s, p, ?, t), and previous graph snapshots
G 1:t−1 = {G 1 , G 2 , . . . , G t−1 }.
Here, the prediction typically involves ranking each entity's assigned score.</p>
<p>In-context Learning for Temporal Knowledge Graph Forecasting</p>
<p>In this work, we focus on 1) modeling appropriate history E q for a given query quadruple q, 2) converting {E q , q} into a prompt θ q , and 3) employing ICL to get prediction y q ∼ P LLM (y q | θ q ) in a zero-shot manner.Here, the history E q is modeled on the facts from the previous graph snapshots
G 1:t−1 = {G 1 , G 2 , .
. ., G t−1 }, and we employ token probabilities for y q to get ranked scores of candidate entities in a zero-shot manner.In the rest of this section, we study history modeling strategies (Sec 3.1), response generation approaches (Sec 3.2), prompt construction templates (Sec 3.3), and common prediction settings (Sec 3.4).</p>
<p>History Modeling</p>
<p>To model the history E q , we filter facts that the known entity or relation in the query q has been involved in.Specifically, given the query quadruple q = (s, p, ?, t) under the object entity prediction setting, we experiment with two different aspects of historical facts:</p>
<p>Entity vs. Pair.Entity includes past facts that contain s, e.g., all historical facts related to Superbowl.In contrast, Pair includes past facts that contain both s and p, e.g., a list of (Superbowl, Champion, Year) as shown in Table 1.</p>
<p>Unidirectional vs. Bidirectional.Unidirectional includes past facts F wherein s (Entity) or (s, p) (Pair) is in the same position as it is in q (e.g., Unidirectional &amp; Pairs and p served as subject and predicate in f ∈ F).Bidirectional includes past facts F wherein s (Entity) or (s, p) (Pair) appear in any valid position (e.g., Bidirectional &amp; Entitys served as subject or object in f ∈ F).As an example of the Bidirectional setting, given q = (Superbowl, Champion, ?, 2023), we include f = (Kupp, Played, Superbowl, 2022) because s (i.e., Superbowl) is present as the object in f .Moreover, in the Bidirectional setting, to preserve the semantics of the facts in the E q , we transform the facts where s appears as an object by 1) swapping the object and subject and 2) replacing the relation with its uniquely defined inverse relation (e.g.,
(f s , f p , f o , f t ) → (f o , f −1 p , f s , f t )).</p>
<p>Response Generation</p>
<p>Given a prompt θ q , we pass it to an LLM to obtain the next token probabilities.Then, we use the obtained probabilities to get a ranked list of entities.However, obtaining scores for entities based on these probabilities is challenging as they may be composed of several tokens.To address this challenge, we utilize a mapped numerical label as an indirect logit to estimate their probabilities (Lin et al., 2022).</p>
<p>Prompt Construction</p>
<p>Given the history E q and query q, we construct a prompt using a pre-defined template θ.Specifically, given the query quadruple q = (s, p, ?, t) under the object entity prediction setting, we present two versions of the template θ with varying levels of information.Our assumption is that each entity or relation has an indexed I(•) (e.g., 0) and a lexical L(•) (e.g., Superbowl) form (See Table 2).</p>
<p>Index.Index displays every fact,
(f s , f p , f o , f t ) ∈ E using the "f t :[I(f s ), I(f p ), n fo . I(f o )]" tem- plate where f s , f o ∈ V, f p ∈ R, f t ∈ T
, n fo denotes an incrementally assigned numerical label (i.e., indirect logit), and I is a mapping from entities to unique indices.For example, in Table 1, we can use the following mappings are for the entities and relations, respectively: {Superbowl → 0, St Louis → 1, Baltimore → 2} and {Champion → 0}.The query q is then represented as "t:[I(s), I(p),", concatenated to the end of the prompt.For subject entity prediction, we follow the same procedure from the other side.</p>
<p>Lexical.Lexical follows the same process as Index but uses lexical form L(•) of entity and relation.Each fact in
(f s , f p , f o , f t ) ∈ E is represented as "f t :[L(f s ), L(f p ), n fo . L(f o )]"
and the query q is represented as "t:[L(s), L(p),", concatenated to the end of the prompt.</p>
<p>Prediction Setting</p>
<p>All the historical facts in the dataset are split into three subsets, D train , D valid , and D test , based on the chronological order with train &lt; valid &lt; test.Given this split, during the evaluation phase, the TKG forecasting task requires models to predict over D test under the following two settings:</p>
<p>Single</p>
<p>Step. timestamp, the ground truth fact for that query is added to the history before moving to the test queries in the next timestamp.</p>
<p>Multi</p>
<p>Step.In this setting, the model is not provided with ground truth facts from past timestamps in the test period and has to rely on its noisy predictions.Hence, after making predictions for a test query in a specific timestamp, instead of the ground truth fact for that query, we add the predicted response to the history before moving to the test queries in the next timestamp.This setting is considered more difficult as the model is forced to rely on its own noisy predictions, which can lead to greater uncertainty with each successive timestamp.</p>
<p>4 Experimental Setup</p>
<p>Datasets</p>
<p>For our experiments, we use the WIKI (Leblay and Chekol, 2018), YAGO (Mahdisoltani et al., 2014), ICEWS14 (García-Durán et al., 2018), and ICEWS18 (Jin et al., 2020) benchmark datasets with the unified splits introduced in previous studies (Gastinger et al., 2022).Additionally, we extract a new temporal forecasting dataset from the Armed Conflict Location &amp; Event Data Project (ACLED) project 2 which provides factual data of crises in a particular region.We specifically focus on incidents of combat and violence against civilians in Cabo Delgado from January 1900 to March 2022, using data from October 2021 to March 2022 as our test set.This dataset aims to investigate whether LLMs leverage prior semantic knowledge to make predictions and how effective they are when deployed in real-world applications.Table 3 presents the statistics of these datasets.</p>
<p>2 https://data.humdata.org/organization/acledModel Family Model Name # Params Instruction-tuned
GPT2 gpt2 124M ✗ gpt2-medium 355M ✗ gpt2-large 774M ✗ gpt2-xl 1.5B ✗ GPT-J gpt-j-6b 6B ✗ GPT-NeoX gpt-neox-20b 20B ✗ InstructGPT gpt-3.5-turbo - ✓
Table 4: Language Models used in the paper.Exact model size of gpt-3.5-turbo is unknown.</p>
<p>Evaluation</p>
<p>We evaluate the models on well-known metrics for link prediction: Hits@k, with k = 1, 3, 10.Following (Gastinger et al., 2022), we report our results in two evaluation settings: 1) Raw retrieves the sorted scores of candidate entities for a given query quadruple and calculates the rank of the correct entity; and 2) Time-aware filter also retrieves the sorted scores but removes the entities that are valid predictions before calculating the rank, preventing them from being considered errors.To illustrate, if the test query is (NBA, Clinch Playoff, ?, 2023) and the true answer is Los Angeles Lakers, there may exist other valid predictions such as (NBA, Clinch Playoff, Milwaukee Bucks, 2023) or (NBA, Clinch Playoff, Boston Celtics, 2023).In such cases, the time-aware filter removes these valid predictions, allowing for accurate determination of the rank of the "Los Angeles Lakers."In this paper, we present performance with the time-aware filter.</p>
<p>Models.</p>
<p>As shown in Table 4, we perform experiments on four language model families.Among those, three are open-sourced: GPT2 (Radford et al., 2019), GPT-J (Wang, 2021), and GPT-NeoX (Black et al., 2022).All models employ the GPT-2 byte level BPE tokenizer (Radford et al., 2019) with nearly identical vocabulary size.In addition, we use the gpt-3.5-turbomodel to analyze the performance of the instruction-tuned models.However, we do not directly compare this model to other models in terms of size since the actual model size is unknown.As for the TKG baselines, (i.e., RE-Net (Jin et al., 2020), RE-GCN (Li et al., 2021), TANGO (Han et al., 2021b), xERTE (Han et al., 2021a), TimeTraveler (Sun et al., 2021), CyGNet (Zhu et al., 2021), and TLogic (Liu et al., 2022)), we report the numbers presented in prior research (Gastinger et al., 2022).Appendix A.4 provides more details on baseline models.</p>
<p>Single-Step</p>
<p>Train YAGO WIKI ICEWS14 ICEWS18 ACLED-CD22</p>
<p>H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10  4.4 ICL Implementation Details.
RE-GCN ✓ 0.
We implement our frameworks using Py-Torch (Paszke et al., 2019) and Huggingface (Wolf et al., 2020).We first collate the facts f ∈ D test based on the identical test query to eliminate any repeated inference.To illustrate, suppose there exist two facts in the test set denoted as (s, p, a, t) and (s, p, b, t) in the object prediction scenario.</p>
<p>We consolidate these facts into (s, p, [a, b], t) and forecast only one for (s, p, ?, t).Subsequently, we proceed to generate an output for each test query with history by utilizing the model, obtaining the probability for the first generated token in a greedy approach, and sorting the probability.The outputs are deterministic for every iteration.</p>
<p>We retain the numerical tokens corresponding to the numerical label n that was targeted, selected from the top 100 probability tokens for each test query.To facilitate multi-step prediction, we incorporate the top-k predictions of each test query as supplementary reference history.In this paper, we present results with k = 1.It is important to acknowledge that the prediction may contain minimal or no numerical tokens as a result of inadequate in-context learning.This can lead to problems when evaluating rank-based metrics.To mitigate this, we have established a protocol where the rank of the actual value within the predictions is assigned a value of 100, which is considered incorrect according to our evaluation metric.</p>
<p>For instruction-tuned model, we use the manual curated system instructions in Appendix A.3.</p>
<p>Experimental Results</p>
<p>In-context learning for TKG Forecasting</p>
<p>In this section, we present a multifaceted performance analysis of ICL under Index &amp; Unidirection prompt strategy for both Entity and Pair history.</p>
<p>Q1 we run a comparative analysis between GPT-NeoX and heuristic-rules (i.e., frequency &amp; recency) on the ICEWS14 dataset, with history length set to 100.frequency identifies the target that appears most frequently in the provided history while recency selects the target associated with the most recent fact in the provided history.The reason for our focus on ICEWS is that each quadruple represents a single distinct event in time.In contrast, the process of constructing YAGO and WIKI involves converting durations to two timestamps to display events across the timeline.This step has resulted in recency heuristics outperforming all of the existing models, showcasing the shortcoming of existing TKG benchmarks (See Appendix A.5).The experimental results presented in Table 6 demonstrate that ICL exhibits superior performance to rule-based baselines.This finding suggests that ICL does not solely rely on specific biases to make predictions, but rather it actually learns more sophisticated patterns from historical data.</p>
<p>Q3: How does ICL use the sequential and temporal information of events?To assess the ability of LLMs to comprehend the temporal information of historical events, we compare the performance of prompts with and without timestamps.Specifically, we utilize the original prompt format, "f t :[I(f s ), I(f r ), n fo .I(f o )]", and the time-removed prompt format, "[I(f s ), I(f r ), n fo .I(f o )]", make the comparison (See Appendix A.2). Additionally, we shuffle the historical facts in the time-removed prompt format to see how the model is affected by the corruption of sequential information.Figure 1 shows that the absence of time reference can lead to a deterioration in performance, while the random arrangement of historical events may further exacerbate this decline in performance.This observation implies that the model has the capability to forecast the subsequent event by comprehending the sequential order of events.</p>
<p>Single-Step Prompt ICEWS14</p>
<p>H@1 gpt-3.5-turboindex 0.1615 gpt-3.5-turbolexical 0.1858</p>
<p>Table 7: Performance (Hits@1) between index and lexical for gpt-3.5-turbo.</p>
<p>Q4: How does instruction-tuning affect ICL's performance?To investigate the impact of instruction-tuning on ICL, we employ the gpt-3.5-turbomodel with manually curated system instruction detailed in Appendix 4.4.Since the size of this model is not publicly disclosed, it is challenging to make direct comparisons with other models featured in this paper.Moreover, since this model does not provide output probabilities, we are only able to report the Hit@1 metric.Table 7 showcases that the performance of the lexical prompts exceeds that of the index prompts by 0.024, suggesting that instruction-tuned models can make better use of semantic priors.This behavior is different from the other foundation LLMs, where the performance gap between the two prompt types was insignificant (See Figure 4 (a)).</p>
<p>Q5: How does history length affect ICL's performance?To evaluate the impact of the history length provided in the prompt, we conduct a set of experiments using varying history lengths.For this purpose, we use the best performing prompt format for each benchmark, i.e., Entity for WIKI, YAGO, ICEWS18, and Pair for ICEWS14.Our results, as shown in Figure 2, indicate a consistent improvement in performance as the history length increases.This suggests that the models learn better as additional historical facts are presented.This observation is connected to few-shot learning in other domains, where performance improves as the number of examples per label increases.However, in our case, the historical patterns presented in the prompt do not explicitly depict the input-label mapping but rather aid in inferring the next step.</p>
<p>Q6: What is the relation between ICL's performance and model size?Here, we analyze the connection between model size and performance.Our results, as presented in Figure 3, conform to the expected trend of better performance with larger models.This finding aligns with prior works showing the scaling law of in-context learning performance.Our findings are still noteworthy since they  show how scaling model size can facilitate more powerful pattern inference for forecasting tasks.</p>
<p>Prompt Construction for TKG Forecasting</p>
<p>To determine the most effective prompt variation, we run a set of experiments on all prompt variations, using GPT-J (Wang, 2021) and under the singlestep setting.Comprehensive results for prompt variations can be found in Appendix A.5.</p>
<p>Index vs. Lexical Our first analysis compares the performance of index and lexical prompts.This investigation aims to determine whether the model relies solely on input-label mappings or if it also incorporates semantic priors from pre-training to make predictions.Our results (Figure 4 (a)) show that the performance is almost similar (±4e − 3 on average) across the datasets.This finding is aligned with previous studies indicating that foundation models depend more on input-label mappings and are minimally impacted by semantic priors (Wei et al., 2023).</p>
<p>Unidirectional vs. Bidirectional We next analyze how the relation direction in the history modeling impacts the performance.This analysis aims to ascertain whether including historical facts, where the query entity or pair appears in any position, can improve performance by offering a diverse array of historical facts.Our results (Figure 4 (b)) show that there is a slight decrease in performance when Bidirectional history is employed, with a significant drop in performance observed particularly in the ICEWS benchmarks.These observations may be attributed to the considerably more significant number of entities placed in both subject and object positions in ICEWS benchmarks than YAGO and WIKI benchmarks (See Appendix A.5).This finding highlights the necessity of having robust constraints on the historical data for ICL to comprehend the existing pattern better.Entity vs. Pair Finally, we examine the impact of the history retrieval query on performance.Our hypothesis posits that when the query is limited to a single entity, we can incorporate more diverse historical facts.Conversely, when the query is a pair, we can acquire a more focused set of historical facts related to the query.Our results (Figure 4 (c)) indicate that the performance of the model is dependent on the type of data being processed.Specifically, the WIKI and ICEWS18 benchmarks perform better when the query is focused on the entity, as a broader range of historical facts is available.In contrast, the ICEWS14 benchmark performs better when the query is focused on pairs, as the historical facts present a more focused pattern.</p>
<p>Related Works</p>
<p>Event Forecasting.Forecasting is a complex task that plays a crucial role in decision-making and safety across various domains (Hendrycks et al., 2021).To tackle this challenging task, researchers have explored various approaches, including statistical and judgmental forecasting (Webby and O'Connor, 1996;Armstrong, 2001;Zou et al., 2022).Statistical forecasting involves leveraging probabilistic models (Hyndman and Khandakar, 2008) or neural networks (Li et al., 2018;Sen et al., 2019) to predict trends over time-series data.While this method works well when there are many past observations and minimal distribution shifts, it is limited to numerical data and may not capture the underlying causal factors and dependencies that affect the outcome.On the other hand, judgmental forecasting involves utilizing diverse sources of information, such as news articles and external knowledge bases, to reason and predict future events.Recent works have leveraged language models to enhance reasoning capabilities when analyzing unstructured text data to answer forecasting inquiries (Zou et al., 2022;Jin et al., 2021).Temporal Knowledge Graph.Temporal knowledge graph (TKG) reasoning models are commonly employed in two distinct settings, namely interpolation and extrapolation, based on the facts available from t 0 to t n .(1) Interpolation aims to predict missing facts within this time range from t 0 to t n , and recent works have utilized embeddingbased algorithms to learn low-dimensional representations for entities and relations to score candidate facts (Leblay and Chekol, 2018;García-Durán et al., 2018;Goel et al., 2020;Lacroix et al., 2020);</p>
<p>(2) Extrapolation aims to predict future facts beyond t n .Recent studies have treated TKGs as a sequence of snapshots, each containing facts corresponding to a timestamp t i , and proposed solutions by modeling multi-relational interactions among entities and relations over these snapshots using graph neural networks (Jin et al., 2020;Li et al., 2021;Han et al., 2021b,a), reinforcement learning (Sun et al., 2021) or logical rules (Zhu et al., 2021;Liu et al., 2022).In our work, we focus on the extrapolation setting.</p>
<p>In-context Learning.In-context learning (ICL) has enabled LLMs to accomplish diverse tasks in a few-shot manner without needing parameter adjustments (Brown et al., 2020;Chowdhery et al., 2022).</p>
<p>In order to effectively engage in ICL, models can leverage semantic prior knowledge to accurately predict labels following the structure of in-context exemplars (Min et al., 2022;Razeghi et al., 2022;Xie et al., 2022;Chan et al., 2022;Hahn and Goyal, 2023), and learn the input-label mappings from the in-context examples presented (Wei et al., 2023).</p>
<p>To understand the mechanism of ICL, recent studies have explored the ICL capabilities of LLMs with regards to the impact of semantic prior knowl-edge by examining their correlation with training examples (Min et al., 2022;Razeghi et al., 2022;Xie et al., 2022), data distribution (Chan et al., 2022), and language compositionality (Hahn and Goyal, 2023)  showing the transformer models trained on specific linear function class is actually predicting accurately on new unseen linear functions (Garg et al., 2022).More recently, there is a finding that largeenough models can still do ICL using input-label mappings when semantic prior knowledge is not available (Wei et al., 2023).</p>
<p>Conclusion</p>
<p>In this paper, we examined the forecasting capabilities of in-context learning in large language models.To this end, we experimented with temporal knowledge graph forecasting benchmarks.We presented a framework that converts relevant historical facts into prompts and generates ranked link predictions through token probabilities.Our experimental results demonstrated that without any finetuning and only through ICL, LLMs exhibit comparable performance to current supervised TKG methods that incorporate explicit modules to capture structural and temporal information.We also discovered that using numerical indices instead of entity/relation names does not significantly affect the performance, suggesting that prior semantic knowledge is not critical for overall performance.Additionally, our analysis indicated that ICL helps the model learn irregular patterns from historical facts, beyond simply making predictions based on the most common or the most recent facts in the given context.Together, our results and analyses demonstrated that ICL can be a valuable tool for predicting future links using historical patterns, and also prompted further inquiry into the potential of ICL for additional capabilities.</p>
<p>Limitations</p>
<p>There are certain limitations to our experiments.First, computing resource constraints restrict our experiments to small-scale open-source models.Second, our methodologies have constraints regarding models where the tokenizer vocabulary comprises solely of single-digit numbers as tokens, such as LLAMA (Touvron et al., 2023).The performance of such models exhibits a similar trend in terms of scaling law concerning model size and history length, but these models demonstrate inferior performance compared to other models of the same model size.Third, our methodologies have certain limitations with respect to link prediction settings.</p>
<p>While real-world forecasting can be performed in the transductive setting, where the answer can be an unseen history, our approach is constrained to the inductive setting, where the answer must be one of the histories observed.There are further directions that can be pursued.The first is to explore transductive extrapolation link prediction using LLMs.</p>
<p>The second is to analyze the effects of fine-tuning on the results.Lastly, there is the opportunity to investigate the new capabilities of ICL.</p>
<p>A Appendix</p>
<p>A.1 Prompt Example</p>
<p>Given the test query at timestamp 571, prompt examples for Index and Lexical are shown in Figure 5. Here, we assume the entity dictionary contains "Islamist Militia (Mozambique)" as index 0, "Meluco" as 10, "Namatil" as 36, "Muatide" as 53, "Limala" as 54, and "Nacate" as 55, while relation dictionary contains "Battles" as index 1 and "Violence against civilians" as 4. Also, the history setting is unidirectional entity setting where the history length is set to 5.</p>
<p>A.2 Prompt Example for Analysis</p>
<p>To assess the ability of LLMs to comprehend the sequential information of historical events, we compare the performance of prompts with and without timestamps (See Section 5.1 Q3). Figure 6 shows the prompt examples for time-removed and shuffled version of prompts.A.3 System Instruction for Instruction-tuned models.</p>
<p>For the instruction-model, we use the manual curated system instructions to provide task descriptions and constraint the output format as follow:</p>
<p>You must be able to correctly predict the next {object_label} from a given text consisting of multiple quadruplets in the form of "{time}:[{subject}, {relation}, {object_label}.{object}]" and the query in the form of "{time}:[{subject}, {relation}," in the end.</p>
<p>You must generate only the single number for {object_label} without any explanation.</p>
<p>A.4 Baseline Models RE-Net (Jin et al., 2020) leverages an autoregressive architecture that employs a two-step process for learning temporal dependency from a sequence of graphs and local structural dependency from the vicinity.The model represents the likelihood of a fact occurring as a probability distribution that is conditioned on the sequential history of past snapshots.</p>
<p>RE-GCN (Li et al., 2021) also employs autoregressive architecture while it utilizes multi-layer relation-aware GCN on each graph snapshot to capture the structural dependencies among concurrent facts.Furthermore, the static properties of entities such as entity types, are also incorporated via a static graph constraint component to obtain better entity representations.</p>
<p>TANGO (Han et al., 2021b) employs autoregressive architecture as well but the use of continuous-time embedding in encoding temporal and structural information is a distinguishing feature of the proposed method, as opposed to RE-Net (Jin et al., 2020) (Li et al., 2021) and RE-GCN which operate on a discrete level with regards to time.</p>
<p>xERTE (Han et al., 2021a) employs an attention mechanism that can effectively capture the relevance of important aspects by selectively focusing on them.It employs a sequential reasoning approach over local subgraphs.This process begins with the query and iteratively selects relevant edges of entities within the subgraph, subsequently propagating attention along these edges.After multiple rounds of expansion, the final subgraph represents the interpretable reasoning path towards the predicted outcomes.</p>
<p>TimeTraveler (Sun et al., 2021) employs reinforcement learning for forecasting.The approach involves the use of an agent that navigates through historical knowledge graph snapshots, commencing from the query subject node.Thereafter, it sequentially moves to a new node by leveraging temporal facts that are linked to the current node, with the ultimate objective of halting at the answer node.</p>
<p>To accommodate the issue of unseen-timestamp, the approach incorporates a relative time encoding function that captures time-related information when making decisions.</p>
<p>CyGNet (Zhu et al., 2021) leverages the statistical relevance of historical facts, acknowledging the recurrence of events in the temporal knowledge graph datasets.It incorporates two inference modes, namely Copy and Generation.The Copy mode determines the likelihood of the query being a repetition of relevant past facts.On the other hand, the Generation mode estimates the probability of each potential candidate being the correct prediction, using a linear classifier.The final forecast is obtained by aggregating the outputs of both modes.</p>
<p>TLogic (Liu et al., 2022) mines cyclic temporal logical rules by extracting temporal random walks from a graph.This process involves the extraction of temporal walks from the graph, followed by a lift to a more abstract, semantic level, resulting in the derivation of temporal rules that can generalize to new data.Subsequently, the application of these rules generates answer candidates, with the body groundings in the graph serving as explicit and easily comprehensible explanations for the results obtained.</p>
<p>A.5 Full Experimental Results</p>
<p>Figure 2 :
2
Figure 2: Performance (Hit@1) adheres to the scaling law based on the history length.</p>
<p>Figure 3 :
3
Figure 3: Performance (Hit@1) adheres to the scaling law based on the model size.</p>
<p>Entity vs. Pair</p>
<p>Figure 4 :
4
Figure4: Performance (Hit@1) Analysis on Prompt Variation.The comparable performance exhibited by both the Index and Lexical models indicates that these models rely heavily on learning patterns and are less influenced by semantic priors.Moreover, the Unidirectional model typically outperforms the Bidirectional model, suggesting that the robust constraints on historical data enable the model to comprehend observed patterns better.Finally, the performance of the Entity and Pair models varies depending on the dataset.</p>
<p>Figure 5 :
5
Figure 5: Prompt examples for Index and Lexical settings.</p>
<p>Figure 6 :
6
Figure 6: Prompt examples for time-removed and shuffled version.</p>
<p>Table 2 :
2
Prompt Example.
Category Prompt2000: [Superbowl, Champion, 0. St Louis]Lexical L(•)2001: [Superbowl, Champion, 1. Baltimore] . . . 2023: [Superbowl, Champion,2000: [0, 0, 0. 0]Index I(•)2001: [0, 0, 1. 1] . . . 2023: [0, 0,</p>
<p>Table 3 :
3
Data statistics.Each dataset consists of entities, relations, and historical facts, with the facts within the same time interval identified by the same timestamp.The facts are divided into three subsets based on time, where train &lt; valid &lt; test.
In this setting, for each test query,the model is provided with ground truth facts frompast timestamps in the test period. Hence, aftermaking predictions for a test query in a specific</p>
<p>Table 5 :
5
Performance (Hits@K) comparison between supervised models and ICL for single-step (top) and multistep (bottom) prediction.The first group in each table consists of supervised models, whereas the second group consists of ICL models, i.e., GPT-NeoX with a history length of 100.The best model for each dataset in the first group is shown in bold, and the second best is underlined.
RE-Net CyGNet TLogic✓ ✓ ✓717 0.776 0.534 0.613 0.613 0.742 0.631 0.7060.817 0.662 0.834 0.7150.594 0.648 0.472 0.507 0.525 0.624 0.613 0.6630.678 0.530 0.675 0.6820.278 0.421 0.278 0.408 0.266 0.402 0.265 0.3950.575 0.549 0.545 0.5310.195 0.326 0.184 0.314 0.166 0.295 0.155 0.2720.475 0.461 0.444 0.4120.421 0.464 0.238 0.445 0.408 0.500 0.009 0.0450.502 0.563 0.588 0.094GPT-NeoX (Entity) GPT-NeoX (Pair)✗ ✗0.686 0.793 0.688 0.7930.840 0.8390.543 0.622 0.570 0.6250.655 0.6520.247 0.363 0.236 0.3240.471 0.3950.136 0.224 0.155 0.2450.321 0.3310.319 0.417 0.289 0.4100.500 0.464Single-StepICEWS14 H@1 H@3 H@10 H@1 H@3 H@10 ICEWS18Multi-StepICEWS14 H@1 H@3 H@10 H@1 H@3 H@10 ICEWS18frequency recency0.243 0.387 0.228 0.3870.532 0.5360.141 0.265 0.120 0.2420.409 0.403frequency recency0.222 0.349 0.151 0.2680.460 0.4230.121 0.207 0.074 0.1490.307 0.266GPT-NeoX (Entity) 0.324 0.460 GPT-NeoX (Pair) 0.297 0.4080.565 0.4820.192 0.313 0.196 0.3070.414 0.402GPT-NeoX (Entity) 0.247 0.363 GPT-NeoX (Pair) 0.236 0.3240.471 0.3950.136 0.224 0.155 0.2450.321 0.331(a) Single-step(b) Multi-step</p>
<p>Table 6 :
6
Performance (Hits@K) with rule-based predictions.The best model for each dataset is shown in bold.</p>
<p>in the pre-training corpus.Other recent works show that LLMs can actually learn input-label mappings from in-context examples by</p>
<dl>
<dt>https://github.com/usc-isi-i2/isi-tkg-icl</dt>
<dd>[Superbowl, Champion, St Louis]</dd>
<dd>[Superbowl, Champion, Baltimore]</dd>
<dd>[Superbowl, Champion, New England]</dd>
<dd>[Superbowl, Champion, Tampa Bay]
AcknowledgementThis work was funded in part by the Defense Advanced Research Projects Agency (DARPA) and Army Research Office (ARO) under Contract No. W911NF-21-C-0002 and Contract No. HR00112290106, and with support from the Keston Exploratory Research Award and Amazon.The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, ARO or the U.S. Government.PromptHistory YAGO WIKI ICEWS14 ICEWS18 ACLEDH@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 Index Unidirectional Entity 0.777 0. H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 frequency 0.766 0.859 0. H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 H@1 H@3 H@10 frequency 0.
Jon Scott, Armstrong , Principles of forecasting: a handbook for researchers and practitioners. Springer200130</dd>
</dl>
<p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle Mcdonell, Jason Phang, arXiv:2204.06745Gpt-neox-20b: An open-source autoregressive language model. 2022arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Data distributional properties drive emergent in-context learning in transformers. C Y Stephanie, Adam Chan, Andrew Santoro, Jane X Kyle Lampinen, Aaditya K Wang, Pierre Singh, James Harvey Richemond, Felix Mcclelland, Hill, Advances in Neural Information Processing Systems. 2022</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Learning sequence encoders for temporal knowledge graph completion. Alberto García-Durán, Sebastijan Dumančić, Mathias Niepert, 10.18653/v1/D18-1516Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>What can transformers learn in-context? a case study of simple function classes. Shivam Garg, Dimitris Tsipras, Percy S Liang, Gregory Valiant, Advances in Neural Information Processing Systems. 202235</p>
<p>On the evaluation of methods for temporal knowledge graph forecasting. Julia Gastinger, Timo Sztyler, Lokesh Sharma, Anett Schuelke, NeurIPS 2022 Temporal Graph Learning Workshop. 2022</p>
<p>Diachronic embedding for temporal knowledge graph completion. Rishab Goel, Seyed Mehran Kazemi, Marcus Brubaker, Pascal Poupart, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>A theory of emergent in-context learning as implicit structure induction. Michael Hahn, Navin Goyal, arXiv:2303.079712023arXiv preprint</p>
<p>Explainable subgraph reasoning for forecasting on temporal knowledge graphs. Zhen Han, Peng Chen, Yunpu Ma, Volker Tresp, International Conference on Learning Representations. 2021a</p>
<p>Learning neural ordinary equations for forecasting future links on temporal knowledge graphs. Zhen Han, Zifeng Ding, Yunpu Ma, Yujia Gu, Volker Tresp, 10.18653/v1/2021.emnlp-main.658Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021bOnline and Punta Cana</p>
<p>Unsolved problems in ml safety. Dan Hendrycks, Nicholas Carlini, John Schulman, Jacob Steinhardt, arXiv:2109.139162021arXiv preprint</p>
<p>Automatic time series forecasting: the forecast package for r. J Rob, Yeasmin Hyndman, Khandakar, Journal of statistical software. 272008</p>
<p>ForecastQA: A question answering challenge for event forecasting with temporal text data. Woojeong Jin, Rahul Khanna, Suji Kim, Dong-Ho Lee, Fred Morstatter, Aram Galstyan, Xiang Ren, 10.18653/v1/2021.acl-long.357Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Recurrent event network: Autoregressive structure inferenceover temporal knowledge graphs. Woojeong Jin, Meng Qu, Xisen Jin, Xiang Ren, 10.18653/v1/2020.emnlp-main.541Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Tensor decompositions for temporal knowledge base completion. Timothée Lacroix, Guillaume Obozinski, Nicolas Usunier, International Conference on Learning Representations. 2020</p>
<p>Deriving validity time in knowledge graph. Julien Leblay, Melisachew Wudage, Chekol , Companion proceedings of the the web. 2018. 2018</p>
<p>Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. Yaguang Li, Rose Yu, Cyrus Shahabi, Yan Liu, International Conference on Learning Representations. 2018</p>
<p>Temporal knowledge graph reasoning based on evolutional representation learning. Zixuan Li, Xiaolong Jin, Wei Li, Saiping Guan, Jiafeng Guo, Huawei Shen, Yuanzhuo Wang, Xueqi Cheng, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval2021</p>
<p>Teaching models to express their uncertainty in words. Stephanie Lin, Jacob Hilton, Owain Evans, Transactions on Machine Learning Research. 2022</p>
<p>Tlogic: Temporal logical rules for explainable link forecasting on temporal knowledge graphs. Yushan Liu, Yunpu Ma, Marcel Hildebrandt, Mitchell Joblin, Volker Tresp, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Yago3: A knowledge base from multilingual wikipedias. Farzaneh Mahdisoltani, Joanna Biega, Fabian Suchanek, 7th biennial conference on innovative data systems research. CIDR Conference. 2014</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Pytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, Canada2019. 2019. 2019. December 8-14, 2019</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Impact of pretraining term frequencies on few-shot numerical reasoning. Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting. Rajat Sen, Hsiang-Fu Yu, Inderjit S Dhillon ; Haohai Sun, Jialun Zhong, Yunpu Ma, Zhen Han, Kun He, 10.18653/v1/2021.emnlp-main.655Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2019. 202132Online and Punta CanaTimeTraveler: Reinforcement learning for temporal knowledge graph forecasting</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Ben Wang, Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX. 2021</p>
<p>Judgemental and statistical time series forecasting: a review of the literature. Richard Webby, O' Marcus, Connor, International Journal of forecasting. 1211996</p>
<p>Larger language models do in-context learning differently. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, arXiv:2303.038462023arXiv preprint</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020</p>
<p>An explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, International Conference on Learning Representations. 2022</p>
<p>Calibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, International Conference on Machine Learning. PMLR2021</p>
<p>Learning from history: Modeling temporal knowledge graphs with sequential copy-generation networks. Cunchao Zhu, Muhao Chen, Changjun Fan, Guangquan Cheng, Yan Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Forecasting future world events with neural networks. Andy Zou, Tristan Xiao, Ryan Jia, Joe Kwon, Mantas Mazeika, Richard Li, Dawn Song, Jacob Steinhardt, Owain Evans, Dan Hendrycks, Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 20225680, 4, 1. 55. 0, 4, 3. 10. 0, 1, 2. 36. 0, 1, 0. 54. 0, 1, 4. 53. 0, 1, (a) Index [0, 4, 1. 55. 0, 4, 3. 10] [0, 1, 2. 36] [0, 1, 0. 54. 0, 1, 4. 53. 0, 1, (b) Time-removed [0, 1, 0. 54. 0, 4, 3. 10. 0, 4, 1. 55. 0, 1, 4. 53] [0, 1, 2. 36] [0, 1, (c) Time-removed + Shuffle</p>            </div>
        </div>

    </div>
</body>
</html>
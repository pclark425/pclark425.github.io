<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9282 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9282</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9282</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-263908924</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.08523v1.pdf" target="_blank">LLM-augmented Preference Learning from Natural Language</a></p>
                <p><strong>Paper Abstract:</strong> Finding preferences expressed in natural language is an important but challenging task. State-of-the-art(SotA) methods leverage transformer-based models such as BERT, RoBERTa, etc. and graph neural architectures such as graph attention networks. Since Large Language Models (LLMs) are equipped to deal with larger context lengths and have much larger model sizes than the transformer-based model, we investigate their ability to classify comparative text directly. This work aims to serve as a first step towards using LLMs for the CPC task. We design and conduct a set of experiments that format the classification task into an input prompt for the LLM and a methodology to get a fixed-format response that can be automatically evaluated. Comparing performances with existing methods, we see that pre-trained LLMs are able to outperform the previous SotA models with no fine-tuning involved. Our results show that the LLMs can consistently outperform the SotA when the target text is large -- i.e. composed of multiple sentences --, and are still comparable to the SotA performance in shorter text. We also find that few-shot learning yields better performance than zero-shot learning.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9282.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9282.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>few-shot vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot versus Zero-shot Prompting (in-paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of in-context example-based (few-shot) prompts versus no-example (zero-shot) prompts for LLM preference classification across datasets and models; few-shot generally improved accuracy and output-format conformity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; GPT-3.5-Turbo; LLaMa-2-70B; LLaMa-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown; ~175B (GPT-3.5-Turbo reported); 70B; 13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Comparative Preference Classification (CPC) on College Confidential and Compsent-19</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a text and two alternatives A and B, classify whether A>B, B>A, equal, or no preference (paper focuses on A>B, B>A, N/A). Two benchmark datasets used: College Confidential (multi-sentence discussion posts) and Compsent-19 (single-sentence comparative statements).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot: instruction + input with no examples. Few-shot: instruction + one example per label (examples chosen with >100 words when possible) included inline in prompt; two prompt variants (long/detailed and short/concise) were used in both zero- and few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct comparison between zero-shot and few-shot for each prompt variant (short & long) across datasets and models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Few-shot prompts yielded the best performance in most model/dataset combinations; few-shot short prompt produced the best performance in most cases reported in the paper. (No numeric F1s provided in text.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative: Zero-shot performance was consistently worse than few-shot; exception: for LLaMa-2-70B on Compsent-19 the zero-shot long prompt could beat the zero-shot short prompt. (Paper does not report absolute numeric deltas in main text.)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The authors attribute few-shot improvements to in-context learning: examples show correct outputs and output-format, improving both classification accuracy and response conformity to the prescribed output phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot used one example per label selected from dataset (examples with >100 words, minimum length). Examples excluded from test set. Both 'short' and 'long' prompts tested with temperature/top_p tuned per prompt (short: temp=1 top_p=0.7; long: temp=0.7 top_p=0.1).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9282.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9282.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>short vs long prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Short (concise) versus Long (detailed) Prompt Wording</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of two prompt formulations: a concise short prompt and a detailed long prompt with explicit rules and role/context; effects differ by dataset length and model size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; GPT-3.5-Turbo; LLaMa-2-70B; LLaMa-2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown; ~175B; 70B; 13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Comparative Preference Classification (CPC) on College Confidential and Compsent-19</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same CPC task on two datasets (multi-sentence College Confidential; single-sentence Compsent-19).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Long (detailed) prompt: role specification, many explicit output rules, conversational tone variants, and extended context/instruction; Short (concise) prompt: shorter instruction set with the same required fixed output phrases and triple-backtick-delimited inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct comparison between long and short prompts in zero-shot and few-shot settings, across models and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Few-shot short prompt produced best performance in most model/dataset cases overall; however, for short-input dataset (Compsent-19) zero-shot LLaMa-2-70B performed better with the long prompt than the short prompt, and few-shot long prompt with GPT-4 outperformed few-shot short prompt on Compsent-19.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Detailed instructions help when the input text is short; verbose instructions' benefits can be diminished when combined with a long input (prompt length tradeoff). GPT-4 appears better at handling complex/verbose instructions, so longer prompts can benefit stronger models, especially on short input texts.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Two prompt templates were used and named 'long' (detailed rules/context) and 'short' (paraphrased concise). The authors tuned temperature/top_p separately per prompt type (short: temp=1 top_p=0.7; long: temp=0.7 top_p=0.1). Few-shot and zero-shot tested with both prompt lengths.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9282.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9282.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>retry prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retry Prompting (Iterative Format Correction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage conversational retry technique that appends the model's previous malformed response to the conversation history and prompts it again to produce output strictly matching prescribed format, used to salvage otherwise correct but misformatted outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMa-2-70B; GPT-4; GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B; unknown; ~175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Comparative Preference Classification (CPC) on College Confidential and Compsent-19</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>CPC classification where outputs must exactly match one of a small set of phrases to allow automatic evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Initial standard prompt (short or long). If the model's output is syntactically malformed (does not exactly match required phrase), append the original user message and the malformed assistant message to the conversation and send a 'retry' user message that reiterates the formatting rules and requests only the allowed phrase.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Retry prompting recovered many outputs that were semantically correct but syntactically nonconforming, enabling automatic evaluation; LLaMa-2-70B required retries on a majority of tasks while GPT-4 required retries only on a handful of cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Because LLMs can produce correct answers in varied wording, explicitly reminding them and providing their prior malformed output as context helps them reformat to the exact allowed phrase; retrying is an effective operational workaround when models are produce correct content but not in fixed format.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Retry messages reused the exact rules and instructions; triggered whenever output deviated from the prescribed phrases (e.g., produced explanation rather than the single allowed phrase). The paper shows this technique was necessary to obtain machine-evaluable output from many runs, especially for LLaMa-2 models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9282.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9282.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>summarize-then-classify</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Summarization Pre-step before Classification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage prompting approach where the LLM first summarizes the input text (explicitly ensuring mention of the two alternatives), and then the summary is used as input for the preference classification prompt; evaluated versus direct classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; LLaMa-2-70B (experimented)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown; 70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Comparative Preference Classification (CPC) on College Confidential</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>CPC on multi-sentence forum posts; tested whether an intermediate summarization improves classification when inputs are long.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Two-stage: (1) prompt model to summarize preference while mentioning both alternatives, (2) feed summary into classification prompt. Compared to single-stage direct classification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct (no summary) classification versus summarize-then-classify pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Summarization pre-step did not improve overall performance on the College Confidential dataset and performed worse overall than the direct single-stage prompt; summary approach only showed benefit for very long texts ( >400 words) per analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The summary step may only help when source texts are very long; College Confidential posts were typically shorter than threshold, so summarization introduced loss of signal. Authors hypothesize summarization could benefit longer documents but requires careful prompt design; they attempted prompts but were unable to find summarization prompts that outperformed direct prompting on this corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>The summary prompt was instructed to include names of the two colleges; authors compared direct single-stage classification to the summarize-then-classify pipeline and found no overall gain; per-text-length analysis suggested benefits only for texts >400 words.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9282.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9282.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>delimiters & tone</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Input Delimiters, Instruction Tone, and Rule Formatting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Design choices for prompt formatting: use of uncommon delimiters (triple backticks), capitalization of modal verbs, and conversational yet directive tone to improve instruction adherence and output conformity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4; LLaMa-2-70B; LLaMa-2 (instruct fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown; 70B; 13B/70B variants</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Comparative Preference Classification (CPC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>CPC task where inputs (two alternatives and a comment) are embedded inside a larger instruction prompt; prompt formatting choices evaluated qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Triple-backtick delimiters (```...```) around the two alternatives and the comment. Instruction rules written as short, capitalized requirements (e.g., 'You MUST ...'), with a conversational closing sentence. Avoided delimiters that conflicted with LLaMa-2 instruct templates (e.g., triple-hash).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared delimiter choices (triple-backtick vs triple-hash) and instruction wording (imperative 'Do X' vs 'You MUST X' and conversational phrasing).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Triple-backtick delimiters and capitalized/modal 'MUST' rules increased consistency and helped models separate instructions from data; conversational tone helped rule following. GPT handled other delimiters but they clashed with some LLaMa-2 templates, so triple-backticks chosen for consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Using delimiters that do not appear in normal text reduces model confusion about instruction vs data. Explicit, capitalized directives reduce inconsistent responses. A conversational close reduces rigidness and improved adherence for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Authors report qualitative prompt-engineering practices derived from experiments: delimiters, capitalized MUST statements, and phrasing. These choices were used in the final long and short prompts deployed for experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9282.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9282.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>chain-of-thought (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step internal reasoning from LLMs; cited as a relevant technique in related work but not directly applied in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mentioned as prior art showing improved reasoning on arithmetic/reasoning tasks by eliciting intermediate reasoning steps; not used in the CPC experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Mentioned as a prompting style (chain-of-thought) from related work; not implemented in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Referenced as prior work showing chain-of-thought can improve performance on reasoning tasks, suggesting possible relevance to more complex NLP tasks but no claims or tests in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Cited (Wei et al., and Kojima et al.) in related work; the authors did not run chain-of-thought prompting experiments for CPC in this study.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Models Are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>Large Language Models Are Zero-Shot Reasoners <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Categorizing Comparative Sentences <em>(Rating: 2)</em></li>
                <li>Dependency and Coreference-boosted Multi-Sentence Preference model <em>(Rating: 2)</em></li>
                <li>Entity-aware dependency-based deep graph attention network for comparative preference classification <em>(Rating: 2)</em></li>
                <li>TabLLM: Few-shot Classification of Tabular Data with Large Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9282",
    "paper_id": "paper-263908924",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "few-shot vs zero-shot",
            "name_full": "Few-shot versus Zero-shot Prompting (in-paper)",
            "brief_description": "Comparison of in-context example-based (few-shot) prompts versus no-example (zero-shot) prompts for LLM preference classification across datasets and models; few-shot generally improved accuracy and output-format conformity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4; GPT-3.5-Turbo; LLaMa-2-70B; LLaMa-2-13B",
            "model_size": "unknown; ~175B (GPT-3.5-Turbo reported); 70B; 13B",
            "task_name": "Comparative Preference Classification (CPC) on College Confidential and Compsent-19",
            "task_description": "Given a text and two alternatives A and B, classify whether A&gt;B, B&gt;A, equal, or no preference (paper focuses on A&gt;B, B&gt;A, N/A). Two benchmark datasets used: College Confidential (multi-sentence discussion posts) and Compsent-19 (single-sentence comparative statements).",
            "presentation_format": "Zero-shot: instruction + input with no examples. Few-shot: instruction + one example per label (examples chosen with &gt;100 words when possible) included inline in prompt; two prompt variants (long/detailed and short/concise) were used in both zero- and few-shot settings.",
            "comparison_format": "Direct comparison between zero-shot and few-shot for each prompt variant (short & long) across datasets and models.",
            "performance": "Qualitative: Few-shot prompts yielded the best performance in most model/dataset combinations; few-shot short prompt produced the best performance in most cases reported in the paper. (No numeric F1s provided in text.)",
            "performance_comparison": "Qualitative: Zero-shot performance was consistently worse than few-shot; exception: for LLaMa-2-70B on Compsent-19 the zero-shot long prompt could beat the zero-shot short prompt. (Paper does not report absolute numeric deltas in main text.)",
            "format_effect_size": null,
            "explanation_or_hypothesis": "The authors attribute few-shot improvements to in-context learning: examples show correct outputs and output-format, improving both classification accuracy and response conformity to the prescribed output phrasing.",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot used one example per label selected from dataset (examples with &gt;100 words, minimum length). Examples excluded from test set. Both 'short' and 'long' prompts tested with temperature/top_p tuned per prompt (short: temp=1 top_p=0.7; long: temp=0.7 top_p=0.1).",
            "uuid": "e9282.0"
        },
        {
            "name_short": "short vs long prompt",
            "name_full": "Short (concise) versus Long (detailed) Prompt Wording",
            "brief_description": "Evaluation of two prompt formulations: a concise short prompt and a detailed long prompt with explicit rules and role/context; effects differ by dataset length and model size.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4; GPT-3.5-Turbo; LLaMa-2-70B; LLaMa-2-13B",
            "model_size": "unknown; ~175B; 70B; 13B",
            "task_name": "Comparative Preference Classification (CPC) on College Confidential and Compsent-19",
            "task_description": "Same CPC task on two datasets (multi-sentence College Confidential; single-sentence Compsent-19).",
            "presentation_format": "Long (detailed) prompt: role specification, many explicit output rules, conversational tone variants, and extended context/instruction; Short (concise) prompt: shorter instruction set with the same required fixed output phrases and triple-backtick-delimited inputs.",
            "comparison_format": "Direct comparison between long and short prompts in zero-shot and few-shot settings, across models and datasets.",
            "performance": "Qualitative: Few-shot short prompt produced best performance in most model/dataset cases overall; however, for short-input dataset (Compsent-19) zero-shot LLaMa-2-70B performed better with the long prompt than the short prompt, and few-shot long prompt with GPT-4 outperformed few-shot short prompt on Compsent-19.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Detailed instructions help when the input text is short; verbose instructions' benefits can be diminished when combined with a long input (prompt length tradeoff). GPT-4 appears better at handling complex/verbose instructions, so longer prompts can benefit stronger models, especially on short input texts.",
            "null_or_negative_result": false,
            "experimental_details": "Two prompt templates were used and named 'long' (detailed rules/context) and 'short' (paraphrased concise). The authors tuned temperature/top_p separately per prompt type (short: temp=1 top_p=0.7; long: temp=0.7 top_p=0.1). Few-shot and zero-shot tested with both prompt lengths.",
            "uuid": "e9282.1"
        },
        {
            "name_short": "retry prompting",
            "name_full": "Retry Prompting (Iterative Format Correction)",
            "brief_description": "A two-stage conversational retry technique that appends the model's previous malformed response to the conversation history and prompts it again to produce output strictly matching prescribed format, used to salvage otherwise correct but misformatted outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMa-2-70B; GPT-4; GPT-3.5-Turbo",
            "model_size": "70B; unknown; ~175B",
            "task_name": "Comparative Preference Classification (CPC) on College Confidential and Compsent-19",
            "task_description": "CPC classification where outputs must exactly match one of a small set of phrases to allow automatic evaluation.",
            "presentation_format": "Initial standard prompt (short or long). If the model's output is syntactically malformed (does not exactly match required phrase), append the original user message and the malformed assistant message to the conversation and send a 'retry' user message that reiterates the formatting rules and requests only the allowed phrase.",
            "comparison_format": null,
            "performance": "Qualitative: Retry prompting recovered many outputs that were semantically correct but syntactically nonconforming, enabling automatic evaluation; LLaMa-2-70B required retries on a majority of tasks while GPT-4 required retries only on a handful of cases.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Because LLMs can produce correct answers in varied wording, explicitly reminding them and providing their prior malformed output as context helps them reformat to the exact allowed phrase; retrying is an effective operational workaround when models are produce correct content but not in fixed format.",
            "null_or_negative_result": false,
            "experimental_details": "Retry messages reused the exact rules and instructions; triggered whenever output deviated from the prescribed phrases (e.g., produced explanation rather than the single allowed phrase). The paper shows this technique was necessary to obtain machine-evaluable output from many runs, especially for LLaMa-2 models.",
            "uuid": "e9282.2"
        },
        {
            "name_short": "summarize-then-classify",
            "name_full": "Summarization Pre-step before Classification",
            "brief_description": "A two-stage prompting approach where the LLM first summarizes the input text (explicitly ensuring mention of the two alternatives), and then the summary is used as input for the preference classification prompt; evaluated versus direct classification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4; LLaMa-2-70B (experimented)",
            "model_size": "unknown; 70B",
            "task_name": "Comparative Preference Classification (CPC) on College Confidential",
            "task_description": "CPC on multi-sentence forum posts; tested whether an intermediate summarization improves classification when inputs are long.",
            "presentation_format": "Two-stage: (1) prompt model to summarize preference while mentioning both alternatives, (2) feed summary into classification prompt. Compared to single-stage direct classification.",
            "comparison_format": "Direct (no summary) classification versus summarize-then-classify pipeline.",
            "performance": "Qualitative: Summarization pre-step did not improve overall performance on the College Confidential dataset and performed worse overall than the direct single-stage prompt; summary approach only showed benefit for very long texts ( &gt;400 words) per analysis.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "The summary step may only help when source texts are very long; College Confidential posts were typically shorter than threshold, so summarization introduced loss of signal. Authors hypothesize summarization could benefit longer documents but requires careful prompt design; they attempted prompts but were unable to find summarization prompts that outperformed direct prompting on this corpus.",
            "null_or_negative_result": true,
            "experimental_details": "The summary prompt was instructed to include names of the two colleges; authors compared direct single-stage classification to the summarize-then-classify pipeline and found no overall gain; per-text-length analysis suggested benefits only for texts &gt;400 words.",
            "uuid": "e9282.3"
        },
        {
            "name_short": "delimiters & tone",
            "name_full": "Input Delimiters, Instruction Tone, and Rule Formatting",
            "brief_description": "Design choices for prompt formatting: use of uncommon delimiters (triple backticks), capitalization of modal verbs, and conversational yet directive tone to improve instruction adherence and output conformity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4; LLaMa-2-70B; LLaMa-2 (instruct fine-tuned)",
            "model_size": "unknown; 70B; 13B/70B variants",
            "task_name": "Comparative Preference Classification (CPC)",
            "task_description": "CPC task where inputs (two alternatives and a comment) are embedded inside a larger instruction prompt; prompt formatting choices evaluated qualitatively.",
            "presentation_format": "Triple-backtick delimiters (```...```) around the two alternatives and the comment. Instruction rules written as short, capitalized requirements (e.g., 'You MUST ...'), with a conversational closing sentence. Avoided delimiters that conflicted with LLaMa-2 instruct templates (e.g., triple-hash).",
            "comparison_format": "Compared delimiter choices (triple-backtick vs triple-hash) and instruction wording (imperative 'Do X' vs 'You MUST X' and conversational phrasing).",
            "performance": "Qualitative: Triple-backtick delimiters and capitalized/modal 'MUST' rules increased consistency and helped models separate instructions from data; conversational tone helped rule following. GPT handled other delimiters but they clashed with some LLaMa-2 templates, so triple-backticks chosen for consistency.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Using delimiters that do not appear in normal text reduces model confusion about instruction vs data. Explicit, capitalized directives reduce inconsistent responses. A conversational close reduces rigidness and improved adherence for some models.",
            "null_or_negative_result": false,
            "experimental_details": "Authors report qualitative prompt-engineering practices derived from experiments: delimiters, capitalized MUST statements, and phrasing. These choices were used in the final long and short prompts deployed for experiments.",
            "uuid": "e9282.4"
        },
        {
            "name_short": "chain-of-thought (mention)",
            "name_full": "Chain-of-Thought Prompting (mentioned in related work)",
            "brief_description": "A prompting technique that elicits step-by-step internal reasoning from LLMs; cited as a relevant technique in related work but not directly applied in the paper's experiments.",
            "citation_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": null,
            "task_name": "",
            "task_description": "Mentioned as prior art showing improved reasoning on arithmetic/reasoning tasks by eliciting intermediate reasoning steps; not used in the CPC experiments.",
            "presentation_format": "Mentioned as a prompting style (chain-of-thought) from related work; not implemented in this paper's experiments.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Referenced as prior work showing chain-of-thought can improve performance on reasoning tasks, suggesting possible relevance to more complex NLP tasks but no claims or tests in this paper.",
            "null_or_negative_result": null,
            "experimental_details": "Cited (Wei et al., and Kojima et al.) in related work; the authors did not run chain-of-thought prompting experiments for CPC in this study.",
            "uuid": "e9282.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Models Are Few-Shot Learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Large Language Models Are Zero-Shot Reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Categorizing Comparative Sentences",
            "rating": 2,
            "sanitized_title": "categorizing_comparative_sentences"
        },
        {
            "paper_title": "Dependency and Coreference-boosted Multi-Sentence Preference model",
            "rating": 2,
            "sanitized_title": "dependency_and_coreferenceboosted_multisentence_preference_model"
        },
        {
            "paper_title": "Entity-aware dependency-based deep graph attention network for comparative preference classification",
            "rating": 2,
            "sanitized_title": "entityaware_dependencybased_deep_graph_attention_network_for_comparative_preference_classification"
        },
        {
            "paper_title": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models",
            "rating": 1,
            "sanitized_title": "tabllm_fewshot_classification_of_tabular_data_with_large_language_models"
        }
    ],
    "cost": 0.01324875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLM-augmented Preference Learning from Natural Language
12 Oct 2023</p>
<p>Inwon Kang 
Rensselaer Polytechnic Institute
TroyNYUSA</p>
<p>Sikai Ruan 
Rensselaer Polytechnic Institute
TroyNYUSA</p>
<p>Tyler Ho 
Rensselaer Polytechnic Institute
TroyNYUSA</p>
<p>Jui-Chien Lin 
Rensselaer Polytechnic Institute
TroyNYUSA</p>
<p>Farhad Mohsin 
College of the Holy Cross
WorcesterMAUSA</p>
<p>Oshani Seneviratne 
Rensselaer Polytechnic Institute
TroyNYUSA</p>
<p>Lirong Xia 
Rensselaer Polytechnic Institute
TroyNYUSA</p>
<p>LLM-augmented Preference Learning from Natural Language
12 Oct 2023F92707DEC61AD2C3E7031C7711EC414AarXiv:2310.08523v1[cs.CL]Preference LearningNatural Language ProcessingLarge Language Models
Finding preferences expressed in natural language is an important but challenging task.State-of-the-art(SotA) methods leverage transformer-based models such as BERT, RoBERTa, etc. and graph neural architectures such as graph attention networks.Since Large Language Models (LLMs) are equipped to deal with larger context lengths and have much larger model sizes than the transformerbased model, we investigate their ability to classify comparative text directly.This work aims to serve as a first step towards using LLMs for the CPC task.We design and conduct a set of experiments that format the classification task into an input prompt for the LLM and a methodology to get a fixed-format response that can be automatically evaluated.Comparing performances with existing methods, we see that pre-trained LLMs are able to outperform the previous SotA models with no fine-tuning involved.Our results show that the LLMs can consistently outperform the SotA when the target text is large -i.e.composed of multiple sentences -, and are still comparable to the SotA performance in shorter text.We also find that few-shot learning yields better performance than zero-shot learning.</p>
<p>INTRODUCTION</p>
<p>Group decision making is an important task in multi-agent systems, in which a group of agents aim to make a collective decision based on their preferences.With the advent of artificial intelligence and machine learning techniques, finding better ways of making group decisions efficiently and accurate becomes an important problem [35].In particular, state-of-the-art natural language processing (NLP) techniques have been used to improve preference elicitation, learning, and aggregation.For example, learning preferences from group discussions such as those in forums or chat rooms can provide an unobtrusive way of learning preferences and making a group decision.Learning preferences from text will also help in making better decisions when there are a large number of alternatives or when there is uncertainty about the preferences.</p>
<p>Mohsin et al. [20] proposed a framework for making group decisions from natural language and created a dataset for texts with preferences.However, making group decisions turned out to be a difficult problem because predicting individual preferences is a difficult task by itself.Predicting preferences in text can be as simple as using simple grammatical rules.For example, look for expressions such as: "better than": the sentence "Tea is better than coffee" expresses a straightforward preference towards tea.However, human language is capable of much more nuanced expressions as well.Consider the sentence "I used to think tea is better than coffee.However, that was a while ago.".A glance at the sentence may suggest that the author prefers tea over coffee but the sentence describes the opposite of the preference.While more than this example is needed to confuse most English speakers, such nuances in natural language add to the challenges of preference classification with machine learning (ML) models.Panchenko et al. [23] and Ma et al. [17] proposed methods predicting the preference between two alternatives from individual pieces of texts using modern machine learning techniques such as transformer-based word embeddings [7,25] and graph neural networks.However, the benchmark dataset [24] considered only simple texts consisting of single sentences, and all texts contained mention of both alternatives.Haque et al. [11] and Mohsin et al. [19] proposed methods to improve these performances for more complex but realistic text that included implicit mention of alternatives and multi-sentence texts.However, unlike tasks like sentiment analysis and stance detection, where these NLP techniques achieved high accuracy, predicting preferences is a more difficult task with relatively low accuracy levels.</p>
<p>Large language models (LLMs) and foundation models have brought in a new wave of improvements in artificial intelligence systems.There is potential to use these LLMs in various multi-agent scenarios for different problems, including negotiation, delegation and making group decisions.Quite recently, Meta released a new feature in their chatting app, that allows an LLM-powered chatbot to help make group decisions 1 .However, preference aggregation, deliberation, etc. depends on the language model's ability to identify preferences expressed in the text.The question is:</p>
<p>Can LLMs identify comparative preferences in texts?</p>
<p>Our Contributions We investigate popular LLM's ability to identify and predict preferences in text in this work.For this, we use two benchmark datasets: First, the Compsent-19 dataset [23] which has single sentence texts with mentions of both alternatives, and second, the College Confidential dataset, which contains more complex texts.In particular, we experiment with two versions of Meta's LLaMa-2 model [28], the 13B-parameter version and the 4-bit quantized 70B-parameter version.We also considered OpenAI's popular GPT-3.5-Turbomodel and GPT-4 model [22].We design and experiment with different prompts that ask the LLMs to predict the preference expressed in the text to determine what type of prompt results in the best prediction performance.Our key findings are the following.</p>
<p>The text in College Confidential dataset contains multiple sentences, complex grammatical structure, and pronouns; many mention the same entity more than once.All of these make College Confidential a challenging dataset.Fortunately, we found that LLMs have the ability to handle large and complex context, especially the LLMs with large number of parameters.The size of the models proved to have some effect on the performance.In the case of the smallest model (LLaMa-2-13B), getting it to predict preferences in a wellformatted way was difficult, and when it did predict preferences, it was mostly incorrect.On the one hand, for the LLaMa-2-70B modelwhich is smaller than GPT-3.5-Turbo-,we developed a retry prompt process that is an iterative process (Figure 1) that manages to get well-formatted predictions from the LLM.On the other hand, we did not need this iterative process for OpenAI's GPT-3.5-Turbo and GPT-4 as it produced perfectly formatted responses which allowed for automatic evaluation.Interestingly, both LLaMa-2-70B and GPT-4 significantly outperformed state-of-the-art methods using transformer-based embeddings and graph neural networks for both simple and complex texts (the Compsent-19 and College Confidential datasets).We find that both LLMs perform similarly for Compsent-19 (the single sentence benchmark).And for College Confidential, GPT-4 performs better than LLaMa-2-70B.</p>
<p>While these findings are not surprising given the versatility of LLMs, we note that these performances were a result of non-trivial deliberation.Our methodology allows the LLM to be used without human intervention.Because the LLMs do not output in a fixed format -only restricted to English language -, we design a method to coerce the model to output its responses in a fixed format which can then be integrated into a larger pipeline in an automated way.This work aims to serve as the first step towards integrating LLMs in the CPC task.While the previous state-of-the-art models have their advantage in some aspects, our results show that LLMs are able to effectively handle the CPC task with proper prompting techniques and are able to outperform them, even without finetuning.</p>
<p>Related Works</p>
<p>Preference learning from text.A common approach for learning preferences from agents when wanting to make a group decision is preference elicitation.This involves asking interactive questions to efficiently elicit agents' preferential information that is sufficient for making a group decision (e.g., in [1,5,18,36]).Xia [34] gives a good exposition to different preference learning methods.Mohsin et al. [20] proposed an unobtrusive framework of learning preferences from text (such as in chat rooms or forums) to make group decisions.</p>
<p>Panchenko et al. [23] built the Compsent-19 dataset with the specific goal of categorizing comparative sentences and finding expressed preferences.Panchenko et al. [23] used pre-trained sentence embeddings [6,25], among other features, to train for the classification problem.Since then, other machine learning techniques have been used for predicting preference.Transformer models [29], in particular, have been useful tool in NLP because of their ability to learn large dimensions of data without overfitting.This has led to newer embeddings, both at token and sentence levels [7,9,16].Previous works have used text embeddings and graph neural networks like GAT [30] for preference detection/tagging.These models are effective with textual and graph-structured data.Recent works in preference learning from text [13,15,17] have made use of dependency graphs and graph neural network methods.Li et al. [15] additionally used the knowledge transfer technique from the related sentiment analysis task.But all of these methods were tested on the Compsent-19 benchmark [23] and thus dealt with single sentences with explicit mention of alternatives.On the other hand, Mohsin et al. [20] and Haque et al. [11] introduced new datasets which dealt with implicit preferences, where both entities might not be mentioned.The College Confidential dataset [20] in particular contained multi-sentence texts in a discussion setting.We work with both the simpler Compsent-19 dataset and the more complex College Confidential dataset.Large Language Models.The introduction of LLMs and foundation models [4,22,26,28] such as ChatGPT, LLaMa and Bard have brought forth a new paradigm in many domains of machine learning.In particular, ChatGPT [21] has demonstrated an impressive ability to perform various tasks while displaying human-like qualities through text.While these models mainly operate in natural language domain, past works have found that they can be well suited for other tasks, such as classification or regression [2,12,32].However, the input to an LLM still needs to be encoded into human language, and past works have experimented with different ways of converting the tasks into natural language.</p>
<p>Brown et al. [2] demonstrate that few-shot learning can be utilized to enhance the performance of LLMs.In few-shot learning, the user includes correct examples of the given task to help the model's understanding of the task.Wei et al. [32] show that LLMs can be powerful zero-shot learners when fine-tuning is applied to the pretrained weights.Wei et al. [33] use chain-of-thought prompting to guide the LLM towards making a step-by-step decision and show that this style of prompting can achieve better performance in tasks such as arithmetic or reasoning-based tasks.In addition, Kojima et al. [14] show that LLMs can have decent performance in zero-shot settings by using the chain-of-thought prompting process.Schick and Schtze [27] introduce Pattern Exploiting Training (PET), in which the prompt follows some pattern to elicit a higher quality of response from the LLM.</p>
<p>LLMs have proven to be proficient in non-textual settings as well.Hegselmann et al. [12] explore using LLMs to predict tabular data in a few-shot setting.The benchmark results show that LLMs are able to outperform previous state-of-the-art models, and that even zero-shot learning can achieve nontrivial performance in many instances.</p>
<p>PRELIMINARIES 2.1 Task Description</p>
<p>Given a text , and a two alternatives  and , the goal is to predict the preference relation between  and .Ideally, there can be four possible cases:  is preferred to  (  ),  is preferred to  (  ), both are equally preferred ( = ), and there is no preference relation between the two alternatives (N/A).</p>
<p>This task has sometimes been called comparative preference classification (CPC).We consider two preference datasets in this work: College Confidential [20] and Compsent-19 [24].The College Confidential dataset [20] contains comments from a college admission forum.The authors search for discussion threads where the original poster asks for opinion comparing multiple colleges and collect the following posts to build the dataset.The discussion threads discuss more than two colleges in some cases.The multi-way comparisons are divided into pairwise comparisons.The resulting dataset consists of 2964 pairwise comparison instances, with 4 classes -No Preference,   ,   ,  = .</p>
<p>Compsent-19 [24] is a binary-comparative dataset that contains a single comparative sentence between two alternatives.The alternatives are picked from various domains such as computer science concepts -programming languages, hardware devices -, or brands.The authors query for sentences that contain mentions of both alternatives from the Common Crawl dataset and present a final dataset of 7,199 sentences with 217 unique pairs of alternatives and 3 classes -  ,   , N/A.</p>
<p>Because of the difference in the class representation of the two datasets, we only consider the 3 class cases in this work -  ,   , N/A -, where N/A refers to both No Preference and  = .The distribution of labels, along with the average text size for both datasets, is given in Table 1.The average text size is given in token numbers, where tokens are building blocks of sentences in NLP.Words, punctuations, and parts of words can all be individual tokens.From this, we see that the College Confidential dataset has text that is, on average, more than four times longer than those in Compsent-19.Tables 2 and 3 show a few example texts, along with labels, from each dataset.</p>
<p>NLP and ML Terminology</p>
<p>In this section, we discuss the various NLP techniques that we consider in this work.Large language models (LLM) usually refer to pre-trained models that use the attention mechanism.What distinguishes an LLM from a regular language model is the large amount of data used for pre-training and the number of parameters in the architecture, which are in the order of billions.We will only refer to large-scale transformer-based [29] models that are pre-trained on massive corpuses, such as GPT [21,22] and LLaMa [28] as LLMs to avoid any confusion.In this work, we consider OpenAI's GPT-4 [22] as the state-of-the-art LLM and use a fine-tuned version of Meta's LLaMa-2 [28]'s 70B model 2 and the original 13B chat model 3 as the open-source alternative.Due to the memory constraints, we use the 4-bit quantized version of the 70B model 4 .We will refer to these models as LLaMa-2-70B and LLaMa-2-13B for clarity's sake.</p>
<p>The prompt is what is inputted to the LLM to generate its response.The prompt consists of three different parts: system message, user message, and assistant message.The system message sets the context of the interaction with the LLM, as per the OpenAI official documentation 5 .The system message is used for instructing the model about the input format of the data points and ensuring that the output of LLMs conforms to a specific format.This pattern is also adopted by many open-source LLM models, including the LLaMa-2 models we consider in this work.For example, both the original LLaMa-2 weights and fine-tuned version use all three of these message types.</p>
<p>Prompt engineering refers to building a specific prompt that works the best for the task.Prompt engineering includes designing how to wrap the task into a prompt and how to break the task down into smaller tasks that the LLM can handle.Zero-shot learning.Zero-shot prompts further assume that language models do not need any examples but understand the concept from training data.So, in these prompts, no examples are provided, but the LLM is directly prompted to predict a preference.Kojima et al. [14] first showed some zero-shot predictive capabilities of LLMs.Few-shot learning.Few-shot prompts for language models [2] indicate the scenario where no weights are updated but a few examples are given to the language model to provide context.So, in our task of predicting preferences expressed in text, examples include a text, the names of the alternatives, and the expressed preference.Then, the model will receive instructions to predict preference for a new given text and pair of alternatives.The number of examples provided is a hyperparameter of the algorithm.The name few-shot comes from the general concept of few-shot learning in ML [31].</p>
<p>EXPERIMENTS</p>
<p>In this work, we seek to assess the potential and limitations of using LLMs for classification tasks.We use an example from the prompts used for College Confidential to illustrate the workflow.All of our code is publicly available on Github. 6</p>
<p>Label Distribution Average Token Length
A  B A  B N/A</p>
<p>Prompt Structure</p>
<p>For the College Confidential dataset, the system message starts with the sentences that tell the LLMs that it will be given one comment and two colleges and that its job is to identify the preference between the two colleges in this comment.For Compsent-19, which does not have a single domain, we modify the role to ask it to assume the role of an internet forum user.Some output rules are added to the system message to make the output conform to a specific format.For example, we provide rules such as You MUST respond with "A is preferred over B" if college A is preferred over college B. Once we have an output that conforms to the desired format, we can assess the accuracy of LLMs in classification tasks in an automatic manner.</p>
<p>When sending the prompt to the LLM, a conversation is represented as a list of tuples.The first element of the tuple is the user's input, and the second is the LLM's output.The final response from the model is triggered by sending a list of those tuples, followed by the last user instruction, to which the model will respond.This structure allows us to simulate the history of the interaction between the user and the model before asking for its response.</p>
<p>During our experiments, we find that using clear instructions and capitalizing the instruction words can lead the LLM to have less inconsistent responses.For example, instead of saying do ... if ..., using -you MUST ... if ... leads to a more consistent and rulefollowing output from the LLM.We test two versions of prompts with this setting.The initial version of the prompt is referred to as 6 https://github.com/inwonakng/llm-preference/long, which has detailed rules and context in instruction.We also test with a paraphrased version of this prompt, which we refer to as short.</p>
<p>We also experiment with different hyperparameters available for the LLMs.In particular, we use different values for temperature and top_p of both LLaMa-2 and GPT-4.These parameters control how the best response is chosen by the LLM.Temperature is responsible for how the model chooses each token.Lower temperature values lead the model to choose the tokens with more likelihood.Top P is used to pick a set of tokens that follow a previously selected token.Given a selected token, the following set of tokens is chosen such that the set is the minimum number of tokens whose probability exceeds the P value.We find that temperature = 1 and top_p = 0.7 work the best for the short prompt and temperature = 0.7 and top_p = 0.1 work the best for the long prompt.</p>
<p>Example of short prompt</p>
<p>You will be given two colleges A and B, and a comment.Your job is to identify the preference between the two given colleges in the comment.The names of the two colleges and the comment are delimited with triple backticks.</p>
<p>Here are the rules: You MUST NOT use the colleges' real names.You MUST refer to the colleges as A or B. You MUST respond with <code>No preference</code>if there is no explicit preference in the comment.You MUST respond with <code>A is preferred over B</code>if college A is preferred over college B. You MUST respond with <code>B is preferred over A</code>if college B is preferred over college A. You MUST respond with <code>Equal preference</code>if colleges A and B are equally preferred.You MUST respond with <code>No preference</code>, <code>A is preferred over B</code>, <code>B is preferred over A</code>, or <code>Equal preference</code>.College A: <code alternative_b="alternative_b">{alternative_a}``C ollege B:</code><code>C omment: ```{text}</code>3</p>
<p>.</p>
<p>Few-shot Learning</p>
<p>In few-shot learning, correct examples of the task are added to the prompt to guide the LLM.For each label, we select one data point with that label in the dataset as part of the few-shot example and exclude those points from the testing set.In order to ensure that the examples contain enough content to be helpful while not increasing the prompt length too much, we select the text with the minimum length, which has more than 100 words for each label.</p>
<p>As for zero-shot learning, the examples are simply an empty set, i.e., no example will be used.</p>
<p>Model</p>
<p>Cost per Output Token (10 6 ) Cost per Input Token (10 6 ) Architecture Size Pre-train Token Amount LlaMa-2 70B 0 0 70B 2T GPT-3.5-Turbo 2 1.5 175B* unknown GPT-4 60 30 unknown unknown Table 4: Statistics of individual models.The cost is measured in U.S. dollars.[2,28] * This value is taken from GPT-3 and it is difficult to confirm whether GPT-3.5-Turbocontains the same amount of parameters.</p>
<p>An example of the interaction tuple that is added to the chat history is as the following:</p>
<p>User: <code>Comment: I would prefer Stanford rather than UCB.</code>O ption A: Stanford University <code> ption B: UCB</code> ssistant: <code>`A is preferred over B</code>3</p>
<p>.3 Retry Prompting</p>
<p>Even if we specify that LLMs should produce specific content for each label, there is still a possibility that the LLMs may generate content that does not conform to the format.In some cases, we find that the model is correct in its classification, but we are not able to programmatically evaluate it due to the response being malformed.For example, instead of saying A is preferred over B, the model can respond with ... Therefore, I think A is preferred over B. To overcome cases such as this and allow for an automatic evaluation of the model's output, we use what we call a retry prompt.Instead of discarding the previously correct but malformed response, we rebuild a prompt to continue the conversation and remind the model of the formatting rules again by adding another user message.Using the tuple structure of the conversation history, we construct the retry prompt in a way that appears as the continuation of the task that was incorrectly formatted.We append the tuple of the original task message and the model's incorrect response to the list of the conversation history.The retry message is then sent as the final user input, prompting the model to have access to its possibly correct but incorrectly formatted response to fix the format.Using this technique, we are able to find some sets of rules with which the model's output was consistent for all the test inputs.Figure 1 shows an illustration of this process.</p>
<p>Specifically, a retry request is triggered when LLM produces a statement such as A is better than B in every way, which deviates from the prescribed format.We use the same prompt rule as the previous short prompt rules in {Rules...}, and the retry prompt is as the following:</p>
<p>Example of short retry prompt</p>
<p>You have an incorrect format in your response.</p>
<p>Here is a reminder of the rules: {Rules...}</p>
<p>RESULTS</p>
<p>In summary, we deploy the classification experiments in the following settings and analyze the results on LLaMa-2-70B, GPT-4, GPT-3.5-Turbo with long &amp; detailed prompt and short &amp; concise prompt with zero-shot and few-shot settings on College Confidential and Compsent-19.</p>
<p>Prompt Engineering</p>
<p>We experiment with various prompt techniques to convert the CPC task into a text format.Following best practices suggested by OpenAI 7 and DeepLearningAI8 , we develop two sets of final prompts that are able used as input to the LLM to detect and classify preference in the text effectively.For example, we set the role for the LLM to assume in the instruction (system) prompt and explain the rules in a clear bullet list.We also find that delimiters that are not commonly used in regular English text were best understood by the LLM.We use the triple-backtick ```to wrap the comment and the two alternatives to note that they are separate from the instructions.While GPT was also able to understand other delimiters, such as triple-hash (###), this delimiter clashed with the template used by the instruct fine-tuned LLaMa-2 model used in the work.Because of this reason, we use the triple-backtick delimiter for both models for consistency's sake.</p>
<p>When expressing the rules of the task, such as the formatting and the goal of the task, we find that a simple sentence structure in a commanding tone works best.Interestingly, we also find that using a more conversational tone -e.g.instead of "Do XXX", "Let's do XXX" -helped the model follow the instructions more effectively.For this reason, we express the rules themselves in a simple structure and capitalize the modal verbs -e.g.must, must not -and use a more conversational tone to end the prompt.</p>
<p>Classification Performance</p>
<p>Tables 7 and 5 show the results from our experiments with LLaMa-2, GPT-3.5-Turbo, and GPT-4.</p>
<p>The best score for each dataset and model combination is highlighted in bold.Because of the imbalance of labels in our dataset, we focus on both the Macro and Micro F1 scores, which calculate the unweighted and weighted averages of the individual F1 scores.</p>
<p>The results show that few-shot learning with the short prompt outputs the best performance in most cases.However, more detailed instructions may be helpful when the input text is shorter.The model's performance on shorter text -i.e.Compsent-19 -in Table 7 shows that the zero-shot performance on LLaMa-2 70B with the long prompt can be better than that with the short prompt.This</p>
<p>Output Consistency</p>
<p>Throughout our experiments, we find that the LLM's responses tend to be inconsistent when faced with the same prompts.We observe that this happens often in the LLaMa-2 models and GPT-3.5-Turbo.We note that GPT-4 was able to follow the rules much more consistently than LLaMa-2 -while more than half of the tasks for LLaMa-2 had to be run with the retry prompt, only a handful of cases were needed for GPT-4.For instance, the predicted label drastically changes if the same question is asked again, even though the response is correctly formatted.</p>
<p>We also find that the output from the few-shot is more likely to conform to the output format than zero-shot.It is likely because the examples show the correct outputs, and these examples can help LLM to do in-context learning and understand the output format rules.</p>
<p>The experiments on LLaMa-2 were deployed on the 4-bit quantized version of the 70B model.We also deployed the experiments on the 13B model of LLaMa-2.However, we observe that the LLaMa-2-13B model could not handle the College Confidential or Compsent-19 dataset.Specifically, the output consistency is hard to satisfy on LLaMa-2-13B, especially for zero-shot.On the other hand, LLaMa-2-13B tends to give fixed answers like "No preference" or "Equal preference" for most cases, which results in a bad performance.</p>
<p>Detecting Preference</p>
<p>As expected, the F1 scores for cases where preference is present are much lower than the N/A cases, meaning the models struggled more with the classification task.It is also worth noting that when the input text is a single sentence, as in the Compsent-19 dataset, the LLM tends to perform better in both detecting/classifying the preference.Overall, we again note that GPT-4 was able to outperform the LLaMa-2 models and GPT-3.5-Turbo in both detecting and classifying the preference.</p>
<p>Comparison to Previous Work</p>
<p>We compare our results against the state-of-the-art (SotA) results from previous works that leverage GNN-based models.Ma et al. [17] present their results on the Compsent-19 dataset using the ED-GAT architecture, and Mohsin et al. [19] present the results from MultiSentPref on the College Confidential dataset.Tables 6  and 8 show the comparison between the best LLM and SotA performances.</p>
<p>For College Confidential, we find that the LLM performance from GPT-4 outperforms the MultiSentPref's best performance by a significant margin.Even LLaMa-2-70B can outperform every metric considered except for F1[A &gt; B].</p>
<p>We also find that GPT-4 is able to outperform the best performance from ED-GAT in classifying the performance, albeit by a smaller margin than when compared to MultiSentPref.It is worth noting here that ED-GAT performs better at detecting the lack of preference.This observation is also reflected in a higher F1 micro score as the dataset is unbalanced and contains approximately 2.7 more no preference rows as preference present rows.</p>
<p>It is interesting to note that MultiSentPref [19] is an extension of ED-GAT [17], but it does not perform nearly as well in College Confidential.Notably, while Compsent-19 contains a single sentence per text, College Confidential's content can extend to multiple paragraphs; see Table 1.This disparity suggests that LLM's superiority over SotA likely arises from its capacity to manage extensive context, suggesting that it may be better at handling complex tasks compared to the previous graph-based SotA models.</p>
<p>Overall, we see an improvement over previous SotA models with both GPT-4 and LLaMa-2 in most metrics.GPT</p>
<p>FUTURE WORK</p>
<p>Future extensions of this work can be branched into multiple directions.The first direction would be to improve the LLM's performance by fine-tuning the model or improving the prompt.While fine-tuning the model will improve the performance, the lack of labeled datasets in the CPC domain poses a challenge.The prompt engineering direction can include a more fine-grained approach to the problem, such as handling the two parts of CPC separatelydetecting the preference and classifying only if it exists -or adding a summarization task before the classification to handle longer text better.We considered a version of the summarization task in our work but were not able to find a set of prompts that led to a higher performance than the currently presented single-stage method.Future works could explore more prompt engineering to find a set of prompts that allow the smaller models to handle large text more efficiently.</p>
<p>Another direction is to consider an ensemble learning approach of the LLMs.As seen in the comparison with the SotA models, models from previous work can outperform the current LLM approach.Thus, combining multiple previous models and LLMs in an ensemble could lead to better performance.Another possibility is to consider the LLMs in an ensemble.The LLMs' predictions can be aggregated to form an ensemble output, resulting in better performance.</p>
<p>In another direction, we can generate new text datasets using an LLM to augment the original dataset to remedy the lack of comparative text datasets.This newly augmented dataset could be used to train smaller models that usually need more data than currently available.Thus, a knowledge distillation process could be tried in which the preference predictive capabilities of LLMs can be distilled into smaller and more efficient models.</p>
<p>In a broader direction, we can explore the capabilities of LLMs in multi-agent scenarios such as group decision-making, deliberation, iterative decision-making, etc.We see much ongoing work in this domain, proposing to use LLMs for social choice [8] or to facilitate multi-agent collaborations [3,10].In this same vein, we can work to create an AI agent that assists in making better group decisions under uncertainty.</p>
<p>Finally, any application of an LLM for a specific task inherits the bias present in training the original LLMs.While LLaMa-2 was trained on open-source texts, this is not true for the GPT models, so we can not even be sure of the amount of bias in the models.Future work applying LLMs for preference learning and elicitation should further fine-tune the models to remove any possible biases since this is an even bigger issue when considering comparative texts.Also, while the LLM-based methods outperform state-of-theart models in most cases, they still depend on black box methods.Thus, these methods should be applied cautiously in the real world.In the future, we will look into what steps we can take to make the preference prediction process more transparent, particularly focusing on getting explanations for the predictions.</p>
<p>CONCLUSION</p>
<p>In this work, we consider the task of predicting preferences expressed in text by using LLMs with an automated evaluation scheme.Specifically, we experiment with four types of commercial and opensource LLMs -OpenAI's GPT-4 and GPT-3.5-Turbo,Meta's LLaMa-2-70B and LLaMa-2-13B.We also test the efficacy of different kinds of prompting methods to represent the task in a textual format and find two methods that are able to help the LLMs perform well in the task.Using these two prompts, we test the zero-shot and few-shot techniques to wrap the classification task into a conversational format that the LLM can handle.</p>
<p>Our results show that LLMs are able to outperform the previous SotA approaches in predicting preferences from text.We also find that using few-shot prompts by including examples from the dataset as a part of the prompt can further improve the LLM's performance.While the smaller models are not able to handle a lengthy set of instructions and text as efficiently, we find that the larger model, such as GPT-4, is able to handle both.The comparisons to previous SotA models show that the LLM is an effective replacement when handling longer texts.Some SotA approaches can still outperform our results using the LLMs when the input text is short.The observation that older methods can sometimes outperform LLMs suggests that we can use a sufficiently large enough LLM can be used in combination with other existing techniques for even better performances.To summarize, our main findings are twofold: 1) LLM can be better than SotA models.2) few-shot prompts are better than zero-shot ones.</p>
<p>ETHICAL IMPACTS</p>
<p>Any application of LLMs for a specific task inherits the biases of the original LLM.We consider ethical concerns over this and discuss possible ways of alleviating some of those concerns in the Future Works section.</p>
<p>APPENDIX 7.1 Full prompt for long</p>
<p>Instruction Message</p>
<p>Pretend that you are a user on college confidential forums.Your job is to detect if there exists a preference between two options in a comment.If there exists a preference, you must detect what the preference is.If the author of the comment expresses an explicit preference, you must detect it.You will be given a comment and two alternatives for each task.The options will be denoted by <code>Option A:</code>and <code>Option B:</code>.The comment will be denoted by <code>Comment:</code>.</p>
<p>Rules: -You MUST NOT respond with a summary of the comment.</p>
<p>-You MUST NOT use the options' real names.</p>
<p>-You MUST refer to the options as A or B. -You MUST respond with "No preference"' if there is no strict preference.</p>
<p>-You MUST respond with <code>A is preferred over B</code>if option A is preferred over option B.</p>
<p>-You MUST respond with <code>B is preferred over A</code>if option B is preferred over option A.</p>
<p>-You MUST respond with <code>Equal preference</code>if options A and B are equally preferred.</p>
<p>-You MUST respond using one of the four phrases above.</p>
<p>Retry Message</p>
<p>Your response was incorrect.Let's try again.</p>
<p>Here is a reminder of the rules:</p>
<p>-You MUST ONLY report the preference in the comment.</p>
<p>-You MUST respond only using one of the following phrases: `` o preference<code>,</code>A is preferred over B<code>,</code>B is preferred over A<code>,</code>Equal preference```.Do not say anything else.</p>
<p>-You MUST respond with <code>No preference</code>if there is no strict preference.-You MUST respond with <code>A is preferred over B</code>if option A is preferred over option B.</p>
<p>-You MUST respond with <code>B is preferred over A</code>if option B is preferred over option A.</p>
<p>-You MUST respond with <code>Equal preference</code>if options A and B are equally preferred.</p>
<p>-You MUST NOT use the options's real names.-You MUST ONLY refer to the options as <code>A</code>or <code>B</code>.</p>
<p>-You MUST NOT respond with any other details than the preference expressed in the comment.</p>
<p>-You MUST NOT explain your reasoning behind the response.Only respond with the given phrase.</p>
<p>-You MUST NOT use any punctuation in the response.</p>
<p>Your previous response was not in any of the required responses.Try again and respond with a correct response to the previous comment.You MUST NOT reply the same response.</p>
<p>Full prompt for short</p>
<p>Instruction Message</p>
<p>You will be given two colleges A and B, and a comment.Your job is to identify the preference between the two given colleges in the comment.The names of the two colleges and the comment are delimited with triple backticks.</p>
<p>Here are the rules: You MUST NOT use the colleges' real names.You MUST refer to the colleges as A or B. You MUST respond with <code>No preference</code>if there is no explicit preference in the comment.You MUST respond with <code>A is preferred over B</code>if college A is preferred over college B. You MUST respond with <code>B is preferred over A</code>if college B is preferred over college A. You MUST respond with <code>Equal preference</code>if colleges A and B are equally preferred.You MUST respond with <code>No preference</code>, <code>A is preferred over B</code>, <code>B is preferred over A</code>, or <code>Equal preference</code>.</p>
<p>Retry Message</p>
<p>You have an incorrect format in your response.</p>
<p>Here is a reminder of the rules: You MUST NOT use the colleges' real names.You MUST refer to the colleges as A or B. You MUST respond with <code>No preference</code>if there is no explicit preference in the comment.You MUST respond with <code>A is preferred over B</code>if college A is preferred over college B. You MUST respond with <code>B is preferred over A</code>if college B is preferred over college A. You MUST respond with <code>Equal preference</code>if colleges A and B are equally preferred.You MUST respond with <code>No preference</code>, <code>A is preferred over B</code>, <code>B is preferred over A</code>, or <code>Equal preference</code>.</p>
<p>Table 1 :
1
Statistics for the datasets
Total</p>
<p>Table 2 :
2
Examples from the College Confidential dataset
SentenceLabelGolf is easier to pick up than baseball.  I'm considering learning Python and more PHP ifN/Aany of those would be better.</p>
<p>Table 3 :
3
Examples from the Compsent-19 dataset</p>
<p>-4 consistently outperforms both MultiSentPref and ED-GAT in [preference classification while LLaMa-2 is able to outperform MultiSentPref on College Confidential but falls short of ED-GAT on Compsent-19.While ED-GAT outperforms in the preference detection task in Compsent-19, the difference is insignificant.The large improvement on the College Confidential dataset indicates that LLMs have significantly improved over previous SotA models in classifying longer context-length examples compared to smaller ones.</p>
<p>https://about.fb.com/news/2023/09/introducing-ai-powered-assistants-charactersand-creative-tools/
This was finetuned by Upstage for instructions. https://huggingface.co/TheBloke/ Upstage-Llama-2-70B-instruct-v2-GPTQ
https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML
We use a machine with a single A6000 with 48G VRAM
https://help.openai.com/en/articles/7042661-chatgpt-api-transition-guide
https://help.openai.com/en/articles/6654000-best-practices-for-promptengineering-with-openai-api
8 https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-fordevelopers/
System MessageUser MessageAssistant MessageClassify the preference between the two options expressed in the comment.{Rules...} Option A: Stanford University Option B: UCB Comment: I would prefer Stanford rather than UCB.It seems that A is preferred over BAssistant MessageA is preferred over BRetry MessageYour last response was incorrect.Here is a reminder ... {Rules...} Let's try again.Update promptFigure1: Illustration of the retry prompt process.The incorrect output is appended to the original prompt, followed by a retry message to remind the rules.ModelPrompt suggests that the detailed instructions are helpful, but their effects are diminished when the other part of the prompt becomes lengthy.It is also worth noting that the few-shot long prompt in Compsent-19 outperforms the few-shot short prompt more consistently with GPT-4.This suggests that GPT-4 may be more capable of handling complex/verbose instructions, especially when the input text itself is short.While LLaMa-2 outperformed the previous state-of-the-art, the GPT-4 outperforms the LLaMa-2 models in general.However, LLaMa-2 is able to outperform GPT-3.5-Turbo.In addition to the single-stage classification experiments, we consider another variety of prompting to handle long input.While Compsent-19 contains a single sentence per text, College Confidential dataset's text can be as long as multiple paragraphs.We design an experiment where the LLM first summarizes the input text and uses this summary to run the preference classification.Specifically, the LLM is prompted to summarize the preference expressed in the text while ensuring the output contains the names of the two colleges.This provides the necessary information for the subsequent preference classification task.However, we find that the summary method does not perform as well as expected.When comparing using no summary versus using a summary, we see that no summary prompt outperforms the summary condition in the overall performance.When analyzing(9)as presented by Ma et al.[17].performance by text length, the summary approach only benefits texts longer than 400 words.Since posts in the College Confidential dataset tend to be shorter, the summary task may be unsuitable for this corpus.The poor performance of the summary may be due to the brevity of the source texts in College Confidential as well.
A POMDP formulation of preference elicitation problems. Craig Boutilier, Proceedings of the National Conference on Artificial Intelligence (AAAI). the National Conference on Artificial Intelligence (AAAI)Edmonton, AB, Canada2002</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language Models Are Few-Shot Learners</p>
<p>Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, arXiv:2308.10848Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. 2023. 2023arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Shivani Dohan, Agrawal, 10.48550/arXiv.2204.02311arXiv:2204.02311PaLM: Scaling Language Modeling with Pathways. Mark Omernick, Andrew M Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel, 2022</p>
<p>Vote Elicitation: Complexity and Strategy-Proofness. Vincent Conitzer, Tuomas Sandholm, Proceedings of the National Conference on Artificial Intelligence (AAAI). the National Conference on Artificial Intelligence (AAAI)Edmonton, AB, Canada2002</p>
<p>Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. Alexis Conneau, Douwe Kiela, Holger Schwenk, Loc Barrault, Antoine Bordes, Proceedings of EMNLP 2017. EMNLP 2017Copenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805[cs.CL]BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2019</p>
<p>. Sara Fish, Paul Glz, David C Parkes, Ariel D Procaccia, Gili Rusak, Itai Shapira, Manuel Wthrich, arXiv:2309.012912023. 2023Generative Social ChoicearXiv preprint</p>
<p>SimCSE: Simple Contrastive Learning of Sentence Embeddings. Tianyu Gao, Xingcheng Yao, Danqi Chen, Proceedings of EMNLP 2021. EMNLP 20212021</p>
<p>Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, arXiv:2309.09971MindAgent: Emergent Gaming Interaction. 2023. 2023arXiv preprint</p>
<p>Pixie: Preference in Implicit and Explicit Comparisons. Amanul Haque, Vaibhav Garg, Hui Guo, Munindar P Singh, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Short Papers. the 60th Annual Meeting of the Association for Computational Linguistics20222</p>
<p>TabLLM: Few-shot Classification of Tabular Data with Large Language Models. Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, David Sontag, Proceedings of The 26th International Conference on Artificial Intelligence and Statistics. The 26th International Conference on Artificial Intelligence and StatisticsPMLR2023</p>
<p>Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks. Binxuan Huang, Kathleen Carley, 10.18653/v1/D19-1549Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Large Language Models Are Zero-Shot Reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. 352022. Dec. 2022</p>
<p>Powering Comparative Classification with Sentiment Analysis via Domain Adaptive Knowledge Transfer. Zeyu Li, Yilong Qin, Zihan Liu, Wei Wang, Proceedings of the EMNLP 2021. the EMNLP 20212021</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019. 2019arXiv preprint</p>
<p>Entity-aware dependency-based deep graph attention network for comparative preference classification. Nianzu Ma, Sahisnu Mazumder, Hao Wang, Bing Liu, Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL-2020). Annual Meeting of the Association for Computational Linguistics (ACL-2020)2020</p>
<p>Optimal Communication-Distortion Tradeoff in Voting. Debmalya Mandal, Nisarg Shah, David P Woodruff, Proceedings of ACM EC. ACM EC2020</p>
<p>Dependency and Coreference-boosted Multi-Sentence Preference model. Farhad Mohsin, Inwon Kang, Yuxuan Chen, Jingbo Shang, Lirong Xia, DLG-AAAI-23The 9th International Workshop on Deep Learning on Graphs: Method and Applications. 2023</p>
<p>Rohit Vaish, and Lirong Xia. 2021. Making group decisions from natural language-based preferences. Farhad Mohsin, Lei Luo, Wufei Ma, Inwon Kang, Zhibing Zhao, Ao Liu, Proceedings of the 8th International Workshop on Computational Social Choice (COMSOC). the 8th International Workshop on Computational Social Choice (COMSOC)</p>
<p>OpenAI. 2022. Introducing ChatGPT. </p>
<p>. 10.48550/arXiv.2303.08774arXiv:2303.08774Technical Report</p>
<p>Categorizing Comparative Sentences. Alexander Panchenko, Alexander Bondarenko, Mirco Franzek, Matthias Hagen, Chris Biemann, 10.18653/v1/W19-4516Proceedings of the 6th Workshop on Argument Mining. the 6th Workshop on Argument MiningFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Building a Web-Scale -Parsed Corpus from CommonCrawl. Alexander Panchenko, Eugen Ruppert, Stefano Faralli, Simone P Ponzetto, Chris Biemann, Proceedings of the Eleventh International Conference on Language Resources and Evaluation. the Eleventh International Conference on Language Resources and EvaluationMiyazaki, Japan2018. 2018European Language Resources Association (ELRA)</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of the EMNLP. the EMNLP2014. 2014</p>
<p>Improving Language Understanding by Generative Pre-Training. Alec Radford, Karthik Narasimhan, 2018</p>
<p>Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference. Timo Schick, Hinrich Schtze, 10.18653/v1/2021.eacl-main.20Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European ChapterOnline2021</p>
<p>Edouard Grave, and Guillaume Lample. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothe Lachaux, Baptiste Lacroix, Naman Rozire, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, 10.48550/arXiv.2302.13971arXiv:2302.13971LLaMA: Open and Efficient Foundation Language Models. 2023</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, Illia Polosukhin, Proceedings of NeurIPS. NeurIPS2017. 20172017</p>
<p>Graph Attention Networks. Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li, Yoshua Bengio, International Conference on Learning Representations. 2018</p>
<p>Matching networks for one shot learning. Advances in neural information processing systems. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, 2016. 201629</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS. International Conference on Learning Representations. 2022. 2022</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, Advances in Neural Information Processing Systems. 352022. Dec. 2022</p>
<p>Learning and decision-making from rank data. Lirong Xia, Synthesis Lectures on Artificial Intelligence and Machine Learning. 132019. 2019</p>
<p>Group decision making under uncertain preferences: powered by AI, empowered by AI. Lirong Xia, Annals of the New York Academy of Sciences. 15112022. 2022</p>
<p>A Cost-Effective Framework for Preference Elicitation and Aggregation. Zhibing Zhao, Haoming Li, Junming Wang, Jeffrey Kephart, Nicholas Mattei, Hui Su, Lirong Xia, Proceedings of Uncertainty in Artificial Intelligence. Uncertainty in Artificial Intelligence2018</p>            </div>
        </div>

    </div>
</body>
</html>
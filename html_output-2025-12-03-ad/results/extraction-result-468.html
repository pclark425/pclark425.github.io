<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-468 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-468</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-468</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-254151854</p>
                <p><strong>Paper Title:</strong> A Metrological Perspective on Reproducibility in NLP*</p>
                <p><strong>Paper Abstract:</strong> Abstract Reproducibility has become an increasingly debated topic in NLP and ML over recent years, but so far, no commonly accepted definitions of even basic terms or concepts have emerged. The range of different definitions proposed within NLP/ML not only do not agree with each other, they are also not aligned with standard scientific definitions. This article examines the standard definitions of repeatability and reproducibility provided by the meta-science of metrology, and explores what they imply in terms of how to assess reproducibility, and what adopting them would mean for reproducibility assessment in NLP/ML. It turns out the standard definitions lead directly to a method for assessing reproducibility in quantified terms that renders results from reproduction studies comparable across multiple reproductions of the same original study, as well as reproductions of different original studies. The article considers where this method sits in relation to other aspects of NLP work one might wish to assess in the context of reproducibility.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e468.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e468.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quantified Reproducibility Assessment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metrology-inspired procedure for quantifying the precision (degree of reproducibility) of measured evaluation scores in NLP/ML by (i) specifying conditions of measurement, (ii) collecting measured quantity values across measurements, and (iii) computing a precision metric (CV*) to yield repeatability/reproducibility scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Metrological Perspective on Reproducibility in NLP *</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / Machine Learning (metrology of evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Assessing reproducibility of NLP system evaluation scores (example: weighted F1 scores for multilingual essay scoring variants) by comparing original and reproduction measurements under specified conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Sources explicitly identified: differences in system code and implementation, compile/training information, measurement method specification and implementation, measurement procedure, test set splits, 'performed by' (different teams/people), runtime/compile environments, random seeding methods, different data splits, different methods for obtaining/using word embeddings, errors/mismatches between published descriptions and code, and residual uneliminated variation despite sharing artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Unbiased coefficient of variation (CV*), unbiased sample standard deviation s*, mean, confidence intervals via t-distribution; precision defined as CV* for a set of measured values.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Example application: CV* scores for wF1 across reproductions — mult-base: 14.63; mult-dep −: 4.50; mult-dep +: 4.39; mult-emb −: 17.03; mult-emb +: 16.23. Also reported: in a survey of 513 original/reproduction pairs, only 14% were exactly identical and 60% of differing reproductions were worse than the original.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Precision-based reproducibility score R computed as CV* across measurements; baseline repeatability score R0 when identical conditions available; comparisons of R0 vs R to estimate effect of changed conditions. Also use of subsets of measurements sharing more conditions and reporting R per subset.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>QRA yields numeric reproducibility scores (CV*) that enable comparison across reproductions; example shows large variability across system variants (CV* ranging ~4.39 to ~17.03) and that only 14% of 513 original/reproduction pairs matched exactly (Belz et al. 2021a).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Lack of standardized condition reporting in papers, small sample sizes for reproduction measurements, residual variation even when code/data shared, limited information available from publications preventing repeatability assessment, implementation or evaluation-code errors, and difficulty assigning which changed condition caused variation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Proposed: record and publish standardized conditions of measurement (system code, compile/training info, measurement method spec/implementation, procedure, test set, who performed it); use 2-phase QRA (repeatability baseline then reproducibility tests); compute and report CV* (and confidence intervals); include repeatability assessment during method development; use reproducibility checklists and human-evaluation datasheets; automated CV* checking integrated into development.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not reported as quantified effect sizes from interventions in this paper; QRA is shown to identify weak links (e.g., embeddings pipeline) and point to where improvements are needed, but numerical improvements from mitigations are not presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Varied by scenario; procedure requires n >= 2. Example application used original measurement plus up to seven reproduction scores per variant (i.e., up to n ≈ 8 measured values in examples).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>QRA operationalizes reproducibility as measurement precision (CV*), enabling quantitative comparison of reproducibility across studies; reproducibility in NLP is heterogeneous (CV* in examples ranged ≈4–17), many reproductions differ from originals (only 14% exact matches), and standardized recording of measurement conditions plus repeatability baselines are recommended to identify and reduce sources of variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Metrological Perspective on Reproducibility in NLP*', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e468.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e468.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CV*</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unbiased Coefficient of Variation (CV*)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small-sample corrected, unbiased estimator of the coefficient of variation (standard deviation divided by mean) recommended in this work as the primary quantitative metric for measuring precision (degree of reproducibility) of evaluation scores in NLP/ML.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Metrological Perspective on Reproducibility in NLP *</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / Machine Learning (evaluation metrology)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Quantifying variability/reproducibility of evaluation scores (e.g., weighted F1) across multiple reproductions of an experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>CV* is used to capture variability arising from any changed measurement conditions, including code differences, environment, data splits, seeds, measurement method implementations, and human rater differences.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>CV* defined with small-sample correction: CV* = (1 + 1/(4n)) * (s* / |m|), where s* is unbiased sample standard deviation and m the sample mean; confidence intervals for s* computed with t-distribution; standard error of s* approximated from se(s^2).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Applied to essay scoring reproductions: CV* values reported include 14.63 (mult-base), 4.50 (mult-dep −), 4.39 (mult-dep +), 17.03 (mult-emb −), 16.23 (mult-emb +), indicating substantial differences in reproducibility depending on system/component.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Used as the reproducibility/precision metric R or R0 (when conditions identical) to summarise spread of measured quantity values across n measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>CV* provides a single comparable numeric summary of reproducibility; examples show some methods/features (syntactic features) have low CV* (~4–4.5) indicating better reproducibility, while embedding-based methods show high CV* (~16–17) indicating poor reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>CV* can be sensitive on scales that don't start at zero (requires shifting), assumes approximate normality of measured values for confidence intervals, and small sample sizes increase uncertainty (necessitating unbiased estimators and approximations).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use unbiased estimators (s*), t-distribution CIs, small-sample correction for CV*, and shift scales to start at zero where required; report CV* with confidence intervals and specify measurement conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Demonstrated to give meaningful, comparable reproducibility scores in example reproductions; no direct quantitative reduction-of-variance results reported from applying mitigation procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Intended for samples of measured values; authors recommend n >= 2 but advocate for multiple repeats; example uses up to 8 values per variant.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CV* (with small-sample correction and unbiased s*) is an appropriate, comparable metric for reproducibility/precision in NLP evaluation scores and reveals pronounced differences in reproducibility across system components (e.g., embeddings vs syntactic features).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Metrological Perspective on Reproducibility in NLP*', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e468.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e468.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIM_definitions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VIM repeatability and reproducibility definitions (International Vocabulary of Metrology)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard metrology definitions that define repeatability as measurement precision under repeatability conditions (same procedure, operators, system, conditions, location) and reproducibility as precision under reproducibility conditions (at least one condition differs), framing reproducibility as a property of measurements parameterized by measurand, object, time, and conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>International vocabulary of metrology: Basic and general concepts and associated terms (VIM)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Metrology applied to Natural Language Processing / Machine Learning reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Providing formal definitions and conceptual framing for assessing repeatability and reproducibility of evaluation measurements in NLP/ML.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Conceptualized as any changed condition value among the set: measurement procedure, operators, measuring system, operating conditions, location, time, test data, implementation, environment, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Provides formalism for defining R0 (repeatability) and R (reproducibility) as precision computed over measured values obtained under specified condition sets; does not prescribe metric but implies computing measurement precision.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>VIM does not specify which exact conditions to include for a given measurement type nor how to compute precision; applying these definitions in NLP requires specifying condition attributes and choosing a precision metric suitable for small-sample study designs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Adopting standard definitions encourages explicit specification of conditions of measurement (attribute/value pairs), baseline repeatability assessment before reproducibility tests, and transparent reporting of which conditions were varied.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adopting standard metrological definitions clarifies that reproducibility is a property of measurements (not of conclusions) and points directly to a procedure (compute precision of measured values under specified conditions) for assessing reproducibility quantitatively in NLP/ML.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Metrological Perspective on Reproducibility in NLP*', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e468.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e468.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conditions & Checklists</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditions of Measurement and Reproducibility Checklists (system/code/data/environment/test-procedure templates)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sets of named measurement-condition attributes (e.g., system code, compile/training info, measurement method spec/implementation, test set, who performed measurement) and community checklists (e.g., Pineau's ML reproducibility checklist, ACL reproducibility checklist, the Human Evaluation Datasheet) recommended to capture sources of variation and enable repeatability/reproducibility assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Metrological Perspective on Reproducibility in NLP *</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / Machine Learning (evaluation practice)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Standardizing capture and reporting of attributes (conditions) that can affect measured evaluation outcomes to enable reproducibility assessment and computing precision (CV*).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>The checklists aim to capture variability arising from system code and version, compilation/training details, measurement method specification and implementation, measurement procedure, test set and data splits, who performed evaluation, runtime environment, and other provenance/metadata attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>When used with QRA, these conditions allow grouping measurements by shared condition values and computing R or R0 (CV*) for those groups; checklists themselves are not metrics but enable metric computations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Current practice often lacks comprehensive condition reporting, making baseline repeatability assessment impossible in many reproduction studies; checklists require community adoption and create recording overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Adopt and routinely record standardized condition templates (e.g., Pineau's checklist, ACL checklist, Human Evaluation Datasheet); require authors to report conditions alongside metrics; use these reported conditions to perform QRA and repeatability baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>No direct quantitative effectiveness reported here, but paper argues that routine use of condition templates enables meaningful repeatability baselines and more informative reproducibility assessments (qualitative effectiveness).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly recording conditions of measurement via standard templates or checklists is necessary to (i) enable repeatability assessment as a baseline, (ii) identify which changed conditions drive reproducibility differences, and (iii) make CV*-based reproducibility assessments interpretable and comparable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Metrological Perspective on Reproducibility in NLP*', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Quantifying reproducibility in NLP and ML <em>(Rating: 2)</em></li>
                <li>Quantified reproducibility assessment of NLP results <em>(Rating: 2)</em></li>
                <li>A systematic review of reproducibility research in natural language processing <em>(Rating: 2)</em></li>
                <li>The machine learning reproducibility checklist v2.0 <em>(Rating: 2)</em></li>
                <li>ReproGen'20: A shared task fostering reproducibility (REPROLANG 2020 overview) <em>(Rating: 1)</em></li>
                <li>Reproduction and replication: A case study with automatic essay scoring <em>(Rating: 1)</em></li>
                <li>Automatic proficiency scoring of Czech, English, German, Italian, and Spanish learner essays <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-468",
    "paper_id": "paper-254151854",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "QRA",
            "name_full": "Quantified Reproducibility Assessment",
            "brief_description": "A metrology-inspired procedure for quantifying the precision (degree of reproducibility) of measured evaluation scores in NLP/ML by (i) specifying conditions of measurement, (ii) collecting measured quantity values across measurements, and (iii) computing a precision metric (CV*) to yield repeatability/reproducibility scores.",
            "citation_title": "A Metrological Perspective on Reproducibility in NLP *",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Processing / Machine Learning (metrology of evaluation)",
            "experimental_task": "Assessing reproducibility of NLP system evaluation scores (example: weighted F1 scores for multilingual essay scoring variants) by comparing original and reproduction measurements under specified conditions.",
            "variability_sources": "Sources explicitly identified: differences in system code and implementation, compile/training information, measurement method specification and implementation, measurement procedure, test set splits, 'performed by' (different teams/people), runtime/compile environments, random seeding methods, different data splits, different methods for obtaining/using word embeddings, errors/mismatches between published descriptions and code, and residual uneliminated variation despite sharing artifacts.",
            "variability_measured": true,
            "variability_metrics": "Unbiased coefficient of variation (CV*), unbiased sample standard deviation s*, mean, confidence intervals via t-distribution; precision defined as CV* for a set of measured values.",
            "variability_results": "Example application: CV* scores for wF1 across reproductions — mult-base: 14.63; mult-dep −: 4.50; mult-dep +: 4.39; mult-emb −: 17.03; mult-emb +: 16.23. Also reported: in a survey of 513 original/reproduction pairs, only 14% were exactly identical and 60% of differing reproductions were worse than the original.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Precision-based reproducibility score R computed as CV* across measurements; baseline repeatability score R0 when identical conditions available; comparisons of R0 vs R to estimate effect of changed conditions. Also use of subsets of measurements sharing more conditions and reporting R per subset.",
            "reproducibility_results": "QRA yields numeric reproducibility scores (CV*) that enable comparison across reproductions; example shows large variability across system variants (CV* ranging ~4.39 to ~17.03) and that only 14% of 513 original/reproduction pairs matched exactly (Belz et al. 2021a).",
            "reproducibility_challenges": "Lack of standardized condition reporting in papers, small sample sizes for reproduction measurements, residual variation even when code/data shared, limited information available from publications preventing repeatability assessment, implementation or evaluation-code errors, and difficulty assigning which changed condition caused variation.",
            "mitigation_methods": "Proposed: record and publish standardized conditions of measurement (system code, compile/training info, measurement method spec/implementation, procedure, test set, who performed it); use 2-phase QRA (repeatability baseline then reproducibility tests); compute and report CV* (and confidence intervals); include repeatability assessment during method development; use reproducibility checklists and human-evaluation datasheets; automated CV* checking integrated into development.",
            "mitigation_effectiveness": "Not reported as quantified effect sizes from interventions in this paper; QRA is shown to identify weak links (e.g., embeddings pipeline) and point to where improvements are needed, but numerical improvements from mitigations are not presented here.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Varied by scenario; procedure requires n &gt;= 2. Example application used original measurement plus up to seven reproduction scores per variant (i.e., up to n ≈ 8 measured values in examples).",
            "key_findings": "QRA operationalizes reproducibility as measurement precision (CV*), enabling quantitative comparison of reproducibility across studies; reproducibility in NLP is heterogeneous (CV* in examples ranged ≈4–17), many reproductions differ from originals (only 14% exact matches), and standardized recording of measurement conditions plus repeatability baselines are recommended to identify and reduce sources of variability.",
            "uuid": "e468.0",
            "source_info": {
                "paper_title": "A Metrological Perspective on Reproducibility in NLP*",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "CV*",
            "name_full": "Unbiased Coefficient of Variation (CV*)",
            "brief_description": "A small-sample corrected, unbiased estimator of the coefficient of variation (standard deviation divided by mean) recommended in this work as the primary quantitative metric for measuring precision (degree of reproducibility) of evaluation scores in NLP/ML.",
            "citation_title": "A Metrological Perspective on Reproducibility in NLP *",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Processing / Machine Learning (evaluation metrology)",
            "experimental_task": "Quantifying variability/reproducibility of evaluation scores (e.g., weighted F1) across multiple reproductions of an experiment.",
            "variability_sources": "CV* is used to capture variability arising from any changed measurement conditions, including code differences, environment, data splits, seeds, measurement method implementations, and human rater differences.",
            "variability_measured": true,
            "variability_metrics": "CV* defined with small-sample correction: CV* = (1 + 1/(4n)) * (s* / |m|), where s* is unbiased sample standard deviation and m the sample mean; confidence intervals for s* computed with t-distribution; standard error of s* approximated from se(s^2).",
            "variability_results": "Applied to essay scoring reproductions: CV* values reported include 14.63 (mult-base), 4.50 (mult-dep −), 4.39 (mult-dep +), 17.03 (mult-emb −), 16.23 (mult-emb +), indicating substantial differences in reproducibility depending on system/component.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Used as the reproducibility/precision metric R or R0 (when conditions identical) to summarise spread of measured quantity values across n measurements.",
            "reproducibility_results": "CV* provides a single comparable numeric summary of reproducibility; examples show some methods/features (syntactic features) have low CV* (~4–4.5) indicating better reproducibility, while embedding-based methods show high CV* (~16–17) indicating poor reproducibility.",
            "reproducibility_challenges": "CV* can be sensitive on scales that don't start at zero (requires shifting), assumes approximate normality of measured values for confidence intervals, and small sample sizes increase uncertainty (necessitating unbiased estimators and approximations).",
            "mitigation_methods": "Use unbiased estimators (s*), t-distribution CIs, small-sample correction for CV*, and shift scales to start at zero where required; report CV* with confidence intervals and specify measurement conditions.",
            "mitigation_effectiveness": "Demonstrated to give meaningful, comparable reproducibility scores in example reproductions; no direct quantitative reduction-of-variance results reported from applying mitigation procedures.",
            "comparison_with_without_controls": false,
            "number_of_runs": "Intended for samples of measured values; authors recommend n &gt;= 2 but advocate for multiple repeats; example uses up to 8 values per variant.",
            "key_findings": "CV* (with small-sample correction and unbiased s*) is an appropriate, comparable metric for reproducibility/precision in NLP evaluation scores and reveals pronounced differences in reproducibility across system components (e.g., embeddings vs syntactic features).",
            "uuid": "e468.1",
            "source_info": {
                "paper_title": "A Metrological Perspective on Reproducibility in NLP*",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "VIM_definitions",
            "name_full": "VIM repeatability and reproducibility definitions (International Vocabulary of Metrology)",
            "brief_description": "Standard metrology definitions that define repeatability as measurement precision under repeatability conditions (same procedure, operators, system, conditions, location) and reproducibility as precision under reproducibility conditions (at least one condition differs), framing reproducibility as a property of measurements parameterized by measurand, object, time, and conditions.",
            "citation_title": "International vocabulary of metrology: Basic and general concepts and associated terms (VIM)",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Metrology applied to Natural Language Processing / Machine Learning reproducibility",
            "experimental_task": "Providing formal definitions and conceptual framing for assessing repeatability and reproducibility of evaluation measurements in NLP/ML.",
            "variability_sources": "Conceptualized as any changed condition value among the set: measurement procedure, operators, measuring system, operating conditions, location, time, test data, implementation, environment, etc.",
            "variability_measured": null,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": null,
            "reproducibility_metrics": "Provides formalism for defining R0 (repeatability) and R (reproducibility) as precision computed over measured values obtained under specified condition sets; does not prescribe metric but implies computing measurement precision.",
            "reproducibility_results": null,
            "reproducibility_challenges": "VIM does not specify which exact conditions to include for a given measurement type nor how to compute precision; applying these definitions in NLP requires specifying condition attributes and choosing a precision metric suitable for small-sample study designs.",
            "mitigation_methods": "Adopting standard definitions encourages explicit specification of conditions of measurement (attribute/value pairs), baseline repeatability assessment before reproducibility tests, and transparent reporting of which conditions were varied.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": null,
            "number_of_runs": null,
            "key_findings": "Adopting standard metrological definitions clarifies that reproducibility is a property of measurements (not of conclusions) and points directly to a procedure (compute precision of measured values under specified conditions) for assessing reproducibility quantitatively in NLP/ML.",
            "uuid": "e468.2",
            "source_info": {
                "paper_title": "A Metrological Perspective on Reproducibility in NLP*",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Conditions & Checklists",
            "name_full": "Conditions of Measurement and Reproducibility Checklists (system/code/data/environment/test-procedure templates)",
            "brief_description": "Sets of named measurement-condition attributes (e.g., system code, compile/training info, measurement method spec/implementation, test set, who performed measurement) and community checklists (e.g., Pineau's ML reproducibility checklist, ACL reproducibility checklist, the Human Evaluation Datasheet) recommended to capture sources of variation and enable repeatability/reproducibility assessment.",
            "citation_title": "A Metrological Perspective on Reproducibility in NLP *",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural Language Processing / Machine Learning (evaluation practice)",
            "experimental_task": "Standardizing capture and reporting of attributes (conditions) that can affect measured evaluation outcomes to enable reproducibility assessment and computing precision (CV*).",
            "variability_sources": "The checklists aim to capture variability arising from system code and version, compilation/training details, measurement method specification and implementation, measurement procedure, test set and data splits, who performed evaluation, runtime environment, and other provenance/metadata attributes.",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": null,
            "reproducibility_metrics": "When used with QRA, these conditions allow grouping measurements by shared condition values and computing R or R0 (CV*) for those groups; checklists themselves are not metrics but enable metric computations.",
            "reproducibility_results": null,
            "reproducibility_challenges": "Current practice often lacks comprehensive condition reporting, making baseline repeatability assessment impossible in many reproduction studies; checklists require community adoption and create recording overhead.",
            "mitigation_methods": "Adopt and routinely record standardized condition templates (e.g., Pineau's checklist, ACL checklist, Human Evaluation Datasheet); require authors to report conditions alongside metrics; use these reported conditions to perform QRA and repeatability baselines.",
            "mitigation_effectiveness": "No direct quantitative effectiveness reported here, but paper argues that routine use of condition templates enables meaningful repeatability baselines and more informative reproducibility assessments (qualitative effectiveness).",
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Explicitly recording conditions of measurement via standard templates or checklists is necessary to (i) enable repeatability assessment as a baseline, (ii) identify which changed conditions drive reproducibility differences, and (iii) make CV*-based reproducibility assessments interpretable and comparable.",
            "uuid": "e468.3",
            "source_info": {
                "paper_title": "A Metrological Perspective on Reproducibility in NLP*",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Quantifying reproducibility in NLP and ML",
            "rating": 2,
            "sanitized_title": "quantifying_reproducibility_in_nlp_and_ml"
        },
        {
            "paper_title": "Quantified reproducibility assessment of NLP results",
            "rating": 2,
            "sanitized_title": "quantified_reproducibility_assessment_of_nlp_results"
        },
        {
            "paper_title": "A systematic review of reproducibility research in natural language processing",
            "rating": 2,
            "sanitized_title": "a_systematic_review_of_reproducibility_research_in_natural_language_processing"
        },
        {
            "paper_title": "The machine learning reproducibility checklist v2.0",
            "rating": 2,
            "sanitized_title": "the_machine_learning_reproducibility_checklist_v20"
        },
        {
            "paper_title": "ReproGen'20: A shared task fostering reproducibility (REPROLANG 2020 overview)",
            "rating": 1,
            "sanitized_title": "reprogen20_a_shared_task_fostering_reproducibility_reprolang_2020_overview"
        },
        {
            "paper_title": "Reproduction and replication: A case study with automatic essay scoring",
            "rating": 1,
            "sanitized_title": "reproduction_and_replication_a_case_study_with_automatic_essay_scoring"
        },
        {
            "paper_title": "Automatic proficiency scoring of Czech, English, German, Italian, and Spanish learner essays",
            "rating": 1,
            "sanitized_title": "automatic_proficiency_scoring_of_czech_english_german_italian_and_spanish_learner_essays"
        }
    ],
    "cost": 0.01239825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Metrological Perspective on Reproducibility in NLP *</p>
<p>Anya Belz anya.belz@adaptcentre.ie 
ADAPT Research Centre
Dublin City University University of Aberdeen</p>
<p>A Metrological Perspective on Reproducibility in NLP *
10.1162/coli
Reproducibility has become an increasingly debated topic in NLP and ML over recent years, but so far, no commonly accepted definitions of even basic terms or concepts have emerged. The range of different definitions proposed within NLP/ML not only do not agree with each other, they are also not aligned with standard scientific definitions. This article examines the standard definitions of repeatability and reproducibility provided by the meta-science of metrology, and explores what they imply in terms of how to assess reproducibility, and what adopting them would mean for reproducibility assessment in NLP/ML. It turns out the standard definitions lead directly to a method for assessing reproducibility in quantified terms that renders results from reproduction studies comparable across multiple reproductions of the same original study, as well as reproductions of different original studies. The article considers where this method sits in relation to other aspects of NLP work one might wish to assess in the context of reproducibility. * This article makes the general case for (i) adopting general scientific definitions of reproducibility and repeatability, and (ii) computing precision as the quantified measure of the degree of reproducibility of scores, as in other scientific disciplines. The QRA approach itself is described and discussed in more detail in Belz (2021), and tested on a variety of reproduction scenarios in Belz, Popovic, and Mille (2022). The code for computing CV * and several example score sets from the above papers can be found here: github.com/asbelz/coeff-var.</p>
<p>Introduction</p>
<p>Reproducibility of results is coming under increasing scrutiny in the machine learning (ML) and natural language processing (NLP) fields, against the background of a perceived reproducibility crisis in science more widely (Baker 2016), and NLP/ML specifically (Mieskes et al. 2019). There have been workshops and checklist initiatives, conferences promoting reproducibility via calls, chairs' blogs and special themes, and the first shared tasks, including REPROLANG'20 (Branco et al. 2020) and ReproGen'21 (Belz et al. 2021b).</p>
<p>Despite this growing body of research on reproducibility, it has been observed that no standard terms and definitions have emerged (Cohen et al. 2018). For the Association of Computing Machinery (ACM 2020), results have been reproduced if obtained in a different study by a different team using artifacts supplied in part by the original authors, and replicated if obtained in a different study by a different team using artifacts not supplied by the original authors. For Drummond (2009), replicability is defined as an experiment being able to be re-run exactly, whereas reproducibility is the ability to obtain the same result by different means. For Rougier et al. (2017), " [r]eproducing the result of a computation means running the same software on the same input data and obtaining the same results. [...]. Replicating a published result means writing and then running new software based on the description of a computational model or method." Wieling, Rawee, and van Noord (2018) tie reproducibility to "the same data and methods," and Whitaker (2017), followed by Schloss (2018), tie definitions of reproducibility, replicability, robustness, and generalizability to different combinations of same vs. different data and code. For Cohen et al. (2018) reproducibility is "a property of the outcomes of an experiment: arriving-or not-at the same conclusions, findings, or values," whereas they "exclude issues related to the ability to repeat the experiments reported in a paper [which is] replicability or repeatability."</p>
<p>No two of the above definitions are fully in agreement with each other, and none are entirely aligned with the standard scientific definitions of repeatability and reproducibility used in other fields and codified in the International Vocabulary of Metrology (VIM) (JCGM 2012). In fact, the US National Information Standards Organization had to ask the ACM to "harmonize its terminology and definitions with those used in the broader scientific research community" (ACM 2020), which prompted the latter to switch its definitions of replicability and reproducibility, without however achieving full alignment with the standard definitions of the two terms.</p>
<p>The remainder of this article examines the VIM definitions and the kind of approach to assessing reproducibility their adoption for NLP/ML would result in. Section 2 presents and discusses the VIM definitions (JCGM 2012), Section 3 lays out what reproducibility assessment looks like if wholly based on them, including practical steps and an example application in NLP. Section 4 discusses the limits of the proposed approach and a variety of questions that have come up in discussions and reviews of this work. The article concludes (Section 5) with a consideration of what would need to change for Quantified Reproducibility Assessment (QRA) to become part of NLP workflows, and what incentive the field might have to accept the implied overhead.</p>
<p>Definitions from VIM and Implications for Reproducibility Assessment</p>
<p>The International Vocabulary of Metrology (VIM) (JCGM 2012) defines repeatability and reproducibility as follows (defined terms in bold, see VIM for subsidiary defined terms): 2.21 measurement repeatability (or repeatability, for short) is measurement precision under a set of repeatability conditions of measurement.</p>
<p>2.20 a repeatability condition of measurement (repeatability condition) is a condition of measurement, out of a set of conditions that includes the same measurement procedure, same operators, same measuring system, same operating conditions and same location, and replicate measurements on the same or similar objects over a short period of time. 1 2.25 measurement reproducibility (reproducibility) is measurement precision under reproducibility conditions of measurement.</p>
<p>2.24 a reproducibility condition of measurement (reproducibility condition) is a condition of measurement, out of a set of conditions that includes different locations, operators, measuring systems, etc. A specification should give the conditions changed and unchanged, to the extent practical.</p>
<p>In other words, repeatability and reproducibility are properties not of objects, scores, results, or conclusions, but of measurements M that are parameterized by measurand m, object O, time t, and conditions of measurement C and return a measured quantity value v. They are defined as measurement precision, that is, quantified by calculating the precision of a set of measured quantity values, relative to a set of conditions of measurement, which have to be known and specified for assessment of repeatability and reproducibility to be meaningful. These definitions map directly to the following formal definitions of repeatability and reproducibility. First, repeatability R 0 (where all conditions of measurement are fixed):
R 0 (M 1 , M 2 , . . . M n ) := Precision(v 1 , v 2 , . . . v n ), where M i : (m, O, t i , C) → v i(1)
and the M i are repeat measurements for measurand m performed on object O at different times t i under (the same) set of conditions C. Reproducibility R is defined in the same way except that condition values differ for at least one condition in the M i (hence C i is here subscripted):
R(M 1 , M 2 , . . . M n ) := Precision(v 1 , v 2 , . . . v n ), where M i : (m, O, t i , C i ) → v i(2)
VIM does not tell us how to compute precision. Below, the unbiased coefficient of variation is used, but other measures are possible. The members of each set of conditions can be construed as attribute/value pairs each consisting of a name and a value. VIM does not tell us which exact set of measurement conditions to use; these depend on measurement type.</p>
<p>Assessing Reproduction Results in Practice</p>
<p>Generally, when carrying out a reproduction study of some original work in ML/NLP, we consider the system and how it was created and trained, the scores that were obtained for it, and the claims made on the basis of the scores. In existing work, the corresponding questions that are addressed in assessing the results of reproduction are of three broad types: how easily can the system be recreated; how similar are the scores; and can the same claims be supported. Only the second of these is about reproducibility in the general scientific sense, and only is it answerable from metrology. In order to have a complete approach to answering it in practice we need three more components in addition to the definitions from Section 2: (i) a method for computing precision; (ii) specified conditions of measurement; and (iii) a procedure for carrying out reproducibility assessments. Each is addressed in turn below, followed by an example application.</p>
<p>Computing Precision.</p>
<p>In other fields, measurement precision is typically reported in terms of the coefficient of variation (CV) defined as the standard deviation over the mean (of a sample of measured quantity values). Other values reported can include: mean along side standard deviation with 95% confidence intervals, coefficient of variation, and percentage of values within n standard deviations. CV serves well as the "headline" result, because it is a general measure, not in the unit of the measurements (unlike mean and standard deviation), providing a quantification of degree of reproducibility that is comparable across studies (Ahmed 1995, page 57). This also holds for percentage within n standard deviations but the latter is less recognized and intuitive.</p>
<p>In reproduction studies in NLP/ML, sample sizes tend to be very small. We therefore need to use de-biased sample estimators: We use the unbiased sample standard deviation, denoted s * , with confidence intervals calculated using a t-distribution, and standard error of s * approximated on the basis of the standard error of the unbiased sample variance se(s 2 ) as se s 2 (s * ) ≈ 1 2σ se(s 2 ) (Rao 1973). Assuming that measured quantity values are normally distributed, we calculate the standard error of the sample variance in the usual way: se(s 2 ) = 2σ 4 n−1 . Finally, we also use a small sample correction (Sokal and Rohlf 1971) for the coefficient of variation, which gives us the unbiased coefficient of variation CV * , defined as follows: 2
CV * = 1 + 1 4n s * |m|
where s * is the unbiased standard deviation, and m the sample mean.</p>
<p>(3)</p>
<p>Before applying CV * to values on scales that do not start at 0 (in NLP this happens mostly in human evaluations) values need to be shifted to start at 0 to ensure comparability. 3</p>
<p>Conditions of Measurement.</p>
<p>Individual conditions are specified by name and value, and their role is to capture those attributes of a measurement where different values may cause differences in measured quantity values. For repeatability assessment, conditions need to capture all sources of variation in results (although not necessarily each with a separate condition). It so happens that much of the reproducibility work in ML and NLP has so far focused on what standard conditions of measurement (information about system, data, dependencies, computing environment, etc.) for metric measurements need to be specified in order to enable repeatability assessment, even if it hasn't been couched in these terms. Reproducibility checklists such as those provided by Pineau (2020) and the ACL 4 for metric evaluation, and evaluation datasheets like the one proposed by Shimorina and Belz (2021) for human evaluation, are lists of types of information (attributes), for which authors are asked to provide information (values), that can directly be construed as conditions of measurement. In the example in the following section, we use the following conditions of measurement, based on the above checklists and described in more detail in Belz, Popovic, and Mille (2022):</p>
<p>1.</p>
<p>Object conditions: (a) System code, (b) Compile/training information.</p>
<p>2.</p>
<p>Measurement method conditions: 5 (a) Measurement method specification, (b) Measurement method implementation.</p>
<p>3.</p>
<p>Measurement procedure conditions: 6 (a) Measurement procedure, (b) Test set, (c) Performed by.</p>
<p>The names of the conditions of measurement used in this paper are as given above. The values for each condition characterize how measurements differ in respect to the condition; in reporting results from QRA tests below, we use paper and method identifiers as shorthand for distinct condition values (full details in each case being available from the referenced papers). The intention here is not to propose a definitive set of conditions for general use; that is beyond the scope of this article, and at any rate should evolve by consensus over time.</p>
<p>Reproducibility Assessment Procedure. We now have all the components needed for quantified reproducibility assessment, but how do we go about carrying it out in practice? From metrology we know we need to compute the precision of two or more measured quantity values obtained in measurements of the same object and measurand with specified conditions of measurement, identical in the case of repeatability and differing in the case of reproducibility. There are two possible starting positions: (a) the original and (some) reproduction studies have been carried out; and (b) the original study has been carried out, but no reproductions.</p>
<p>In the case of a, the conditions of measurement that can be used are limited to the information that can be gleaned from publications, code/data repositories, and the authors. This rules out repeatability assessment in most cases. In such cases, reproducibility assessment involves the steps indicated in the bottom half of Figure 1, termed 1-Phase QRA. The example QRA below is such a case. If the set of measurements includes subsets of measurements that have more conditions in common, then it makes sense to report R for these subsets separately.</p>
<p>In the case of b, repeatability assessment can be carried out, and arguably should be carried out before any reproducibility assessment. For a true estimate of the variation resulting from given differences in condition values, the baseline variation, present when all condition values are the same, needs to be known. Repeatability assessment should therefore be carried out prior to reproducibility assessment. For example, if the coefficient of variation is x C 0 under identical conditions C 0 , and x C i under varied conditions C i , then it is the difference between x C 0 and x C i that estimates the effect of varying the conditions. The steps in this 2-phase reproducibility assessment are shown at the top of Figure 1.</p>
<p>Example Application. 7 As an example of how QRA can be used in practice we apply the approach outlined above, in its 1-phase version with conditions of measurement and CV * as given above, to a set of reproductions of the essay scoring system evaluations reported by Vajjala and Rama (2018), which were carried out as part of REPROLANG (Branco et al. 2020). More particularly, we look at five of the 11 multilingual essay scoring system variants and the weighted F1 scores (wF1) computed for them. Table 1 2-PHASE QUANTIFIED REPRODUCIBILITY ASSESSMENT</p>
<p>REPEATABILITY PHASE</p>
<ol>
<li>Select measurement to be assessed, identify shared object and measurand.</li>
</ol>
<p>2.</p>
<p>Select initial set of repeatability conditions of measurement C 0 , specify value for each condition. 3.</p>
<p>Perform n ≥ 2 reproduction measurements to yield measured quantity
values v 0 1 , v 0 2 , . . . v 0 n . 4.
Compute precision for v 0 1 , v 0 2 , . . . v 0 n , giving repeatability score R 0 . 5.</p>
<p>Unless precision is as small as desired, identify additional conditions that had different values in some of the reproduction measurements, and add them to the set of measurement conditions, also updating the measurements to ensure same values for the new conditions. Repeat Steps 3-5.</p>
<p>REPRODUCIBILITY PHASE</p>
<p>6.</p>
<p>From the final set of repeatability conditions, select the conditions to vary, and specify the different values to test. 7.</p>
<p>For each combination of differing condition values:</p>
<p>(a) Carry out n reproduction tests, yielding measured quantity values v 1 , v 2 , . . . v n .</p>
<p>(b) Compute precision for v 1 , v 2 , . . . v n , giving reproducibility score R.</p>
<p>Report all resulting R scores, alongside baseline R 0 score.</p>
<p>1-PHASE QUANTIFIED REPRODUCIBILITY ASSESSMENT</p>
<p>1.</p>
<p>For set of n measurements to be assessed, identify object and measurand.</p>
<p>2.</p>
<p>Identify all conditions of measurement C for which information is available for all measurements M i , and specify condition values for each measurement.</p>
<p>3.</p>
<p>Gather the n measured quantity values v 1 , v 2 , . . . v n . 4.</p>
<p>Compute precision of v 1 , v 2 , . . . v n , giving reproducibility score R.</p>
<p>Report resulting R score.</p>
<p>Figure 1</p>
<p>Top: Steps in 2-Phase QRA with baseline repeatability assessment (omit Step 5 for standard conditions of measurement). Bottom: Steps in 1-Phase QRA where baseline repeatability assessment is impossible). shows object (system variant) and measurand (wF1) in the first two columns. The baseline classifier (mult-base) uses document length (number of words) as its only feature. For the other variants, +/− indicates that the multilingual classifier was / was not given information about which language the input was in; mult-dep uses n-grams over dependency relation, dependent POS, and head POS triples; and mult-emb uses word and character embeddings. The mult-base model is a logistic regressor, the others are random forests. As can be see from the Performed by column and the measured quantity values (wF1 scores) in the second-to-last column, for each system variant, we have the original Table 1 QRA results for five of the multilingual essay scoring system variants reported by Vajjala and Rama (2018). Reproductions by Huber and Çöltekin (2020); Arhiliuc, Mitrović, and Granitzer (2020); Bestgen (2020); Caines and Buttery (2020 score reported by Vajjala and Rama, and seven reproduction scores from four different papers (Huber and Çöltekin 2020;Arhiliuc, Mitrović, and Granitzer 2020;Bestgen 2020;Caines and Buttery 2020). The conditions of measurement described above are given in columns 3-9; for space reasons the condition values are given as paper and method IDs; for example, Test set = Va.&amp;Ra./d2 means the original data was used, but the test set split was different. Finally, CV * scores are given in the last column. Results show that system variant pairs that differ only in whether they use language information have very similar CV * scores. For example, mult-dep − (without language information) and mult-dep + (with language information) have a CV * of 4.5 and 4.39, respectively. This tendency holds for all such pairs (including the ones not shown here), indicating that using language information makes next to no difference to reproducibility. 8 Results also show that the syntactic information is obtained/used in a way that is particularly reproducible, whereas the word embeddings are obtained/used in a way that is particularly hard to reproduce. Overall, the random forest models using syntactic features have the best reproducibility; the logistic regressors using domainspecific features (mult-dom systems not included here) have the worst.</p>
<p>These insights can be used in different ways. The substantial differences in CV * seen here might prompt further code checking (and indeed there seem to have been issues with the mult-base code according to three of the reproduction papers). Knowing that the method for obtaining word embeddings is associated with particularly poor reproducibility might prompt an attempt to improve it. Automated CV * checking can be built into further development of the system to ensure good reproducibility of baseline variants.</p>
<p>Discussion</p>
<p>Is QRA all we need? QRA directly addresses the second of the questions (how similar are scores) mentioned at the start of Section 3 by computing degree of reproducibility as a function of scores. It also contributes to answering the first (how recreatable is the system) by providing pointers to which conditions of measurement (which aspect of system, training, compiling, evaluation, etc.) may be causing poor reproducibility. And it contributes to answering the third question (can the same conclusions be drawn) by providing a quantitative basis for deciding what is otherwise a subjective judgment. That is not to say that QRA does everything; in particular, conducting study-level comparisons (e.g., correlation between sets of scores, system rankings, and significant differences) provides additional insights (Fokkens et al. 2013).</p>
<p>What counts as a good level of reproducibility? This differs substantially between disciplines and contexts. In bio-science assays, precision (CV) ranges from &lt;10 for enzyme assays, to 20-50 for in vivo and cell-based assays, and &gt;300 for virus titre assays (AAH/USFWS n.d.). For NLP, typical CV ranges would have to be established over time, but it seems clear that we would expect them to be much lower (better) for metric-based measurements than for human-assessments. For wF1 measurements, the &gt; 15 CV * scores above seem very high.</p>
<p>Does it matter how similar scores are? CV * provides a quantitative basis for deciding if the same conclusions can be drawn as discussed above, but looking at the degree of similarity of scores relative to similarity of conditions of measurement can also reveal important information, as exemplified by the systematic patterns in CV * found in the example application in the last section. Knowing what are expected degrees of reproducibility for a type of system or evaluation method helps pinpoint problems when those expectations aren't met.</p>
<p>If score differences in reproductions of the same system are larger than score differences reported as a new state of the art for the same task elsewhere, then that's a problem because it is unclear whether higher scores are in fact due to methodological improvements. If reproduction scores are, as would seem reasonable to assume, normally distributed, then overall, half of all reproduction scores should be better than the original scores, and half worse. But that is not the case: a recent survey (Belz et al. 2021a) showed that 60% of all reproductions that are different are in fact worse. In this way, examining differences between scores can facilitate higher-level insights, in this case, an apparent tendency to cherry-pick better results.</p>
<p>Differences in quality between studies. In the course of a reproduction study, it can happen that the reproducing authors discover what they consider errors in code or mismatches between the published description of the code and the code itself. In such cases, some judgment is involved: there is no point in reproducing an erroneous original study; if the code is different from the description, but not otherwise deemed problematic, a reproduction with corrected description might still make sense. Beyond actual errors, conditions of measurement capture the similarities and differences (some of which may be deemed a matter of quality) between different studies.</p>
<p>QRA in NLP workflows. In order to put metrological principles and quantified reproducibility assessment into practice, two things are needed at the field level: (i) recording conditions of measurement for all evaluations using a standard template; and (ii) routinely reporting CV * and conditions of measurement for reproduction studies. Ideally, the third element would be to conduct repeatability assessment as part of developing new evaluation methods, and usefully even as part of new applications of existing methods.</p>
<p>Conclusion</p>
<p>The reproducibility debate in NLP/ML has been framed in terms of exactly what information we need to share so that others are guaranteed to obtain the same metric scores, and initially the expectation was that "[r]eproducibility would be quite easy to achieve in machine learning simply by sharing the full code used for experiments" (Sonnenburg et al. 2007(Sonnenburg et al. , page 2450. What is becoming clear, however, is that no matter how much of our code, data, and ancillary information we share, residual amounts of variation remain that are stubbornly resistant to being eliminated. A recent survey (Belz et al. 2021a) found that just 14% of the 513 original/reproduction score pairs analyzed were exactly the same. Judging the remainder simply "not reproduced" is of limited usefulness, as some are much closer to being the same than others. On the other hand, judging whether the same conclusions can be drawn on the basis of possibly different scores without a quantitative basis is prone to subjectivity and low agreement.</p>
<p>QRA offers an alternative by quantifying the closeness of results in a way that is comparable across different studies and can be used to establish, over time, expected levels of closeness for different types of systems and evaluation methods. Such expected CV * levels and the individual CV * results for given reproductions moreover provide a quantitative basis for deciding whether the same conclusions can be drawn, as well as providing information about the recreatability of systems and evaluation methods.</p>
<p>Reproducibility is one of the cornerstones of scientific research: Inability to reproduce results within accepted limits is, with few exceptions, seen as casting doubt on their validity. Building QRA into NLP workflows comes with an overhead, both in terms of recording information about systems and evaluations in a standardized form, and in terms of carrying out reproduciblity assessments. It can often seem in NLP that the performance-improvement paradigm and the pressure to maximize publications does not allow for any additional work not directly contributing to increasing performance or publications. In medicine and the physical sciences more generally it's inconceivable that reproducibility assessment could be viewed as an optional extra. As NLP/ML matures as a science, perhaps it's time that we too insist on our results being verifiably reliable, including in the important sense of being reproducible.</p>
<p>). OTE = outputs vs. targets evaluation; si = different random seeding method; envi = different compile/run-time environments; di = different test data splits. Ra./s? Arhiliuc&amp;al. wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Arhiliuc&amp;al. 0.426 Va.&amp; Ra./s1 Va.&amp; Ra./env1 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra.Ra./s1 Va.&amp; Ra./env3 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Cai.&amp;But. Ra./s? Arhiliuc&amp;al. wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Arhiliuc&amp;al. 0.650 Va.&amp; Ra./s1 Va.&amp; Ra./env1 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Bestgen 0.651 Va.&amp; Ra./s1 Va.&amp; Ra./env2 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra.Va.&amp; Ra./s1 Va.&amp; Ra./env3 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Cai.&amp;But. Va.&amp; Ra./s2 Hub.&amp;Colt. wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d2 Hub.&amp;Colt. 0.661 Va.&amp; Ra./s? Arhiliuc&amp;al. wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Arhiliuc&amp;al. 0.652 Va.&amp; Ra./s1 Va.&amp; Ra./env1 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Bestgen 0.653 Va.&amp; Ra./s1 Va.&amp; Ra./env2 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Bestgen 0.699 Va.&amp; Ra./s2 Va.&amp; Ra./env2 wF1(o,t) ≈Va.&amp; Ra. OTE Va.&amp; Ra./d3 Bestgen 0.712 Va.&amp; Ra./s1 Va.&amp; Ra./env3 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Cai.&amp;But. Ra./s1 Va.&amp; Ra./env3 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Cai.&amp;But. Ra./s? Arhiliuc&amp;al. wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Arhiliuc&amp;al. 0.681 Va.&amp; Ra./s1 Va.&amp; Ra./env1 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Bestgen 0.659 Va.&amp; Ra./s1 Va.&amp; Ra./env2 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra.Object conditions 
Measurement method 
Measurement procedure 
Measured 
Object 
Meas-
conditions 
conditions 
quantity CV  *  </p>
<p>urand 
Code by 
Compiled/ 
Method Implemented Proc-
Test set 
Performed 
value 
trained by 
by 
edure 
by </p>
<p>mult-base 
wF1 </p>
<p>Va.&amp; Ra./s1 
Va.&amp; Ra. 
wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Va.&amp; Ra. 
0.428 </p>
<p>14.63 </p>
<p>Va.&amp; Ra./s2 Hub.&amp;Colt. wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d2 Hub.&amp;Colt. 
0.493 
Va.&amp; /d1 
Bestgen 
0.574 
Va.&amp; Ra./s1 Va.&amp; Ra./env2 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 
Bestgen 
0.579 </p>
<p>Va.&amp; Ra./s2 Va.&amp; Ra./env2 wF1(o,t) ≈Va.&amp; Ra. OTE Va.&amp; Ra./d3 </p>
<p>Bestgen 
0.590 
Va.&amp; 0.574 
Cai.&amp;But. 
Cai.&amp;But. 
wF1(o,t) Cai.&amp;But. OTE Va.&amp;Ra./d4 Cai.&amp;But. 
0.600 </p>
<p>mult-dep − wF1 </p>
<p>Va.&amp; Ra./s1 
Va.&amp; Ra. 
wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Va.&amp; Ra. 
0.703 </p>
<p>4.5 </p>
<p>Va.&amp; Ra./s2 Hub.&amp;Colt. wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d2 Hub.&amp;Colt. 
0.660 
Va.&amp; /d1 
Bestgen 
0.699 </p>
<p>Va.&amp; Ra./s2 Va.&amp; Ra./env2 wF1(o,t) ≈Va.&amp; Ra. OTE Va.&amp; Ra./d3 </p>
<p>Bestgen 
0.711 
0.651 
Cai.&amp;But. 
Cai.&amp;But. 
wF1(o,t) Cai.&amp;But. OTE Va.&amp;Ra./d4 Cai.&amp;But. 
0.710 </p>
<p>mult-dep + wF1 </p>
<p>Va.&amp; Ra./s1 
Va.&amp; Ra. 
wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Va.&amp; Ra. 
0.693 </p>
<p>4.39 </p>
<p>0.653 
Cai.&amp;But. 
Cai.&amp;But. 
wF1(o,t) Cai.&amp;But. OTE Va.&amp;Ra./d4 Cai.&amp;But. 
0.716 </p>
<p>mult-emb − wF1 </p>
<p>Va.&amp; Ra./s1 
Va.&amp; Ra. 
wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Va.&amp; Ra. 
0.693 </p>
<p>17.03 </p>
<p>Va.&amp; Ra./s2 Hub.&amp;Colt. wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d2 Hub.&amp;Colt. 
0.658 
Va.&amp; Ra./s? Arhiliuc&amp;al. wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Arhiliuc&amp;al. 
0.683 
Va.&amp; Ra./s1 Va.&amp; Ra./env1 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 
Bestgen 
0.668 
Va.&amp; Ra./s1 Va.&amp; Ra./env2 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 
Bestgen 
0.692 </p>
<p>Va.&amp; Ra./s2 Va.&amp; Ra./env2 wF1(o,t) ≈Va.&amp; Ra. OTE Va.&amp; Ra./d3 </p>
<p>Bestgen 
0.689 
Va.&amp; 0.659 
Cai.&amp;But. 
Cai.&amp;But. 
wF1(o,t) Cai.&amp;But. OTE Va.&amp;Ra./d4 Cai.&amp;But. 
0.391 </p>
<p>mult-emb + wF1 </p>
<p>Va.&amp; Ra./s1 
Va.&amp; Ra. 
wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Va.&amp; Ra. 
0.689 </p>
<p>16.23 </p>
<p>Va.&amp; Ra./s2 Hub.&amp;Colt. wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d2 Hub.&amp;Colt. 
0.662 
Va.&amp; /d1 
Bestgen 
0.681 </p>
<p>Va.&amp; Ra./s2 Va.&amp; Ra./env2 wF1(o,t) ≈Va.&amp; Ra. OTE Va.&amp; Ra./d3 </p>
<p>Bestgen 
0.684 
Va.&amp; Ra./s1 Va.&amp; Ra./env3 wF1(o,t) Va.&amp; Ra. OTE Va.&amp; Ra./d1 Cai.&amp;But. 
0.657 
Cai.&amp;But. 
Cai.&amp;But. 
wF1(o,t) Cai.&amp;But. OTE Va.&amp;Ra./d4 Cai.&amp;But. 
0.401 </p>
<p>In physical measurements objects can change simply as a function of time.
Code and data are available here: https://github.com/asbelz/coeff-var. 3 Otherwise CV * reflects differences solely due to different minimum values. 4 https://2021.aclweb.org/calls/reproducibility-checklist/.
For definition of 'measurement method', see VIM 2.5. 6 For definition of 'measurement procedure', see VIM 2.6. 7 We report four example applications, including human evaluations, in full detail elsewhere(Belz, Popovic, and Mille 2022), including the other six multilingual system variants from theVajjala and Rama (2018) reproductions below.
The high CV * for the baseline system may be due to an issue with the evaluation code (macro-F1 instead of weighted-F1), as reported byBestgen (2020);Caines and Buttery (2020); andHuber and Çöltekin (2020).</p>
<p>Assay validation methods: Definitions and terms. Aquatic Animal Health Program. N Aah/Usfws, U.S. Fish &amp; Wildlife ServiceAAH/USFWS. n.d. Assay validation methods: Definitions and terms. Aquatic Animal Health Program, U.S. Fish &amp; Wildlife Service.</p>
<p>ACM. 2020. Artifact review and badging. 1ACM. 2020. Artifact review and badging, Version 1.1, August 24, 2020. https:// www.acm.org/publications/policies/ artifact-review-and-badging-current</p>
<p>A pooling methodology for coefficient of variation. S E Ahmed, Sankhyā: The Indian Journal of Statistics, Series B. 57Ahmed, S. E. 1995. A pooling methodology for coefficient of variation. Sankhyā: The Indian Journal of Statistics, Series B, 57:57-75.</p>
<p>Language proficiency scoring. Cristina Arhiliuc, Jelena Mitrović, Michael Granitzer, Proceedings of The 12th Language Resources and Evaluation Conference. The 12th Language Resources and Evaluation ConferenceArhiliuc, Cristina, Jelena Mitrović, and Michael Granitzer. 2020. Language proficiency scoring. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5624-5630.</p>
<p>Reproducibility crisis. Monya Baker, Nature. 53326Baker, Monya. 2016. Reproducibility crisis. Nature, 533(26):353-366.</p>
<p>Anya Belz, arXiv:2109.01211Quantifying reproducibility in NLP and ML. arXiv preprintBelz, Anya. 2021. Quantifying reproducibility in NLP and ML. arXiv preprint arXiv:2109.01211.</p>
<p>A systematic review of reproducibility research in natural language processing. Anya Belz, Shubham Agarwal, Anastasia Shimorina, Ehud Reiter, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeBelz, Anya, Shubham Agarwal, Anastasia Shimorina, and Ehud Reiter. 2021a. A systematic review of reproducibility research in natural language processing. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 381-393.</p>
<p>Quantified reproducibility assessment of NLP results. Anya Belz, Maja Popovic, Simon Mille, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL'22). the 60th Annual Meeting of the Association for Computational Linguistics (ACL'22)Belz, Anya, Maja Popovic, and Simon Mille. 2022. Quantified reproducibility assessment of NLP results. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL'22), pages 16-28.</p>
<p>The ReproGen shared task on reproducibility of human evaluations in NLG: Overview and results. Anya Belz, Anastasia Shimorina, Shubham Agarwal, Ehud Reiter, Proceedings of the 14th International Conference on Natural Language Generation. the 14th International Conference on Natural Language GenerationBelz, Anya, Anastasia Shimorina, Shubham Agarwal, and Ehud Reiter. 2021b. The ReproGen shared task on reproducibility of human evaluations in NLG: Overview and results. In Proceedings of the 14th International Conference on Natural Language Generation, pages 249-258.</p>
<p>Reproducing monolingual, multilingual and cross-lingual CEFR predictions. Yves Bestgen, Proceedings of the 12th Language Resources and Evaluation Conference. the 12th Language Resources and Evaluation ConferenceBestgen, Yves. 2020. Reproducing monolingual, multilingual and cross-lingual CEFR predictions. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 5595-5602.</p>
<p>André Moreira, and Willem Elbers. 2020. A shared task of a new, collaborative type to foster reproducibility: A first exercise in the area of language science and technology with REPROLANG 2020. António Branco, Nicoletta Calzolari, Piek Vossen, Gertjan Van Noord, João Dieter Van Uytvanck, Luís Silva, Gomes, Proceedings of the 12th Language Resources and Evaluation Conference. the 12th Language Resources and Evaluation ConferenceBranco, António, Nicoletta Calzolari, Piek Vossen, Gertjan Van Noord, Dieter van Uytvanck, João Silva, Luís Gomes, André Moreira, and Willem Elbers. 2020. A shared task of a new, collaborative type to foster reproducibility: A first exercise in the area of language science and technology with REPROLANG 2020. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 5539-5545.</p>
<p>Automatic proficiency scoring of Czech, English, German, Italian, and Spanish learner essays. Andrew Caines, Paula Buttery, Proceedings of the 12th Language Resources and Evaluation Conference. the 12th Language Resources and Evaluation Conference2020Caines, Andrew and Paula Buttery. 2020. REPROLANG 2020: Automatic proficiency scoring of Czech, English, German, Italian, and Spanish learner essays. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 5614-5623.</p>
<p>Three dimensions of reproducibility in natural language processing. K Cohen, Jingbo Bretonnel, Pierre Xia, Tiffany Zweigenbaum, Orin Callahan, Foster Hargraves, Nancy Goss, Aurélie Ide, Cyril Névéol, Lawrence E Grouin, Hunter, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)Cohen, K. Bretonnel, Jingbo Xia, Pierre Zweigenbaum, Tiffany Callahan, Orin Hargraves, Foster Goss, Nancy Ide, Aurélie Névéol, Cyril Grouin, and Lawrence E. Hunter. 2018. Three dimensions of reproducibility in natural language processing. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), pages 156-165.</p>
<p>Replicability is not reproducibility: Nor is it good science. Chris Drummond, Presented at 4th Workshop on Evaluation Methods for Machine Learning held at ICML'09. Drummond, Chris. 2009. Replicability is not reproducibility: Nor is it good science. Presented at 4th Workshop on Evaluation Methods for Machine Learning held at ICML'09.</p>
<p>Offspring from reproduction problems: What replication failure teaches us. Antske Fokkens, Marieke Van Erp, Marten Postma, Ted Pedersen, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. the 51st Annual Meeting of the Association for Computational LinguisticsLong Papers1Piek Vossen, and Nuno FreireFokkens, Antske, Marieke van Erp, Marten Postma, Ted Pedersen, Piek Vossen, and Nuno Freire. 2013. Offspring from reproduction problems: What replication failure teaches us. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1691-1701.</p>
<p>Reproduction and replication: A case study with automatic essay scoring. Eva Huber, Agrı Çöltekin, Proceedings of the 12th Language Resources and Evaluation Conference. the 12th Language Resources and Evaluation ConferenceHuber, Eva and Ç agrı Çöltekin. 2020. Reproduction and replication: A case study with automatic essay scoring. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 5603-5613.</p>
<p>International vocabulary of metrology: Basic and general concepts and associated terms (VIM). Jcgm , Joint Committee for Guides in MetrologyJCGM. 2012. International vocabulary of metrology: Basic and general concepts and associated terms (VIM). Joint Committee for Guides in Metrology. https://www.bipm .org/utils/common/documents/jcgm /JCGM 200 2012.pdf.</p>
<p>Community perspective on replicability in natural language processing. Margot Mieskes, Karën Fort, Aurélie Névéol, Cyril Grouin, Kevin Cohen, Proceedings of the International Conference on Recent Advances in Natural Language Processing. the International Conference on Recent Advances in Natural Language ProcessingRANLP 2019Mieskes, Margot, Karën Fort, Aurélie Névéol, Cyril Grouin, and Kevin Cohen. 2019. Community perspective on replicability in natural language processing. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019), pages 768-775.</p>
<p>The machine learning reproducibility checklist v2. Joelle Pineau, Pineau, Joelle. 2020. The machine learning reproducibility checklist v2.0. https:// www.cs.mcgill.ca/ jpineau /ReproducibilityChecklist.pdf.</p>
<p>Linear Statistical Inference and its Applications. Calyampudi Rao, Radhakrishna, WileyRao, Calyampudi Radhakrishna. 1973. Linear Statistical Inference and its Applications. Wiley.</p>
<p>Sustainable computational science: The ReScience initiative. Nicolas P Rougier, Konrad Hinsen, Frédéric Alexandre, Thomas Arildsen, Lorena A Barba, C Y Fabien, C Titus Benureau, Pierre De Brown, Ozan Buyl, Andrew P Caglayan, Davison, PeerJ Computer Science. 3142Rougier, Nicolas P., Konrad Hinsen, Frédéric Alexandre, Thomas Arildsen, Lorena A. Barba, Fabien C. Y. Benureau, C. Titus Brown, Pierre De Buyl, Ozan Caglayan, Andrew P. Davison, et al. 2017. Sustainable computational science: The ReScience initiative. PeerJ Computer Science, 3:e142.</p>
<p>Identifying and overcoming threats to reproducibility, replicability, robustness, and generalizability in microbiome research. Patrick D Schloss, MBio. 93Schloss, Patrick D. 2018. Identifying and overcoming threats to reproducibility, replicability, robustness, and generalizability in microbiome research. MBio, 9(3):e00525-18.</p>
<p>The human evaluation datasheet 1.0: A template for recording details of human evaluation experiments in NLP. Anastasia Shimorina, Anya Belz, arXiv:3910940arXiv preprintShimorina, Anastasia and Anya Belz. 2021. The human evaluation datasheet 1.0: A template for recording details of human evaluation experiments in NLP. arXiv preprint arXiv:3910940.</p>
<p>Biometry: The Principles and Practice of Statistics in Biological Research. R R Sokal, F J Rohlf, W. H. FreemanSokal, R. R. and F. J. Rohlf. 1971. Biometry: The Principles and Practice of Statistics in Biological Research. W. H. Freeman.</p>
<p>The need for open source software in machine learning. Sonnenburg, Mikio L Soren, Braun, Cheng Soon, Samy Ong, Leon Bengio, Geoffrey Bottou, Yann Holmes, Klaus-Robert Lecunn, Fernando Muller, Carl Edward Pereira, Rasmussen, Journal of Machine Learning Research. 8Sonnenburg, Soren, Mikio L. Braun, Cheng Soon Ong, Samy Bengio, Leon Bottou, Geoffrey Holmes, Yann LeCunn, Klaus-Robert Muller, Fernando Pereira, Carl Edward Rasmussen, et al. 2007. The need for open source software in machine learning. Journal of Machine Learning Research, 8:2443-2466.</p>
<p>Experiments with universal CEFR classification. Sowmya Vajjala, Taraka Rama, Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications. the Thirteenth Workshop on Innovative Use of NLP for Building Educational ApplicationsVajjala, Sowmya and Taraka Rama. 2018. Experiments with universal CEFR classification. In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 147-153.</p>
<p>The MT Reproducibility Checklist. Presented at the Open Science in Practice Summer School. Kirstie Whitaker, Whitaker, Kirstie. 2017. The MT Reproducibility Checklist. Presented at the Open Science in Practice Summer School (https://osip2017.epfl.ch/page -145979.html) 2017.</p>
<p>Reproducibility in computational linguistics: Are we willing to share?. Martijn Wieling, Josine Rawee, Gertjan Van Noord, Computational Linguistics. 444Wieling, Martijn, Josine Rawee, and Gertjan van Noord. 2018. Reproducibility in computational linguistics: Are we willing to share? Computational Linguistics, 44(4):641-649.</p>            </div>
        </div>

    </div>
</body>
</html>
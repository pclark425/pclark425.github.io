<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5992 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5992</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5992</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-272380956</p>
                <p><strong>Paper Title:</strong> Large Language Models for Mental Health Applications: Systematic Review</p>
                <p><strong>Paper Abstract:</strong> Background: Large language models (LLMs) are advanced artificial neural networks trained on extensive datasets to accurately understand and generate natural language. While they have received much attention and demonstrated potential in digital health, their application in mental health, particularly in clinical settings, has generated considerable debate. Objective: This systematic review aims to critically assess the use of LLMs in mental health, specifically focusing on their applicability and efficacy in early screening, digital interventions, and clinical settings. By systematically collating and assessing the evidence from current studies, our work analyzes models, methodologies, data sources, and outcomes, thereby highlighting the potential of LLMs in mental health, the challenges they present, and the prospects for their clinical use. Methods: Adhering to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines, this review searched 5 open-access databases: MEDLINE (accessed by PubMed), IEEE Xplore, Scopus, JMIR, and ACM Digital Library. Keywords used were ( mental health OR mental illness OR mental disorder OR psychiatry ) AND ( large language models ). This study included articles published between January 1, 2017, and April 30, 2024, and excluded articles published in languages other than English. Results: In total, 40 articles were evaluated, including 15 (38%) articles on mental health conditions and suicidal ideation detection through text analysis, 7 (18%) on the use of LLMs as mental health conversational agents, and 18 (45%) on other applications and evaluations of LLMs in mental health. LLMs show good effectiveness in detecting mental health issues</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5992.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5992.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments an LLM's generative output by first retrieving relevant passages from an external document store or knowledge base and conditioning generation on those retrieved evidences to reduce hallucinations and provide up-to-date facts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines a retrieval component (search/index over external documents) with an LLM generator so that responses are grounded in retrieved evidence; presented in the paper as a recommended approach to keep LLM outputs current and evidence-based.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Paper mentions general use with instruction-tuned LLMs and gives GPT-4 / ChatGPT as example LLMs in nearby discussion; RAG itself is described as model-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Inputs described as user prompts plus queries against an external document corpus (paper states 'numerous documents' / 'external knowledge bases'); the review does not specify an exact number of scholarly papers processed in any experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Retrieval + grounded generation: retrieve supporting passages from a document corpus and use them as context for LLM generation so outputs are supported by source excerpts rather than only the LLM's parametric memory.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Evidence-augmented natural language answers or summaries (responses that cite or incorporate retrieved document passages); proposed outputs include more up-to-date, evidence-based answers rather than free-form hallucinated text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>The review does not report a specific RAG evaluation experiment; it recommends standard evaluation approaches such as human evaluation of factuality and referencing, and suggests RAG can be evaluated by checking whether generated outputs cite correct retrieved evidence and by human expert assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>No empirical RAG results are reported in this systematic review; RAG is described conceptually as a mitigation for outdated LLM training cutoffs and hallucinations rather than evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not specified for RAG in the review; general suggestion that RAG should retrieve from up-to-date external knowledge bases or corpora (e.g., domain-specific corpora, literature collections), but no concrete scholarly-corpus name is provided in the RAG discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires high-quality retrieval corpora and accurate retrieval; integration complexity between retriever and generator; does not by itself guarantee elimination of hallucinations if retrieval fails or if generator misuses retrieved context; the review notes lack of concrete benchmarked clinical evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Positioned against plain LLM generation with fixed training cutoffs (e.g., GPT-4 with cutoff); RAG is described as superior for keeping answers current and more evidence-grounded, but the review provides no quantitative comparative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Mental Health Applications: Systematic Review', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5992.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5992.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-aug</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Graph Augmentation / Knowledge Graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using knowledge graphs to represent extracted entities and relations from text to structure scientific facts and make LLM outputs more interpretable and amenable to reasoning and synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge-graph augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructing or augmenting a knowledge graph from textual sources (potentially with LLM-assisted relation extraction) to organize facts and relationships that can be used to support structured synthesis and improve interpretability of LLM-derived claims.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not tied to a particular LLM in the review; suggested as a complementary technique to LLMs (e.g., used alongside attention-based transformers such as GPT-family or instruction-tuned models).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Inputs would be textual documents (papers, clinical notes, social media posts) though the review does not specify exact corpus sizes; the paper mentions knowledge graphs as a promising approach for outlining logical relationships and facts extracted from many documents.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Information extraction (entities/relations) from documents followed by graph construction; LLMs could be used to extract relations or to map natural language statements into graph triples; used as a structured distillation of collective evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured knowledge graph / set of triples and possibly human-readable summaries derived from the graph; aims to produce interpretable representations of relationships among concepts (facts, claims).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not empirically evaluated in this review; proposed evaluation would include checking relation/extraction accuracy against expert-annotated ground truth and assessing interpretability and utility for downstream tasks via human expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>No concrete outcomes or metrics reported in the review for knowledge-graph-based pipelines; idea presented as a recommended future direction to improve interpretability and traceability.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not specified; the review suggests constructing domain-specific curated datasets (expert-labeled) to build reliable mental-health lexicons and graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires high-quality extraction and normalization across heterogeneous sources; mapping noisy or contradictory text into a coherent causal graph is nontrivial; paper highlights lack of expert-annotated multilingual datasets and the black-box nature of LLMs as obstacles.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Presented as complementary to plain generative LLM outputs (which are less interpretable); knowledge graphs are suggested to enhance interpretability relative to end-to-end generation but no empirical comparison is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Mental Health Applications: Systematic Review', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5992.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5992.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-guided causal embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-guided dedicated embedding space aligned with a causal graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed technique to create a specialized embedding space, guided by LLMs, that aligns textual embeddings with a causal/graph structure to enable approximate counterfactual matching and more causally coherent synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-guided causal embedding space</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>During training, a dedicated embedding space is constructed (guided by an LLM) so that document/text embeddings correspond to nodes or positions in a causal graph; this facilitates matching, retrieval, and counterfactual-style comparisons for synthesis and theory construction.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Described as a conceptual/training method applicable to transformer LLMs (paper references GPT-family models elsewhere) but no specific implementation or model version is reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Not concretely specified; intended inputs are document-level texts (e.g., papers, transcripts) from which embeddings are learned and aligned to a causal graph representation; paper does not provide corpus size or exact input counts.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Embed documents into a specialized vector space whose geometry corresponds to causal relations, then use nearest-neighbor / graph-based aggregation to synthesize evidence and approximate counterfactuals for theory distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Matches or retrieved clusters aligned with causal concepts, enabling structured summaries or causal hypotheses rather than only surface-level summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>The review suggests this as a research direction; concrete evaluation approaches are not given but would likely include ability to recover known causal relations and expert assessment of synthesized causal claims.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>No empirical results reported in the reviewed literature; concept presented as a promising avenue to improve interpretability and counterfactual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not specified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Citation notes conceptual challenges: need to define causal graphs, curate reliable training signals, and ensure alignment between learned embeddings and causal structure; lack of public, expert-annotated datasets is a barrier.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Framed as potentially superior to naive embedding-based retrieval for causal reasoning and theory distillation, but no empirical comparisons are provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Mental Health Applications: Systematic Review', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5992.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5992.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hallucination-detect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hallucination detection and mitigation (metric ensembles & self-contradiction detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Techniques mentioned to detect and mitigate hallucinations in LLM outputs, including ensembles of metrics to flag ungrounded claims and methods to detect self-contradictory statements from models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Metric-ensemble hallucination detection / self-contradiction detection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Use multiple automated metrics and disagreement/self-contradiction checks to detect likely hallucinated content produced by LLMs; recommended as part of quality control when synthesizing large corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Discussed generally for large instruction-tuned LLMs (GPT-3.5/GPT-4/others) rather than tied to a single model in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Applied to generated outputs (summaries or claims) produced from document corpora; the review does not specify corpus sizes or precise input volumes for these detection methods.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Not a distillation method per se but a post-generation verification step: apply multiple detection metrics (e.g., factual consistency metrics, entailment checks, cross-retrieval checks) and self-consistency checks to flag and filter outputs before final synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Flagged or filtered summaries/claims with confidence scores or alerts indicating suspected hallucinations; can be used to trigger human review.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>The review references research on evaluation/detection methods (metric ensembles and self-contradiction detection) but does not report experimental evaluations within the mental-health literature covered; suggests human-in-the-loop verification is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>No quantitative outcomes from the review itself; referenced literature (not evaluated in this review) is cited as investigating these detection approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not specified for hallucination detection within reviewed studies; general benchmark approaches implied (human-annotated factuality datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Detection metrics are imperfect, can have false positives/negatives, and require careful selection/tuning; review stresses the need for human expert oversight despite automated detection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Presented as mitigation layered on top of generative pipelines and complementary to retrieval grounding (RAG); no head-to-head empirical comparisons reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Mental Health Applications: Systematic Review', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5992.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5992.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temp-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoding temperature tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adjusting the sampling temperature parameter during decoding to trade off randomness and determinism of LLM outputs; suggested as a simple lever to reduce hallucinations and increase factual consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Decoding temperature tuning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Modulate the LLM sampling 'temperature' (lower values -> more deterministic outputs) to reduce the likelihood of improbable or hallucinatory continuations when producing summaries or synthesized claims.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Described as applicable to common LLMs (e.g., GPT-family, instruction-tuned models); the review references temperature as a generic model parameter rather than reporting use with a specific version in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Applied to prompt+context inputs during generation; no specific corpus size reported.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Parameter-level mitigation rather than structural distillation: produce outputs with lower temperature (more deterministic) when factual precision is prioritized, and higher temperature when creativity/empathy is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated text with differing variability; low-temperature outputs are more repetitive/deterministic (favored for documentation/summaries), high-temperature outputs are more diverse (favored for therapeutic conversation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>The review discusses conceptual trade-offs and suggests task-specific tuning; no concrete experiments or metrics are reported in this paper measuring temperature effects on synthesis of scholarly content.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>No quantitative results reported in the review; temperature tuning is presented as a recommended control variable with task-dependent trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Not applicable / not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Lowering temperature may reduce hallucinations but also reduce creativity and personalization (undesirable in therapeutic contexts); optimal temperature is task-dependent and nontrivial to choose.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Described as orthogonal to retrieval grounding and post-hoc hallucination detection; can be combined with RAG and metric ensembles but is not a substitute for grounding or human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models for Mental Health Applications: Systematic Review', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What is RAG (Retrieval Enhanced Generation)? <em>(Rating: 2)</em></li>
                <li>What is retrieval-augmented generation (RAG)? <em>(Rating: 2)</em></li>
                <li>Self-contradictory hallucinations of large language models: evaluation, detection and mitigation <em>(Rating: 2)</em></li>
                <li>Metric ensembles for hallucination detection <em>(Rating: 2)</em></li>
                <li>Towards interpretable mental health analysis with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5992",
    "paper_id": "paper-272380956",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A method that augments an LLM's generative output by first retrieving relevant passages from an external document store or knowledge base and conditioning generation on those retrieved evidences to reduce hallucinations and provide up-to-date facts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "Combines a retrieval component (search/index over external documents) with an LLM generator so that responses are grounded in retrieved evidence; presented in the paper as a recommended approach to keep LLM outputs current and evidence-based.",
            "llm_model_used": "Paper mentions general use with instruction-tuned LLMs and gives GPT-4 / ChatGPT as example LLMs in nearby discussion; RAG itself is described as model-agnostic.",
            "input_type_and_size": "Inputs described as user prompts plus queries against an external document corpus (paper states 'numerous documents' / 'external knowledge bases'); the review does not specify an exact number of scholarly papers processed in any experiment.",
            "distillation_approach": "Retrieval + grounded generation: retrieve supporting passages from a document corpus and use them as context for LLM generation so outputs are supported by source excerpts rather than only the LLM's parametric memory.",
            "output_type": "Evidence-augmented natural language answers or summaries (responses that cite or incorporate retrieved document passages); proposed outputs include more up-to-date, evidence-based answers rather than free-form hallucinated text.",
            "evaluation_methods": "The review does not report a specific RAG evaluation experiment; it recommends standard evaluation approaches such as human evaluation of factuality and referencing, and suggests RAG can be evaluated by checking whether generated outputs cite correct retrieved evidence and by human expert assessment.",
            "results": "No empirical RAG results are reported in this systematic review; RAG is described conceptually as a mitigation for outdated LLM training cutoffs and hallucinations rather than evaluated in this paper.",
            "datasets_or_benchmarks": "Not specified for RAG in the review; general suggestion that RAG should retrieve from up-to-date external knowledge bases or corpora (e.g., domain-specific corpora, literature collections), but no concrete scholarly-corpus name is provided in the RAG discussion.",
            "challenges_or_limitations": "Requires high-quality retrieval corpora and accurate retrieval; integration complexity between retriever and generator; does not by itself guarantee elimination of hallucinations if retrieval fails or if generator misuses retrieved context; the review notes lack of concrete benchmarked clinical evaluations.",
            "comparisons_to_other_methods": "Positioned against plain LLM generation with fixed training cutoffs (e.g., GPT-4 with cutoff); RAG is described as superior for keeping answers current and more evidence-grounded, but the review provides no quantitative comparative experiments.",
            "uuid": "e5992.0",
            "source_info": {
                "paper_title": "Large Language Models for Mental Health Applications: Systematic Review",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "KG-aug",
            "name_full": "Knowledge-Graph Augmentation / Knowledge Graphs",
            "brief_description": "Using knowledge graphs to represent extracted entities and relations from text to structure scientific facts and make LLM outputs more interpretable and amenable to reasoning and synthesis.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Knowledge-graph augmentation",
            "system_description": "Constructing or augmenting a knowledge graph from textual sources (potentially with LLM-assisted relation extraction) to organize facts and relationships that can be used to support structured synthesis and improve interpretability of LLM-derived claims.",
            "llm_model_used": "Not tied to a particular LLM in the review; suggested as a complementary technique to LLMs (e.g., used alongside attention-based transformers such as GPT-family or instruction-tuned models).",
            "input_type_and_size": "Inputs would be textual documents (papers, clinical notes, social media posts) though the review does not specify exact corpus sizes; the paper mentions knowledge graphs as a promising approach for outlining logical relationships and facts extracted from many documents.",
            "distillation_approach": "Information extraction (entities/relations) from documents followed by graph construction; LLMs could be used to extract relations or to map natural language statements into graph triples; used as a structured distillation of collective evidence.",
            "output_type": "Structured knowledge graph / set of triples and possibly human-readable summaries derived from the graph; aims to produce interpretable representations of relationships among concepts (facts, claims).",
            "evaluation_methods": "Not empirically evaluated in this review; proposed evaluation would include checking relation/extraction accuracy against expert-annotated ground truth and assessing interpretability and utility for downstream tasks via human expert review.",
            "results": "No concrete outcomes or metrics reported in the review for knowledge-graph-based pipelines; idea presented as a recommended future direction to improve interpretability and traceability.",
            "datasets_or_benchmarks": "Not specified; the review suggests constructing domain-specific curated datasets (expert-labeled) to build reliable mental-health lexicons and graphs.",
            "challenges_or_limitations": "Requires high-quality extraction and normalization across heterogeneous sources; mapping noisy or contradictory text into a coherent causal graph is nontrivial; paper highlights lack of expert-annotated multilingual datasets and the black-box nature of LLMs as obstacles.",
            "comparisons_to_other_methods": "Presented as complementary to plain generative LLM outputs (which are less interpretable); knowledge graphs are suggested to enhance interpretability relative to end-to-end generation but no empirical comparison is provided.",
            "uuid": "e5992.1",
            "source_info": {
                "paper_title": "Large Language Models for Mental Health Applications: Systematic Review",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLM-guided causal embeddings",
            "name_full": "LLM-guided dedicated embedding space aligned with a causal graph",
            "brief_description": "A proposed technique to create a specialized embedding space, guided by LLMs, that aligns textual embeddings with a causal/graph structure to enable approximate counterfactual matching and more causally coherent synthesis.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM-guided causal embedding space",
            "system_description": "During training, a dedicated embedding space is constructed (guided by an LLM) so that document/text embeddings correspond to nodes or positions in a causal graph; this facilitates matching, retrieval, and counterfactual-style comparisons for synthesis and theory construction.",
            "llm_model_used": "Described as a conceptual/training method applicable to transformer LLMs (paper references GPT-family models elsewhere) but no specific implementation or model version is reported in the review.",
            "input_type_and_size": "Not concretely specified; intended inputs are document-level texts (e.g., papers, transcripts) from which embeddings are learned and aligned to a causal graph representation; paper does not provide corpus size or exact input counts.",
            "distillation_approach": "Embed documents into a specialized vector space whose geometry corresponds to causal relations, then use nearest-neighbor / graph-based aggregation to synthesize evidence and approximate counterfactuals for theory distillation.",
            "output_type": "Matches or retrieved clusters aligned with causal concepts, enabling structured summaries or causal hypotheses rather than only surface-level summaries.",
            "evaluation_methods": "The review suggests this as a research direction; concrete evaluation approaches are not given but would likely include ability to recover known causal relations and expert assessment of synthesized causal claims.",
            "results": "No empirical results reported in the reviewed literature; concept presented as a promising avenue to improve interpretability and counterfactual reasoning.",
            "datasets_or_benchmarks": "Not specified in the review.",
            "challenges_or_limitations": "Citation notes conceptual challenges: need to define causal graphs, curate reliable training signals, and ensure alignment between learned embeddings and causal structure; lack of public, expert-annotated datasets is a barrier.",
            "comparisons_to_other_methods": "Framed as potentially superior to naive embedding-based retrieval for causal reasoning and theory distillation, but no empirical comparisons are provided in the review.",
            "uuid": "e5992.2",
            "source_info": {
                "paper_title": "Large Language Models for Mental Health Applications: Systematic Review",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Hallucination-detect",
            "name_full": "Hallucination detection and mitigation (metric ensembles & self-contradiction detection)",
            "brief_description": "Techniques mentioned to detect and mitigate hallucinations in LLM outputs, including ensembles of metrics to flag ungrounded claims and methods to detect self-contradictory statements from models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Metric-ensemble hallucination detection / self-contradiction detection",
            "system_description": "Use multiple automated metrics and disagreement/self-contradiction checks to detect likely hallucinated content produced by LLMs; recommended as part of quality control when synthesizing large corpora.",
            "llm_model_used": "Discussed generally for large instruction-tuned LLMs (GPT-3.5/GPT-4/others) rather than tied to a single model in the review.",
            "input_type_and_size": "Applied to generated outputs (summaries or claims) produced from document corpora; the review does not specify corpus sizes or precise input volumes for these detection methods.",
            "distillation_approach": "Not a distillation method per se but a post-generation verification step: apply multiple detection metrics (e.g., factual consistency metrics, entailment checks, cross-retrieval checks) and self-consistency checks to flag and filter outputs before final synthesis.",
            "output_type": "Flagged or filtered summaries/claims with confidence scores or alerts indicating suspected hallucinations; can be used to trigger human review.",
            "evaluation_methods": "The review references research on evaluation/detection methods (metric ensembles and self-contradiction detection) but does not report experimental evaluations within the mental-health literature covered; suggests human-in-the-loop verification is necessary.",
            "results": "No quantitative outcomes from the review itself; referenced literature (not evaluated in this review) is cited as investigating these detection approaches.",
            "datasets_or_benchmarks": "Not specified for hallucination detection within reviewed studies; general benchmark approaches implied (human-annotated factuality datasets).",
            "challenges_or_limitations": "Detection metrics are imperfect, can have false positives/negatives, and require careful selection/tuning; review stresses the need for human expert oversight despite automated detection.",
            "comparisons_to_other_methods": "Presented as mitigation layered on top of generative pipelines and complementary to retrieval grounding (RAG); no head-to-head empirical comparisons reported in the review.",
            "uuid": "e5992.3",
            "source_info": {
                "paper_title": "Large Language Models for Mental Health Applications: Systematic Review",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Temp-tuning",
            "name_full": "Decoding temperature tuning",
            "brief_description": "Adjusting the sampling temperature parameter during decoding to trade off randomness and determinism of LLM outputs; suggested as a simple lever to reduce hallucinations and increase factual consistency.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Decoding temperature tuning",
            "system_description": "Modulate the LLM sampling 'temperature' (lower values -&gt; more deterministic outputs) to reduce the likelihood of improbable or hallucinatory continuations when producing summaries or synthesized claims.",
            "llm_model_used": "Described as applicable to common LLMs (e.g., GPT-family, instruction-tuned models); the review references temperature as a generic model parameter rather than reporting use with a specific version in experiments.",
            "input_type_and_size": "Applied to prompt+context inputs during generation; no specific corpus size reported.",
            "distillation_approach": "Parameter-level mitigation rather than structural distillation: produce outputs with lower temperature (more deterministic) when factual precision is prioritized, and higher temperature when creativity/empathy is needed.",
            "output_type": "Generated text with differing variability; low-temperature outputs are more repetitive/deterministic (favored for documentation/summaries), high-temperature outputs are more diverse (favored for therapeutic conversation).",
            "evaluation_methods": "The review discusses conceptual trade-offs and suggests task-specific tuning; no concrete experiments or metrics are reported in this paper measuring temperature effects on synthesis of scholarly content.",
            "results": "No quantitative results reported in the review; temperature tuning is presented as a recommended control variable with task-dependent trade-offs.",
            "datasets_or_benchmarks": "Not applicable / not specified.",
            "challenges_or_limitations": "Lowering temperature may reduce hallucinations but also reduce creativity and personalization (undesirable in therapeutic contexts); optimal temperature is task-dependent and nontrivial to choose.",
            "comparisons_to_other_methods": "Described as orthogonal to retrieval grounding and post-hoc hallucination detection; can be combined with RAG and metric ensembles but is not a substitute for grounding or human oversight.",
            "uuid": "e5992.4",
            "source_info": {
                "paper_title": "Large Language Models for Mental Health Applications: Systematic Review",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What is RAG (Retrieval Enhanced Generation)?",
            "rating": 2,
            "sanitized_title": "what_is_rag_retrieval_enhanced_generation"
        },
        {
            "paper_title": "What is retrieval-augmented generation (RAG)?",
            "rating": 2,
            "sanitized_title": "what_is_retrievalaugmented_generation_rag"
        },
        {
            "paper_title": "Self-contradictory hallucinations of large language models: evaluation, detection and mitigation",
            "rating": 2,
            "sanitized_title": "selfcontradictory_hallucinations_of_large_language_models_evaluation_detection_and_mitigation"
        },
        {
            "paper_title": "Metric ensembles for hallucination detection",
            "rating": 2,
            "sanitized_title": "metric_ensembles_for_hallucination_detection"
        },
        {
            "paper_title": "Towards interpretable mental health analysis with large language models",
            "rating": 1,
            "sanitized_title": "towards_interpretable_mental_health_analysis_with_large_language_models"
        }
    ],
    "cost": 0.01628175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models for Mental Health Applications: Systematic Review</p>
<p>Zhijun Guo 
Institute of Health Informatics University College
London, LondonUnited Kingdom</p>
<p>MScAlvina Lai 
Institute of Health Informatics University College
London, LondonUnited Kingdom</p>
<p>Johan H Thygesen 
Institute of Health Informatics University College
London, LondonUnited Kingdom</p>
<p>Joseph Farrington 
Institute of Health Informatics University College
London, LondonUnited Kingdom</p>
<p>MScThomas Keen 
Institute of Health Informatics University College
London, LondonUnited Kingdom</p>
<p>Great Ormond Street Institute of Child Health
University College London
LondonUnited Kingdom</p>
<p>DPhilKezhi Li 
Institute of Health Informatics University College
London, LondonUnited Kingdom</p>
<p>Dphil 
Xsl  Fo </p>
<p>Institute of Health Informatics University College
222London</p>
<p>Euston Road London
NW1 2DAUnited Kingdom</p>
<p>Large Language Models for Mental Health Applications: Systematic Review
38B1F3172E082C5110D4D2D59AFEA12C10.2196/57400large language modelsmental healthdigital health careChatGPTBidirectional Encoder Representations from TransformersBERT
Background: Large language models (LLMs) are advanced artificial neural networks trained on extensive datasets to accurately understand and generate natural language.While they have received much attention and demonstrated potential in digital health, their application in mental health, particularly in clinical settings, has generated considerable debate.Objective: This systematic review aims to critically assess the use of LLMs in mental health, specifically focusing on their applicability and efficacy in early screening, digital interventions, and clinical settings.By systematically collating and assessing the evidence from current studies, our work analyzes models, methodologies, data sources, and outcomes, thereby highlighting the potential of LLMs in mental health, the challenges they present, and the prospects for their clinical use.Methods:Adhering to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines, this review searched 5 open-access databases: MEDLINE (accessed by PubMed), IEEE Xplore, Scopus, JMIR, and ACM Digital Library.Keywords used were (mental health OR mental illness OR mental disorder OR psychiatry) AND (large language models).This study included articles published between January 1, 2017, and April 30, 2024, and excluded articles published in languages other than English.Results: In total, 40 articles were evaluated, including 15 (38%) articles on mental health conditions and suicidal ideation detection through text analysis, 7 (18%) on the use of LLMs as mental health conversational agents, and 18 (45%) on other applications and evaluations of LLMs in mental health.LLMs show good effectiveness in detecting mental health issues and providing accessible, destigmatized eHealth services.However, assessments also indicate that the current risks associated with clinical use might surpass their benefits.These risks include inconsistencies in generated text; the production of hallucinations; and the absence of a comprehensive, benchmarked ethical framework.Conclusions:This systematic review examines the clinical applications of LLMs in mental health, highlighting their potential and inherent risks.The study identifies several issues: the lack of multilingual datasets annotated by experts, concerns regarding the accuracy and reliability of generated content, challenges in interpretability due to the "black box" nature of LLMs, and ongoing ethical dilemmas.These ethical concerns include the absence of a clear, benchmarked ethical framework; data privacy issues; and the potential for overreliance on LLMs by both physicians and patients, which could compromise traditional medical practices.As a result, LLMs should not be considered substitutes for professional mental health services.However, the rapid development of LLMs underscores their potential as valuable clinical aids, emphasizing the need for continued research and development in this area.</p>
<p>Introduction</p>
<p>Mental Health</p>
<p>Mental health, a critical component of overall well-being, is at the forefront of global health challenges [1].In 2019, an estimated 970 million individuals worldwide experienced mental illness, accounting for 12.5% of the global population [2].Anxiety and depression are among the most prevalent psychological conditions, affecting 301 million and 280 million individuals, respectively [2].In addition, 40 million people experienced bipolar disorder, 24 million experienced schizophrenia, and 14 million experienced eating disorders [3].These mental disorders collectively contribute to an estimated US $5 trillion in global economic losses annually [4].Despite the staggering prevalence, many cases remain undetected or untreated, with the resources allocated to the diagnosis and treatment of mental illness far less than the negative impact it has on society [5].Globally, untreated mental illnesses affect 5% of the population in high-income countries and 19% of the population in low-and middle-income countries [3].The COVID-19 pandemic has further exacerbated the challenges faced by mental health services worldwide [6], as the demand for these services increased while access was decreased [7].This escalating crisis underscores the urgent need for more innovative and accessible mental health care approaches.</p>
<p>Mental illness treatment encompasses a range of modalities, including medication, psychotherapy, support groups, hospitalization, and complementary and alternative medicine [8].However, the societal stigma attached to mental illnesses often deters people from seeking appropriate care [9].Influenced by the fear of judgment and concerns about costly, ineffective treatments [10], many people with mental illness avoid or delay psychotherapy [11].The COVID-19 crisis and other global pandemics have underscored the importance of digital tools, such as telemedicine and mobile apps, in delivering care during critical times [12].In this evolving context, large language models (LLMs) present new possibilities for enhancing the delivery and effectiveness of mental health care.</p>
<p>Recent technological advancements have revealed some unique advantages of LLMs in mental health.These models, capable of processing and generating text akin to human communication, provide accessible support directly to users [13].A study analyzing 2917 Reddit (Reddit, Inc) user reviews found that conversational agents (CAs) powered by LLMs are valued for their nonjudgmental listening and effective problem-solving advice.This aspect is particularly beneficial for individuals considered socially marginalized, as it enables them to be heard and understood without the need for direct social interaction [14].Moreover, LLMs enhance the accessibility of mental health services, which are notably undersupplied globally [15].Recent data reveals substantial delays in traditional mental health care delivery; 23% of individuals with mental illnesses report waiting for &gt;12 weeks for face-to-face psychotherapy sessions [16], with 12% waiting for &gt;6 months and 6% waiting for &gt;1 year [16].In addition, 43% of adults with mental illness indicate that such long waits have exacerbated their conditions [16].</p>
<p>Telemedicine, enhanced by LLMs, offers a practical alternative that expedites service delivery and could flatten traditional health care hierarchies [17].This includes real-time counseling sessions through CAs that are not only cost-effective but also accessible anytime and from any location.By reducing the reliance on physical visits to traditional health care settings, telemedicine has the potential to decentralize access to medical expertise and diminish the hierarchical structures within the health care system [17].Mental health chatbots developed using language models, such as Woebot [18] and Wysa [19], have been gaining recognition.Both chatbots follow the principles of cognitive behavioral therapy and are designed to equip users with self-help tools for managing their mental health issues [20].In clinical practice, LLMs hold the potential to support the automatic assessment of therapists' adherence to evidence-based practices and the development of systems that offer real-time feedback and support for patient homework between sessions [21].These models also have the potential to provide feedback on psychotherapy or peer support sessions, which is especially beneficial for clinicians with less training and experience [21].Currently, these applications are still in the proposal stage.Although promising, they are not yet widely used in routine clinical settings, and further evaluation of their feasibility and effectiveness is necessary.</p>
<p>The deployment of LLMs in mental health also poses several risks, particularly for groups considered vulnerable.Challenges such as inconsistencies in the content generated and the production of "hallucinatory" content may mislead or harm users [22], raising serious ethical concerns.In response, authorities such as the World Health Organization have developed ethical guidelines for artificial intelligence (AI) research in health care, emphasizing the importance of data privacy; human oversight; and the principle that AI tools should augment, rather than replace, human practitioners [23].These potential problems with LLMs in health care have gained considerable industry attention, underscoring the need for a comprehensive and responsible evaluation of LLMs' applications in mental health.The following section will further explore the workings of LLMs and their potential applications in mental health and critically evaluate the opportunities and challenges they introduce.</p>
<p>LLMs in Mental Health</p>
<p>LLMs represent advancements in machine learning, characterized by their ability to understand and generate human-like text with high accuracy [24].The efficacy of these models is typically evaluated using benchmarks designed to assess their linguistic fidelity and contextual relevance.Common metrics include Bilingual Evaluation Understudy for translation accuracy and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) for summarization tasks [25].LLMs are characterized</p>
<p>XSL  FO</p>
<p>RenderX by their scale, often encompassing billions of parameters, setting them apart from traditional language models [26].This breakthrough is largely due to the transformer architecture, a deep neural network structure that uses a "self-attention" mechanism developed by Vaswani et al [27].This allows LLMs to process information in parallel rather than sequentially, greatly enhancing speed and contextual understanding [27].To clearly define the scope of this study concerning LLMs, we specify that an LLM must use the transformer architecture and contain a high number of parameters, traditionally at least 1 billion, to qualify as "large" [28].This criterion encompasses models such as GPT (OpenAI) and Bidirectional Encoder Representations from Transformers (BERT; Google AI).Although the standard BERT model, with only 0.34 billion parameters [29], does not meet the traditional criteria for "large," its sophisticated bidirectional design and pivotal role in establishing new natural language processing (NLP) benchmarks justify its inclusion among notable LLMs [30].The introduction of ChatGPT (OpenAI) in 2022 generated substantial public and academic interest in LLMs, underlining their transformative potential within the field of AI [31].Other state-of-the-art LLMs include Large Language Model Meta AI (LLaMA; Meta AI) and Pathways Language Model (PaLM; Google AI), as illustrated in Table 1 [32][33][34][35].</p>
<p>LLMs are primarily designed to learn fundamental statistical patterns of language [36].Initially, these models were used as the basis for fine-tuning task-specific models rather than training those models from scratch, offering a more resource-efficient approach [37].This fine-tuning process involves adjusting a pretrained model to a specific task by further training it on a smaller, task-specific dataset [38].However, developments in larger and more sophisticated models have reduced the need for extensive fine-tuning in some cases.Notably, some advanced LLMs can now effectively understand and execute tasks specified through natural language prompts without extensive task-specific fine-tuning [39].Instruction fine-tuned models undergo additional training on pairs of user requests and appropriate responses.This training allows them to generalize across various complex tasks, such as sentiment analysis, which previously required explicit fine-tuning by researchers or developers [40].A key part of the input to these models, such as ChatGPT and Gemini (Google AI), includes a system prompt, often hidden from the user, which guides the model on how to interpret and respond to user prompts.For example, it might direct the model to act as a helpful mental health assistant.In addition, "prompt engineering" has emerged as a crucial technique in optimizing model performance.Prompt engineering involves crafting input texts that guide the model to produce the desired output without additional training.For example, refining a prompt from "Tell me about current events in health care" to "Summarize today's top news stories about technology in health care" provides the model with more specific guidance, which can enhance the relevance and accuracy of its responses [41].While prompt engineering can be highly effective and reduce the need to retrain the model, it is important to be wary of "hallucinations," a phenomenon where models confidently generate incorrect or irrelevant outputs [42].This can be particularly challenging in high-accuracy scenarios, such as health care and medical applications [43][44][45][46].Thus, while prompt engineering reduces the reliance on extensive fine-tuning, it underscores the need for thorough evaluation and testing to ensure the reliability of model outputs in sensitive applications.</p>
<p>The existing literature includes a review of the application of machine learning and NLP in mental health [47], analyses of LLMs in medicine [32], and a scoping review of LLMs in mental health.These studies have demonstrated the effectiveness of NLP for tasks such as text categorization and sentiment analysis [47] and provided a broad overview of LLM applications in mental health [48].However, a gap remains in systematically reviewing state-of-the-art LLMs in mental health, particularly in the comprehensive assessment of literature published since the introduction of the transformer architecture in 2017.This systematic review addresses these gaps by providing a more in-depth analysis; evaluating the quality and applicability of studies; and exploring ethical challenges specific to LLMs, such as data privacy, interpretability, and clinical integration.Unlike previous reviews, this study excludes preprints, follows a rigorous search strategy with clear inclusion and exclusion criteria (using Cohen  to assess the interreviewer agreement), and uses a detailed assessment of study quality and bias (using the Risk of Bias 2 tool) to ensure the reliability and reproducibility of the findings.</p>
<p>Guided by specific research questions, this systematic review critically assesses the use of LLMs in mental health, focusing on their applicability and efficacy in early screening, digital interventions, and clinical settings, as well as the methodologies and data sources used.The findings of this study highlight the potential of LLMs in enhancing mental health diagnostics and</p>
<p>Methods</p>
<p>This systematic review followed the PRISMA (Preferred Reporting Items for Systematic Review and Meta-Analyses) guidelines [49].The protocol was registered on PROSPERO (CRD42024508617).A PRISMA checklist is available in Multimedia Appendix 1.</p>
<p>Search Strategies</p>
<p>The search was initiated on August 3, 2024, and completed on August 6, 2024, by 1 author (ZG).ZG systematically searched 5 databases: MEDLINE, IEEE Xplore, Scopus, JMIR, and ACM Digital Library using the following search keywords: (mental health OR mental illness OR mental disorder OR psychiatry) and (large language models).These keywords were consistently applied across each database to ensure a uniform search strategy.To conduct a comprehensive and precise search for relevant literature, strategies were tailored for different databases.All metadata were searched in MEDLINE and IEEE Xplore, whereas the search in Scopus was confined to titles, abstracts, and keywords.The JMIR database used the criteria exact match feature to refine search results and enhance precision.In the ACM Digital Library database, the search focused on full text.The screening of all citations involved four steps:</p>
<ol>
<li>Initial search.All relevant citations were imported into a Zotero (Corporation for Digital Scholarship) citation manager library. 2. Preliminary inclusion.Citations were initially screened based on predefined inclusion criteria. 3. Duplicate removal.Citations were consolidated into a single group, from which duplicates were eliminated. 4. Final inclusion.The remaining references were carefully evaluated against the inclusion criteria to determine their suitability.</li>
</ol>
<p>Study Selection and Eligibility Criteria</p>
<p>All the articles that matched the search criteria were double screened by 2 independent reviewers (ZG and KL) to ensure that each article fell within the scope of LLMs in mental health.This process involved the removal of duplicates followed by a detailed manual evaluation of each article to confirm adherence to our predefined inclusion criteria, ensuring a comprehensive and focused review.To quantify the agreement level between the reviewers and ensure objectivity, interrater reliability was calculated using Cohen , with a score of 0.84 indicating a good level of agreement.In instances of disagreement, a third reviewer (AL) was consulted to achieve consensus.</p>
<p>To assess the risk of bias, we used the Risk of Bias 2 tool, as recommended for Cochrane Reviews.The results have been visualized in Multimedia Appendix 2. We thoroughly examined each study for potential biases that could impact the validity of the results.These included biases from the randomization process, deviations from intended interventions, missing outcome data, inaccuracies in outcome measurement, and selective reporting of results.This comprehensive assessment ensures the credibility of each study.</p>
<p>The criteria for selecting articles were as follows: we limited our search to English-language publications, focusing on articles published between January 1, 2017, and April 30, 2024.This timeframe was chosen considering the substantial developments in the field of LLMs in 2017, marked notably by the introduction of the transformer architecture, which has greatly influenced academic and public interest in this area.</p>
<p>In this review, the original research articles and available full-text papers have been carefully selected, aiming to focus on the application of LLMs in mental health.To comply with the PRISMA guidelines, articles that have not been published in a peer-reviewed venue, including those only available on a preprint server, were excluded.Owing to the limited literature specifically addressing the mental health applications of LLMs, we included review articles to ensure a comprehensive perspective.The selection criteria focused on direct applications, expert evaluations, and ethical considerations related to the use of LLMs in mental health contexts, with the goal of providing a thorough analysis of this rapidly developing field.</p>
<p>Information Extraction</p>
<p>The data extraction process was jointly conducted by 2 reviewers (ZG and KL), focusing on examining the application scenarios, model architecture, data sources, methodologies used, and main outcomes from selected studies on LLMs in mental health.</p>
<p>Initially, we categorized each study to determine its main objectives and applications.The categorization process was conducted in 2 steps.First, after reviewing all the included articles, we grouped them into 3 primary categories: detection of mental health conditions and suicidal ideation through text, LLM use for mental health CAs, and other applications and evaluation of the LLMs in mental health.In the second step, we performed a more detailed categorization.After a thorough, in-depth reading of each article within these broad categories, we refined the classifications based on the specific goals of the studies.Following this, we summarized the main model architectures of the LLMs used and conducted a thorough examination of data sources, covering both public and private datasets.We noted that some review articles lacked detail on dataset content; therefore, we focused on providing comprehensive information on public datasets, including their origins and sample sizes.We also investigated the various methods used across different studies, including data collection strategies and analytic methodologies.We examined their comparative structures and statistical techniques to offer a clear understanding of how these methods are applied in practice.</p>
<p>Finally, we documented the main outcomes of each study, recording significant results and aligning them with relevant performance metrics and evaluation criteria.This included providing quantitative data where applicable to underscore these findings.We used a narrative approach to synthesize the information, integrating and comparing results from various studies to emphasize the efficacy and impact of LLMs on mental health.This narrative synthesis allowed us to highlight the efficacy and impact of LLMs in mental health, providing quantitative data where applicable to underscore these findings.The results of this analysis are presented in Tables S1-S3 in Multimedia Appendix 3 [14,, each corresponding to 1 of the primary categories.</p>
<p>Results</p>
<p>Strategy and Screening Process</p>
<p>The PRISMA diagram of the systematic screening process can be seen in Figure 1.  in Multimedia Appendix 3 according to the 3 categories.</p>
<p>Mental Health Conditions and Suicidal Ideation Detection Through Text</p>
<p>Early intervention and screening are crucial in mitigating the global burden of mental health issues [132].We examined the performance of LLMs in detecting mental health conditions and suicidal ideation through textual analysis.LLMs play an important role in sentiment analysis [75,81],</p>
<p>which categorizes text into overall polarity classes, such as positive, neutral, negative, and occasionally mixed, and emotion classification, which assigns labels such as "joy," "sadness," "anger," and "fear" [75].These analyses enable the detection of emotional states and potential mental health issues from textual data, facilitating early intervention [133].Stigall et al [75] demonstrated the efficacy of these models, with their study showing that Emotion-aware BERT Tiny, a fine-tuned variant of BERT, achieved an accuracy of 93.14% in sentiment analysis and 85.46% in emotion analysis.This performance surpasses that of baseline models, including BERT-Base Cased and BERTTiny-Pretrained [75], underscoring the advantages and validity of fine-tuning in enhancing model performance.LLMs have also demonstrated robust accuracy in detecting and classifying a range of mental health syndromes, including social anxiety, loneliness, and generalized anxiety.Vajre et al [69] introduced PsychBERT, developed using a diverse training dataset from both social media texts and academic literature, which achieved an F 1 -score of 0.63, outperforming traditional deep learning approaches such as convolutional neural networks and long short-term memory networks, which recorded F 1 -scores of 0.57 and 0.51, respectively [69].In research on detecting suicidal ideation using LLMs, Diniz et al [54] showcased the high efficacy of the BERTimbau large model within a non-English (Portuguese) context, achieving an accuracy of 0.955, precision of 0.961, and an F 1 -score of 0.954.The assessment of the BERT model by Metzler et al [65] found that it correctly identified 88.5% of tweets as suicidal or off-topic, performing comparably to human analysts and other leading models.However, Levkovich et al [70] noted that while GPT-4 assessments of suicide risk closely aligned with those by mental health professionals, it overestimated suicidal ideation.These results underscore that while LLMs have the potential to identify tweets reflecting suicidal ideation with accuracy comparable to psychological professionals, extensive follow-up studies are required to establish their practical application in clinical settings.</p>
<p>LLMs in Mental Health CAs</p>
<p>In the growing field of mental health digital support, the implementation of LLMs as CAs has exhibited both promising advantages [14,84,91,96] and significant challenges [92,96].</p>
<p>The studies by Ma et al [14] and Heston [96] demonstrate the effectiveness of CAs powered by LLMs in providing timely, nonjudgmental mental health support.This intervention is particularly important for those who lack ready access to a therapist due to constraints such as time, distance, and work, as well as for certain populations considered socially marginalized, such as older adults who experience chronic loneliness and a lack of companionship [14,97].The qualitative analysis of user interactions on Reddit by Ma et al [14] highlights that LLMs encourage users to speak up and boost their confidence by providing personalized and responsive interactions.In addition, VHope, a DialoGPT-enabled mental health CA, was evaluated by 3 experts who rated its responses as 67% relevant, 78% human-like, and 79% empathetic [84].The development and implementation of a non-English CA for emotion capture and categorization was explored in a study by Zygadlo et al [92].Faced with a scarcity of Polish datasets, the study adapted by translating an existing database of personal conversations from English into Polish, which decreased the accuracy in tasks from 90% in English to 80% in Polish [92].</p>
<p>While the performance remained commendable, it highlighted the challenges posed by the lack of robust datasets in languages other than English, impacting the effectiveness of CAs across different linguistic environments.However, findings by He et al [98] suggest that the availability of language-specific datasets is not the sole determinant of CA performance.In their study, although ERNIE Bot was trained in Chinese and ChatGPT in English, ChatGPT demonstrated greater empathy for Chinese users [98].This implies that factors beyond the training language and dataset availability, such as model architecture or training methodology, can also affect the empathetic responsiveness of LLMs, underscoring the complexity of human-AI interaction.</p>
<p>Meanwhile, the reliability of LLM-driven CAs in high-risk scenarios remains a concern [14,96].An evaluation of 25 CAs found that in tests involving suicide scenarios, only 2 included suicide hotline referrals during the conversation [96].This suggests that while these CAs can detect extreme emotions, few are equipped to take effective preventive measures.Furthermore, CAs often struggle with maintaining consistent communication due to limited memory capacity, leading to disruptions in conversation flow and negatively affecting user experience [14].</p>
<p>Other Applications and Evaluation of the LLMs in Mental Health</p>
<p>ChatGPT has gained attention for its unparalleled ability to generate human-like text and analyze large amounts of textual data, attracting the interest of many researchers and practitioners [100].Numerous evaluations of LLMs in mental health have focused on ChatGPT, exploring its utility across various scenarios such as clinical diagnosis [100,106,111], treatment planning [106,128,131], medication guidance [105,109,129], patient management [106], psychiatry examinations [118], and psychology education [102], among others [107,110,127,130].</p>
<p>Research has highlighted ChatGPT's accuracy in diagnosing various psychiatric conditions [106,110,111,126].For example, Franco D'Souza et al [100] evaluated ChatGPT's responses to 100 clinical psychiatric cases, awarding it an "A" rating in 61 cases, with no errors in the diagnoses of different psychiatric disorders and no unacceptable responses, underscoring ChatGPT's expertise and interpretative capacity in psychiatry.Further supporting this, Schubert et al [118] assessed the performance of ChatGPT 4.0 using neurology board-style examination questions, finding that it answered 85% of the questions correctly, surpassing the average human performance of 73.8%.Meanwhile, in a study of LLMs regarding the prognosis and long-term outcomes of depression, GPT-4, Claude (Anthropic), and Bard (Google AI) showed strong agreement with mental health professionals.They all recommended a combination of psychotherapy and antidepressant medication in every case [130].This not only proves the reliability of LLMs for mental health assessment but also highlights their usefulness in providing valuable support and guidance for individuals seeking information or coping with mental illness.</p>
<p>However, the direct deployment of LLMs, such as ChatGPT, in clinical settings carries inherent risks.The outputs of LLMs are heavily influenced by prompt engineering, which can lead to inconsistencies that undermine clinical reliability [102,[105][106][107]109].For example, Farhat et al [105] conducted a critical evaluation of ChatGPT's ability to generate medication guidelines through detailed cross-questioning and noted that altering prompts substantially changed the responses.While ChatGPT typically provided helpful advice and recommended seeking expert consultation, it occasionally produced inappropriate medication suggestions.Perlis et al [129] verified this, showing that GPT-4 Turbo suggested medications that were considered less efficient choices or contraindicated by experts in 12% of the cases.Moreover, LLMs often lack the necessary clinical judgment capabilities.This issue was highlighted in the study by Grabb [109], which revealed that despite built-in safeguards, ChatGPT remains susceptible to generating extreme and potentially hazardous recommendations.A particularly alarming example was ChatGPT advising a patient with depression to engage in high-risk activities such as bungee jumping as a means of seeking pleasure [109].These LLMs depend on prompt engineering [102,105,109], which means their responses can vary widely depending on the wording and context of the prompts given.The system prompts, which are predefined instructions given to the model, and the prompts used by the experimental team, such as those in the study by Farhat et al [105], guide the behavior of ChatGPT and similar LLMs.These prompts are designed to accommodate a variety of user requests within legal and ethical boundaries.However, while these boundaries are intended to ensure safe and appropriate responses, they often fail to align with the nuanced sensitivities required in psychological contexts.This mismatch underscores a significant deficiency in the clinical judgment and control of LLMs within sensitive mental health settings.</p>
<p>Further research into other LLMs in the mental health sector has shown a range of capabilities and limitations.For example, a study by Sezgin et al [111] highlighted Language Model for Dialogue Applications' (LaMDA's) proficiency in managing complex inquiries about postpartum depression that require medical insight or nuanced understanding; however, they pointed out its challenges with straightforward, factual questions, such as "What are antidepressants?"[111].Assessments of LLMs such as LLaMA-7B, ChatGLM-6B, and Alpaca, involving 50 interns specializing in mental illness, received favorable feedback regarding the fluency of these models in a clinical context, with scores &gt;9.5 out of 10.However, the results also indicated that the responses of these LLMs often failed to address mental health issues adequately, demonstrated limited professionalism, and resulted in decreased usability [116].</p>
<p>Similarly, a study on psychiatrists' perceptions of using LLMs such as Bard and Bing AI (Microsoft Corp) in mental health care revealed mixed feelings.While 40% of physicians indicated that they would use such LLMs to assist in answering clinical questions, some expressed serious concerns about their reliability, confidentiality, and potential to damage the patient-physician relationship [130].</p>
<p>Discussion</p>
<p>Principal Findings</p>
<p>In the context of the wider prominence of LLMs in the literature [14,50,57,60,61,69,96,130], this study supports the assertion that interest in LLMs is growing in the field of mental health.Figure 2 indicates an increase in the number of mental health studies using LLMs, with a notable surge observed in 2023 following the introduction of ChatGPT in late 2022.Although we included articles only up to the end of April 2024, it is evident that the number of articles related to LLMs in the field of mental health continues to show a steady increase in 2024.This marks a substantial change in the discourse around LLMs, reflecting their broader acceptance and integration into various aspects of mental health research and practice.The progression from text analysis to a diverse range of applications highlights the academic community's recognition of the multifaceted uses of LLMs.LLMs are increasingly used for complex psychological assessments, including early screening, diagnosis, and therapeutic interventions.The findings of this study demonstrate that LLMs are highly effective in analyzing textual data to assess mental states and identify suicidal ideation [50,54,57,60,61,65,66,68,69,72,78], although their categorization often tends to be binary [50,54,65,68,69,72,78].These LLMs possess extensive knowledge in the field of mental health and are capable of generating empathic responses that closely resemble human interactions [97,98,107].They show great potential for providing mental health interventions with improved prognoses [50,96,110,127,128,131], with the majority being recognized by psychologists for their appropriateness and accuracy [98,100,129].The careful and rational application of LLMs can enhance mental health care efficiently and at a lower cost, which is crucial in areas with limited health care capacity.However, there are currently no studies available that provide evaluative evidence to support the clinical use of LLMs.</p>
<p>Limitations</p>
<p>Limitations of Using LLMs in Mental Health</p>
<p>On the basis of the works of literature, the strengths and weaknesses of applying the LLMs in mental health are summarized in Multimedia Appendix 5.</p>
<p>LLMs have a broad range of applications in the mental health field.These models excel in user interaction, provide empathy and anonymity, and help reduce the stigma associated with mental illness [14,107], potentially encouraging more patients to participate in treatment.They also offer a convenient, personalized, and cost-effective way for individuals to access mental health services at any time and from any location, which can be particularly helpful for populations considered socially isolated, especially older adults [60,84,97].In addition, LLMs can help reduce the burden of care during times of severe health care resource shortages and patient overload, such as during the COVID-19 pandemic [68].Although previous research has highlighted the potential of LLMs in mental health, it is evident that they are not yet ready for clinical use due to unresolved technical risks and ethical issues.</p>
<p>The use of LLMs in mental health, particularly those fine-tuned for specific tasks such as ChatGPT, reveals clear limitations.The effectiveness of these models heavily depends on the specificity of user-generated prompts.Inappropriate or imprecise prompts can disrupt the conversation's flow and diminish the model's effectiveness [75,96,105,107,109].Even small changes in the content or tone of prompts can sometimes lead to significant variations in responses, which can be particularly problematic in health care settings where interpretability and consistency are critical [14,105,107].Furthermore, LLMs lack clinical judgment and are not equipped to handle emergencies [95,108].While they can generally capture extreme emotions and recognize scenarios requiring urgent action, such as suicide ideation [54,65,70,72,78], they often fail to provide direct, practical measures, typically only advising users to seek professional help [96].In addition, the inherent bias in LLM training data [66,106] can lead to the propagation of stereotypical, discriminatory, or biased viewpoints.This bias can also give rise to hallucinations, that is, LLMs producing erroneous or misleading information [105,131].Furthermore, hallucinations may stem from overfitting the training data or a lack of context understanding [134].Such inaccuracies can have serious consequences, such as providing incorrect medical information, reinforcing harmful stereotypes, or failing to recognize and appropriately respond to mental health crises [131].For example, an LLM might reinforce a harmful belief held by a user, potentially exacerbating their mental health issues.It could also generate nonfactual, overly optimistic, or pessimistic medical advice, delaying appropriate professional intervention.These issues could undermine the integrity and fairness of social psychology [102,105,106,110].</p>
<p>Another critical concern is the "black box" nature of LLMs [105,107,131].This lack of interpretability complicates the application of LLMs in mental health, where trustworthiness and clarity are important.When we talk about neural networks as black boxes, we know details such as what they were trained with, how they were trained, and what the weights are.However, with many new LLMs, such as GPT-3.5 and 4, researchers and practitioners often access the models via web interfaces or application programming interfaces without complete knowledge of the training data, methods, and model updates.This situation not only presents the traditional challenges associated with neural networks but also has all these additional problems that come from the "hidden" model.</p>
<p>Ethical concern is another significant challenge associated with applying LLMs in mental health.Debates are emerging around issues such as digital personhood, informed consent, the risk of manipulation, and the appropriateness of AI in mimicking human interactions [60,102,105,106,135].A primary ethical concern is the potential alteration of the traditional therapist-patient relationship.Individuals may struggle to fully grasp the advantages and disadvantages of LLM derivatives, often choosing these options for their lower cost or greater convenience.This shift could lead to an increased reliance on the emotional support provided by AI [14], inadvertently positioning AI as the primary diagnostician and decision maker for mental health issues, thereby undermining trust in conventional health care settings.Moreover, therapists may become overly reliant on LLM-generated answers and use them in clinical decision-making, overlooking the complexities involved in clinical assessment.This reliance could compromise their professional judgment and reduce opportunities for in-depth engagement with patients [17,129,130].Furthermore, the dehumanization and technocratic nature of mental health care has the potential to depersonalize and dehumanize patients [136], where decisions are more driven by algorithms than by human insight and empathy.This can lead to decisions becoming mechanized, lacking empathy, and detached from ethics [137].AI systems may fail to recognize or adequately interpret the subtle and often nonverbal cues, such as the tone of voice, facial expressions, and the emotional weightage behind words, which are critical in traditional therapeutic settings [136].These cues are essential for comprehensively understanding a patient's condition and providing empathetic care.</p>
<p>In addition, the current roles and accuracy of LLMs in mental health are limited.For instance, while LLMs can categorize a patient's mood or symptoms, most of these categorizations are binary, such as depressed or not depressed [50,65].This oversimplification can lead to misdiagnoses.Data security and user privacy in clinical settings are also of utmost concern [14,54,60,96,130].Although approximately 70% of psychiatrists believe that managing medical documents will be more efficient using LLMs, many still have concerns about their reliability and privacy [97,130,131].These concerns could have a devastating impact on patient privacy and undermine the trust between physicians and patients if confidential treatment records stored in LLM databases are compromised.Beyond the technical limitations of AI, the current lack of an industry-benchmarked ethical framework and accountability system hinders the true application of LLMs in clinical practice [131].</p>
<p>Limitations of the Selected Articles</p>
<p>Several limitations were identified in the literature review.A significant issue is the age bias present in the social media data used for depression and mental health screening.Social media platforms tend to attract younger demographics, leading to an underrepresentation of older age groups [65].Furthermore, most studies have focused on social media platforms, such as Twitter, primarily used by English-speaking populations, which may result in a lack of insight into mental health patterns in non-English-speaking regions.Our review included studies in Polish, Chinese, Portuguese, and Malay, all of which highlighted the significant limitations of LLMs caused by the availability and size of databases [54,61,92,98,116].For instance, due to the absence of a dedicated Polish-language mental health database, a Polish study had to rely on machine-translated English databases [92].While the LLMs achieve 80% accuracy in categorizing emotions and moods in Polish, this is still lower than the 90% accuracy observed in the original English dataset.This discrepancy highlights that the accuracy of LLMs can be affected by the quality of the database.</p>
<p>Another limitation of this study is the low diversity of LLMs studied.Although we used "large language models" as keywords in our search phase, the vast majority of identified studies (39/40, 98%) focused on BERT and its variants, as well as the GPT model, as one of the models studied.Therefore, this review provides only a limited picture of the variability expected in applicability between different LLMs.In addition, the rapid development of LLM technologies presents a limitation; this study can only reflect current advancements and may not encompass future advances or the full potential of LLMs.For instance, in tests involving psychologically relevant questions and answers, GPT-3.5 achieved an accuracy of 66.8%, while GPT-4.0reached an accuracy of 85%, compared to the average human score of 73.8% [118].Evaluating ChatGPT at different stages separately and comparing its performance to that of humans can lead to varied conclusions.In the assessment of prognosis and treatment planning for depression using LLMs, GPT 3.5 demonstrated a distinctly pessimistic prognosis that differed significantly from those of GPT-4, Claude, Bard, and mental health professionals [128].Therefore, continuous monitoring and evaluation are essential to fully understand and effectively use the advancements in LLM technologies.</p>
<p>Opportunities and Future Work</p>
<p>Implementing technologies involving LLMs within the health care provision of real patients demands thorough and multifaceted evaluations.It is imperative for both industry and researchers to not let rollout exceed proportional requirements for evidence on safety and efficacy.At the level of the service provider, this includes providing explicit warnings to the public</p>
<p>XSL  FO</p>
<p>RenderX to discourage mistaking LLM functionality for clinical reliability.For example, GPT-4 introduced the ability to process and interpret image inputs within conversational contexts, leading OpenAI to issue an official warning that GPT-4 is not approved for analyzing specialized medical images such as computed tomography scans [138].</p>
<p>A key challenge to address in LLM research is the tendency to produce incoherent text or hallucinations.Future efforts could focus on training LLMs specifically for mental health applications, using datasets with expert labeling to reduce bias and create specialized mental health lexicons [84,102,116].The creation of specialized datasets could take advantage of the customizable nature of LLMs, fostering the development of models that cater to the distinct needs of varied demographic groups.For instance, unlike models designed for health care professionals that assist in tasks such as data documentation, symptom analysis, medication management, and postoperative care, LLMs intended for patient interaction might be trained with an emphasis on empathy and comfortable dialogue.</p>
<p>Another critical concern is the problem of outdated training data in LLMs.Traditional LLMs, such as GPT-4 (with a cutoff date up to October 2023), rely on potentially outdated training data, limiting their ability to incorporate recent events or information.This can compromise the accuracy and relevance of their responses, leading to the generation of uninformative or incorrect answers, known as "hallucinations" [139].</p>
<p>Retrieval-augmented generation (RAG) technology offers a solution by retrieving facts from external knowledge bases, ensuring that LLMs use the most accurate and up-to-date information [140].By searching for relevant information from numerous documents, RAG enhances the generation process with the most recent and contextually relevant content [141].</p>
<p>In addition, RAG includes evidence-based information, increasing the reliability and credibility of LLM responses [139].</p>
<p>To further enhance the reliability of LLM content and minimize hallucinations, recent studies suggest adjusting model parameters, such as the "temperature" setting [142][143][144].The temperature parameter influences the randomness and predictability of outputs [145].Lowering the temperature typically results in more deterministic outputs, enhancing coherence and reducing irrelevant content [146].However, this adjustment can also limit the model's creativity and adaptability, potentially making it less effective in scenarios requiring diverse or nuanced responses.In mental therapy, where nuanced and sensitive responses are essential, maintaining an optimal balance is crucial.While a lower temperature can ensure accuracy, which is important for tasks such as clinical documentation, it may not suit therapeutic dialogues where personalized engagement is key.Low temperatures can lead to repetitive and impersonal responses, reducing patient engagement and therapeutic effectiveness.To mitigate these risks, regular updates of the model incorporating the latest therapeutic practices and clinical feedback are essential.Such updates could refine the model's understanding and response mechanisms, ensuring it remains a safe and effective tool for mental health care.Nevertheless, determining the "optimal" temperature setting is challenging, primarily due to the variability in tasks and interaction contexts, which require different levels of creativity and precision.</p>
<p>Data privacy is another important area of concern.Many LLMs, such as ChatGPT and Claude, involve sending data to third-party servers, which poses the risk of data leakage.Current studies have found that LLMs can be enhanced by privacy-enhancing techniques, such as zero-knowledge proofs, differential privacy, and federated learning [147].In addition, privacy can be preserved by replacing identifying information in textual data with generic tokens.For example, when recording sensitive information (eg, names, addresses, or credit card numbers), using alternatives to mask tokens can help protect user data from unauthorized access [148].This obfuscation technique ensures that sensitive user information is not stored directly, thereby enhancing data security.</p>
<p>The lack of interpretability in LLM decision-making is another crucial area for future research on health care applications.Future research should examine the models' architecture, training, and inferential processes for clearer understanding.Detailed documentation of training datasets, sharing of model architectures, and third-party audits would ideally form part of this undertaking.Investigating techniques such as attention mechanisms and modular architectures could illuminate aspects of neural network processing.The implementation of knowledge graphs might help in outlining logical relationships and facts [149].In addition, another promising approach involves creating a dedicated embedding space during training, guided by an LLM.This space aligns with a causal graph and aids in identifying matches that approximate counterfactuals [146].</p>
<p>Before deploying LLMs in mental health settings, a comprehensive assessment of their reliability, safety, fairness, abuse resistance, interpretability, compliance with social norms, robustness, performance, linguistic accuracy, and cognitive ability is essential.It is also crucial to foster collaborative relationships among mental health professionals, patients, AI researchers, and policy makers.LLMs, for instance, have demonstrated initial competence in providing medication advice; however, their responses can sometimes be inconsistent or include inappropriate suggestions.As such, LLMs require professional oversight and should not be used independently.Nevertheless, when used as decision aids, LLMs have the potential to enhance health care efficiency.This study calls on developers of LLMs to collaborate with authoritative regulators in actively developing ethical guidelines for AI research in health care.These guidelines should aim to adopt a balanced approach that considers the multifaceted nature of LLMs and ensures their responsible integration into medical practice.They are expected to become industry benchmarks, facilitating the future development of LLMs in mental health.</p>
<p>Conclusions</p>
<p>This review examines the use of LLMs in mental health applications, including text-based screening for mental health conditions, detection of suicidal ideation, CAs, clinical use, and other related applications.Despite the potential of LLMs, challenges such as the production of hallucinatory or harmful information, output inconsistency, and ethical concerns remain.Nevertheless, as technology advances and ethical guidelines improve, LLMs are expected to become increasingly integral</p>
<p>Figure 1 .
1
Figure 1.PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) flow of the selection process.LLM: large language model.</p>
<p>Figure 2 .
2
Figure 2. Number of articles included in this literature review, grouped by year of publication and application field.The black line indicates the total number of articles in each year.CA: conversational agent.</p>
<p>XSL  FO RenderX interventions</p>
<p>while also identifying key challenges such as inconsistencies in model outputs and the lack of robust ethical guidelines.These insights suggest that, while LLMs hold promise, their use should be supervised by physicians, and they are not yet ready for widespread clinical implementation.</p>
<p>[66]tudies focusing on early screening for depression, comparing results horizontally is challenging due to variations in datasets, training methods, and models across different investigations.Nonetheless, substantial evidence supports the significant potential of LLMs in detecting depression from text-based data.For example, Danner et al[57]conducted a comparative analysis using a convolutional neural network on the Distress Analysis Interview Corpus-Wizard of Oz dataset, achieving F 1 -scores of 0.53 and 0.59; however, their use of GPT-3.5 demonstrated superior performance, with an F 1 -score of 0.78.Another study involving the E-Distress Analysis Interview Corpus dataset (an extension of Distress Analysis Interview Corpus-Wizard of Oz) used the Robustly Optimized BERT Approach for Depression Detection to predict the Patient Health Questionnaire-8 scores from textual data.This approach identified 3 levels of depression and achieved the lowest mean absolute error of 3.65 in Patient Health Questionnaire-8 scores[66].
RenderX
[50,57,60,61,66]69,72,75,78,81] the efficacy of early screening for depression using LLMs[50,57,60,61,66,68], while another (1/40, 2%) simultaneously addressed both depression and anxiety[60].One comprehensive study examined various psychiatric conditions, including depression, social anxiety, loneliness, anxiety, and other prevalent mental health issues[69].Two (5%) of the 40 articles assessed and compared the ability of LLMs to perform sentiment and emotion analysis[75,81], and 5 (12%) articles focused on the capability of LLMs to analyze textual content for detecting suicidal ideation[54,65,70,72,78].Most studies (10/40, 25%) used BERT and its variants as one of the primary models[50,54,57,62,65,66,68,69,75,78], while GPT models were also commonly used (8/40, 20%)[57,60,61,66,70,72,78,81].Most training data (10/40, 25%) comprised social media posts[50,54,62,65,68,69,72,75,78,81]from platforms such as Twitter (Twitter, Inc), Reddit, and Sina Weibo (Sina corporation), covering languages such as English, Malay dialects, Chinese, and Portuguese.In addition, 5 (12%) of the 40 studies used datasets consisting of clinical transcripts and patient interviews[50,57,60,61,66], providing deeper insights into LLM applications in clinical mental health settings.XSL  FO</p>
<p>JMIR Ment Health 2024 | vol. 11 | e57400 | p. 2 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 3 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 5 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 6 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 7 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 8 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 9 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 10 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 11 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 12 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 13 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 14 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 15 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 16 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 17 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 18 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 19 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
JMIR Ment Health 2024 | vol. 11 | e57400 | p. 20 https://mental.jmir.org/2024/1/e57400 (page number not for citation purposes)
AcknowledgmentsThis work was funded by the UK Research and Innovation (UKRI) Centre for Doctoral Training in artificial intelligence-enabled health care systems (grant EP/S021612/1).The funders were not involved in the study design, data collection, analysis, publication decisions, or manuscript writing.The views expressed in the text are those of the authors and not those of the funder.Data AvailabilityThe authors ensure that all pertinent data have been incorporated in the manuscript and the multimedia appendices.For access to the research data, interested parties may contact the corresponding author (KL) subject to a reasonable request.Authors' ContributionsZG and KL contributed to the conception and design of the study.ZG, KL, and AL contributed to the development of the search strategy.Database search outputs were screened by ZG, and data were extracted by ZG and KL.An assessment of the risk of bias in the included studies was performed by ZG and KL.ZG completed the literature review, collated the data, performed the data analysis, interpreted the results, and wrote the first draft of the manuscript.KL, AL, JHT, JF, and TK reviewed the manuscript and provided multiple rounds of guidance in the writing of the manuscript.All authors read and approved the final version of the manuscript.Conflicts of InterestNone declared.MultimediaMultimedia Appendix 3Summary of the 40 selected articles from the literature on large language models in mental applications, categorized into each group.[DOCX File , 78 KB-Multimedia Appendix 3]Multimedia Appendix 4List of the studies excluded at the full-text screening stage.[DOCX File , 30 KB-Multimedia Appendix 4]Multimedia Appendix 5Summary of the strengths and weaknesses of applying the large language models in mental health.[DOCX File , 24 KB-MultimediaAppendixZhijun Guo, Alvina Lai, Johan H Thygesen, Joseph Farrington, Thomas Keen, Kezhi Li.Originally published in JMIR Mental Health (https://mental.jmir.org),18.10.2024.This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/),which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Mental Health, is properly cited.The complete bibliographic information, a link to the original publication on https://mental.jmir.org/,as well as this copyright and license information must be included.
World Health Organization. Jun 17, 2022Mental health. accessed 2024-04-15</p>
<p>Mental disorders. World Health Organization. Jun 8, 2022accessed 2024-04-15</p>
<p>MHPSS worldwide: facts and figures. Government of the Netherlands. accessed 2024-04-15</p>
<p>Quantifying the global burden of mental disorders and their economic value. D Arias, S Saxena, S Verguet, 10.1016/j.eclinm.2022.101675EClinicalMedicine. Dec. 541016752022FREE Full text. Medline: 36193171</p>
<p>Detecting individuals with severe mental illness using artificial intelligence applied to magnetic resonance imaging. W Zhang, C Yang, Z Cao, Z Li, L Zhuo, Y Tan, 10.1016/j.ebiom.2023.104541EBioMedicine. 90104541Apr 2023FREE Full text. Medline: 36996601</p>
<p>Mental health and COVID-19: early evidence of the pandemic's impact: scientific brief. 2 March 2022. Mar 2, 2022accessed 2024-04-15</p>
<p>Global impact of the COVID-19 pandemic on mental health services: a systematic review. G S Duden, S Gersdorf, K Stengler, 10.1016/j.jpsychires.2022.08.013J Psychiatr Res. 154Oct 2022FREE Full text. Medline: 36055116</p>
<p>. 2024-04-15Mental health treatments. Mental Health America. </p>
<p>Stigma, prejudice and discrimination against people with mental illness. American Psychiatric Association. accessed 2024-04-15</p>
<p>Almost half of Americans don't seek professional help for mental disorders. M T Nietzel, Forbes. May 24, 2021accessed 2024-04-15</p>
<p>Why do people avoid mental health treatment? Thriveworks. 2024-04-15Aug 9, 2022</p>
<p>Digital mental health and COVID-19: using technology today to accelerate the curve on access and quality tomorrow. J Torous, Jn Myrick, K Rauseo-Ricupero, N Firth, J , 10.2196/18848JMIR Ment Health. Mar. 263e188482020FREE Full text. Medline: 32213476</p>
<p>Large-language-models (LLM)-based AI chatbots: architecture, in-depth analysis and their performance evaluation. V Kumar, P Srivastava, A Dwivedi, I Budhiraja, D Ghosh, V Goyal, Recent Trends in Image Processing and Pattern Recognition. K C Santosh, A Makkar, M Conway, A K Singh, A Vacavant, Abou El Kalam, A , Cham, SwitzerlandSpringer2024</p>
<p>Understanding the benefits and challenges of using large language model-based conversational agents for mental well-being support. Z Ma, Y Mei, Z Su, AMIA Annu Symp Proc. 20232023FREE Full text. Medline: 38222348</p>
<p>Towards interpretable mental health analysis with large language models. K Yang, S Ji, T Zhang, Q Xie, Z Kuang, S Ananiadou, 10.18653/v1/2023.emnlp-main.370April 6, 2023FREE Full text</p>
<p>Patients turning to A and E as wait times for NHS mental health treatment spiral. The Guardian. Oct 10, 2022accessed 2024-04-16</p>
<p>An ethical perspective on the democratization of mental health with generative artificial intelligence. Z Elyoseph, T Gur, Y Haber, T Simon, T Angert, Y Navon, 10.2196/preprints.58011March 2, 2024JMIR Preprints. PreprintFREE Full text</p>
<p>Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (Woebot): a randomized controlled trial. K K Fitzpatrick, A Darcy, M Vierhile, 10.2196/mental.7785JMIR Ment Health. 42e19Jun 06. 2017FREE Full text. Medline: 28588005</p>
<p>2024-04-16Wysa -everyday mental health. Wysa. </p>
<p>An overview of chatbot-based mobile mental health apps: insights from app description and user reviews. M D Haque, S Rubya, 10.2196/44838JMIR Mhealth Uhealth. 11e44838May 22. 2023FREE Full text. Medline: 37213181</p>
<p>Large language models could change the future of behavioral healthcare: a proposal for responsible development and evaluation. Npj Ment Health Res. E C Stade, S W Stirman, L H Ungar, C L Boland, H A Schwartz, D B Yaden, 10.1038/s44184-024-00056-zApr 02. 2024312FREE Full text</p>
<p>Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine. EBioMedicine. S Harrer, 10.1016/j.ebiom.2023.104512Apr 202390104512FREE Full text. Medline: 36924620</p>
<p>Ethics and governance of artificial intelligence for health: guidance on large multi-modal models. World Health Organization. 2024accessed 2024-04-16</p>
<p>What are large language models (LLMs)? IBM. accessed 2024-04-16</p>
<p>2024-04-16LLM evaluation: key metrics and best practices. AISERA. </p>
<p>Better language models and their implications. OpenAI. Feb 14. 2019accessed 2024-04-16</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, June 12, 2017FREE Full text</p>
<p>What are large language models (LLMs)? TechTarget. S M Kerner, accessed 2024-05-16</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, 10.5260/chara.21.2.8arXiv. Preprint posted online on October. 112018FREE Full text</p>
<p>A Rogers, O Kovaleva, A Rumshisky, 10.1162/tacl_a_00349A primer in BERTology: what we know about how BERT works. arXiv. Preprint posted online on. February 27. 2020FREE Full text</p>
<p>ChatGPT a year on: 3 ways the AI chatbot has completely changed the world in 12 months. Euronews. Nov 30, 2023chatgpt-a-year-on-3-ways-the-ai-chatbot-has-completely-changed-the-world-in-12-months. accessed 2024-04-16</p>
<p>Large language models in medicine. A J Thirunavukarasu, D S Ting, K Elangovan, L Gutierrez, T F Tan, D S Ting, 10.1038/s41591-023-02448-8Medline: 37460753]Nat Med. Aug. 1782023</p>
<p>The best Large Language Models (LLMs) of 2024. G Hickey, 2024-08-08TechRadar. 2024</p>
<p>10+ large language model examples -benchmark and use cases in '24. C Dilmegani, accessed 2024-04-16</p>
<p>Timeline of AI and language models. 2024-04-16Life Architect</p>
<p>Large language models explained. boost.ai. M Priest, Feb 20. 2024accessed 2024-04-16</p>
<p>Efficient fine-tuning of BERT on the edge. D Vucetic, M Tayaranian, M Ziaeefard, J J Clark, B H Meyer, W J Gross, Proceedings of the IEEE International Symposium on Circuits and Systems. the IEEE International Symposium on Circuits and Systems2022. 2022. May 27-June 1, 2022</p>
<p>. T X Austin, 10.1109/iscas48785.2022.9937567</p>
<p>understanding-large-language-models-and-fine-tuning-for-business-scenarios-a-simple-guide-42f44cb687f0. M Kumar, Oct 27, 2023Understanding large language models and fine-tuning for business scenarios: a simple guide. Medium. accessed 2024-04-16</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. S Mishra, D Khashabi, C Baral, H Hajishirzi, 10.18653/v1/2022.acl-long.244Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland2022. May 22-27, 2022ACL 2022</p>
<p>Instruction tuning for large language models: a survey. S Zhang, L Dong, X Li, S Zhang, X Sun, S Wang, 10.48550/arXiv.2308.10792August 21. 2023</p>
<p>A developer's guide to prompt engineering and LLMs. GitHub. J Berryman, A Ziegler, 2024accessed 2024-04-15</p>
<p>Climbing towards NLU: on meaning, form, and understanding in the age of data. E M Bender, A Koller, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020. 2020. July 5-10, 2020</p>
<p>. Online, 10.18653/v1/2020.acl-main.463</p>
<p>BioBERT: a pre-trained biomedical language representation model for biomedical text mining. J Lee, W Yoon, S Kim, D Kim, S Kim, C H So, 10.1093/bioinformatics/btz682Bioinformatics. Feb. 1542020FREE Full text. Medline: 31501885</p>
<p>ClinicalBERT: modeling clinical notes and predicting hospital readmission. K Huang, J Altosaar, R Ranganath, 10.48550/arXiv.1904.05342April 10, 2019</p>
<p>Clinically applicable AI system for accurate diagnosis, quantitative measurements, and prognosis of COVID-19 pneumonia using computed tomography. K Zhang, X Liu, J Shen, Z Li, Y Sang, X Wu, 10.1016/j.cell.2020.08.029Cell. 18251360Sep 03. 2020FREE Full text. Medline: 32888496</p>
<p>Response to "attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine. M Trengove, R Vandersluis, L Goetz, 10.1016/j.ebiom.2023.104671EBioMedicine. 93104671Jul 2023FREE Full text. Medline: 37327676</p>
<p>Machine learning and natural language processing in mental health: systematic review. Le Glaz, A Haralambous, Y Kim-Dufor, D H Lenca, P Billot, R Ryan, T C , 10.2196/15708J Med Internet Res. 235e15708May 04. 2021FREE Full text. Medline: 33944788</p>
<p>Large language models in mental health care: a scoping review. Y Hua, F Liu, K Yang, Z Li, Y H Sheu, P Zhou, 10.48550/arXiv.2401.02984arXiv. Preprint posted online on. January 1, 2024</p>
<p>Preferred reporting items for systematic reviews and meta-analyses: the PRISMA statement. D Moher, A Liberati, J Tetzlaff, D G Altman, Group, 10.7326/0003-4819-151-4-200908180-00135Ann Intern Med. Aug. 1842009FREE Full text. Medline: 19622511</p>
<p>AI-enhanced mental health diagnosis: leveraging transformers for early detection of depression tendency in textual data. S Verma, Vishal, R C Joshi, M K Dutta, S Jezek, R Burget, 10.1109/icumt61075.2023.10333301Proceedings of the 15th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops. 2023. Presented at: ICUMT 2023. the 15th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops. 2023. Presented at: ICUMT 2023Ghent, BelgiumOctober 30-November 1, 2023</p>
<p>RoBERTa: a robustly optimized BERT pretraining approach. Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, 10.48550/arXiv.1907.11692July 26. 2019</p>
<p>Mental health corpus. R Namdari, accessed 2024-04-17</p>
<p>Depression: Reddit dataset (cleaned). Kaggle. accessed 2024-04-17</p>
<p>Boamente: a natural language processing-based digital phenotyping tool for smart monitoring of suicidal ideation. Healthcare (Basel). E J Diniz, J E Fontenele, A C De Oliveira, V H Bastos, S Teixeira, R L Rablo, 10.3390/healthcare10040698Apr 08. 202210698FREE Full text. Medline: 35455874</p>
<p>Transformers: state-of-the-art natural language processing. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline2020. November 16-20. 2020EMNLP 2020 -Demos</p>
<p>BERTimbau: pretrained BERT models for Brazilian Portuguese. F Souza, R Nogueira, R Lotufo, Proceedings of the 9th Brazilian Conference on Intelligent Systems. the 9th Brazilian Conference on Intelligent Systems2020. 2020. October 20-23. 2020</p>
<p>. Rio Grande, Brazil, 10.1007/978-3-030-61377-8_28</p>
<p>Advancing mental health diagnostics: GPT-based method for depression detection. M Danner, B Hadzic, Gerhardt S Ludwig, S Uslu, I Shao, P , 10.23919/sice59929.2023.10354236Proceedings of the 62nd Annual Conference of the Society of Instrument and Control Engineers. the 62nd Annual Conference of the Society of Instrument and Control EngineersTsu, Japan2023. 2023. September 6-9, 2023Presented at</p>
<p>The distress analysis interview corpus of human and computer interviews. J Gratch, R Artstein, G Lucas, G Stratou, S Scherer, A Nazarian, Proceedings of the Ninth International Conference on Language Resources and Evaluation. the Ninth International Conference on Language Resources and EvaluationReykjavik, Iceland2014. 2014. May 26-31. 2014</p>
<p>Classifying anxiety and depression through LLMs virtual interactions: a case study with ChatGPT. Y Tao, M Yang, H Shen, Z Yang, Z Weng, B Hu, 10.1109/bibm58861.2023.10385305Proceedings of the IEEE International Conference on Bioinformatics and Biomedicine. the IEEE International Conference on Bioinformatics and Biomedicine2023. 2023. December 5-8, 2023Presented at. Istanbul, Turkiye</p>
<p>Depression detection on Malay dialects using GPT-3. M F Hayati, Md Ali, M A , Md Rosli, A N , Proceedings of the IEEE-EMBS Conference on Biomedical Engineering and Sciences. the IEEE-EMBS Conference on Biomedical Engineering and Sciences2022. 2022. December 7-9, 2022</p>
<p>. Kuala Lumpur, Malaysia, 10.1109/iecbes54088.2022.10079554</p>
<p>Depression risk prediction for Chinese microblogs via deep-learning methods: content analysis. X Wang, S Chen, T Li, W Li, Y Zhou, J Zheng, 10.2196/17958JMIR Med Inform. Jul. 2972020e17958. [FREE Full text. Medline: 32723719</p>
<p>XLNet: generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R Salakhutdinov, Q V Le, 10.48550/arXiv.1906.08237June 19. 2019</p>
<p>Assessing depression risk in Chinese microblogs: a corpus and machine learning methods. X Wang, S Chen, T Li, W Li, Y Zhou, J Zheng, Proceedings of the IEEE International Conference on Healthcare Informatics. the IEEE International Conference on Healthcare Informatics2019. 2019. June 10-13, 2019Presented at</p>
<p>. China Xi'an, 10.1109/ichi.2019.8904506</p>
<p>Detecting potentially harmful and protective suicide-related content on Twitter: machine learning approach. H Metzler, H Baginski, T Niederkrotenthaler, D Garcia, 10.2196/34705J Med Internet Res. Aug. 178e347052022FREE Full text. Medline: 35976193</p>
<p>Exploring the capabilities of a language model-only approach for depression detection in text data. M Sadeghi, B Egger, R Agahi, R Richer, K Capito, L H Rupp, 10.1109/bhi58575.2023.10313367Proceedings of the IEEE EMBS International Conference on Biomedical and Health Informatics. the IEEE EMBS International Conference on Biomedical and Health InformaticsPittsburgh, PA2023. 2023. October 15-18, 2023Presented at</p>
<p>SimSensei kiosk: a virtual human interviewer for healthcare decision support. D Devault, R Artstein, G Benn, T Dey, E Fast, A Gainer, Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems. the 2014 International Conference on Autonomous Agents and Multi-Agent Systems2014. 2014Presented at: AAMAS '14; May 5-9</p>
<p>. France Paris, 10.1609/aaai.v29i1.9777</p>
<p>Monitoring depression trends on Twitter during the COVID-19 pandemic: observational study. JMIR Infodemiology. Y Zhang, H Lyu, Y Liu, X Zhang, Y Wang, J Luo, 10.2196/26769Jul 18, 20211e26769FREE Full text. Medline: 34458682</p>
<p>PsychBERT: a mental health language model for social media mental health behavioral analysis. V Vajre, M Naylor, U Kamath, A Shehu, 10.1109/BIBM52615.2021.9669469Proceedings of the IEEE International Conference on Bioinformatics and Biomedicine. the IEEE International Conference on Bioinformatics and BiomedicineHouston, TX2021. December 9-12, 2021BIBM 2021</p>
<p>Suicide risk assessments through the eyes of ChatGPT-3.5 versus ChatGPT-4: vignette study. I Levkovich, Z Elyoseph, 10.2196/51232JMIR Ment Health. 10e51232Sep 20. 2023FREE Full text. Medline: 37728984</p>
<p>The effect of perceived burdensomeness and thwarted belongingness on therapists' assessment of patients' suicide risk. Y Levi-Belz, E Gamliel, 10.1080/10503307.2015.1013161Psychother Res. 264Jul 2016Medline: 25751580</p>
<p>Transfer learning for risk classification of social media posts: model evaluation study. D Howard, M M Maslej, J Lee, J Ritchie, G Woollard, L French, 10.2196/15371Medline: 32401222]J Med Internet Res. May. 135e153712020FREE Full text</p>
<p>Overview Deepmoji, Massachusetts Institute of Technology Media Lab. accessed 2024-04-17</p>
<p>. D Cer, Y Yang, S Y Kong, N Hua, N Limtiaco, St, R John, 10.18653/v1/d18-2029Universal sentence encoder. arXiv. Preprint posted online on March. 292018FREE Full text</p>
<p>Large language models performance comparison of emotion and sentiment classification. W Stigall, M A Khan, D Attota, F Nweke, Y Pei, Proceedings of the 2024 ACM Southeast Conference. 2024. Presented at: ACMSE '24. the 2024 ACM Southeast Conference. 2024. Presented at: ACMSE '24April 18-20, 2024</p>
<p>. G A Marietta, 10.1145/3603287.3651183</p>
<p>Twitter tweets sentiment dataset. 2024-08-06</p>
<p>Socially aware synthetic data generation for suicidal ideation detection using large language models. H Ghanadian, I Nejadgholi, H A Osman, 10.1109/access.2024.3358206IEEE Access. 122024</p>
<p>2024-08-06FLAN-T5. Hugging Face. </p>
<p>ChatGPT for suicide risk assessment on social media: quantitative evaluation of model performance, potentials and limitations. H Ghanadian, I Nejadgholi, H A Osman, 10.18653/v1/2023.wassa-1.16Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, &amp; Social Media Analysis. the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, &amp; Social Media AnalysisToronto, ON2023. July 14. 2023WASSA@ACL 2023</p>
<p>A comparison of ChatGPT and fine-tuned open pre-trained transformers (OPT) against widely used sentiment analysis tools: sentiment analysis of COVID-19 survey data. J A Lossio-Ventura, R Weger, A Y Lee, E P Guinee, J Chung, L Atlas, 10.2196/50150JMIR Ment Health. 11e50150Jan 25. 2024FREE Full text. Medline: 38271138</p>
<p>COVID-19 and mental health: predicted mental health status is associated with clinical symptoms and pandemic-related psychological and behavioral responses. medRxiv. Preprint posted online on October 14. J Y Chung, A Gibbons, L Atlas, E Ballard, M Ernst, S Japee, 10.1101/2021.10.12.212649022021FREE Full text. Medline: 34671781</p>
<p>US public concerns about the COVID-19 pandemic from results of a survey given via social media. L M Nelson, J F Simard, A Oluyomi, V Nava, L G Rosas, M Bondy, 10.1001/jamainternmed.2020.1369Medline: 32259192]JAMA Intern Med. 1807Jul 01. 2020FREE Full text</p>
<p>A hybrid response generation model for an empathetic conversational agent. J L Beredo, E C Ong, Proceedings of the International Conference on Asian Language Processing (IALP). the International Conference on Asian Language Processing (IALP)2022. 2022. October 27-28, 2022Presented at</p>
<p>. Singapore Singapore, 10.1109/ialp57159.2022.9961311</p>
<p>Therapist vibe: children's expressions of their emotions through storytelling with a chatbot. K A Santos, E Ong, R Resurreccion, Proceedings of the Interaction Design and Children Conference. the Interaction Design and Children Conference2020. June 21-24. 2020Presented at: IDC '20</p>
<p>. U K London, 10.1145/3392063.3394405</p>
<p>Towards building mental health resilience through storytelling with a chatbot. E Ong, M J Go, R Lao, J Pastor, L B To, Proceedings of the 29th International Conference on Computers in Education. 2021. Presented at: ICCE 2021. the 29th International Conference on Computers in Education. 2021. Presented at: ICCE 2021OnlineNovember 22-26, 2021</p>
<p>PERMA model. Corporate Finance Institute. accessed 2024-04-17</p>
<p>Towards empathetic open-domain conversation models: a new benchmark and dataset. H Rashkin, E M Smith, M Li, Y L Boureau, 10.18653/v1/p19-1534arXiv. Preprint posted online on November. 12018FREE Full text</p>
<p>Investigating the acceptability and perceived effectiveness of a chatbot in helping students assess their well-being. D E Sia, M J Yu, J L Daliva, J Montenegro, E Ong, 10.1145/3429360.3468177Proceedings of the Asian CHI Symposium 2021. 2021. Presented at: Asian CHI '21. the Asian CHI Symposium 2021. 2021. Presented at: Asian CHI '21Yokohama, JapanMay 8-13. 2021</p>
<p>Predicting individual well-being through the language of social media. H A Schwartz, M Sap, M L Kern, J C Eichstaedt, A Kapelner, M Agrawal, Pac Symp Biocomput. 212016FREE Full text. Medline: 26776214</p>
<p>CareBot: a mental health ChatBot. R Crasto, L Dias, D Miranda, D Kayande, 10.1109/incet51464.2021.9456326Proceedings of the 2nd International Conference for Emerging Technology. the 2nd International Conference for Emerging TechnologyBelagavi, India2021. May 21-23, 2021Presented at: INCET 2021</p>
<p>A therapeutic dialogue agent for Polish language. A Zygadlo, Proceedings of the 9th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos. the 9th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos2021. 2021. September 28-October 1, 2021</p>
<p>. Japan Nara, 10.1109/aciiw52867.2021.9666281</p>
<p>2024-04-18Conversational AI platform. Rasa Technologies Inc. </p>
<p>Industrial-strength natural language processing in Python. spaCy. 2024-04-18</p>
<p>DailyDialog: a manually labelled multi-turn dialogue dataset. Y Li, H Su, X Shen, W Li, Z Cao, S Niu, 10.48550/arXiv.1710.03957October 11, 2017</p>
<p>Safety of large language models in addressing depression. Cureus. T F Heston, 10.7759/cureus.50729Medline: 38111813]Dec 18, 202315e50729FREE Full text</p>
<p>Towards designing a ChatGPT conversational companion for elderly people. A Alessa, H Al-Khalifa, 10.1145/3594806.3596572Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments. the 16th International Conference on PErvasive Technologies Related to Assistive EnvironmentsCorfu, Greece2023. July 5-7, 2023Presented at: PETRA '23</p>
<p>Physician versus large language model chatbot responses to web-based questions from autistic patients in Chinese: cross-sectional comparative analysis. W He, W Zhang, Y Jin, Q Zhou, H Zhang, Q Xia, 10.2196/54706J Med Internet Res. Apr. 30e547062024FREE Full text. Medline: 38687566</p>
<p>Knowledge transfer between physicians from different geographical regions in China's online health communities. Inf Technol Manag. Z Deng, Z Deng, S Liu, R Evans, 10.1007/s10799-023-00400-3May 192023FREE Full text. Medline: 37359990</p>
<p>Appraising the performance of ChatGPT in psychiatry using 100 clinical case vignettes. D' Franco, R Souza, S Amanullah, M Mathew, K M Surapaneni, 10.1016/j.ajp.2023.103770Asian J Psychiatr. 89103770Nov 2023Medline: 37812998</p>
<p>100 Cases in Psychiatry. B Wright, S Dave, N Dogra, 2017CRC PressBoca Raton, FLSecond Edition</p>
<p>Can we use ChatGPT for mental health and substance use education? Examining its quality and potential harms. S Spallek, L Birrell, S Kershaw, E K Devine, L Thornton, 10.2196/51243JMIR Med Educ. Nov. 30e512432023FREE Full text. Medline: 38032714</p>
<p>Evidence-based information for the community. 2024-04-18Cracks in the Ice. </p>
<p>Positive choices: drug and alcohol education -get informed, stay smart, stay safe. Positive Choices. 2024-04-18</p>
<p>ChatGPT as a complementary mental health resource: a boon or a bane. F Farhat, 10.1007/s10439-023-03326-7Ann Biomed Eng. 525May 2024Medline: 37477707</p>
<p>ChatGPT: opportunities, risks and priorities for psychiatry. Y Wei, L Guo, C Lian, J Chen, 10.1016/j.ajp.2023.103808Asian J Psychiatr. 90103808Dec 2023Medline: 37898100</p>
<p>Investigating large language models' perception of emotion using appraisal theory. N Yongsatianchot, P G Torshizi, S Marsella, 10.1109/aciiw59127.2023.10388194Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos. the 11th International Conference on Affective Computing and Intelligent Interaction Workshops and DemosCambridge, MA2023. 2023. September 10-13, 2023Presented at</p>
<p>2024-04-18Xenova / text-davinci-003. Hugging Face. </p>
<p>The impact of prompt engineering in large language model performance: a psychiatric example. D Grabb, 10.21037/jmai-23-71J Med Artif Intell. 6Oct 2023</p>
<p>The plasticity of ChatGPT's mentalizing abilities: personalization for personality structures. Front Psychiatry. D Hadar-Shoval, Z Elyoseph, M Lvovsky, 10.3389/fpsyt.2023.1234397Medline: 37720897]Sep 01. 2023141234397FREE Full text</p>
<p>Clinical accuracy of large language models and Google search responses to postpartum depression questions: cross-sectional study. E Sezgin, F Chekeni, J Lee, S Keim, 10.2196/49240J Med Internet Res. Sep. 11e492402023FREE Full text. Medline: 37695668</p>
<p>LaMDA: our breakthrough conversation technology. Google. E Collins, Z Ghahramani, May 18, 2021accessed 2024-04-18</p>
<p>How do you feel? Using natural language processing to automatically rate emotion in psychotherapy. M J Tanana, C S Soma, P B Kuo, N M Bertagnolli, A Dembe, B T Pace, 10.3758/s13428-020-01531-zBehav Res Methods. 535Oct 2021FREE Full text</p>
<p>Publisher of streaming video, audio, and text library databases that promote research, teaching, and learning across disciplines, including music, counseling, history, business and more. Alexander Streetaccessed 2024-04-18</p>
<p>Knowledge-enhanced pre-training large language model for depression diagnosis and treatment. X Wang, K Liu, C Wang, 10.1109/ccis59572.2023.10263217Proceedings of the IEEE 9th International Conference on Cloud Computing and Intelligent Systems. the IEEE 9th International Conference on Cloud Computing and Intelligent SystemsDali, China2023. 2023. August 12-13, 2023Presented at</p>
<p>Performance of large language models on a neurology board-style examination. M C Schubert, W Wick, V Venkataramani, 10.1001/jamanetworkopen.2023.46721JAMA Netw Open. 612e2346721Dec 01. 2023FREE Full text. Medline: 38060223</p>
<p>2024-04-18Neurology board review questions and practice tests. BoardVitals. </p>
<p>Trajectories of sentiment in 11,816 psychoactive narratives. S F Friedman, G Ballentine, 10.1002/hup.2889Medline: 38117133]Hum Psychopharmacol. 391e2889Jan 2024</p>
<p>Cloze" readability scores as indices of individual differences in comprehension and aptitude. W L Taylor, 10.1037/h0040591J Appl Psychol. 4111957</p>
<p>. Erowid Homepage, Erowid, 2024-04-18</p>
<p>Automated design of ligands to polypharmacological profiles. J Besnard, G F Ruda, V Setola, K Abecassis, R M Rodriguiz, X P Huang, 10.1038/nature11691Nature. 4927428Dec 13. 2012FREE Full text. Medline: 23235874</p>
<p>Allen Brain Atlas: an integrated spatio-temporal portal for exploring the central nervous system. S M Sunkin, L Ng, C Lau, T Dolbeare, T L Gilbert, C L Thompson, 10.1093/nar/gks1042Nucleic Acids Res. 41Jan 2013FREE Full text. Medline: 23193282</p>
<p>GoEmotions: a dataset of fine-grained emotions. D Demszky, D Movshovitz-Attias, J Ko, A Cowen, G Nemade, S Ravi, 10.18653/v1/2020.acl-main.372Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline2020. 2020. July 5-10, 2020</p>
<p>Automatic post-traumatic stress disorder diagnosis via clinical transcripts: a novel text augmentation with large language models. Y Wu, J Chen, K Mao, Y Zhang, 10.1109/biocas58349.2023.10388714Proceedings of the IEEE Biomedical Circuits and Systems Conference. the IEEE Biomedical Circuits and Systems ConferenceToronto, ON2023. 2023. October 19-21, 2023Presented at</p>
<p>Exploring the use of large language models for improving the awareness of mindfulness. H Kumar, Y Wang, J Shi, I Musabirov, N A Farb, J J Williams, 10.1145/3544549.3585614Proceedings of the Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. 2023. Presented at: CHI EA '23. the Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. 2023. Presented at: CHI EA '23Hamburg, GermanyApril 23-28, 2023</p>
<p>Assessing prognosis in depression: comparing perspectives of AI models, mental health professionals and the general public. Z Elyoseph, I Levkovich, S Shinan-Altman, 10.1136/fmch-2023-002583Fam Med Community Health. 12Jan 09. 2024Suppl 1):e002583. [FREE Full text. Medline: 38199604</p>
<p>Clinical decision support for bipolar depression using large language models. R H Perlis, J F Goldberg, M J Ostacher, C D Schneck, 10.1038/s41386-024-01841-2Neuropsychopharmacology. 499Aug 2024Medline: 38480911</p>
<p>Psychiatrists' experiences and opinions of generative artificial intelligence in mental healthcare: an online mixed methods survey. C Blease, A Worthen, J Torous, 10.1016/j.psychres.2024.115724Psychiatry Res. Mar. 3331157242024FREE Full text. Medline: 38244285</p>
<p>Future of ADHD care: evaluating the efficacy of ChatGPT in therapy enhancement. Healthcare (Basel). S Berrezueta-Guzman, M Kandil, M L Martn-Ruiz, Pau De La Cruz, I Krusche, S , 10.3390/healthcare12060683Mar 19. 202412683FREE Full text. Medline: 38540647</p>
<p>Prevention and early intervention in youth mental health: is it time for a multidisciplinary and trans-diagnostic model for care?. M Colizzi, A Lasalvia, M Ruggeri, 10.1186/s13033-020-00356-9Int J Ment Health Syst. Mar. 24232020FREE Full text. Medline: 32226481</p>
<p>A review on sentiment analysis and emotion detection from text. P Nandwani, R Verma, 10.1007/s13278-021-00776-6Soc Netw Anal Min. 111812021FREE Full text. Medline: 34484462</p>
<p>A survey on hallucination in large language models: principles, taxonomy, challenges, and open questions. L Huang, W Yu, W Ma, W Zhong, Z Feng, H Wang, 10.48550/arXiv.2311.05232arXiv. Preprint posted online on November. 92023</p>
<p>GPT-4, and other large language models: the next revolution for clinical microbiology?. A Egli, Chatgpt, 10.1093/cid/ciad407Medline: 37399030]Clin Infect Dis. 779Nov 11. 2023FREE Full text</p>
<p>Beneficent dehumanization: employing artificial intelligence and carebots to mitigate shame-induced barriers to medical care. A Palmer, D Schwan, 10.1111/bioe.12986Medline: 34942057]Bioethics. 362Feb 2022</p>
<p>Dehumanization in medicine: causes, solutions, and functions. O S Haque, A Waytz, 10.1177/1745691611429706Medline: 26168442]Perspect Psychol Sci. Mar. 722012</p>
<p>Image inputs for ChatGPT -FAQ. accessed 2024-04-18</p>
<p>What is RAG (Retrieval Enhanced Generation)? Amazon Web Services. 2024-08-08</p>
<p>. Openai Models, Platform, 2024-08-08</p>
<p>What is retrieval-augmented generation (RAG)?. 2024-08-08Google Cloud</p>
<p>Self-contradictory hallucinations of large language models: evaluation, detection and mitigation. N Mndler, J He, S Jenko, M Vechev, 10.48550/arXiv.2305.15852May 25, 2023</p>
<p>Metric ensembles for hallucination detection. G C Forbes, P Katlana, Z Ortiz, 10.48550/arXiv.2310.10495October 16, 2023</p>
<p>Chatbots and large language models in radiology: a practical primer for clinical and research applications. R Bhayana, 10.1148/radiol.232756Radiology. 3101e232756Jan 2024Medline: 38226883</p>
<p>2024-04-27What is LLM temperature? Iguazio. </p>
<p>LLM optimization parameters. accessed 2024-04-27</p>
<p>Y Yao, J Duan, K Xu, Y Cai, Z Sun, Y Zhang, 10.1016/j.hcc.2024.100211A survey on large language model (LLM) security and privacy: the good, the bad, and the ugly. High Confid Comput. Jun 20244100211</p>
<p>Recovering from privacy-preserving masking with large language models. A Vats, Z Liu, P Su, D Paul, Y Ma, Y Pang, 10.1109/icassp48485.2024.10448234Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. the IEEE International Conference on Acoustics, Speech and Signal ProcessingSeoul, Republic of Korea2024. 2024. April 14-19, 2024Presented at</p>
<p>The black box problem: opaque inner workings of large language models. S Ramlochan, Prompt Engineering. accessed 2024-04-18</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2301 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2301</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2301</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-250446676</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.00146v1.pdf" target="_blank">On the Relationships between Graph Neural Networks for the Simulation of Physical Systems and Classical Numerical Methods</a></p>
                <p><strong>Paper Abstract:</strong> Recent developments in Machine Learning approaches for modelling physical systems have begun to mirror the past development of numerical methods in the computational sciences. In this survey, we begin by providing an example of this with the parallels between the development trajectories of graph neural network acceleration for physical simulations and particle-based approaches. We then give an overview of simulation approaches, which have not yet found their way into state-of-the-art Machine Learning methods and hold the potential to make Machine Learning approaches more accurate and more efficient. We conclude by presenting an outlook on the potential of these approaches for making Machine Learning models for science more efficient.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2301.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2301.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Network-based Simulators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned simulator using an encoder-processor-decoder Graph Network architecture that learns a differential operator mapping histories of particle states to accelerations and is rolled out with an explicit integrator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to simulate complex physics with graph networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Physical simulation (particle and fluid dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn time-evolution of particle systems (positions, velocities, accelerations) for fluids and fluid–solid interactions by learning the dynamical update (acceleration) from past states.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Limited-to-moderate; trained on simulated datasets (authors report training and evaluating on datasets up to 85k particles) — labeled supervised trajectories (state → acceleration) generated from classical simulators; accessibility depends on simulated data generation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Graph-structured time-series: particle states (positions, velocities, types) with edges for neighbors within a cutoff radius; temporal stacking of multiple past states into node/edge features.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — many interacting bodies (tens to 10⁵ particles), nonlinear dynamics, long-horizon rollout instability (distribution shift), multi-scale interactions; high dimensional state per particle.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging application of GNNs to well-established simulation domains (MD, SPH); classical solvers are mature but learned solvers are relatively new.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — scientific validity of trajectories matters (not just short-term prediction); interpretable/physics-consistent behavior is desirable though learned black-box accelerations are used.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Graph Neural Networks (encoder-processor-decoder) trained supervised to predict accelerations</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Encoder maps stacked past states into node and edge latent features (edges between particles within a cutoff). A multi-layer message-passing processor (several GN layers) iteratively updates the graph; decoder predicts accelerations (and other quantities). Predicted accelerations are integrated with a deterministic integrator (e.g., semi-implicit Euler). Training uses MSE on acceleration; data augmentations include additive Gaussian noise and multi-step loss variants.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning / physics-informed architecture (graph-structured, hybrid with classical integrators)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and well-suited for particle- and mesh-like physical systems; limitations include difficulty with deforming meshes/thin shells without additional mesh structure and sensitivity to rollout distribution shift; modifications (e.g., mesh edges, noise augmentation) often required.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Reported ability to simulate systems up to 85k particles (from referenced work); no further numeric error metrics provided in this survey paper (null for specific accuracy/RMSE values).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Works well for fluid simulations and fluid–solid interactions, but struggles on highly deforming meshes (e.g., thin shells) and suffers from long-horizon distribution shift unless regularized (noise, multi-step loss).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — can accelerate simulations and enable learning across particle/mesh representations; potential to replace or augment classical solvers where data exist, improving speed and enabling hybrid solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to Molecular Dynamics and Smoothed Particle Hydrodynamics: GNS requires histories of states (non-Markovian input), learns interactions rather than evaluating analytic potentials, and uses learned accelerations vs. analytic force models; performs well on fluids but inferior for deforming meshes compared to mesh-aware approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Architectural biases (graph structure, translational/permutation equivariance), use of physical priors (relational inductive biases), and data augmentation (noise, multi-step losses) improve rollout stability and generalization; failure modes arise when mesh deformation or high-frequency detail are not represented in graph.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Graph Network-based learned simulators are well-suited to particle-like, local-interaction systems but require architectural and training biases (mesh structure, noise, multi-step loss) to handle long-horizon stability and complex mesh deformation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2301.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2301.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MeshGraphNets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MeshGraphNets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based learned simulator that augments Graph Network simulators with mesh-specific edges, adaptive remeshing, and extended outputs (e.g., stress fields) to better model mesh-based continuum mechanics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning mesh-based simulation with graph networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Continuum mechanics / mesh-based physical simulation (fluid/solid mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn the dynamics on mesh discretizations to predict next states and field quantities (pressure, stress) with the ability to handle mesh connectivity and adaptive remeshing.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate; trained on simulation data produced on meshes. Data are supervised trajectories and per-node/element field labels produced by classical FEM/FVM solvers; accessibility depends on simulation setups.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Graph representing mesh nodes and two types of edges (mesh-type and collision-type), with node features (state, geometry) and edge features (displacements), plus optional remeshing events.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — complex mesh connectivity, potential large numbers of degrees of freedom, need to represent high-frequency phenomena and adapt mesh to preserve information under deformation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging — builds directly on mature mesh-based numerical methods (FEM, remeshing techniques) but application of learned methods to meshes is recent.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High-to-medium — accurate stress/pressure fields and physically consistent remeshing behavior are important for scientific and engineering validity, so interpretable/physics-consistent outputs are valuable.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Graph Neural Networks extended for mesh structures with explicit mesh and collision edges, adaptive remeshing, and multi-target outputs</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Input includes a predefined mesh; encoder-processor-decoder GN structure with two classes of edges (mesh connectivity edges and collision edges) whose interactions are superimposed; supports adaptive remeshing heuristics and extends outputs to predict dynamical fields like pressure and stress; trained supervised with MSE and optionally with noise augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning / physics-biased GNNs / hybrid (learned components + classical remeshing strategies)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>More appropriate than particle-only GNNs for mesh-based problems and deforming domains; remeshing and mesh-edge decomposition improve modeling of deforming meshes and high-frequency detail.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>No specific numeric error metrics provided in the survey; qualitative claims of outperforming GNS on mesh tasks and ability to model stress and pressure fields (quantitative details in original Pfaff et al. 2021 paper).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Improves representation of meshes over GNS, can adaptively remesh to retain high-frequency information, and predicts additional physical fields; overall more robust for mesh-based simulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for engineering workflows reliant on mesh simulations — can accelerate mesh-based solvers, enable learned surrogate models for FEM/FVM, and unify particle and mesh representations under GNN frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to GNS: MeshGraphNets uses explicit mesh edges and remeshing and extends outputs, which addresses GNS shortcomings on deforming meshes; aligns closely with classical remeshing strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Explicit encoding of mesh topology, decomposition of interaction functions by edge type, adaptive remeshing, and inclusion of additional predicted fields (stress/pressure) are key to better performance on mesh problems.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Incorporating classical mesh concepts (mesh edges, remeshing, multiple edge-interaction superposition) into GNN architectures substantially improves learned simulators' ability to handle deforming meshes and predict field quantities.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2301.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2301.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph Networks / GNNs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Neural Networks / Graph Network (GN) framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general framework for learning on graph-structured data that uses node/edge/global features, message passing (processor), and encoder-decoder architectures to model local interactions and aggregate information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Relational inductive biases, deep learning, and graph networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Broadly applied to physical simulations (molecular dynamics, fluid mechanics), PDE approximation, coarse-graining, and surrogate modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Model interactions between many agents/particles/mesh nodes to predict dynamics or field quantities where interactions are local and relational.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varies by domain; often simulated labeled trajectories (moderate data), in some scientific tasks data may be scarce — but GNNs exploit locality to be data-efficient in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Graph-structured spatial data, sometimes with temporal stacking (time series), node/edge features, and irregular sampling (non-grid).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — many interacting units, nonlinearity, multi-scale dependencies; receptive field expands with depth, potentially leading to over-smoothing.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Rapidly maturing for scientific ML; classical graph-based relational modeling is established but GNNs for PDEs and large-scale physics is an active research area.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — embedding physical symmetries (equivariance, invariants) and conservation laws is beneficial; purely black-box may be insufficient for high-stakes scientific inference.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Message-passing Graph Neural Networks (encoder-processor-decoder, equivariant variants)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Nodes represent physical entities, edges represent interactions (often within a cutoff radius). Message functions and aggregation (sum) encode superposition. Variants include equivariant message passing for vectors/tensors, stacking multiple GN layers, and multi-step or noise-augmented training to improve long-horizon stability.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning / physics-informed ML / equivariant deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited to unstructured spatial domains and problems with local interactions; can emulate stencil and kernel-based solvers when properly biased; limited by choice of receptive field, depth, and training data.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Equivariant GNNs reported up to order-of-magnitude improvements in some molecular property prediction tasks (cited Batzner et al. 2022), but quantitative metrics depend on task and dataset (null in survey for many specific tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>GNNs effectively capture locality and relational structure; equivariant variants significantly improve data efficiency and accuracy; success depends on alignment of architecture with physical symmetries and training strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — GNNs provide a unifying framework to represent particles and meshes, support coarse-graining, and facilitate multi-scale modeling in scientific simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to grid-based CNNs and spectral methods: GNNs better handle irregular domains and unstructured data; can incorporate stencil-like operations but require different biases for grid-like efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Incorporation of physical inductive biases (equivariance, locality, superposition), appropriate graph construction (cutoff radius, edge types), and training strategies (noise augmentation, multi-step loss) are key.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Graph-based architectures are naturally aligned with many physical simulation problems, and embedding classical numerical biases (symmetries, locality, superposition) into GNNs greatly enhances effectiveness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2301.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2301.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural ODEs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Ordinary Differential Equations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural networks that define continuous-time dynamics by parameterizing the derivative (dynamics) function and using ODE integrators for forward simulation and training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural ordinary differential equations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Differential equation solvers and continuous-time modeling in physical systems; a conceptual precursor to Neural PDEs and learned integrators.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn continuous-time dynamics from data by learning the time derivative function and using numerical ODE solvers for integration and training.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varies; often moderate labeled trajectory data; training can be done on short trajectories but long-horizon behavior poses challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time-series state vectors; can be combined with graph-structured encodings for particle systems.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Can be high depending on state dimensionality and stiffness of dynamics; requires careful integrator/step-size control for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established as a research methodology since 2018; extensions to PDEs and learned integrators are active research areas.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — learned continuous dynamics can be interpretable in some settings but may lack mechanistic guarantees unless constrained.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Continuous-time neural network defining ODE right-hand-side, integrated by adaptive ODE solvers</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>A neural net parameterizes f(x,t;θ) = dx/dt; outputs are integrated with adaptive integrators (e.g., Dormand–Prince) during forward pass and adjoint or other methods for gradients; suggests combining with physics-informed losses and integrator-order considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised / continuous-time deep learning / hybrid with numerical integrators</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable for systems where continuous-time modeling is natural; integration with higher-order or adaptive integrators improves robustness; may struggle with stiff systems without special handling.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Survey notes it inspired Neural PDEs but provides no specific quantitative metrics here (null).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Enables learned continuous dynamics and motivated subsequent neural PDE work; combining with adaptive integrators and considering order of integrator is important for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant conceptual impact — provides a framework to meld neural networks with numerical integrators and motivates learned integrators and stability analyses for learned simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Classical ODE solvers provide provable stability under known RHS; Neural ODEs trade analytic RHS for learned RHS and rely on integrator properties — adaptive step-size ideas translate from classical ODE methods.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Using appropriate integrator order and adaptive step-size control, and enforcing physical biases or constraints, improves stability and long-term accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Marrying neural parameterizations of dynamics with classical adaptive integrators transfers important numerical-stability tools to learned simulators and motivates Neural PDE extensions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2301.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2301.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Message-passing neural PDE solvers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Message Passing Neural PDE Solvers / Neural PDEs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural architectures that treat PDE solution operators via message-passing (GNN) paradigms to approximate PDE dynamics on unstructured domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Message passing neural PDE solvers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>PDE solution and surrogate modeling for continuum physics (fluids, elasticity, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Approximate the evolution operator of PDEs using learned message-passing architectures that respect locality and possibly equivariance to predict fields and dynamics on meshes/particles.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Typically moderate; training uses simulation-generated datasets from classical PDE solvers; some PDE tasks may be data-constrained for multi-scale or complex materials.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Graph or mesh data representing spatial discretization (nodes, elements), with time-series labels for field quantities.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — PDEs can be nonlinear, multi-scale, stiff, and require resolving boundary/initial conditions and conservation properties.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging; recent work is actively exploring how to combine numerical methods insights with GNNs for PDEs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — conservation laws and boundary conditions are crucial; explicit inclusion of symmetries and numerical biases is beneficial for scientific correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Message-passing Graph Neural Networks specialized to approximate PDE solution operators</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Use of encoder-processor-decoder GN stacks on spatial discretization graphs; architectures may include equivariant layers, multi-step loss, noise augmentation, and physics-inspired feature engineering; trained supervised to match PDE solver outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning / physics-informed ML / hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited for unstructured spatial discretizations and local PDE operators; requires architectural biases to handle conservation and boundary conditions reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>No specific numeric values in the survey; referenced works report task-specific improvements (see original Message Passing Neural PDE Solvers paper).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Promising for approximating PDE solvers on meshes/particles, especially when incorporating equivariance and numerical insights; long-horizon stability remains a challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — could provide fast surrogates for expensive PDE solvers and enable data-driven acceleration of simulations, especially in engineering contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to stencil/kernel-based numerical solvers and to spectral operator learning (e.g., FNO): message-passing GNNs better handle unstructured domains while spectral methods excel on regular grids.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Embedding numerical biases (equivariance, conservation), appropriate graph construction, and training strategies (multi-step, noise) enhance performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Message-passing GNNs can approximate PDE solution operators on unstructured domains but require integration of classical numerical insights to ensure stability, conservation, and long-horizon fidelity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2301.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2301.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E(3)-equivariant GNNs / NequIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>E(3)-equivariant Graph Neural Networks (e.g., NequIP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph neural networks enforcing equivariance to 3D Euclidean transformations (rotations, translations, reflections) to predict vector/tensorial properties in atomistic and molecular systems, improving data efficiency and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Atomistic simulation / molecular dynamics / interatomic potential learning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn interatomic potentials or force fields that are equivariant to rotations and translations to predict forces, energies, and dynamical evolution accurately and data-efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Often limited (high cost per sample from ab initio simulations), so data-efficiency is important; supervised labeled datasets of energies/forces are used.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Graph of atoms with 3D coordinates and species labels; tasks involve vector/tensor targets (forces, stresses) and scalar energies.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High — quantum-level interactions are complex and high-dimensional; symmetry constraints reduce effective complexity but learning remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Rapidly maturing; equivariant GNNs represent state of the art for many interatomic prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — conservation laws, symmetry, and physical correctness are critical; models often aim to be physically interpretable and conservative.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Equivariant Message Passing Neural Networks (E(3)-equivariant GNNs)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Message-passing GN architecture with features and operations designed to be equivariant under 3D rotations/translations; uses vector/tensor representations rather than only scalars; improves sample efficiency and accuracy for predicting forces and energies.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning / equivariant deep learning / physics-informed</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable to atomistic problems where rotational equivariance is a core symmetry; less directly applicable to problems lacking these symmetries.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Survey cites claims of up to an order-of-magnitude performance improvement compared to invariant methods in molecular property prediction (from Batzner et al. 2022), but no task-specific numbers given here.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Significant performance boost in data-efficiency and accuracy when equivariance is enforced; supports learning accurate interatomic potentials with less data.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables more accurate and data-efficient learning of interatomic potentials and large-scale atomistic dynamics, facilitating accelerated MD and material discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to invariant (scalar-only) GNNs like SchNet: equivariant models perform substantially better on vector/tensor prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Incorporation of symmetry constraints (equivariance), use of local interactions, and message-passing design aligned with physics are key contributors to success.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Enforcing Euclidean equivariance in GNNs aligns architectural inductive bias with physical symmetries, yielding substantial gains in data efficiency and predictive accuracy for atomistic simulations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2301.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2301.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fourier Neural Operator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fourier Neural Operator (FNO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural operator approach that learns mappings between function spaces (solutions of PDEs) by applying convolutions in Fourier space, enabling fast approximation of parametric PDE solution operators on regular grids.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fourier neural operator for parametric partial differential equations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Parametric PDE solution and surrogate modeling (fluid dynamics, continuum mechanics) on regular grids</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn a mapping from PDE parameters/boundary conditions to solution fields efficiently across parameter space, enabling fast inference for many-query or real-time applications.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate-to-abundant for simulated datasets on regular grids; requires labeled pairs of parameter → solution from classical solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Regular-grid spatial data (images/volumetric grids), continuous fields amenable to spectral transforms.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High for nonlinear PDEs and multi-scale phenomena; but spectral representation can reduce complexity for smooth problems.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Relatively recent but rapidly adopted for regular-grid PDE surrogate tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — yields accurate operators but may lack fine-grained interpretability; useful for fast surrogate modeling where fidelity matters.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Neural operator using Fourier transforms (spectral convolutions)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Applies learned linear transforms in Fourier space (global spectral convolution) combined with local nonlinearities to map input fields/parameters to output solution fields; highly efficient on regular, grid-based data.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning / operator learning / spectral deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited for PDEs on regular grids and periodic or smoothly varying domains; less suited to highly irregular/unstructured domains where GNNs excel.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>No specific numeric metrics provided in the survey; original FNO papers report strong performance and generalization across parameter regimes (not enumerated here).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Very effective on regular-grid PDE problems and for parametric generalization; complementary to GNNs which handle unstructured domains.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for rapid surrogate PDE solvers on grids, inverse problems, and parameter studies where many evaluations are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to GNNs and stencil-based methods: FNOs excel on regular grids using spectral efficiency, while GNNs handle unstructured meshes and local interactions better.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Exploiting global spectral structure of PDE solutions and using learned spectral filters enable strong operator approximation and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Spectral neural operators like FNO are powerful for grid-based PDE surrogate modeling but complement rather than replace graph-based learned solvers that target unstructured domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2301.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2301.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Noise augmentation (additive Gaussian noise)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Additive Gaussian Noise Data Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training technique that corrupts inputs with Gaussian noise to regularize learned simulators and improve long-horizon rollout stability by forcing robustness to accumulating errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Training stability for learned physical simulators (GNS, MeshGraphNets)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Mitigate distribution shift and error accumulation during long rollouts of learned simulators by making the network robust to noisy inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Applied to graph-structured time-series inputs (particle/node states); noise added to node/edge features during training.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Technique-level — interacts with long-horizon nonlinearity of learned dynamics and stability of integrators.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Common ML technique applied to physics-informed learning; survey notes it is widely used in the referenced learned simulator literature.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium — improves empirical robustness but introduces hyperparameters (noise scale) requiring tuning; physical interpretation possible via mesoscopic particle models.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Supervised training with additive Gaussian input noise</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>During training, inputs (node/edge features or past states) are corrupted with Gaussian noise of tuned scale to regularize and emulate the effect of accumulated errors; sometimes tied to ideas from mesoscopic models (Brownian noise) to set the noise scale.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised regularization / data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable to learned simulators to extend stable rollout horizons; requires tuning of noise scale hyperparameters and may not substitute for structural modeling of underlying physics.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>No numeric metrics in survey; qualitative reports indicate it permits much longer rollouts in referenced works (Sanchez-Gonzalez et al. 2020; Pfaff et al. 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Effective empirical regularizer that increases long-horizon stability, but performance is sensitive to chosen noise scale and introduces extra hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate to high as a practical training trick to stabilize learned simulators; connecting noise scale to physical mesoscopic considerations could improve principled tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to multi-step loss training: results vary across works — some report multi-step loss outperforms noise, others report the opposite; no universal winner.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Proper tuning of noise scale and grounding noise magnitude in physical reasoning (e.g., Fluctuation–Dissipation, particle mass scaling) improve success.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Input noise injection is a practical regularizer that improves rollout stability of learned simulators, but its effectiveness depends on appropriately chosen scales and can be better informed by physical mesoscopic models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2301.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2301.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-step loss</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-step Loss / Rollout Supervision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training strategy where the model is unrolled for multiple steps during training and the loss is accumulated over the trajectory to encourage the model to correct its own future errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Training stability and long-horizon prediction for learned simulators (GNN-based simulators, Neural PDEs)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Reduce distribution shift and error accumulation in long rollouts by training the model to minimize multi-step prediction error rather than single-step error alone.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Temporal graph sequences; training generates short trajectories from model predictions and compares to ground truth over multiple future steps.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Technique-level complexity related to nonlinearity and stability over multiple time steps; increases computational cost during training.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established technique in sequential modeling and recently applied to physics simulators; evidence for effectiveness is mixed across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low-to-medium — aims at empirical robustness rather than mechanistic interpretability; may implicitly correct nonphysical behavior but does not enforce physics constraints explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Multi-step rollout supervised training</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>During training, the learned model is used to iteratively predict multiple future states; the loss is summed over these predicted future steps (and may include teacher forcing initially); this forces the network to account for its own prediction errors.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning / sequence training / curriculum learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable to iterative simulators to improve long-horizon accuracy; increases training cost and sometimes less effective than noise augmentation depending on task and implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>No unified quantitative metrics in survey; referenced works report mixed empirical outcomes (Brandstetter et al. 2022 reported better performance than noise in some cases; Sanchez-Gonzalez et al. 2020 reported opposite).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Can improve stability by forcing future-correction behavior, but mixed evidence suggests task-dependent effectiveness; may serve similar role as adaptive integrators in classical solvers but does not change timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Moderate — a useful training strategy for improving rollout stability, especially when combined with other physical biases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly to noise injection: mixed results across studies; conceptually related to adaptive integrator ideas from classical ODE solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Proper rollout length, curriculum scheduling, and combination with other biases (equivariance, integrator order) influence success.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Training with multi-step losses can improve long-horizon simulator stability by making the model correct its own future errors, but its relative effectiveness compared to noise augmentation is task-dependent and not conclusive.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to simulate complex physics with graph networks <em>(Rating: 2)</em></li>
                <li>Learning mesh-based simulation with graph networks <em>(Rating: 2)</em></li>
                <li>Neural ordinary differential equations <em>(Rating: 2)</em></li>
                <li>E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials <em>(Rating: 2)</em></li>
                <li>Fourier neural operator for parametric partial differential equations <em>(Rating: 2)</em></li>
                <li>Message passing neural PDE solvers <em>(Rating: 2)</em></li>
                <li>Simulating liquids with graph networks <em>(Rating: 1)</em></li>
                <li>Machine learning-accelerated computational fluid dynamics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2301",
    "paper_id": "paper-250446676",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "GNS",
            "name_full": "Graph Network-based Simulators",
            "brief_description": "A learned simulator using an encoder-processor-decoder Graph Network architecture that learns a differential operator mapping histories of particle states to accelerations and is rolled out with an explicit integrator.",
            "citation_title": "Learning to simulate complex physics with graph networks",
            "mention_or_use": "use",
            "scientific_problem_domain": "Physical simulation (particle and fluid dynamics)",
            "problem_description": "Learn time-evolution of particle systems (positions, velocities, accelerations) for fluids and fluid–solid interactions by learning the dynamical update (acceleration) from past states.",
            "data_availability": "Limited-to-moderate; trained on simulated datasets (authors report training and evaluating on datasets up to 85k particles) — labeled supervised trajectories (state → acceleration) generated from classical simulators; accessibility depends on simulated data generation pipelines.",
            "data_structure": "Graph-structured time-series: particle states (positions, velocities, types) with edges for neighbors within a cutoff radius; temporal stacking of multiple past states into node/edge features.",
            "problem_complexity": "High — many interacting bodies (tens to 10⁵ particles), nonlinear dynamics, long-horizon rollout instability (distribution shift), multi-scale interactions; high dimensional state per particle.",
            "domain_maturity": "Emerging application of GNNs to well-established simulation domains (MD, SPH); classical solvers are mature but learned solvers are relatively new.",
            "mechanistic_understanding_requirements": "Medium — scientific validity of trajectories matters (not just short-term prediction); interpretable/physics-consistent behavior is desirable though learned black-box accelerations are used.",
            "ai_methodology_name": "Graph Neural Networks (encoder-processor-decoder) trained supervised to predict accelerations",
            "ai_methodology_description": "Encoder maps stacked past states into node and edge latent features (edges between particles within a cutoff). A multi-layer message-passing processor (several GN layers) iteratively updates the graph; decoder predicts accelerations (and other quantities). Predicted accelerations are integrated with a deterministic integrator (e.g., semi-implicit Euler). Training uses MSE on acceleration; data augmentations include additive Gaussian noise and multi-step loss variants.",
            "ai_methodology_category": "Supervised learning / physics-informed architecture (graph-structured, hybrid with classical integrators)",
            "applicability": "Applicable and well-suited for particle- and mesh-like physical systems; limitations include difficulty with deforming meshes/thin shells without additional mesh structure and sensitivity to rollout distribution shift; modifications (e.g., mesh edges, noise augmentation) often required.",
            "effectiveness_quantitative": "Reported ability to simulate systems up to 85k particles (from referenced work); no further numeric error metrics provided in this survey paper (null for specific accuracy/RMSE values).",
            "effectiveness_qualitative": "Works well for fluid simulations and fluid–solid interactions, but struggles on highly deforming meshes (e.g., thin shells) and suffers from long-horizon distribution shift unless regularized (noise, multi-step loss).",
            "impact_potential": "High — can accelerate simulations and enable learning across particle/mesh representations; potential to replace or augment classical solvers where data exist, improving speed and enabling hybrid solvers.",
            "comparison_to_alternatives": "Compared qualitatively to Molecular Dynamics and Smoothed Particle Hydrodynamics: GNS requires histories of states (non-Markovian input), learns interactions rather than evaluating analytic potentials, and uses learned accelerations vs. analytic force models; performs well on fluids but inferior for deforming meshes compared to mesh-aware approaches.",
            "success_factors": "Architectural biases (graph structure, translational/permutation equivariance), use of physical priors (relational inductive biases), and data augmentation (noise, multi-step losses) improve rollout stability and generalization; failure modes arise when mesh deformation or high-frequency detail are not represented in graph.",
            "key_insight": "Graph Network-based learned simulators are well-suited to particle-like, local-interaction systems but require architectural and training biases (mesh structure, noise, multi-step loss) to handle long-horizon stability and complex mesh deformation.",
            "uuid": "e2301.0"
        },
        {
            "name_short": "MeshGraphNets",
            "name_full": "MeshGraphNets",
            "brief_description": "A graph-based learned simulator that augments Graph Network simulators with mesh-specific edges, adaptive remeshing, and extended outputs (e.g., stress fields) to better model mesh-based continuum mechanics.",
            "citation_title": "Learning mesh-based simulation with graph networks",
            "mention_or_use": "use",
            "scientific_problem_domain": "Continuum mechanics / mesh-based physical simulation (fluid/solid mechanics)",
            "problem_description": "Learn the dynamics on mesh discretizations to predict next states and field quantities (pressure, stress) with the ability to handle mesh connectivity and adaptive remeshing.",
            "data_availability": "Moderate; trained on simulation data produced on meshes. Data are supervised trajectories and per-node/element field labels produced by classical FEM/FVM solvers; accessibility depends on simulation setups.",
            "data_structure": "Graph representing mesh nodes and two types of edges (mesh-type and collision-type), with node features (state, geometry) and edge features (displacements), plus optional remeshing events.",
            "problem_complexity": "High — complex mesh connectivity, potential large numbers of degrees of freedom, need to represent high-frequency phenomena and adapt mesh to preserve information under deformation.",
            "domain_maturity": "Emerging — builds directly on mature mesh-based numerical methods (FEM, remeshing techniques) but application of learned methods to meshes is recent.",
            "mechanistic_understanding_requirements": "High-to-medium — accurate stress/pressure fields and physically consistent remeshing behavior are important for scientific and engineering validity, so interpretable/physics-consistent outputs are valuable.",
            "ai_methodology_name": "Graph Neural Networks extended for mesh structures with explicit mesh and collision edges, adaptive remeshing, and multi-target outputs",
            "ai_methodology_description": "Input includes a predefined mesh; encoder-processor-decoder GN structure with two classes of edges (mesh connectivity edges and collision edges) whose interactions are superimposed; supports adaptive remeshing heuristics and extends outputs to predict dynamical fields like pressure and stress; trained supervised with MSE and optionally with noise augmentation.",
            "ai_methodology_category": "Supervised learning / physics-biased GNNs / hybrid (learned components + classical remeshing strategies)",
            "applicability": "More appropriate than particle-only GNNs for mesh-based problems and deforming domains; remeshing and mesh-edge decomposition improve modeling of deforming meshes and high-frequency detail.",
            "effectiveness_quantitative": "No specific numeric error metrics provided in the survey; qualitative claims of outperforming GNS on mesh tasks and ability to model stress and pressure fields (quantitative details in original Pfaff et al. 2021 paper).",
            "effectiveness_qualitative": "Improves representation of meshes over GNS, can adaptively remesh to retain high-frequency information, and predicts additional physical fields; overall more robust for mesh-based simulation tasks.",
            "impact_potential": "High for engineering workflows reliant on mesh simulations — can accelerate mesh-based solvers, enable learned surrogate models for FEM/FVM, and unify particle and mesh representations under GNN frameworks.",
            "comparison_to_alternatives": "Compared to GNS: MeshGraphNets uses explicit mesh edges and remeshing and extends outputs, which addresses GNS shortcomings on deforming meshes; aligns closely with classical remeshing strategies.",
            "success_factors": "Explicit encoding of mesh topology, decomposition of interaction functions by edge type, adaptive remeshing, and inclusion of additional predicted fields (stress/pressure) are key to better performance on mesh problems.",
            "key_insight": "Incorporating classical mesh concepts (mesh edges, remeshing, multiple edge-interaction superposition) into GNN architectures substantially improves learned simulators' ability to handle deforming meshes and predict field quantities.",
            "uuid": "e2301.1"
        },
        {
            "name_short": "Graph Networks / GNNs (general)",
            "name_full": "Graph Neural Networks / Graph Network (GN) framework",
            "brief_description": "A general framework for learning on graph-structured data that uses node/edge/global features, message passing (processor), and encoder-decoder architectures to model local interactions and aggregate information.",
            "citation_title": "Relational inductive biases, deep learning, and graph networks",
            "mention_or_use": "use",
            "scientific_problem_domain": "Broadly applied to physical simulations (molecular dynamics, fluid mechanics), PDE approximation, coarse-graining, and surrogate modeling.",
            "problem_description": "Model interactions between many agents/particles/mesh nodes to predict dynamics or field quantities where interactions are local and relational.",
            "data_availability": "Varies by domain; often simulated labeled trajectories (moderate data), in some scientific tasks data may be scarce — but GNNs exploit locality to be data-efficient in many cases.",
            "data_structure": "Graph-structured spatial data, sometimes with temporal stacking (time series), node/edge features, and irregular sampling (non-grid).",
            "problem_complexity": "High — many interacting units, nonlinearity, multi-scale dependencies; receptive field expands with depth, potentially leading to over-smoothing.",
            "domain_maturity": "Rapidly maturing for scientific ML; classical graph-based relational modeling is established but GNNs for PDEs and large-scale physics is an active research area.",
            "mechanistic_understanding_requirements": "Medium — embedding physical symmetries (equivariance, invariants) and conservation laws is beneficial; purely black-box may be insufficient for high-stakes scientific inference.",
            "ai_methodology_name": "Message-passing Graph Neural Networks (encoder-processor-decoder, equivariant variants)",
            "ai_methodology_description": "Nodes represent physical entities, edges represent interactions (often within a cutoff radius). Message functions and aggregation (sum) encode superposition. Variants include equivariant message passing for vectors/tensors, stacking multiple GN layers, and multi-step or noise-augmented training to improve long-horizon stability.",
            "ai_methodology_category": "Supervised learning / physics-informed ML / equivariant deep learning",
            "applicability": "Well-suited to unstructured spatial domains and problems with local interactions; can emulate stencil and kernel-based solvers when properly biased; limited by choice of receptive field, depth, and training data.",
            "effectiveness_quantitative": "Equivariant GNNs reported up to order-of-magnitude improvements in some molecular property prediction tasks (cited Batzner et al. 2022), but quantitative metrics depend on task and dataset (null in survey for many specific tasks).",
            "effectiveness_qualitative": "GNNs effectively capture locality and relational structure; equivariant variants significantly improve data efficiency and accuracy; success depends on alignment of architecture with physical symmetries and training strategies.",
            "impact_potential": "High — GNNs provide a unifying framework to represent particles and meshes, support coarse-graining, and facilitate multi-scale modeling in scientific simulations.",
            "comparison_to_alternatives": "Compared to grid-based CNNs and spectral methods: GNNs better handle irregular domains and unstructured data; can incorporate stencil-like operations but require different biases for grid-like efficiency.",
            "success_factors": "Incorporation of physical inductive biases (equivariance, locality, superposition), appropriate graph construction (cutoff radius, edge types), and training strategies (noise augmentation, multi-step loss) are key.",
            "key_insight": "Graph-based architectures are naturally aligned with many physical simulation problems, and embedding classical numerical biases (symmetries, locality, superposition) into GNNs greatly enhances effectiveness.",
            "uuid": "e2301.2"
        },
        {
            "name_short": "Neural ODEs",
            "name_full": "Neural Ordinary Differential Equations",
            "brief_description": "Neural networks that define continuous-time dynamics by parameterizing the derivative (dynamics) function and using ODE integrators for forward simulation and training.",
            "citation_title": "Neural ordinary differential equations",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Differential equation solvers and continuous-time modeling in physical systems; a conceptual precursor to Neural PDEs and learned integrators.",
            "problem_description": "Learn continuous-time dynamics from data by learning the time derivative function and using numerical ODE solvers for integration and training.",
            "data_availability": "Varies; often moderate labeled trajectory data; training can be done on short trajectories but long-horizon behavior poses challenges.",
            "data_structure": "Time-series state vectors; can be combined with graph-structured encodings for particle systems.",
            "problem_complexity": "Can be high depending on state dimensionality and stiffness of dynamics; requires careful integrator/step-size control for stability.",
            "domain_maturity": "Established as a research methodology since 2018; extensions to PDEs and learned integrators are active research areas.",
            "mechanistic_understanding_requirements": "Medium — learned continuous dynamics can be interpretable in some settings but may lack mechanistic guarantees unless constrained.",
            "ai_methodology_name": "Continuous-time neural network defining ODE right-hand-side, integrated by adaptive ODE solvers",
            "ai_methodology_description": "A neural net parameterizes f(x,t;θ) = dx/dt; outputs are integrated with adaptive integrators (e.g., Dormand–Prince) during forward pass and adjoint or other methods for gradients; suggests combining with physics-informed losses and integrator-order considerations.",
            "ai_methodology_category": "Supervised / continuous-time deep learning / hybrid with numerical integrators",
            "applicability": "Applicable for systems where continuous-time modeling is natural; integration with higher-order or adaptive integrators improves robustness; may struggle with stiff systems without special handling.",
            "effectiveness_quantitative": "Survey notes it inspired Neural PDEs but provides no specific quantitative metrics here (null).",
            "effectiveness_qualitative": "Enables learned continuous dynamics and motivated subsequent neural PDE work; combining with adaptive integrators and considering order of integrator is important for robustness.",
            "impact_potential": "Significant conceptual impact — provides a framework to meld neural networks with numerical integrators and motivates learned integrators and stability analyses for learned simulators.",
            "comparison_to_alternatives": "Classical ODE solvers provide provable stability under known RHS; Neural ODEs trade analytic RHS for learned RHS and rely on integrator properties — adaptive step-size ideas translate from classical ODE methods.",
            "success_factors": "Using appropriate integrator order and adaptive step-size control, and enforcing physical biases or constraints, improves stability and long-term accuracy.",
            "key_insight": "Marrying neural parameterizations of dynamics with classical adaptive integrators transfers important numerical-stability tools to learned simulators and motivates Neural PDE extensions.",
            "uuid": "e2301.3"
        },
        {
            "name_short": "Message-passing neural PDE solvers",
            "name_full": "Message Passing Neural PDE Solvers / Neural PDEs",
            "brief_description": "Neural architectures that treat PDE solution operators via message-passing (GNN) paradigms to approximate PDE dynamics on unstructured domains.",
            "citation_title": "Message passing neural PDE solvers",
            "mention_or_use": "mention",
            "scientific_problem_domain": "PDE solution and surrogate modeling for continuum physics (fluids, elasticity, etc.)",
            "problem_description": "Approximate the evolution operator of PDEs using learned message-passing architectures that respect locality and possibly equivariance to predict fields and dynamics on meshes/particles.",
            "data_availability": "Typically moderate; training uses simulation-generated datasets from classical PDE solvers; some PDE tasks may be data-constrained for multi-scale or complex materials.",
            "data_structure": "Graph or mesh data representing spatial discretization (nodes, elements), with time-series labels for field quantities.",
            "problem_complexity": "High — PDEs can be nonlinear, multi-scale, stiff, and require resolving boundary/initial conditions and conservation properties.",
            "domain_maturity": "Emerging; recent work is actively exploring how to combine numerical methods insights with GNNs for PDEs.",
            "mechanistic_understanding_requirements": "High — conservation laws and boundary conditions are crucial; explicit inclusion of symmetries and numerical biases is beneficial for scientific correctness.",
            "ai_methodology_name": "Message-passing Graph Neural Networks specialized to approximate PDE solution operators",
            "ai_methodology_description": "Use of encoder-processor-decoder GN stacks on spatial discretization graphs; architectures may include equivariant layers, multi-step loss, noise augmentation, and physics-inspired feature engineering; trained supervised to match PDE solver outputs.",
            "ai_methodology_category": "Supervised learning / physics-informed ML / hybrid",
            "applicability": "Well-suited for unstructured spatial discretizations and local PDE operators; requires architectural biases to handle conservation and boundary conditions reliably.",
            "effectiveness_quantitative": "No specific numeric values in the survey; referenced works report task-specific improvements (see original Message Passing Neural PDE Solvers paper).",
            "effectiveness_qualitative": "Promising for approximating PDE solvers on meshes/particles, especially when incorporating equivariance and numerical insights; long-horizon stability remains a challenge.",
            "impact_potential": "High — could provide fast surrogates for expensive PDE solvers and enable data-driven acceleration of simulations, especially in engineering contexts.",
            "comparison_to_alternatives": "Compared conceptually to stencil/kernel-based numerical solvers and to spectral operator learning (e.g., FNO): message-passing GNNs better handle unstructured domains while spectral methods excel on regular grids.",
            "success_factors": "Embedding numerical biases (equivariance, conservation), appropriate graph construction, and training strategies (multi-step, noise) enhance performance.",
            "key_insight": "Message-passing GNNs can approximate PDE solution operators on unstructured domains but require integration of classical numerical insights to ensure stability, conservation, and long-horizon fidelity.",
            "uuid": "e2301.4"
        },
        {
            "name_short": "E(3)-equivariant GNNs / NequIP",
            "name_full": "E(3)-equivariant Graph Neural Networks (e.g., NequIP)",
            "brief_description": "Graph neural networks enforcing equivariance to 3D Euclidean transformations (rotations, translations, reflections) to predict vector/tensorial properties in atomistic and molecular systems, improving data efficiency and accuracy.",
            "citation_title": "E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Atomistic simulation / molecular dynamics / interatomic potential learning",
            "problem_description": "Learn interatomic potentials or force fields that are equivariant to rotations and translations to predict forces, energies, and dynamical evolution accurately and data-efficiently.",
            "data_availability": "Often limited (high cost per sample from ab initio simulations), so data-efficiency is important; supervised labeled datasets of energies/forces are used.",
            "data_structure": "Graph of atoms with 3D coordinates and species labels; tasks involve vector/tensor targets (forces, stresses) and scalar energies.",
            "problem_complexity": "High — quantum-level interactions are complex and high-dimensional; symmetry constraints reduce effective complexity but learning remains challenging.",
            "domain_maturity": "Rapidly maturing; equivariant GNNs represent state of the art for many interatomic prediction tasks.",
            "mechanistic_understanding_requirements": "High — conservation laws, symmetry, and physical correctness are critical; models often aim to be physically interpretable and conservative.",
            "ai_methodology_name": "Equivariant Message Passing Neural Networks (E(3)-equivariant GNNs)",
            "ai_methodology_description": "Message-passing GN architecture with features and operations designed to be equivariant under 3D rotations/translations; uses vector/tensor representations rather than only scalars; improves sample efficiency and accuracy for predicting forces and energies.",
            "ai_methodology_category": "Supervised learning / equivariant deep learning / physics-informed",
            "applicability": "Highly applicable to atomistic problems where rotational equivariance is a core symmetry; less directly applicable to problems lacking these symmetries.",
            "effectiveness_quantitative": "Survey cites claims of up to an order-of-magnitude performance improvement compared to invariant methods in molecular property prediction (from Batzner et al. 2022), but no task-specific numbers given here.",
            "effectiveness_qualitative": "Significant performance boost in data-efficiency and accuracy when equivariance is enforced; supports learning accurate interatomic potentials with less data.",
            "impact_potential": "High — enables more accurate and data-efficient learning of interatomic potentials and large-scale atomistic dynamics, facilitating accelerated MD and material discovery.",
            "comparison_to_alternatives": "Compared to invariant (scalar-only) GNNs like SchNet: equivariant models perform substantially better on vector/tensor prediction tasks.",
            "success_factors": "Incorporation of symmetry constraints (equivariance), use of local interactions, and message-passing design aligned with physics are key contributors to success.",
            "key_insight": "Enforcing Euclidean equivariance in GNNs aligns architectural inductive bias with physical symmetries, yielding substantial gains in data efficiency and predictive accuracy for atomistic simulations.",
            "uuid": "e2301.5"
        },
        {
            "name_short": "Fourier Neural Operator",
            "name_full": "Fourier Neural Operator (FNO)",
            "brief_description": "A neural operator approach that learns mappings between function spaces (solutions of PDEs) by applying convolutions in Fourier space, enabling fast approximation of parametric PDE solution operators on regular grids.",
            "citation_title": "Fourier neural operator for parametric partial differential equations",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Parametric PDE solution and surrogate modeling (fluid dynamics, continuum mechanics) on regular grids",
            "problem_description": "Learn a mapping from PDE parameters/boundary conditions to solution fields efficiently across parameter space, enabling fast inference for many-query or real-time applications.",
            "data_availability": "Moderate-to-abundant for simulated datasets on regular grids; requires labeled pairs of parameter → solution from classical solvers.",
            "data_structure": "Regular-grid spatial data (images/volumetric grids), continuous fields amenable to spectral transforms.",
            "problem_complexity": "High for nonlinear PDEs and multi-scale phenomena; but spectral representation can reduce complexity for smooth problems.",
            "domain_maturity": "Relatively recent but rapidly adopted for regular-grid PDE surrogate tasks.",
            "mechanistic_understanding_requirements": "Medium — yields accurate operators but may lack fine-grained interpretability; useful for fast surrogate modeling where fidelity matters.",
            "ai_methodology_name": "Neural operator using Fourier transforms (spectral convolutions)",
            "ai_methodology_description": "Applies learned linear transforms in Fourier space (global spectral convolution) combined with local nonlinearities to map input fields/parameters to output solution fields; highly efficient on regular, grid-based data.",
            "ai_methodology_category": "Supervised learning / operator learning / spectral deep learning",
            "applicability": "Well-suited for PDEs on regular grids and periodic or smoothly varying domains; less suited to highly irregular/unstructured domains where GNNs excel.",
            "effectiveness_quantitative": "No specific numeric metrics provided in the survey; original FNO papers report strong performance and generalization across parameter regimes (not enumerated here).",
            "effectiveness_qualitative": "Very effective on regular-grid PDE problems and for parametric generalization; complementary to GNNs which handle unstructured domains.",
            "impact_potential": "High for rapid surrogate PDE solvers on grids, inverse problems, and parameter studies where many evaluations are needed.",
            "comparison_to_alternatives": "Compared to GNNs and stencil-based methods: FNOs excel on regular grids using spectral efficiency, while GNNs handle unstructured meshes and local interactions better.",
            "success_factors": "Exploiting global spectral structure of PDE solutions and using learned spectral filters enable strong operator approximation and generalization.",
            "key_insight": "Spectral neural operators like FNO are powerful for grid-based PDE surrogate modeling but complement rather than replace graph-based learned solvers that target unstructured domains.",
            "uuid": "e2301.6"
        },
        {
            "name_short": "Noise augmentation (additive Gaussian noise)",
            "name_full": "Additive Gaussian Noise Data Augmentation",
            "brief_description": "A training technique that corrupts inputs with Gaussian noise to regularize learned simulators and improve long-horizon rollout stability by forcing robustness to accumulating errors.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Training stability for learned physical simulators (GNS, MeshGraphNets)",
            "problem_description": "Mitigate distribution shift and error accumulation during long rollouts of learned simulators by making the network robust to noisy inputs.",
            "data_availability": null,
            "data_structure": "Applied to graph-structured time-series inputs (particle/node states); noise added to node/edge features during training.",
            "problem_complexity": "Technique-level — interacts with long-horizon nonlinearity of learned dynamics and stability of integrators.",
            "domain_maturity": "Common ML technique applied to physics-informed learning; survey notes it is widely used in the referenced learned simulator literature.",
            "mechanistic_understanding_requirements": "Low-to-medium — improves empirical robustness but introduces hyperparameters (noise scale) requiring tuning; physical interpretation possible via mesoscopic particle models.",
            "ai_methodology_name": "Supervised training with additive Gaussian input noise",
            "ai_methodology_description": "During training, inputs (node/edge features or past states) are corrupted with Gaussian noise of tuned scale to regularize and emulate the effect of accumulated errors; sometimes tied to ideas from mesoscopic models (Brownian noise) to set the noise scale.",
            "ai_methodology_category": "Supervised regularization / data augmentation",
            "applicability": "Applicable to learned simulators to extend stable rollout horizons; requires tuning of noise scale hyperparameters and may not substitute for structural modeling of underlying physics.",
            "effectiveness_quantitative": "No numeric metrics in survey; qualitative reports indicate it permits much longer rollouts in referenced works (Sanchez-Gonzalez et al. 2020; Pfaff et al. 2021).",
            "effectiveness_qualitative": "Effective empirical regularizer that increases long-horizon stability, but performance is sensitive to chosen noise scale and introduces extra hyperparameters.",
            "impact_potential": "Moderate to high as a practical training trick to stabilize learned simulators; connecting noise scale to physical mesoscopic considerations could improve principled tuning.",
            "comparison_to_alternatives": "Compared to multi-step loss training: results vary across works — some report multi-step loss outperforms noise, others report the opposite; no universal winner.",
            "success_factors": "Proper tuning of noise scale and grounding noise magnitude in physical reasoning (e.g., Fluctuation–Dissipation, particle mass scaling) improve success.",
            "key_insight": "Input noise injection is a practical regularizer that improves rollout stability of learned simulators, but its effectiveness depends on appropriately chosen scales and can be better informed by physical mesoscopic models.",
            "uuid": "e2301.7"
        },
        {
            "name_short": "Multi-step loss",
            "name_full": "Multi-step Loss / Rollout Supervision",
            "brief_description": "A training strategy where the model is unrolled for multiple steps during training and the loss is accumulated over the trajectory to encourage the model to correct its own future errors.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Training stability and long-horizon prediction for learned simulators (GNN-based simulators, Neural PDEs)",
            "problem_description": "Reduce distribution shift and error accumulation in long rollouts by training the model to minimize multi-step prediction error rather than single-step error alone.",
            "data_availability": null,
            "data_structure": "Temporal graph sequences; training generates short trajectories from model predictions and compares to ground truth over multiple future steps.",
            "problem_complexity": "Technique-level complexity related to nonlinearity and stability over multiple time steps; increases computational cost during training.",
            "domain_maturity": "Established technique in sequential modeling and recently applied to physics simulators; evidence for effectiveness is mixed across papers.",
            "mechanistic_understanding_requirements": "Low-to-medium — aims at empirical robustness rather than mechanistic interpretability; may implicitly correct nonphysical behavior but does not enforce physics constraints explicitly.",
            "ai_methodology_name": "Multi-step rollout supervised training",
            "ai_methodology_description": "During training, the learned model is used to iteratively predict multiple future states; the loss is summed over these predicted future steps (and may include teacher forcing initially); this forces the network to account for its own prediction errors.",
            "ai_methodology_category": "Supervised learning / sequence training / curriculum learning",
            "applicability": "Applicable to iterative simulators to improve long-horizon accuracy; increases training cost and sometimes less effective than noise augmentation depending on task and implementation.",
            "effectiveness_quantitative": "No unified quantitative metrics in survey; referenced works report mixed empirical outcomes (Brandstetter et al. 2022 reported better performance than noise in some cases; Sanchez-Gonzalez et al. 2020 reported opposite).",
            "effectiveness_qualitative": "Can improve stability by forcing future-correction behavior, but mixed evidence suggests task-dependent effectiveness; may serve similar role as adaptive integrators in classical solvers but does not change timestep.",
            "impact_potential": "Moderate — a useful training strategy for improving rollout stability, especially when combined with other physical biases.",
            "comparison_to_alternatives": "Compared directly to noise injection: mixed results across studies; conceptually related to adaptive integrator ideas from classical ODE solvers.",
            "success_factors": "Proper rollout length, curriculum scheduling, and combination with other biases (equivariance, integrator order) influence success.",
            "key_insight": "Training with multi-step losses can improve long-horizon simulator stability by making the model correct its own future errors, but its relative effectiveness compared to noise augmentation is task-dependent and not conclusive.",
            "uuid": "e2301.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to simulate complex physics with graph networks",
            "rating": 2,
            "sanitized_title": "learning_to_simulate_complex_physics_with_graph_networks"
        },
        {
            "paper_title": "Learning mesh-based simulation with graph networks",
            "rating": 2,
            "sanitized_title": "learning_meshbased_simulation_with_graph_networks"
        },
        {
            "paper_title": "Neural ordinary differential equations",
            "rating": 2,
            "sanitized_title": "neural_ordinary_differential_equations"
        },
        {
            "paper_title": "E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials",
            "rating": 2,
            "sanitized_title": "e_3equivariant_graph_neural_networks_for_dataefficient_and_accurate_interatomic_potentials"
        },
        {
            "paper_title": "Fourier neural operator for parametric partial differential equations",
            "rating": 2,
            "sanitized_title": "fourier_neural_operator_for_parametric_partial_differential_equations"
        },
        {
            "paper_title": "Message passing neural PDE solvers",
            "rating": 2,
            "sanitized_title": "message_passing_neural_pde_solvers"
        },
        {
            "paper_title": "Simulating liquids with graph networks",
            "rating": 1,
            "sanitized_title": "simulating_liquids_with_graph_networks"
        },
        {
            "paper_title": "Machine learning-accelerated computational fluid dynamics",
            "rating": 1,
            "sanitized_title": "machine_learningaccelerated_computational_fluid_dynamics"
        }
    ],
    "cost": 0.018470499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On the Relationships between Graph Neural Networks for the Simulation of Physical Systems and Classical Numerical Methods</p>
<p>Artur Toshev 
Ludger Paehler 
Andrea Panizza 
Nikolaus Adams 
On the Relationships between Graph Neural Networks for the Simulation of Physical Systems and Classical Numerical Methods</p>
<p>Recent developments in Machine Learning approaches for modelling physical systems have begun to mirror the past development of numerical methods in the computational sciences. In this survey we begin by providing an example of this with the parallels between the development trajectories of graph neural network acceleration for physical simulations and particle-based approaches. We then give an overview of simulation approaches, which have not yet found their way into state-ofthe-art Machine Learning methods and hold the potential to make Machine Learning approaches more accurate and more efficient. We conclude by presenting an outlook on the potential of these approaches for making Machine Learning models for science more efficient.</p>
<p>Introduction</p>
<p>Recent years have seen an ever-larger push towards the application of Machine Learning to problems from the physical sciences such as Molecular Dynamics (Musaelian et al., 2022b), coarse-graining , the time-evolution of incompressible fluid flows , learning governing equations from data (Brunton et al., 2016;Cranmer et al., 2020), large-scale transformer models for chemistry (Frey et al., 2022), and the acceleration of numerical simulations with machine learning techniques (Kochkov et al., 2021). All of these algorithms build on the infrastructure underpinning modern Machine Learning in combing state-of-the-art approaches with a deep understanding of the physical problems at hand. This begs the questions if there exist more insights and tricks hidden in existing, classical approaches in the physical sciences which have the potential to maybe not only make the algorithm for the particular problem class more efficient, but maybe even  </p>
<p>Machine Learning in general?</p>
<p>Inspired by recent theoretical advances in the algorithmic alignment between Graph Neural Networks (GNNs) and dynamic programming Veličković et al., 2020), we surmise that the extension of this analysis to classical PDE solvers, and the physical considerations they incorporate, enables us to learn from the development trajectory in the physical sciences to inform the development of new algorithms. In this workshop paper we make the following contributions towards this goal:</p>
<p>• A comparison of the development of graph-based learned solvers, and the proximity of their development ideas to the development of Smoothed Particle Hydrodynamics starting from Molecular Dynamics in the physical sciences.</p>
<p>• An analysis of classical numerical solvers, and their algorithmic features to inform new ideas for new algorithms.</p>
<p>MeshGraphNets and its relation to classical methods</p>
<p>An excellent example of the parallels between the development of Machine Learning methods for the sciences and arXiv:2304.00146v1 [cs.LG] 31 Mar 2023 the development of classical approaches is the recent development of graph-based simulators. When we relate their inherent assumptions and techniques to the development of particle-based methods, starting with Molecular Dynamics, a great many parallels arise. For an impression of the scales the classical methods operate on, and where graph-based simulators are placed in relation, please refer to Figure 1.</p>
<p>In this section, we analyze the structure of two of the first mature learned solvers (GNS , MeshGraphNets (Pfaff et al., 2021)) and how these two approaches align with three of the classical methods (MD, FPM, SPH). We select these learned algorithms because they were one of the first of their kind to show promising results on real world data. Also, GNS is trained directly on SPH data which further motivates an algorithmic comparison.</p>
<p>Graph Neural Network-based Approaches to Simulation</p>
<p>The Graph Network (GN) (Battaglia et al., 2018) is a framework that generalizes graph-based learning and specifically the Graph Neural Network (GNN) architecture by Scarselli et al. (2008). However, in this work, we use the terms GN and GNN interchangeably. Adopting the Graph Network formulation, the main design choices are the choice of update-function, and aggregation-function. For physicsinformed modeling this gives us the ability to blur the line between classical methods and graph-based methods by including biases similar to CNNs for non-regular grids, as well as encoding physical laws into our network structure with the help of spatial equivariance/invariance, local interactions, the superposition principle, and differential equations. E.g. translational equivariance can easily be incorporated using relative positions between neighboring nodes, or the superposition principle can be encoded in graphs by using the summation aggregation over the representation of forces as edge features.</p>
<p>Viewing MeshGraphNets (Pfaff et al., 2021) from a physicsmotivated perspective, we argue that MeshGraphNets originate from Molecular Dynamics. To present this argument in all its clarity, we have to begin with its predecessor: the Graph Network-based Simulators (GNS) (Sanchez-Gonzalez et al., 2020).</p>
<p>GRAPH NETWORK-BASED SIMULATORS</p>
<p>The Graph Networks-based Simulator builds on the encoderprocessor-decoder approach, where Graph Networks are applied iteratively on the encoded space. Proving GNS' ability to simulate systems with up to 85k particles, their approach can be summarized as follows.</p>
<p>Let X t denote the states of a particle system at time t. X might contain the position, velocity, type of particle, or any other physical information specific to a material particle. A set of k + 1 subsequent past states X t 0:K = X t0 , X t1 , . . . , X t k if given to the network. The core task is to then learn the differential operator d θ , which approximates the dynamics
d θ : X t k −→ Y t k , X t k+1 = Update X t k , d θ .
Here, Y t is the acceleration, which is used to obtain the next state X t+1 via integration using a deterministic "Update" routine, e.g. semi-implicit Euler scheme. The differential operator d θ is learned with the encoder-processor-decoder approach where the encoder takes in 1 to 10 previous states, and encodes them into a graph. This graph consists of nodes -latent representation of the states X -and edges -between each pair of particles closer than some cut-off radius there is another latent vector, which initially contains the distance or displacement information. The processor is then a multilayer Graph Network of which the exact number of message-passing Graph Networks is a hyperparameter. The result on the graph-space is then decoded back to physical space. The loss is computed as the mean-squared error between the learned acceleration, and the target acceleration. While the approach showed promising results for fluid simulations, and fluid-solid interactions, it struggled on deforming meshes, such as thin shells.</p>
<p>MESHGRAPHNETS</p>
<p>To better represent meshes MeshGraphNets (Pfaff et al., 2021) supplemented the Graph Network simulation with an additional set of edges to define a mesh, on which interactions can be learned. Closely related to the superposition principle in physics, the principle of splitting a complicated function into the sum of multiple simpler ones, the interaction function is split into the interaction of mesh-type edges and collision-type edges.</p>
<p>Following the widespread use of remeshing in engineering, MeshGraphNets have the ability to adaptively remesh to model a wider spectrum of dynamics. Mesh deformation without adaptive remeshing would lead to the loss of high frequency information.</p>
<p>The last major improvement of MeshGraphNets over GNS is extending the output vector Y with additional components to also predict further targets, such as the stress field.</p>
<p>In difference to the Graph Network-based Simulators, the input here includes a predefined mesh and the output is extended to contain dynamical features like pressure. 
E M E W V e 0M ij e 0W ij q t i q t+1 i pi pi M t M t+1 M t+2
Obstacle mesh nodes G Figure 2. Illustration of the MeshGraphNets scheme with a decomposition of its algorithm into the encoder, processor, and decoder (Image source: Pfaff et al. (2021)).</p>
<p>Similarities between the Development Trajectories of Particle-based Methods and Graph Neural Network-based Approaches to Simulations</p>
<p>Beginning with Molecular Dynamics, the earliest and most fundamental particle-based method, we will now outline the similarities between the development trajectories, and the derivations inherent to them, of MeshGraphNets and the development of particle-based methods in physics.</p>
<p>SIMILARITIES TO MOLECULAR DYNAMICS</p>
<p>Molecular Dynamics is a widely used simulation method which generates the trajectories of an N-body atomic system. For the sake of intellectual clarity we restrict ourselves to its simplest form, the unconstrained Hamiltonian mechanics description.</p>
<p>The construction of connections, and edges is one of the clearest similarities between Molecular Dynamics and MeshGraphNets. Both can potentially have a mesh as an input, and both compute the interactions based on spatial distances up to a fixed threshold. Iterative updates, or the repeated application of Graph Network layers in the Mesh-GraphNets, extend the effective interaction radius beyond the immediate neighbourhood of a particle such that all particles can be interacted with. Both approaches are at the same time translationally invariant w.r.t. accelerations, and permutation equivariant w.r.t. the particles, and use a symplectic time-integrator. While there are theoretical reasons for this choice in Molecular Dynamics, it is choice of convenience in the context of learned approaches. The main difference between the two approaches lies in the computation of the accelerations. In Molecular Dynamics the derivative of a predefined potential function is evaluated, whereas a learned model is used in the Graph Network-based Simulators.</p>
<p>SIMILARITIES TO SMOOTHED PARTICLE HYDRODYNAMICS</p>
<p>A closer relative to the Graph Network-based Simulators is the Smoothed Particle Hydrodynamics algorithm originating from astrophysics (Lucy, 1977;Gingold &amp; Monaghan, 1977). Smoothed Particle Hydrodynamics discretizes the governing equations of fluid dynamics, the Navier-Stokes equations, with kernels such that the discrete particles follow Newtonian mechanics with the equivalent of a prescribed molecular potential. Both, Smoothed Particle Hydrodynamics, and Graph Network-based Simulators obey the continuum assumption, whereas Molecular Dynamics presumes a discrete particle distribution, and is constrained to extremely short time intervals.</p>
<p>THE DIFFERENCES</p>
<p>Summarizing the key differences between the closely related approaches, Molecular Dynamics and Smoothed Particle Hydrodynamics both take one past state X t as an input, whereas Graph-based approaches require a history of k states X t 0:K . Molecular Dynamics encodes geometric relations in the potential, MeshGraphNets encode the geometry in the mesh, while there exists no direct way for inclusion in the other two approaches. Molecular Dynamics, and Smoothed Particle Hydrodynamics explicitly encode physical laws, for learned methods all these parameters and relations have to be learned from data.</p>
<p>A key advancement of MeshGraphNets, coming from the Graph Network-based Simulators, is the explicit superimposition of solutions on both sets of edges, which far outperforms the implicit distinction of interactions. This approach is equally applicable to all conventional particle-, and mesh-based simulations in engineering. Borrowing the Fluid Particle Model from fluid mechanics, we can subsequently connect the classical methods with the learned approaches by viewing meshes and particles as the same entity under the fluid-particle paradigm.</p>
<p>CONNECTING MESHGRAPHNETS TO GRAPH NEURAL NETWORK-BASED SIMULATIONS WITH THE FLUID PARTICLE MODEL</p>
<p>The Fluid Particle Model (Espanol, 1998) is a mesoscopic Newtonian model, as seen in Figure 1, situated on an intermediate scale between the microscopic Molecular Dynamics and the macroscopic Smoothed Particle Hydrodynamics. It views particles from the point of view of a Voronoi tesselation of the molecular fluid, see Figure 3. The Voronoi tesselation coarse-grains the atomistic system to a pseudoparticle system with ensembles of atoms in thermal equilibrium summarized as pseudoparticles. This pseudoparticle construction is closely related to the MeshGraphNets construction, where each mesh node also corresponds to  (Hoogerbrugge &amp; Koelman, 1992) also both operate on pseudoparticles. All of these approaches share that they have to presume a large enough number of atoms per pseudoparticles to be viewed as a thermodynamic system.</p>
<p>Especially in Dissipative Particle Dynamics one injects</p>
<p>Gaussian noise to approximate a physical system, just as is done for Graph Network-based Simulators and Mesh-GraphNets to stabilize the training. We surmise that this injection of noise into graph-based simulators amounts to forcing the learned model to predict the true output despite the noisy inputs, hence leading the model to converge to the central limit of the estimated conditional distribution of the acceleration.</p>
<p>The construction of Voronoi tesselations governs that the size of the cells is to be inversely proportional to variations in their properties, hence leading to more sampling in regions with high property variation. The very same argument based on the curvature as a heuristic is being used to derive the mesh refinement of the MeshGraphNets algorithm.</p>
<p>Relation to Numerical Schemes</p>
<p>After the recent success of Neural ODEs solvers (Chen et al., 2018), it has taken almost four years to start considering Neural PDEs in general (Brandstetter et al., 2022 (Hoogerbrugge &amp; Koelman, 1992), Volume of Fluid Method (VOF) (Hirt &amp; Nichols, 1981), Particle-in-Cell (PIC) (Brackbill &amp; Ruppel, 1986), Material Point Method (MPM) (Sulsky et al., 1993), Discrete Element Method (DEM) (Cundall &amp; Strack, 1979), and Meshless FEM (MFEM). Finally, there are also approaches to solving PDEs without any discretization as in Sawhney et al. (2022). Each of these methods works best for a specific type of PDE, boundary/initial conditions, and parameter range. In this section we compare concepts from these classical methods to state-of-the-art learned algorithms.</p>
<p>Data augmentation with white noise</p>
<p>Two popular papers corrupting training inputs with additive Gaussian noise include Sanchez-Gonzalez et al. (2020); Pfaff et al. (2021), as described before. The goal of this approach is to force the model to deal with accumulating noise leading to a distribution shift during longer rollouts. Thus, the noise acts as an effective regularization technique, which in these two papers allows for much longer trajectories than seen during training. However, one major issue with this approach is that the scale of the noise is represented by two new hyperparameters, which have to be tuned manually (Pfaff et al. (2021), Appendix 2.2).</p>
<p>A perspective on noise injection coming from the physical sciences is to see it through the lens of mesoscopic particle methods like the Fluid Particle Model and Dissipative Particle Dynamics, in which the noise originates from the Brownian motion at small scales. Although GNS and Mesh-GraphNets operate on scales too large for the relevance of Brownian motion, the Fluid Particle Model provides a principled way of relating particle size and noise scale. The underlying considerations from statistical mechanics might aid to a better understanding of the influence of training noise and in turn make approaches based on it more efficient.</p>
<p>Data augmentation by multi-step loss</p>
<p>Another way of dealing with the distribution shift is by training a model to correct its own mistakes via some form of a multi-step loss, i.e. during training a short trajectory is generated and the loss is summed over one or multiple past steps (Tompson et al., 2017;Um et al., 2020;Ummenhofer et al., 2020;Brandstetter et al., 2022). The results on this vary with some researchers reporting better performance than with noise injection (Brandstetter et al., 2022), while others report the opposite experience (Sanchez-Gonzalez et al., 2020).</p>
<p>Looking at classical solvers for something related to the multi-step loss, it is natural to think of adaptive time integrators used by default in ODE routines like ODE45 in Matlab (Dormand &amp; Prince, 1980). Adaptive integrators work by generating two short trajectories of the same time length, but with different step sizes, and as long as the outcome with larger steps differs within some bounds, then the step size is increased. This guarantees some level of long-term rollout stability just as attempted with the multi-step loss, but the multi-step loss forces the network to implicitly correct for future deviations of the trajectory without actually changing the step size. The adaptive step-size idea has gained popularity in ML with the introduction of Neural ODEs (Chen et al., 2018).</p>
<p>Equivariance bias</p>
<p>Numerical PDE solvers come in two flavors: stencil-based and kernel-based, both of which are equivariant to translation, rotation, and reflection in space (Euclidean group equivariance), as well as translation in time (by Noether's theorem). These properties arise from the conservation of energy, which is a fundamental principle in physics. While equivariance, with respect to the Euclidean group, has been around for a couple of years on grids (Weiler et al., 2018), its extension to the grid-free (Lagrangian) setting is gaining popularity just recently (Brandstetter et al., 2021;Schütt et al., 2021;Batzner et al., 2022;Musaelian et al., 2022a).</p>
<p>Here, we talk about equivariance in terms of a neural net operation on vectors, which rotates the output exactly the same way as the input is rotated, as opposed to working with scalar values, which is called an invariant operation, e.g. SchNet (Schütt et al., 2017). The performance boost by including equivariant features is significant and reaches up to an order of magnitude compared to invariant methods (Batzner et al., 2022).</p>
<p>Input multiple past steps</p>
<p>Another common performance improvement in neural net training is observed by stacking multiple past states as an input Pfaff et al., 2021;Brandstetter et al., 2022). One argument supporting this approach is overfitting prevention by inputting more data (Pfaff et al., 2021). Looking at conventional solvers we very rarely see multiple past states as input and this is done for materials with memory property, e.g. rheological fluids or "smart" materials. Thus, providing multiple past states implicitly assumes that there is some nonphysical non-Markovian retardation process, which in most cases does not correspond to the physics used for training data generated.</p>
<p>The only physical justification of a multi-step input we are aware of arises if we train the model to learn a coarsegrained representation of the system. Li et al. (2015) showed that explicit memory effects are necessary in Dissipative Particle Dynamics for the correct coarse-graining of a complex dynamical system using the Mori-Zwanzig formalism. Given that papers like GNS and MeshGraphNets do not make use of coarse-graining, it is questionable why we observe improvement in performance and whether this trick generalizes well to different settings.</p>
<p>Spatial multi-scale modeling</p>
<p>Conventional multi-scale methods include, among others, all types of coarse-graining, Wavelet-based methods (e.g. Kim et al. (2008)), and the Fast Multipole Method (Rokhlin, 1985). Graph Networks seem especially suitable for tasks like coarse-graining as they are designed to work on unstructured domains, opposed for example to approaches using Wavelet or Fourier transforms, which require regular grids. GNNs seem especially promising with many applications in Molecular Dynamics (Husic et al., 2020) and engineering (Lino et al., 2021;Valencia et al., 2022;Migus et al., 2022;Han et al., 2022). It is particularly interesting to see works like Migus et al. (2022) inspired by multi-resolution methods and Valencia et al. (2022) resembling geometric coarse-graining by weighted averaging. All these methods rely on the fact that physical systems exhibit multi-scale behavior, meaning that the trajectory of a particle depends on its closest neighbors, but also on more far-reaching weaker forces. Splitting the scales and combining their contributions can greatly reduce computation. One of the great advantages of GNNs is their capability to operate on irregularly spaced data, which is necessary for most coarse-graining approaches.</p>
<p>Locality of interactions</p>
<p>In most cases, graph-based approaches to solving PDEs define the edges in the graph, based on an interaction radius. Methods using the Graph Network architecture (Battaglia et al., 2018) effectively expand the receptive field of each node with every further layer, in the extreme case resulting in the phenomenon known as over-smoothing. But if we keep the number of layers reasonably low, the receptive field will always be larger compared to a conventional simulation with the same radius. Until recently, it was thought that a large receptive field is the reason for the success of learned simulators, but Musaelian et al. (2022a) question that assumption. In this paper, an equivariant graph network with fixed interaction neighbors performs on a par with the very similar Graph Network-based method NequIP (Batzner et al., 2022) on molecular property prediction tasks. This finding supports the physics-based argument about the locality of interactions.</p>
<p>Mesh vs Particle</p>
<p>GNN-based simulation approaches offer the flexibility to combine particles and meshes out-of-the-box. If we then train one neural network to reproduce the results of a Finite Element solution on a mesh and Smoothed Particle Hydrodynamics solution over particles, this is where learned methods really shine. This was achieved with the Mesh-GraphNets framework (Pfaff et al., 2021). We argue that the transition from particles to meshes is a direct result of a coarse-graining procedure using Voronoi tessellation, which is related to the derivation of the Fluid Particle Model. The main assumption in this derivation is that each mesh cell should be small enough that it can be treated as being in equilibrium -similar to the assumption made when discretizing a domain with points.</p>
<p>Stencils</p>
<p>We talk about stencils when operating on regular grids. Although this is not the main strength of GNNs, there are some useful concepts from stencil-based simulations, which are conventionally nontrivial to generalize to particles, but can easily be adapted with GNNs. Brandstetter et al. (2022) state that their paper is motivated by the observation that the Weighted Essentially Non-Oscillatory scheme (WENO) (Shu, 1998) can be written as a special case of a GNN. Another work, inspired by the general idea of the Finite Volume Method, looking at the fluxes at the left and right cell boundary, was developed by Praditia et al. (2021). Inspired by the Finite Element Method, finite element networks were introduced by weighting the contributions of neighbouring cells by their volume, as is done in Finite Element analysis (Lienen &amp; Günnemann, 2022).</p>
<p>Integration schemes</p>
<p>In addition to the time-step adaptation mentioned in relation to multi-step losses, another topic investigated in literature is the order of the integrator (Sanchez-Gonzalez et al., 2019). This work points to the fact that higher order integrators lead to much better robustness, with respect to the choice of an integration time step. Another interesting question discussed in this paper is whether symplectic integrators improve performance of a learned Hamiltonian neural net.</p>
<p>The answer seems to be that the symplectic property is much less important than the order of the integrator, which is in contrast with conventional Molecular Dynamics integrators, which work extremely poorly if not symplectic.</p>
<p>Untapped Ideas from Classical Approaches</p>
<p>In this subsection, we introduce potentially useful ideas from conventional differential equation solvers in science, which to the best of our knowledge have not been adapted in main-stream learned PDE solvers yet. Figure 4 is a collection of these concepts in the form of a word cloud.</p>
<p>Noise during inference</p>
<p>Adding noise to the inputs during training has proven to be useful, but has not been done during testing. One idea would be to use noise during inference to emulate Brownian motion. And one further topic we already mentioned is the relation of the noise scale to particle mass. From mesoscopic methods and the Fluctuation-dissipation theorem we would expect the noise to scale as 1/ √ m if a coarser representation is used.</p>
<p>Multiple time steps</p>
<p>Learned Molecular Dynamics simulations stick to using only the last past state and doing the same for larger-scale simulations might partially explain the unphysical behavior of the GNS method demonstrated in Klimesch et al. (2022). For coarse-graining though a longer history might be helpful.</p>
<p>Feature Engineering</p>
<p>From the Volume of Fluid Method we could adapt the idea of including features corresponding to the ratio of different material, if we are interested in simulating multi-material flows. The Discrete Element Method suggests encoding much more features like rotational degree of freedom (in magnetic field or simulating friction), stateful contact information (contact simulations), and often complicated geometry (for non-spherical, e.g. granular particles). Inspired by shock-capturing methods used routinely for the solution of nonlinear fluid dynamics problems (Ketcheson et al., 2020), one could think of further hand-tuned node features indicating the presence of a shock.</p>
<p>Particles and Grid</p>
<p>There are a number of methods using the best of both particle and grid worlds like the Particle-in-Cell method and its successor Material Point Method. The idea of updating the node features and from time to time also based on the grid cell they belong to, might speed up simulations and is worth exploring. Now, if we restrict ourselves to regularly spaced particles, respectively grid cells, our solver toolkit becomes much richer with methods like the Fast Fourier Transform (which has already seen great success with the Fourier Neural Operator ) and the Wavelet Transform (as used in the PDE-Net (Long et al., 2018)) at our disposal, as mentioned above in the context of multi-scale modeling.</p>
<p>Integrator</p>
<p>Taking the perspective of Neural ODEs (Chen et al., 2018) with the neural network learning the perfect acceleration, one could arguably expect the next evolutionary step to be the combination of learned integrators with adaptive integration schemes. Incorporating insights from classical numerical methods, one should possibly seek to define an equivalent stability criterion for learned methods as the Courant-Friedrichs-Lewy (CFL) condition for classical numerical methods. This would in turn aid in bounding the time, and subsequently explore time steps smaller than the critical value.</p>
<p>Conclusion &amp; Discussion</p>
<p>In this article, we claim that studying classical PDE solvers and their past development offers a direct path to the acceleration of the development of learned PDE solvers. Examples in literature show that biasing a learned solver by means of architectural design, data augmentation, feature engineering, etc. incorporating existing knowledge from classical solvers can greatly improve performance, explainability, and data-efficiency.</p>
<p>In Section 2 we show how this development has already subconsciously played out in the development of graph-based learned solvers following the same development as particlebased methods such as Molecular Dynamics, Smoothed Particle Hydrodynamics, and the Fluid-Particle Model. This investigation is revisited for algorithmic comparisons and illustrations of the limitations of classical solvers later on. In Section 3 we then focus on ideas from classical approaches which have found their way into recent learned solver literature, and discuss the physical interpretation of these developments. In the discussed examples, the included physically motivated biases are used to improve robustness w.r.t. hyperparameter choices, lower errors, and speed-up inference.</p>
<p>Section 4 takes a glimpse into a possible version of the future with ideas which have, to the best of our knowledge, not yet been integrated in learned methods. Given the elaborate history of classical methods, and the short, but highly dynamic history of learned approaches, there is still a lot of potential to be realized within the latter by incorporating insights from the former.</p>
<p>Going further, many exciting problems in the physical sciences, such as simulations involving multiple spatial scales, multiple temporal scales, non-Newtonian fluids, or phasechanging materials, are heavily data-constrained and will hence have to rely on insights from classical methods for Machine Learning approaches to become feasible.</p>
<p>2nd AI4Science Workshop at the 39 th International Conference on Machine Learning (ICML), 2022. Copyright 2022 by the author(s).</p>
<p>Figure 1 .
1Characterization of the physical scales the example methods of section 2 operate on. The Graph Network-based approaches MeshGraphNets, and Graph Network-based Simulators are placed in relation to their classical counterparts.</p>
<p>Figure 3 .
3Single points (left), Delaunay triangulation (middle), and Voronoi diagram (right)(Image source:Rokicki &amp; Gawell (2016) the cell center of a simulated pseudoparticle. Smoothed Particle Hydrodynamics as well as Dissipative Particle Dynamics</p>
<p>Figure 4 .
4Overview of the currently under-utilized ideas discussed in Section 4 for Machine Learning approaches for the physical sciences.
Department of Mechanical Engineering, Technical University of Munich, Munich, Germany 2 ML Collective. Correspondence to: Artur Toshev <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#114;&#116;&#117;&#114;&#46;&#116;&#111;&#115;&#104;&#101;&#118;&#64;&#116;&#117;&#109;&#46;&#100;&#101;">&#97;&#114;&#116;&#117;&#114;&#46;&#116;&#111;&#115;&#104;&#101;&#118;&#64;&#116;&#117;&#109;&#46;&#100;&#101;</a>.</p>
<p>P W Battaglia, J B Hamrick, V Bapst, A Sanchez-Gonzalez, V Zambaldi, M Malinowski, A Tacchetti, D Raposo, A Santoro, R Faulkner, arXiv:1806.01261Relational inductive biases, deep learning, and graph networks. arXiv preprintBattaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez- Gonzalez, A., Zambaldi, V., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner, R., et al. Rela- tional inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.</p>
<p>E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. S Batzner, A Musaelian, L Sun, M Geiger, J P Mailoa, M Kornbluth, N Molinari, T E Smidt, B Kozinsky, Nature communications. 131Batzner, S., Musaelian, A., Sun, L., Geiger, M., Mailoa, J. P., Kornbluth, M., Molinari, N., Smidt, T. E., and Kozinsky, B. E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nature communications, 13(1):1-11, 2022.</p>
<p>Flip: A method for adaptively zoned, particle-in-cell calculations of fluid flows in two dimensions. J U Brackbill, H M Ruppel, Journal of Computational physics. 652Brackbill, J. U. and Ruppel, H. M. Flip: A method for adap- tively zoned, particle-in-cell calculations of fluid flows in two dimensions. Journal of Computational physics, 65 (2):314-343, 1986.</p>
<p>Geometric and physical quantities improve e (3) equivariant message passing. J Brandstetter, R Hesselink, E Van Der Pol, E Bekkers, M Welling, arXiv:2110.02905arXiv preprintBrandstetter, J., Hesselink, R., van der Pol, E., Bekkers, E., and Welling, M. Geometric and physical quantities improve e (3) equivariant message passing. arXiv preprint arXiv:2110.02905, 2021.</p>
<p>Message passing neural PDE solvers. J Brandstetter, D E Worrall, M Welling, International Conference on Learning Representations. Brandstetter, J., Worrall, D. E., and Welling, M. Message passing neural PDE solvers. In International Conference on Learning Representations, 2022.</p>
<p>Discovering governing equations from data by sparse identification of nonlinear dynamical systems. S L Brunton, J L Proctor, J N Kutz, Proceedings of the national academy of sciences. the national academy of sciences113Brunton, S. L., Proctor, J. L., and Kutz, J. N. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences, 113(15):3932-3937, 2016.</p>
<p>Neural ordinary differential equations. R T Q Chen, Y Rubanova, J Bettencourt, D K Duvenaud, Advances in Neural Information Processing Systems. Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R.31Chen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duve- naud, D. K. Neural ordinary differential equations. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31. Cur- ran Associates, Inc., 2018.</p>
<p>Discovering symbolic models from deep learning with inductive biases. M Cranmer, A Sanchez Gonzalez, P Battaglia, R Xu, K Cranmer, D Spergel, S Ho, Advances in Neural Information Processing Systems. 33Cranmer, M., Sanchez Gonzalez, A., Battaglia, P., Xu, R., Cranmer, K., Spergel, D., and Ho, S. Discovering sym- bolic models from deep learning with inductive biases. Advances in Neural Information Processing Systems, 33: 17429-17442, 2020.</p>
<p>A discrete numerical model for granular assemblies. geotechnique. P A Cundall, O D Strack, 29Cundall, P. A. and Strack, O. D. A discrete numerical model for granular assemblies. geotechnique, 29(1):47- 65, 1979.</p>
<p>A family of embedded runge-kutta formulae. J R Dormand, P J Prince, Journal of computational and applied mathematics. 61Dormand, J. R. and Prince, P. J. A family of embedded runge-kutta formulae. Journal of computational and ap- plied mathematics, 6(1):19-26, 1980.</p>
<p>Fluid particle model. P Espanol, Physical Review E. 5732930Espanol, P. Fluid particle model. Physical Review E, 57(3): 2930, 1998.</p>
<p>Neural scaling of deep chemical models. N Frey, R Soklaski, S Axelrod, S Samsi, R Gomez-Bombarelli, C Coley, V Gadepally, Frey, N., Soklaski, R., Axelrod, S., Samsi, S., Gomez- Bombarelli, R., Coley, C., and Gadepally, V. Neural scaling of deep chemical models. 2022.</p>
<p>Smoothed particle hydrodynamics: theory and application to non-spherical stars. R A Gingold, J J Monaghan, Monthly notices of the royal astronomical society. 1813Gingold, R. A. and Monaghan, J. J. Smoothed particle hydrodynamics: theory and application to non-spherical stars. Monthly notices of the royal astronomical society, 181(3):375-389, 1977.</p>
<p>Predicting physics in mesh-reduced space with temporal attention. X Han, H Gao, T Pfaff, J.-X Wang, L Liu, International Conference on Learning Representations. Han, X., Gao, H., Pfaff, T., Wang, J.-X., and Liu, L. Pre- dicting physics in mesh-reduced space with temporal attention. In International Conference on Learning Rep- resentations, 2022.</p>
<p>Volume of fluid (vof) method for the dynamics of free boundaries. C W Hirt, B D Nichols, Journal of computational physics. 391Hirt, C. W. and Nichols, B. D. Volume of fluid (vof) method for the dynamics of free boundaries. Journal of computa- tional physics, 39(1):201-225, 1981.</p>
<p>Simulating microscopic hydrodynamic phenomena with dissipative particle dynamics. P Hoogerbrugge, J Koelman, Europhysics Letters). 193155EPLHoogerbrugge, P. and Koelman, J. Simulating microscopic hydrodynamic phenomena with dissipative particle dy- namics. EPL (Europhysics Letters), 19(3):155, 1992.</p>
<p>Coarse graining molecular dynamics with graph neural networks. B E Husic, N E Charron, D Lemm, J Wang, A Pérez, M Majewski, A Krämer, Y Chen, S Olsson, G De Fabritiis, The Journal of chemical physics. 15319194101Husic, B. E., Charron, N. E., Lemm, D., Wang, J., Pérez, A., Majewski, M., Krämer, A., Chen, Y., Olsson, S., de Fab- ritiis, G., et al. Coarse graining molecular dynamics with graph neural networks. The Journal of chemical physics, 153(19):194101, 2020.</p>
<p>Riemann Problems and Jupyter Solutions. D I Ketcheson, R J Leveque, M J Razo, 10.1137/1.9781611976212Society for Industrial and Applied Mathematics. 2020Ketcheson, D. I., LeVeque, R. J., and del Razo, M. J. Rie- mann Problems and Jupyter Solutions. Society for Indus- trial and Applied Mathematics, Philadelphia, PA, 2020. doi: 10.1137/1.9781611976212.</p>
<p>Wavelet turbulence for fluid simulation. T Kim, N Thürey, D James, M Gross, ACM Transactions on Graphics (TOG). 273Kim, T., Thürey, N., James, D., and Gross, M. Wavelet turbulence for fluid simulation. ACM Transactions on Graphics (TOG), 27(3):1-6, 2008.</p>
<p>Simulating liquids with graph networks. J Klimesch, P Holl, N Thuerey, arXiv:2203.07895arXiv preprintKlimesch, J., Holl, P., and Thuerey, N. Simulating liquids with graph networks. arXiv preprint arXiv:2203.07895, 2022.</p>
<p>Machine learning-accelerated computational fluid dynamics. D Kochkov, J A Smith, A Alieva, Q Wang, M P Brenner, S Hoyer, Proceedings of the National Academy of Sciences. 118212021Kochkov, D., Smith, J. A., Alieva, A., Wang, Q., Brenner, M. P., and Hoyer, S. Machine learning-accelerated com- putational fluid dynamics. Proceedings of the National Academy of Sciences, 118(21), 2021.</p>
<p>Incorporation of memory effects in coarse-grained modeling via the mori-zwanzig formalism. Z Li, X Bian, X Li, G E Karniadakis, The Journal of chemical physics. 14324243128Li, Z., Bian, X., Li, X., and Karniadakis, G. E. Incorpora- tion of memory effects in coarse-grained modeling via the mori-zwanzig formalism. The Journal of chemical physics, 143(24):243128, 2015.</p>
<p>Fourier neural operator for parametric partial differential equations. Z Li, N Kovachki, K Azizzadenesheli, B Liu, K Bhattacharya, A Stuart, Anandkumar , A , arXiv:2010.08895arXiv preprintLi, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhat- tacharya, K., Stuart, A., and Anandkumar, A. Fourier neural operator for parametric partial differential equa- tions. arXiv preprint arXiv:2010.08895, 2020.</p>
<p>Learning the dynamics of physical systems from sparse observations with finite element networks. M Lienen, S Günnemann, International Conference on Learning Representations (ICLR). 2022Lienen, M. and Günnemann, S. Learning the dynamics of physical systems from sparse observations with finite ele- ment networks. In International Conference on Learning Representations (ICLR), 2022.</p>
<p>Simulating continuum mechanics with multi-scale graph neural networks. M Lino, C Cantwell, A A Bharath, S Fotiadis, arXiv:2106.04900arXiv preprintLino, M., Cantwell, C., Bharath, A. A., and Fotiadis, S. Simulating continuum mechanics with multi-scale graph neural networks. arXiv preprint arXiv:2106.04900, 2021.</p>
<p>Pde-net: Learning pdes from data. Z Long, Y Lu, X Ma, Dong , B , International Conference on Machine Learning. PMLRLong, Z., Lu, Y., Ma, X., and Dong, B. Pde-net: Learning pdes from data. In International Conference on Machine Learning, pp. 3208-3216. PMLR, 2018.</p>
<p>A numerical approach to the testing of the fission hypothesis. L B Lucy, The astronomical journal. 82Lucy, L. B. A numerical approach to the testing of the fission hypothesis. The astronomical journal, 82:1013- 1024, 1977.</p>
<p>Multiscale physical representations for approximating PDE solutions with graph neural operators. L Migus, Y Yin, J A Mazari, ICLR 2022 Workshop on Geometrical and Topological Representation Learning. Migus, L., Yin, Y., Mazari, J. A., and patrick gallinari. Multi- scale physical representations for approximating PDE solutions with graph neural operators. In ICLR 2022 Workshop on Geometrical and Topological Representa- tion Learning, 2022.</p>
<p>Learning local equivariant representations for large-scale atomistic dynamics. A Musaelian, S Batzner, A Johansson, L Sun, C J Owen, M Kornbluth, B Kozinsky, Musaelian, A., Batzner, S., Johansson, A., Sun, L., Owen, C. J., Kornbluth, M., and Kozinsky, B. Learning local equivariant representations for large-scale atomistic dy- namics, 2022a.</p>
<p>Learning local equivariant representations for large-scale atomistic dynamics. A Musaelian, S Batzner, A Johansson, L Sun, C J Owen, M Kornbluth, B Kozinsky, arXiv:2204.05249arXiv preprintMusaelian, A., Batzner, S., Johansson, A., Sun, L., Owen, C. J., Kornbluth, M., and Kozinsky, B. Learning local equivariant representations for large-scale atomistic dy- namics. arXiv preprint arXiv:2204.05249, 2022b.</p>
<p>Learning mesh-based simulation with graph networks. T Pfaff, M Fortunato, A Sanchez-Gonzalez, P Battaglia, International Conference on Learning Representations. Pfaff, T., Fortunato, M., Sanchez-Gonzalez, A., and Battaglia, P. Learning mesh-based simulation with graph networks. In International Conference on Learning Rep- resentations, 2021.</p>
<p>Finite volume neural network : Modeling subsurface contaminant transport. T Praditia, M Karlbauer, S Otte, S Oladyshkin, M V Butz, W Nowak, doi: 10.48550/ arXiv.2104.06010ICLR 2021 : Ninth International Conference on Learning Representations. 2021Cornell UniversityPraditia, T., Karlbauer, M., Otte, S., Oladyshkin, S., Butz, M. V., and Nowak, W. Finite volume neural network : Modeling subsurface contaminant transport. In ICLR 2021 : Ninth International Conference on Learning Rep- resentations. Cornell University, 2021. doi: 10.48550/ arXiv.2104.06010.</p>
<p>Rapid solution of integral equations of classical potential theory. V Rokhlin, Journal of computational physics. 602Rokhlin, V. Rapid solution of integral equations of classical potential theory. Journal of computational physics, 60(2): 187-207, 1985.</p>
<p>Voronoi diagrams-architectural and structural rod structure research model optimization. W Rokicki, E Gawell, MAZOWSZE Studia Regionalne. 19Rokicki, W. and Gawell, E. Voronoi diagrams-architectural and structural rod structure research model optimization. MAZOWSZE Studia Regionalne, (19):155-164, 2016.</p>
<p>Hamiltonian graph networks with ode integrators. A Sanchez-Gonzalez, V Bapst, K Cranmer, P Battaglia, arXiv:1909.12790arXiv preprintSanchez-Gonzalez, A., Bapst, V., Cranmer, K., and Battaglia, P. Hamiltonian graph networks with ode inte- grators. arXiv preprint arXiv:1909.12790, 2019.</p>
<p>Learning to simulate complex physics with graph networks. A Sanchez-Gonzalez, J Godwin, T Pfaff, R Ying, J Leskovec, P Battaglia, International Conference on Machine Learning. PMLRSanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R., Leskovec, J., and Battaglia, P. Learning to simulate com- plex physics with graph networks. In International Con- ference on Machine Learning, pp. 8459-8468. PMLR, 2020.</p>
<p>Gridfree Monte Carlo for PDEs with spatially varying coefficients. R Sawhney, D Seyb, W Jarosz, K Crane, 10.1145/3528223.3530134Proceedings of SIGGRAPH). SIGGRAPH)41Sawhney, R., Seyb, D., Jarosz, W., and Crane, K. Grid- free Monte Carlo for PDEs with spatially varying coeffi- cients. ACM Transactions on Graphics (Proceedings of SIGGRAPH), 41(4), July 2022. doi: 10.1145/3528223. 3530134.</p>
<p>The graph neural network model. F Scarselli, M Gori, A C Tsoi, M Hagenbuchner, G Monfardini, IEEE transactions on neural networks. 201Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. The graph neural network model. IEEE transactions on neural networks, 20(1):61-80, 2008.</p>
<p>Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. Advances in neural information processing systems. K Schütt, P.-J Kindermans, H E Sauceda Felix, S Chmiela, A Tkatchenko, K.-R Müller, 30Schütt, K., Kindermans, P.-J., Sauceda Felix, H. E., Chmiela, S., Tkatchenko, A., and Müller, K.-R. Schnet: A continuous-filter convolutional neural network for model- ing quantum interactions. Advances in neural information processing systems, 30, 2017.</p>
<p>Equivariant message passing for the prediction of tensorial properties and molecular spectra. K Schütt, O Unke, M Gastegger, PMLRProceedings of the 38th International Conference on Machine Learning. Meila, M. and Zhang, T.the 38th International Conference on Machine Learning139Schütt, K., Unke, O., and Gastegger, M. Equivariant mes- sage passing for the prediction of tensorial properties and molecular spectra. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Ma- chine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9377-9388. PMLR, 18-24 Jul 2021.</p>
<p>Essentially non-oscillatory and weighted essentially non-oscillatory schemes for hyperbolic conservation laws. C.-W Shu, Advanced numerical approximation of nonlinear hyperbolic equations. SpringerShu, C.-W. Essentially non-oscillatory and weighted es- sentially non-oscillatory schemes for hyperbolic conser- vation laws. In Advanced numerical approximation of nonlinear hyperbolic equations, pp. 325-432. Springer, 1998.</p>
<p>A particle method for history-dependent materials. D Sulsky, Z Chen, H L Schreyer, Sandia National Labs., Albuquerque, NM (United StatesTechnical reportSulsky, D., Chen, Z., and Schreyer, H. L. A particle method for history-dependent materials. Technical report, Sandia National Labs., Albuquerque, NM (United States), 1993.</p>
<p>Accelerating Eulerian fluid simulation with convolutional networks. J Tompson, K Schlachter, P Sprechmann, K Perlin, PMLRProceedings of the 34th International Conference on Machine Learning. Precup, D. and Teh, Y. W.the 34th International Conference on Machine Learning70Tompson, J., Schlachter, K., Sprechmann, P., and Perlin, K. Accelerating Eulerian fluid simulation with convo- lutional networks. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Ma- chine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 3424-3433. PMLR, 06-11 Aug 2017.</p>
<p>Solver-in-the-loop: Learning from differentiable physics to interact with iterative pde-solvers. K Um, R Brand, Y R Fei, P Holl, N Thuerey, H Larochelle, M Ranzato, R Hadsell, M Balcan, Advances in Neural Information Processing Systems. Lin, H.Curran Associates, Inc33Um, K., Brand, R., Fei, Y. R., Holl, P., and Thuerey, N. Solver-in-the-loop: Learning from differentiable physics to interact with iterative pde-solvers. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, vol- ume 33, pp. 6111-6122. Curran Associates, Inc., 2020.</p>
<p>Lagrangian fluid simulation with continuous convolutions. B Ummenhofer, L Prantl, N Thuerey, V Koltun, International Conference on Learning Representations. Ummenhofer, B., Prantl, L., Thuerey, N., and Koltun, V. Lagrangian fluid simulation with continuous convolutions. In International Conference on Learning Representations, 2020.</p>
<p>REMus-GNN: A rotation-equivariant model for simulating continuum dynamics. M L Valencia, S Fotiadis, A A Bharath, C D Cantwell, ICLR 2022 Workshop on Geometrical and Topological Representation Learning. Valencia, M. L., Fotiadis, S., Bharath, A. A., and Cantwell, C. D. REMus-GNN: A rotation-equivariant model for simulating continuum dynamics. In ICLR 2022 Workshop on Geometrical and Topological Representation Learn- ing, 2022.</p>
<p>Neural execution of graph algorithms. P Veličković, R Ying, M Padovano, R Hadsell, C Blundell, International Conference on Learning Representations. Veličković, P., Ying, R., Padovano, M., Hadsell, R., and Blundell, C. Neural execution of graph algorithms. In International Conference on Learning Representations, 2020.</p>
<p>Towards physics-informed deep learning for turbulent flow prediction. R Wang, K Kashinath, M Mustafa, A Albert, Yu , R , Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningWang, R., Kashinath, K., Mustafa, M., Albert, A., and Yu, R. Towards physics-informed deep learning for turbulent flow prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pp. 1457-1466, 2020.</p>
<p>W Wang, M Xu, C Cai, B K Miller, T Smidt, Y Wang, J Tang, R Gómez-Bombarelli, arXiv:2201.12176Generative coarse-graining of molecular conformations. arXiv preprintWang, W., Xu, M., Cai, C., Miller, B. K., Smidt, T., Wang, Y., Tang, J., and Gómez-Bombarelli, R. Genera- tive coarse-graining of molecular conformations. arXiv preprint arXiv:2201.12176, 2022.</p>
<p>3d steerable cnns: Learning rotationally equivariant features in volumetric data. M Weiler, M Geiger, M Welling, W Boomsma, T S Cohen, Advances in Neural Information Processing Systems. 31Weiler, M., Geiger, M., Welling, M., Boomsma, W., and Cohen, T. S. 3d steerable cnns: Learning rotationally equivariant features in volumetric data. Advances in Neu- ral Information Processing Systems, 31, 2018.</p>
<p>What can neural networks reason about? In ICLR. K Xu, J Li, M Zhang, S S Du, K Ichi Kawarabayashi, S Jegelka, Xu, K., Li, J., Zhang, M., Du, S. S., ichi Kawarabayashi, K., and Jegelka, S. What can neural networks reason about? In ICLR, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
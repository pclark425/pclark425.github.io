<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2357 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2357</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2357</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-12d1d070a53d4084d88a77b8b143bad51c40c38f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/12d1d070a53d4084d88a77b8b143bad51c40c38f" target="_blank">Reinforcement Learning: A Survey</a></p>
                <p><strong>Paper Venue:</strong> Journal of Artificial Intelligence Research</p>
                <p><strong>Paper TL;DR:</strong> Central issues of reinforcement learning are discussed, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state.</p>
                <p><strong>Paper Abstract:</strong> This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word "reinforcement." The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2357.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2357.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning (RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of machine-learning methods in which an agent learns behavior through trial-and-error interactions with a dynamic environment, optimizing long-run cumulative reward under models such as finite-horizon, infinite-horizon discounted, or average-reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Sequential decision-making / control (including robotics, adaptive control, planning, and general MDP problems)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learning policies that map states to actions to maximize long-run accumulated reward in environments where state transitions and rewards may be stochastic and rewards may be delayed.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Data are generated online by agent interactions (trial-and-error). No labeled input/output pairs are provided a priori; amount of data depends on task and can be large (often costly in real-world tasks); for discrete-theory cases the paper assumes the entire state space can be enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured as state-action-reward-next-state tuples (experience tuples) drawn from a Markov decision process; typically discrete states/actions in theory sections, but continuous spaces and function approximation discussed elsewhere in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: nontrivial temporal credit assignment (delayed rewards), exploration-exploitation trade-off, potentially very large state/action spaces (combinatorial explosion), and computational complexity that grows with state-space size and with 1/(1-γ) for discounted problems.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging (as of the survey): rapid recent growth in ML/AI communities but less mature than adaptive control; many foundational theoretical results exist for discrete MDPs, but practical scalability and non-stationarity remain active research areas.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — model-free RL can operate without explicit mechanistic models (black-box interaction acceptable), while model-based RL requires learning or assuming mechanistic transition/reward models.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Reinforcement learning (general methods: model-free and model-based)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Framework using experience tuples ⟨s,a,r,s'⟩; includes model-free temporal-difference methods (e.g., TD(λ), Q-learning), model-based estimation of T and R and planning (value/policy iteration), and hybrid architectures (Dyna) that interleave model learning, planning, and acting. Algorithms may use eligibility traces, sample backups, asynchronous updates, and function approximation for large/continuous spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable to sequential control problems where interactions are possible and long-run performance matters; limited by sample complexity and computational cost for large/continuous problems. Works both in deterministic and stochastic settings; choice of optimality criterion (finite horizon, discounted, average reward) affects applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Generally effective in small to moderate discrete MDPs; many algorithms guarantee asymptotic optimality under conditions but can be sample-inefficient and computationally expensive. Success depends strongly on exploration strategy, model assumptions, and whether generalization is used.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High: provides a principled way to learn control policies from experience without explicit programming of behaviors; potential to enable learning in robotics, planning, and adaptive control where explicit models are unavailable or expensive to obtain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted with search/genetic algorithms (behavior search) and classical adaptive control (model-based, smooth, with fixed structure). RL leverages MDP structure and allows learning from interaction rather than assuming fixed dynamical models.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of sufficient exploratory experience, appropriate choice of optimality model (discounting vs. average reward), use of generalization/hierarchy to handle large state spaces, and hybrid use of learned models to reduce experience requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>RL is appropriate for sequential decision problems learned from interaction, but its practical success hinges on balancing sample efficiency (model-based or hybrid methods) and computational costs while addressing exploration-exploitation and delayed credit assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning: A Survey', 'publication_date_yy_mm': '1996-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2357.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2357.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q-learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Q-learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-free temporal-difference algorithm that learns an action-value function Q(s,a) via online updates and derives a policy by choosing actions that maximize Q for the current state; provably converges to optimal Q* under standard coverage and step-size conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Markov decision processes with delayed reward; general control and planning problems (including gridworld shortest-path tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Estimate optimal action-values and derive an optimal policy solely from experience tuples without learning an explicit model of environment transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires repeated online visits to state-action pairs; convergence guarantees assume each state-action pair is visited infinitely often (i.e., potentially large data requirement).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured tabular state-action pairs for discrete MDPs; extendable to function approximation for continuous/high-dimensional spaces (discussed conceptually).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Convergence can be slow in large state-action spaces; computational cost per update is modest but sample complexity is large; sensitive to exploration schedule and learning rate decay.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established method within RL literature (well-known, widely-used baseline algorithm).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low — model-free; does not require mechanistic model of transitions/rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Q-learning (off-policy TD control)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Online update: Q(s,a) := Q(s,a) + α [ r + γ max_{a'} Q(s',a') - Q(s,a) ], using experience tuples ⟨s,a,r,s'⟩; exploration strategy must ensure sufficient coverage; can be combined with eligibility traces (TD(λ)) or function approximation extensions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Reinforcement learning (model-free)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for model-free control when agent can gather experience and exploration coverage can be ensured; not ideal when real-world interactions are extremely costly due to high sample requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>In the 3277-state gridworld reported in the paper (shortest-path formulation): Steps before convergence = 531,000; Backups before convergence = 531,000.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Described as the most popular and practical model-free algorithm; exploration-insensitive convergence (converges regardless of exploration policy given sufficient coverage), but it can converge slowly and be sample inefficient compared with model-based or hybrid methods.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High as a foundational model-free RL algorithm; useful baseline and practical option where model learning is infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared experimentally to Dyna and prioritized sweeping in a gridworld: Q-learning required many more real interaction steps (531k) than Dyna (62k) and prioritized sweeping (28k), though it incurred fewer total backups than Dyna.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Correct setting of learning rate α, adequate exploration to ensure coverage, and potential combination with eligibility traces or function approximation for speed-ups.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Q-learning offers a simple, model-free route to optimal control with strong convergence guarantees, but in practice its sample inefficiency motivates model-based or hybrid approaches when real experience is costly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning: A Survey', 'publication_date_yy_mm': '1996-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2357.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2357.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dyna</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dyna (Sutton)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid architecture that interleaves real experience, incremental model learning (estimating \hT and \hR), and planning by using the learned model to perform simulated updates, thereby trading computation for reduced real-world experience.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Reinforcement learning for MDPs where real interactions are costly (e.g., robotics, navigation/shortest-path problems)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Reduce the number of real-world interactions required to learn an effective policy by using a learned model to generate additional simulated backups (planning updates).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses the same online experience tuples to build a model; reduces dependence on large amounts of real interaction by augmenting with simulated data from the learned model.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Model stores estimated transition probabilities and rewards (tabular or statistical summaries) for state-action pairs; suitable for discrete MDPs and extendable with function approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Increases computational load (per real step performs k simulated backups), but reduces the number of real interaction steps required; complexity depends on chosen k and model maintenance costs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Presented as a practical middle ground between model-free and naive model-based methods; recognized as a useful architecture for problems with expensive real experience.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — requires learning an approximate mechanistic model of transitions and rewards, but that model is learned empirically rather than assumed.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Dyna (hybrid model-based/model-free RL)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>On each real transition ⟨s,a,r,s'⟩: update the learned model \hT,\hR; perform a direct Q-like update for the experienced transition; then perform k simulated updates by sampling stored/plausible state-action pairs and applying model-based backups Q(s_k,a_k) := \hR + γ Σ_{s'} \hT(s_k,a_k,s') max_{a'} Q(s',a'). The parameter k controls planning effort versus acting.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Hybrid (model-based + model-free reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate when computation is cheaper than collecting real interactions (e.g., simulated or costly real-world domains); requires maintaining and updating a model which may be impractical in very large/continuous spaces without function approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>In the 3277-state gridworld reported in the paper: Steps before convergence = 62,000; Backups before convergence = 3,055,000 (with k=200 backups per transition).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Substantially reduces the number of real interaction steps needed compared to pure model-free Q-learning (an order-of-magnitude fewer steps), at the cost of many more internal planning/backups (computational expense).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant for problems where real-world experience is expensive; demonstrates a practical trade-off between computation and sample efficiency that can be tuned via k.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly to Q-learning and prioritized sweeping in the paper's experiment: Dyna required far fewer real steps than Q-learning but more total backups; prioritized sweeping used even fewer steps than Dyna in the reported experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Quality of the learned model (\hT,\hR), choice of k (number of planning updates), and the exploration mechanism; performance improves when simulated backups focus on informative state-action pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Interleaving learned-model planning with real experience (Dyna) can dramatically reduce required real interactions at the cost of additional computation, providing a practical trade-off for expensive real-world learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning: A Survey', 'publication_date_yy_mm': '1996-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2357.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2357.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prioritized Sweeping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prioritized Sweeping</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based planning technique that prioritizes updates (backups) to states based on the estimated urgency or potential impact of the update, using a priority queue to focus computation where it will most improve value estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Reinforcement learning / planning in MDPs (used in navigation/shortest-path tasks and other discrete-state problems)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Improve planning efficiency by concentrating backups on states whose value estimates are likely to change most, thereby accelerating convergence of value/Q estimates with fewer real interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on an estimated model built from experience; augmented by simulated backups prioritized by expected value-change.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Model of state transitions and rewards (tabular or statistical form) along with a priority queue for candidate state-action backups.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Reduces effective computational/sample complexity by focusing computation; complexity dominated by model maintenance and priority queue operations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Described as a practical improvement to model-based planning; used in experiments and cited as effective for many problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — requires an empirical model to estimate which states are likely to change, but does not require analytical mechanistic models.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Prioritized sweeping (model-based prioritized planning)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Maintain an estimated model and a priority queue of state-action pairs; when a transition significantly changes a state's estimate, predecessors are queued with priorities proportional to expected impact; perform backups in priority order to accelerate propagation of value changes.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Model-based reinforcement learning / planning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Particularly suitable when models are learned and transition structure is sparse so that focused planning yields gains; works well in deterministic or sparsely-connected MDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>In the 3277-state gridworld reported in the paper: Steps before convergence = 28,000; Backups before convergence = 1,010,000 (with k=200 backups per transition and priority queue often emptying before all backups were used).</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Shows the best reduction in real interaction steps among the three compared methods in the reported experiment (fewer steps than Dyna and Q-learning), with fewer total backups than Dyna in that experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables more sample-efficient planning by focusing computation, useful when models can be learned and computation can be focused on high-impact updates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared experimentally to Q-learning and Dyna: prioritized sweeping required the fewest real steps to converge and fewer backups than Dyna in the reported gridworld task.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Sparsity of transition dynamics, reliability of model estimates for predecessor relationships, and an effective priority metric for ordering backups.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Focusing planning computation on state-action pairs with highest potential impact (prioritized sweeping) substantially reduces required real interactions and can be computationally efficient compared to unguided planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning: A Survey', 'publication_date_yy_mm': '1996-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2357.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2357.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gittins Indices</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gittins Allocation Indices</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theoretically optimal index-based solution method for the k-armed bandit problem under discounted reward, assigning each arm an index I(n,w) computed from its success/failure history and choosing the highest index.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Single-state bandit problems / exploration-exploitation (applications include robotic manipulation with immediate reward)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Select among k independent options (arms) to maximize discounted cumulative reward over time when each arm has unknown payoff probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Data are counts of successes and failures per arm; compact sufficient statistics (n_i, w_i) are maintained; amount of data required depends on horizon and discounting.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Low-dimensional count statistics per arm (tabular), often beta/posterior distributions for probabilities under conjugate priors.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Computationally efficient for bandits given precomputed index tables; complexity arises when extending to delayed-reward or multi-state problems where no analogous index policy exists.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Well-established theoretical solution for discounted bandits (Gittins theorem); practical tables exist for many parameter settings.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low for bandit setting (statistics-based indices suffice); not applicable when delayed reinforcement or state transitions are significant.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Gittins allocation index policy</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>For each arm, compute index I(n,w) (from posterior statistics and discount factor) that balances immediate expected payoff and value of information; select arm with highest index. Table-based indices are available for many parameter regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Bandit-theoretic index policy / reinforcement learning (single-state)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Exactly applicable to discounted k-armed bandit problems; limited applicability to multi-state or delayed-reward MDPs as no general index analog is known.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Provably optimal under discounted-bandit assumptions and computationally simple given index tables; noted in the paper as useful in an application to robotic manipulation with immediate reward (cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High within the class of bandit problems; limited for broader RL problems with state transitions or delayed rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Superior (optimal) for discounted single-state bandits compared to ad-hoc exploration heuristics, but inapplicable to delayed-reward multi-state problems where other RL methods are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of discount factor consistent with theory, tractable posterior representation for arms, and precomputed index tables for efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>For single-state discounted bandits, index policies like Gittins' provide an optimal and computationally simple exploration-exploitation solution, but this advantage does not extend straightforwardly to delayed, multi-state settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning: A Survey', 'publication_date_yy_mm': '1996-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2357.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2357.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TD(λ) / AHC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal-Difference Learning (TD(λ)) and Adaptive Heuristic Critic (AHC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>TD(λ) is a family of temporal-difference policy-evaluation algorithms using eligibility traces to assign credit to recently visited states; AHC combines a critic that estimates state values (TD) with a reinforcement-learning actor to improve policies (an incremental variant of policy iteration).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Policy evaluation and control in MDPs; used within adaptive control and actor-critic architectures (including robotics and control tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Efficient temporal credit assignment: estimate state values and use them to guide policy improvement when rewards are delayed across trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on online sequences of experience tuples; effectiveness depends on amount of sequential data and proper decay of learning rates and eligibility traces.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Sequential time-series of visited states and rewards; eligibility traces summarize recent visitation history.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Improves convergence speed over one-step methods for long-delayed credit assignment, but computational and memory costs increase with maintaining eligibilities for many states.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established class of algorithms with theoretical and empirical results; widely used as a component in RL architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low to medium — algorithm learns value estimates empirically; AHC requires tuning of relative learning rates between actor and critic.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>TD(λ) and Adaptive Heuristic Critic (actor-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>TD(λ) updates state-value estimates using eligibility traces e(s) that decay with λ and γ; AHC pairs a critic that learns V(s) via TD with an actor that optimizes actions using the critic's heuristic values, operating either alternately (guaranteed convergence under conditions) or simultaneously (practical implementations).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Reinforcement learning (temporal-difference / actor-critic)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Useful for problems with delayed rewards and long credit-assignment horizons; enables faster empirical learning for some tasks when λ is tuned appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>TD(λ) often converges faster for larger λ, but maintaining eligibilities is more expensive; AHC can be difficult to tune (learning rates) and may be harder to work with than Q-learning in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Important for accelerating learning in sequential tasks and forms the basis of many actor-critic and policy-gradient methods developed later.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to Q-learning: AHC separates actor/critic roles and may require more careful tuning; Q-learning is simpler and exploration-insensitive in convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Appropriate choice of λ and learning rates, stability of actor-critic coordination, and suitability of simultaneous vs alternating update schedules.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Eligibility traces (TD(λ)) provide a practical mechanism to speed up learning from delayed rewards, and actor-critic (AHC) architectures can implement policy iteration incrementally, but require careful tuning to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning: A Survey', 'publication_date_yy_mm': '1996-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2357.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2357.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Certainty Equivalence (Model-based RL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Certainty-Equivalence / Model-based Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that first estimates environment transition and reward models from data, then computes an optimal policy using dynamic programming (value/policy iteration) based on the estimated model; when done continuously, this is known as continual model-based control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Control and planning tasks where accurate models can be estimated; adaptive control contexts</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Obtain an optimal controller by estimating the mechanistic model (T and R) from experience and applying planning algorithms to that model.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires sufficient exploratory data to obtain reliable model estimates; naive random exploration can be extremely inefficient (may require exponential data in some environments).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Tabulated transition and reward statistics for state-action pairs (or parametric model estimates for continuous dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Computationally demanding (solving dynamic programming or linear programs repeatedly), and exploration strategy design is critical; naively splitting into separate learning and acting phases has risks with non-stationarity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Conceptually straightforward and well-understood; practical implementations can be computationally intensive, motivating hybrid approaches (e.g., Dyna).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High in the sense that the method builds/uses an explicit mechanistic model; good model structure assumptions improve efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Certainty-equivalence (learn model then plan)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Collect transition/reward statistics from experience to estimate \T and \R; periodically or continually solve planning algorithms (value or policy iteration) on the estimated model to derive/update a policy; trade-offs exist between data-collection strategies and computation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Model-based reinforcement learning / planning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate when computation is cheap and real-world experience is costly, and when an accurate model can be estimated from data; less suitable when exploration is dangerous or random exploration is infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Makes efficient use of data when models are accurate, but naive separation into distinct learning and acting phases is risky (inefficient exploration, vulnerable to non-stationarity); continual certainty-equivalence mitigates some issues but remains computationally demanding.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Potentially large in domains where simulation/compute is cheap relative to data collection, but practical barriers (exploration strategy, computation) limit straightforward use.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared unfavorably (in raw computational cost) to Dyna, which aims to achieve similar data efficiency with less computation by incremental model learning and planning; model-free methods use less computation per experience but require more data.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Quality and sufficiency of exploratory data, structure of chosen model class (parametric assumptions), and computational resources for repeated planning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Learning an explicit model and planning on it can greatly improve data efficiency relative to model-free methods, but practical deployment requires careful exploration design and often heavy computation, motivating hybrid solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning: A Survey', 'publication_date_yy_mm': '1996-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2357.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2357.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Learning Automata (Linear Reward-Inaction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning Automata (e.g., Linear Reward-Inaction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of adaptive algorithms for multi-armed bandit problems that adjust action-selection probabilities incrementally based on observed successes/failures; the linear reward-inaction rule increases probability of successful actions and leaves probabilities unchanged on failures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Single-state bandit problems and simple adaptive decision problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Incrementally adapt a probability distribution over actions to converge to near-optimal action selection in bandit-like tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on counts of trials and successes per action; requires repeated trials and sufficient data for probabilities to adjust.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Low-dimensional probability vectors over discrete actions updated based on scalar rewards (0/1 or real-valued).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Relatively low complexity in the bandit setting; convergence properties depend on step-size parameter α and can be slow when α is small.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established area with theoretical results (Narendra & Thathachar), particularly for bandit problems and finite-state automata formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Low — these are adaptive, statistical procedures not requiring detailed mechanistic models.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Learning automata / linear reward-inaction</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Maintain action probability vector p; on rewarded action i: p_i := p_i + α(1-p_i), p_j := p_j - α p_j for j≠i; on failure, do nothing (inaction). Converges to a pure action distribution as α→0, though may converge to suboptimal actions with nonzero probability unless α is tuned small.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Adaptive stochastic optimization / bandit algorithms</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Suitable for simple bandit/adaptive choice problems where a compact probability representation suffices; not directly applicable to multi-state or delayed-reward MDPs without extension.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Converges with probability 1 to a deterministic action distribution but can converge to the wrong action; probability of incorrect convergence can be made arbitrarily small by reducing α (at the cost of slower learning).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Useful in low-dimensional adaptive selection tasks; foundational in the study of finite-state adaptive controllers and bandit algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Less powerful than optimal-index methods for bandits (e.g., Gittins indices) when those apply, but conceptually simple and easy to implement.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Choice of step-size α (trade-off between speed and probability of correct convergence) and suitability to single-state bandit assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Simple probability-update rules can adaptively identify good actions in bandit settings, but tuning trade-offs (speed vs. correctness) limit their direct applicability to more complex RL problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reinforcement Learning: A Survey', 'publication_date_yy_mm': '1996-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Q-learning <em>(Rating: 2)</em></li>
                <li>Integrated architectures for learning, planning, and reacting based on approximating dynamic programming <em>(Rating: 2)</em></li>
                <li>Prioritized sweeping: Reinforcement learning with less data and less computation <em>(Rating: 2)</em></li>
                <li>Multi-armed Bandit Allocation Indices <em>(Rating: 2)</em></li>
                <li>Learning to predict by the methods of temporal differences <em>(Rating: 1)</em></li>
                <li>Neuronlike adaptive elements that can solve difficult learning control problems <em>(Rating: 1)</em></li>
                <li>A theory of learning automata <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2357",
    "paper_id": "paper-12d1d070a53d4084d88a77b8b143bad51c40c38f",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "Reinforcement Learning",
            "name_full": "Reinforcement Learning (RL)",
            "brief_description": "A class of machine-learning methods in which an agent learns behavior through trial-and-error interactions with a dynamic environment, optimizing long-run cumulative reward under models such as finite-horizon, infinite-horizon discounted, or average-reward.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Sequential decision-making / control (including robotics, adaptive control, planning, and general MDP problems)",
            "problem_description": "Learning policies that map states to actions to maximize long-run accumulated reward in environments where state transitions and rewards may be stochastic and rewards may be delayed.",
            "data_availability": "Data are generated online by agent interactions (trial-and-error). No labeled input/output pairs are provided a priori; amount of data depends on task and can be large (often costly in real-world tasks); for discrete-theory cases the paper assumes the entire state space can be enumerated.",
            "data_structure": "Structured as state-action-reward-next-state tuples (experience tuples) drawn from a Markov decision process; typically discrete states/actions in theory sections, but continuous spaces and function approximation discussed elsewhere in the paper.",
            "problem_complexity": "High: nontrivial temporal credit assignment (delayed rewards), exploration-exploitation trade-off, potentially very large state/action spaces (combinatorial explosion), and computational complexity that grows with state-space size and with 1/(1-γ) for discounted problems.",
            "domain_maturity": "Emerging (as of the survey): rapid recent growth in ML/AI communities but less mature than adaptive control; many foundational theoretical results exist for discrete MDPs, but practical scalability and non-stationarity remain active research areas.",
            "mechanistic_understanding_requirements": "Medium — model-free RL can operate without explicit mechanistic models (black-box interaction acceptable), while model-based RL requires learning or assuming mechanistic transition/reward models.",
            "ai_methodology_name": "Reinforcement learning (general methods: model-free and model-based)",
            "ai_methodology_description": "Framework using experience tuples ⟨s,a,r,s'⟩; includes model-free temporal-difference methods (e.g., TD(λ), Q-learning), model-based estimation of T and R and planning (value/policy iteration), and hybrid architectures (Dyna) that interleave model learning, planning, and acting. Algorithms may use eligibility traces, sample backups, asynchronous updates, and function approximation for large/continuous spaces.",
            "ai_methodology_category": "Reinforcement learning",
            "applicability": "Applicable to sequential control problems where interactions are possible and long-run performance matters; limited by sample complexity and computational cost for large/continuous problems. Works both in deterministic and stochastic settings; choice of optimality criterion (finite horizon, discounted, average reward) affects applicability.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Generally effective in small to moderate discrete MDPs; many algorithms guarantee asymptotic optimality under conditions but can be sample-inefficient and computationally expensive. Success depends strongly on exploration strategy, model assumptions, and whether generalization is used.",
            "impact_potential": "High: provides a principled way to learn control policies from experience without explicit programming of behaviors; potential to enable learning in robotics, planning, and adaptive control where explicit models are unavailable or expensive to obtain.",
            "comparison_to_alternatives": "Contrasted with search/genetic algorithms (behavior search) and classical adaptive control (model-based, smooth, with fixed structure). RL leverages MDP structure and allows learning from interaction rather than assuming fixed dynamical models.",
            "success_factors": "Availability of sufficient exploratory experience, appropriate choice of optimality model (discounting vs. average reward), use of generalization/hierarchy to handle large state spaces, and hybrid use of learned models to reduce experience requirements.",
            "key_insight": "RL is appropriate for sequential decision problems learned from interaction, but its practical success hinges on balancing sample efficiency (model-based or hybrid methods) and computational costs while addressing exploration-exploitation and delayed credit assignment.",
            "uuid": "e2357.0",
            "source_info": {
                "paper_title": "Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "1996-04"
            }
        },
        {
            "name_short": "Q-learning",
            "name_full": "Q-learning",
            "brief_description": "A model-free temporal-difference algorithm that learns an action-value function Q(s,a) via online updates and derives a policy by choosing actions that maximize Q for the current state; provably converges to optimal Q* under standard coverage and step-size conditions.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Markov decision processes with delayed reward; general control and planning problems (including gridworld shortest-path tasks)",
            "problem_description": "Estimate optimal action-values and derive an optimal policy solely from experience tuples without learning an explicit model of environment transitions.",
            "data_availability": "Requires repeated online visits to state-action pairs; convergence guarantees assume each state-action pair is visited infinitely often (i.e., potentially large data requirement).",
            "data_structure": "Structured tabular state-action pairs for discrete MDPs; extendable to function approximation for continuous/high-dimensional spaces (discussed conceptually).",
            "problem_complexity": "Convergence can be slow in large state-action spaces; computational cost per update is modest but sample complexity is large; sensitive to exploration schedule and learning rate decay.",
            "domain_maturity": "Established method within RL literature (well-known, widely-used baseline algorithm).",
            "mechanistic_understanding_requirements": "Low — model-free; does not require mechanistic model of transitions/rewards.",
            "ai_methodology_name": "Q-learning (off-policy TD control)",
            "ai_methodology_description": "Online update: Q(s,a) := Q(s,a) + α [ r + γ max_{a'} Q(s',a') - Q(s,a) ], using experience tuples ⟨s,a,r,s'⟩; exploration strategy must ensure sufficient coverage; can be combined with eligibility traces (TD(λ)) or function approximation extensions.",
            "ai_methodology_category": "Reinforcement learning (model-free)",
            "applicability": "Appropriate for model-free control when agent can gather experience and exploration coverage can be ensured; not ideal when real-world interactions are extremely costly due to high sample requirements.",
            "effectiveness_quantitative": "In the 3277-state gridworld reported in the paper (shortest-path formulation): Steps before convergence = 531,000; Backups before convergence = 531,000.",
            "effectiveness_qualitative": "Described as the most popular and practical model-free algorithm; exploration-insensitive convergence (converges regardless of exploration policy given sufficient coverage), but it can converge slowly and be sample inefficient compared with model-based or hybrid methods.",
            "impact_potential": "High as a foundational model-free RL algorithm; useful baseline and practical option where model learning is infeasible.",
            "comparison_to_alternatives": "Compared experimentally to Dyna and prioritized sweeping in a gridworld: Q-learning required many more real interaction steps (531k) than Dyna (62k) and prioritized sweeping (28k), though it incurred fewer total backups than Dyna.",
            "success_factors": "Correct setting of learning rate α, adequate exploration to ensure coverage, and potential combination with eligibility traces or function approximation for speed-ups.",
            "key_insight": "Q-learning offers a simple, model-free route to optimal control with strong convergence guarantees, but in practice its sample inefficiency motivates model-based or hybrid approaches when real experience is costly.",
            "uuid": "e2357.1",
            "source_info": {
                "paper_title": "Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "1996-04"
            }
        },
        {
            "name_short": "Dyna",
            "name_full": "Dyna (Sutton)",
            "brief_description": "A hybrid architecture that interleaves real experience, incremental model learning (estimating \\hT and \\hR), and planning by using the learned model to perform simulated updates, thereby trading computation for reduced real-world experience.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Reinforcement learning for MDPs where real interactions are costly (e.g., robotics, navigation/shortest-path problems)",
            "problem_description": "Reduce the number of real-world interactions required to learn an effective policy by using a learned model to generate additional simulated backups (planning updates).",
            "data_availability": "Uses the same online experience tuples to build a model; reduces dependence on large amounts of real interaction by augmenting with simulated data from the learned model.",
            "data_structure": "Model stores estimated transition probabilities and rewards (tabular or statistical summaries) for state-action pairs; suitable for discrete MDPs and extendable with function approximation.",
            "problem_complexity": "Increases computational load (per real step performs k simulated backups), but reduces the number of real interaction steps required; complexity depends on chosen k and model maintenance costs.",
            "domain_maturity": "Presented as a practical middle ground between model-free and naive model-based methods; recognized as a useful architecture for problems with expensive real experience.",
            "mechanistic_understanding_requirements": "Medium — requires learning an approximate mechanistic model of transitions and rewards, but that model is learned empirically rather than assumed.",
            "ai_methodology_name": "Dyna (hybrid model-based/model-free RL)",
            "ai_methodology_description": "On each real transition ⟨s,a,r,s'⟩: update the learned model \\hT,\\hR; perform a direct Q-like update for the experienced transition; then perform k simulated updates by sampling stored/plausible state-action pairs and applying model-based backups Q(s_k,a_k) := \\hR + γ Σ_{s'} \\hT(s_k,a_k,s') max_{a'} Q(s',a'). The parameter k controls planning effort versus acting.",
            "ai_methodology_category": "Hybrid (model-based + model-free reinforcement learning)",
            "applicability": "Appropriate when computation is cheaper than collecting real interactions (e.g., simulated or costly real-world domains); requires maintaining and updating a model which may be impractical in very large/continuous spaces without function approximation.",
            "effectiveness_quantitative": "In the 3277-state gridworld reported in the paper: Steps before convergence = 62,000; Backups before convergence = 3,055,000 (with k=200 backups per transition).",
            "effectiveness_qualitative": "Substantially reduces the number of real interaction steps needed compared to pure model-free Q-learning (an order-of-magnitude fewer steps), at the cost of many more internal planning/backups (computational expense).",
            "impact_potential": "Significant for problems where real-world experience is expensive; demonstrates a practical trade-off between computation and sample efficiency that can be tuned via k.",
            "comparison_to_alternatives": "Compared directly to Q-learning and prioritized sweeping in the paper's experiment: Dyna required far fewer real steps than Q-learning but more total backups; prioritized sweeping used even fewer steps than Dyna in the reported experiment.",
            "success_factors": "Quality of the learned model (\\hT,\\hR), choice of k (number of planning updates), and the exploration mechanism; performance improves when simulated backups focus on informative state-action pairs.",
            "key_insight": "Interleaving learned-model planning with real experience (Dyna) can dramatically reduce required real interactions at the cost of additional computation, providing a practical trade-off for expensive real-world learning.",
            "uuid": "e2357.2",
            "source_info": {
                "paper_title": "Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "1996-04"
            }
        },
        {
            "name_short": "Prioritized Sweeping",
            "name_full": "Prioritized Sweeping",
            "brief_description": "A model-based planning technique that prioritizes updates (backups) to states based on the estimated urgency or potential impact of the update, using a priority queue to focus computation where it will most improve value estimates.",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Reinforcement learning / planning in MDPs (used in navigation/shortest-path tasks and other discrete-state problems)",
            "problem_description": "Improve planning efficiency by concentrating backups on states whose value estimates are likely to change most, thereby accelerating convergence of value/Q estimates with fewer real interactions.",
            "data_availability": "Relies on an estimated model built from experience; augmented by simulated backups prioritized by expected value-change.",
            "data_structure": "Model of state transitions and rewards (tabular or statistical form) along with a priority queue for candidate state-action backups.",
            "problem_complexity": "Reduces effective computational/sample complexity by focusing computation; complexity dominated by model maintenance and priority queue operations.",
            "domain_maturity": "Described as a practical improvement to model-based planning; used in experiments and cited as effective for many problems.",
            "mechanistic_understanding_requirements": "Medium — requires an empirical model to estimate which states are likely to change, but does not require analytical mechanistic models.",
            "ai_methodology_name": "Prioritized sweeping (model-based prioritized planning)",
            "ai_methodology_description": "Maintain an estimated model and a priority queue of state-action pairs; when a transition significantly changes a state's estimate, predecessors are queued with priorities proportional to expected impact; perform backups in priority order to accelerate propagation of value changes.",
            "ai_methodology_category": "Model-based reinforcement learning / planning",
            "applicability": "Particularly suitable when models are learned and transition structure is sparse so that focused planning yields gains; works well in deterministic or sparsely-connected MDPs.",
            "effectiveness_quantitative": "In the 3277-state gridworld reported in the paper: Steps before convergence = 28,000; Backups before convergence = 1,010,000 (with k=200 backups per transition and priority queue often emptying before all backups were used).",
            "effectiveness_qualitative": "Shows the best reduction in real interaction steps among the three compared methods in the reported experiment (fewer steps than Dyna and Q-learning), with fewer total backups than Dyna in that experiment.",
            "impact_potential": "Enables more sample-efficient planning by focusing computation, useful when models can be learned and computation can be focused on high-impact updates.",
            "comparison_to_alternatives": "Compared experimentally to Q-learning and Dyna: prioritized sweeping required the fewest real steps to converge and fewer backups than Dyna in the reported gridworld task.",
            "success_factors": "Sparsity of transition dynamics, reliability of model estimates for predecessor relationships, and an effective priority metric for ordering backups.",
            "key_insight": "Focusing planning computation on state-action pairs with highest potential impact (prioritized sweeping) substantially reduces required real interactions and can be computationally efficient compared to unguided planning.",
            "uuid": "e2357.3",
            "source_info": {
                "paper_title": "Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "1996-04"
            }
        },
        {
            "name_short": "Gittins Indices",
            "name_full": "Gittins Allocation Indices",
            "brief_description": "A theoretically optimal index-based solution method for the k-armed bandit problem under discounted reward, assigning each arm an index I(n,w) computed from its success/failure history and choosing the highest index.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Single-state bandit problems / exploration-exploitation (applications include robotic manipulation with immediate reward)",
            "problem_description": "Select among k independent options (arms) to maximize discounted cumulative reward over time when each arm has unknown payoff probabilities.",
            "data_availability": "Data are counts of successes and failures per arm; compact sufficient statistics (n_i, w_i) are maintained; amount of data required depends on horizon and discounting.",
            "data_structure": "Low-dimensional count statistics per arm (tabular), often beta/posterior distributions for probabilities under conjugate priors.",
            "problem_complexity": "Computationally efficient for bandits given precomputed index tables; complexity arises when extending to delayed-reward or multi-state problems where no analogous index policy exists.",
            "domain_maturity": "Well-established theoretical solution for discounted bandits (Gittins theorem); practical tables exist for many parameter settings.",
            "mechanistic_understanding_requirements": "Low for bandit setting (statistics-based indices suffice); not applicable when delayed reinforcement or state transitions are significant.",
            "ai_methodology_name": "Gittins allocation index policy",
            "ai_methodology_description": "For each arm, compute index I(n,w) (from posterior statistics and discount factor) that balances immediate expected payoff and value of information; select arm with highest index. Table-based indices are available for many parameter regimes.",
            "ai_methodology_category": "Bandit-theoretic index policy / reinforcement learning (single-state)",
            "applicability": "Exactly applicable to discounted k-armed bandit problems; limited applicability to multi-state or delayed-reward MDPs as no general index analog is known.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Provably optimal under discounted-bandit assumptions and computationally simple given index tables; noted in the paper as useful in an application to robotic manipulation with immediate reward (cited work).",
            "impact_potential": "High within the class of bandit problems; limited for broader RL problems with state transitions or delayed rewards.",
            "comparison_to_alternatives": "Superior (optimal) for discounted single-state bandits compared to ad-hoc exploration heuristics, but inapplicable to delayed-reward multi-state problems where other RL methods are needed.",
            "success_factors": "Availability of discount factor consistent with theory, tractable posterior representation for arms, and precomputed index tables for efficiency.",
            "key_insight": "For single-state discounted bandits, index policies like Gittins' provide an optimal and computationally simple exploration-exploitation solution, but this advantage does not extend straightforwardly to delayed, multi-state settings.",
            "uuid": "e2357.4",
            "source_info": {
                "paper_title": "Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "1996-04"
            }
        },
        {
            "name_short": "TD(λ) / AHC",
            "name_full": "Temporal-Difference Learning (TD(λ)) and Adaptive Heuristic Critic (AHC)",
            "brief_description": "TD(λ) is a family of temporal-difference policy-evaluation algorithms using eligibility traces to assign credit to recently visited states; AHC combines a critic that estimates state values (TD) with a reinforcement-learning actor to improve policies (an incremental variant of policy iteration).",
            "citation_title": "",
            "mention_or_use": "use",
            "scientific_problem_domain": "Policy evaluation and control in MDPs; used within adaptive control and actor-critic architectures (including robotics and control tasks)",
            "problem_description": "Efficient temporal credit assignment: estimate state values and use them to guide policy improvement when rewards are delayed across trajectories.",
            "data_availability": "Operates on online sequences of experience tuples; effectiveness depends on amount of sequential data and proper decay of learning rates and eligibility traces.",
            "data_structure": "Sequential time-series of visited states and rewards; eligibility traces summarize recent visitation history.",
            "problem_complexity": "Improves convergence speed over one-step methods for long-delayed credit assignment, but computational and memory costs increase with maintaining eligibilities for many states.",
            "domain_maturity": "Established class of algorithms with theoretical and empirical results; widely used as a component in RL architectures.",
            "mechanistic_understanding_requirements": "Low to medium — algorithm learns value estimates empirically; AHC requires tuning of relative learning rates between actor and critic.",
            "ai_methodology_name": "TD(λ) and Adaptive Heuristic Critic (actor-critic)",
            "ai_methodology_description": "TD(λ) updates state-value estimates using eligibility traces e(s) that decay with λ and γ; AHC pairs a critic that learns V(s) via TD with an actor that optimizes actions using the critic's heuristic values, operating either alternately (guaranteed convergence under conditions) or simultaneously (practical implementations).",
            "ai_methodology_category": "Reinforcement learning (temporal-difference / actor-critic)",
            "applicability": "Useful for problems with delayed rewards and long credit-assignment horizons; enables faster empirical learning for some tasks when λ is tuned appropriately.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "TD(λ) often converges faster for larger λ, but maintaining eligibilities is more expensive; AHC can be difficult to tune (learning rates) and may be harder to work with than Q-learning in practice.",
            "impact_potential": "Important for accelerating learning in sequential tasks and forms the basis of many actor-critic and policy-gradient methods developed later.",
            "comparison_to_alternatives": "Compared conceptually to Q-learning: AHC separates actor/critic roles and may require more careful tuning; Q-learning is simpler and exploration-insensitive in convergence.",
            "success_factors": "Appropriate choice of λ and learning rates, stability of actor-critic coordination, and suitability of simultaneous vs alternating update schedules.",
            "key_insight": "Eligibility traces (TD(λ)) provide a practical mechanism to speed up learning from delayed rewards, and actor-critic (AHC) architectures can implement policy iteration incrementally, but require careful tuning to be effective.",
            "uuid": "e2357.5",
            "source_info": {
                "paper_title": "Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "1996-04"
            }
        },
        {
            "name_short": "Certainty Equivalence (Model-based RL)",
            "name_full": "Certainty-Equivalence / Model-based Reinforcement Learning",
            "brief_description": "An approach that first estimates environment transition and reward models from data, then computes an optimal policy using dynamic programming (value/policy iteration) based on the estimated model; when done continuously, this is known as continual model-based control.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Control and planning tasks where accurate models can be estimated; adaptive control contexts",
            "problem_description": "Obtain an optimal controller by estimating the mechanistic model (T and R) from experience and applying planning algorithms to that model.",
            "data_availability": "Requires sufficient exploratory data to obtain reliable model estimates; naive random exploration can be extremely inefficient (may require exponential data in some environments).",
            "data_structure": "Tabulated transition and reward statistics for state-action pairs (or parametric model estimates for continuous dynamics).",
            "problem_complexity": "Computationally demanding (solving dynamic programming or linear programs repeatedly), and exploration strategy design is critical; naively splitting into separate learning and acting phases has risks with non-stationarity.",
            "domain_maturity": "Conceptually straightforward and well-understood; practical implementations can be computationally intensive, motivating hybrid approaches (e.g., Dyna).",
            "mechanistic_understanding_requirements": "High in the sense that the method builds/uses an explicit mechanistic model; good model structure assumptions improve efficiency.",
            "ai_methodology_name": "Certainty-equivalence (learn model then plan)",
            "ai_methodology_description": "Collect transition/reward statistics from experience to estimate \\T and \\R; periodically or continually solve planning algorithms (value or policy iteration) on the estimated model to derive/update a policy; trade-offs exist between data-collection strategies and computation.",
            "ai_methodology_category": "Model-based reinforcement learning / planning",
            "applicability": "Appropriate when computation is cheap and real-world experience is costly, and when an accurate model can be estimated from data; less suitable when exploration is dangerous or random exploration is infeasible.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Makes efficient use of data when models are accurate, but naive separation into distinct learning and acting phases is risky (inefficient exploration, vulnerable to non-stationarity); continual certainty-equivalence mitigates some issues but remains computationally demanding.",
            "impact_potential": "Potentially large in domains where simulation/compute is cheap relative to data collection, but practical barriers (exploration strategy, computation) limit straightforward use.",
            "comparison_to_alternatives": "Compared unfavorably (in raw computational cost) to Dyna, which aims to achieve similar data efficiency with less computation by incremental model learning and planning; model-free methods use less computation per experience but require more data.",
            "success_factors": "Quality and sufficiency of exploratory data, structure of chosen model class (parametric assumptions), and computational resources for repeated planning.",
            "key_insight": "Learning an explicit model and planning on it can greatly improve data efficiency relative to model-free methods, but practical deployment requires careful exploration design and often heavy computation, motivating hybrid solutions.",
            "uuid": "e2357.6",
            "source_info": {
                "paper_title": "Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "1996-04"
            }
        },
        {
            "name_short": "Learning Automata (Linear Reward-Inaction)",
            "name_full": "Learning Automata (e.g., Linear Reward-Inaction)",
            "brief_description": "A family of adaptive algorithms for multi-armed bandit problems that adjust action-selection probabilities incrementally based on observed successes/failures; the linear reward-inaction rule increases probability of successful actions and leaves probabilities unchanged on failures.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Single-state bandit problems and simple adaptive decision problems",
            "problem_description": "Incrementally adapt a probability distribution over actions to converge to near-optimal action selection in bandit-like tasks.",
            "data_availability": "Operates on counts of trials and successes per action; requires repeated trials and sufficient data for probabilities to adjust.",
            "data_structure": "Low-dimensional probability vectors over discrete actions updated based on scalar rewards (0/1 or real-valued).",
            "problem_complexity": "Relatively low complexity in the bandit setting; convergence properties depend on step-size parameter α and can be slow when α is small.",
            "domain_maturity": "Established area with theoretical results (Narendra & Thathachar), particularly for bandit problems and finite-state automata formulations.",
            "mechanistic_understanding_requirements": "Low — these are adaptive, statistical procedures not requiring detailed mechanistic models.",
            "ai_methodology_name": "Learning automata / linear reward-inaction",
            "ai_methodology_description": "Maintain action probability vector p; on rewarded action i: p_i := p_i + α(1-p_i), p_j := p_j - α p_j for j≠i; on failure, do nothing (inaction). Converges to a pure action distribution as α→0, though may converge to suboptimal actions with nonzero probability unless α is tuned small.",
            "ai_methodology_category": "Adaptive stochastic optimization / bandit algorithms",
            "applicability": "Suitable for simple bandit/adaptive choice problems where a compact probability representation suffices; not directly applicable to multi-state or delayed-reward MDPs without extension.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Converges with probability 1 to a deterministic action distribution but can converge to the wrong action; probability of incorrect convergence can be made arbitrarily small by reducing α (at the cost of slower learning).",
            "impact_potential": "Useful in low-dimensional adaptive selection tasks; foundational in the study of finite-state adaptive controllers and bandit algorithms.",
            "comparison_to_alternatives": "Less powerful than optimal-index methods for bandits (e.g., Gittins indices) when those apply, but conceptually simple and easy to implement.",
            "success_factors": "Choice of step-size α (trade-off between speed and probability of correct convergence) and suitability to single-state bandit assumptions.",
            "key_insight": "Simple probability-update rules can adaptively identify good actions in bandit settings, but tuning trade-offs (speed vs. correctness) limit their direct applicability to more complex RL problems.",
            "uuid": "e2357.7",
            "source_info": {
                "paper_title": "Reinforcement Learning: A Survey",
                "publication_date_yy_mm": "1996-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Q-learning",
            "rating": 2
        },
        {
            "paper_title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming",
            "rating": 2
        },
        {
            "paper_title": "Prioritized sweeping: Reinforcement learning with less data and less computation",
            "rating": 2
        },
        {
            "paper_title": "Multi-armed Bandit Allocation Indices",
            "rating": 2
        },
        {
            "paper_title": "Learning to predict by the methods of temporal differences",
            "rating": 1
        },
        {
            "paper_title": "Neuronlike adaptive elements that can solve difficult learning control problems",
            "rating": 1
        },
        {
            "paper_title": "A theory of learning automata",
            "rating": 1
        }
    ],
    "cost": 0.02235425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Reinforcement Learning: A Survey</h1>
<p>Leslie Pack Kaelbling<br>Michael L. Littman<br>Computer Science Department, Box 1910, Brown University<br>Providence, RI 02912-1910 USA<br>Andrew W. Moore<br>Smith Hall 221, Carnegie Mellon University, 5000 Forbes Avenue Pittsburgh, PA 15213 USA</p>
<p>LPK@CS.BROWN.EDU<br>MLITTMAN@CS.BROWN.EDU</p>
<p>AWM@CS.CMU.EDU</p>
<h4>Abstract</h4>
<p>This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word "reinforcement." The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.</p>
<h2>1. Introduction</h2>
<p>Reinforcement learning dates back to the early days of cybernetics and work in statistics, psychology, neuroscience, and computer science. In the last five to ten years, it has attracted rapidly increasing interest in the machine learning and artificial intelligence communities. Its promise is beguiling-a way of programming agents by reward and punishment without needing to specify how the task is to be achieved. But there are formidable computational obstacles to fulfilling the promise.</p>
<p>This paper surveys the historical basis of reinforcement learning and some of the current work from a computer science perspective. We give a high-level overview of the field and a taste of some specific approaches. It is, of course, impossible to mention all of the important work in the field; this should not be taken to be an exhaustive account.</p>
<p>Reinforcement learning is the problem faced by an agent that must learn behavior through trial-and-error interactions with a dynamic environment. The work described here has a strong family resemblance to eponymous work in psychology, but differs considerably in the details and in the use of the word "reinforcement." It is appropriately thought of as a class of problems, rather than as a set of techniques.</p>
<p>There are two main strategies for solving reinforcement-learning problems. The first is to search in the space of behaviors in order to find one that performs well in the environment. This approach has been taken by work in genetic algorithms and genetic programming,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The standard reinforcement-learning model.
as well as some more novel search techniques (Schmidhuber, 1996). The second is to use statistical techniques and dynamic programming methods to estimate the utility of taking actions in states of the world. This paper is devoted almost entirely to the second set of techniques because they take advantage of the special structure of reinforcement-learning problems that is not available in optimization problems in general. It is not yet clear which set of approaches is best in which circumstances.</p>
<p>The rest of this section is devoted to establishing notation and describing the basic reinforcement-learning model. Section 2 explains the trade-off between exploration and exploitation and presents some solutions to the most basic case of reinforcement-learning problems, in which we want to maximize the immediate reward. Section 3 considers the more general problem in which rewards can be delayed in time from the actions that were crucial to gaining them. Section 4 considers some classic model-free algorithms for reinforcement learning from delayed reward: adaptive heuristic critic, $T D(\lambda)$ and Q-learning. Section 5 demonstrates a continuum of algorithms that are sensitive to the amount of computation an agent can perform between actual steps of action in the environment. Generalization-the cornerstone of mainstream machine learning research-has the potential of considerably aiding reinforcement learning, as described in Section 6. Section 7 considers the problems that arise when the agent does not have complete perceptual access to the state of the environment. Section 8 catalogs some of reinforcement learning's successful applications. Finally, Section 9 concludes with some speculations about important open problems and the future of reinforcement learning.</p>
<h1>1.1 Reinforcement-Learning Model</h1>
<p>In the standard reinforcement-learning model, an agent is connected to its environment via perception and action, as depicted in Figure 1. On each step of interaction the agent receives as input, $i$, some indication of the current state, $s$, of the environment; the agent then chooses an action, $a$, to generate as output. The action changes the state of the environment, and the value of this state transition is communicated to the agent through a scalar reinforcement signal, $r$. The agent's behavior, $B$, should choose actions that tend to increase the long-run sum of values of the reinforcement signal. It can learn to do this over time by systematic trial and error, guided by a wide variety of algorithms that are the subject of later sections of this paper.</p>
<p>Formally, the model consists of</p>
<ul>
<li>a discrete set of environment states, $\mathcal{S}$;</li>
<li>a discrete set of agent actions, $\mathcal{A}$; and</li>
<li>a set of scalar reinforcement signals; typically ${0,1}$, or the real numbers.</li>
</ul>
<p>The figure also includes an input function $I$, which determines how the agent views the environment state; we will assume that it is the identity function (that is, the agent perceives the exact state of the environment) until we consider partial observability in Section 7.</p>
<p>An intuitive way to understand the relation between the agent and its environment is with the following example dialogue.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Environment:</th>
<th style="text-align: left;">You are in state 65 . You have 4 possible actions.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Agent:</td>
<td style="text-align: left;">I'll take action 2.</td>
</tr>
<tr>
<td style="text-align: left;">Environment:</td>
<td style="text-align: left;">You received a reinforcement of 7 units. You are now in state</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">15. You have 2 possible actions.</td>
</tr>
<tr>
<td style="text-align: left;">Agent:</td>
<td style="text-align: left;">I'll take action 1.</td>
</tr>
<tr>
<td style="text-align: left;">Environment:</td>
<td style="text-align: left;">You received a reinforcement of -4 units. You are now in state</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">65. You have 4 possible actions.</td>
</tr>
<tr>
<td style="text-align: left;">Agent:</td>
<td style="text-align: left;">I'll take action 2.</td>
</tr>
<tr>
<td style="text-align: left;">Environment:</td>
<td style="text-align: left;">You received a reinforcement of 5 units. You are now in state</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">44. You have 5 possible actions.</td>
</tr>
<tr>
<td style="text-align: left;">$\vdots$</td>
<td style="text-align: left;">$\vdots$</td>
</tr>
</tbody>
</table>
<p>The agent's job is to find a policy $\pi$, mapping states to actions, that maximizes some long-run measure of reinforcement. We expect, in general, that the environment will be non-deterministic; that is, that taking the same action in the same state on two different occasions may result in different next states and/or different reinforcement values. This happens in our example above: from state 65 , applying action 2 produces differing reinforcements and differing states on two occasions. However, we assume the environment is stationary; that is, that the probabilities of making state transitions or receiving specific reinforcement signals do not change over time. ${ }^{1}$</p>
<p>Reinforcement learning differs from the more widely studied problem of supervised learning in several ways. The most important difference is that there is no presentation of input/output pairs. Instead, after choosing an action the agent is told the immediate reward and the subsequent state, but is not told which action would have been in its best long-term interests. It is necessary for the agent to gather useful experience about the possible system states, actions, transitions and rewards actively to act optimally. Another difference from supervised learning is that on-line performance is important: the evaluation of the system is often concurrent with learning.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Some aspects of reinforcement learning are closely related to search and planning issues in artificial intelligence. AI search algorithms generate a satisfactory trajectory through a graph of states. Planning operates in a similar manner, but typically within a construct with more complexity than a graph, in which states are represented by compositions of logical expressions instead of atomic symbols. These AI algorithms are less general than the reinforcement-learning methods, in that they require a predefined model of state transitions, and with a few exceptions assume determinism. On the other hand, reinforcement learning, at least in the kind of discrete cases for which theory has been developed, assumes that the entire state space can be enumerated and stored in memory-an assumption to which conventional search algorithms are not tied.</p>
<h1>1.2 Models of Optimal Behavior</h1>
<p>Before we can start thinking about algorithms for learning to behave optimally, we have to decide what our model of optimality will be. In particular, we have to specify how the agent should take the future into account in the decisions it makes about how to behave now. There are three models that have been the subject of the majority of work in this area.</p>
<p>The finite-horizon model is the easiest to think about; at a given moment in time, the agent should optimize its expected reward for the next $h$ steps:</p>
<p>$$
E\left(\sum_{t=0}^{h} r_{t}\right)
$$</p>
<p>it need not worry about what will happen after that. In this and subsequent expressions, $r_{t}$ represents the scalar reward received $t$ steps into the future. This model can be used in two ways. In the first, the agent will have a non-stationary policy; that is, one that changes over time. On its first step it will take what is termed a $h$-step optimal action. This is defined to be the best action available given that it has $h$ steps remaining in which to act and gain reinforcement. On the next step it will take a $(h-1)$-step optimal action, and so on, until it finally takes a 1 -step optimal action and terminates. In the second, the agent does receding-horizon control, in which it always takes the $h$-step optimal action. The agent always acts according to the same policy, but the value of $h$ limits how far ahead it looks in choosing its actions. The finite-horizon model is not always appropriate. In many cases we may not know the precise length of the agent's life in advance.</p>
<p>The infinite-horizon discounted model takes the long-run reward of the agent into account, but rewards that are received in the future are geometrically discounted according to discount factor $\gamma$, (where $0 \leq \gamma&lt;1$ ):</p>
<p>$$
E\left(\sum_{t=0}^{\infty} \gamma^{t} r_{t}\right)
$$</p>
<p>We can interpret $\gamma$ in several ways. It can be seen as an interest rate, a probability of living another step, or as a mathematical trick to bound the infinite sum. The model is conceptually similar to receding-horizon control, but the discounted model is more mathematically tractable than the finite-horizon model. This is a dominant reason for the wide attention this model has received.</p>
<p>Another optimality criterion is the average-reward model, in which the agent is supposed to take actions that optimize its long-run average reward:</p>
<p>$$
\lim <em t="0">{h \rightarrow \infty} E\left(\frac{1}{h} \sum</em>\right)
$$}^{h} r_{t</p>
<p>Such a policy is referred to as a gain optimal policy; it can be seen as the limiting case of the infinite-horizon discounted model as the discount factor approaches 1 (Bertsekas, 1995). One problem with this criterion is that there is no way to distinguish between two policies, one of which gains a large amount of reward in the initial phases and the other of which does not. Reward gained on any initial prefix of the agent's life is overshadowed by the long-run average performance. It is possible to generalize this model so that it takes into account both the long run average and the amount of initial reward than can be gained. In the generalized, bias optimal model, a policy is preferred if it maximizes the long-run average and ties are broken by the initial extra reward.</p>
<p>Figure 2 contrasts these models of optimality by providing an environment in which changing the model of optimality changes the optimal policy. In this example, circles represent the states of the environment and arrows are state transitions. There is only a single action choice from every state except the start state, which is in the upper left and marked with an incoming arrow. All rewards are zero except where marked. Under a finite-horizon model with $h=5$, the three actions yield rewards of $+6.0,+0.0$, and +0.0 , so the first action should be chosen; under an infinite-horizon discounted model with $\gamma=0.9$, the three choices yield $+16.2,+59.0$, and +58.5 so the second action should be chosen; and under the average reward model, the third action should be chosen since it leads to an average reward of +11 . If we change $h$ to 1000 and $\gamma$ to 0.2 , then the second action is optimal for the finite-horizon model and the first for the infinite-horizon discounted model; however, the average reward model will always prefer the best long-term average. Since the choice of optimality model and parameters matters so much, it is important to choose it carefully in any application.</p>
<p>The finite-horizon model is appropriate when the agent's lifetime is known; one important aspect of this model is that as the length of the remaining lifetime decreases, the agent's policy may change. A system with a hard deadline would be appropriately modeled this way. The relative usefulness of infinite-horizon discounted and bias-optimal models is still under debate. Bias-optimality has the advantage of not requiring a discount parameter; however, algorithms for finding bias-optimal policies are not yet as well-understood as those for finding optimal infinite-horizon discounted policies.</p>
<h1>1.3 Measuring Learning Performance</h1>
<p>The criteria given in the previous section can be used to assess the policies learned by a given algorithm. We would also like to be able to evaluate the quality of learning itself. There are several incompatible measures in use.</p>
<ul>
<li>Eventual convergence to optimal. Many algorithms come with a provable guarantee of asymptotic convergence to optimal behavior (Watkins \&amp; Dayan, 1992). This is reassuring, but useless in practical terms. An agent that quickly reaches a plateau</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparing models of optimality. All unlabeled arrows produce a reward of zero.
at $99 \%$ of optimality may, in many applications, be preferable to an agent that has a guarantee of eventual optimality but a sluggish early learning rate.</p>
<ul>
<li>Speed of convergence to optimality. Optimality is usually an asymptotic result, and so convergence speed is an ill-defined measure. More practical is the speed of convergence to near-optimality. This measure begs the definition of how near to optimality is sufficient. A related measure is level of performance after a given time, which similarly requires that someone define the given time.</li>
</ul>
<p>It should be noted that here we have another difference between reinforcement learning and conventional supervised learning. In the latter, expected future predictive accuracy or statistical efficiency are the prime concerns. For example, in the well-known PAC framework (Valiant, 1984), there is a learning period during which mistakes do not count, then a performance period during which they do. The framework provides bounds on the necessary length of the learning period in order to have a probabilistic guarantee on the subsequent performance. That is usually an inappropriate view for an agent with a long existence in a complex environment.</p>
<p>In spite of the mismatch between embedded reinforcement learning and the train/test perspective, Fiechter (1994) provides a PAC analysis for Q-learning (described in Section 4.2) that sheds some light on the connection between the two views.</p>
<p>Measures related to speed of learning have an additional weakness. An algorithm that merely tries to achieve optimality as fast as possible may incur unnecessarily large penalties during the learning period. A less aggressive strategy taking longer to achieve optimality, but gaining greater total reinforcement during its learning might be preferable.</p>
<ul>
<li>Regret. A more appropriate measure, then, is the expected decrease in reward gained due to executing the learning algorithm instead of behaving optimally from the very beginning. This measure is known as regret (Berry \&amp; Fristedt, 1985). It penalizes mistakes wherever they occur during the run. Unfortunately, results concerning the regret of algorithms are quite hard to obtain.</li>
</ul>
<h1>1.4 Reinforcement Learning and Adaptive Control</h1>
<p>Adaptive control (Burghes \&amp; Graham, 1980; Stengel, 1986) is also concerned with algorithms for improving a sequence of decisions from experience. Adaptive control is a much more mature discipline that concerns itself with dynamic systems in which states and actions are vectors and system dynamics are smooth: linear or locally linearizable around a desired trajectory. A very common formulation of cost functions in adaptive control are quadratic penalties on deviation from desired state and action vectors. Most importantly, although the dynamic model of the system is not known in advance, and must be estimated from data, the structure of the dynamic model is fixed, leaving model estimation as a parameter estimation problem. These assumptions permit deep, elegant and powerful mathematical analysis, which in turn lead to robust, practical, and widely deployed adaptive control algorithms.</p>
<h2>2. Exploitation versus Exploration: The Single-State Case</h2>
<p>One major difference between reinforcement learning and supervised learning is that a reinforcement-learner must explicitly explore its environment. In order to highlight the problems of exploration, we treat a very simple case in this section. The fundamental issues and approaches described here will, in many cases, transfer to the more complex instances of reinforcement learning discussed later in the paper.</p>
<p>The simplest possible reinforcement-learning problem is known as the $k$-armed bandit problem, which has been the subject of a great deal of study in the statistics and applied mathematics literature (Berry \&amp; Fristedt, 1985). The agent is in a room with a collection of $k$ gambling machines (each called a "one-armed bandit" in colloquial English). The agent is permitted a fixed number of pulls, $h$. Any arm may be pulled on each turn. The machines do not require a deposit to play; the only cost is in wasting a pull playing a suboptimal machine. When arm $i$ is pulled, machine $i$ pays off 1 or 0 , according to some underlying probability parameter $p_{i}$, where payoffs are independent events and the $p_{i} \mathrm{~s}$ are unknown. What should the agent's strategy be?</p>
<p>This problem illustrates the fundamental tradeoff between exploitation and exploration. The agent might believe that a particular arm has a fairly high payoff probability; should it choose that arm all the time, or should it choose another one that it has less information about, but seems to be worse? Answers to these questions depend on how long the agent is expected to play the game; the longer the game lasts, the worse the consequences of prematurely converging on a sub-optimal arm, and the more the agent should explore.</p>
<p>There is a wide variety of solutions to this problem. We will consider a representative selection of them, but for a deeper discussion and a number of important theoretical results, see the book by Berry and Fristedt (1985). We use the term "action" to indicate the agent's choice of arm to pull. This eases the transition into delayed reinforcement models in Section 3. It is very important to note that bandit problems fit our definition of a reinforcement-learning environment with a single state with only self transitions.</p>
<p>Section 2.1 discusses three solutions to the basic one-state bandit problem that have formal correctness results. Although they can be extended to problems with real-valued rewards, they do not apply directly to the general multi-state delayed-reinforcement case.</p>
<p>Section 2.2 presents three techniques that are not formally justified, but that have had wide use in practice, and can be applied (with similar lack of guarantee) to the general case.</p>
<h1>2.1 Formally Justified Techniques</h1>
<p>There is a fairly well-developed formal theory of exploration for very simple problems. Although it is instructive, the methods it provides do not scale well to more complex problems.</p>
<h3>2.1.1 Dynamic-Programming Approach</h3>
<p>If the agent is going to be acting for a total of $h$ steps, it can use basic Bayesian reasoning to solve for an optimal strategy (Berry \&amp; Fristedt, 1985). This requires an assumed prior joint distribution for the parameters $\left{p_{i}\right}$, the most natural of which is that each $p_{i}$ is independently uniformly distributed between 0 and 1 . We compute a mapping from belief states (summaries of the agent's experiences during this run) to actions. Here, a belief state can be represented as a tabulation of action choices and payoffs: $\left{n_{1}, w_{1}, n_{2}, w_{2}, \ldots, n_{k}, w_{k}\right}$ denotes a state of play in which each arm $i$ has been pulled $n_{i}$ times with $w_{i}$ payoffs. We write $V^{*}\left(n_{1}, w_{1}, \ldots, n_{k}, w_{k}\right)$ as the expected payoff remaining, given that a total of $h$ pulls are available, and we use the remaining pulls optimally.</p>
<p>If $\sum_{i} n_{i}=h$, then there are no remaining pulls, and $V^{<em>}\left(n_{1}, w_{1}, \ldots, n_{k}, w_{k}\right)=0$. This is the basis of a recursive definition. If we know the $V^{</em>}$ value for all belief states with $t$ pulls remaining, we can compute the $V^{*}$ value of any belief state with $t+1$ pulls remaining:</p>
<p>$$
\begin{aligned}
V^{<em>}\left(n_{1}, w_{1}, \ldots, n_{k}, w_{k}\right) &amp; =\max <em i="i">{i} E\left[\begin{array}{c}
\text { Future payoff if agent takes action } i, \
\text { then acts optimally for remaining pulls }
\end{array}\right] \
&amp; =\max </em> V^{}\binom{\rho_{i</em>}\left(n_{1}, w_{i}, \ldots, n_{i}+1, w_{i}+1, \ldots, n_{k}, w_{k}\right)+}{\left(1-\rho_{i}\right) V^{*}\left(n_{1}, w_{i}, \ldots, n_{i}+1, w_{i}, \ldots, n_{k}, w_{k}\right)}
\end{aligned}
$$</p>
<p>where $\rho_{i}$ is the posterior subjective probability of action $i$ paying off given $n_{i}, w_{i}$ and our prior probability. For the uniform priors, which result in a beta distribution, $\rho_{i}=$ $\left(w_{i}+1\right) /\left(n_{i}+2\right)$.</p>
<p>The expense of filling in the table of $V^{*}$ values in this way for all attainable belief states is linear in the number of belief states times actions, and thus exponential in the horizon.</p>
<h3>2.1.2 Gittins Allocation Indices</h3>
<p>Gittins gives an "allocation index" method for finding the optimal choice of action at each step in $k$-armed bandit problems (Gittins, 1989). The technique only applies under the discounted expected reward criterion. For each action, consider the number of times it has been chosen, $n$, versus the number of times it has paid off, $w$. For certain discount factors, there are published tables of "index values," $I(n, w)$ for each pair of $n$ and $w$. Look up the index value for each action $i, I\left(n_{i}, w_{i}\right)$. It represents a comparative measure of the combined value of the expected payoff of action $i$ (given its history of payoffs) and the value of the information that we would get by choosing it. Gittins has shown that choosing the action with the largest index value guarantees the optimal balance between exploration and exploitation.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: A Tsetlin automaton with $2 N$ states. The top row shows the state transitions that are made when the previous action resulted in a reward of 1 ; the bottom row shows transitions after a reward of 0 . In states in the left half of the figure, action 0 is taken; in those on the right, action 1 is taken.</p>
<p>Because of the guarantee of optimal exploration and the simplicity of the technique (given the table of index values), this approach holds a great deal of promise for use in more complex applications. This method proved useful in an application to robotic manipulation with immediate reward (Salganicoff \&amp; Ungar, 1995). Unfortunately, no one has yet been able to find an analog of index values for delayed reinforcement problems.</p>
<h1>2.1.3 Learning Automata</h1>
<p>A branch of the theory of adaptive control is devoted to learning automata, surveyed by Narendra and Thathachar (1989), which were originally described explicitly as finite state automata. The Tsetlin automaton shown in Figure 3 provides an example that solves a 2 -armed bandit arbitrarily near optimally as $N$ approaches infinity.</p>
<p>It is inconvenient to describe algorithms as finite-state automata, so a move was made to describe the internal state of the agent as a probability distribution according to which actions would be chosen. The probabilities of taking different actions would be adjusted according to their previous successes and failures.</p>
<p>An example, which stands among a set of algorithms independently developed in the mathematical psychology literature (Hilgard \&amp; Bower, 1975), is the linear reward-inaction algorithm. Let $p_{i}$ be the agent's probability of taking action $i$.</p>
<ul>
<li>When action $a_{i}$ succeeds,</li>
</ul>
<p>$$
\begin{aligned}
p_{i} &amp; :=p_{i}+\alpha\left(1-p_{i}\right) \
p_{j} &amp; :=p_{j}-\alpha p_{j} \text { for } j \neq i
\end{aligned}
$$</p>
<ul>
<li>When action $a_{i}$ fails, $p_{j}$ remains unchanged (for all $j$ ).</li>
</ul>
<p>This algorithm converges with probability 1 to a vector containing a single 1 and the rest 0 's (choosing a particular action with probability 1). Unfortunately, it does not always converge to the correct action; but the probability that it converges to the wrong one can be made arbitrarily small by making $\alpha$ small (Narendra \&amp; Thathachar, 1974). There is no literature on the regret of this algorithm.</p>
<h1>2.2 Ad-Hoc Techniques</h1>
<p>In reinforcement-learning practice, some simple, ad hoc strategies have been popular. They are rarely, if ever, the best choice for the models of optimality we have used, but they may be viewed as reasonable, computationally tractable, heuristics. Thrun (1992) has surveyed a variety of these techniques.</p>
<h3>2.2.1 Greedy Strategies</h3>
<p>The first strategy that comes to mind is to always choose the action with the highest estimated payoff. The flaw is that early unlucky sampling might indicate that the best action's reward is less than the reward obtained from a suboptimal action. The suboptimal action will always be picked, leaving the true optimal action starved of data and its superiority never discovered. An agent must explore to ameliorate this outcome.</p>
<p>A useful heuristic is optimism in the face of uncertainty in which actions are selected greedily, but strongly optimistic prior beliefs are put on their payoffs so that strong negative evidence is needed to eliminate an action from consideration. This still has a measurable danger of starving an optimal but unlucky action, but the risk of this can be made arbitrarily small. Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method (Kaelbling, 1993b) (described shortly), the exploration bonus in Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a), and the exploration mechanism in prioritized sweeping (Moore \&amp; Atkeson, 1993).</p>
<h3>2.2.2 Randomized Strategies</h3>
<p>Another simple exploration strategy is to take the action with the best estimated expected reward by default, but with probability $p$, choose an action at random. Some versions of this strategy start with a large value of $p$ to encourage initial exploration, which is slowly decreased.</p>
<p>An objection to the simple strategy is that when it experiments with a non-greedy action it is no more likely to try a promising alternative than a clearly hopeless alternative. A slightly more sophisticated strategy is Boltzmann exploration. In this case, the expected reward for taking action $a, E R(a)$ is used to choose an action probabilistically according to the distribution</p>
<p>$$
P(a)=\frac{e^{E R(a) / T}}{\sum_{a^{\prime} \in A} e^{E R\left(a^{\prime}\right) / T}}
$$</p>
<p>The temperature parameter $T$ can be decreased over time to decrease exploration. This method works well if the best action is well separated from the others, but suffers somewhat when the values of the actions are close. It may also converge unnecessarily slowly unless the temperature schedule is manually tuned with great care.</p>
<h3>2.2.3 Interval-based Techniques</h3>
<p>Exploration is often more efficient when it is based on second-order information about the certainty or variance of the estimated values of actions. Kaelbling's interval estimation algorithm (1993b) stores statistics for each action $a_{i}: w_{i}$ is the number of successes and $n_{i}$ the number of trials. An action is chosen by computing the upper bound of a $100 \cdot(1-\alpha) \%$</p>
<p>confidence interval on the success probability of each action and choosing the action with the highest upper bound. Smaller values of the $\alpha$ parameter encourage greater exploration. When payoffs are boolean, the normal approximation to the binomial distribution can be used to construct the confidence interval (though the binomial should be used for small $n$ ). Other payoff distributions can be handled using their associated statistics or with nonparametric methods. The method works very well in empirical trials. It is also related to a certain class of statistical techniques known as experiment design methods (Box \&amp; Draper, 1987), which are used for comparing multiple treatments (for example, fertilizers or drugs) to determine which treatment (if any) is best in as small a set of experiments as possible.</p>
<h1>2.3 More General Problems</h1>
<p>When there are multiple states, but reinforcement is still immediate, then any of the above solutions can be replicated, once for each state. However, when generalization is required, these solutions must be integrated with generalization methods (see section 6); this is straightforward for the simple ad-hoc methods, but it is not understood how to maintain theoretical guarantees.</p>
<p>Many of these techniques focus on converging to some regime in which exploratory actions are taken rarely or never; this is appropriate when the environment is stationary. However, when the environment is non-stationary, exploration must continue to take place, in order to notice changes in the world. Again, the more ad-hoc techniques can be modified to deal with this in a plausible manner (keep temperature parameters from going to 0 ; decay the statistics in interval estimation), but none of the theoretically guaranteed methods can be applied.</p>
<h2>3. Delayed Reward</h2>
<p>In the general case of the reinforcement learning problem, the agent's actions determine not only its immediate reward, but also (at least probabilistically) the next state of the environment. Such environments can be thought of as networks of bandit problems, but the agent must take into account the next state as well as the immediate reward when it decides which action to take. The model of long-run optimality the agent is using determines exactly how it should take the value of the future into account. The agent will have to be able to learn from delayed reinforcement: it may take a long sequence of actions, receiving insignificant reinforcement, then finally arrive at a state with high reinforcement. The agent must be able to learn which of its actions are desirable based on reward that can take place arbitrarily far in the future.</p>
<h3>3.1 Markov Decision Processes</h3>
<p>Problems with delayed reinforcement are well modeled as Markov decision processes (MDPs). An MDP consists of</p>
<ul>
<li>a set of states $\mathcal{S}$,</li>
<li>
<p>a set of actions $\mathcal{A}$,</p>
</li>
<li>
<p>a reward function $R: \mathcal{S} \times \mathcal{A} \rightarrow \Re$, and</p>
</li>
<li>a state transition function $T: \mathcal{S} \times \mathcal{A} \rightarrow \Pi(\mathcal{S})$, where a member of $\Pi(\mathcal{S})$ is a probability distribution over the set $\mathcal{S}$ (i.e. it maps states to probabilities). We write $T\left(s, a, s^{\prime}\right)$ for the probability of making a transition from state $s$ to state $s^{\prime}$ using action $a$.
The state transition function probabilistically specifies the next state of the environment as a function of its current state and the agent's action. The reward function specifies expected instantaneous reward as a function of the current state and action. The model is Markov if the state transitions are independent of any previous environment states or agent actions. There are many good references to MDP models (Bellman, 1957; Bertsekas, 1987; Howard, 1960; Puterman, 1994).</li>
</ul>
<p>Although general MDPs may have infinite (even uncountable) state and action spaces, we will only discuss methods for solving finite-state and finite-action problems. In section 6, we discuss methods for solving problems with continuous input and output spaces.</p>
<h1>3.2 Finding a Policy Given a Model</h1>
<p>Before we consider algorithms for learning to behave in MDP environments, we will explore techniques for determining the optimal policy given a correct model. These dynamic programming techniques will serve as the foundation and inspiration for the learning algorithms to follow. We restrict our attention mainly to finding optimal policies for the infinite-horizon discounted model, but most of these algorithms have analogs for the finitehorizon and average-case models as well. We rely on the result that, for the infinite-horizon discounted model, there exists an optimal deterministic stationary policy (Bellman, 1957).</p>
<p>We will speak of the optimal value of a state-it is the expected infinite discounted sum of reward that the agent will gain if it starts in that state and executes the optimal policy. Using $\pi$ as a complete decision policy, it is written</p>
<p>$$
V^{*}(s)=\max <em t="0">{\pi} E\left(\sum</em>\right)
$$}^{\infty} \gamma^{t} r_{t</p>
<p>This optimal value function is unique and can be defined as the solution to the simultaneous equations</p>
<p>$$
V^{<em>}(s)=\max <em s_prime="s^{\prime">{a}\left(R(s, a)+\gamma \sum</em>\right) V^{} \in \mathcal{S}} T\left(s, a, s^{\prime</em>}\left(s^{\prime}\right)\right), \forall s \in \mathcal{S}
$$</p>
<p>which assert that the value of a state $s$ is the expected instantaneous reward plus the expected discounted value of the next state, using the best available action. Given the optimal value function, we can specify the optimal policy as</p>
<p>$$
\pi^{<em>}(s)=\arg \max <em s_prime="s^{\prime">{a}\left(R(s, a)+\gamma \sum</em>\right) V^{} \in \mathcal{S}} T\left(s, a, s^{\prime</em>}\left(s^{\prime}\right)\right)
$$</p>
<h3>3.2.1 Value Iteration</h3>
<p>One way, then, to find an optimal policy is to find the optimal value function. It can be determined by a simple iterative algorithm called value iteration that can be shown to converge to the correct $V^{*}$ values (Bellman, 1957; Bertsekas, 1987).</p>
<div class="codehilite"><pre><span></span><code><span class="n">initialize</span><span class="w"> </span><span class="n">V</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="w"> </span><span class="n">arbitrarily</span>
<span class="n">loop</span><span class="w"> </span><span class="n">until</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="n">good</span><span class="w"> </span><span class="n">enough</span>
<span class="w">    </span><span class="n">loop</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">s</span><span class="p">\</span><span class="k">in</span><span class="p">\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">S</span><span class="p">}</span>
<span class="w">        </span><span class="n">loop</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">a</span><span class="p">\</span><span class="n">inA</span>
<span class="w">            </span><span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="o">,</span><span class="n">a</span><span class="p">)</span><span class="o">:=</span><span class="n">R</span><span class="p">(</span><span class="n">s</span><span class="o">,</span><span class="n">a</span><span class="p">)</span><span class="o">+</span><span class="p">\</span><span class="n">gamma</span><span class="p">\</span><span class="n">sum_</span><span class="p">{</span><span class="n">s</span><span class="o">^</span><span class="p">{\</span><span class="n">prime</span><span class="p">}\</span><span class="k">in</span><span class="w"> </span><span class="p">\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">S</span><span class="p">}}</span><span class="w"> </span><span class="n">T</span><span class="p">(</span><span class="n">s</span><span class="o">,</span><span class="n">a</span><span class="o">,</span><span class="n">s</span><span class="o">^</span><span class="p">{\</span><span class="n">prime</span><span class="p">})</span><span class="n">V</span><span class="p">(</span><span class="n">s</span><span class="o">^</span><span class="p">{\</span><span class="n">prime</span><span class="p">})</span>
<span class="w">            </span><span class="n">V</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:=</span><span class="p">\</span><span class="nb">max</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="o">,</span><span class="n">a</span><span class="p">)</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="n">loop</span>
<span class="k">end</span><span class="w"> </span><span class="n">loop</span>
</code></pre></div>

<p>It is not obvious when to stop the value iteration algorithm. One important result bounds the performance of the current greedy policy as a function of the Bellman residual of the current value function (Williams \&amp; Baird, 1993b). It says that if the maximum difference between two successive value functions is less than $\epsilon$, then the value of the greedy policy, (the policy obtained by choosing, in every state, the action that maximizes the estimated discounted reward, using the current estimate of the value function) differs from the value function of the optimal policy by no more than $2 \epsilon \gamma /(1-\gamma)$ at any state. This provides an effective stopping criterion for the algorithm. Puterman (1994) discusses another stopping criterion, based on the span semi-norm, which may result in earlier termination. Another important result is that the greedy policy is guaranteed to be optimal in some finite number of steps even though the value function may not have converged (Bertsekas, 1987). And in practice, the greedy policy is often optimal long before the value function has converged.</p>
<p>Value iteration is very flexible. The assignments to $V$ need not be done in strict order as shown above, but instead can occur asynchronously in parallel provided that the value of every state gets updated infinitely often on an infinite run. These issues are treated extensively by Bertsekas (1989), who also proves convergence results.</p>
<p>Updates based on Equation 1 are known as full backups since they make use of information from all possible successor states. It can be shown that updates of the form</p>
<p>$$
Q(s, a):=Q(s, a)+\alpha\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right)
$$</p>
<p>can also be used as long as each pairing of $a$ and $s$ is updated infinitely often, $s^{\prime}$ is sampled from the distribution $T\left(s, a, s^{\prime}\right), r$ is sampled with mean $R(s, a)$ and bounded variance, and the learning rate $\alpha$ is decreased slowly. This type of sample backup (Singh, 1993) is critical to the operation of the model-free methods discussed in the next section.</p>
<p>The computational complexity of the value-iteration algorithm with full backups, per iteration, is quadratic in the number of states and linear in the number of actions. Commonly, the transition probabilities $T\left(s, a, s^{\prime}\right)$ are sparse. If there are on average a constant number of next states with non-zero probability then the cost per iteration is linear in the number of states and linear in the number of actions. The number of iterations required to reach the optimal value function is polynomial in the number of states and the magnitude of the largest reward if the discount factor is held constant. However, in the worst case the number of iterations grows polynomially in $1 /(1-\gamma)$, so the convergence rate slows considerably as the discount factor approaches 1 (Littman, Dean, \&amp; Kaelbling, 1995b).</p>
<h1>3.2.2 Policy Iteration</h1>
<p>The policy iteration algorithm manipulates the policy directly, rather than finding it indirectly via the optimal value function. It operates as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="n">choose</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">arbitrary</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="p">\(\</span><span class="n">pi</span><span class="o">^</span><span class="p">{\</span><span class="n">prime</span><span class="p">}\)</span>
<span class="n">loop</span>
<span class="w">    </span><span class="p">\(\</span><span class="n">pi</span><span class="o">:=</span><span class="p">\</span><span class="n">pi</span><span class="o">^</span><span class="p">{\</span><span class="n">prime</span><span class="p">}\)</span>
<span class="w">    </span><span class="n">compute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">function</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="p">\(\</span><span class="n">pi</span><span class="p">\)</span><span class="w"> </span><span class="o">:</span>
<span class="w">        </span><span class="n">solve</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">linear</span><span class="w"> </span><span class="n">equations</span>
<span class="w">            </span><span class="p">\(</span><span class="n">V_</span><span class="p">{\</span><span class="n">pi</span><span class="p">}(</span><span class="n">s</span><span class="p">)</span><span class="o">=</span><span class="n">R</span><span class="p">(</span><span class="n">s</span><span class="o">,</span><span class="w"> </span><span class="p">\</span><span class="n">pi</span><span class="p">(</span><span class="n">s</span><span class="p">))</span><span class="o">+</span><span class="p">\</span><span class="n">gamma</span><span class="w"> </span><span class="p">\</span><span class="n">sum_</span><span class="p">{</span><span class="n">s</span><span class="o">^</span><span class="p">{\</span><span class="n">prime</span><span class="p">}</span><span class="w"> </span><span class="p">\</span><span class="k">in</span><span class="w"> </span><span class="p">\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">S</span><span class="p">}}</span><span class="w"> </span><span class="n">T</span><span class="p">\</span><span class="n">left</span><span class="p">(</span><span class="n">s</span><span class="o">,</span><span class="w"> </span><span class="p">\</span><span class="n">pi</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">,</span><span class="w"> </span><span class="n">s</span><span class="o">^</span><span class="p">{\</span><span class="n">prime</span><span class="p">}\</span><span class="n">right</span><span class="p">)</span><span class="w"> </span><span class="n">V_</span><span class="p">{\</span><span class="n">pi</span><span class="p">}\</span><span class="n">left</span><span class="p">(</span><span class="n">s</span><span class="o">^</span><span class="p">{\</span><span class="n">prime</span><span class="p">}\</span><span class="n">right</span><span class="p">)\)</span>
<span class="w">    </span><span class="n">improve</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">state</span><span class="o">:</span>
<span class="w">        </span><span class="p">\(\</span><span class="n">pi</span><span class="o">^</span><span class="p">{\</span><span class="n">prime</span><span class="p">}(</span><span class="n">s</span><span class="p">)</span><span class="o">:=</span><span class="p">\</span><span class="n">arg</span><span class="w"> </span><span class="p">\</span><span class="nb">max</span><span class="w"> </span><span class="n">_</span><span class="p">{</span><span class="n">a</span><span class="p">}\</span><span class="n">left</span><span class="p">(</span><span class="n">R</span><span class="p">(</span><span class="n">s</span><span class="o">,</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="o">+</span><span class="p">\</span><span class="n">gamma</span><span class="w"> </span><span class="p">\</span><span class="n">sum_</span><span class="p">{</span><span class="n">s</span><span class="o">^</span><span class="p">{\</span><span class="n">prime</span><span class="p">}</span><span class="w"> </span><span class="p">\</span><span class="k">in</span><span class="w"> </span><span class="p">\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">S</span><span class="p">}}</span><span class="w"> </span><span class="n">T</span><span class="p">\</span><span class="n">left</span><span class="p">(</span><span class="n">s</span><span class="o">,</span><span class="w"> </span><span class="n">a</span><span class="o">,</span><span class="w"> </span><span class="n">s</span><span class="o">^</span><span class="p">{\</span><span class="n">prime</span><span class="p">}\</span><span class="n">right</span><span class="p">)</span><span class="w"> </span><span class="n">V_</span><span class="p">{\</span><span class="n">pi</span><span class="p">}\</span><span class="n">left</span><span class="p">(</span><span class="n">s</span><span class="o">^</span><span class="p">{\</span><span class="n">prime</span><span class="p">}\</span><span class="n">right</span><span class="p">)\</span><span class="n">right</span><span class="p">)\)</span>
<span class="n">until</span><span class="w"> </span><span class="p">\(\</span><span class="n">pi</span><span class="o">=</span><span class="p">\</span><span class="n">pi</span><span class="o">^</span><span class="p">{\</span><span class="n">prime</span><span class="p">}\)</span>
</code></pre></div>

<p>The value function of a policy is just the expected infinite discounted reward that will be gained, at each state, by executing that policy. It can be determined by solving a set of linear equations. Once we know the value of each state under the current policy, we consider whether the value could be improved by changing the first action taken. If it can, we change the policy to take the new action whenever it is in that situation. This step is guaranteed to strictly improve the performance of the policy. When no improvements are possible, then the policy is guaranteed to be optimal.</p>
<p>Since there are at most $|\mathcal{A}|^{|\mathcal{S}|}$ distinct policies, and the sequence of policies improves at each step, this algorithm terminates in at most an exponential number of iterations (Puterman, 1994). However, it is an important open question how many iterations policy iteration takes in the worst case. It is known that the running time is pseudopolynomial and that for any fixed discount factor, there is a polynomial bound in the total size of the MDP (Littman et al., 1995b).</p>
<h3>3.2.3 Enhancement to Value Iteration and Policy Iteration</h3>
<p>In practice, value iteration is much faster per iteration, but policy iteration takes fewer iterations. Arguments have been put forth to the effect that each approach is better for large problems. Puterman's modified policy iteration algorithm (Puterman \&amp; Shin, 1978) provides a method for trading iteration time for iteration improvement in a smoother way. The basic idea is that the expensive part of policy iteration is solving for the exact value of $V_{\pi}$. Instead of finding an exact value for $V_{\pi}$, we can perform a few steps of a modified value-iteration step where the policy is held fixed over successive iterations. This can be shown to produce an approximation to $V_{\pi}$ that converges linearly in $\gamma$. In practice, this can result in substantial speedups.</p>
<p>Several standard numerical-analysis techniques that speed the convergence of dynamic programming can be used to accelerate value and policy iteration. Multigrid methods can be used to quickly seed a good initial approximation to a high resolution value function by initially performing value iteration at a coarser resolution (Rüde, 1993). State aggregation works by collapsing groups of states to a single meta-state solving the abstracted problem (Bertsekas \&amp; Castañon, 1989).</p>
<h1>3.2.4 Computational Complexity</h1>
<p>Value iteration works by producing successive approximations of the optimal value function. Each iteration can be performed in $O(|A||S|^{2})$ steps, or faster if there is sparsity in the transition function. However, the number of iterations required can grow exponentially in the discount factor (Condon, 1992); as the discount factor approaches 1, the decisions must be based on results that happen farther and farther into the future. In practice, policy iteration converges in fewer iterations than value iteration, although the per-iteration costs of $O\left(|A||S|^{2}+|S|^{3}\right)$ can be prohibitive. There is no known tight worst-case bound available for policy iteration (Littman et al., 1995b). Modified policy iteration (Puterman \&amp; Shin, 1978) seeks a trade-off between cheap and effective iterations and is preferred by some practictioners (Rust, 1996).</p>
<p>Linear programming (Schrijver, 1986) is an extremely general problem, and MDPs can be solved by general-purpose linear-programming packages (Derman, 1970; D'Epenoux, 1963; Hoffman \&amp; Karp, 1966). An advantage of this approach is that commercial-quality linear-programming packages are available, although the time and space requirements can still be quite high. From a theoretic perspective, linear programming is the only known algorithm that can solve MDPs in polynomial time, although the theoretically efficient algorithms have not been shown to be efficient in practice.</p>
<h2>4. Learning an Optimal Policy: Model-free Methods</h2>
<p>In the previous section we reviewed methods for obtaining an optimal policy for an MDP assuming that we already had a model. The model consists of knowledge of the state transition probability function $T\left(s, a, s^{\prime}\right)$ and the reinforcement function $R(s, a)$. Reinforcement learning is primarily concerned with how to obtain the optimal policy when such a model is not known in advance. The agent must interact with its environment directly to obtain information which, by means of an appropriate algorithm, can be processed to produce an optimal policy.</p>
<p>At this point, there are two ways to proceed.</p>
<ul>
<li>Model-free: Learn a controller without learning a model.</li>
<li>Model-based: Learn a model, and use it to derive a controller.</li>
</ul>
<p>Which approach is better? This is a matter of some debate in the reinforcement-learning community. A number of algorithms have been proposed on both sides. This question also appears in other fields, such as adaptive control, where the dichotomy is between direct and indirect adaptive control.</p>
<p>This section examines model-free learning, and Section 5 examines model-based methods.</p>
<p>The biggest problem facing a reinforcement-learning agent is temporal credit assignment. How do we know whether the action just taken is a good one, when it might have farreaching effects? One strategy is to wait until the "end" and reward the actions taken if the result was good and punish them if the result was bad. In ongoing tasks, it is difficult to know what the "end" is, and this might require a great deal of memory. Instead, we will use insights from value iteration to adjust the estimated value of a state based on</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Architecture for the adaptive heuristic critic.
the immediate reward and the estimated value of the next state. This class of algorithms is known as temporal difference methods (Sutton, 1988). We will consider two different temporal-difference learning strategies for the discounted infinite-horizon model.</p>
<h1>4.1 Adaptive Heuristic Critic and $\operatorname{TD}(\lambda)$</h1>
<p>The adaptive heuristic critic algorithm is an adaptive version of policy iteration (Barto, Sutton, \&amp; Anderson, 1983) in which the value-function computation is no longer implemented by solving a set of linear equations, but is instead computed by an algorithm called $T D(0)$. A block diagram for this approach is given in Figure 4. It consists of two components: a critic (labeled AHC), and a reinforcement-learning component (labeled RL). The reinforcement-learning component can be an instance of any of the $k$-armed bandit algorithms, modified to deal with multiple states and non-stationary rewards. But instead of acting to maximize instantaneous reward, it will be acting to maximize the heuristic value, $v$, that is computed by the critic. The critic uses the real external reinforcement signal to learn to map states to their expected discounted values given that the policy being executed is the one currently instantiated in the RL component.</p>
<p>We can see the analogy with modified policy iteration if we imagine these components working in alternation. The policy $\pi$ implemented by RL is fixed and the critic learns the value function $V_{\pi}$ for that policy. Now we fix the critic and let the RL component learn a new policy $\pi^{\prime}$ that maximizes the new value function, and so on. In most implementations, however, both components operate simultaneously. Only the alternating implementation can be guaranteed to converge to the optimal policy, under appropriate conditions. Williams and Baird explored the convergence properties of a class of AHC-related algorithms they call "incremental variants of policy iteration" (Williams \&amp; Baird, 1993a).</p>
<p>It remains to explain how the critic can learn the value of a policy. We define $\left\langle s, a, r, s^{\prime}\right\rangle$ to be an experience tuple summarizing a single transition in the environment. Here $s$ is the agent's state before the transition, $a$ is its choice of action, $r$ the instantaneous reward it receives, and $s^{\prime}$ its resulting state. The value of a policy is learned using Sutton's $T D(0)$ algorithm (Sutton, 1988) which uses the update rule</p>
<p>$$
V(s):=V(s)+\alpha\left(r+\gamma V\left(s^{\prime}\right)-V(s)\right)
$$</p>
<p>Whenever a state $s$ is visited, its estimated value is updated to be closer to $r+\gamma V\left(s^{\prime}\right)$, since $r$ is the instantaneous reward received and $V\left(s^{\prime}\right)$ is the estimated value of the actually occurring next state. This is analogous to the sample-backup rule from value iteration-the only difference is that the sample is drawn from the real world rather than by simulating a known model. The key idea is that $r+\gamma V\left(s^{\prime}\right)$ is a sample of the value of $V(s)$, and it is</p>
<p>more likely to be correct because it incorporates the real $r$. If the learning rate $\alpha$ is adjusted properly (it must be slowly decreased) and the policy is held fixed, $T D(0)$ is guaranteed to converge to the optimal value function.</p>
<p>The $T D(0)$ rule as presented above is really an instance of a more general class of algorithms called $T D(\lambda)$, with $\lambda=0 . T D(0)$ looks only one step ahead when adjusting value estimates; although it will eventually arrive at the correct answer, it can take quite a while to do so. The general $T D(\lambda)$ rule is similar to the $T D(0)$ rule given above,</p>
<p>$$
V(u):=V(u)+\alpha\left(r+\gamma V\left(s^{\prime}\right)-V(s)\right) e(u)
$$</p>
<p>but it is applied to every state according to its eligibility $e(u)$, rather than just to the immediately previous state, $s$. One version of the eligibility trace is defined to be</p>
<p>$$
e(s)=\sum_{k=1}^{t}(\lambda \gamma)^{t-k} \delta_{s, s_{k}}, \text { where } \delta_{s, s_{k}}= \begin{cases}1 \text { if } s=s_{k} \ 0 \text { otherwise }\end{cases}
$$</p>
<p>The eligibility of a state $s$ is the degree to which it has been visited in the recent past; when a reinforcement is received, it is used to update all the states that have been recently visited, according to their eligibility. When $\lambda=0$ this is equivalent to $T D(0)$. When $\lambda=1$, it is roughly equivalent to updating all the states according to the number of times they were visited by the end of a run. Note that we can update the eligibility online as follows:</p>
<p>$$
e(s):= \begin{cases}\gamma \lambda e(s)+1 &amp; \text { if } s=\text { current state } \ \gamma \lambda e(s) &amp; \text { otherwise }\end{cases}
$$</p>
<p>It is computationally more expensive to execute the general $T D(\lambda)$, though it often converges considerably faster for large $\lambda$ (Dayan, 1992; Dayan \&amp; Sejnowski, 1994). There has been some recent work on making the updates more efficient (Cichosz \&amp; Mulawka, 1995) and on changing the definition to make $T D(\lambda)$ more consistent with the certainty-equivalent method (Singh \&amp; Sutton, 1996), which is discussed in Section 5.1.</p>
<h1>4.2 Q-learning</h1>
<p>The work of the two components of AHC can be accomplished in a unified manner by Watkins' Q-learning algorithm (Watkins, 1989; Watkins \&amp; Dayan, 1992). Q-learning is typically easier to implement. In order to understand Q-learning, we have to develop some additional notation. Let $Q^{<em>}(s, a)$ be the expected discounted reinforcement of taking action $a$ in state $s$, then continuing by choosing actions optimally. Note that $V^{</em>}(s)$ is the value of $s$ assuming the best action is taken initially, and so $V^{<em>}(s)=\max _{a} Q^{</em>}(s, a) . Q^{*}(s, a)$ can hence be written recursively as</p>
<p>$$
Q^{<em>}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} T\left(s, a, s^{\prime}\right) \max _{a^{\prime}} Q^{</em>}\left(s^{\prime}, a^{\prime}\right)
$$</p>
<p>Note also that, since $V^{<em>}(s)=\max _{a} Q^{</em>}(s, a)$, we have $\pi^{<em>}(s)=\arg \max _{a} Q^{</em>}(s, a)$ as an optimal policy.</p>
<p>Because the $Q$ function makes the action explicit, we can estimate the $Q$ values online using a method essentially the same as $T D(0)$, but also use them to define the policy,</p>
<p>because an action can be chosen just by taking the one with the maximum $Q$ value for the current state.</p>
<p>The Q-learning rule is</p>
<p>$$
Q(s, a):=Q(s, a)+\alpha\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right)
$$</p>
<p>where $\left\langle s, a, r, s^{\prime}\right\rangle$ is an experience tuple as described earlier. If each action is executed in each state an infinite number of times on an infinite run and $\alpha$ is decayed appropriately, the $Q$ values will converge with probability 1 to $Q^{*}$ (Watkins, 1989; Tsitsiklis, 1994; Jaakkola, Jordan, \&amp; Singh, 1994). Q-learning can also be extended to update states that occurred more than one step previously, as in $T D(\lambda)$ (Peng \&amp; Williams, 1994).</p>
<p>When the $Q$ values are nearly converged to their optimal values, it is appropriate for the agent to act greedily, taking, in each situation, the action with the highest $Q$ value. During learning, however, there is a difficult exploitation versus exploration trade-off to be made. There are no good, formally justified approaches to this problem in the general case; standard practice is to adopt one of the ad hoc methods discussed in section 2.2.</p>
<p>AHC architectures seem to be more difficult to work with than Q-learning on a practical level. It can be hard to get the relative learning rates right in AHC so that the two components converge together. In addition, Q-learning is exploration insensitive: that is, that the Q values will converge to the optimal values, independent of how the agent behaves while the data is being collected (as long as all state-action pairs are tried often enough). This means that, although the exploration-exploitation issue must be addressed in Q-learning, the details of the exploration strategy will not affect the convergence of the learning algorithm. For these reasons, Q-learning is the most popular and seems to be the most effective model-free algorithm for learning from delayed reinforcement. It does not, however, address any of the issues involved in generalizing over large state and/or action spaces. In addition, it may converge quite slowly to a good policy.</p>
<h1>4.3 Model-free Learning With Average Reward</h1>
<p>As described, Q-learning can be applied to discounted infinite-horizon MDPs. It can also be applied to undiscounted problems as long as the optimal policy is guaranteed to reach a reward-free absorbing state and the state is periodically reset.</p>
<p>Schwartz (1993) examined the problem of adapting Q-learning to an average-reward framework. Although his R-learning algorithm seems to exhibit convergence problems for some MDPs, several researchers have found the average-reward criterion closer to the true problem they wish to solve than a discounted criterion and therefore prefer R-learning to Q-learning (Mahadevan, 1994).</p>
<p>With that in mind, researchers have studied the problem of learning optimal averagereward policies. Mahadevan (1996) surveyed model-based average-reward algorithms from a reinforcement-learning perspective and found several difficulties with existing algorithms. In particular, he showed that existing reinforcement-learning algorithms for average reward (and some dynamic programming algorithms) do not always produce bias-optimal policies. Jaakkola, Jordan and Singh (1995) described an average-reward learning algorithm with guaranteed convergence properties. It uses a Monte-Carlo component to estimate the expected future reward for each state as the agent moves through the environment. In</p>
<p>addition, Bertsekas presents a Q-learning-like algorithm for average-case reward in his new textbook (1995). Although this recent work provides a much needed theoretical foundation to this area of reinforcement learning, many important problems remain unsolved.</p>
<h1>5. Computing Optimal Policies by Learning Models</h1>
<p>The previous section showed how it is possible to learn an optimal policy without knowing the models $T\left(s, a, s^{\prime}\right)$ or $R(s, a)$ and without even learning those models en route. Although many of these methods are guaranteed to find optimal policies eventually and use very little computation time per experience, they make extremely inefficient use of the data they gather and therefore often require a great deal of experience to achieve good performance. In this section we still begin by assuming that we don't know the models in advance, but we examine algorithms that do operate by learning these models. These algorithms are especially important in applications in which computation is considered to be cheap and real-world experience costly.</p>
<h3>5.1 Certainty Equivalent Methods</h3>
<p>We begin with the most conceptually straightforward method: first, learn the $T$ and $R$ functions by exploring the environment and keeping statistics about the results of each action; next, compute an optimal policy using one of the methods of Section 3. This method is known as certainty equivlance (Kumar \&amp; Varaiya, 1986).</p>
<p>There are some serious objections to this method:</p>
<ul>
<li>It makes an arbitrary division between the learning phase and the acting phase.</li>
<li>How should it gather data about the environment initially? Random exploration might be dangerous, and in some environments is an immensely inefficient method of gathering data, requiring exponentially more data (Whitehead, 1991) than a system that interleaves experience gathering with policy-building more tightly (Koenig \&amp; Simmons, 1993). See Figure 5 for an example.</li>
<li>The possibility of changes in the environment is also problematic. Breaking up an agent's life into a pure learning and a pure acting phase has a considerable risk that the optimal controller based on early life becomes, without detection, a suboptimal controller if the environment changes.</li>
</ul>
<p>A variation on this idea is certainty equivalence, in which the model is learned continually through the agent's lifetime and, at each step, the current model is used to compute an optimal policy and value function. This method makes very effective use of available data, but still ignores the question of exploration and is extremely computationally demanding, even for fairly small state spaces. Fortunately, there are a number of other model-based algorithms that are more practical.</p>
<h3>5.2 Dyna</h3>
<p>Sutton's Dyna architecture $(1990,1991)$ exploits a middle ground, yielding strategies that are both more effective than model-free learning and more computationally efficient than</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: In this environment, due to Whitehead (1991), random exploration would take take $O\left(2^{n}\right)$ steps to reach the goal even once, whereas a more intelligent exploration strategy (e.g. "assume any untried action leads directly to goal") would require only $O\left(n^{2}\right)$ steps.
the certainty-equivalence approach. It simultaneously uses experience to build a model ( $\hat{T}$ and $\hat{R}$ ), uses experience to adjust the policy, and uses the model to adjust the policy.</p>
<p>Dyna operates in a loop of interaction with the environment. Given an experience tuple $\left\langle s, a, s^{\prime}, r\right\rangle$, it behaves as follows:</p>
<ul>
<li>Update the model, incrementing statistics for the transition from $s$ to $s^{\prime}$ on action $a$ and for receiving reward $r$ for taking action $a$ in state $s$. The updated models are $\hat{T}$ and $\hat{R}$.</li>
<li>Update the policy at state $s$ based on the newly updated model using the rule</li>
</ul>
<p>$$
Q(s, a):=\hat{R}(s, a)+\gamma \sum_{s^{\prime}} \hat{T}\left(s, a, s^{\prime}\right) \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)
$$</p>
<p>which is a version of the value-iteration update for $Q$ values.</p>
<ul>
<li>Perform $k$ additional updates: choose $k$ state-action pairs at random and update them according to the same rule as before:</li>
</ul>
<p>$$
Q\left(s_{k}, a_{k}\right):=\hat{R}\left(s_{k}, a_{k}\right)+\gamma \sum_{s^{\prime}} \hat{T}\left(s_{k}, a_{k}, s^{\prime}\right) \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)
$$</p>
<ul>
<li>Choose an action $a^{\prime}$ to perform in state $s^{\prime}$, based on the $Q$ values but perhaps modified by an exploration strategy.</li>
</ul>
<p>The Dyna algorithm requires about $k$ times the computation of Q-learning per instance, but this is typically vastly less than for the naive model-based method. A reasonable value of $k$ can be determined based on the relative speeds of computation and of taking action.</p>
<p>Figure 6 shows a grid world in which in each cell the agent has four actions (N, S, E, W ) and transitions are made deterministically to an adjacent cell, unless there is a block, in which case no movement occurs. As we will see in Table 1, Dyna requires an order of magnitude fewer steps of experience than does Q-learning to arrive at an optimal policy. Dyna requires about six times more computational effort, however.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: A 3277 -state grid world. This was formulated as a shortest-path reinforcementlearning problem, which yields the same result as if a reward of 1 is given at the goal, a reward of zero elsewhere and a discount factor is used.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Steps before <br> convergence</th>
<th style="text-align: right;">Backups before <br> convergence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Q-learning</td>
<td style="text-align: right;">531,000</td>
<td style="text-align: right;">531,000</td>
</tr>
<tr>
<td style="text-align: left;">Dyna</td>
<td style="text-align: right;">62,000</td>
<td style="text-align: right;">$3,055,000$</td>
</tr>
<tr>
<td style="text-align: left;">prioritized sweeping</td>
<td style="text-align: right;">28,000</td>
<td style="text-align: right;">$1,010,000$</td>
</tr>
</tbody>
</table>
<p>Table 1: The performance of three algorithms described in the text. All methods used the exploration heuristic of "optimism in the face of uncertainty": any state not previously visited was assumed by default to be a goal state. Q-learning used its optimal learning rate parameter for a deterministic maze: $\alpha=1$. Dyna and prioritized sweeping were permitted to take $k=200$ backups per transition. For prioritized sweeping, the priority queue often emptied before all backups were used.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>This assumption may be disappointing; after all, operation in non-stationary environments is one of the motivations for building learning systems. In fact, many of the algorithms described in later sections are effective in slowly-varying non-stationary environments, but there is very little theoretical analysis in this area.</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
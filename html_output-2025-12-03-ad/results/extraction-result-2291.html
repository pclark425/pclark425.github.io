<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2291 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2291</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2291</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-252735265</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.02666v1.pdf" target="_blank">When not to use machine learning: a perspective on potential and limitations</a></p>
                <p><strong>Paper Abstract:</strong> The unparalleled success of artificial intelligence (AI) in the technology sector has catalyzed an enormous amount of research in the scientific community. It has proven to be a powerful tool, but as with any rapidly developing field, the deluge of information can be overwhelming, confusing and sometimes misleading. This can make it easy to become lost in the same hype cycles that have historically ended in the periods of scarce funding and depleted expectations known as AI Winters. Furthermore, while the importance of innovative, high-risk research cannot be overstated, it is also imperative to understand the fundamental limits of available techniques, especially in young fields where the rules appear to be constantly rewritten and as the likelihood of application to high-stakes scenarios increases. In this perspective, we highlight the guiding principles of data-driven modeling, how these principles imbue models with almost magical predictive power, and how they also impose limitations on the scope of problems they can address. Particularly, understanding when not to use data-driven techniques, such as machine learning, is not something commonly explored, but is just as important as knowing how to apply the techniques properly. We hope that the discussion to follow provides researchers throughout the sciences with a better understanding of when said techniques are appropriate, the pitfalls to watch for, and most importantly, the confidence to leverage the power they can provide.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2291.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2291.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Network Potentials</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Network Interatomic Potentials</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ML models (typically neural networks) trained to predict potential energy surfaces and forces for atomic systems to accelerate molecular dynamics and materials simulations; they are fit on configurations from specific systems and excel at interpolation within those configuration spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>materials science / molecular dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Replace or accelerate ab initio/empirical potentials by learning energy and force mappings from atomic configurations to enable long-timescale molecular dynamics on specific systems.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>limited to moderate; training data are generated (expensive) from quantum calculations or high-fidelity simulations for target systems, so datasets are typically system-specific and not globally abundant; labeled (energies/forces) and accessible within the simulation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>structured, high-dimensional numerical arrays representing atomic positions/configurations (graphs/neighborhood descriptors are often used), typically fixed-dimensional descriptors per atom or permutation-invariant representations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high dimensionality (many atoms), nonlinear mapping from structure to energy/forces, large search space of configurations but often constrained by focusing on a specific system; computational complexity moderate-to-high for training and high for data generation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging-to-mature within computational materials/chemistry; established literature and practices exist (multiple references cited), domain knowledge widely available.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium - scientific interpretability useful but black-box potentials are often acceptable when they reproduce energies/forces accurately; trust and uncertainty quantification are important for deployment in MD.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Neural networks for interatomic potentials (e.g., Behler–Parrinello-style potentials / NN potentials)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>High-capacity feedforward or graph-based neural networks trained on energy and force labels from ab initio simulations; models encode permutation, rotation, and translation invariances via descriptors or architectures; trained with regression losses to reproduce energies/forces for configurations of the target system.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning (regression)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate when the potential is trained on configurations representative of the deployment system; limitation: poor extrapolation to out-of-distribution configurations or different chemical systems.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Described as high-impact and effective when fit to a narrow, relevant configuration space — they 'perform to desired accuracy on the systems they're trained on' but are not expected to generalize to other systems.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for accelerating simulations, enabling longer-timescale MD and large-scale simulations for trained systems; scalable for many simulations within the trained domain but not generalizable without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to classical/ab initio potentials: NN potentials trade generalizability for accuracy and speed within the trained domain; the paper emphasizes interpolation strength vs extrapolation weakness rather than providing numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Restricting the configuration space to the target system, revisiting configurations during simulations, high-quality labeled simulation data, and encoding relevant invariances or prior belief lead to success.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Neural network potentials succeed when trained on the same distribution of atomic configurations they will be used on, leveraging interpolation strength but failing when asked to extrapolate beyond that training manifold.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2291.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2291.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Variational Autoencoders (VAEs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative latent-variable models that learn compressed latent representations and can be used for tasks like molecular generation by mapping discrete representations (e.g., SMILES) into continuous latent spaces where interpolation can discover new candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>molecular discovery / materials and chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Encode molecules into continuous latent spaces to enable interpolation and generation of novel molecules with targeted properties.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>varies; molecular datasets exist but coverage of chemical space can be limited for specialized properties; datasets are labeled when property targets exist (supervised) or unlabeled for pure generative modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>structured/unstructured hybrid: original data often strings (SMILES) or graphs; latent space is continuous, numeric, and high-dimensional.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high: large combinatorial chemical space, nonlinearity between structure and properties; generative search space is huge.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging with active research; methods are established but use-cases and best practices continue to develop.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium to high for scientific discovery — interpretability of generated candidates is desired for validation, but black-box generation may be acceptable for candidate suggestion followed by mechanistic validation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Variational Autoencoder with latent-space molecular interpolation</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Encoder-decoder architecture that compresses molecular representations (e.g., SMILES) into continuous latent vectors and decodes back to molecular representations; latent space distances are used for interpolation to propose new molecules; trained to minimize reconstruction loss and regularize latent distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>unsupervised / generative modeling (with downstream supervised property predictors sometimes)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable for exploring chemical space and proposing candidates when a meaningful latent embedding and property-conditioned mapping exist; relies on good encodings and that distances in latent space reflect property variation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Powerful for interpolation-driven discovery when latent distances correlate with properties, but effectiveness depends strongly on the chosen encoding and whether latent-space distances meaningfully reflect target properties.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Can accelerate candidate generation and ideation in molecular/material design; impact limited if latent space does not align with relevant property gradients or if generated candidates are invalid/out-of-distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Implicitly contrasted with heuristic or enumerative search: VAEs enable continuous interpolation and generative design but require careful encoding; no numerical comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Meaningful, mathematically rigorous encodings (e.g., SMILES → latent), ensuring latent distances correspond to target property changes, and sufficient labeled data for property conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Latent-space generative models can discover novel molecules by exploiting numeric distances in latent space, but this hinges on the encoding aligning latent-space geometry with the properties of interest.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2291.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2291.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gaussian Processes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Processes (GPs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bayesian nonparametric regression models that use kernels to encode prior beliefs about correlation lengths and provide uncertainty estimates; useful for interpolation and active learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>general scientific regression / surrogate modeling / active learning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn smooth functions from sparse observations with principled uncertainty quantification, and guide sampling in active learning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>suitable for limited-to-moderate datasets; perform well when data are scarce and when prior kernel structure aligns with true correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>structured numerical feature vectors; typically medium-dimensional inputs for standard GPs (scalability is limited to moderate dataset sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>moderate; GPs struggle with very high-dimensional inputs and very large datasets due to computational scaling, but encode nonlinearity via kernels.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>mature statistical method with well-established theory and practices (text cites standard GP references).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium - GPs provide interpretable priors (kernels) and uncertainty estimates, useful when some interpretability and error bars are required.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Gaussian Process regression with kernel priors (e.g., RBF, periodic kernels)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Nonparametric Bayesian regression where covariance (kernel) functions encode correlation lengths and prior beliefs; training infers kernel hyperparameters from data and yields posterior predictive mean and uncertainty; periodic kernels can encode periodicity as prior information.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning (Bayesian regression)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for interpolation within the training distribution and for uncertainty-aware sampling; less suitable for large-scale high-dimensional datasets without approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Described as producing more reliable interpolation and sensible behavior outside training data (reverting to prior mean) compared to unconstrained neural networks; well-suited for small-data regimes and active learning.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant for science problems with scarce data and where uncertainty quantification and principled priors improve model reliability and guide experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to neural networks: GPs with appropriate kernels interpolate more sensibly in low-data regimes and provide uncertainty estimates, whereas neural networks can behave unpredictably outside training regions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Encoding correct priors via kernels (e.g., periodic kernels when periodicity exists), appropriate handling of kernel hyperparameters, and using GPs for problems with moderate dimensionality and limited data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>GPs succeed when prior beliefs (kernels) encode problem structure, enabling reliable interpolation and uncertainty quantification where unconstrained high-capacity models may fail.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2291.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2291.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reinforcement Learning (Deep Q)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Q Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A type of reinforcement learning that uses neural networks to map states to action-value estimates (Q-values), enabling sequential decision-making; mentioned as an example of ML mapping states to decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>general decision-making / autonomous experimentation (conceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn policies for sequential decision problems by using neural networks to approximate action-value functions, enabling an agent to choose actions that maximize cumulative reward.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>depends on environment; data generated via interactions (online) rather than large static labeled datasets; can be expensive if real experiments are used.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>state-action-reward sequences (time-series / sequential data); inputs can be high-dimensional observations (images, spectra, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high: sequential decision-making, delayed rewards, large state-action spaces, sample inefficiency in many domains.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>well-established in CS, emerging in physical sciences for autonomous experimentation and control.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium - interpretability of policies may be desired, but black-box policies are sometimes acceptable for automation; safety and interpretability important in high-stakes experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Deep Q-Networks (DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Neural networks approximate Q(s,a) trained with temporal-difference style losses using experience replay; applicable in an outer loop where the agent probes the environment, receives rewards, and refines its decision-maker.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Potentially applicable to autonomous experimentation and sequential decision problems; must consider sample efficiency and reliability, and often requires simulation or safe experimental budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Mentioned conceptually as an example of ML mapping states to decisions; no experimental results provided in this paper, but RL frameworks can incorporate re-training loops for continuous improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for autonomous/closed-loop experimentation if sample efficiency and uncertainty handling are addressed; can accelerate discovery when combined with uncertainty-aware sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Not directly compared; RL contrasted conceptually with supervised models and active learning as part of outer-loop strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Access to sufficient (or simulated) interactions, reliable reward design, incorporation of uncertainty-aware sampling, and safe/efficient experimental protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Reinforcement learning provides a framework for sequential experimental decision-making, but its success depends on sample efficiency and integration with uncertainty-aware strategies in scientific contexts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2291.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2291.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active Learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strategy that iteratively selects the most informative new data points (often via uncertainty estimates) to label and add to the training set, thereby expanding the model's interpolation domain and improving performance efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>general experimental design / autonomous experimentation / data-efficient modeling</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Efficiently acquire new labeled data in regions of high uncertainty or expected information gain to improve model performance while minimizing costly data acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>useful when labeled data are scarce or expensive (common in scientific experiments); relies on the ability to generate or label selected samples.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>applies to structured numerical features, images, spectra, molecular configurations, etc.; essentially model-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>moderate-to-high depending on model and acquisition strategy; involves evaluating uncertainty over large candidate spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>established in ML and increasingly used in science and autonomous labs; mature methodologies exist but domain-specific adaptations often required.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>low-to-medium - primarily used to improve predictive performance though scientific interpretability of sampled points is valuable.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Uncertainty-driven Active Learning (e.g., GP/ensemble uncertainty sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Use models that provide uncertainty estimates (Gaussian Processes, ensembles) to identify input regions with high predictive uncertainty and sample new labels there, then retrain to expand interpolation window.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning with active sampling (interactive / closed-loop)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable when data labeling is costly and models can quantify uncertainty; helps detect and remedy extrapolation by expanding training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Presented as an effective mitigation for extrapolation issues, enabling the algorithm to detect when it is extrapolating and actively expand its interpolation window by acquiring targeted data.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for experimental sciences and simulation-driven campaigns by reducing labeled-data requirements and focusing resources on informative measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted with passive data collection; active learning is portrayed as superior for efficient coverage of the relevant data manifold when labels are costly.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Reliable model uncertainty estimates, the ability to acquire labeled data at suggested points (experimental throughput), and closing the loop to retrain models iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Active learning mitigates extrapolation risk by using uncertainty-aware sampling to expand the model's interpolation domain with minimal additional data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2291.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2291.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inverse Problems (ML for Inversion)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine Learning for Inverse Problems (e.g., phase retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of ML to problems where a measured signal must be mapped back to its source (an inverse mapping), which can be ill-posed; ML is suitable only when the inverse mapping is non-degenerate and a function.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>imaging / signal processing (coherent diffraction imaging, phase retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Recover latent structure (e.g., object phases or geometric structure) from observed signals (e.g., intensities or spectra) where direct inversion may lose information and be ill-posed.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>often limited; experimental measurements can be scarce and noisy; label generation for supervised inversion may be expensive or impossible when ground truth absent.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>observational signals such as spectra, diffraction intensities (arrays/images), time-series; typically dense numerical arrays with potential loss of phase or other latent variables.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high: problems can be ill-posed, non-unique solutions, high sensitivity to small input changes (instability), low SNR regimes make inversion infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>well-established discipline of inverse problems; ML applications are growing but constrained by problem identifiability and data quality.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>high - mechanistic constraints or additional prior information often required to make inversion feasible and scientifically meaningful; interpretability and uncertainty quantification are critical.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Supervised ML for inverse mapping (with uncertainty-aware variants)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train regression or probabilistic models to map observables to source parameters where possible; incorporate uncertainty-aware models to quantify inversion ambiguity; must restrict to non-degenerate subspaces so mapping is functional.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning (regression) with probabilistic/uncertainty-aware extensions</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Limited applicability: only appropriate when the inverse is sufficiently well-posed or when additional prior/correlative information enables a functional mapping; not suitable when fundamental information loss (e.g., phase) prevents unique recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Paper warns ML can fail dramatically on ill-posed inverse problems or low SNR situations; where non-degenerate subspaces exist ML can excel by finding subtle patterns, but if inversion ambiguity remains, ML predictions (even with error bars) may be useless.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Potentially high when inversion is feasible (e.g., specific constrained settings), enabling fast inference; negligible or misleading when fundamental identifiability is absent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to analytical inversion methods and physics-based approaches — ML can help where correlations permit inversion but cannot recover information that is fundamentally lost.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Existence of correlations that make the inverse mapping functional, adequate SNR, availability of representative labeled data, and use of uncertainty-aware models to assess confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>ML is only suitable for inverse problems when the inverse mapping is a well-defined function or when priors/correlations restrict the solution space; otherwise ML predictions are unreliable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2291.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2291.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simulation-to-Experiment Transfer (Domain Shift)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training on Simulated Data and Deploying on Experimental Data (Domain Shift)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of models trained on simulated (synthetic) data for deployment on experimental measurements, which can fail if the two data distributions differ (distribution shift); some cases succeed when simulation fidelity is high.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>materials characterization / spectroscopy / general scientific measurement</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Train ML models on inexpensive simulated data and apply them to expensive experimental data, aiming to save labeling cost and accelerate analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Simulated data are abundant/cheap relative to experimental data which are scarce/expensive; simulated data are labeled by construction while experimental labeled sets are limited.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>spectra, pair distribution functions, images, structural data — numerical arrays with domain-specific features.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>moderate-to-high: domain discrepancy can be subtle and large, high-dimensional features, and differing noise/artefact characteristics between simulation and experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>actively researched with known pitfalls; some mature successes exist but general transfer remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium - understanding simulation-experiment discrepancies and instrument effects is necessary; validation on experimental holdout data required.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Supervised models trained on simulated data (with possible transfer learning)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Fit models (e.g., classifiers/regressors) on synthetic datasets and attempt to apply to experimental data; transfer learning or domain adaptation may be used where possible but require some experimental labels for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning with transfer learning / domain adaptation considerations</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable in some cases where simulations reproduce experimental signals closely (examples cited), but often fails when simulation fidelity is insufficient and distributions do not match; requires experimental validation data.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Paper notes mixed outcomes: some successful examples (predicting space group from pair distribution data, nanoparticle sizes from XAS) and some failures (experimental vs simulated XAS across diverse crystals); stresses that training and testing distributions must match deployment distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High cost/time savings when simulation-to-experiment transfer succeeds; risk of invalid predictions when distributions differ, so potential for wasted resources if not validated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Implicit comparison to training on experimental data directly (preferable when feasible) and to transfer learning; paper recommends training on deployment-distributed data where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High simulation fidelity to experimental conditions, careful validation on held-out experimental data, and availability of some experimental labels for transfer learning or domain adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Models trained on simulated data can work for experiments only when the simulation faithfully reproduces deployment conditions; otherwise distribution shift leads to failure and validation on deployment data is essential.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2291.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2291.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-driving Laboratories</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-driving Laboratories / Autonomous Experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated experimental platforms that combine ML-driven decision-making, active learning, and high-throughput experiments to accelerate discovery and optimization of materials/chemistries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>materials discovery / chemistry / autonomous experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Close the loop between experiment and decision-making: select experiments adaptively to explore design spaces efficiently and optimize desired properties.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Data are generated online by the autonomous system; experimental throughput and cost constrain dataset size — often moderate datasets accumulated over runs.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>multimodal: experimental parameters (tabular), measured outputs (spectra, images), and time-series from sequential experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>high: sequential decision-making under experimental constraints, noisy measurements, large parameter spaces, and need for uncertainty and safety handling.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>emerging and rapidly developing with demonstrated case studies; interdisciplinary integration required (robotics, ML, domain expertise).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium to high - experimenters typically need interpretable evidence for scientific insight though ML-driven suggestions can be accepted for optimization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Autonomous experimentation pipelines combining supervised models, active learning, and optimization/reinforcement strategies</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Integrate predictive models (for property prediction), uncertainty estimation (GPs/ensembles), acquisition functions for selecting next experiments (active learning/optimization), and control systems for executing experiments; iterative retraining refines models.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>hybrid: supervised learning + active learning + reinforcement/optimization loop</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited for problems where experiments are expensive and high-throughput automation exists; success depends on reliable predictive models and uncertainty estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Cited as an impactful application area with many references; autonomous labs can accelerate discovery and optimization but depend on good data selection and domain-aware modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High: can substantially reduce experimental cycles and cost, enable faster discovery, and improve resource allocation when properly integrated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted with manual experimental campaigns; autonomous approaches are more efficient when experiments can be automated and models can guide acquisitions reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Reliable uncertainty-aware models, robust automation, thoughtful experiment design, and ensuring model training data reflect deployment conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Autonomous experimentation leverages active learning and uncertainty estimation to accelerate discovery, but its success hinges on realistic modeling of the experimental deployment distribution and robust closed-loop design.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2291.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2291.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random Forests</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Forests</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble tree-based supervised learning method noted here to contrast with gradient-based models — not trained by gradient descent and often robust in settings where smoothness assumptions are weaker.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>general supervised prediction tasks across scientific domains</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Use ensemble decision trees to predict targets from features, often robust to non-smooth relationships and not requiring gradient-based optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>suitable for limited-to-moderate labeled datasets; performance improves with more labeled examples but robust to some noise and feature types.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>structured tabular data, categorical and numerical features; can be applied to derived features from spectra/images.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>moderate; handles nonlinear interactions but may struggle with extremely high-dimensional raw data without feature engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>mature and widely used across scientific disciplines.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>low-to-medium - variable importance offers some interpretability though not fully mechanistic.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Random Forests (ensemble decision trees)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train multiple decision trees on bootstrapped samples and average predictions to reduce variance; not optimized by gradient descent and can handle non-smooth feature-target relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning (ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate when smoothness assumptions for gradient methods are not met; robust baseline for many tasks and useful when gradient-based smoothness is not present.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Presented as an example of a method that does not require gradient-based optimizers and may generalize better when smoothness assumptions fail; no experimental results provided.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Useful as a robust baseline method in many scientific ML tasks and for feature-importance insights.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted qualitatively with gradient-based neural networks; random forests may be preferable when features-to-target mapping lacks smoothness.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Quality features/encodings, sufficient labeled data, and appropriate hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Tree-based ensemble methods can be preferable to gradient-based models when the target function lacks smoothness or when gradient-based optimization is unsuitable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2291.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2291.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ensemble / Uncertainty-aware Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensemble Models and Uncertainty-aware Modeling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use ensembles (or Bayesian methods like GPs) to quantify predictive uncertainty and guide sampling or provide calibrated error estimates for scientific predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>general scientific prediction, active learning, and uncertainty quantification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide predictions accompanied by uncertainty estimates to inform decision-making, detect extrapolation, and support active learning acquisition choices.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>helpful when data are limited and additional targeted data acquisition is planned; require enough data to form diverse ensemble members or infer posterior distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>applies broadly to numerical feature vectors, spectra, images mapped through appropriate encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>varies; uncertainty estimation adds computational overhead but is vital for safety and active sampling in complex domains.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>well-established methods with growing adoption in scientific ML.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>medium - uncertainty estimates aid interpretability and trust but do not substitute for mechanistic understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Ensemble methods / uncertainty-aware models (e.g., bootstrapped ensembles, Bayesian approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Combine multiple models or use Bayesian formulations to produce predictive distributions or confidence intervals; used to identify high-uncertainty regions for sampling and to present calibrated error bars with predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning with probabilistic/ensemble uncertainty quantification</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable when decisions must account for model confidence; essential for active learning and detecting out-of-distribution inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Portrayed as effective tools for sampling and detecting extrapolation; can enable predictions with error estimates though in severely ill-posed inversions the error bars may be unhelpful.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for improving safety and reliability of ML-driven scientific workflows and for efficient experimental design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Preferred over single deterministic models when uncertainty estimation or active sampling is needed; contrasted with black-box deterministic neural nets that lack uncertainty measures.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Proper ensemble diversity, calibrated uncertainty estimates, and integration with acquisition/decision policies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Uncertainty-aware models are essential for scientific deployment, enabling detection of extrapolation and guiding targeted data acquisition to expand reliable prediction regions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gaussian Processes for Machine Learning <em>(Rating: 2)</em></li>
                <li>Deep learning with Python <em>(Rating: 2)</em></li>
                <li>Pattern recognition and machine learning <em>(Rating: 2)</em></li>
                <li>AI: the tumultuous history of the search for artificial intelligence <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2291",
    "paper_id": "paper-252735265",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "Neural Network Potentials",
            "name_full": "Neural Network Interatomic Potentials",
            "brief_description": "ML models (typically neural networks) trained to predict potential energy surfaces and forces for atomic systems to accelerate molecular dynamics and materials simulations; they are fit on configurations from specific systems and excel at interpolation within those configuration spaces.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "materials science / molecular dynamics",
            "problem_description": "Replace or accelerate ab initio/empirical potentials by learning energy and force mappings from atomic configurations to enable long-timescale molecular dynamics on specific systems.",
            "data_availability": "limited to moderate; training data are generated (expensive) from quantum calculations or high-fidelity simulations for target systems, so datasets are typically system-specific and not globally abundant; labeled (energies/forces) and accessible within the simulation pipeline.",
            "data_structure": "structured, high-dimensional numerical arrays representing atomic positions/configurations (graphs/neighborhood descriptors are often used), typically fixed-dimensional descriptors per atom or permutation-invariant representations.",
            "problem_complexity": "high dimensionality (many atoms), nonlinear mapping from structure to energy/forces, large search space of configurations but often constrained by focusing on a specific system; computational complexity moderate-to-high for training and high for data generation.",
            "domain_maturity": "emerging-to-mature within computational materials/chemistry; established literature and practices exist (multiple references cited), domain knowledge widely available.",
            "mechanistic_understanding_requirements": "medium - scientific interpretability useful but black-box potentials are often acceptable when they reproduce energies/forces accurately; trust and uncertainty quantification are important for deployment in MD.",
            "ai_methodology_name": "Neural networks for interatomic potentials (e.g., Behler–Parrinello-style potentials / NN potentials)",
            "ai_methodology_description": "High-capacity feedforward or graph-based neural networks trained on energy and force labels from ab initio simulations; models encode permutation, rotation, and translation invariances via descriptors or architectures; trained with regression losses to reproduce energies/forces for configurations of the target system.",
            "ai_methodology_category": "supervised learning (regression)",
            "applicability": "Applicable and appropriate when the potential is trained on configurations representative of the deployment system; limitation: poor extrapolation to out-of-distribution configurations or different chemical systems.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Described as high-impact and effective when fit to a narrow, relevant configuration space — they 'perform to desired accuracy on the systems they're trained on' but are not expected to generalize to other systems.",
            "impact_potential": "High for accelerating simulations, enabling longer-timescale MD and large-scale simulations for trained systems; scalable for many simulations within the trained domain but not generalizable without retraining.",
            "comparison_to_alternatives": "Compared conceptually to classical/ab initio potentials: NN potentials trade generalizability for accuracy and speed within the trained domain; the paper emphasizes interpolation strength vs extrapolation weakness rather than providing numeric comparisons.",
            "success_factors": "Restricting the configuration space to the target system, revisiting configurations during simulations, high-quality labeled simulation data, and encoding relevant invariances or prior belief lead to success.",
            "key_insight": "Neural network potentials succeed when trained on the same distribution of atomic configurations they will be used on, leveraging interpolation strength but failing when asked to extrapolate beyond that training manifold.",
            "uuid": "e2291.0"
        },
        {
            "name_short": "Variational Autoencoders (VAEs)",
            "name_full": "Variational Autoencoders",
            "brief_description": "Generative latent-variable models that learn compressed latent representations and can be used for tasks like molecular generation by mapping discrete representations (e.g., SMILES) into continuous latent spaces where interpolation can discover new candidates.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "molecular discovery / materials and chemistry",
            "problem_description": "Encode molecules into continuous latent spaces to enable interpolation and generation of novel molecules with targeted properties.",
            "data_availability": "varies; molecular datasets exist but coverage of chemical space can be limited for specialized properties; datasets are labeled when property targets exist (supervised) or unlabeled for pure generative modeling.",
            "data_structure": "structured/unstructured hybrid: original data often strings (SMILES) or graphs; latent space is continuous, numeric, and high-dimensional.",
            "problem_complexity": "high: large combinatorial chemical space, nonlinearity between structure and properties; generative search space is huge.",
            "domain_maturity": "emerging with active research; methods are established but use-cases and best practices continue to develop.",
            "mechanistic_understanding_requirements": "medium to high for scientific discovery — interpretability of generated candidates is desired for validation, but black-box generation may be acceptable for candidate suggestion followed by mechanistic validation.",
            "ai_methodology_name": "Variational Autoencoder with latent-space molecular interpolation",
            "ai_methodology_description": "Encoder-decoder architecture that compresses molecular representations (e.g., SMILES) into continuous latent vectors and decodes back to molecular representations; latent space distances are used for interpolation to propose new molecules; trained to minimize reconstruction loss and regularize latent distribution.",
            "ai_methodology_category": "unsupervised / generative modeling (with downstream supervised property predictors sometimes)",
            "applicability": "Applicable for exploring chemical space and proposing candidates when a meaningful latent embedding and property-conditioned mapping exist; relies on good encodings and that distances in latent space reflect property variation.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Powerful for interpolation-driven discovery when latent distances correlate with properties, but effectiveness depends strongly on the chosen encoding and whether latent-space distances meaningfully reflect target properties.",
            "impact_potential": "Can accelerate candidate generation and ideation in molecular/material design; impact limited if latent space does not align with relevant property gradients or if generated candidates are invalid/out-of-distribution.",
            "comparison_to_alternatives": "Implicitly contrasted with heuristic or enumerative search: VAEs enable continuous interpolation and generative design but require careful encoding; no numerical comparisons provided.",
            "success_factors": "Meaningful, mathematically rigorous encodings (e.g., SMILES → latent), ensuring latent distances correspond to target property changes, and sufficient labeled data for property conditioning.",
            "key_insight": "Latent-space generative models can discover novel molecules by exploiting numeric distances in latent space, but this hinges on the encoding aligning latent-space geometry with the properties of interest.",
            "uuid": "e2291.1"
        },
        {
            "name_short": "Gaussian Processes",
            "name_full": "Gaussian Processes (GPs)",
            "brief_description": "Bayesian nonparametric regression models that use kernels to encode prior beliefs about correlation lengths and provide uncertainty estimates; useful for interpolation and active learning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "general scientific regression / surrogate modeling / active learning",
            "problem_description": "Learn smooth functions from sparse observations with principled uncertainty quantification, and guide sampling in active learning strategies.",
            "data_availability": "suitable for limited-to-moderate datasets; perform well when data are scarce and when prior kernel structure aligns with true correlations.",
            "data_structure": "structured numerical feature vectors; typically medium-dimensional inputs for standard GPs (scalability is limited to moderate dataset sizes).",
            "problem_complexity": "moderate; GPs struggle with very high-dimensional inputs and very large datasets due to computational scaling, but encode nonlinearity via kernels.",
            "domain_maturity": "mature statistical method with well-established theory and practices (text cites standard GP references).",
            "mechanistic_understanding_requirements": "medium - GPs provide interpretable priors (kernels) and uncertainty estimates, useful when some interpretability and error bars are required.",
            "ai_methodology_name": "Gaussian Process regression with kernel priors (e.g., RBF, periodic kernels)",
            "ai_methodology_description": "Nonparametric Bayesian regression where covariance (kernel) functions encode correlation lengths and prior beliefs; training infers kernel hyperparameters from data and yields posterior predictive mean and uncertainty; periodic kernels can encode periodicity as prior information.",
            "ai_methodology_category": "supervised learning (Bayesian regression)",
            "applicability": "Highly applicable for interpolation within the training distribution and for uncertainty-aware sampling; less suitable for large-scale high-dimensional datasets without approximations.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Described as producing more reliable interpolation and sensible behavior outside training data (reverting to prior mean) compared to unconstrained neural networks; well-suited for small-data regimes and active learning.",
            "impact_potential": "Significant for science problems with scarce data and where uncertainty quantification and principled priors improve model reliability and guide experiments.",
            "comparison_to_alternatives": "Compared qualitatively to neural networks: GPs with appropriate kernels interpolate more sensibly in low-data regimes and provide uncertainty estimates, whereas neural networks can behave unpredictably outside training regions.",
            "success_factors": "Encoding correct priors via kernels (e.g., periodic kernels when periodicity exists), appropriate handling of kernel hyperparameters, and using GPs for problems with moderate dimensionality and limited data.",
            "key_insight": "GPs succeed when prior beliefs (kernels) encode problem structure, enabling reliable interpolation and uncertainty quantification where unconstrained high-capacity models may fail.",
            "uuid": "e2291.2"
        },
        {
            "name_short": "Reinforcement Learning (Deep Q)",
            "name_full": "Deep Q Reinforcement Learning",
            "brief_description": "A type of reinforcement learning that uses neural networks to map states to action-value estimates (Q-values), enabling sequential decision-making; mentioned as an example of ML mapping states to decisions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "general decision-making / autonomous experimentation (conceptual)",
            "problem_description": "Learn policies for sequential decision problems by using neural networks to approximate action-value functions, enabling an agent to choose actions that maximize cumulative reward.",
            "data_availability": "depends on environment; data generated via interactions (online) rather than large static labeled datasets; can be expensive if real experiments are used.",
            "data_structure": "state-action-reward sequences (time-series / sequential data); inputs can be high-dimensional observations (images, spectra, etc.).",
            "problem_complexity": "high: sequential decision-making, delayed rewards, large state-action spaces, sample inefficiency in many domains.",
            "domain_maturity": "well-established in CS, emerging in physical sciences for autonomous experimentation and control.",
            "mechanistic_understanding_requirements": "medium - interpretability of policies may be desired, but black-box policies are sometimes acceptable for automation; safety and interpretability important in high-stakes experiments.",
            "ai_methodology_name": "Deep Q-Networks (DQN)",
            "ai_methodology_description": "Neural networks approximate Q(s,a) trained with temporal-difference style losses using experience replay; applicable in an outer loop where the agent probes the environment, receives rewards, and refines its decision-maker.",
            "ai_methodology_category": "reinforcement learning",
            "applicability": "Potentially applicable to autonomous experimentation and sequential decision problems; must consider sample efficiency and reliability, and often requires simulation or safe experimental budgets.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Mentioned conceptually as an example of ML mapping states to decisions; no experimental results provided in this paper, but RL frameworks can incorporate re-training loops for continuous improvement.",
            "impact_potential": "High for autonomous/closed-loop experimentation if sample efficiency and uncertainty handling are addressed; can accelerate discovery when combined with uncertainty-aware sampling.",
            "comparison_to_alternatives": "Not directly compared; RL contrasted conceptually with supervised models and active learning as part of outer-loop strategies.",
            "success_factors": "Access to sufficient (or simulated) interactions, reliable reward design, incorporation of uncertainty-aware sampling, and safe/efficient experimental protocols.",
            "key_insight": "Reinforcement learning provides a framework for sequential experimental decision-making, but its success depends on sample efficiency and integration with uncertainty-aware strategies in scientific contexts.",
            "uuid": "e2291.3"
        },
        {
            "name_short": "Active Learning",
            "name_full": "Active Learning",
            "brief_description": "A strategy that iteratively selects the most informative new data points (often via uncertainty estimates) to label and add to the training set, thereby expanding the model's interpolation domain and improving performance efficiently.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "general experimental design / autonomous experimentation / data-efficient modeling",
            "problem_description": "Efficiently acquire new labeled data in regions of high uncertainty or expected information gain to improve model performance while minimizing costly data acquisition.",
            "data_availability": "useful when labeled data are scarce or expensive (common in scientific experiments); relies on the ability to generate or label selected samples.",
            "data_structure": "applies to structured numerical features, images, spectra, molecular configurations, etc.; essentially model-agnostic.",
            "problem_complexity": "moderate-to-high depending on model and acquisition strategy; involves evaluating uncertainty over large candidate spaces.",
            "domain_maturity": "established in ML and increasingly used in science and autonomous labs; mature methodologies exist but domain-specific adaptations often required.",
            "mechanistic_understanding_requirements": "low-to-medium - primarily used to improve predictive performance though scientific interpretability of sampled points is valuable.",
            "ai_methodology_name": "Uncertainty-driven Active Learning (e.g., GP/ensemble uncertainty sampling)",
            "ai_methodology_description": "Use models that provide uncertainty estimates (Gaussian Processes, ensembles) to identify input regions with high predictive uncertainty and sample new labels there, then retrain to expand interpolation window.",
            "ai_methodology_category": "supervised learning with active sampling (interactive / closed-loop)",
            "applicability": "Highly applicable when data labeling is costly and models can quantify uncertainty; helps detect and remedy extrapolation by expanding training distribution.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Presented as an effective mitigation for extrapolation issues, enabling the algorithm to detect when it is extrapolating and actively expand its interpolation window by acquiring targeted data.",
            "impact_potential": "High for experimental sciences and simulation-driven campaigns by reducing labeled-data requirements and focusing resources on informative measurements.",
            "comparison_to_alternatives": "Contrasted with passive data collection; active learning is portrayed as superior for efficient coverage of the relevant data manifold when labels are costly.",
            "success_factors": "Reliable model uncertainty estimates, the ability to acquire labeled data at suggested points (experimental throughput), and closing the loop to retrain models iteratively.",
            "key_insight": "Active learning mitigates extrapolation risk by using uncertainty-aware sampling to expand the model's interpolation domain with minimal additional data.",
            "uuid": "e2291.4"
        },
        {
            "name_short": "Inverse Problems (ML for Inversion)",
            "name_full": "Machine Learning for Inverse Problems (e.g., phase retrieval)",
            "brief_description": "Application of ML to problems where a measured signal must be mapped back to its source (an inverse mapping), which can be ill-posed; ML is suitable only when the inverse mapping is non-degenerate and a function.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "imaging / signal processing (coherent diffraction imaging, phase retrieval)",
            "problem_description": "Recover latent structure (e.g., object phases or geometric structure) from observed signals (e.g., intensities or spectra) where direct inversion may lose information and be ill-posed.",
            "data_availability": "often limited; experimental measurements can be scarce and noisy; label generation for supervised inversion may be expensive or impossible when ground truth absent.",
            "data_structure": "observational signals such as spectra, diffraction intensities (arrays/images), time-series; typically dense numerical arrays with potential loss of phase or other latent variables.",
            "problem_complexity": "high: problems can be ill-posed, non-unique solutions, high sensitivity to small input changes (instability), low SNR regimes make inversion infeasible.",
            "domain_maturity": "well-established discipline of inverse problems; ML applications are growing but constrained by problem identifiability and data quality.",
            "mechanistic_understanding_requirements": "high - mechanistic constraints or additional prior information often required to make inversion feasible and scientifically meaningful; interpretability and uncertainty quantification are critical.",
            "ai_methodology_name": "Supervised ML for inverse mapping (with uncertainty-aware variants)",
            "ai_methodology_description": "Train regression or probabilistic models to map observables to source parameters where possible; incorporate uncertainty-aware models to quantify inversion ambiguity; must restrict to non-degenerate subspaces so mapping is functional.",
            "ai_methodology_category": "supervised learning (regression) with probabilistic/uncertainty-aware extensions",
            "applicability": "Limited applicability: only appropriate when the inverse is sufficiently well-posed or when additional prior/correlative information enables a functional mapping; not suitable when fundamental information loss (e.g., phase) prevents unique recovery.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Paper warns ML can fail dramatically on ill-posed inverse problems or low SNR situations; where non-degenerate subspaces exist ML can excel by finding subtle patterns, but if inversion ambiguity remains, ML predictions (even with error bars) may be useless.",
            "impact_potential": "Potentially high when inversion is feasible (e.g., specific constrained settings), enabling fast inference; negligible or misleading when fundamental identifiability is absent.",
            "comparison_to_alternatives": "Compared conceptually to analytical inversion methods and physics-based approaches — ML can help where correlations permit inversion but cannot recover information that is fundamentally lost.",
            "success_factors": "Existence of correlations that make the inverse mapping functional, adequate SNR, availability of representative labeled data, and use of uncertainty-aware models to assess confidence.",
            "key_insight": "ML is only suitable for inverse problems when the inverse mapping is a well-defined function or when priors/correlations restrict the solution space; otherwise ML predictions are unreliable.",
            "uuid": "e2291.5"
        },
        {
            "name_short": "Simulation-to-Experiment Transfer (Domain Shift)",
            "name_full": "Training on Simulated Data and Deploying on Experimental Data (Domain Shift)",
            "brief_description": "Use of models trained on simulated (synthetic) data for deployment on experimental measurements, which can fail if the two data distributions differ (distribution shift); some cases succeed when simulation fidelity is high.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "materials characterization / spectroscopy / general scientific measurement",
            "problem_description": "Train ML models on inexpensive simulated data and apply them to expensive experimental data, aiming to save labeling cost and accelerate analysis.",
            "data_availability": "Simulated data are abundant/cheap relative to experimental data which are scarce/expensive; simulated data are labeled by construction while experimental labeled sets are limited.",
            "data_structure": "spectra, pair distribution functions, images, structural data — numerical arrays with domain-specific features.",
            "problem_complexity": "moderate-to-high: domain discrepancy can be subtle and large, high-dimensional features, and differing noise/artefact characteristics between simulation and experiment.",
            "domain_maturity": "actively researched with known pitfalls; some mature successes exist but general transfer remains challenging.",
            "mechanistic_understanding_requirements": "medium - understanding simulation-experiment discrepancies and instrument effects is necessary; validation on experimental holdout data required.",
            "ai_methodology_name": "Supervised models trained on simulated data (with possible transfer learning)",
            "ai_methodology_description": "Fit models (e.g., classifiers/regressors) on synthetic datasets and attempt to apply to experimental data; transfer learning or domain adaptation may be used where possible but require some experimental labels for validation.",
            "ai_methodology_category": "supervised learning with transfer learning / domain adaptation considerations",
            "applicability": "Applicable in some cases where simulations reproduce experimental signals closely (examples cited), but often fails when simulation fidelity is insufficient and distributions do not match; requires experimental validation data.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Paper notes mixed outcomes: some successful examples (predicting space group from pair distribution data, nanoparticle sizes from XAS) and some failures (experimental vs simulated XAS across diverse crystals); stresses that training and testing distributions must match deployment distribution.",
            "impact_potential": "High cost/time savings when simulation-to-experiment transfer succeeds; risk of invalid predictions when distributions differ, so potential for wasted resources if not validated.",
            "comparison_to_alternatives": "Implicit comparison to training on experimental data directly (preferable when feasible) and to transfer learning; paper recommends training on deployment-distributed data where possible.",
            "success_factors": "High simulation fidelity to experimental conditions, careful validation on held-out experimental data, and availability of some experimental labels for transfer learning or domain adaptation.",
            "key_insight": "Models trained on simulated data can work for experiments only when the simulation faithfully reproduces deployment conditions; otherwise distribution shift leads to failure and validation on deployment data is essential.",
            "uuid": "e2291.6"
        },
        {
            "name_short": "Self-driving Laboratories",
            "name_full": "Self-driving Laboratories / Autonomous Experimentation",
            "brief_description": "Automated experimental platforms that combine ML-driven decision-making, active learning, and high-throughput experiments to accelerate discovery and optimization of materials/chemistries.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "materials discovery / chemistry / autonomous experimentation",
            "problem_description": "Close the loop between experiment and decision-making: select experiments adaptively to explore design spaces efficiently and optimize desired properties.",
            "data_availability": "Data are generated online by the autonomous system; experimental throughput and cost constrain dataset size — often moderate datasets accumulated over runs.",
            "data_structure": "multimodal: experimental parameters (tabular), measured outputs (spectra, images), and time-series from sequential experiments.",
            "problem_complexity": "high: sequential decision-making under experimental constraints, noisy measurements, large parameter spaces, and need for uncertainty and safety handling.",
            "domain_maturity": "emerging and rapidly developing with demonstrated case studies; interdisciplinary integration required (robotics, ML, domain expertise).",
            "mechanistic_understanding_requirements": "medium to high - experimenters typically need interpretable evidence for scientific insight though ML-driven suggestions can be accepted for optimization tasks.",
            "ai_methodology_name": "Autonomous experimentation pipelines combining supervised models, active learning, and optimization/reinforcement strategies",
            "ai_methodology_description": "Integrate predictive models (for property prediction), uncertainty estimation (GPs/ensembles), acquisition functions for selecting next experiments (active learning/optimization), and control systems for executing experiments; iterative retraining refines models.",
            "ai_methodology_category": "hybrid: supervised learning + active learning + reinforcement/optimization loop",
            "applicability": "Well-suited for problems where experiments are expensive and high-throughput automation exists; success depends on reliable predictive models and uncertainty estimates.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Cited as an impactful application area with many references; autonomous labs can accelerate discovery and optimization but depend on good data selection and domain-aware modeling.",
            "impact_potential": "High: can substantially reduce experimental cycles and cost, enable faster discovery, and improve resource allocation when properly integrated.",
            "comparison_to_alternatives": "Contrasted with manual experimental campaigns; autonomous approaches are more efficient when experiments can be automated and models can guide acquisitions reliably.",
            "success_factors": "Reliable uncertainty-aware models, robust automation, thoughtful experiment design, and ensuring model training data reflect deployment conditions.",
            "key_insight": "Autonomous experimentation leverages active learning and uncertainty estimation to accelerate discovery, but its success hinges on realistic modeling of the experimental deployment distribution and robust closed-loop design.",
            "uuid": "e2291.7"
        },
        {
            "name_short": "Random Forests",
            "name_full": "Random Forests",
            "brief_description": "An ensemble tree-based supervised learning method noted here to contrast with gradient-based models — not trained by gradient descent and often robust in settings where smoothness assumptions are weaker.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "general supervised prediction tasks across scientific domains",
            "problem_description": "Use ensemble decision trees to predict targets from features, often robust to non-smooth relationships and not requiring gradient-based optimization.",
            "data_availability": "suitable for limited-to-moderate labeled datasets; performance improves with more labeled examples but robust to some noise and feature types.",
            "data_structure": "structured tabular data, categorical and numerical features; can be applied to derived features from spectra/images.",
            "problem_complexity": "moderate; handles nonlinear interactions but may struggle with extremely high-dimensional raw data without feature engineering.",
            "domain_maturity": "mature and widely used across scientific disciplines.",
            "mechanistic_understanding_requirements": "low-to-medium - variable importance offers some interpretability though not fully mechanistic.",
            "ai_methodology_name": "Random Forests (ensemble decision trees)",
            "ai_methodology_description": "Train multiple decision trees on bootstrapped samples and average predictions to reduce variance; not optimized by gradient descent and can handle non-smooth feature-target relationships.",
            "ai_methodology_category": "supervised learning (ensemble)",
            "applicability": "Appropriate when smoothness assumptions for gradient methods are not met; robust baseline for many tasks and useful when gradient-based smoothness is not present.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Presented as an example of a method that does not require gradient-based optimizers and may generalize better when smoothness assumptions fail; no experimental results provided.",
            "impact_potential": "Useful as a robust baseline method in many scientific ML tasks and for feature-importance insights.",
            "comparison_to_alternatives": "Contrasted qualitatively with gradient-based neural networks; random forests may be preferable when features-to-target mapping lacks smoothness.",
            "success_factors": "Quality features/encodings, sufficient labeled data, and appropriate hyperparameter tuning.",
            "key_insight": "Tree-based ensemble methods can be preferable to gradient-based models when the target function lacks smoothness or when gradient-based optimization is unsuitable.",
            "uuid": "e2291.8"
        },
        {
            "name_short": "Ensemble / Uncertainty-aware Models",
            "name_full": "Ensemble Models and Uncertainty-aware Modeling",
            "brief_description": "Use ensembles (or Bayesian methods like GPs) to quantify predictive uncertainty and guide sampling or provide calibrated error estimates for scientific predictions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "general scientific prediction, active learning, and uncertainty quantification",
            "problem_description": "Provide predictions accompanied by uncertainty estimates to inform decision-making, detect extrapolation, and support active learning acquisition choices.",
            "data_availability": "helpful when data are limited and additional targeted data acquisition is planned; require enough data to form diverse ensemble members or infer posterior distributions.",
            "data_structure": "applies broadly to numerical feature vectors, spectra, images mapped through appropriate encoders.",
            "problem_complexity": "varies; uncertainty estimation adds computational overhead but is vital for safety and active sampling in complex domains.",
            "domain_maturity": "well-established methods with growing adoption in scientific ML.",
            "mechanistic_understanding_requirements": "medium - uncertainty estimates aid interpretability and trust but do not substitute for mechanistic understanding.",
            "ai_methodology_name": "Ensemble methods / uncertainty-aware models (e.g., bootstrapped ensembles, Bayesian approaches)",
            "ai_methodology_description": "Combine multiple models or use Bayesian formulations to produce predictive distributions or confidence intervals; used to identify high-uncertainty regions for sampling and to present calibrated error bars with predictions.",
            "ai_methodology_category": "supervised learning with probabilistic/ensemble uncertainty quantification",
            "applicability": "Highly applicable when decisions must account for model confidence; essential for active learning and detecting out-of-distribution inputs.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Portrayed as effective tools for sampling and detecting extrapolation; can enable predictions with error estimates though in severely ill-posed inversions the error bars may be unhelpful.",
            "impact_potential": "High for improving safety and reliability of ML-driven scientific workflows and for efficient experimental design.",
            "comparison_to_alternatives": "Preferred over single deterministic models when uncertainty estimation or active sampling is needed; contrasted with black-box deterministic neural nets that lack uncertainty measures.",
            "success_factors": "Proper ensemble diversity, calibrated uncertainty estimates, and integration with acquisition/decision policies.",
            "key_insight": "Uncertainty-aware models are essential for scientific deployment, enabling detection of extrapolation and guiding targeted data acquisition to expand reliable prediction regions.",
            "uuid": "e2291.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gaussian Processes for Machine Learning",
            "rating": 2,
            "sanitized_title": "gaussian_processes_for_machine_learning"
        },
        {
            "paper_title": "Deep learning with Python",
            "rating": 2,
            "sanitized_title": "deep_learning_with_python"
        },
        {
            "paper_title": "Pattern recognition and machine learning",
            "rating": 2,
            "sanitized_title": "pattern_recognition_and_machine_learning"
        },
        {
            "paper_title": "AI: the tumultuous history of the search for artificial intelligence",
            "rating": 1,
            "sanitized_title": "ai_the_tumultuous_history_of_the_search_for_artificial_intelligence"
        }
    ],
    "cost": 0.01873925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>When not to use machine learning: a perspective on potential and limitations</p>
<p>Matthew R Carbone 
Computational Science Initiative
Brookhaven National Laboratory
11973UptonNew York</p>
<p>When not to use machine learning: a perspective on potential and limitations
(Dated: October 7, 2022)artificial intelligencecomputation/computingmachine learningmodeling
The unparalleled success of artificial intelligence (AI) in the technology sector has catalyzed an enormous amount of research in the scientific community. It has proven to be a powerful tool, but as with any rapidly developing field, the deluge of information can be overwhelming, confusing and sometimes misleading. This can make it easy to become lost in the same hype cycles that have historically ended in the periods of scarce funding and depleted expectations known as AI Winters. Furthermore, while the importance of innovative, high-risk research cannot be overstated, it is also imperative to understand the fundamental limits of available techniques, especially in young fields where the rules appear to be constantly rewritten and as the likelihood of application to high-stakes scenarios increases. In this perspective, we highlight the guiding principles of data-driven modeling, how these principles imbue models with almost magical predictive power, and how they also impose limitations on the scope of problems they can address. Particularly, understanding when not to use data-driven techniques, such as machine learning, is not something commonly explored, but is just as important as knowing how to apply the techniques properly. We hope that the discussion to follow provides researchers throughout the sciences with a better understanding of when said techniques are appropriate, the pitfalls to watch for, and most importantly, the confidence to leverage the power they can provide.</p>
<p>I. INTRODUCTION</p>
<p>The story of artificial intelligence (AI) began in the mid 1900's, when computer scientists started considering a simple question: "can machines think?" [1,2]. Ever since, the mythical goal of achieving true "human-like" AI has framed decades of scientific conversation and has motivated many key breakthroughs, including the backbone of modern machine learning (ML) algorithms: neural networks [3][4][5]. As the computational machinery required to realize the practical applications of ML would come decades later [6], its full potential would not yet be immediately understood. That time has come, and despite the turbulence of multiple "AI winters" over the past decades [7][8][9], we are currently living the AI/ML revolution.</p>
<p>Broadly, AI and ML are two related families of methods that fall under the larger "data-driven" umbrella. Built upon well established theory in the statistics and applied mathematics communities [4], modern-day AI and ML is best understood as the intersection of powerful modeling paradigms with "big-data" and bleeding edge hardware (e.g. graphics processing units; GPUs). The general interpretation (though not the only one [10]) is that AI is a superset of ML [11] and consists of techniques that are used to mimic human cognition and decisionmaking, whereas ML is more focused on the mathematical and numerical approaches. Often, ML is described as the ability of a program to learn a task without being programmed with task-specific heuristics [12]. However, the distinction between AI and ML is not germane to most * mcarbone@bnl.gov applications (and many applications use parts of both), hence the blanket term "AI/ML" is used commonly as a substitute for "data-driven" in many contexts. In this perspective, we will focus primarily on supervised ML, though many of the key points to come apply to datadriven approaches in general.</p>
<p>Cruising behind the slipstream created by tremendous success in the technology sector, ML has found wide applicability in the materials, chemical and physical sciences. For example, the discovery, characterization and design of new materials, molecules and nanoparticles [13][14][15][16][17][18][19][20][21][22][23][24], surrogate models for spectroscopy and other properties [25][26][27][28], self-driving laboratories/autonomous experimentation [29][30][31][32][33][34], and neural network potentials [35][36][37][38][39] have all been powered by ML and related methods. The current state of ML in materials science specifically has also been thoroughly documented in many excellent reviews [15,[40][41][42] that cover subject matter ranging from applications to computational screening and interpretation. On a related note, for technical details and timely tutorials, we refer those interested readers to Refs. 43 and 44. However, while the scope of ML-relevant problems is huge, not every problem can effectively leverage the power ML provides. Worse still, sometimes ML may seem to be a perfectly reasonable choice only to fail dramatically [45]; such failures can often be traced back to the foundations of any ML tool: the data.</p>
<p>In this perspective, we ask and answer a foundational question which ultimately has everything to do with data: when should you not use machine learning? ML is the jackhammer of the applied math world, and is able to channel incredible power provided by the interplay of highly flexible models, large databases and GPUenabled supercomputers. But you wouldn't use a jackhammer to do brain surgery. At least for the time being, there are classes of problems for which ML is not wellsuited [43,46]. We address this issue not to dissuade researchers from using these methods, but rather to empower them to do so correctly, and to avoid wasting valuable time and resources. Understanding the limitations and application spaces of our tools will help us build better ones, and solve larger problems more confidently and with more consistency.</p>
<p>II. THE DEVIL'S IN THE DISTANCE</p>
<p>Newcomers to the field of ML will find themselves immediately buried under an avalanche of enticing algorithms applicable to their scientific problem [47]. Many of these choices are so sophisticated that it is unreasonable to expect any ML non-expert to understand their finer nuances and how/why they can fail. The steep learning curve combined with their intrinsic complexity, mythical "black box" nature and stunning ability to make accurate predictions can make ML appear almost magical. It may come as a surprise then that in spite of said complexities, almost all supervised ML models are paradigmatically the same and are built upon a familiar quantity: distance.</p>
<p>The supervised ML problem is one of minimizing the distance between predicted and true values mapped by an approximate function on the appropriate inputs. A distance can be a proper metric, such as the Euclidean or Manhattan norms, or something less pedestrian, such as a divergence (a distance measure between two distributions) or a cross-entropy loss function. Regardless, the principle is the same: consider un-regularized, supervised ML, where given a source of ground truth F and a ML model f θ , the goal is to find parameters θ such that the distance between F (x) and f θ (x) is as small as possible for all possible x in some use case. While this is only one type of ML, most techniques share this common theme. For example, Deep Q reinforcement learning [48] leverages neural networks to map states (inputs) to decisions (outputs), and unsupervised learning algorithms rely on the same notion of distance to perform clustering and dimensionality reduction that supervised learning techniques use to minimize loss functions. Variational autoencoders [49][50][51] try not only to minimize reconstruction loss, but simultaneously keep a compressed, latent representation as close to some target distribution as possible (usually for use in generative applications). Numerical optimization is the engine that systematically tunes model parameters θ in gradient-based ML, 1 and its only objective is to minimize some measure of distance between ground truth and model predictions.</p>
<p>Additionally, in order to increase the confidence that ML models will be successful for a given task, it helps if the desired function is smooth: i.e. a small change in a feature should ideally correspond to a relatively small change in the target. This idea is more readily defined for regression than for classification, and the data being amenable to gradient-based methods is is not strictly required for ML to be successful. For example, e.g. Random Forests are not generally trained using gradientbased optimizers, but satisfying this requirement will usually help models generalize more effectively. The distance between the features of any two points of data is informed entirely by their numerical vector representation, and while these representations can be intuitive or human-interpretable, they must be mathematically rigorous.</p>
<p>The devil here, so to speak, is that what might appear intuitive to the experimenter may not be to the machine. For example, consider the problem of discovering novel molecules with certain properties. Molecules can be first encoded in string format (e.g. SMILES [53]), and then a numerical latent representation. The structure of this latent space is informed by some target property [16], and because any point in the latent space is just a numeric vector living in a vector space, a distance can be easily defined. This powerful encoding method can be used to "interpolate between molecules" and thus discover new ones that perhaps we haven't previously considered, but it still relies on the principle of distance, both between molecules in the compressed latent space, and their target properties.</p>
<p>Concretely, the length scales for differentiating between data points in the feature space are set by the corresponding targets. Large changes in target values between data points can cause ML models to "focus" on the changes in the input space that caused it, possibly at the expense of failing to capture small changes. This is often referred to as the bias-variance trade-off. Most readers may be familiar with the concept of over-fitting: for instance, essentially any set of observations can be fit exactly by an arbitrarily high-order polynomial, but doing so will produce wildly varying results during inference and be unlikely to have captured anything meaningful about the underlying function. Conversely, a linear fit will only capture the most simple trends to the point of being useless for any nonlinear phenomena. Fig. 1 showcases a common middle ground, where the primary trend of the data is captured by a Gaussian Process [54], and smaller fluctuations are understood as noisy variations around that trend.</p>
<p>Consider a more realistic example: the Materials Project [55] database contains many geometry-relaxed structures, each with different compositions, space groups and local symmetries at 0 Kelvin. Thus, within this database, changes in e.g. the optical properties of these materials is primarily due to the aforementioned structural differences and not due to thermal disorder (i.e. distortions) one would find when running a molec-FIG. 1. A Gaussian Process, with a radial basis function kernel, fit to example data F (x) = 5x 2 + sin(50x)/3 + N (µ = 0, σ = 0.1). The small, noisy, high-frequency oscillations are not well-fit by a Gaussian Process, as the primary length scale in the data is defined by the distances between the three clusters, not in between them. Sophisticated, non-isotropic kernels could model both trends in principle, but the construction of such kernels often requires significant prior information about the problem, the knowledge of which might make modeling unnecessary. ular dynamics simulation. A ML model trained on this data could be limited in the sense that changes in certain structural motifs would be well-captured and others would not, necessitating caution when scoping its effective use cases. Conversely, in data-driven modeling, considering the distances between data points in both the input and output spaces, and thus which changes in your features are most contributing to the variance in the targets, can be instrumental in constructing effective training sets targeted to specific applications.</p>
<p>Inverse problems, and cases with extremely low signalto-noise ratios (SNR), are another case in which the usefulness of ML varies significantly problem-to-problem. ML can only model functions: cases in which each input maps to a unique output. Inverse problems, or those in which a signal is used to resolve its source, are often ill-posed, making it challenging if not impossible to represent them by functions. In these cases, ML is not immediately appropriate, and the problem statement must be refined until a "non-degenerate" subspace is found and can be modeled by a function. Once this space is identified, ML can excel, because it can pick out subtle patterns in this space where heuristics or human intuition may fail to do so, but the developed mapping must be a function. This can also be understood in the language of distance: in the inverse problem, an "epsilon-small" change in the input can result in an extremely large change in the out-put. 2 For example, consider phase retrieval in coherent diffraction imaging [56]. If a detector measures only the intensity of the signal, all phase information is lost by definition, and unless correlations between the intensity and phase exist (which are specific to the sub-problem of interest) and permit such an inverse mapping [57], there is no way to confidently retrieve the phase information. The degree to which this is possible in general depends entirely on the specific system and the data available.</p>
<p>The same issue can be found when the SNR is low or close to 1: no data-driven technique will be useful if targets cannot be distinguished from each other. Relatedly, if the uncertainty during inference on an inverse problem can be accurately quantified, then it is possible to use uncertainty-aware models to make predictions with error estimations [58][59][60]. However, in cases where the inversion is sufficiently ill-posed, the SNR will be so low that different results cannot be resolved. So while it might be possible to make predictions with error bars, they may not be useful.</p>
<p>In summary, it is always an instructive exercise to consider distances between the properties of entries in your database when attempting any data-driven modeling. The key is to fully understand the property of interest, and to choose a database and encoding such that different data points are sufficiently discriminatory with respect to the target property. If this is not possible, no data-driven method will be able to perform effectively. In these cases, utilization of other prior knowledge, more effective data screening or simply another type of technique entirely might be required. We note that there is no harm in simply trying new techniques on e.g. inverse problems where it is unclear how much pertinent information is present in a database that could allow an inverse mapping to be learned. Such possibility, however, should never be confused with certainty.</p>
<p>III. ON THE IMPORTANCE OF DATA DISTRIBUTIONS</p>
<p>To quantify the effectiveness of trained models, evaluation should always conclude with the presentation of evaluation metrics collected on a "testing" subset of the database. In order to avoid training and hyperparameter tuning 3 biases, the testing set should be disjoint from the set of data that was used to train and tune the models (the training and cross-validation databases, respectively). Put even more simply, the rule of thumb is to "blind" yourself from bias as best you can: take a chunk of data from the full space of data of interest, and not use it, look at it, or otherwise glean any information from it until you are ready to present results on a trained model and completed pipeline. As long as information from the testing set is not used during development, any approach to model tuning is appropriate (though some, such as using cross-validation, are highly recommended). These are the best practices which are critical to any successful ML project, and they are often highlighted in more technical tutorials [43,44] (and in more technical detail than presented here), but this is not the complete story.</p>
<p>The testing set is almost always discussed as an unbiased sample, a litmus test for how the model will perform on data it hasn't seen before. However, there is another use for the testing set: it should represent the real-world deployment scenario. In other words, the testing set should not only be disjoint from data the model and pipeline have seen before, it should also ideally represent the data on which the model must be performant. It is paramount to keep in mind that these two uses of the testing set are not always the same.</p>
<p>If the training data comes from a different distribution than data from your deployment scenario, it is highly likely the trained model will fail. Human intuition can actually take us far here, as there is a way to easily sanitycheck if any two sets of data do not come from the same distribution. Simply re-combine them and sample randomly. If you can easily tell the difference (i.e., determine from which distribution a sample originated), your testing set is "out-of-sample" with respect to the training set. This will not always be the case (see e.g. adversarial examples, where human-imperceptible modifications to images can cause otherwise highly accurate ML models to go haywire [61]), but in many scientific problems, it is a critical exercise to perform when planning a research campaign. For example, this can often happen when attempting to train a model on computer-simulated data (which is relatively cheap to obtain) and then deploying it on experimental data (which often requires expensive and time-consuming experiments). Such a use case is common in science, since we tend to have much less data available than in e.g. image recognition problems in computer science (where more labeled data can be simply bought). Indeed, there are some cases where the simulation is sufficiently accurate when compared to experimental measurements (e.g. predicting the space group from pair distribution data [20] or nanoparticle sizes from x-ray absorption spectra; XAS [14]). Other cases will not work nearly as well, such as when comparing experimental and simulated XAS across a diverse crystal structure database [19]. Fig. 2 showcases this possible failure scenario.</p>
<p>While it is not a hard-and-fast rule, one should never take for granted that data-driven models fit on one set of data will perform well on another. On the contrary, ML performs best (and is most powerful) when it is explicitly fit on the same type of data it is expected to perform on. This is a limitation that is often interpreted as weakness; FIG. 2. A possible failure scenario in data-driven modeling: the model is fit to the training (including cross-validation) data, and evaluated on a testing set. The testing set is sampled (but is disjoint) from the same data as the training set, resulting in strong distribution-wise overlap with the training data and likely good performance. The deployment case is sampled from a different database, the distribution of which may only partially overlap with data the model is familiar with.</p>
<p>on the contrary, this is actually a strength. For example, one high-impact example demonstrating this feature is that of neural network potentials [35][36][37][38][39], where the space of possible atomic configurations is kept small (potentials are fit on specific systems), and configurations are revisited throughout long molecular dynamics simulations. Potentials fit on one system are not expected to perform well on another, but they do perform to desired accuracy on the systems they're trained on.</p>
<p>Ensuring proper data selection is not a technical challenge, it is a human one [45]. When considering if ML is the right tool for your problem the real-world deployment scenario must be considered. When at all possible, one should simply fit the model on data from the same distribution as in said deployment. In cases where there is not enough deployment data to fit models on (or to do transfer learning [62]) it is unlikely ML methods will perform well. If the scenario outlined in Fig. 2 is a possibility, sufficient labeled data from the deployment case must be available to validate that the model is working properly. If these criteria cannot be satisfied, then there is no feasible way to validate the trained models in the desired use cases. Consequently, if the model's performance cannot be verified, it cannot be used.</p>
<p>IV. THE DATA-DRIVEN "NO FREE LUNCH THEOREM"</p>
<p>The considerable flexibility and information capacity of modern ML models (such as neural networks) comes at a cost. While exceptional at "interpolating" within and close to the "convex hull" defined by the boundaries of the training set, they will often fail in spectacular fashion when tasked with predicting outside of this region [36,46]. This limitation can be partially addressed by the encoding of prior belief, which can take many forms, such as the functional form of the model, correlation information between input features, or boundary behavior. The information content of prior belief can be a game-changer: indeed, even the term "datadriven" can be somewhat misleading [9] (though the description "information-driven", while perhaps more accurate, may be a bit too ambiguous). That said, no datadriven model can make reliable predictions outside of the union of the data and prior information it was trained on. This information-theoretic perspective is tautological, but is often overlooked despite its significant implications: data-driven models generalize, they do not extrapolate beyond the aforementioned union with any reliability.</p>
<p>It is important to keep in mind that there is a difference between some trained ML model and an overhead algorithm that is operating for a particular use case which might be using one or more trained models. Often, these algorithms will involve a re-training step in which the inference or decision-making model is continuously updated. For example, reinforcement learning involves an "outer loop" in which the environment is probed and feedback acquired through a reward function, decisions are made, and the decision maker refined. Gaussian Processes and ensemble methods are excellent choices for sampling new data because they naturally quantify uncertainty. Data can be sampled where uncertainty, and thus the likelihood of out-of-sample data, is high. Computer scientists already have a name for this: active learning. This can help the user understand where the model is predicting outside of it's information-theoretic interpolation window, and shore up the model's weaknesses by adding new data to the training set. Another way to interpret active learning is that the algorithm itself is detecting when it is extrapolating, and actively expanding its interpolation window to compensate.</p>
<p>To demonstrate the point, consider the apparently simple 1-dimensional example in Fig. 3, where a neural network and Gaussian Process are tasked with modeling the function F (x) = sin(4x) with minimal observations. The neural network was trained only to minimize the mean squared error loss function on the provided data. Thus, it ends up fitting the data quite well around where data exists, but fails completely when far away from those points. These failures are essentially random and unpredictable, and are due to the particular state of the neural network's weights. Even "within" the convex hull of the training data, in the region x ∈ [−1, 1], the neural network does not perform, because the optimizer has no incentive to focus on that region (due to lack of ground truth information). On the other hand, the Gaussian Process is trained using kernels that explicitly encode correlation lengths, limiting the possible set of interpolating functions in the region x ∈ [−1, 1]. This ultimately results in a much better fit, consistent with the learned length scale of the kernel and the data used to fit it. That said, any model that operates on the Bayesian paradigm of starting with a prior belief which is then updated to a posterior when fit on data will revert back to the mean when sufficiently outside of the space of the data it is fit on; this is clearly observed here (the mean of the prior is 0).</p>
<p>One other important observation is that despite the natural human conclusion that the data is likely periodic (which we reach by simply looking at the black markers in Fig. 3), the neural network cannot intuit this without prior knowledge. Thousands more data points could be provided, spanning a much larger range in x, but no amount of data will encode periodicity in a way that a data-driven model understands. On the other hand, when encoded as a prior belief, either in the form of a kernel or perhaps even through explicit selection of a periodic function, only a handful of data points are necessary to completely characterize it. Expecting the model to understand periodicity without explicitly imbuing it with said information is akin to information-theoretic extrapolation: a non-starter.</p>
<p>Unfortunately, most modeling problems of interest are not quite as simple as the 1-dimensional example in Fig. 3. Often, they have much higher-dimensional inputs and outputs, and display complicated non-linear behavior. It is much harder to visualize and interpret results in these situations (though tools are available, such as dimensionality reduction, e.g. Principal Component Analysis), and intuition derived from trial and error is usually the go-to method for understanding when and why the model is not performing well. At the least, care should be taken when formulating data-driven solutions to ensure that the information used to fit the models is carefully thought through. If it is possible the deployment scenario will include out-of-sample data, uncertainty quantification and active learning should be considered. Most of all, the way that humans understand and process data is very different from the way ML algorithms do, and that should never be overlooked.</p>
<p>V. OUTLOOK: AVOIDING THE NEXT AI WINTER</p>
<p>AI/ML has been transformative in our society over the past decade. Especially in the technology sector, it has been applied with unnervingly surgical accuracy in targeted advertising, image recognition and neural translation, just to name a few examples. In these problem spaces, AI/ML is massively successful because they leverage its greatest strengths: the ability to process huge databases, and complex pattern recognition. Naturally, the scientific community has taken note and applied AI/ML to great effect in research acceleration and discovery. However, these techniques are not a cure-all, and cannot be applied to every problem. Without a doubt, we should keep pushing the boundaries of how we can apply AI/ML in science, but expectations should be kept appropriately measured.</p>
<p>The original AI Winters were caused by outrageously inflated expectations, spurred on by the promise of true AI, and while the actual winter always returns, it is hard to say if another AI Winter is on the horizon [8,63]. In retrospect, the original idea of creating a synthetic autonomous thinker akin to a human was incredibly arrogant. After all, we're competing with millions of years of evolutionary instinct and development, including the most complex black box AI/ML algorithm we know of: the human brain. We would pose a simple question: why compete when we can collaborate?</p>
<p>One day, humanity will likely create sentient AI, 4 but we're not there yet, and for better or for worse, we're not even close. What we do have is a wonderful suite of data-driven tools, including those found in the domain of AI/ML, which have the potential to significantly accelerate scientific research and discovery. These tools are meant to empower the experimenter, not to replace them. For the foreseeable future, we must still rely on human researchers to start problems with scientific hypotheses, find appropriate use cases for data-driven tools, and to apply them properly. AI/ML is not magic, and it is of the utmost importance, not only for each individual research project but also to the future of AI/ML in science, that its potential is never taken for granted.</p>
<p>FIG. 3 .
3A simple example of fitting example data (black) with two models: a simple neural network (red) and Gaussian Process with a periodic kernel (blue).
The classic numerical optimizer is gradient/stochastic gradient descent, with more recently established developments showing systematic improvements in deep learning, e.g. Adam[52].
In the inverse problem, an "input" might be an observable, such as a spectrum, and the output, a structure, such as in the structure refinement problem.3 Hyperparameters are un-trained parameters of the model and training procedure, such as the choice of loss function, or the number of neurons in some layer of a neural network.
For the sake of argument, we will take humanity's survival for granted.
CONFLICTS OF INTERESTMRC declares no conflicts of interest.
. A M Turing, Mind. 59433A. M. Turing, Mind 59, 433 (1950).</p>
<p>. M I Jordan, T M Mitchell, Science. 349255M. I. Jordan and T. M. Mitchell, Science 349, 255 (2015).</p>
<p>. W S Mcculloch, W Pitts, Bull Math Biophys. 5115W. S. McCulloch and W. Pitts, Bull Math Biophys. 5, 115 (1943).</p>
<p>W S Sarle, Proceedings of the Nineteenth Annual SAS Users Groups International Conference. the Nineteenth Annual SAS Users Groups International ConferenceSAS Institute, IncW. S. Sarle, in Proceedings of the Nineteenth Annual SAS Users Groups International Conference (SAS Institute, Inc., 1994) p. 1538-1550.</p>
<p>. M Paliwal, U A Kumar, Expert Syst. Appl. 362M. Paliwal and U. A. Kumar, Expert Syst. Appl. 36, 2 (2009).</p>
<p>C M Bishop, N M Nasrabadi, Pattern recognition and machine learning. Springer4C. M. Bishop and N. M. Nasrabadi, Pattern recognition and machine learning, Vol. 4 (Springer, 2006).</p>
<p>AI: the tumultuous history of the search for artificial intelligence. D Crevier, Basic Books, IncD. Crevier, AI: the tumultuous history of the search for artificial intelligence (Basic Books, Inc., 1993).</p>
<p>. J Hendler, IEEE Intell. Syst. 23J. Hendler, IEEE Intell. Syst. 23, 2 (2008).</p>
<p>. K G Reyes, B Maruyama, Bull, 44530K. G. Reyes and B. Maruyama, MRS Bull. 44, 530 (2019).</p>
<p>. P Langley, Mach. Learn. 82275P. Langley et al., Mach. Learn. 82, 275 (2011).</p>
<p>A Holzinger, P Kieseberg, E Weippl, A M Tjoa, International Cross-Domain Conference for Machine Learning and Knowledge Extraction. SpringerA. Holzinger, P. Kieseberg, E. Weippl, and A. M. Tjoa, in International Cross-Domain Conference for Machine Learning and Knowledge Extraction (Springer, 2018) pp. 1-8.</p>
<p>. J B Mitchell, Wiley Interdiscip, Rev. Comput. Mol. Sci. 4468J. B. Mitchell, Wiley Interdiscip. Rev. Comput. Mol. Sci. 4, 468 (2014).</p>
<p>. P Juhás, C L Farrow, X Yang, K R Knox, S J Billinge, Acta Crystallogr. A. 71562P. Juhás, C. L. Farrow, X. Yang, K. R. Knox, and S. J. Billinge, Acta Crystallogr. A 71, 562 (2015).</p>
<p>. J Timoshenko, D Lu, Y Lin, A I Frenkel, J. Phys. Chem. Lett. 85091J. Timoshenko, D. Lu, Y. Lin, and A. I. Frenkel, J. Phys. Chem. Lett. 8, 5091 (2017).</p>
<p>. K T Butler, D W Davies, H Cartwright, O Isayev, A Walsh, Nature. 559547K. T. Butler, D. W. Davies, H. Cartwright, O. Isayev, and A. Walsh, Nature 559, 547 (2018).</p>
<p>. B Sanchez-Lengeling, A Aspuru-Guzik, Science. 361360B. Sanchez-Lengeling and A. Aspuru-Guzik, Science 361, 360 (2018).</p>
<p>. C W Coley, W H Green, K F Jensen, Acc. Chem. Res. 511281C. W. Coley, W. H. Green, and K. F. Jensen, Acc. Chem. Res. 51, 1281 (2018).</p>
<p>. R Gómez-Bombarelli, J N Wei, D Duvenaud, J M Hernández-Lobato, B Sánchez-Lengeling, D Sheberla, J Aguilera-Iparraguirre, T D Hirzel, R P Adams, A Aspuru-Guzik, ACS Cent. Sci. 4268R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernández-Lobato, B. Sánchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik, ACS Cent. Sci. 4, 268 (2018).</p>
<p>. M R Carbone, S Yoo, M Topsakal, D Lu, Phys. Rev. Mater. 333604M. R. Carbone, S. Yoo, M. Topsakal, and D. Lu, Phys. Rev. Mater. 3, 033604 (2019).</p>
<p>. C.-H Liu, Y Tao, D Hsu, Q Du, S J Billinge, Acta Crystallogr. A. 75633C.-H. Liu, Y. Tao, D. Hsu, Q. Du, and S. J. Billinge, Acta Crystallogr. A 75, 633 (2019).</p>
<p>. Y Zhang, X He, Z Chen, Q Bai, A M Nolan, C A Roberts, D Banerjee, T Matsunaga, Y Mo, C Ling, Nat. Comm. 101Y. Zhang, X. He, Z. Chen, Q. Bai, A. M. Nolan, C. A. Roberts, D. Banerjee, T. Matsunaga, Y. Mo, and C. Ling, Nat. Comm. 10, 1 (2019).</p>
<p>. S B Torrisi, M R Carbone, B A Rohr, J H Montoya, Y Ha, J Yano, S K Suram, L Hung, Comput. Mater. 61S. B. Torrisi, M. R. Carbone, B. A. Rohr, J. H. Montoya, Y. Ha, J. Yano, S. K. Suram, and L. Hung, npj Comput. Mater. 6, 1 (2020).</p>
<p>. R Mercado, T Rastemo, E Lindelöf, G Klambauer, O Engkvist, H Chen, E J Bjerrum, Mach. Learn. Sci. Technol. 225023R. Mercado, T. Rastemo, E. Lindelöf, G. Klambauer, O. Engkvist, H. Chen, and E. J. Bjerrum, Mach. Learn. Sci. Technol. 2, 025023 (2021).</p>
<p>. V D Mouchlis, A Afantitis, A Serra, M Fratello, A G Papadiamantis, V Aidinis, I Lynch, D Greco, G Melagraki, Int. J. Mol. Sci. 221676V. D. Mouchlis, A. Afantitis, A. Serra, M. Fratello, A. G. Papadiamantis, V. Aidinis, I. Lynch, D. Greco, and G. Melagraki, Int. J. Mol. Sci. 22, 1676 (2021).</p>
<p>J Gilmer, S S Schoenholz, P F Riley, O Vinyals, G E Dahl, International conference on machine learning. PMLRJ. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, in International conference on machine learn- ing (PMLR, 2017) pp. 1263-1272.</p>
<p>. T Xie, J C Grossman, Phys. Rev. Lett. 120145301T. Xie and J. C. Grossman, Phys. Rev. Lett. 120, 145301 (2018).</p>
<p>. M R Carbone, M Topsakal, D Lu, S Yoo, Phys. Rev. Lett. 124156401M. R. Carbone, M. Topsakal, D. Lu, and S. Yoo, Phys. Rev. Lett. 124, 156401 (2020).</p>
<p>. C D Rankine, T Penfold, J. Chem. Phys. 156164102C. D. Rankine and T. Penfold, J. Chem. Phys. 156, 164102 (2022).</p>
<p>. M M Noack, G S Doerk, R Li, M Fukuto, K G Yager, Sci. Rep. 101M. M. Noack, G. S. Doerk, R. Li, M. Fukuto, and K. G. Yager, Sci. Rep. 10, 1 (2020).</p>
<p>. B P Macleod, F G Parlane, T D Morrissey, F Häse, L M Roch, K E Dettelbach, R Moreira, L P Yunker, M B Rooney, J R Deeth, Sci. Adv. 68867B. P. MacLeod, F. G. Parlane, T. D. Morrissey, F. Häse, L. M. Roch, K. E. Dettelbach, R. Moreira, L. P. Yunker, M. B. Rooney, J. R. Deeth, et al., Sci. Adv. 6, eaaz8867 (2020).</p>
<p>. R W Epps, M S Bowen, A A Volk, K Abdel-Latif, S Han, K G Reyes, A Amassian, M Abolhasani, Adv. Mater. 32R. W. Epps, M. S. Bowen, A. A. Volk, K. Abdel-Latif, S. Han, K. G. Reyes, A. Amassian, and M. Abolhasani, Adv. Mater. 32, 2001626 (2020).</p>
<p>. M M Noack, P H Zwart, D M Ushizima, M Fukuto, K G Yager, K C Elbert, C B Murray, A Stein, G S , M. M. Noack, P. H. Zwart, D. M. Ushizima, M. Fukuto, K. G. Yager, K. C. Elbert, C. B. Murray, A. Stein, G. S.</p>
<p>. E H Doerk, Tsai, Nat. Rev. Phys. 3685Doerk, E. H. Tsai, et al., Nat. Rev. Phys. 3, 685 (2021).</p>
<p>. F Bateni, R W Epps, K Antami, R Dargis, J A Bennett, K G Reyes, M Abolhasani, Adv. Intell. Syst. 2200017F. Bateni, R. W. Epps, K. Antami, R. Dargis, J. A. Ben- nett, K. G. Reyes, and M. Abolhasani, Adv. Intell. Syst. , 2200017 (2022).</p>
<p>. T Konstantinova, P M Maffettone, B Ravel, S I Campbell, A M Barbour, D Olds, Digital Discovery. T. Konstantinova, P. M. Maffettone, B. Ravel, S. I. Campbell, A. M. Barbour, and D. Olds, Digital Dis- covery , (2022).</p>
<p>. J Behler, M Parrinello, Phys. Rev. Lett. 98146401J. Behler and M. Parrinello, Phys. Rev. Lett. 98, 146401 (2007).</p>
<p>. J Behler, J. Chem. Phys. 13474106J. Behler, J. Chem. Phys. 134, 074106 (2011).</p>
<p>. J Behler, Phys. Chem. Chem. Phys. 1317930J. Behler, Phys. Chem. Chem. Phys. 13, 17930 (2011).</p>
<p>. N Artrith, A Urban, Comput. Mater. Sci. 114135N. Artrith and A. Urban, Comput. Mater. Sci. 114, 135 (2016).</p>
<p>. J Behler, Chem. Rev. 12110037J. Behler, Chem. Rev. 121, 10037 (2021).</p>
<p>. C P Gomes, B Selman, J M Gregoire, MRS Bull. 44538C. P. Gomes, B. Selman, and J. M. Gregoire, MRS Bull. 44, 538 (2019).</p>
<p>. J Wei, X Chu, X.-Y Sun, K Xu, H.-X Deng, J Chen, Z Wei, M Lei, InfoMat. 1338J. Wei, X. Chu, X.-Y. Sun, K. Xu, H.-X. Deng, J. Chen, Z. Wei, and M. Lei, InfoMat 1, 338 (2019).</p>
<p>. D Morgan, R Jacobs, Annu. Rev. Mater. Res. 5071D. Morgan and R. Jacobs, Annu. Rev. Mater. Res. 50, 71 (2020).</p>
<p>. A Y Wang, R J Murdock, S K Kauwe, A O Oliynyk, A Gurlo, J Brgoch, K A Persson, T D Sparks, Chem. Mater. 324954A. Y.-T. Wang, R. J. Murdock, S. K. Kauwe, A. O. Oliynyk, A. Gurlo, J. Brgoch, K. A. Persson, and T. D. Sparks, Chem. Mater. 32, 4954 (2020).</p>
<p>. N Artrith, K T Butler, F.-X Coudert, S Han, O Isayev, A Jain, A Walsh, Nat. Chem. 13505N. Artrith, K. T. Butler, F.-X. Coudert, S. Han, O. Isayev, A. Jain, and A. Walsh, Nat. Chem. 13, 505 (2021).</p>
<p>N Sambasivan, S Kapania, H Highfill, D Akrong, P Paritosh, L M Aroyo, proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. the 2021 CHI Conference on Human Factors in Computing SystemsN. Sambasivan, S. Kapania, H. Highfill, D. Akrong, P. Paritosh, and L. M. Aroyo, in proceedings of the 2021 CHI Conference on Human Factors in Computing Sys- tems (2021) pp. 1-15.</p>
<p>F Chollet, Deep learning with Python (Simon and Schuster. F. Chollet, Deep learning with Python (Simon and Schus- ter, 2021).</p>
<p>. T O Ayodele, New Advances in Machine Learning. 319T. O. Ayodele, New Advances in Machine Learning 3, 19 (2010).</p>
<p>. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 518529V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve- ness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., Nature 518, 529 (2015).</p>
<p>D P Kingma, M Welling, 2nd International Conference on Learning Representations. Y. Bengio and Y. LeCunBanff, AB, CanadaConference Track ProceedingsD. P. Kingma and M. Welling, in 2nd International Con- ference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Pro- ceedings, edited by Y. Bengio and Y. LeCun (2014).</p>
<p>I Higgins, L Matthey, A Pal, C Burgess, X Glorot, M Botvinick, S Mohamed, A Lerchner, 5th International Conference on Learning Representations. Toulon, FranceConference Track ProceedingsI. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner, in 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Confer- ence Track Proceedings (OpenReview.net, 2017).</p>
<p>. C Miles, M R Carbone, E J Sturm, D Lu, A Weichselbaum, K Barros, R M Konik, Phys. Rev. B. 104235111C. Miles, M. R. Carbone, E. J. Sturm, D. Lu, A. We- ichselbaum, K. Barros, and R. M. Konik, Phys. Rev. B. 104, 235111 (2021).</p>
<p>. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba, arXiv preprint arXiv:1412.6980 (2014).</p>
<p>. D Weininger, J. Chem. Inf. Model. 2831D. Weininger, J. Chem. Inf. Model. 28, 31 (1988).</p>
<p>C E Rasmussen, C K I Williams, Gaussian Processes for Machine Learning. CambridgeMIT PressC. E. Rasmussen and C. K. I. Williams, Gaussian Pro- cesses for Machine Learning (Cambridge: MIT Press, 2006).</p>
<p>. A Jain, S P Ong, G Hautier, W Chen, W D Richards, S Dacek, S Cholia, D Gunter, D Skinner, G Ceder, K A Persson, Mater, 111002A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, and K. a. Persson, APL Mater. 1, 011002 (2013).</p>
<p>. F Zhang, B Chen, G R Morrison, J Vila-Comamala, M Guizar-Sicairos, I K Robinson, Nat. Comm. 71F. Zhang, B. Chen, G. R. Morrison, J. Vila-Comamala, M. Guizar-Sicairos, and I. K. Robinson, Nat. Comm. 7, 1 (2016).</p>
<p>. L Wu, S Yoo, A F Suzana, T A Assefa, J Diao, R J Harder, W Cha, I K Robinson, Comput. Mater. 71L. Wu, S. Yoo, A. F. Suzana, T. A. Assefa, J. Diao, R. J. Harder, W. Cha, and I. K. Robinson, npj Comput. Mater. 7, 1 (2021).</p>
<p>. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, JMLR. 151929N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, JMLR 15, 1929 (2014).</p>
<p>Advances in neural information processing systems. A G Wilson, P Izmailov, 334697A. G. Wilson and P. Izmailov, Advances in neural infor- mation processing systems 33, 4697 (2020).</p>
<p>. L V Jospin, H Laga, F Boussaid, W Buntine, M Bennamoun, IEEE Computational Intelligence Magazine. 1729L. V. Jospin, H. Laga, F. Boussaid, W. Buntine, and M. Bennamoun, IEEE Computational Intelligence Mag- azine 17, 29 (2022).</p>
<p>I J Goodfellow, J Shlens, C Szegedy, International Conference on Learning Representations (ICLR. I. J. Goodfellow, J. Shlens, and C. Szegedy, in Interna- tional Conference on Learning Representations (ICLR) (2015).</p>
<p>. K Weiss, T M Khoshgoftaar, D Wang, J. Big Data. 31K. Weiss, T. M. Khoshgoftaar, and D. Wang, J. Big Data 3, 1 (2016).</p>
<p>. L Floridi, Philos. Technol. 331L. Floridi, Philos. Technol. 33, 1 (2020).</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6657 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6657</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6657</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-84c03d0ec9dd00de3dc7ea7577ceefc9f093c564</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/84c03d0ec9dd00de3dc7ea7577ceefc9f093c564" target="_blank">MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is found that MuggleMath is weak in out-of-domain math reasoning generalization from AugGSM8K to MATH and from AugMATH to GSM8K, which suggests that augmenting queries that cover a broader range of subjects is more beneficial for generalization.</p>
                <p><strong>Paper Abstract:</strong> In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create two new dataset AugGSM8K and AugMATH, by complicating and diversifying the queries and sampling multiple reasoning paths from GSM8K and MATH. We obtained a series of LLMs called MuggleMath by fine-tuning LLaMA models on AugGSM8K and AugMATH. MuggleMath substantially achieves new state-of-the-art on GSM8K and MATH. A log-linear relationship and a segmented log-linear are presented between MuggleMath's performance and the amount of augmented data on GSM8K and MATH, respectively. We also find that it is weak in out-of-domain math reasoning generalization from AugGSM8K to MATH and from AugMATH to GSM8K, which suggests that augmenting queries that cover a broader range of subjects is more beneficial for generalization. We release our codes and augmented data in https://github.com/OFA-Sys/gsm8k-ScRel.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6657.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6657.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuggleMath-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuggleMath (LLaMA-derived fine-tuned model, 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LLaMA-family 7B decoder-only transformer fully fine-tuned on AugGSM8K / AugMATH (synthetic queries and chain-of-thought style responses generated by GPT-3.5/GPT-4) to improve multi-step math word problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuggleMath-7B (fine-tuned LLaMA 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (LLaMA family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained LLaMA checkpoint fully fine-tuned on GSM8K plus AugGSM8K (query-augmented problems generated by GPT-3.5/GPT-4 and response aug. via GPT-4/GPT-3.5); training used system prompt, 1-shot response-format prompt for augmentation, and manual filtering rules.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K (in-domain) and MATH (out-of-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step arithmetic word problems (elementary school math in GSM8K) and general math reasoning when evaluated on MATH subsets</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems with chain-of-thought style reasoning paths (textual solutions ending with numeric answer)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K: elementary-school multi-step; MATH: high-school/competition-level (hard)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Supervised fine-tuning on synthetic chain-of-thought responses; augmentation generation used: 1-shot response prompt, query augmentation (5 types), temperature variations (0.0/1.0), greedy decoding for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (exact numeric answer; greedy decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 66.0% accuracy (Table 4); MATH: 18.4% accuracy (Table 5, in-domain AugMATH fine-tune result for 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors visualized problem embeddings (t-SNE) using the 15th-layer last-token LLaMA-2-7B representations showing AugGSM8K and GSM8K occupy similar regions while MATH is largely separate; training vs test accuracy are positively correlated; analysis of response-quality effects showed that noisy/wrong GPT-generated responses can still help weaker models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited out-of-domain generalization (AugGSM8K → MATH yields marginal gains); some augmented responses contained no final answer or overly long/incorrect reasoning paths requiring manual filtering; majority-voting filtering of responses sometimes reduced performance; wrong/generated-but-plausible reasoning can both help and harm depending on model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>In-domain performance increases roughly log-linearly with amount of augmented queries in the 13k–97k range; response-augmentation shows saturation (for 7B models benefit plateaus after ~3 responses per augmented query, ~97K responses). Fitted scaling: y = 10.7 * log(x) + 13.2 (see Table 3 for 7B fit on GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6657.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6657.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuggleMath-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuggleMath (LLaMA-2-derived fine-tuned model, 13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LLaMA-2 13B decoder-only transformer fully fine-tuned on AugGSM8K / AugMATH synthetic chain-of-thought data to improve multi-step math problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuggleMath-13B (fine-tuned LLaMA-2 13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (LLaMA-2 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained LLaMA-2-13B fully fine-tuned with GSM8K augmented by AugGSM8K (GPT-3.5/GPT-4 generated queries and multiple response paths) and AugMATH variants; same system prompt and training hyperparameters as other experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K (in-domain) and MATH (in-domain for AugMATH experiments and out-of-domain for AugGSM8K→MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step arithmetic and algebraic word problems (GSM8K) and harder contest-style math (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems with textual reasoning chains (chain-of-thought) and numerical final answers</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K: elementary multi-step; MATH: high-school and competition-level (hard)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Supervised fine-tuning on synthetic chain-of-thought; augmentation used 1-shot response-format prompts; evaluated with greedy decoding; tests included multi-task and transfer learning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (exact numeric answer)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 74.3% accuracy (Table 4, MuggleMath 13B); MATH: 30.7% accuracy (Table 5, MuggleMath 13B on AugMATH+MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Scaling fits show smaller slope vs 7B (harder to improve for better base models): fitted GSM8K scaling y = 7.6 * log(x) + 36.3 (Table 3); embedding-space t-SNE reveals dataset distribution mismatch explaining poor transfer; analysis of difficulty categories (easy/medium/hard by reasoning steps) shows augmentation of hard problems yields larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>OOD generalization limited (AugGSM8K yields little MATH improvement); majority-voting response filtering sometimes reduces performance; stronger models are harmed more by low-quality (incorrect) augmented responses compared to weaker ones.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Log-linear improvement with augmented query amount but with smaller coefficient than 7B; on MATH a segmented log-linear relationship observed (low-data and high-data segments with different log slopes). Response-augmentation yields continued gains for 13B across a wider response-volume range (37K–157K) than for 7B before plateauing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6657.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6657.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuggleMath-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuggleMath (LLaMA-2-derived fine-tuned model, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large LLaMA-2 70B decoder-only transformer fine-tuned with Augmented GSM8K/MATH data to push state-of-the-art open-source performance on math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuggleMath-70B (fine-tuned LLaMA-2 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (LLaMA-2 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained LLaMA-2-70B fully fine-tuned on GSM8K plus AugGSM8K / AugMATH synthetic datasets generated primarily by GPT-4 with filtered chain-of-thought responses.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step arithmetic word problems and hard math reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems with chain-of-thought style solutions</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K elementary multi-step; MATH hard</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Supervised fine-tuning on augmented chain-of-thought responses; augmentation used GPT-4-generated responses (low temperature variants) and 1-shot formatting; greedy decoding for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 82.7% accuracy (Table 4, MuggleMath 70B); MATH: 36.3% accuracy (Table 5, MuggleMath 70B on AugMATH+MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Large model achieves highest absolute gains but exhibits smaller relative slope in scaling fit (harder to improve further with limited augmentation); embedding visualization suggests dataset distribution mismatch still affects transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Although high in-domain accuracy, still limited cross-dataset generalization (AugGSM8K→MATH); noisy augmented responses can still influence learning and must be filtered; diminishing returns present at large model scales.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Absolute performance higher; relative benefit per doubling of augmented queries smaller than for smaller models; log-linear trends on GSM8K persist but with reduced slope; segmented log-linear behavior on MATH observed at larger data scales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6657.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6657.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI) used as generator for query and response augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary large language model (GPT-4) used in this work to generate diverse augmented queries and high-quality chain-of-thought response paths for supervised fine-tuning data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (used as generator for augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (proprietary OpenAI family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in paper (proprietary); used as an oracle to generate diverse queries and multiple reasoning paths for dataset augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used to generate training data for GSM8K and MATH augmentations (AugGSM8K and AugMATH)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Generation of synthetic multi-step reasoning paths and rephrasings for arithmetic/word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompts (1-shot response prompt for stable format), temperature variations used (0.0 and 1.0)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Capable of producing both simple and complex (increased-complexity) augmented problems</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>1-shot response-format prompt to stabilize chain-of-thought outputs; used for query augmentation and to sample multiple reasoning paths per augmented query; some zero-shot variants also tested</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not directly evaluated on benchmark; used to produce training data whose downstream effect measured by accuracy of fine-tuned models</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Generators: GPT-4-based responses produced higher-quality augmentation leading to larger SFT gains than GPT-3.5 (comparing subsets $∈$ Table 2/experiment descriptions). No absolute accuracy reported for GPT-4 as solver in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper reports generator choice matters: GPT-4 responses yield better SFT performance than GPT-3.5; temperature changes (0.0 vs 1.0) had small impact when GPT-4 used for responses; 1-shot prompt stabilized format and improved downstream gains vs zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Augmented responses sometimes lacked final answer, were excessively long, or contained incorrect reasoning and required manual filtering; even GPT-4 has non-negligible error rates on MATH problems.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Augmentation quality from GPT-4 yields strong downstream gains; however, marginal returns and OOD transfer limited by distributional mismatch between generated queries and target dataset subjects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6657.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6657.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo used as a cheaper generator for queries/responses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller OpenAI model used to generate augmented queries and responses; generally produced lower-quality responses than GPT-4 but still useful for augmentation at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo (generation role)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (OpenAI family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (proprietary, smaller than GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified; used to generate augmented queries and some response paths (temperature=1.0 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used only as a generator for SFT data (AugGSM8K subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Generation of synthetic queries and chain-of-thought style responses for arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompts; responses produced in a stabilized python-dictionary / formatted style via 1-shot prompt where used</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Generated both simpler and more complex variants depending on prompt</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Used same query-augmentation prompts (5 types) and 1-shot response-format prompt in some subsets; responses sampled at temperature 1.0 in many subsets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not directly evaluated; downstream SFT with GPT-3.5-generated responses yielded smaller gains than GPT-4-generated responses (see experiment comparisons in Section 3.3/3.4)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>When response augmentation used GPT-3.5, SFT gains were significantly lower than using GPT-4 (e.g., comparing $∈$ subsets in Table 2; exact numeric downstream differences reported in the paper's tables).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors found generator model quality influences SFT efficacy: GPT-4 responses > GPT-3.5 responses; temperature differences for GPT-4 had modest effect; prompts (1-shot) matter to ensure consistent response formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>GPT-3.5 produces more incorrect or inconsistent reasoning paths which can still help weaker models but hurts stronger models when used as SFT labels; responses sometimes required manual filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Using higher-quality generator (GPT-4) yields better downstream scaling with augmented data; lower-quality generator still provides benefit but with smaller slope.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scaling relationship on learning mathematical reasoning with large language models <em>(Rating: 2)</em></li>
                <li>How well do large language models perform in arithmetic tasks? <em>(Rating: 2)</em></li>
                <li>WizardMath: Empowering mathematical reasoning for large language models via reinforced evol-instruct <em>(Rating: 2)</em></li>
                <li>Chain‑of‑thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Let's verify step by step <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6657",
    "paper_id": "paper-84c03d0ec9dd00de3dc7ea7577ceefc9f093c564",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "MuggleMath-7B",
            "name_full": "MuggleMath (LLaMA-derived fine-tuned model, 7B)",
            "brief_description": "A LLaMA-family 7B decoder-only transformer fully fine-tuned on AugGSM8K / AugMATH (synthetic queries and chain-of-thought style responses generated by GPT-3.5/GPT-4) to improve multi-step math word problem solving.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MuggleMath-7B (fine-tuned LLaMA 7B)",
            "model_family": "decoder-only transformer (LLaMA family)",
            "model_size": "7B",
            "training_data_description": "Pretrained LLaMA checkpoint fully fine-tuned on GSM8K plus AugGSM8K (query-augmented problems generated by GPT-3.5/GPT-4 and response aug. via GPT-4/GPT-3.5); training used system prompt, 1-shot response-format prompt for augmentation, and manual filtering rules.",
            "benchmark_name": "GSM8K (in-domain) and MATH (out-of-domain)",
            "task_type": "Multi-step arithmetic word problems (elementary school math in GSM8K) and general math reasoning when evaluated on MATH subsets",
            "problem_format": "Natural-language word problems with chain-of-thought style reasoning paths (textual solutions ending with numeric answer)",
            "difficulty_level": "GSM8K: elementary-school multi-step; MATH: high-school/competition-level (hard)",
            "prompting_method": "Supervised fine-tuning on synthetic chain-of-thought responses; augmentation generation used: 1-shot response prompt, query augmentation (5 types), temperature variations (0.0/1.0), greedy decoding for evaluation",
            "performance_metric": "Accuracy (exact numeric answer; greedy decoding)",
            "performance_value": "GSM8K: 66.0% accuracy (Table 4); MATH: 18.4% accuracy (Table 5, in-domain AugMATH fine-tune result for 7B)",
            "internal_analysis": "Authors visualized problem embeddings (t-SNE) using the 15th-layer last-token LLaMA-2-7B representations showing AugGSM8K and GSM8K occupy similar regions while MATH is largely separate; training vs test accuracy are positively correlated; analysis of response-quality effects showed that noisy/wrong GPT-generated responses can still help weaker models.",
            "failure_modes": "Limited out-of-domain generalization (AugGSM8K → MATH yields marginal gains); some augmented responses contained no final answer or overly long/incorrect reasoning paths requiring manual filtering; majority-voting filtering of responses sometimes reduced performance; wrong/generated-but-plausible reasoning can both help and harm depending on model capacity.",
            "scaling_trend": "In-domain performance increases roughly log-linearly with amount of augmented queries in the 13k–97k range; response-augmentation shows saturation (for 7B models benefit plateaus after ~3 responses per augmented query, ~97K responses). Fitted scaling: y = 10.7 * log(x) + 13.2 (see Table 3 for 7B fit on GSM8K).",
            "uuid": "e6657.0",
            "source_info": {
                "paper_title": "MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MuggleMath-13B",
            "name_full": "MuggleMath (LLaMA-2-derived fine-tuned model, 13B)",
            "brief_description": "A LLaMA-2 13B decoder-only transformer fully fine-tuned on AugGSM8K / AugMATH synthetic chain-of-thought data to improve multi-step math problem solving.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MuggleMath-13B (fine-tuned LLaMA-2 13B)",
            "model_family": "decoder-only transformer (LLaMA-2 family)",
            "model_size": "13B",
            "training_data_description": "Pretrained LLaMA-2-13B fully fine-tuned with GSM8K augmented by AugGSM8K (GPT-3.5/GPT-4 generated queries and multiple response paths) and AugMATH variants; same system prompt and training hyperparameters as other experiments.",
            "benchmark_name": "GSM8K (in-domain) and MATH (in-domain for AugMATH experiments and out-of-domain for AugGSM8K→MATH)",
            "task_type": "Multi-step arithmetic and algebraic word problems (GSM8K) and harder contest-style math (MATH)",
            "problem_format": "Natural-language word problems with textual reasoning chains (chain-of-thought) and numerical final answers",
            "difficulty_level": "GSM8K: elementary multi-step; MATH: high-school and competition-level (hard)",
            "prompting_method": "Supervised fine-tuning on synthetic chain-of-thought; augmentation used 1-shot response-format prompts; evaluated with greedy decoding; tests included multi-task and transfer learning experiments.",
            "performance_metric": "Accuracy (exact numeric answer)",
            "performance_value": "GSM8K: 74.3% accuracy (Table 4, MuggleMath 13B); MATH: 30.7% accuracy (Table 5, MuggleMath 13B on AugMATH+MATH)",
            "internal_analysis": "Scaling fits show smaller slope vs 7B (harder to improve for better base models): fitted GSM8K scaling y = 7.6 * log(x) + 36.3 (Table 3); embedding-space t-SNE reveals dataset distribution mismatch explaining poor transfer; analysis of difficulty categories (easy/medium/hard by reasoning steps) shows augmentation of hard problems yields larger gains.",
            "failure_modes": "OOD generalization limited (AugGSM8K yields little MATH improvement); majority-voting response filtering sometimes reduces performance; stronger models are harmed more by low-quality (incorrect) augmented responses compared to weaker ones.",
            "scaling_trend": "Log-linear improvement with augmented query amount but with smaller coefficient than 7B; on MATH a segmented log-linear relationship observed (low-data and high-data segments with different log slopes). Response-augmentation yields continued gains for 13B across a wider response-volume range (37K–157K) than for 7B before plateauing.",
            "uuid": "e6657.1",
            "source_info": {
                "paper_title": "MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MuggleMath-70B",
            "name_full": "MuggleMath (LLaMA-2-derived fine-tuned model, 70B)",
            "brief_description": "Large LLaMA-2 70B decoder-only transformer fine-tuned with Augmented GSM8K/MATH data to push state-of-the-art open-source performance on math benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MuggleMath-70B (fine-tuned LLaMA-2 70B)",
            "model_family": "decoder-only transformer (LLaMA-2 family)",
            "model_size": "70B",
            "training_data_description": "Pretrained LLaMA-2-70B fully fine-tuned on GSM8K plus AugGSM8K / AugMATH synthetic datasets generated primarily by GPT-4 with filtered chain-of-thought responses.",
            "benchmark_name": "GSM8K and MATH",
            "task_type": "Multi-step arithmetic word problems and hard math reasoning",
            "problem_format": "Natural-language word problems with chain-of-thought style solutions",
            "difficulty_level": "GSM8K elementary multi-step; MATH hard",
            "prompting_method": "Supervised fine-tuning on augmented chain-of-thought responses; augmentation used GPT-4-generated responses (low temperature variants) and 1-shot formatting; greedy decoding for evaluation",
            "performance_metric": "Accuracy",
            "performance_value": "GSM8K: 82.7% accuracy (Table 4, MuggleMath 70B); MATH: 36.3% accuracy (Table 5, MuggleMath 70B on AugMATH+MATH)",
            "internal_analysis": "Large model achieves highest absolute gains but exhibits smaller relative slope in scaling fit (harder to improve further with limited augmentation); embedding visualization suggests dataset distribution mismatch still affects transfer.",
            "failure_modes": "Although high in-domain accuracy, still limited cross-dataset generalization (AugGSM8K→MATH); noisy augmented responses can still influence learning and must be filtered; diminishing returns present at large model scales.",
            "scaling_trend": "Absolute performance higher; relative benefit per doubling of augmented queries smaller than for smaller models; log-linear trends on GSM8K persist but with reduced slope; segmented log-linear behavior on MATH observed at larger data scales.",
            "uuid": "e6657.2",
            "source_info": {
                "paper_title": "MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 (augmentation)",
            "name_full": "GPT-4 (OpenAI) used as generator for query and response augmentation",
            "brief_description": "Proprietary large language model (GPT-4) used in this work to generate diverse augmented queries and high-quality chain-of-thought response paths for supervised fine-tuning data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 (used as generator for augmentation)",
            "model_family": "decoder-only transformer (proprietary OpenAI family)",
            "model_size": "not specified (proprietary)",
            "training_data_description": "Not specified in paper (proprietary); used as an oracle to generate diverse queries and multiple reasoning paths for dataset augmentation.",
            "benchmark_name": "Used to generate training data for GSM8K and MATH augmentations (AugGSM8K and AugMATH)",
            "task_type": "Generation of synthetic multi-step reasoning paths and rephrasings for arithmetic/word problems",
            "problem_format": "Natural-language prompts (1-shot response prompt for stable format), temperature variations used (0.0 and 1.0)",
            "difficulty_level": "Capable of producing both simple and complex (increased-complexity) augmented problems",
            "prompting_method": "1-shot response-format prompt to stabilize chain-of-thought outputs; used for query augmentation and to sample multiple reasoning paths per augmented query; some zero-shot variants also tested",
            "performance_metric": "Not directly evaluated on benchmark; used to produce training data whose downstream effect measured by accuracy of fine-tuned models",
            "performance_value": "Generators: GPT-4-based responses produced higher-quality augmentation leading to larger SFT gains than GPT-3.5 (comparing subsets $∈$ Table 2/experiment descriptions). No absolute accuracy reported for GPT-4 as solver in this paper.",
            "internal_analysis": "Paper reports generator choice matters: GPT-4 responses yield better SFT performance than GPT-3.5; temperature changes (0.0 vs 1.0) had small impact when GPT-4 used for responses; 1-shot prompt stabilized format and improved downstream gains vs zero-shot.",
            "failure_modes": "Augmented responses sometimes lacked final answer, were excessively long, or contained incorrect reasoning and required manual filtering; even GPT-4 has non-negligible error rates on MATH problems.",
            "scaling_trend": "Augmentation quality from GPT-4 yields strong downstream gains; however, marginal returns and OOD transfer limited by distributional mismatch between generated queries and target dataset subjects.",
            "uuid": "e6657.3",
            "source_info": {
                "paper_title": "MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5 (augmentation)",
            "name_full": "GPT-3.5-turbo used as a cheaper generator for queries/responses",
            "brief_description": "A smaller OpenAI model used to generate augmented queries and responses; generally produced lower-quality responses than GPT-4 but still useful for augmentation at scale.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo (generation role)",
            "model_family": "decoder-only transformer (OpenAI family)",
            "model_size": "not specified (proprietary, smaller than GPT-4)",
            "training_data_description": "Not specified; used to generate augmented queries and some response paths (temperature=1.0 in experiments)",
            "benchmark_name": "Used only as a generator for SFT data (AugGSM8K subsets)",
            "task_type": "Generation of synthetic queries and chain-of-thought style responses for arithmetic word problems",
            "problem_format": "Natural-language prompts; responses produced in a stabilized python-dictionary / formatted style via 1-shot prompt where used",
            "difficulty_level": "Generated both simpler and more complex variants depending on prompt",
            "prompting_method": "Used same query-augmentation prompts (5 types) and 1-shot response-format prompt in some subsets; responses sampled at temperature 1.0 in many subsets",
            "performance_metric": "Not directly evaluated; downstream SFT with GPT-3.5-generated responses yielded smaller gains than GPT-4-generated responses (see experiment comparisons in Section 3.3/3.4)",
            "performance_value": "When response augmentation used GPT-3.5, SFT gains were significantly lower than using GPT-4 (e.g., comparing $∈$ subsets in Table 2; exact numeric downstream differences reported in the paper's tables).",
            "internal_analysis": "Authors found generator model quality influences SFT efficacy: GPT-4 responses &gt; GPT-3.5 responses; temperature differences for GPT-4 had modest effect; prompts (1-shot) matter to ensure consistent response formatting.",
            "failure_modes": "GPT-3.5 produces more incorrect or inconsistent reasoning paths which can still help weaker models but hurts stronger models when used as SFT labels; responses sometimes required manual filtering.",
            "scaling_trend": "Using higher-quality generator (GPT-4) yields better downstream scaling with augmented data; lower-quality generator still provides benefit but with smaller slope.",
            "uuid": "e6657.4",
            "source_info": {
                "paper_title": "MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scaling relationship on learning mathematical reasoning with large language models",
            "rating": 2
        },
        {
            "paper_title": "How well do large language models perform in arithmetic tasks?",
            "rating": 2
        },
        {
            "paper_title": "WizardMath: Empowering mathematical reasoning for large language models via reinforced evol-instruct",
            "rating": 2
        },
        {
            "paper_title": "Chain‑of‑thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Let's verify step by step",
            "rating": 2
        }
    ],
    "cost": 0.01685225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning</h1>
<p>Chengpeng $\mathbf{L i}^{12 <em>}$, Zheng Yuan ${ }^{2}$, Hongyi Yuan ${ }^{2 </em>}$, Guanting Dong ${ }^{2 *}$, Keming Lu ${ }^{2}$ Jiancan Wu ${ }^{1}$, Chuanqi Tan ${ }^{2}$, Xiang Wang ${ }^{11 \frac{1}{2}}$, Chang Zhou ${ }^{2}$<br>${ }^{1}$ University of Science and Technology of China<br>${ }^{2}$ Alibaba Group<br>{lichengpeng.lcp, yuanzheng.yuanzhen, ericzhou.zc}@alibaba-inc.com<br>{wujcan, xiangwang1223}@gmail.com</p>
<h4>Abstract</h4>
<p>In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create two new dataset AugGSM8K and AugMATH, by complicating and diversifying the queries and sampling multiple reasoning paths from GSM8K and MATH. We obtained a series of LLMs called MuggleMath by fine-tuning LLaMA models on AugGSM8K and AugMATH. MuggleMath substantially achieves new state-of-the-art on GSM8K and MATH. A log-linear relationship and a segmented loglinear are presented between MuggleMath's performance and the amount of augmented data on GSM8K and MATH, respectively. We also find that it is weak in out-of-domain math reasoning generalization from AugGSM8K to MATH and from AugMATH to GSM8K, which suggests that augmenting queries that cover a broader range of subjects is more beneficial for generalization. We release our codes and augmented data in https://github.com/ OFA-Sys/gsm8k-ScRel.</p>
<h2>1 Introduction</h2>
<p>The emergence of large language models (LLMs) (Ouyang et al., 2022; Anil et al., 2023;</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>OpenAI, 2023) has profoundly revolutionized the field of natural language processing, exhibiting versatile performance in various tasks like code generation (Chen et al., 2021; Luo et al., 2023b; Wang et al., 2024b; Wei et al., 2023), instruction following (Longpre et al., 2023; Zhou et al., 2023a; Lei et al., 2024), long context question answering (Tworkowski et al., 2023; Luo et al., 2024), and math reasoning (Wei et al., 2022; Taylor et al., 2022; Lewkowycz et al., 2022a). Math reasoning as a representative reasoning task is widely studied to access the reasoning abilities in LLMs (Cobbe et al., 2021; Hendrycks et al., 2021a). Proprietary LLMs, such as GPT-3.5, and GPT4 (OpenAI, 2023) have shown exceptional mathematical reasoning abilities, while there remains a substantial gap between open-source LLMs, such as GPT-J (Wang and Komatsuzaki, 2021) and LLaMA (Touvron et al., 2023a,b)) and the cutting-edge proprietary models.</p>
<p>To enable better mathematical reasoning abilities in open-sourced LLMs, they generally undergo a fine-tuning stage on supervised reasoning datasets. A series of efforts are committed to enhancing the mathematical reasoning capabilities of open-source LLMs, where a mainstream approach involves first augmenting new mathematical problems and answers, followed by supervised fine-tuning on the augmented dataset (Yuan et al., 2023a; Luo et al., 2023a; Yu et al., 2023). This type of approach has achieved good results, and in this paper, we would like to explore what are the key factors affecting the effectiveness of data augmentation for mathematical reasoning tasks and the scaling relationship between the amount of data augmentation and model performance. Specifically, with the help of proprietary models (GPT-3.5 and GPT-4), we applied five types of mathematical problem augmentation methods based on human experience in creating variations of mathematical problems similar to Luo et al. (2023b,a). We further generated multiple rea-</p>
<p>soning paths for each augmented problem since distinct reasoning paths can also enhance chain-of-thought reasoning (Huang et al., 2022; Zhu et al., 2023a; Yuan et al., 2023a). We obtained two new datasets called AugGSM8K and AugMATH after data augmentation on two widely used mathematical reasoning datasets GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b). By supervised fine-tuning on the open-source LLaMA (Touvron et al., 2023a) and LLaMA-2 (Touvron et al., 2023b) LLMs on AugGSM8K and AugMATH, we obtained a series of models dubbed MuggleMath. We find that with sufficient amounts of data, MuggleMath achieves a new state-of-the-art on GSM8K and MATH. In addition to this, we find a log-linear relationship between the performance of MuggleMath and the amount of data augmentation over a range of data volumes on GSM8K and a segmented log-linear relationship on MATH.</p>
<p>Although MuggleMath achieves strong performance on the GSM8K and MATH test set, the rationales for performance improvement by data augmentation remain unclear. We are therefore interested in the specific reason behind the performance improvement and whether it brings enhancement in LLMs’ mathematical reasoning capabilities generally.</p>
<p>To validate the generalization of MuggleMath, we conduct multi-task learning and analyze the transferability with AugGSM8K and AugMATH. We found that LLMs trained with supervised learning after data augmentation on GSM8K only bring marginal improvements to performance on MATH and the similar conclusion is fit for AugMATH and GSM8K. By visualizing the data distribution in the embedding space of LLaMA-2-7B, we observe that the embedding distribution of problems in AugGSM8K is very close to that of GSM8K, but significantly different from the problem distribution in the MATH dataset. The reason behind can be attributed to the fact that GSM8K and MATH have different reasoning difficulty, response style and require different mathematical knowledge.</p>
<p>The main contributions of our work can be summarized as follows:</p>
<ul>
<li>By augmenting GSM8K and MATH with various queries and multiple reasoning paths, we curate GSM8K and MATH to two new datasets named AugGSM8K and AugMATH.</li>
<li>We utilize AugGSM8K and AugMATH for fine-tuning the LLaMA and LLaMA-2 mod- els to obtain MuggleMath, which greatly improves the in-domain performance of the open-sourced LLMs on GSM8K and MATH, achieving new state-of-the-art performances.</li>
<li>We find a log-linear relationship between the accuracy of the model on the test set and the amount of data augmentation within a certain range while the coefficient is similar to augmenting new human-written samples on GSM8K. When it comes to MATH, a a segmented log-linear relationship is found.</li>
<li>We demonstrate that the performance gains from data augmentation on GSM8K and MATH are difficult to generalize to each other, which indicates a need of diverse original queries in augmenting math data.</li>
</ul>
<h2>2 Related Works</h2>
<p>Mathematical Reasoning for Large Language Models Mathematical reasoning is a crucial ability to examine large language models (Cobbe et al., 2021; Hendrycks et al., 2021a; Wei et al., 2022; Yuan et al., 2023b). The mathematical reasoning ability of LLMs can be enhanced by math-related pre-training (Hendrycks et al., 2021a; Lewkowycz et al., 2022a; Taylor et al., 2022; Lightman et al., 2023a) and math-related supervised finetuning (Yuan et al., 2023a; Luo et al., 2023a; Yue et al., 2023b; Yu et al., 2023). Query augmentation (Luo et al., 2023a; Yu et al., 2023) and response augmentation (Huang et al., 2022; Zelikman et al., 2022; Ni et al., 2023; Zhu et al., 2023b; Yuan et al., 2023a) are useful techniques to improve math in-domain performances during SFT. Query augmentation methods usually generate rephrased, easier, or harder problems and use proprietary LLMs to generate answers. Response augmentation methods generate new reasoning paths for problems in the training set. They could rely on answers in the training set to filter the generated reasoning paths. Yuan et al. invests the scaling relationship on supervised LLMs math performance with pre-train loss, supervised data amount, and augmented reasoning path amount. Our work is further investigate on scaling relationships with query augmentation and out-of-domain generalization. MetaMath(Yu et al., 2023) is a contemporary work that is similar to us in the augmentation method. The distinction lies in MetaMath's focus on rewriting original questions to create new ones using the questions’ mathemat-</p>
<p>ical relationships. In contrast, our efforts are centered on generating new problems with equal or greater difficulty levels. Moreover, our work investigates the quantitative relationship between query and response augment amounts and in-domain and out-of-domain performances.</p>
<p>Data Augmentation for LLM Data augmentation is a common technique to improve downstream task performance in NLP (Feng et al., 2021). In the era of large language models, data augmentation is usually used for generating instruction following SFT datasets (Wang et al., 2023b; Taori et al., 2023; Xue et al., 2023). Queries (Ding et al., 2023; Xu et al., 2023) and responses (Mukherjee et al., 2023) of SFT datasets can both be augmented by prompting state-of-the-art proprietary LLMs. Compared with their work, we are concentrated on augmenting math SFT dataset and we are more interested in scaling relationships on in-domain and out-ofdomain generalizations.</p>
<p>Out-of-Distribution Generalization The challenge of out-of-distribution (OOD) generalization has garnered widespread attention across various domains (Karras et al., 2018; Wang et al., 2021; Song et al., 2023; Zhou et al., 2023b; Peng et al., 2024) in machine learning. This issue arises when the distribution of data encountered by a model during testing diverges from that of the training phase, leading to a decline in model performance. The OOD problem is multifaceted (Lipton et al., 2018; Schölkopf et al., 2012; Tran et al., 2022; Cai et al., 2023), with subcategories such as covariate shifts and concept shifts, among others. To mitigate the effects of OOD scenarios, a diverse array of strategies has been developed, including unsupervised domain generalization (Wang et al., 2021; Zhou et al., 2023b), stable learning (Shen et al., 2020; Kuang et al., 2020), invariant representation learning (Creager et al., 2021), causal learning (Peters et al., 2015), and invariant risk minimization (Mao et al., 2023), multi-task Learning (Dong et al., 2024; Wang et al., 2024a) and contrastive learning (Peng et al., 2023). Recent trends in the community have shown a growing preference for performance enhancement through data augmentation during the Self-supervised Finetuning (SFT) stage in large-scale models. However, the extent of OOD issues associated with this method and their severity remain underexplored. This study aims to fill this gap by conducting empirical experiments and providing a visual analysis to assess the impact of data augmentation on OOD generalization in the context of large models.</p>
<h2>3 Experiments</h2>
<p>We first introduce our experimental setup in Section 3.1 and dataset augmentation method in Section 3.2. Then we conduct analyses spanning several aspects of data augmentation to answer the three research questions in abstract. For space saving, we mainly analyze the augmentation on GSM8K and detailed discussion of augmentation on MATH are list in Appendix A.</p>
<h3>3.1 Experimental Setup</h3>
<p>Problem Definition We define the math reasoning SFT dataset as $\mathcal{D}=\left{q_{i}, a_{i}\right}<em i="i">{i}$, where $q</em>\right}}$ is a question and $a_{i}$ is a reasoning path with an answer. We augment the SFT dataset to a new dataset $\mathcal{D}^{\prime}=\left{q_{i}^{\prime}, a_{i}^{\prime<em _in="{in" _text="\text">{i}$. We apply SFT on the pre-trained language models and measure the augmented SFT dataset based on the accuracy of the in-domain test set $\mathcal{D}</em>$. We calculate the accuracy based on greedy decoding.}}$ and out-of-domain test set $\mathcal{D}_{\text {out }</p>
<p>Datasets GSM8K (Cobbe et al., 2021) is a dataset with elementary school math word problems with 7,473 training problems and 1,319 testing problems. The test set of GSM8K is viewed as in-domain test datset for AugGSM8K and out-of-domain test dataset for AugMATH. MATH (Hendrycks et al., 2021a) is a dataset with challenging high-school math problems. Problems are classified into the following topics: Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus. Problems in MATH are harder and more diverse than in GSM8K. We use test set of MATH as our in-domain test dataset for AugMATH and out-of-domain test dataset for AugGSM8K.</p>
<p>Training We employ state-of-the-art opensource LLMs for fine-tuning, including LLaMA-1 7B (Touvron et al., 2023a), LLaMA-2 7B, LLaMA2 13B, and LLaMA-2 70B (Touvron et al., 2023b), all of which undergo full fine-tuning. We adopt system prompt from (Taori et al., 2023) for finetuning and listed in Appendix B. We use AdamW for optimization. The training proceeds for three epochs with a learning rate of 2e-5, a warmup ratio of 0.03 , and a cosine learning rate scheduler. We do not apply early stops to choose checkpoints. The hardware setup involves 32 NVIDIA A100 GPUs.</p>
<p>3.2 Dataset Augmentation</p>
<p>Query Augmentation To generate new queries for GSM8K, we use gpt-3.5-turbo-0613 and gpt-4-0613 as the generator. Inspired by EvolInstruct [luo2023evolinstruct], we find that the diversity and complexity of queries in augmented datasets play a vital role in improving math reasoning benchmark performance. We employ human knowledge from authors in modifying mathematical problems for query augmentation. Below are five query augmentation methods used in our experiments: Change specific numbers; Introduce fractions or percentages; Combine multiple concepts; Include a conditional statement; Increase the complexity of the problem. The examples and detailed prompts we used for query augmentation are listed in Appendix C. The examples of augmented queries are shown in Table 10.</p>
<p>Response Augmentation Instead of using the trained SFT model proposed by [yuan2023sft] we use gpt-3.5-turbo-0613 and gpt-4-0613 to augment more reasoning paths . The main reason is that we can not filter out wrong reasoning paths without final answers. Thus we need to use a model that is as accurate as possible which is the state-of-the-art LLMs ChatGPT. We use a 1-shot prompt to ensure augmented response formats. The response prompt we used for query augmentation is listed in Appendix D. Augmented responses can result in some unconventional answers, such as excessively long reasoning paths and reasoning paths that do not contain an answer at their end. We devise manual rules to filter out these corresponding query-response pairs and manual rules are detailed in Appendix F. The examples of augmented responses are shown in Table 11.</p>
<p>Augmented Dataset The original GSM8K training set has 7,473 samples. We augment 5 more queries for each query in the training set and yield $7,473 \times 5=37,365$ augmented queries. We run this query augmentation three times with $\mathcal{D}<em 3="3">{1}, \mathcal{D}</em>}$ by GPT-3.5 and $\mathcal{D<em i="i">{2}$ by GPT-4, and $\left|\mathcal{D}</em>}\right|=37,365$. Then we generate one response for each augmented query for $\mathcal{D<em i="i">{i}$ and apply response filtering. We consider the query-response pairs after filtering as $\mathcal{D}</em>}^{j}$. We obtain approximately 30,000 query-response pairs for each $\mathcal{D<em 1="1">{i}^{j}$. To explore the performance differences of different augmented settings, we generate five responses on the augmented queries $\mathcal{D}</em>}$ with GPT-4's temperature set to $1.0\left(\mathcal{D<em 1="1">{1}^{1} \sim \mathcal{D}</em>\right)$,
}^{5<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparison of test set accuracy on GSM8K for models of varying scales after fine-tuning on AugGSM8K subsets with different query augmentation strategies.
one response with GPT-4's temperature set to 0.0 $\left(\mathcal{D}<em 1="1">{1}^{6}\right)$, and one response with GPT-3.5's temperature set to $1.0\left(\mathcal{D}</em>}^{7}\right)$. We also try a zero-shot response generation named $\mathcal{D<em 2="2">{1}^{8}$. We use GPT-4 to augment responses as $\mathcal{D}</em>}^{1}, \mathcal{D<em 2="2">{3}^{1}$. Since $\mathcal{D}</em>$. We refer to the union of all augmented data and the original GSM8K training set as AugGSM8K, upon which we conduct experiments using various subsets. Detailed augmented dataset notations are listed in Table 1.}^{1}$ is significantly larger than other subsets, we downsample it to $\mathcal{D}_{2}^{1</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Subset</th>
<th style="text-align: center;">Query</th>
<th style="text-align: center;">Response</th>
<th style="text-align: center;">Temp.</th>
<th style="text-align: center;">Size (K)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\mathcal{D}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.5</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}<em 1="1">{1}^{1} \sim \mathcal{D}</em>$}^{5</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{1}^{6}$</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{1}^{7}$</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{1}^{8}$</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{2}^{1}$</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">35</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{2}^{1}$</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{D}_{3}^{1}$</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">30</td>
</tr>
</tbody>
</table>
<p>Table 1: The description of different subsets of the augmented in-domain dataset AugGSM8K.</p>
<h3>3.3 RQ1. What Strategies of Data Augmentation are More Effective</h3>
<p>Query Augmentation Types We want to examine whether query augmentation works for math reasoning SFT since [luo2023evolinstruct] applies PPO which cannot make an apple-to-apple comparison. Each query in the original training dataset is augmented with 5 different types. We cluster these queries based on the query types. We apply SFT on the original training set $(\mathcal{D})$, each query type augmentation, and a combination of them $\left(\mathcal{D}+\mathcal{D}_{1}^{1}\right)$. Results are shown in Figure 1 and Table 20, where mixed-augmentation represents $1 / 5$ of the aug-</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>7B</th>
<th>7B-2</th>
<th>13B-2</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\mathcal{D}$</td>
<td>35.9</td>
<td>41.6</td>
<td>50.0</td>
</tr>
<tr>
<td>$+\mathcal{D}_{1}^{1}\times 0.8$</td>
<td>51.1</td>
<td>56.6</td>
<td>63.2</td>
</tr>
<tr>
<td>$+\mathcal{D}_{1}^{1}$</td>
<td>53.0</td>
<td>57.0</td>
<td>65.5</td>
</tr>
<tr>
<td>$+\mathcal{D}_{1}^{6}$</td>
<td>51.6</td>
<td>58.0</td>
<td>63.8</td>
</tr>
<tr>
<td>$+\mathcal{D}_{1}^{7}$</td>
<td>41.3</td>
<td>46.7</td>
<td>52.8</td>
</tr>
<tr>
<td>$+\mathcal{D}_{1}^{8}$</td>
<td>49.4</td>
<td>53.3</td>
<td>62.2</td>
</tr>
<tr>
<td>$+\mathcal{D}_{2}^{1}$</td>
<td>52.3</td>
<td>57.8</td>
<td>63.3</td>
</tr>
</tbody>
</table>
<p>mented data of each strategy being randomly sampled and then combined. Compared with no augment, each query augment method can improve the in-domain performance. Increase complexity augmentation method improves most among all of them. This suggests that enhancing the complexity of queries is one of the key factors influencing the sample efficiency of data augmentation. We randomly sampled one-fifth of the combined dataset and named it Mixed-augmentation. We can observe that the Mixed-augmentation method exhibits performance marginally lower than that of the best Increase Complexity approach. However, it can be seen from Table LABEL:tab:13B-aug that with larger data size, mixed-augmentation can reach the best performance. Therefore, it is necessary to mix different types of queries to improve the diversity of augmented questions. There are more discussions in Appendix I about improving existing data augmentation methods.</p>
<p>Query and Response Sources Here we examine how the query and response quality influence the augmented model performance. We list results in Table 2, and draw the following conclusions: (a) Comparing $\mathcal{D}<em 1="1">{2}^{1}$ and $\mathcal{D}</em>}^{6}$, we find that the queries generated by GPT-4 and GPT-3.5 have no significant impact on SFT performance. (b) Comparing $\mathcal{D<em 1="1">{1}^{1}$ and $\mathcal{D}</em>}^{6}$, we can conclude that when using GPT-4 to generate responses, the temperature has no significant impact on SFT performance. (c) Comparing $\mathcal{D<em 1="1">{1}^{1}$ and $\mathcal{D}</em>}^{8}$, we can conclude that, compared to the zero-shots generation method, the response augmentation prompt we propose plays a substantial role in enhancing the quality of the generated data (+3.6 for LLaMA-7B, +3.7 for LLaMA-2-7B, +3.3 for LLaMA-2-13B). The main reason we consider this is our 1-shot setting stabilizes the response format. (d) Comparing $\mathcal{D<em 1="1">{1}^{7}$ (25K) and $\mathcal{D}</em>\times 0.8$ (24K), we can conclude that, compared to GPT-3.5, the response augmented using GPT-4 yields significantly better results for SFT.}^{1</p>
<h3>3.4 RQ2. What is the Scaling Relationship between the Amount of Augmented Data and Model Performance</h3>
<p>Query Augmentation Amount We examine how query augmentation amount affects the in-domain performance. We examine seven data volume configurations including partitioning $\mathcal{D}<em 1="1">{1}^{1}$ into proportions of $0,0.2,0.4,0.6,0.8,1.0$, as well as $\mathcal{D}</em>}^{1}+\mathcal{D<em 1="1">{2}^{1}$ and $\mathcal{D}</em>}^{1}+\mathcal{D<em 3="3">{2}^{1}+\mathcal{D}</em>}^{1}$ as the augmented datasets. Each augmented query only has one augmented response. They are mixed with GSM8K $\mathcal{D}$ to apply SFT. From Table 2 and Table 21, we can find that within the data volume range of 13-97K, the in-domain performance exhibits a log-linear relationship with the query amount. We employ linear regression to approximate this relationship. As shown in Figure 3, pre-training models with better initial math reasoning capabilities exhibit a smaller slope which is consistent with [yuan2023a]. This suggests it is harder to improve reasoning ability for a better pre-trained model. We also conduct validations on our fitted scaling law with an interpolate point at a query amount of 17K $(\mathcal{D}+\mathcal{D<em 1="1">{1}^{1} \times 0.3)$ and an extrapolate point at a query amount of 104K $(\mathcal{D}+\mathcal{D}</em>}^{1}+\mathcal{D<em 3="3">{2}^{1}+\mathcal{D}</em>)$, discovering that the regression offers accurate predictions of model performances. We should notice this scaling law cannot be correct within all dataset size ranges since the test set accuracy is bounded.}^{1</p>
<p>Besides, the fitted regression shows when query augmentation amount doubles, LLaMA-7B models will improve $10.7 \times \log (2)=7.4$, LLaMA2-7B will improve $9.8 \times \log (2)=6.8$, and LLaMA213B will improve $7.6 \times \log (2)=5.3$. As shown in [yuan2023a], it is estimated that when human-written sample amount doubles, LLaMA7B models will improve 6.5 score, LLaMA2-7B will improve 6.6 score, and LLaMA2-13B models will improve 5.5 score. Query augmentation is similarly effective to human-written samples in term of in-domain performance. This demonstrates that query augmentation benefits from the performing proprietary LLMs on GSM8K, thus the sample quality generated by query augmentation is as high as those of human-written samples. When it comes to the relationship of model performance and augmented data size, a segmented log-linear relationship is presented. The detailed discussion</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison of test set accuracy on GSM8K for models of varying scales after fine-tuning on AugGSM8K subsets with different query augmentation amount.</p>
<p>is in Appendix A.</p>
<h3>Response Augmentation Amount</h3>
<p>We further investigate under the data augmentation setting, if we keep the number of queries constant and increase the number of responses, how the in-domain performance changes. We use $\mathcal{D}_{1}$ as augmented queries and vary the response amount from 1 to 5 per augmented query. We also try majority voting (Wang et al., 2023a; Huang et al., 2022) to filter the augmented response since we cannot know the correct answer. In Figure 3 and Table 22, we find for LLaMA-7B and LLaMA-2-7B models, once the response data volume reaches 97K (3 responses per query), further increase the number of responses do not yield performance improvement. Before this point, model performance improves as the response amount increases. Thus, query augmentation with accurate responses seems more effective than only response augmentation with the augmented data size scales up. As for the LLaMA-2-13B model, within the data volume range of 37K to 157K, model performance consistently rises in a roughly linear fashion with increasing data volume, with a slower rate than the 7B model within the ascending interval. We then investigate the performance impact brought by majority voting filtering. If all responses have different answers, we discard the corresponding query-response. Surprisingly, we find that after applying majority voting, the model performance at the same data scale is generally lower than not applying filtering. A possible explanation is that even wrong responses generated by GPT-4 are useful for worse models (LLaMA) to improve their abilities. Another explanation is the reduction in the number of queries, as we discard the corresponding query when all response answers are different. To study the relationship between response quality and model performance, we discuss more in Appendix J.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparison of test set accuracy on GSM8K for models of varying scales after fine-tuning on AugGSM8K subsets with different response augmentation amount.</p>
<h3>Combination</h3>
<p>We investigate how the combination of query augmentation and response augmentation in AugGSM8K will affect the model's performance. Results are listed in Table 4. It demonstrates that query augmentation and response augmentation can complement each other to a certain extent to improve in-domain performance. We conduct SFT on AugGSM8K and AugMATH to obtain MuggleMath which effectively improves the in-domain accuracy on GSM8K and MATH test set in Table 4 and Table 5. These models outperform previous state-of-the-art open-sourced models with a very large margin for 7B and 13B models. <strong>Model comparisons of MuggleMath and a broader range of state-of-the-art approaches are in Table 26.</strong> Case studies of MuggleMath are listed in Table 12. Moreover, our work suggests that high-quality math training data can be synthesized through powerful large language models by relying solely on simple prompt, without the need for complex designs.</p>
<h3>3.5 RQ3. Can Data Augmentation Incentivize Generalization to Out-of-domain Mathematical Reasoning Tasks?</h3>
<p>We have found that query and response augmentation significantly improves in-domain math reasoning performance. But we really interested in whether we can improve performances on out-of-domain distribution. We employ multi-task learning and transfer learning to see how models fine-</p>
<p>| Model | 7B | 7B-2 | 13B-2 |
| Estimation | $y=10.7 \log (x)+13.2$ | $y=9.8 \log (x)+21.3$ | $y=7.6 \log (x)+36.3$ |
| :-- | :--: | :--: | :--: |
| $\mathrm{x}=17$ prediction | 43.4 | 49.2 | 57.7 |
| $\mathrm{x}=17$ observation | 43.4 | 50.0 | 56.0 |
| $\mathrm{x}=104$ prediction | 62.7 | 67.0 | 71.4 |
| $\mathrm{x}=104$ observation | 62.1 | 66.7 | 70.8 |</p>
<p>Table 3: The scaling law on amounts of augmented query in GSM8K.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">7B</th>
<th style="text-align: center;">7B-2</th>
<th style="text-align: center;">13B-2</th>
<th style="text-align: center;">70B-2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathcal{D}$</td>
<td style="text-align: left;">35.9</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">63.2</td>
</tr>
<tr>
<td style="text-align: left;">RFT (Yuan et al., 2023a)</td>
<td style="text-align: left;">49.1</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">64.8</td>
</tr>
<tr>
<td style="text-align: left;">WizardMath (Luo et al., 2023a)</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">81.6</td>
</tr>
<tr>
<td style="text-align: left;">$+\mathcal{D}<em 1="1">{1}^{1}+\mathcal{D}</em>$}^{2}+\mathcal{D}_{1}^{3</td>
<td style="text-align: left;">61.3</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">$+\mathcal{D}<em 2="2">{1}^{1}+\mathcal{D}</em>$}^{1}+\mathcal{D}_{3}^{1</td>
<td style="text-align: left;">61.4</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">$+\sum_{i=1}^{9} \mathcal{D}<em 2="2">{1}^{i}+\mathcal{D}</em>$}^{1}+\mathcal{D}_{3}^{1</td>
<td style="text-align: left;">65.4</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">82.3</td>
</tr>
<tr>
<td style="text-align: left;">MuggleMath</td>
<td style="text-align: left;">$\mathbf{6 6 . 0}$</td>
<td style="text-align: center;">$\mathbf{6 9 . 8}(+14.9)$</td>
<td style="text-align: center;">$\mathbf{7 4 . 3}(+10.4)$</td>
<td style="text-align: center;">$\mathbf{8 2 . 7}(+1.1)$</td>
</tr>
</tbody>
</table>
<p>Table 4: In-domain performance of MuggleMath(AugGSM8K and AugMATH) on GSM8K.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">7B</th>
<th style="text-align: center;">7B-2</th>
<th style="text-align: center;">13B-2</th>
<th style="text-align: center;">70B-2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathcal{D}$</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">14.4</td>
</tr>
<tr>
<td style="text-align: left;">WizardMath (Luo et al., 2023a)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">22.7</td>
</tr>
<tr>
<td style="text-align: left;">MuggleMath</td>
<td style="text-align: center;">$\mathbf{1 8 . 4}$</td>
<td style="text-align: center;">$\mathbf{2 5 . 8}(+15.1)$</td>
<td style="text-align: center;">$\mathbf{3 0 . 7}(+16.7)$</td>
<td style="text-align: center;">$\mathbf{3 6 . 3}(+13.6)$</td>
</tr>
</tbody>
</table>
<p>Table 5: In-domain performance of MuggleMath(AugMATH and AugGSM8K) on MATH. See more details of AugMATH in Appendix A
tuned on AugGSM8K perform on the MATH test set, where we sample 500 questions as test test like (Lightman et al., 2023b). We list results in Table 6. We find that (1) Multi-task learning and transfer learning outperform single-task supervised fine-tuning on LLaMA2-7/13B and do not improve on LLaMA-7B. (2) Although augmenting more query and response can improve GSM8K significantly, it has little to no help in improving MATH performance which indicates that in-domain augmentation data on the GSM8K dataset yields only marginal benefits for the MATH dataset in this setting. Case studies of models performed on MATH are listed in Table 13.</p>
<p>To further investigate why AugGSM8K helps little on the MATH dataset, we use t-SNE in Figure 4 to visualize the hidden representation of problems encoded by LLaMA2-7B, that is the 15-th layer of last token representation of the problem. We find GSM8K and MATH are separated in hidden space and only some of the problems in MATH are laid in the span of GSM8K. The augmented GSM8K problems are laid in the same span of GSM8K which makes sense why it improves little for MATH.</p>
<p>To investiate if the proposed augmentation method improve performance on this subset of MATH, we find that while there is an overlap in the embedding space distribution between MATH and GSM8K, it is relatively small compared with that between GSM8K and AugGSM8K. In the transfer learning setting, training first on GSM8K and then on MATH with LLaMA-13B-2 does provide some benefits for certain subsets, such as Prealgebra, Algebra, and Geometry. However, if we train first on AugGSM8K and then on MATH, this benefit is not only marginal but may even lead to a decrease in performance on other subsets, like Geometry and Prealgebra, which could be related to the data proportions. Overall, the performance improvement on the MATH dataset from augmentation on GSM8K is minimal, even on subsets like Prealgebra, where there is some overlap. For 7B size and multi-task learning setting, we can draw the similar conclusion. Detailed results are listed in Table 28 to Table 33. This suggests if we want to improve math reasoning benchmark performances for LLMs, we can choose to apply augmentation on diverse math subjects. For the generalization from AugMATH to GSM8K, the detailed experiments</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The embedding visualization of queries in GSM8K, MATH and AugGSM8K.
and explanation are in Appendix A. Moreover, we conduct experiments on more out-of-distribution mathematical dataset in Appendix A.3.</p>
<h2>4 Discussion</h2>
<h3>4.1 Training set vs. Test set accuracy</h3>
<p>Query and response augmentation generate similar problems for the training set which leads to better training set accuracy. We have shown augmentations improve the accuracy of the in-domain test set. We want to investigate the relationship between the accuracy of the training set and the test set to find if the accuracy of the training set can be a performance indicator. We sample 500 samples from the original training set to calculate the accuracy. From Figure 8, the training and test accuracy generally exhibit a positive correlation across different augmented data which shows the training accuracy could be an indicator of in-domain performance unless deliberately overfitting.</p>
<h3>4.2 Make more augmentation on harder problems</h3>
<p>During the query augmentation process, it is crucial to understand which kind of queries should be augmented. Augmenting too many easy problems may not be effective since the model may have mastered this level of problems. Here we examine if the model improves more when we augment more on harder or wrong problems. We define hard problems based on the number of equations, specifically, problems with fewer than three reasoning steps as easy, those with exactly three steps as medium, and those with more than three steps as hard(see more details in Appendix H). We define wrong problems if the SFT model solves them incorrectly. We apply
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The performance of SFT models with different difficulty augmentation on GSM8K.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The performance of SFT models with wrong problem augmentation on GSM8K.</p>
<p>SFT on subsets of AugGSM8K with augmented queries on easy, medium, hard, wrong, and random problems. From Figure 5 and Figure 24, it is evident that for LLaMA-2-7B and LLaMA-2-13B, the performance gain from augmenting hard problems is significantly higher than that from augmenting other types of problems. From Figure 6 and Figure 25, we find that augmenting incorrect problems on three models consistently improves more than random query augmentation. In addition, we have conducted an analysis of our model's performance on test set problems of varying difficulty. Our analysis of the 7B-2 model's performance on the test set, shows accuracy rates of $0.55,0.42$, and 0.21 for easy, medium, and hard problems, respectively, while MuggleMath achieves higher accuracy rates of $0.73,0.70$, and 0.64 for the same problem categories. This significant performance boost on difficult questions can be attributed to the fact that the augmented problems we generated are generally more complex than the original problems.</p>
<table>
<thead>
<tr>
<th>Training Setting</th>
<th>7B</th>
<th>7B-2</th>
<th>13B-2</th>
</tr>
</thead>
<tbody>
<tr>
<td>In Context Learning on MATH</td>
<td>2.9</td>
<td>2.5</td>
<td>3.9</td>
</tr>
<tr>
<td>Supervised Fine-tuning on MATH</td>
<td>4.8</td>
<td>5.8</td>
<td>6.0</td>
</tr>
<tr>
<td>Multi-task learning</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>+ MATH</td>
<td>4.6</td>
<td>6.2</td>
<td>7.6</td>
</tr>
<tr>
<td>$+\mathcal{D}_{1}^{1}+$ MATH</td>
<td>4.8</td>
<td>4.8</td>
<td>8.4</td>
</tr>
<tr>
<td>Transfer learning</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$\mathcal{D} \rightarrow$ MATH</td>
<td>4.4</td>
<td>6.0</td>
<td>9.4</td>
</tr>
<tr>
<td>$+\mathcal{D}_{1}^{1} \rightarrow$ MATH</td>
<td>6.2</td>
<td>5.6</td>
<td>7.8</td>
</tr>
<tr>
<td>$+\sum_{1}^{3} \mathcal{D}<em 2="2">{1}^{i}+\dot{\mathcal{D}}</em> \rightarrow$ MATH}^{1}+\mathcal{D}_{3}^{1</td>
<td>5.6</td>
<td>8.4</td>
<td>9.4</td>
</tr>
<tr>
<td>$+\sum_{i=1}^{i-1} \mathcal{D}<em 2="2">{1}^{i}$-majority voting $+\mathcal{D}</em> \rightarrow$ MATH}^{1}+\mathcal{D}_{3}^{1</td>
<td>5.6</td>
<td>6.0</td>
<td>9.0</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison of test set accuracy on MATH. Multi-task learning means that we fine-tune the models on the mixed dataset of AugGSM8K subset and MATH. Transfer learning means that we first fine-tune the models on subsets of AugGSM8K and then fine-tune on MATH.</p>
<h3>4.3 Result on the Perturbed Test Set</h3>
<p>We have perturbed two new test sets based on the original GSM8K test set, Change-Test and AugTest in in Table 27.</p>
<p>Upon evaluating our model on these two perturbed test sets, we found that the performance of MuggleMath consistently and significantly exceeds that of the model fine-tuned on GSM8K alone. This observation suggests that our data augmentation techniques not only enhance the model's ability to solve the original problems but also contribute to its improved performance on varied and perturbed inputs, thereby indicating a robust generalization capability in in-domain dataset.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we investigate the scaling property of query and response augmentation with respect to math reasoning in-domain and out-of-domain performance. We find that query and response augmentation can improve in-domain performance very effectively which has a similar improvement to human-written query-response pairs augmentation. Although we can obtain state-of-the-art indomain performance by such augmentation, We find that the math reasoning ability improved by the in-domain augmented data is hard to generalized to out-of-domain datasets. Therefore, when using synthetic data, increasing the diversity of the queries to improve the overall mathematical reasoning ability of the LLMs is crucial. In the future, how to use synthetic data to enhance the overall mathematical reasoning ability of the model is a very important research topic.</p>
<h2>Limitations</h2>
<p>In this study, we focused on the domain of mathematical reasoning-a key area of interest for the large language model (LLM) research community. We investigated the effectiveness of data augmentation techniques on both in-domain and out-ofdomain performance. While our work provides insights into the performance scalability and generalizability of Chain-of-Thought (COT) enhanced models in mathematical reasoning, it's important to acknowledge certain limitations: (1)Depth of Generalizability Research: Although our study is among the initial efforts to evaluate the generalizability of COT-augmented models in math reasoning, the study did not extensively explore solutions to enhance out-of-domain performance across a broader spectrum of mathematical reasoning. (2)Unique Data Augmentation Process: We utilized generative models, specifically gpt-3.5-turbo-0613 and gpt-4-0613, for data augmentation, which means that others using the same prompts may not be able to replicate our exact dataset. However, by varying the generation models' temperature setting, we have verified the robustness of our methodology. This suggests that even if the data augmentation process were to be repeated, similar performance enhancements could be achieved, underscoring the reproducibility of the results despite the unique nature of the data generation.</p>
<h2>6 Acknowledgement</h2>
<p>We thank all anonymous reviewers for their helpful comments and suggestions. This research is supported by the National Science and Technology Major Project (2023ZD0121102) and National Natural Science Foundation of China (92270114).</p>
<h2>References</h2>
<p>Alibaba. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>BaichuanInc. 2023. Baichuan 2. technical report. arXiv preprint arXiv:2309.16609.</p>
<p>Tiffany Tianhui Cai, Hongseok Namkoong, and Steve Yadlowsky. 2023. Diagnosing model performance under distribution shift.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code.
W. Chiang, Z. Li, Z. Lin, Y. Sheng, H. Zhang Z. Wu, L. Zheng, S. Zhuang, Y. Zhuang, J. Gonzalez, I. Stoica, and E. Xing. 2023. icuna: An open-source chatbot impressing gpt-4 with 90 Technical Report.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Elliot Creager, Jörn-Henrik Jacobsen, and Richard S. Zemel. 2021. Environment inference for invariant learning. pages 2189-2200. PMLR.</p>
<p>Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233.</p>
<p>Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2024. How abilities in large language models are affected by supervised fine-tuning data composition.</p>
<p>Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation approaches for NLP. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 968-988, Online. Association for Computational Linguistics.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: program-aided language models. In ICML 2023.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021a. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the MATH dataset.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve.</p>
<p>Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2018. Progressive growing of gans for improved quality, stability, and variation. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157, San</p>
<p>Diego, California. Association for Computational Linguistics.</p>
<p>Kun Kuang, Ruoxuan Xiong, Peng Cui, Susan Athey, and Bo Li. 2020. Stable prediction with model misspecification and agnostic distribution shift. pages 4485-4492. AAAI Press.</p>
<p>Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. 2023. Platypus: Quick, cheap, and powerful refinement of llms.</p>
<p>Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, and Sirui Wang. 2024. Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022a. Solving quantitative reasoning problems with language models.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022b. Solving quantitative reasoning problems with language models. In NeurIPS.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023a. Let's verify step by step. arXiv preprint arXiv:2305.20050.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023b. Let's verify step by step.</p>
<p>Zachary C. Lipton, Yu-Xiang Wang, and Alexander J. Smola. 2018. Detecting and correcting for label shift with black box predictors. pages 3128-3136. PMLR.</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688.</p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2023. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In $I C L R$.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023a. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.</p>
<p>Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang, Chenghao Ma, Guanting Dong, Meina Song, Wei Lin, Yifan Zhu, and Luu Anh Tuan. 2024. Chatkbqa: A generate-thenretrieve framework for knowledge base question answering with fine-tuned large language models.</p>
<p>Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023b. Wizardcoder: Empowering code large language models with evolinstruct. arXiv preprint arXiv:2306.08568.</p>
<p>Yuzhou Mao, Liu Yu, Yi Yang, Fan Zhou, and Ting Zhong. 2023. Debiasing intrinsic bias and application bias jointly via invariant risk minimization (student abstract). AAAI Press.</p>
<p>Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing english math word problem solvers. In $A C L$.</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707.</p>
<p>Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. 2023. Learning math reasoning from self-sampled correct and partially-correct solutions. In The Eleventh International Conference on Learning Representations.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In NAACL-HLT.</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for falcon LLM: outperforming curated corpora with web data, and web data only.</p>
<p>Ru Peng, Qiuyang Duan, Haobo Wang, Jiachen Ma, Yanbo Jiang, Yongjun Tu, Xiu Jiang, and Junbo Zhao. 2023. Came: Contrastive automated model evaluation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2012120132.</p>
<p>Ru Peng, Heming Zou, Haobo Wang, Yawen Zeng, Zenan Huang, and Junbo Zhao. 2024. Energybased automated model evaluation. arXiv preprint arXiv:2401.12689.</p>
<p>Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. 2015. Causal inference using invariant prediction: identification and confidence intervals.</p>
<p>Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code.</p>
<p>Bernhard Schölkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris M. Mooij. 2012. On causal and anticausal learning.</p>
<p>Zheyan Shen, Peng Cui, Tong Zhang, and Kun Kuang. 2020. Stable learning via sample reweighting. pages 5692-5699. AAAI Press.</p>
<p>Xiaoshuai Song, Keqing He, Pei Wang, Guanting Dong, Yutao Mou, Jingang Wang, Yunsen Xian, Xunliang Cai, and Weiran Xu. 2023. Large language models meet open-world intent discovery and recognition: An evaluation of chatgpt.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science.</p>
<p>InternLM Team. 2023a. Internlm: A multilingual language model with progressively enhanced capabilities. https://github.com/InternLM/InternLM.</p>
<p>MosaicML NLP Team. 2023b. Introducing mpt-7b: A new standard for open-source, commercially usable llms. Accessed: 2023-05-05.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,</p>
<p>Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.</p>
<p>Dustin Tran, Jeremiah Z. Liu, Michael W. Dusenberry, Du Phan, Mark Collier, Jie Ren, Kehang Han, Zi Wang, Zelda Mariet, Huiyi Hu, Neil Band, Tim G. J. Rudner, Karan Singhal, Zachary Nado, Joost van Amersfoort, Andreas Kirsch, Rodolphe Jenatton, Nithum Thain, Honglin Yuan, Kelly Buchanan, Kevin Murphy, D. Sculley, Yarin Gal, Zoubin Ghahramani, Jasper Snoek, and Balaji Lakshminarayanan. 2022. Plex: Towards reliability using pretrained large model extensions.</p>
<p>Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. 2023. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170.</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax.</p>
<p>Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, and Tao Qin. 2021. Generalizing to unseen domains: A survey on domain generalization. pages $4627-4635$.</p>
<p>Pei Wang, Yejie Wang, Muxi Diao, Keqing He, Guanting Dong, and Weiran Xu. 2024a. Multi-perspective consistency enhances confidence estimation in large language models. arXiv preprint arXiv:2402.11279.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023a. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Yejie Wang, Keqing He, Guanting Dong, Pei Wang, Weihao Zeng, Muxi Diao, Yutao Mou, Mengdi Zhang, Jingang Wang, Xunliang Cai, and Weiran Xu. 2024b. Dolphcoder: Echo-locating code large language models with diverse and multi-objective instruction tuning.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh</p>
<p>Hajishirzi. 2023b. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484-13508, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you need.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions.</p>
<p>Mingfeng Xue, Dayiheng Liu, Kexin Yang, Guanting Dong, Wenqiang Lei, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2023. Occuquest: Mitigating occupational bias for inclusive large language models.</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models.</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023a. Scaling relationship on learning mathematical reasoning with large language models.</p>
<p>Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023b. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023a. Mammoth: Building math generalist models through hybrid instruction.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023b. MAmmoTH: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STar: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.</p>
<p>Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023a. Instruction-following evaluation for large language models.</p>
<p>Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. 2023b. Domain generalization: A survey. IEEE Trans. Pattern Anal. Mach. Intell., pages 4396-4415.</p>
<p>Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. 2023a. Solving math word problems via cooperative reasoning induced language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4471-4485, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. 2023b. Solving math word problems via cooperative reasoning induced language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4471-4485, Toronto, Canada. Association for Computational Linguistics.</p>
<h2>Appendix A Augmentation on MATH: AugMATH</h2>
<h3>A.1 Query and response augmentation on MATH</h3>
<p>We conducted simple query and response enhancements for MATH whose training set has 7,500 problems. Each query in the MATH dataset was rewritten in five different ways, and we used GPT-4 to sample each variation eight times with a temperature setting of 0.7. Ultimately, we generated $7,500 \times 5 \times 8=300,000$ synthetic MATH data points, Considering that MATH encompasses a broader range of topics—such as elementary calculus and geometry—rather than mainly algebraic problems like GSM8K, we used our GSM8K query augmentation method as a guide and allowed GPT-4 to reference our approach during the query augmentation process without being strictly confined to those five methods. Details on the augmentation prompts can be found in the Appendix E. Since the MATH dataset comprises 7,500 questions that include seven subjects—Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus—we can decompose MATH into seven subsets each covering different mathematical subjects. Therefore, our augmented dataset, AugMATH, has already undergone augmentation across various mathematical topics. The results on LLaMA-2-7B are listed in Table 7.</p>
<h3>A.2 Relationship of performance and augmented data size</h3>
<p>We conduct SFT of LLaMA-2-7B on subsets of AugMATH, the results are evaluated on MATH test set with 5,000 problems.</p>
<p>We observe from Table 8 a segmented log-linear relationship in the data size’s impact on the model’s performance. This relationship is evident when the data volume is either less than 37.5 k or greater than 82.5 k . We conduct log-linear fittings for these two distinct segments separately, denoting $x$ as the data size in thousands and $y$ as the accuracy on the MATH test set. In the range of 7.5 k to 37.5 k , the log-linear relationship can be fitted as $y=1.47+2.45 \log (x)$, with coefficient of determination 0.992 . In the range of 82.5 k to 307.5 k , it can be fitted as $y=-13.56+6.33 \log (x)$, with coefficient of determination 0.985 . We can see that once the data volume reaches a certain threshold, the performance gains from doubling the data become much more substantial.</p>
<h3>A.3 Generalizability of AugMATH to GSM8K</h3>
<p>From Table 8, we observe that as the volume of AugMATH data increases, the performance on GSM8K does not continuously rise but rather exhibits significant fluctuations. However, on the whole, when the full AugMATH dataset (300K) is utilized, the fine-tuned performance on GSM8K improves from 40.3 to 45.4. This increase is relatively modest, especially considering that just 30 K of AugGSM8K can elevate the model’s performance on GSM8K to a score of 58.</p>
<p>In summary, we can see that our proposed simple data augmentation method is equally effective on the MATH dataset, but exhibits a piecewise logarithmic-linear property; although it appears that augmentation on MATH provides certain gains for GSM8K, these gains are very slight relative to the amount of data augmented.</p>
<h3>A.4 Generalizability on other OOD datasets</h3>
<p>To further illustrate generalizability, we conducted tests on five additional datasets: GSM-Hard (Gao et al.), SVAMP (Patel et al., 2021), TabMWP (Lu et al., 2023), ASDiv (Miao et al., 2020), and MAWPS (Koncel-Kedziorski et al., 2016).</p>
<p>It can be seen from Table 9 that MuggleMATH not only outperforms the base model and SFT model on other datasets, but also exhibits a significant performance advantage compared to the state-of-the-art augmentation method, WizardMath. Overall, by performing augmentation on both GSM8K and MATH, we can obtain robust capabilities across a wide range of datasets. The reason behind may be that these out-of-ditribution datasets are actually similar to the distribution of GSM8K and are not so out-of-ditribution.</p>
<h2>B Instruction prompt for training and inference</h2>
<p>Here is the instruction prompt used for the training and inference stage.</p>
<h2>Fine-tuning system prompt</h2>
<p>Below is an instruction that describes a task. Write a response that appropriately completes the request.### Instruction: <strong>Query.</strong> ### Response:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">SFT dataset</th>
<th style="text-align: center;">Algebra</th>
<th style="text-align: center;">Counting \&amp; Probability</th>
<th style="text-align: center;">Geometry</th>
<th style="text-align: center;">Intermediate Algebra</th>
<th style="text-align: center;">Number Theory</th>
<th style="text-align: center;">Prealgebra</th>
<th style="text-align: center;">Precalculus</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MATH</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">4.6</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">2.7</td>
</tr>
<tr>
<td style="text-align: left;">AugMATH</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">12.8</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance of AugMATH Across Various Mathematical Domains</p>
<table>
<thead>
<tr>
<th style="text-align: left;">AugMATH + MATH size</th>
<th style="text-align: center;">Accuracy on MATH Test Set (\%)</th>
<th style="text-align: center;">Transfer learning to GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">40.3</td>
</tr>
<tr>
<td style="text-align: left;">7.5 k</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">42.5</td>
</tr>
<tr>
<td style="text-align: left;">15 k</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">41.8</td>
</tr>
<tr>
<td style="text-align: left;">22.5 k</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">41.9</td>
</tr>
<tr>
<td style="text-align: left;">30 k</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">43.1</td>
</tr>
<tr>
<td style="text-align: left;">37.5 k</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">41.1</td>
</tr>
<tr>
<td style="text-align: left;">82.5 k</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">43.2</td>
</tr>
<tr>
<td style="text-align: left;">157.5 k</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">42.8</td>
</tr>
<tr>
<td style="text-align: left;">232.5 k</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">46.5</td>
</tr>
<tr>
<td style="text-align: left;">307.5 k</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">45.4</td>
</tr>
</tbody>
</table>
<p>Table 8: Performance of different AugMATH + MATH size on the MATH Test Set</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">GSM-Hard</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">TabMWP</th>
<th style="text-align: center;">ASDiv</th>
<th style="text-align: center;">MAWPS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLaMA-2-7B</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">60.9</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-2-7B-SFT</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">60.0</td>
</tr>
<tr>
<td style="text-align: left;">WizardMath</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">73.7</td>
</tr>
<tr>
<td style="text-align: left;">MuggleMATH (AugGSM8K)</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">89.8</td>
</tr>
<tr>
<td style="text-align: left;">MuggleMATH (AugMATH)</td>
<td style="text-align: center;">17.1</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">68.5</td>
</tr>
</tbody>
</table>
<p>Table 9: Model performance on different OOD mathematical datasets</p>
<h2>C Query augmentation prompt for GSM8K</h2>
<p>Here is the query augmentation prompt we use for GSM8K. We require the models to generate five different augmented problems with our provided example. We use gpt-3.5-turbo-0613 and gpt-4-0613 APIs with a temperature of 1.0 to obtain augmented problems.</p>
<h2>Query augmentation prompt</h2>
<p>I want you to act as a math teacher. I will provide a grade school math question and you will help to to create more challenging math questions by given ways. Given the question: "James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year?", you will modify it by following ideas:</p>
<ol>
<li>Change specific numbers: James writes
a 2-page letter to 2 different friends 3 times a week. How many pages does he write in 4 years?</li>
<li>Introduce fractions or percentages: James writes a 3-page letter to 2 different friends twice a week. Each week, he adds $50 \%$ more pages to each letter. How many pages does he write in a month?</li>
<li>Combine multiple concepts: James writes a 3-page letter to 2 different friends twice a week. He uses both sides of the paper and each side can hold 250 words. If James writes 100 words per minute, how long does it take for him to write all the letters in a week?</li>
<li>Include a conditional statement: James writes a 3-page letter to 2 different friends twice a week. If it's a holiday, he writes an additional 5-page letter to each friend. Considering there are 10 holidays in a year,</li>
</ol>
<p>how many pages does he write in a year?
5. Increase the complexity of the problem: James writes a 3-page letter to 2 different friends twice a week. In addition, he writes a 5-page letter to 3 other friends once a week. How many pages does he write in a month, assuming there are 4 weeks in a month?
Now you are given the question:
<strong>A new math problem here.</strong></p>
<h2>D Response augmentation prompt for GSM8K</h2>
<p>We use this prompt to generate responses to ensure the response format which can be viewed as 1 shot setting. We use gpt-3.5-turbo-0613 and gpt-4-0613 with temperature 0.0 or 1.0.</p>
<h2>Response augmentation prompt</h2>
<p>I want you to act as an excellent math solver. You will solve the given math question step by step. You need to reply with a python dictionary in the same format as the given examples. Retain decimals to three decimal places. The formulas in the process need to use the format: $48 / 2=« 48 / 2=24 » 24$ clips. The end of response needs to be: #### {answer}.
Examples: {"query": "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?", "response": "Natalia sold 48/2 $=« 48 / 2=24 » 24$ clips in May.Natalia sold $48+24=« 48+24=72 » 72$ clips altogether in April and May.#### 72"} .
The given question:
<strong>A new math problem here.</strong></p>
<h2>E Query and response augmentation prompt for MATH</h2>
<p>We use this prompt to augment MATH dataset and use gpt-4-0613 with temperature 0.0 .</p>
<h2>Response augmentation prompt</h2>
<p>I want you to act as a math teacher. You should think of some ways to help students do variation training for challenging com-
petition mathematics problems. For example, for a question-solution pair, Question0: James writes a 3-page letter to 2 different friends twice a week. How many pages does he write in a year? Solution0: He writes each friend $3 \times 2=6$ pages a week. So he writes $6 \times 2=12$ pages every week That means he writes $12 \times 52=624$ pages a year ##end0" we can propose 5 types of variation exercises, and response with:</p>
<ol>
<li>Change specific numbers: Question1: James writes a 5-page letter to 3 different friends 4 times a week. How many pages does he write in 3 years? Solution1: To calculate the total number of pages James writes in 3 years, let's first figure out how many pages he writes each week and then multiply that by the number of weeks in 3 years. He writes each friend a 5-page letter, so for 3 friends, that's $5 \times 3=15$ pages per writing session. He writes 4 times a week, so the weekly total is $15 \times 4=60$ pages. There are 52 weeks in a year, so in one year, he writes $60 \times 52=3120$ pages. Finally, over the course of 3 years, he writes $3120 \times 3=9360$ pages. ##end1</li>
<li>Introduce fractions or percentages: Question2: James writes a 3-page letter to 2 different friends twice a week. Each week, he adds $100 \%$ more pages to each letter. How many pages does he write in a month? Solution2: Let's take this step by step: In the first week, James writes a 3-page letter to 2 friends twice a week, which is $3 \times 2 \times 2=12$ pages in total for the first week. n the second week, he writes $100 \%$ more pages, thus doubling the number of pages in each letter. So he writes $6 \times 2 \times 2=24$ pages in total for the second week. In the third week, he again writes double the previous week's pages, so $12 \times 2 \times 2=48$ pages in total for the third week. In the fourth week, the number of pages doubles again, which results in $24 \times 2 \times 2=96$ pages in total for the fourth week. Now, we'll add up the pages from all four weeks to find out how many pages he writes in a month: $12+24+48+96=180$ pages. Therefore, in a month (assuming a 4-week month), James writes 180 pages.</li>
</ol>
<p>##end2
3. Combine multiple concepts: Question3: James writes a 3-page letter to 2 different friends twice a week. He uses both sides of the paper, and each side can hold 250 words. If James writes at a speed of 100 words per minute, how long does it take him to write all the letters in a week? Solution3: To find out how long it takes James to write all the letters in a week, we first calculate how many words he writes in total. Each letter is 3 pages long, and he writes to 2 friends, which is $3 \times 2=6$ pages per writing session. Since he writes twice a week, the total number of pages per week is $6 \times 2=12$ pages. Considering each page has two sides and each side holds 250 words, the number of words on one page is $250 \times 2=500$ words. Therefore, the total number of words James writes in a week is $500 \times 12=6000$ words. Given James writes at a speed of 100 words per minute, the time it takes him to write all the letters in a week is calculated by dividing the total number of words by his writing speed: 6000 words $\div 100$ words $/$ minute $=60$ minutes. So, James takes 60 minutes to write all the letters in a week. ##end3
4. Include a conditional statement: Question4: XX Solution4: XX ##end4
5. Increase the complexity of the problem: Question5: XX Solution5: XX ##end5
Now, find five suitable variation training methods for the new problem. Be careful not to let existing methods limit your thinking. Instead, propose variation training methods that are specifically tailored to the given problem:
Question0: ${ }^{<em> </em>}$ A new math problem here.<strong>
Solution0: ${ }^{<em> </em>}$ corresponding solution here here.</strong>
Please response with the given example format(including Questions and solutions)</p>
<h2>F Response filter</h2>
<p>We filter out generated responses by following rules.</p>
<ul>
<li>Delete the responses without an answer.</li>
<li>Delete the responses that are excessively lengthy $(&gt;1500)$.</li>
<li>Remove superfluous characters beyond the reasoning path and the answer.</li>
</ul>
<h2>G Case Study of GSM8K</h2>
<p>There are examples of different methods for generating new queries in Table 10 and different reasoning paths for the same query in Table 11. Some examples of MuggleMath-13B answering questions from the GSM8K test set are in Table 12 and wrong reasoning processes are labeled in red.</p>
<h2>H Difficulty level definition on GSM8K</h2>
<p>We conducted a statistical analysis of the reasoning paths required for 7,473 questions in the GSM8K training set, categorizing them as hard, medium, and easy. Specifically, we defined questions with more than three formulas, exactly three formulas, and less than three formulas as hard, medium, and easy, respectively. This categorization yielded a balanced distribution with 2,357 easy, 2,360 medium, and 2,756 hard problems. This approach ensures a relatively equal number of problems in each category.</p>
<h2>I Discussion of augmentation methods for GSM8K</h2>
<h2>I. 1 How to improve existing data augmentation methods?</h2>
<p>If one wants to further improve using these ideas, there are two relatively direct approaches to enhance our current methodology.</p>
<ul>
<li>The first approach involves thinking of additional methods beyond the initial five. Although it is impossible to exhaust all the methods of rewriting queries, it is relatively easy to propose some similar rewriting approaches and examples, akin to the five schemes we have suggested. For example, "Introduce unknown quantities", "Introduce a range", "Finding averages" may be reasonable. Limited by resources, we only employ "Introduce unknown quantities" to expand the dataset.</li>
<li>The second approach is to augment the queries we have expanded again. We conducted further enhancement on "mixed-augmentation" while maintaining the same data volume. We called it "Mixed-augmentation-second".</li>
</ul>
<table>
<thead>
<tr>
<th>Method</th>
<th>Query</th>
</tr>
</thead>
<tbody>
<tr>
<td>Original</td>
<td>Weng earns $\$ 12$ an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?</td>
</tr>
<tr>
<td>Change specific numbers</td>
<td>Weng earns $\$ 15$ an hour for babysitting. Last week, she babysat for 3 hours and 45 minutes every day. How much did she earn in total last week?</td>
</tr>
<tr>
<td>Introduce fractions or percentages</td>
<td>Weng earns $\$ 12$ an hour for babysitting. However, for every hour she babysits beyond the first 3 hours, she earns an additional $20 \%$ per hour. If she babysat for 5 hours yesterday, how much did she earn?</td>
</tr>
<tr>
<td>Combine multiple concepts</td>
<td>Weng earns $\$ 12$ an hour for babysitting, but gets paid only $75 \%$ of the total at the time of service. The rest is paid at the end of the month. If she babysat for 3 hours yesterday, how much was she paid immediately and how much will she receive later?</td>
</tr>
<tr>
<td>Include a conditional statement</td>
<td>Weng earns $\$ 12$ an hour for babysitting. If it's a weekend, she gets a bonus of $\$ 5$. If she babysat for 2 hours last Sunday, how much did she earn?</td>
</tr>
<tr>
<td>Increase the complexity of the problem</td>
<td>Weng earns $\$ 12$ an hour for babysitting during weekdays and $\$ 15$ an hour on weekends. If she babysat for 2 hours on Tuesday and 3 hours on Saturday, how much did she earn in total?</td>
</tr>
</tbody>
</table>
<p>Table 10: Examples of different methods for generating new queries.</p>
<p>Taking the prompt in the question as an instance, the original problem is: "James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year?" Our five methods are as follows:</p>
<ul>
<li>Change specific numbers: James writes a 2page letter to 2 different friends 3 times a week. How many pages does he write in 4 years?</li>
<li>Introduce fractions or percentages: James writes a 3-page letter to 2 different friends twice a week. Each week, he adds $50 \%$ more pages to each letter. How many pages does he write in a month?</li>
<li>Combine multiple concepts: James writes a 3-page letter to 2 different friends twice a week. He uses both sides of the paper, and each side can hold 250 words. If James writes 100 words per minute, how long does it take for him to write all the letters in a week?</li>
<li>Include a conditional statement: James writes a 3-page letter to 2 different friends twice a week. If it's a holiday, he writes an additional</li>
</ul>
<p>5-page letter to each friend. Considering there are 10 holidays in a year, how many pages does he write in a year?</p>
<ul>
<li>Increase the complexity of the problem: James writes a 3-page letter to 2 different friends twice a week. In addition, he writes a 5-page letter to 3 other friends once a week. How many pages does he write in a month, assuming there are 4 weeks in a month?
In addition to these, we can conceive of other methods:</li>
<li>Introduce unknown quantities: James writes some pages to his friends every week. After 4 years, he has written 1,440 pages. How many pages does James write each week?</li>
<li>Introduce a range: James writes about 150200 words per page. If he writes a 4-page letter, what is the maximum and minimum number of total words he writes?</li>
<li>Finding Averages: James wrote 5 letters last week. The number of pages were 2, 3, 4, 3, and 2 , respectively. What was the average number of pages per letter?</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Query</th>
<th style="text-align: left;">Weng earns $\$ 12$ an hour for regular babysitting but <br> doubles her rate during weekends. Yesterday, she did <br> 50 minutes of babysitting and it was a weekend. How <br> much did she earn?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Response1</td>
<td style="text-align: left;">Weng earns $\$ 12$ per hour and doubles her rate dur- <br> ing weekends, so she earned $\$ 12 * 2=24$ per hour <br> yesterday. There are 60 minutes in an hour, so Weng <br> did 50 minutes / 60 minutes/hour $=0.833$ hours of <br> babysitting. Therefore, Weng earned $\$ 24 * 0.833=$ <br> 19.992, which we can round to $\$ 20$.</td>
</tr>
<tr>
<td style="text-align: left;">Response2</td>
<td style="text-align: left;">Weng earns $\$ 12 * 2=24$ an hour during weekends. <br> 50 minutes is $50 / 60=0.833$ of an hour. So, Weng <br> earned $24 * 0.833=19.992$ dollars yesterday.</td>
</tr>
<tr>
<td style="text-align: left;">Response3</td>
<td style="text-align: left;">During weekends, Weng earns $12 * 2=24$ dollars per <br> hour for babysitting. There are 60 minutes in an <br> hour. So, 50 minutes is $50 / 60=0.833$ of an hour. <br> Weng earned $24 * 0.833=20$ dollars for 50 minutes <br> of babysitting during the weekend.</td>
</tr>
<tr>
<td style="text-align: left;">Response4</td>
<td style="text-align: left;">Her weekend rate is $12 * 2=24$ dollars per hour. There <br> are 60 minutes in an hour, so she worked $50 / 60=$ <br> 0.833 hours. So, Weng earned $24 * 0.833=19.992$ <br> dollars.</td>
</tr>
<tr>
<td style="text-align: left;">Response5</td>
<td style="text-align: left;">Weng's rate during weekends is $12 * 2=24$ dollars <br> per hour. Since an hour has 60 minutes, Weng earned <br> $24 *(50 / 60)=20$ dollars yesterday.</td>
</tr>
<tr>
<td style="text-align: left;">Response6</td>
<td style="text-align: left;">Weng earns $\$ 12 * 2=24$ an hour during weekends. <br> She worked for $50 / 60=0.833$ hour yesterday. So, <br> Weng earned $0.833 * \$ 24=19.992$ yesterday.</td>
</tr>
<tr>
<td style="text-align: left;">Response7</td>
<td style="text-align: left;">Weng earns $\$ 12$ an hour, but during the weekend, <br> she doubles her rate which means she earns $12 * 2=$ <br> 24 dollars an hour during the weekend. Babysitting <br> for 50 minutes is equivalent to $50 / 60=0.833$ hours. <br> Therefore, Weng earned $24 * 0.833=19.992$ dollars <br> from babysitting yesterday.</td>
</tr>
</tbody>
</table>
<p>Table 11: Different reasoning paths for the same query.</p>
<p>We can observe that the "Introduce Unknown Quantities" strategy is essentially as effective as the other five methods from Table 14. Continuing to augment the expanded queries, however, yields slightly less effective results compared to the original five methods. A possible reason for this could be that the problems created through secondary augmentation are too complex, resulting in a lower accuracy rate in the responses provided by GPT-4, thereby impacting the efficacy of the augmentation.</p>
<h2>I. 2 Will mixed-augmentation has advantages over increase the complexity if we enlarge the size of data?</h2>
<p>Increasing the dataset size to observe whether the mixed-augmentation approach can yield better results due to its diverse enhancements is indeed necessary. We categorized the data of different query augmentations in $\mathcal{D}<em 2="2">{1}^{1}, \mathcal{D}</em>$ and performed random sampling from these mixed datasets to obtain a comparable quantity of data. We conduct a SFT on LLaMA-2-7B, the results are list in Table 15.}^{1}, \mathcal{D}_{3}^{1</p>
<h2>J The effect of augmentation response quality of GSM8K</h2>
<h2>J. 1 Under what circumstances can wrong answers have a positive effect?</h2>
<p>To investigate the effect of augmentation on response quality, we aim to collect datasets with responses that are uniformly incorrect, uniformly correct, and partially correct. We amalgamate $\mathcal{D}<em 1="1">{1}^{1}, \mathcal{D}</em>}^{2}$ , $\mathcal{D<em 1="1">{1}^{3}, \mathcal{D}</em>}^{4}, \mathcal{D<em 1="1">{1}^{5}, \mathcal{D}</em>$, and categorize the combined dataset into three groups:}^{6}, \mathcal{D}_{1}^{7</p>
<ul>
<li>The first category consists of cases where all seven responses are the same.</li>
<li>The second category includes instances where each of the seven responses is distinct.</li>
<li>The third category encompasses queries with 2 to 6 varied answers.</li>
</ul>
<p>To mitigate the influence of the query, we eliminate the first and second categories and focus on the third category, where we vote on the answers. The answer with the majority of votes is deemed the correct response, while the others are classified as incorrect. We randomly select one correct and one incorrect response for each query from the third category to form the sets $\mathcal{D}<em _text="\text" _wrong="{wrong">{\text {corrct }}$ and $\mathcal{D}</em>}}$, ensuring both sets contain identical queries. Subsequently, we randomly sample half of the queries from $\mathcal{D<em _text="\text" _wrong="{wrong">{\text {corrct }}$ and pair the remaining queries from $\mathcal{D}</em>}}$ to create $\mathcal{D<em _corrct="{corrct" _text="\text">{\text {half }}$, which shares the same queries as $\mathcal{D}</em>$ in Table 16. Although the estimated accuracy may not be precise, it serves to indicate the trend.}}$ and $\mathcal{D}_{\text {wrong }</p>
<p>We conduct Supervised Fine-Tuning (SFT) on these three datasets on two models with varying accuracy: LLaMA-2-7B-SFT (41.6 on GSM8K test set), which is fine-tuned from the GSM8K training set, and MuggleMATH-7B (68.4 on GSM8K test set), which is fine-tuned from five subsets of AugGSM8K. The results are as follows:</p>
<p>From Table 16, we can draw some conclusions: (1)For LLaMA-2-7B-SFT, the higher the accuracy of the fine-tuned dataset, the more substantial the performance gain. There is an improvement of 4.5 percentage points even when fine-tuned on a dataset consisting solely of incorrect responses. This may be attributable to the presence of correct reasoning steps within the responses with wrong answers, as mentioned in (Lightman et al., 2023b). (2)For MuggleMATH-7B, the lower the accuracy of the dataset, the more significant the performance degradation.</p>
<p>In summary, erroneous data still contributes to performance improvement for models with poorer performance, but it has a detrimental effect on models with superior performance.</p>
<h2>J. 2 The relationship between response quality and data volume.</h2>
<p>To delve deeper into the relationship between the quality and quantity of augmented data, we partition $\mathcal{D}<em _text="\text" _wrong="{wrong">{\text {corrct }}, \mathcal{D}</em>$ into different fractions and conduct Supervised Fine-Tuning (SFT) on LLaMA-2-7B-SFT (41.6 on GSM8K test set).}}$ and $\mathcal{D}_{\text {half }</p>
<p>From these results in table 17, we can draw some insightful conclusions: (1)As the volume of data is increased, even datasets of poor quality can still yield performance gains for LLaMA-2-7BSFT. (2)The higher the quality of the responses, the more substantial the performance improvements achieved with each doubling of the data volume.</p>
<h2>J. 3 The Accuracy and difficulty of different query augmentation categories</h2>
<p>In an effort to discern the performance disparities among various query augmentation techniques, we evaluate the accuracy of responses across different categories. By employing majority voting to determine the reference answer from $\mathcal{D}<em 1="1">{1}^{1}, \mathcal{D}</em>}^{2}, \mathcal{D<em 1="1">{1}^{3}, \mathcal{D}</em>}^{4}$, $\mathcal{D<em 1="1">{1}^{5}, \mathcal{D}</em>$, we consider queries that yield seven distinct answers as incorrect. Additionally, we compute the average number of reasoning steps for each augmentation method-a formula such as "30+90=«30+90=120»120" is counted as a single step.}^{6}$ and $\mathcal{D}_{1}^{7</p>
<p>From the Table 18, we observe a general tradeoff between the number of reasoning steps and accuracy. As corroborated in section 4.2, the augmentation yields greater benefits for more challenging problems. Notably, the "Increase Complexity" method, while exhibiting the lowest dataset accuracy, involves the highest number of reasoning steps. This suggests that augmenting with more complex problems can lead to more substantial benefits for the model.</p>
<h2>K A more detailed comparison of GSM8K, MATH, AugGSM8K, and AugMATH</h2>
<p>In GSM8K and AugGSM8K, a formula such as "30+90=«30+90=120»120" is counted as a single step.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The relationship of accuracy on the training set and the amounts of augmentation data.</p>
<p>Finding a reasonable way to calculate reasoning steps is challenging in the MATH and AugMATH datasets, as the answers in the MATH dataset are in LaTeX format and vary greatly. We consider one sentence in the response (ending with a period or semicolon) as one step of reasoning. An interesting finding is that the number of reasoning steps in AugMATH is significantly lower than in MATH. One possible explanation could be an issue of linguistic style; GPT-4 and humans might use a different number of sentences for the same number of reasoning steps. Therefore, the reasoning steps in MATH are difficult to compare. The second reason may be that GPT-4 has only a 42% accuracy rate on MATH problems, and on the more challenging MuggleMATH, it provides incorrect responses for most questions. These guys incorrect responses may have fewer reasoning steps than are actually required for the problems.</p>
<p>The reason why a model fails to generalize from one dataset to another after data augmentation might be influenced by many factors. From the perspective of human learning, no matter how much one studies elementary school mathematics, it would be quite difficult to solve high school math problems. Therefore, we believe that simply performing data augmentation during the SFT (Supervised Fine-Tuning) phase to increase accuracy on a dataset, while it may yield significant performance gains, might not lead to as substantial improvements for large models across a wide variety of reasoning tasks as it may appear.</p>
<h2><strong>L Detailed Experimental results</strong></h2>
<p>We list the detailed experimental results of different settings here.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The relationship of accuracy on the GSM8K test set and the original GSM8K training set.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Work done during internships at Alibaba Group.
${ }^{1}$ Corresponding author.
${ }^{2}$ Xiang Wang is also affiliated with Institute of Dataspace, Hefei Comprehensive National Science Center.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
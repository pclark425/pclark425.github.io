<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7991 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7991</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7991</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-275820736</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.13299v2.pdf" target="_blank">Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents</a></p>
                <p><strong>Paper Abstract:</strong> Materials discovery and design are essential for advancing technology across various industries by enabling the development of application-specific materials. Recent research has leveraged Large Language Models (LLMs) to accelerate this process. We explore the potential of LLMs to generate viable hypotheses that, once validated, can expedite materials discovery. Collaborating with materials science experts, we curated a novel dataset from recent journal publications, featuring real-world goals, constraints, and methods for designing real-world applications. Using this dataset, we test LLM-based agents that generate hypotheses for achieving given goals under specific constraints. To assess the relevance and quality of these hypotheses, we propose a novel scalable evaluation metric that emulates the process a materials scientist would use to evaluate a hypothesis critically. Our curated dataset, proposed method, and evaluation framework aim to advance future research in accelerating materials discovery and design with LLMs.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7991.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7991.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATDESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MATDESIGN benchmark/dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated benchmark of 50 paper-derived instances (goals, constraints, ground-truth materials and methods) from January 2024 journals created to evaluate LLM-generated materials-discovery hypotheses while avoiding pretraining data leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (Hypotheses Generator used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis (materials discovery/design)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Benchmark dataset for hypothesis evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Structured inputs (goal + constraints) with ground-truth materials and methods used to compute similarity-based and quality-based evaluations of generated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Closeness (Concept Overlap, Property Overlap, Keyword Matching) and Quality (six criteria)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Closeness components: 1–5 ordinal scales for Concept Overlap, Property Overlap, Keyword Matching; Quality: unweighted average of six 1–5 scales (Alignment, Scientific Plausibility, Innovation/Novelty, Testability, Feasibility/Scalability, Impact Potential).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MATDESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Ground-truth extracted by materials-science experts; human evaluation by four PhD materials-science students using the same metrics (see paper: 4 reviewers).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used as test set for experiments reported in-paper (50 instances); automated metrics reported across configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dataset size is small (50 papers) and may not capture full domain diversity; selected temporally to avoid pretraining leakage (Jan 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7991.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7991.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACCELMAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ACCELMAT agentic framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based multi-agent pipeline for goal-driven hypothesis generation in materials design consisting of a Hypotheses Generation Agent (HGA), multi-LLM Critic Agents (CAs), a Summarizer Agent (SA), and an Evaluation Agent (EA); optional integration with a materials knowledge graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (HGA); critics: GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Flash; evaluator: OpenAI-o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hypothesis generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Multi-stage agentic evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate 20 hypotheses per instance with HGA; have three LLM-based critics evaluate and provide feedback; consolidate feedback with a summarizer; iterate up to 5 cycles; finalize unanimously-approved hypotheses for scoring by the Evaluation Agent.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Closeness and Quality metrics (see entries)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Consensus criterion: hypotheses only proceed if unanimously approved by three critics; iteration cap of 5; final scores computed by evaluation agent on defined scales.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MATDESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human experts reviewed finalized suggestions; their assessments used the same Closeness and Quality metrics; 4 PhD students performed independent evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Three configurations tested: Without feedback (Closeness 70%, Quality 79.67%); With feedback (Closeness 73.33%, Quality 85.67%); With KG+Feedback (Closeness 80%, Quality 89%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Automated evaluation rankings paralleled human expert ratings; the KG+feedback configuration received the best human and automated scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Relies on LLM-based critics whose unanimous agreement does not guarantee scientific correctness; cost/runtime constraints limited to 20 hypotheses per goal; dependence on human-provided constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7991.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7991.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Closeness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Closeness evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Objective similarity-based metric measuring how close a generated hypothesis is to the ground truth via three subcomponents: Concept Overlap, Property Overlap, and Keyword Matching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI-o1-preview (Evaluation Agent) evaluating GPT-4o-generated hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric for hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Closeness (Concept, Property, Keyword)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compare generated hypothesis to ground truth along three axes: (1) Concept Overlap (degree core ideas/methods align), (2) Property Overlap (alignment of material properties quantitative/qualitative), (3) Keyword Matching (entity and method keyword matches).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Concept Overlap / Property Overlap / Keyword Matching (each on 1–5 ordinal scale); aggregate reported as % (paper reports composite Closeness score).</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Each subcomponent uses 1–5 scale with textual definitions (1=no overlap/match, 5=complete/perfect match); composite Closeness reported as percentage across instances (e.g., 70.0%, 73.33%, 80.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MATDESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human evaluators used the same 1–5 scales and provided per-hypothesis reasoning; 4 PhD students participated.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Average Closeness by configuration — Without FB: 70%; With FB: 73.33%; With FB+KG: 80%. Also individual per-suggestion sub-scores reported (see Appendix examples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Automated Closeness evaluations by the Evaluation Agent aligned with human expert rankings (paper reports consistency between automated and human evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Ordinal nature and dependence on textual judgments can be subjective; limited interpretability when using purely embedding/text-similarity metrics; potentially sensitive to wording differences even when concepts align.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7991.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7991.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quality evaluation metric (six criteria)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-criterion quality assessment for generated hypotheses averaging six expert-defined criteria: Alignment, Scientific Plausibility, Innovation/Novelty, Testability, Feasibility/Scalability, and Impact Potential.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI-o1-preview (Evaluation Agent) assessing GPT-4o outputs; also used by human experts</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation metric for hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Quality metric (6-criterion average)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Score each hypothesis 1–5 on six criteria; compute unweighted average as overall Quality score; textual guidance and scales provided in Appendix Table 6.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Six 1–5 ordinal scores (Alignment; Scientific Plausibility; Innovation/Novelty; Testability; Feasibility & Scalability; Impact Potential); overall Quality = mean of six scores, reported as % in paper (e.g., 79.67% ≈ mean*20).</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Per-criterion definitions: 1=poor/not aligned/not testable/etc., through 5=fully aligned/completely plausible/highly testable/completely feasible/transformative impact; overall reported as percentage.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MATDESIGN (used to score generated hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Four PhD students independently rated hypotheses using same six-criteria rubric; their scores compared to automated EA outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Quality scores by config — Without FB: 79.67%; With FB: 85.67%; With FB+KG: 89%. Per-hypothesis criterion scores reported in examples (e.g., Suggestion 1: Alignment 3, Plausibility 4, Innovation 4, Testability 2, Feasibility 2, Impact 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Automated Quality evaluations were consistent with human expert ratings; human reviewers gave similar relative ratings across configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Unweighted averaging may under/over-emphasize criteria; subjectivity in rating instructions; reliance on LLM-based evaluation agent (possible hallucinations or calibration differences).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7991.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7991.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI-o1-preview (EA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI o1-preview Evaluation Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source LLM (OpenAI-o1-preview) used as the Evaluation Agent to score generated hypotheses on Closeness and Quality metrics, chosen for nuanced semantic assessment beyond lexical overlap metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI-o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science (used as evaluator of materials hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automated evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based hypothesis evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Prompted LLM evaluates each hypothesis against ground truth and rubric, outputs numeric ratings and justifications for Closeness submetrics and Quality criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Produces 1–5 ordinal ratings per submetric/criterion and aggregated composite scores reported as percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>See Closeness and Quality metric definitions; EA applies textual rating prompts (Appendix E) to produce scores.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MATDESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>EA outputs compared against 4 human expert evaluations for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>EA produced the reported Closeness and Quality scores across the three ACCELMAT configurations; alignment with human experts was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>EA-based automated rankings paralleled human expert results; both found KG+feedback configuration best.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>LLM evaluator may be opaque and subject to its own biases/hallucinations; chosen over ROUGE/BERTScore due to better semantic judgement but still not perfect.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7991.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7991.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Critic Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-LLM Critic system (GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Flash)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three LLM-based critic agents that evaluate each generated hypothesis for adherence to goals/constraints and provide structured JSON feedback used for iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (one critic and HGA), Claude-3.5-Sonnet, Gemini-1.5-Flash</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>critique/evaluation agents</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Multi-critic iterative feedback and consensus</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Each critic assesses whether a suggestion meets goals/constraints, returns detailed JSON feedback (YES/NO + reasoning); unanimous YES across three critics is required to finalize a hypothesis; otherwise, Summarizer consolidates feedback for refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-suggestion binary 'Meets goal and constraints' (YES/NO) plus textual reasoning; consensus count reported (out of 20).</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Consensus metric = number of hypotheses (out of 20) unanimously approved by all three critics per instance/configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MATDESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Critic feedback used in iterative loop; separate human experts later evaluated final outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Average number of universally-agreed hypotheses per 20: Without FB: 11; With FB: 18; With FB+KG: 19 (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Higher consensus among critics correlated with higher human approval and automated scores; critics' unanimous agreement does not guarantee absolute scientific correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Reliance on LLM critics risks shared hallucinations or correlated errors; unanimous LLM agreement is not a substitute for domain-expert validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7991.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7991.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Expert Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert assessment (4 PhD students)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Independent human evaluation by four materials-science PhD students using the same Closeness and Quality rubric to validate automated evaluation results and provide domain judgment on hypothesis usefulness and novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Human experts evaluating GPT-4o/ACCELMAT outputs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>human evaluation of hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human expert scoring on Closeness and Quality</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Four PhD students independently rated 42 suggestions using the defined Closeness submetrics and six Quality criteria, providing qualitative commentary.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-hypothesis 1–5 ratings on Closeness submetrics and six Quality criteria; qualitative feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Same 1–5 scales defined in the rubric; inter-rater agreement not quantitatively reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MATDESIGN (subset of 42 suggestions from experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>4 independent PhD student reviewers; used same evaluation metrics as automated system; qualitative comments recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Human ratings paralleled automated EA results: KG+feedback configuration scored highest; without-feedback configuration underperformed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Paper reports consistency between automated EA and human evaluations (validating automated metric alignment), but no detailed inter-rater statistics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small number of human reviewers; no inter-rater agreement statistics provided; human reviewers noted lack of deep domain specificity in many suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7991.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7991.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERTScore+MaterialsBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERTScore computed with MaterialsBERT model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embedding-based lexical/semantic similarity baseline (BERTScore) computed using a domain-adapted MaterialsBERT model to compare generated hypotheses to ground truth; reported as a comparative baseline and criticized for interpretability limits in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MaterialsBERT (model used to compute BERTScore); hypotheses generated by GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science / NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>automated similarity metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BERTScore (MaterialsBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute embedding-based similarity between generated hypotheses and ground truth using MaterialsBERT representations; report percent similarity scores per configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>BERTScore percentage (higher = more similar); reported per-configuration averages.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported values in paper: Hypotheses Generation Without Feedback: 52.66%; With Feedback: 60.59%; With KG+Feedback: 50.30%.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MATDESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used for automated baseline comparison; not used for human scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>BERTScore results did not align with LLM-based EA or human expert judgments (KG+Feedback performed worst by BERTScore despite best human/EA scores), highlighting interpretability issues.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>BERTScore disagreed with human/EA rankings in at least one configuration; authors note interpretability and domain-fit limitations of embedding scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Authors caution that embedding-based metrics require domain fine-tuning and may not capture contextual/functional similarities important in hypothesis evaluation; BERTScore differences were hard to interpret in terms of practical scientific alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7991.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7991.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consensus metric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Critic consensus (# agreed hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple operational metric counting how many of the 20 generated hypotheses achieved unanimous approval from all three Critic Agents; used as a proxy for viability and alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Critic Agents (GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Flash) evaluating GPT-4o outputs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science / evaluation process</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>consensus-based evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Number of unanimously-approved hypotheses per batch (out of 20)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each generated batch of 20 hypotheses, count those that all three critics mark as meeting goal/constraints; used to compare configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Integer count (0–20) per instance averaged across dataset; reported per-configuration averages.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported averages: Without FB: 11/20; With FB: 18/20; With FB+KG: 19/20.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MATDESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Correlation with human expert acceptance reported qualitatively; higher consensus correlated with higher human approval.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Demonstrated increase in consensus when feedback and knowledge graph are added (11→18→19 average agreed suggestions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>Greater critic consensus aligned with higher human expert ratings, but unanimous critic agreement does not guarantee scientific accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Consensus among LLM critics can reflect shared model biases; not a definitive indicator of correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7991.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7991.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feasibility & Impact scales</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feasibility and Impact Potential ordinal scales (rubric definitions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operational rubrics used within the Quality metric defining Feasibility (Not Feasible → Completely Feasible) and Impact Potential (No Impact → Transformative Impact) across 1–5 scales with textual anchors for ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied by OpenAI-o1-preview and human experts to evaluate GPT-4o outputs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>evaluation criteria/scales</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Feasibility and Impact Potential scales (1–5)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Feasibility assesses implementability and scale-up potential; Impact Potential measures likely field advancement if implemented; both use 1–5 descriptors provided in Appendix and Table 6.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>1–5 ordinal scores with textual definitions; incorporated into overall Quality mean.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Feasibility: 1=Not Feasible → 5=Completely Feasible; Impact: 1=No Impact → 5=Transformative Impact. Example descriptors provided in Appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MATDESIGN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used by 4 PhD reviewer panel and EA to score hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Per-suggestion Feasibility and Impact scores reported in qualitative examples; contributed to reported Quality averages (see Quality entry).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Coarse ordinal bins can hide nuance; Feasibility conflates technical difficulty and scale-up economics; no weighting applied in overall Quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MatKG (Venugopal and Olivetti, 2024) <em>(Rating: 2)</em></li>
                <li>BERTScore <em>(Rating: 2)</em></li>
                <li>MaterialsBERT (Shetty et al., 2023) <em>(Rating: 2)</em></li>
                <li>Benchmarks for chemistry/materials LLM evaluation (Guo et al., 2023) <em>(Rating: 1)</em></li>
                <li>Benchmarks for materials LLM evaluation (Zaki et al., 2023) <em>(Rating: 1)</em></li>
                <li>LLM-based agentic hypothesis generation works (Jia et al., 2024) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7991",
    "paper_id": "paper-275820736",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "MATDESIGN",
            "name_full": "MATDESIGN benchmark/dataset",
            "brief_description": "A curated benchmark of 50 paper-derived instances (goals, constraints, ground-truth materials and methods) from January 2024 journals created to evaluate LLM-generated materials-discovery hypotheses while avoiding pretraining data leakage.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (Hypotheses Generator used in experiments)",
            "model_size": null,
            "scientific_domain": "materials science",
            "theory_type": "hypothesis (materials discovery/design)",
            "evaluation_method_name": "Benchmark dataset for hypothesis evaluation",
            "evaluation_method_description": "Structured inputs (goal + constraints) with ground-truth materials and methods used to compute similarity-based and quality-based evaluations of generated hypotheses.",
            "evaluation_metric": "Closeness (Concept Overlap, Property Overlap, Keyword Matching) and Quality (six criteria)",
            "metric_definition": "Closeness components: 1–5 ordinal scales for Concept Overlap, Property Overlap, Keyword Matching; Quality: unweighted average of six 1–5 scales (Alignment, Scientific Plausibility, Innovation/Novelty, Testability, Feasibility/Scalability, Impact Potential).",
            "dataset_or_benchmark": "MATDESIGN",
            "human_evaluation_details": "Ground-truth extracted by materials-science experts; human evaluation by four PhD materials-science students using the same metrics (see paper: 4 reviewers).",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Used as test set for experiments reported in-paper (50 instances); automated metrics reported across configurations.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Dataset size is small (50 papers) and may not capture full domain diversity; selected temporally to avoid pretraining leakage (Jan 2024).",
            "uuid": "e7991.0",
            "source_info": {
                "paper_title": "Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ACCELMAT",
            "name_full": "ACCELMAT agentic framework",
            "brief_description": "An LLM-based multi-agent pipeline for goal-driven hypothesis generation in materials design consisting of a Hypotheses Generation Agent (HGA), multi-LLM Critic Agents (CAs), a Summarizer Agent (SA), and an Evaluation Agent (EA); optional integration with a materials knowledge graph.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (HGA); critics: GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Flash; evaluator: OpenAI-o1-preview",
            "model_size": null,
            "scientific_domain": "materials science",
            "theory_type": "hypothesis generation pipeline",
            "evaluation_method_name": "Multi-stage agentic evaluation pipeline",
            "evaluation_method_description": "Generate 20 hypotheses per instance with HGA; have three LLM-based critics evaluate and provide feedback; consolidate feedback with a summarizer; iterate up to 5 cycles; finalize unanimously-approved hypotheses for scoring by the Evaluation Agent.",
            "evaluation_metric": "Closeness and Quality metrics (see entries)",
            "metric_definition": "Consensus criterion: hypotheses only proceed if unanimously approved by three critics; iteration cap of 5; final scores computed by evaluation agent on defined scales.",
            "dataset_or_benchmark": "MATDESIGN",
            "human_evaluation_details": "Human experts reviewed finalized suggestions; their assessments used the same Closeness and Quality metrics; 4 PhD students performed independent evaluations.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Three configurations tested: Without feedback (Closeness 70%, Quality 79.67%); With feedback (Closeness 73.33%, Quality 85.67%); With KG+Feedback (Closeness 80%, Quality 89%).",
            "comparison_to_human_generated": true,
            "comparison_results": "Automated evaluation rankings paralleled human expert ratings; the KG+feedback configuration received the best human and automated scores.",
            "limitations_noted": "Relies on LLM-based critics whose unanimous agreement does not guarantee scientific correctness; cost/runtime constraints limited to 20 hypotheses per goal; dependence on human-provided constraints.",
            "uuid": "e7991.1",
            "source_info": {
                "paper_title": "Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Closeness",
            "name_full": "Closeness evaluation metric",
            "brief_description": "Objective similarity-based metric measuring how close a generated hypothesis is to the ground truth via three subcomponents: Concept Overlap, Property Overlap, and Keyword Matching.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI-o1-preview (Evaluation Agent) evaluating GPT-4o-generated hypotheses",
            "model_size": null,
            "scientific_domain": "materials science",
            "theory_type": "evaluation metric for hypotheses",
            "evaluation_method_name": "Closeness (Concept, Property, Keyword)",
            "evaluation_method_description": "Compare generated hypothesis to ground truth along three axes: (1) Concept Overlap (degree core ideas/methods align), (2) Property Overlap (alignment of material properties quantitative/qualitative), (3) Keyword Matching (entity and method keyword matches).",
            "evaluation_metric": "Concept Overlap / Property Overlap / Keyword Matching (each on 1–5 ordinal scale); aggregate reported as % (paper reports composite Closeness score).",
            "metric_definition": "Each subcomponent uses 1–5 scale with textual definitions (1=no overlap/match, 5=complete/perfect match); composite Closeness reported as percentage across instances (e.g., 70.0%, 73.33%, 80.0%).",
            "dataset_or_benchmark": "MATDESIGN",
            "human_evaluation_details": "Human evaluators used the same 1–5 scales and provided per-hypothesis reasoning; 4 PhD students participated.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Average Closeness by configuration — Without FB: 70%; With FB: 73.33%; With FB+KG: 80%. Also individual per-suggestion sub-scores reported (see Appendix examples).",
            "comparison_to_human_generated": true,
            "comparison_results": "Automated Closeness evaluations by the Evaluation Agent aligned with human expert rankings (paper reports consistency between automated and human evaluations).",
            "limitations_noted": "Ordinal nature and dependence on textual judgments can be subjective; limited interpretability when using purely embedding/text-similarity metrics; potentially sensitive to wording differences even when concepts align.",
            "uuid": "e7991.2",
            "source_info": {
                "paper_title": "Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Quality",
            "name_full": "Quality evaluation metric (six criteria)",
            "brief_description": "A multi-criterion quality assessment for generated hypotheses averaging six expert-defined criteria: Alignment, Scientific Plausibility, Innovation/Novelty, Testability, Feasibility/Scalability, and Impact Potential.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenAI-o1-preview (Evaluation Agent) assessing GPT-4o outputs; also used by human experts",
            "model_size": null,
            "scientific_domain": "materials science",
            "theory_type": "evaluation metric for hypotheses",
            "evaluation_method_name": "Quality metric (6-criterion average)",
            "evaluation_method_description": "Score each hypothesis 1–5 on six criteria; compute unweighted average as overall Quality score; textual guidance and scales provided in Appendix Table 6.",
            "evaluation_metric": "Six 1–5 ordinal scores (Alignment; Scientific Plausibility; Innovation/Novelty; Testability; Feasibility & Scalability; Impact Potential); overall Quality = mean of six scores, reported as % in paper (e.g., 79.67% ≈ mean*20).",
            "metric_definition": "Per-criterion definitions: 1=poor/not aligned/not testable/etc., through 5=fully aligned/completely plausible/highly testable/completely feasible/transformative impact; overall reported as percentage.",
            "dataset_or_benchmark": "MATDESIGN (used to score generated hypotheses)",
            "human_evaluation_details": "Four PhD students independently rated hypotheses using same six-criteria rubric; their scores compared to automated EA outputs.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Quality scores by config — Without FB: 79.67%; With FB: 85.67%; With FB+KG: 89%. Per-hypothesis criterion scores reported in examples (e.g., Suggestion 1: Alignment 3, Plausibility 4, Innovation 4, Testability 2, Feasibility 2, Impact 4).",
            "comparison_to_human_generated": true,
            "comparison_results": "Automated Quality evaluations were consistent with human expert ratings; human reviewers gave similar relative ratings across configurations.",
            "limitations_noted": "Unweighted averaging may under/over-emphasize criteria; subjectivity in rating instructions; reliance on LLM-based evaluation agent (possible hallucinations or calibration differences).",
            "uuid": "e7991.3",
            "source_info": {
                "paper_title": "Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "OpenAI-o1-preview (EA)",
            "name_full": "OpenAI o1-preview Evaluation Agent",
            "brief_description": "A closed-source LLM (OpenAI-o1-preview) used as the Evaluation Agent to score generated hypotheses on Closeness and Quality metrics, chosen for nuanced semantic assessment beyond lexical overlap metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI-o1-preview",
            "model_size": null,
            "scientific_domain": "materials science (used as evaluator of materials hypotheses)",
            "theory_type": "automated evaluator",
            "evaluation_method_name": "LLM-based hypothesis evaluation",
            "evaluation_method_description": "Prompted LLM evaluates each hypothesis against ground truth and rubric, outputs numeric ratings and justifications for Closeness submetrics and Quality criteria.",
            "evaluation_metric": "Produces 1–5 ordinal ratings per submetric/criterion and aggregated composite scores reported as percentages.",
            "metric_definition": "See Closeness and Quality metric definitions; EA applies textual rating prompts (Appendix E) to produce scores.",
            "dataset_or_benchmark": "MATDESIGN",
            "human_evaluation_details": "EA outputs compared against 4 human expert evaluations for validation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "EA produced the reported Closeness and Quality scores across the three ACCELMAT configurations; alignment with human experts was reported.",
            "comparison_to_human_generated": true,
            "comparison_results": "EA-based automated rankings paralleled human expert results; both found KG+feedback configuration best.",
            "limitations_noted": "LLM evaluator may be opaque and subject to its own biases/hallucinations; chosen over ROUGE/BERTScore due to better semantic judgement but still not perfect.",
            "uuid": "e7991.4",
            "source_info": {
                "paper_title": "Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Critic Agents",
            "name_full": "Multi-LLM Critic system (GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Flash)",
            "brief_description": "Three LLM-based critic agents that evaluate each generated hypothesis for adherence to goals/constraints and provide structured JSON feedback used for iterative refinement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (one critic and HGA), Claude-3.5-Sonnet, Gemini-1.5-Flash",
            "model_size": null,
            "scientific_domain": "materials science",
            "theory_type": "critique/evaluation agents",
            "evaluation_method_name": "Multi-critic iterative feedback and consensus",
            "evaluation_method_description": "Each critic assesses whether a suggestion meets goals/constraints, returns detailed JSON feedback (YES/NO + reasoning); unanimous YES across three critics is required to finalize a hypothesis; otherwise, Summarizer consolidates feedback for refinement.",
            "evaluation_metric": "Per-suggestion binary 'Meets goal and constraints' (YES/NO) plus textual reasoning; consensus count reported (out of 20).",
            "metric_definition": "Consensus metric = number of hypotheses (out of 20) unanimously approved by all three critics per instance/configuration.",
            "dataset_or_benchmark": "MATDESIGN",
            "human_evaluation_details": "Critic feedback used in iterative loop; separate human experts later evaluated final outputs.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Average number of universally-agreed hypotheses per 20: Without FB: 11; With FB: 18; With FB+KG: 19 (Table 5).",
            "comparison_to_human_generated": true,
            "comparison_results": "Higher consensus among critics correlated with higher human approval and automated scores; critics' unanimous agreement does not guarantee absolute scientific correctness.",
            "limitations_noted": "Reliance on LLM critics risks shared hallucinations or correlated errors; unanimous LLM agreement is not a substitute for domain-expert validation.",
            "uuid": "e7991.5",
            "source_info": {
                "paper_title": "Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Human Expert Evaluation",
            "name_full": "Human expert assessment (4 PhD students)",
            "brief_description": "Independent human evaluation by four materials-science PhD students using the same Closeness and Quality rubric to validate automated evaluation results and provide domain judgment on hypothesis usefulness and novelty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Human experts evaluating GPT-4o/ACCELMAT outputs",
            "model_size": null,
            "scientific_domain": "materials science",
            "theory_type": "human evaluation of hypotheses",
            "evaluation_method_name": "Human expert scoring on Closeness and Quality",
            "evaluation_method_description": "Four PhD students independently rated 42 suggestions using the defined Closeness submetrics and six Quality criteria, providing qualitative commentary.",
            "evaluation_metric": "Per-hypothesis 1–5 ratings on Closeness submetrics and six Quality criteria; qualitative feedback.",
            "metric_definition": "Same 1–5 scales defined in the rubric; inter-rater agreement not quantitatively reported in paper.",
            "dataset_or_benchmark": "MATDESIGN (subset of 42 suggestions from experiments)",
            "human_evaluation_details": "4 independent PhD student reviewers; used same evaluation metrics as automated system; qualitative comments recorded.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Human ratings paralleled automated EA results: KG+feedback configuration scored highest; without-feedback configuration underperformed.",
            "comparison_to_human_generated": null,
            "comparison_results": "Paper reports consistency between automated EA and human evaluations (validating automated metric alignment), but no detailed inter-rater statistics provided.",
            "limitations_noted": "Small number of human reviewers; no inter-rater agreement statistics provided; human reviewers noted lack of deep domain specificity in many suggestions.",
            "uuid": "e7991.6",
            "source_info": {
                "paper_title": "Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "BERTScore+MaterialsBERT",
            "name_full": "BERTScore computed with MaterialsBERT model",
            "brief_description": "An embedding-based lexical/semantic similarity baseline (BERTScore) computed using a domain-adapted MaterialsBERT model to compare generated hypotheses to ground truth; reported as a comparative baseline and criticized for interpretability limits in this task.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MaterialsBERT (model used to compute BERTScore); hypotheses generated by GPT-4o",
            "model_size": null,
            "scientific_domain": "materials science / NLP evaluation",
            "theory_type": "automated similarity metric",
            "evaluation_method_name": "BERTScore (MaterialsBERT)",
            "evaluation_method_description": "Compute embedding-based similarity between generated hypotheses and ground truth using MaterialsBERT representations; report percent similarity scores per configuration.",
            "evaluation_metric": "BERTScore percentage (higher = more similar); reported per-configuration averages.",
            "metric_definition": "Reported values in paper: Hypotheses Generation Without Feedback: 52.66%; With Feedback: 60.59%; With KG+Feedback: 50.30%.",
            "dataset_or_benchmark": "MATDESIGN",
            "human_evaluation_details": "Used for automated baseline comparison; not used for human scoring.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "BERTScore results did not align with LLM-based EA or human expert judgments (KG+Feedback performed worst by BERTScore despite best human/EA scores), highlighting interpretability issues.",
            "comparison_to_human_generated": true,
            "comparison_results": "BERTScore disagreed with human/EA rankings in at least one configuration; authors note interpretability and domain-fit limitations of embedding scores.",
            "limitations_noted": "Authors caution that embedding-based metrics require domain fine-tuning and may not capture contextual/functional similarities important in hypothesis evaluation; BERTScore differences were hard to interpret in terms of practical scientific alignment.",
            "uuid": "e7991.7",
            "source_info": {
                "paper_title": "Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Consensus metric",
            "name_full": "Critic consensus (# agreed hypotheses)",
            "brief_description": "Simple operational metric counting how many of the 20 generated hypotheses achieved unanimous approval from all three Critic Agents; used as a proxy for viability and alignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Critic Agents (GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Flash) evaluating GPT-4o outputs",
            "model_size": null,
            "scientific_domain": "materials science / evaluation process",
            "theory_type": "consensus-based evaluation metric",
            "evaluation_method_name": "Number of unanimously-approved hypotheses per batch (out of 20)",
            "evaluation_method_description": "For each generated batch of 20 hypotheses, count those that all three critics mark as meeting goal/constraints; used to compare configurations.",
            "evaluation_metric": "Integer count (0–20) per instance averaged across dataset; reported per-configuration averages.",
            "metric_definition": "Reported averages: Without FB: 11/20; With FB: 18/20; With FB+KG: 19/20.",
            "dataset_or_benchmark": "MATDESIGN",
            "human_evaluation_details": "Correlation with human expert acceptance reported qualitatively; higher consensus correlated with higher human approval.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Demonstrated increase in consensus when feedback and knowledge graph are added (11→18→19 average agreed suggestions).",
            "comparison_to_human_generated": true,
            "comparison_results": "Greater critic consensus aligned with higher human expert ratings, but unanimous critic agreement does not guarantee scientific accuracy.",
            "limitations_noted": "Consensus among LLM critics can reflect shared model biases; not a definitive indicator of correctness.",
            "uuid": "e7991.8",
            "source_info": {
                "paper_title": "Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Feasibility & Impact scales",
            "name_full": "Feasibility and Impact Potential ordinal scales (rubric definitions)",
            "brief_description": "Operational rubrics used within the Quality metric defining Feasibility (Not Feasible → Completely Feasible) and Impact Potential (No Impact → Transformative Impact) across 1–5 scales with textual anchors for ratings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied by OpenAI-o1-preview and human experts to evaluate GPT-4o outputs",
            "model_size": null,
            "scientific_domain": "materials science",
            "theory_type": "evaluation criteria/scales",
            "evaluation_method_name": "Feasibility and Impact Potential scales (1–5)",
            "evaluation_method_description": "Feasibility assesses implementability and scale-up potential; Impact Potential measures likely field advancement if implemented; both use 1–5 descriptors provided in Appendix and Table 6.",
            "evaluation_metric": "1–5 ordinal scores with textual definitions; incorporated into overall Quality mean.",
            "metric_definition": "Feasibility: 1=Not Feasible → 5=Completely Feasible; Impact: 1=No Impact → 5=Transformative Impact. Example descriptors provided in Appendix.",
            "dataset_or_benchmark": "MATDESIGN",
            "human_evaluation_details": "Used by 4 PhD reviewer panel and EA to score hypotheses.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Per-suggestion Feasibility and Impact scores reported in qualitative examples; contributed to reported Quality averages (see Quality entry).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Coarse ordinal bins can hide nuance; Feasibility conflates technical difficulty and scale-up economics; no weighting applied in overall Quality.",
            "uuid": "e7991.9",
            "source_info": {
                "paper_title": "Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MatKG (Venugopal and Olivetti, 2024)",
            "rating": 2
        },
        {
            "paper_title": "BERTScore",
            "rating": 2
        },
        {
            "paper_title": "MaterialsBERT (Shetty et al., 2023)",
            "rating": 2
        },
        {
            "paper_title": "Benchmarks for chemistry/materials LLM evaluation (Guo et al., 2023)",
            "rating": 1
        },
        {
            "paper_title": "Benchmarks for materials LLM evaluation (Zaki et al., 2023)",
            "rating": 1
        },
        {
            "paper_title": "LLM-based agentic hypothesis generation works (Jia et al., 2024)",
            "rating": 1
        }
    ],
    "cost": 0.0177015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents
8 Feb 2025</p>
<p>Shrinidhi Kumbhar skumbha4@asu.edu 
Arizona State University</p>
<p>Venkatesh Mishra vmishr23@asu.edu 
Arizona State University</p>
<p>Kevin Coutinho 
Arizona State University</p>
<p>Divij Handa 
Arizona State University</p>
<p>Ashif Iquebal 
Arizona State University</p>
<p>Chitta Baral chitta@asu.edu 
Arizona State University</p>
<p>Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents
8 Feb 2025ACEB657C4027710FFFAFE202B26B2F34arXiv:2501.13299v2[cs.CL]Final Rankings: Suggestion 7 Concept Overlap: 4 Property Overlap: 4 Keyword Matching: 4 Suggestion 6 Concept Overlap: 3 Property Overlap: 3 Keyword Matching: 3 Suggestion 1 Concept Overlap: 2 Property Overlap: 2
Materials discovery and design are essential for advancing technology across various industries by enabling the development of applicationspecific materials.Recent research has leveraged Large Language Models (LLMs) to accelerate this process.We explore the potential of LLMs to generate viable hypotheses that, once validated, can expedite materials discovery.Collaborating with materials science experts, we curated a novel dataset from recent journal publications, featuring real-world goals, constraints, and methods for designing realworld applications.Using this dataset, we test LLM-based agents that generate hypotheses for achieving given goals under specific constraints.To assess the relevance and quality of these hypotheses, we propose a novel scalable evaluation metric that emulates the process a materials scientist would use to evaluate a hypothesis critically.Our curated dataset, proposed method, and evaluation framework aim to advance future research in accelerating materials discovery and design with LLMs. 1</p>
<p>Introduction</p>
<p>The discovery and design of materials to meet specific application needs is essential to address critical challenges we face today (Jain et al., 2013).Traditional methods for materials discovery and design are time-intensive and resource-heavy, requiring researchers to conduct extensive literature reviews, explore vast compositional, chemical, and structural spaces through simulations, and perform laborious lab-based experiments (Davies et al., 2016;Hautier et al., 2012).Recent advances in machine learning and data-driven approaches have accelerated materials discovery by enabling predictions of material structures and properties (Liu et al., 1 Data and code are available at https://github.com/shri071/Hypothesis-Generation-for-Materials-Discovery-and-Design-Using-Goal-Driven-and-Constraint-Guided-LLM Figure 1: Overview of our iterative hypothesis generation and evaluation pipeline.Starting from an input prompt and a knowledge graph, the Hypotheses Generator (GPT-4o) proposes 20 hypotheses, which are then reviewed by three critics-GPT-4o, Claude-3.5-Sonnet,and Gemini-1.5-Flash.Their feedback is consolidated by the Summarizer (GPT-4o); if unanimous agreement is not reached, the hypotheses along with critic feedback are fed back to the Hypotheses Generator for refinement and are re-evaluated by the critics.Once approved, the final hypotheses proceed to the Evaluation Agent (OpenAI-o1-preview) for scoring.2017;Oganov et al., 2019) and in proposing novel materials (Chen and Ong, 2022;Merchant et al., 2023;Ren et al., 2022;Fung et al., 2022).However, these methods rely on extensive training datasets and can't process natural language, limiting their flexibility for hypothesis generation.</p>
<p>To address these limitations, recent research in natural language processing (NLP) and materials science has explored the use of LLMs for hypothesis generation (Jia et al., 2024;Sprueill et al., 2024;Ghafarollahi and Buehler, 2024).While promising, these methods are often restricted to a specific material or property and rely on domain-specific external tools that run simulations, which are costly and time-intensive.To overcome these challenges, we design an LLM-based agent for ACCELerating MATerials discovery and design, ACCELMAT.Specifically, our architecture consists of a Hypotheses Generation Agent, a multi-LLM Critic system with iterative feedback, a Summarizer Agent to consolidate all feedback, and an Evaluation Agent to assess the hypotheses.An overview of our architecture is shown in Fig 1 .To assess the performance of our system, we introduce MATDESIGN, a dataset developed in collaboration with materials science experts.While existing benchmarks, such as those proposed by Zaki et al. (2023) and Guo et al. (2023), have proven valuable for evaluating LLMs knowledge and capability in material science and chemistry tasks, they are limited to assessing LLMs knowledge within graduate-level subdomains of materials science or narrowly focused chemistry tasks, failing to evaluate their capability to generate hypotheses for realworld materials discovery and design tailored for a specific application under given constraints.Furthermore, our dataset is constructed from research papers published in leading journals in 2024, ensuring it lies beyond the knowledge cutoff of all LLMs employed in our study2 .</p>
<p>Finally, to evaluate the hypotheses generated by our agent, the evaluation metric is divided into two primary components: Closeness and Quality.Closeness measures how close the generated hypothesis is with the ground truth.Quality assesses the Alignment, Scientific Plausibility, Novelty, Feasibility, Scalability, Testability and Impact Potential of the hypothesis within its domain.These metrics mirror the systematic and rigorous approach employed by material scientists when validating hypotheses, providing a robust framework for comprehensive evaluation.</p>
<p>In summary, our contributions are as follows:</p>
<p>• Create a novel benchmark, MATDESIGN, consisting of goals and constraints, along with their corresponding materials and methods.</p>
<p>• Develop an LLM-based agentic framework to generate and refine material discovery and design hypotheses.</p>
<p>• Propose a scalable evaluation metric to measure the relevance and quality of the generated hypotheses for material discovery and design.2 Background and Related Work</p>
<p>LLMs for Materials Discovery and Design</p>
<p>Machine learning and data-driven methods have significantly accelerated materials science research by reducing the time and resources required for discovery and design (Liu et al., 2017;Oganov et al., 2019).Generative models have furthered these advancements, as evidenced by works like Chen and Ong (2022) 2021), which have provided researchers with powerful tools to aid material discovery and design.Recently, LLMs have emerged as a promising approach in this domain, with efforts falling into two main categories.</p>
<p>The first category involves fine-tuning LLMs on domain-specific datasets or corpora (Sirumalla et al., 2024;Özçelik et al., 2024;Song et al., 2023;Jacobs et al., 2024).By training the models on materials science data, researchers can enhance their ability to predict material properties or address domain-specific questions.However, these approaches are resource-intensive and rely heavily on the availability of extensive, high-quality datasets, which can be challenging to compile.</p>
<p>The second category focuses on leveraging agent-based frameworks that integrate LLMs with domain-specific tools, APIs, and databases to fa-cilitate hypothesis generation and refinement (Jia et al., 2024;Sprueill et al., 2024;Ghafarollahi and Buehler, 2024;Zhang et al., 2024).These multiagent systems enable iterative improvements by combining LLM outputs with specialized domain knowledge, simulations, and experimental feedback.While effective, these methods are often constrained by the scope of available tools, the specificity of the materials or properties studied, and the accessibility of supporting resources.This limits their applicability and ease of use across broader material science challenges.</p>
<p>To address these limitations, our work introduces an LLM-based agentic framework designed to support hypothesis generation and exploration across a wide range of material science applications.As summarized in Table 1, unlike existing methods, our approach minimizes dependency on domainspecific tools and enhances generalizability, enabling more accessible and scalable materials discovery and design workflows.Parallel efforts by Ding et al. (2024) and Yang et al. (2024), have explored distinct approaches, with the former leveraging databases to identify materials with desired properties and utilizing fine-tuned LLMs to suggest modifications, and the latter employing literaturebased insights and mutation algorithms to rediscover chemical hypotheses.</p>
<p>Benchmarks for Materials Discovery and Design with LLMs</p>
<p>The increasing use of LLMs in scientific domains (Choi and Lee, 2024;Taylor et al., 2022;Chen et al., 2022;Cavanagh et al., 2024;Singhal et al., 2022) underscores the importance of rigorous evaluation frameworks to assess their performance.Existing benchmarks, such as those by Guo et al. (2023) and Zaki et al. (2023) assess LLMs on a range of chemistry and material science tasks respectively.However, these benchmarks fail to assess LLM's ability to generate valid hypotheses for materials discovery or design under specific goals and constraints.Furthermore, these benchmarks are susceptible to overlap with LLM pretraining corpora, particularly given that most open-source and closed-source LLMs have a knowledge cutoff in late 2023.This overlap raises concerns about the novelty and validity of benchmark evaluation.</p>
<p>To mitigate these issues, we developed a new dataset in collaboration with materials science experts.This dataset, derived from 50 research papers published in journals from January 2024, includes entries structured with clearly defined goal statements, relevant constraints, and the materials and methods required to achieve the specified objectives.As summarized in Table 2, our benchmark not only ensures independence from pretraining data but also emphasizes real-world applicability by focusing on hypothesis generation tasks, providing a robust foundation for evaluating LLMs in materials discovery and design.</p>
<p>MATDESIGN</p>
<p>To evaluate the capacity of LLMs to generate materials discovery and design hypotheses, we curated a specialized benchmark.The dataset consists of information extracted from 50 research papers published from January 2024 in prominent journals such as Nature, Nature Communications, and Progress in Organic Coatings.Each entry in the dataset includes the following manually extracted and structured components:</p>
<p>• Goal Statement: Description of material development for a specific application, serving as the first part of the input to our framework.</p>
<p>• Constraints: Conditions that steer the hypothesis generation process, serving as the final part of the input to the framework.</p>
<p>• Materials: Key materials and their compositions, serving as the first part of the ground truth.</p>
<p>• Methods: Brief description of synthesis methods, forming the final part of the ground truth.</p>
<p>Materials Science experts assisted in all extractions, ensuring the benchmark's accuracy and relevance.The selection of publications from January 2024 is essential to ensure that the ground truth information (material names, compositions, and synthesis methods) associated with the respective goals and constraints is not present in the training corpus of LLMs we use, which have a training cutoff of late 2023.This temporal constraint enables a rigorous evaluation of the LLMs ability to generate genuinely novel hypotheses, relying on their internalized understanding of materials science principles rather than retrieving pre-existing information.An example instance is shown in Table 3.</p>
<p>ACCELMAT</p>
<p>The LLM-based multi-agent framework designed for this study consists of four key components, each</p>
<p>Goal</p>
<p>Constraints</p>
<p>Material Name and Methods</p>
<p>Develop a scalable extrinsic selfhealing coating system for corrosion protection of metallic structures in offshore environments.</p>
<p>1) The material should incorporate a self-healing mechanism triggered by a single environmental factor (e.g., water).</p>
<p>2) The self-healing material should allow multiple healing events.</p>
<p>. . .</p>
<p>Materials:</p>
<p>Core-shell nanofibers synthesized using coaxial electrospinning.Organosilane compounds, specifically silyl esters, used as the self-healing agent.Metallic substrates (e.g., steel) for corrosion tests.Methods: Coaxial electrospinning of core-shell nanofibers with an organosilane compound (silyl ester) as the healing agent. . . .</p>
<p>Critic Agents (CA):</p>
<p>The second component consists of three Critic Agents-GPT-4o, Claude-3.5-SonnetAnthropic (2024), andGemini-1.5-Flash Team et al. (2024a).These agents are provided with the hypotheses generated by the HGA, goal statement, and constraints.Their role is to evaluate each hypothesis, assessing its alignment with the goal and constraints.Each critic gives detailed feedback to guide subsequent hypothesis refinement cycles.The prompts used for CA can be found in Appendix C.4.</p>
<p>Summarizer Agent (SA):</p>
<p>The Summarizer Agent consolidates and organizes the feedback from all three CAs into a structured format.It then provides this comprehensive feedback to the HGA to guide the refinement process.We use GPT-4o as the SA.The prompt used for SA is provided in Appendix C.5.</p>
<p>Evaluation Agent (EA):</p>
<p>The Evaluation Agent is used to evaluate the closeness and quality of the generated hypotheses.We use OpenAI-o1-preview Jaech et al. (2024) as our evaluation agent.</p>
<p>The HGA, CA, and SA form the hypothesis generation and refinement framework.The EA is used solely to evaluate the generated hypotheses.</p>
<p>Evaluation Metrics</p>
<p>To assess the generated hypotheses comprehensively, we adopt a dual-metric evaluation framework.The first metric, Closeness, measures the degree of alignment between the generated and ground truth hypotheses.The second metric, Quality, evaluates the generated hypotheses based on six distinct criteria.Together, these metrics provide a holistic assessment: Closeness facilitates evaluation in scenarios with ground truth data, while Quality enables robust evaluation in cases where ground truth hypotheses are unavailable.The evaluation prompts used for measuring Closeness and Quality can be found in Appendix E.</p>
<p>Closeness</p>
<p>The Closeness metric measures the similarity between the generated and ground truth hypotheses in the dataset, focusing on the following:</p>
<ol>
<li>
<p>Concept Overlap: Assesses the degree to which the core ideas, methods, and scientific concepts in the generated hypothesis align with those in the ground truth.</p>
</li>
<li>
<p>Property Overlap: Evaluates the extent to which the material properties in the generated hypothesis align with those in the ground truth, encompassing both quantitative values and qualitative descriptions.</p>
</li>
<li>
<p>Keyword Matching: Assesses the accuracy of specific entities (e.g., material names, chemical compounds, synthesis methods) and keywords in the generated hypothesis by comparing them to those in the ground truth.</p>
</li>
</ol>
<p>The objective scores for evaluating Closeness are presented in Table 4.</p>
<p>Quality</p>
<p>The second part of the evaluation focuses on the Quality of the generated hypotheses, assessed across six defined criteria developed in collaboration with materials science experts.This metric is particularly valuable for evaluating hypotheses in scenarios where no ground truth is available for the given goals and constraints.The detailed definition associated with the ratings is described in Table 6.The overall Quality score is calculated as the unweighted average of the scores across all six criteria.While a weighted average approach could be used to emphasize specific criteria, the unweighted average is used in this study for simplicity.</p>
<p>To enable scalable and accurate evaluation, we utilize OpenAI-o1-preview.This model was selected over traditional metrics like ROUGE, which are limited in capturing nuanced semantic similarities.While embedding-based models offer an alternative, they often require fine-tuning on domainspecific datasets and may fail to adequately capture contextual meanings critical for evaluating hypotheses in materials science.For comparative purposes, traditional evaluation is provided in Appendix G.A set of generated hypotheses evaluated with the Evaluation Agent and our proposed evaluation metric can be found in Appendix F.  (2024), we designed three distinct configurations of ACCELMAT tailored for material discovery and design, with the first configuration as a baseline mentioned in the section 6.1.These configurations are detailed in the subsections below.</p>
<p>Hypotheses Generation without Feedback from Critics</p>
<p>As a baseline, for this configuration, we evaluate the standalone hypothesis generation capabilities of the HGA without external feedback.The HGA generates 20 hypotheses with detailed reasoning based on a given goal statement and constraints.These hypotheses are independently reviewed by three CAs, who assess their alignment with the goal and adherence to constraints.Only hypotheses unanimously validated by all three CAs are finalized for evaluation, ensuring reliability through consensus.This setup isolates the HGA's performance, focusing on its ability to generate high-quality hypotheses autonomously.Refer to Appendix D.1 for the prompts used for this configuration.</p>
<p>Hypotheses Generation with Feedback from Critics</p>
<p>This configuration introduces an iterative feedback loop involving the HGA, three CAs, and SA to improve the Closeness and Quality of the hypotheses.</p>
<p>The HGA generates 20 hypotheses, which the CAs evaluate for alignment with a goal and constraints.If unanimous agreement is not reached, each CA provides individual feedback, which the SA consolidates into cohesive feedback for the HGA to refine its hypotheses.This process iterates up to five times or stops early if consensus is achieved, finalizing unanimously validated hypotheses for evaluation.Refer to Appendix D.2 for prompts for all agents used in this configuration.</p>
<p>Hypotheses Generation with Knowledge Graph and Feedback from Critics</p>
<p>This configuration builds upon the second setup by incorporating a comprehensive materials science knowledge graph.In addition to the HGA, CAs, and SA components, this configuration integrates contextual information from MatKG (Venugopal and Olivetti, 2024), the largest publicly available knowledge graph in materials science.</p>
<p>To tailor the hypothesis generation process, keywords relevant to specific applications mentioned are extracted from the goal statement and constraints using GPT-4o.These keywords are then utilized to query MatKG, retrieving relevant materials and their associated properties.The extracted information is subsequently provided to the HGA as supplementary context, enabling it to generate hypotheses that are both novel and aligned with the predefined goals and constraints.</p>
<p>By leveraging the extensive and structured data within MatKG, this setup enhances the HGA's ability to propose innovative and efficient material combinations.Detailed prompts for this configuration can be found in Appendix D.3.</p>
<p>Results and Analysis</p>
<p>This section presents the analysis of the performance of various configurations of the ACCEL-MAT.The results are summarized in Figure 2.   Hypotheses Generation without Feedback The first configuration examined is the HGA operating without critical feedback loops.This baseline configuration revealed several challenges:</p>
<p>• Lack of Consensus: Generated hypotheses frequently failed to achieve unanimous agreement among critics, indicating inconsistencies in the reasoning and alignment with the provided goal.</p>
<p>• Incomplete Adherence to Constraints: A significant number of hypotheses did not fully respect the constraints set for material selection.Missing key details in the reasoning process was a recurring issue.</p>
<p>• Bias in Material and Method Selection: The system exhibited a noticeable preference for certain materials and methods, which constrained the exploration of alternative solutions and reduced diversity in the output.</p>
<p>These shortcomings resulted in hypotheses that were often too generic and lacked the detail needed for practical implementation.This can be seen from the lowest feasibility score.</p>
<p>Quantitatively, the average closeness score for this configuration was 70%, which was 10% lower than the best-performing configuration.Similarly, the Quality metric suffered, achieving only 79.67%, representing a 9.33% decline relative to the optimal setup.These results underscore the limitations of operating without a feedback mechanism.</p>
<p>Hypotheses Generation with Feedback Introducing feedback loops from critics into the HGA demonstrated substantial improvements across all metrics:</p>
<p>• Enhanced Constraint Adherence: Feedback iterations enabled hypotheses to align more closely with predefined goals and constraints, as visible with increased Alignment score and Closeness score.</p>
<p>• Increased Diversity and Feasibility: Feedback facilitated exploration of a broader range of materials and methods, reflecting improved conceptual understanding and hypothesis diversity.The diverse suggestions were equally feasible, demonstrating a balance between variety and practicality in addressing the given goals and constraints.</p>
<p>• Refined Methodology: Successive iterations improved the procedural reasoning and robustness of generated hypotheses, resulting in more actionable outputs.</p>
<p>This feedback-enabled configuration achieved a Closeness score of 73.33%, a 3.33% improvement, and a Quality score of 85.67%, marking a 6% increase over the feedback-free setup.</p>
<p>However, limitations persisted.The hypotheses remained focused on well-established methodologies, with limited exploration of unconventional solutions.While the reasoning for material selection improved, it still lacked sufficient depth to propose unique yet actionable alternatives.These results emphasize the importance of feedback loops in enhancing system performance while pointing to the need for additional mechanisms to promote innovation.</p>
<p>Hypotheses Generation with Knowledge Graph and Feedback Integrating a knowledge graph and feedback from critics yielded the best performance, demonstrating significant advantages:</p>
<p>• Diverse and Novel Combinations: The use of the knowledge graph enabled the exploration of more diverse and novel material combinations, facilitating innovative approaches to material selection and design.This resulted in hypotheses that were both unique and wellaligned with the constraints and objectives.</p>
<p>• Consensus Among Critics: The integration of feedback ensured a highest rate of agreement among critics, indicating a strong alignment with design goals and constraints.</p>
<p>• Improved Feasibility: The majority of the hypotheses generated were not only diverse but also highly feasible, reflecting the robustness of this configuration.</p>
<p>Quantitatively, this configuration achieved the highest Closeness score of 80%, a 6.67% improvement over the feedback-only setup, and a Quality score of 89%, marking a 3.33% increase.</p>
<p>Despite these successes, in some cases, the information retrieved from the knowledge graph, while relevant to the goal and constraints, tends to focus on fundamental materials.This resulted in broad and generalized suggestions that lack the specificity needed for practical implementation.These findings demonstrate the effectiveness of combining knowledge graphs with feedback while highlighting opportunities for refining specificity and depth.</p>
<p>Increased Consensus among Critics with Feedback and Knowledge Graph Generating more hypotheses increases the likelihood of aligning with the ground truth by improving the chances of capturing correct outputs that can assist material science researchers.However, we wanted to restrict the cost per instance and the runtime, hence we restricted ACCELMAT to generate 20 hypotheses per goal even though we agree that more hypotheses would potentially be better.As shown in</p>
<p>Human Expert Evaluation</p>
<p>A set of 42 suggestions, encompassing all three configurations, were independently evaluated by four PhD students in Materials Science.Their assessments employed the same evaluation metrics used by the automated system.For the first configuration, Hypotheses Generation without Feedback, human reviewers found the proposals scientifically valid but lacking in innovation.The reasoning behind material selection was minimal, and the focus on conventional materials and standard practices yielded low novelty.</p>
<p>When feedback loops were introduced, the hypotheses became more refined but continued to emphasize known materials with relatively shallow justifications.Although valid, the output remained constrained to well-documented methods, offering limited creativity.</p>
<p>In contrast, integrating both the Knowledge Graph and the critic feedback elicited the most positive responses.The system proposed a broader range of hypotheses and introduced new material combinations.Despite occasional oversimplifications and some lack of domain specificity, experts appreciated the heightened creativity and potential for innovation.</p>
<p>Overall, the human evaluations paralleled the automated results.The configuration without feedback consistently underperformed, the feedbackbased system improved outcomes, and the system enhanced with the Knowledge Graph and feedback achieved the most favorable ratings.This consis-tency validates the automated evaluation with the evaluation Agent.</p>
<p>Conclusion</p>
<p>In this work, we introduced MATDESIGN, a novel benchmark for evaluating the ability of LLMs to generate innovative material hypotheses rather than merely reproducing pre-trained knowledge.Comprising 50 recently published journal papers (as of January 2024), it offers a reliable test bed for ensuring originality in the generated outputs.</p>
<p>Building on this benchmark, our ACCELMAT framework demonstrates promising results in producing both novel and feasible material hypotheses.Although the generated suggestions often fall short of providing the depth of reasoning or methodological details required for immediate practical application, they serve as a powerful starting point, enabling researchers to refine and extend the hypotheses further.</p>
<p>Finally, we show that our evaluation metric, though not on par with human-level analyses, aligns well with expert judgments.This corroborates the metric's reliability for hypothesis assessment in materials discovery and design and a feedback system which underscores its potential to guide iterative improvements in automated hypothesis-generation systems.</p>
<p>Ethics Statement</p>
<p>We use AI assistants, specifically Grammarly and ChatGPT, were utilized to correct grammatical errors and restructure sentences.</p>
<p>Limitation</p>
<p>While the curated dataset plays a crucial role in ensuring the novelty of generated hypotheses, its size (50 papers) may not fully capture the diversity of materials science research.Despite the experts' best efforts to identify all relevant papers published in renowned journals since January 2024, expanding the dataset as new papers become available could enhance its variety and enable the generation of more innovative hypotheses, providing a more comprehensive evaluation of the LLMs capabilities.</p>
<p>Another limitation arises from relying solely on LLMs for feedback or critique, as even unanimous agreement among state-of-the-art LLM-based critics does not guarantee scientific accuracy.The risk of hallucinated or flawed suggestions remains a challenge.Additionally, our LLM-based agents depend on human-provided constraints to guide hypothesis generation.Advancing the ability of LLMs to autonomously identify and apply relevant constraints could move us closer to achieving fully autonomous materials design and discovery.</p>
<p>Appendix A Open Source Experiments</p>
<p>For comparing open and closed source models, we implemented three configurations described in Section 6 using open source models.Our setup included LLaMA-3.1 70B-Instruct as the Hypotheses Generator, while Gemma-2-27B-Instruct Team et al.</p>
<p>B Table of the six quality assessment criteria</p>
<p>Table 6 summarizes the six quality assessment criteria defined in Section 5.</p>
<p>C Input Prompts</p>
<p>C.1 Goals and Constraints</p>
<p>Goal and constraints provided as input Goal: Develop a scalable extrinsic self-healing coating system for corrosion protection of metallic structures in offshore environments.</p>
<p>Constraints:</p>
<p>1) The material should incorporate a self-healing mechanism triggered by a simple environmental factor (e.g., water).</p>
<p>2) The self-healing material should allow multiple healing events.</p>
<p>3) The coating must maintain its structural integrity and protective capabilities even after mechanical damage.</p>
<p>4) The material should be compatible with scalable application techniques.</p>
<p>5) The healing mechanism should not rely on complex multi-component reactions but rather a single-component system</p>
<p>C.2 Prompt for experts list finder</p>
<p>We use the below prompt for dynamically setting the behaviour of Hypotheses Generator relevant to the provided Goal.The output of this prompt is set as the system prompt for the Hypotheses Generator as shown in Appendix section D.1</p>
<p>Prompt for experts list finder Generate a list of experts required to achieve the below mentioned goal: Develop a scalable extrinsic self-healing coating system for corrosion protection of metallic structures in offshore environments. .</p>
<p>Just list the top 5 experts in the format "Expert_1, Expert_2, Expert_3, Expert_4, Expert_5"</p>
<p>C.3 Prompt for Hypotheses Generator</p>
<p>Prompt for Hypotheses Generator Generate 20 novel and innovative suggestions that strictly satisfy the provided goal and constraints.While building on existing scientific knowledge, also incorporate creative and innovative thinking to propose unique solutions that push the boundaries of current approaches.Ensure that each suggestion is scientifically plausible, grounded in reasoning, and offers a fresh perspective without introducing unsupported information.The ideas should have real-world potential and meet all constraints provided.Provide reason for each suggestion.The suggestions must be in the below mentioned format in a JSON object.For example: { Suggestion_1: { Materials: "Give a comprehensive and detailed description of list of materials, along with their names and properties which achieve help the goal and satisfy all constraints" Methods: "Give detailed description of novel, innovative methods or techniques needed to develop the materials suggested"</p>
<p>Reasoning: "Give a detailed reasoning for the suggestion" }, . . .Suggestion_20: { Materials: "Give a comprehensive and detailed description of list of materials, along with their names and properties which achieve help the goal and satisfy all constraints" Methods: "Give detailed description of novel, innovative methods or techniques needed to develop the materials suggested" Reasoning:"Give a detailed reasoning for the suggestion } }</p>
<p>C.4 Prompt for Critic Feedback</p>
<p>Prompt for Critic Feedback Goal_statement:-{goal_statement} Constraints:-{constraint_list} Suggestions: generated_suggestions.</p>
<p>Given the goal statement, constraints, and suggestions for materials design and discovery, evaluate each suggestion carefully.Provide detailed, constructive feedback on whether each suggestion has the potential to meet the goal and satisfy the constraints.</p>
<p>If any suggestion falls short, explain precisely where it fails and suggest specific improvements or adjustments that will help guide the generation agent to produce more accurate and innovative suggestions in the next cycle.</p>
<p>Ensure the feedback is actionable and focused on refining the generation process to align more closely with the goal and constraints in future iterations.</p>
<p>The detailed feedback should be in the below JSON format strictly:</p>
<p>{ "Feedback_for_suggestion_1": { "Meets_the_goal_statement_and_satisfies_all_constraints_strictly": "YES/NO", "Reasoning": " " } . . ."Feedback_for_suggestion_20": { "Meets_the_goal_statement_and_satisfies_all_constraints_strictly": "YES/NO", "Reasoning": " " }, "Overall_Feedback_for_improvement_for_future_suggestion_generation": (list of points in a single line) }</p>
<p>C.5 Prompt for Summarizer</p>
<p>Prompt for Summarizer</p>
<p>Your task is to summarize feedback provided by multiple critics for a list of generated hypotheses.The summary must capture essential points and detailed insights of the feedback generated by all three critics.The final summary generated must be such that it will help in refining future generation of hypotheses.The feedback is as follows:</p>
<p>Critic_1 feedback: {feedback_from_claude} The expert names are generated by the prompting Hypotheses Generator the experts list finder prompt, which it uses as system prompt for itself.</p>
<p>System prompt for Hypotheses Generator</p>
<p>You are an innovative Materials Scientist, Corrosion Engineer, Polymer Chemist, Environmental Scientist, Chemical Engineer capable of doing impactful materials discovery and design.</p>
<p>User input prompt to the Hypotheses Generator Goal: Develop a scalable extrinsic self-healing coating system for corrosion protection of metallic structures in offshore environments.</p>
<p>Constraints:</p>
<p>1) The material should incorporate a self-healing mechanism triggered by a simple environmental factor (e.g., water)</p>
<p>2) The self-healing material should allow multiple healing events.</p>
<p>3) The coating must maintain its structural integrity and protective capabilities even after mechanical damage.</p>
<p>4) The material should be compatible with scalable application techniques.</p>
<p>5) The healing mechanism should not rely on complex multi-component reactions but rather a single-component system.Generate 20 novel and innovative suggestions that strictly satisfy the provided goal and constraints.While building on existing scientific knowledge, also incorporate creative and innovative thinking to propose unique solutions that push the boundaries of current approaches.</p>
<p>Ensure that each suggestion is scientifically plausible, grounded in reasoning, and offers a fresh perspective without introducing unsupported information.The ideas should have real-world potential and meet all constraints provided.Provide reason for each suggestion.The suggestions must be in the below mentioned format in a JSON object.For example: { Suggestion_1: { Materials: "Give a comprehensive and detailed description of list of materials, along with their names and properties which achieve help the goal and satisfy all constraints" Methods: "Give detailed description of novel, innovative methods or techniques needed to develop the materials suggested" Reasoning: "Give a detailed reasoning for the suggestion" }, . . .Suggestion_20: { Materials: "Give a comprehensive and detailed description of list of materials, along with their names and properties which achieve help the goal and satisfy all constraints" Methods: "Give detailed description of novel, innovative methods or techniques needed to develop the materials suggested" Reasoning:"Give a detailed reasoning for the suggestion } } 20 Hypotheses Generated by Hypotheses Generator Suggestion 1: Materials:Polyurea-based elastomer with embedded microtubules filled with a water-reactive monomer like cyanoacrylate.The microtubules are made from a biodegradable polymer such as polylactic acid (PLA).Methods:Use a co-extrusion process to create microtubules and embed them in the polyurea matrix.After embedding, the PLA can be dissolved to leave behind the microtubules.The cyanoacrylate can then be injected into these tubules.The final coating can be applied using standard spray-painting techniques.</p>
<p>Reasoning: Polyurea provides excellent mechanical properties and durability, while cyanoacrylate polymerizes rapidly upon contact with water.The use of PLA microtubules ensures a continuous supply of the healing agent to damaged areas, allowing for multiple healing events. . . .Suggestion 20: Materials:Hybrid polymer matrix incorporating both hydrophilic and hydrophobic segments, with embedded nanocapsules containing a water-reactive monomer like methyl methacrylate (MMA).Methods:Disperse nanocapsules in the hybrid polymer matrix using mechanical mixing.Apply the coating using spray-painting techniques.</p>
<p>Reasoning:MMA polymerizes upon contact with water, providing a quick and effective healing mechanism.The hybrid polymer matrix ensures compatibility with spray-painting and maintains the coating's mechanical integrity.Reasoning:The suggestion effectively addresses the goal and constraints.The use of polyurea for durability, cyanoacrylate for rapid healing, and PLA microtubules for continuous healing agent supply all contribute to a robust self-healing system.The co-extrusion method ensures scalability, and the spray-painting application aligns with industrial requirements. . . .</p>
<p>Feedback for suggestion 20:</p>
<p>Meets the goal statement and satisfies all constraints strictly:YES.</p>
<p>Reasoning: The polyester coatings with embedded microcapsules containing water-activated DMPA offer good mechanical properties and chemical resistance for offshore applications.The spray-painting compatibility and the single-component healing agent make this suggestion a strong candidate.</p>
<p>Overall Feedback for future suggestion improvement: Explore a broader range of materials, consider incorporating bio-inspired self-healing mechanisms, and investigate the use of advanced characterization techniques to validate the proposed solutions.</p>
<p>Finalised Suggestions Agreed by all Critics</p>
<p>Suggestion 1: Materials:Polyurea-based elastomer with embedded microtubules filled with a water-reactive monomer like cyanoacrylate.The microtubules are made from a biodegradable polymer such as polylactic acid (PLA).Methods:Use a co-extrusion process to create microtubules and embed them in the polyurea matrix.After embedding, the PLA can be dissolved to leave behind the microtubules.The cyanoacrylate can then be injected into these tubules.The final coating can be applied using standard spray-painting techniques.</p>
<p>Reasoning:Polyurea provides excellent mechanical properties and durability, while cyanoacrylate polymerizes rapidly upon contact with water.The use of PLA microtubules ensures a continuous supply of the healing agent to damaged areas, allowing for multiple healing events.</p>
<p>Suggestion 17: Materials:Polyurethane-based elastomer with embedded microtubules filled with a water-reactive monomer like cyanoacrylate.The microtubules are made from a biodegradable polymer such as polylactic acid (PLA).Methods:Use a co-extrusion process to create microtubules and embed them in the polyurethane matrix.After embedding, the PLA can be dissolved to leave behind the microtubules.The cyanoacrylate can then be injected into these tubules.The final coating can be applied using standard spray-painting techniques.</p>
<p>Reasoning:Polyurethane provides excellent mechanical properties and durability, while cyanoacrylate polymerizes rapidly upon contact with water.The use of PLA microtubules ensures a continuous supply of the healing agent to damaged areas, allowing for multiple healing events.</p>
<p>Suggestion 9: Materials:Polyurethane-based elastomer with embedded microtubules filled with a water-reactive monomer like cyanoacrylate.The microtubules are made from a biodegradable polymer such as polylactic acid (PLA).Methods:Use a co-extrusion process to create microtubules and embed them in the polyurethane matrix.After embedding, the PLA can be dissolved to leave behind the microtubules.The cyanoacrylate can then be injected into these tubules.The final coating can be applied using standard spray-painting techniques.</p>
<p>Reasoning:Polyurethane provides excellent mechanical properties and durability, while cyanoacrylate polymerizes rapidly upon contact with water.The use of PLA microtubules ensures a continuous supply of the healing agent to damaged areas, allowing for multiple healing events.</p>
<p>D.2 Hypotheses Generation With feedback from Critics</p>
<p>The input prompt to the Hypotheses Generator and Critics is as shown in the Appendix D.1.</p>
<p>Summarised feedback from Summariser fed to the Hypotheses Generator for refinement Summary of Feedback from Multiple Critics on Generated Hypotheses: Common Approvals and Rejections: Approval Trends: Polyurethane matrix with cyanoacrylate and epoxy resin with TMPTA were universally approved for their scalability, effectiveness in multiple healing events, and compatibility with industrial applications like spray-painting.Fluoropolymer coatings with n-butyl acrylate-filled channels<strong> and </strong>polyurea-based elastomer with polyisocyanate-filled channels were consistently approved for their chemical resistance and suitability for harsh environments, as well as their scalable application methods. . . .Single-component healing systems triggered by water were highly favored across all suggestions.</p>
<p>For future hypothesis generation, focus on combining simple, scalable manufacturing processes with innovative material choices and healing mechanisms.Prioritize single-component, wateractivated healing agents compatible with industrial application techniques.Incorporate advanced characterization methods and additional functionalities to refine and validate the solutions robustly.Exploring hybrid systems and alternative stimuli can further enhance the effectiveness and adaptability of self-healing coatings.</p>
<p>4) The material should be compatible with scalable application techniques.</p>
<p>5) The healing mechanism should not rely on complex multi-component reactions but rather a single-component system.Relevant information from trusted Knowledge Graphs:</p>
<p>Suggested Materials: Aluminum: Aluminum is known for its corrosion resistance and is commonly used in offshore environments.</p>
<p>Graphene: Graphene has excellent barrier properties and mechanical strength, making it suitable for self-healing coatings.Epoxy: Epoxy is widely used in coatings for its strong adhesion and durability, essential for corrosion protection.Zinc: Zinc is often used in coatings for its sacrificial protection properties, which are beneficial for corrosion resistance.</p>
<p>Suggested Properties: Corrosion Resistance: Essential for protecting metallic structures in corrosive offshore environments.Barrier Properties: Important for preventing the ingress of corrosive agents like water and salts.Mechanical Property: Necessary to ensure the coating can withstand mechanical damage and maintain its integrity.Adhesions: Critical for ensuring the coating remains attached to the substrate, especially in harsh conditions.</p>
<p>TASK: Generate 20 novel and innovative suggestions that strictly satisfy the provided goal and constraints.Use the information provided from the trusted knowledge graph as a reference to generate suggestions, but aim to propose novel combinations or entirely new materials and methods.While building on existing scientific knowledge, also incorporate creative and innovative thinking to propose unique solutions that push the boundaries of current approaches.</p>
<p>Ensure that each suggestion is scientifically plausible, grounded in reasoning, and offers a fresh perspective.The ideas should have real-world potential and meet all constraints and the goal provided.Provide scientific reasoning for each suggestion.The suggestions must be in the below mentioned format in a JSON object For example: { Suggestion_1: { Materials: "Give a comprehensive and detailed description of list of materials, along with their names and properties which achieve help the goal and satisfy all constraints" Methods: "Give detailed description of novel, innovative methods or techniques needed to develop the materials suggested" Reasoning: "Give a detailed reasoning for the suggestion" }, . . .Suggestion_20: { Materials: "Give a comprehensive and detailed description of list of materials, along with their names and properties which achieve help the goal and satisfy all constraints" Methods: "Give detailed description of novel, innovative methods or techniques needed to develop the materials suggested" Reasoning:"Give a detailed reasoning for the suggestion } } 20 Hypotheses Generated by Hypotheses Generator with Knowledge Graphs Suggestion 1: Materials:A single-component polyurethane matrix combined with zinc oxide nanoparticles and tannin-derived microcapsules.The tannin-derived microcapsules offer a simple, natural healing mechanism activated by water exposure, while zinc oxide provides dual protection and healing properties.Methods:Utilize a scalable spray-painting method to apply the polyurethane coating.The zinc oxide nanoparticles enhance the barrier properties, and the tannin-derived microcapsules release their content upon contact with water, initiating self-healing.</p>
<p>Reasoning:This design leverages simplicity and sustainability by using tannin-derived agents, aligning with critics' feedback to avoid complex silane-based solutions.Zinc oxide offers excellent corrosion resistance, making it suitable for offshore environments.Evaluation: Concept Overlap (3 -Moderate Overlap): This suggestion shares several core ideas with the ground truth, such as using a water-activated silane-based healing agent and applying the coating through spray techniques.However, it differs significantly in the encapsulation method and the type of polymer matrix used.While the ground truth employs coaxial electrospinning to create core-shell nanofibers, this suggestion uses microcapsules dispersed in a polyurea matrix.The difference in the delivery system (nanofibers vs. microcapsules) marks a critical divergence in methods.</p>
<p>Property Overlap (3 -Moderately Similar): Both coatings aim for immediate self-healing upon exposure to water and provide corrosion protection.The use of a silane-based healing agent is a common property.However, the suggestion lacks specific quantitative properties like the 97.5% recovery of protective properties mentioned in the ground truth.Additionally, the mechanical properties may differ due to the use of polyurea instead of nanofiber structures.Keyword Matching (3 -Partial Match): The suggestion matches several important keywords such as "silane-based healing agent," "water-activated," and "spray application."However, it does not include critical terms from the ground truth like "coaxial electrospinning," "core-shell nanofibers," or "organosilane compound (silyl ester)."</p>
<p>Suggestion 7: Materials: Design a coating using a poly(dimethylsiloxane) (PDMS) matrix with embedded microcapsules containing a water-sensitive alkoxy silane healing agent.Methods: Integrate microencapsulation techniques to uniformly disperse capsules within the PDMS matrix.Use spray application for even distribution over large surfaces.Reasoning: PDMS is known for its flexibility and hydrophobicity, providing a durable base.The alkoxy silane reacts with water to form a protective siloxane layer, ensuring multiple healing cycles and robust corrosion resistance.</p>
<p>Evaluation: Concept Overlap (4 -High Overlap): This suggestion closely aligns with the ground truth by utilizing a silane-based healing agent that reacts with water to initiate self-healing.Both methods involve embedding the healing agent within a polymer matrix and applying the coating via spray techniques.The primary difference lies in the use of microcapsules within a PDMS matrix instead of core-shell nanofibers created by coaxial electrospinning.Property Overlap (4 -Highly Similar): Both coatings are designed to self-heal upon exposure to water and offer corrosion resistance.The suggestion's use of PDMS provides flexibility and durability, which are advantageous properties similar to those achieved with nanofiber coatings.While specific quantitative recovery rates are not mentioned, the focus on multiple healing cycles and robust protection indicates a strong alignment with the ground truth properties.</p>
<p>Keyword Matching (4 -High Match): The suggestion matches most critical keywords, including "silane healing agent," "water-sensitive," "spray application," "self-healing," and "corrosion resistance."Minor discrepancies include the absence of terms like "coaxial electrospinning" and "core-shell nanofibers," but the overall keyword alignment is strong.</p>
<p>Feasibility and Scalability</p>
<p>Not</p>
<p>Feasible: Cannot be implemented with current technology or resources.</p>
<p>Slightly Feasible: Theoretically possible but requires breakthroughs or high investment.</p>
<p>Moderately Feasible:</p>
<p>Can be implemented on a small scale, but larger-scale production faces challenges.</p>
<p>Highly Feasible: Practical for small and moderate scales with manageable adjustments for scale-up.</p>
<p>Completely Feasible: Easily implemented and scaled from lab to industry with minimal changes or investment.</p>
<p>Impact Potential</p>
<p>No Impact: Unlikely to contribute meaningfully to the field.</p>
<p>Low Impact: Offers minor contributions but lacks significant influence.</p>
<p>Moderate Impact: Can contribute positively in specific areas.</p>
<p>High</p>
<p>Impact: Likely to drive significant advancements or address important challenges.</p>
<p>Transformative Impact: Capable of revolutionizing the field or providing solutions to major problems.</p>
<p>Table 6: Evaluation metrics and scales for assessing generated hypotheses based on alignment with research objectives, scientific plausibility, innovation, testability, feasibility, and impact potential.</p>
<p>; Merchant et al. (2023); Court et al. (2020); Xie et al. (2021); Ren et al. (2022); Fung et al. (2022); Long et al. (</p>
<p>Figure 2 :
2
Figure2: The left plot illustrates the Closeness metric scores across three evaluation criteria for the three configurations.The right plot depicts the Quality metric scores across six evaluation criteria for the same configurations.Both plots highlight that integrating feedback from Critic Agents and leveraging contextual knowledge from the Knowledge Graph enhances performance.</p>
<p>(2024b), LLaMA-3.1 70B-Instruct, and Mixtral-8x22B-Instruct-v0.1 MistralAI (2024) served as the three Critic Agents.LLaMA-3.1 70B-Instruct was employed as the Summarizer Agent, and OpenAI-o1-preview was used as the Evaluation Agent.As shown in Figure3, closed source models consistently outperformed their open source counterparts in both closeness and quality metrics.</p>
<p>Figure 3 :
3
Figure 3: The left plot compares the performance of open-source and closed-source models on the Closeness metric, while the right plot compares their Quality scores.Both plots clearly show that closed-source models outperform open-source models significantly.</p>
<p>Table 1 :
1
Comparison of existing LLM agent frameworks with our framework, ACCELMAT, for materials design and discovery hypotheses generation.Our framework covers a wide range of materials and properties while being independent of domain-specific tools.
LLM AgentDiverse Mat Diverse Prop Tool FreeLLMatDesign✓×✓ChemReasoner×✓×SciAgents×✓✓ACCELMAT✓✓✓DatasetMaScQA ChemLLMBench MATDESIGNReal World Constr.××✓Mat. Design Prob.××✓No Data Leakage××✓Difficulty LevelGraduateResearchResearch</p>
<p>Table 2 :
2
Comparison of existing benchmarks to ours-MATDESIGN.Our benchmark provides real-world constraints and goals.It is not present in the pre-training of LLMs and contains more complex problems.</p>
<p>Table 3 :
3
An instance from our dataset, MATDESIGN, extracted from the paper bySpera et al. (2024).The Goal and Constraints are provided as the input to our designed framework.Material Name and Methods are used as ground truth for evaluation.</p>
<p>playing a critical role in the iterative process of hypothesis generation and refinement.Since our aim is to aid material scientists by generating novel and viable hypotheses, we use state-of-the-art proprietary LLMs.Performance using open-source LLMs is provided in Appendix A.1.Hypotheses Generation Agent (HGA):Given the Goal Statement and Constraints, the HGA generates multiple hypotheses, accompanied by reasoning for each.This agent is powered by GPT-4o.The prompts used for HGA are provided in Appendix C.3.</p>
<p>Table 4 :
4
Evaluation Metrics Scores for Closeness between Generated Hypotheses and Ground Truth</p>
<p>Table 5 ,
5
incorporating critic feedback and grounded knowledge from MatKG increased consensus among critics, with an average 19 out of 20 generated suggestions reaching agreement after consecutive feedback cycles.This demonstrates the effectiveness of the critic feedback system in maximizing viable hypothesis generation.
ACCELMAT Framework Type Avg No. of HypothesesAgreed by Critics / 20Without FB11With FB18With FB and KG19</p>
<p>Table 5 :
5
Comparison of the average number of hypotheses agreed upon by all critics (out of 20) across different LLM-based agentic frameworks.Incorporating knowledge graphs and critic feedback maximizes the generation of viable hypotheses.</p>
<p>Meets the goal statement and satisfies all constraints strictly:NO.Reasoning: This suggestion fails to meet constraint 2 as it does not provide a mechanism for multiple healing events or a vascular-like structure for continuous supply of the healing agent.
Overall Feedback for future suggestion improvement:Focus on developing vascular-like structures or continuous supply mechanisms to allow formultiple healing eventsEnsure all suggestions explicitly address each constraint, particularly the requirement for multiplehealing cyclesExplore innovative ways to integrate vascular structures with spray-painting techniques forlarge-scale applicationConsider combining successful elements from different suggestions, such as microtubules orvascular networks with various healing agentsInvestigate methods to create self-replenishing systems within the coating that are compatible withspray applicationCritic 1 GPT-4o feedbackFeedback for suggestion 1: Critic 3 Gemini 1.5 flash feedbackMeets the goal statement and satisfies all constraints strictly:YES. Feedback for suggestion 1:Feedback for suggestion 20:Meets the goal statement and satisfies all constraints strictly:NO.Reasoning: Hybrid polymer matrices and nanocapsules introduce formulation complexity.Ensuring uniform nanocapsule distribution and multiple healing events in a scalable manner ischallenging.Overall Feedback for future suggestion improvement:Focus on single-component healing agents activated by water.Ensure multiple healing events through vascular-like structures or microtubulesAvoid complex multi-step synthesis processes.Ensure compatibility with scalable application techniques like spray-paintingAddress uniform distribution of self-healing agents within the polymer matrix.Critic 2 Claude 3.5 Sonnet feedbackFeedback for suggestion 1:Meets the goal statement and satisfies all constraints strictly:YES.Reasoning: This suggestion meets all constraints. It uses water as a trigger, allows formultiple healing events through microtubules, maintains structural integrity, is compatible withspray-painting, and uses a single-component healing system.. . .Feedback for suggestion 20:
Reasoning: Polyurea-based elastomer with embedded microtubules filled with cyanoacrylate offers a scalable and robust self-healing mechanism triggered by water.The co-extrusion process ensures a continuous supply of the healing agent, and the application is compatible with spray-painting. . . .Meets the goal statement and satisfies all constraints strictly:YES.</p>
<p>Polysiloxane matrix with embedded graphene oxide and tannin-derived microcapsules.Methods:Create sprayable polysiloxane coatings where graphene oxide enhances barrier properties.Tannin-derived microcapsules release healing agents upon water exposure.Reasoning:This suggestion uses natural healing agents and graphene oxide for enhanced performance, aligning with simplicity and environmental considerations.Prompt for Closeness MeasurementYou will be provided with a number of suggested hypotheses which you have to rate by comparing them with the ground truth and based on your rating you have to rank from most rated to lowest rated.The ground-truth has been provided through real-world scientific experimentation to achieve the goal statement below.For the rating and ranking you have to provide detailed reasoning.You have to rank each suggested hypothesis on an individual basis; do not club the ratings of multiple hypotheses together.Below are the rules for rating and ranking.The generated hypothesis contains entirely different concepts with no connection to the core ideas or methods in the ground truth hypothesis. 2 -Minimal Overlap: A few general or broad ideas may be similar, but most key concepts or methods are either missing or significantly different.3-ModerateOverlap:Several core ideas and methods overlap with the ground truth, but some critical concepts are either missing or misrepresented.4-HighOverlap:Most of the core concepts and methods in the generated hypothesis match well with the ground truth, with only minor differences.5 -Complete Overlap: The generated hypothesis fully mirrors the core ideas, methods, and scientific concepts in the ground truth hypothesis.None of the keywords or entities in the generated hypothesis match those in the ground truth.There is a complete divergence in terms of critical terms and concepts.2-MinimalMatch:A small number of non-critical keywords or entities match, but most key terms are either missing or incorrect.3-PartialMatch:Several important keywords and entities match, though some are either missing or not fully aligned.4-HighMatch: Most critical keywords and entities match, with only a few minor discrepancies.5 -Complete Match: All key keywords and entities in the generated hypothesis align exactly with the ground truth.Overcoming scalability challenges with spray painting: A viable spray-painting technique to apply the nanofiber-based coating, making it scalable for large structures.Spray painting ability was achieved by prior dispersion of the nanofibres.Incorporation of water-reactive organosilanes: Water-reactive organosilane (silyl ester) was incorporated as the healing agent within the core of the nanofibers, allowing the coating to heal upon exposure to water without requiring any additional catalysts.Prompt for Quality MeasurementYou will be provided with a number of suggested hypotheses which you have to rate by assesing their quality based on 6 criterias defined below.And based on your rating you have to rank from most rated to lowest rated.For the rating and ranking you have to provide detailed reasoning.You have to rank each suggested hypothesis on an individual basis; do not club the ratings of multiple hypotheses together.Below are the rules for rating and ranking.The hypothesis does not relate to the re-search objectives in any meaningful way and ignores or violates specified constraints.2 -Slightly Aligned: The hypothesis touches on the general topic but misses key objectives or questions.It adheres to some constraints but overlooks or violates others.3 -Moderately Aligned: The hypothesis addresses some aspects of the research objectives but lacks focus on critical elements.It meets most constraints but may miss minor ones or incorporate them incorrectly.4 -Highly Aligned: The hypothesis effectively addresses most research objectives and questions.It complies with all major constraints and key points, with only minor omis-sions.5 -Fully Aligned: The hypothesis directly and comprehensively addresses all key objectives and questions of the research goals.It completely adheres to all specified constraints and fully incorporates all key points and clues.Evaluates how easily and effectively the hy-pothesis can be tested experimentally, considering the avail-ability of required techniques, equipment, and resources in materials science.
Suggestion 2: Materials:Epoxy matrix with embedded graphene oxide and water-responsive natural polymers, such as chitosan. Graphene oxide provides excellent barrier properties, while chitosan acts as a natural, biodegradable healing agent. Methods:Develop a sprayable epoxy coating where graphene oxide enhances mechanical strength. Chitosan microcapsules release upon water contact, enabling multiple healing cycles. Reasoning:Graphene oxide enhances both mechanical and barrier properties. The use of chitosan aligns with sustainability goals, offering a biodegradable option that critics favor. . . . Suggestion 20: 4) Testability: Definition: Scale: 1 -Not Testable: The hypothesis cannot be tested with current or foreseeable experimental techniques. F Evaluated Hypotheses by OpenAI-o1-preview Hypotheses Evaluated with Closeness Metric Suggestion 6: Materials: Incorporate a polyurea coating with dispersed microcapsules containing a water-activated healing agent such as a silane-based compound. Materials:E Evaluation Prompts for OpenAI-o1-preview Rules: 1) Concept Overlap Scale: Definition: This scale evaluates how well the core ideas, methods, and scientific concepts in the generated hypothesis align with those in the ground truth hypothesis. Scale: 1 -No Overlap: 2) Property Overlap: Definition: This scale assesses how closely the material properties described in the generated hypothesis align with those in the ground truth, including quantitative values and qualitative descriptions. Scale: 1 -Not Similar: The generated hypothesis has no overlapping or similar properties to the ground truth hypothesis. Properties are either missing or completely different. 2 -Slightly Similar: A few properties may match, but most are different in magnitude or type. Qualitative descriptions may vary significantly. 3 -Moderately Similar: Some important properties are similar, though others differ in specific values or qualitative descriptions. 4 -Highly Similar: Most of the key properties align well with the ground truth, with only minor differences in values or descriptions. 5 -Perfect Match: The generated hypothesis fully matches the material properties of the ground truth, including both quantitative and qualitative aspects. 3) Keyword Matching: Definition: This scale evaluates how well specific entities (e.g., material names, chemical compounds, synthesis methods) and keywords in the generated hypothesis match those in the ground truth. Scale: Methods: Coaxial electrospinning of core-shell nanofibers: The self-healing material was developed using coaxial electrospinning to create core-shell nanofibers with an organosilane compound (silyl ester) as the healing agent. Scale: 1 -Misaligned: 2) Scientific Plausibility: Definition: Assesses whether the hypothesis is grounded in established scientific principles and theories within material science. Scale: 1 -Not Plausible: Contradicts fundamental scientific laws or principles. 2 -Slightly Plausible: Contains significant scientific inaccuracies or unsupported assumptions. 3 -Moderately Plausible: Generally scientifically sound but includes minor inaccuracies or speculative elements. 4 -Highly Plausible: Scientifically accurate and well-supported by current theories with negligible issues. 5 -Completely Plausible: Fully consistent with established scientific knowledge and principles, with strong theoretical support. 3) Innovation and Novelty: Definition: Measures the degree to which the hypothesis introduces original ideas, perspectives, or methods not previously documented. Scale: 1 -Not Innovative: Restates existing knowledge without introducing new ideas. 2 -Slightly Innovative: Provides minimal new insights or slight variations on known concepts. 3 -Moderately Innovative: Offers some original ideas or novel combinations of existing concepts. 4 -Highly Innovative: Introduces significant new ideas or approaches that could advance the field. 5 -Exceptionally Innovative: Presents groundbreaking ideas or methodologies with the potential to revolutionize the field procedures. 5 -Highly Testable: Allows for rapid, cost effective experimental validation with readily available resources and techniques. 5) Feasibility and Scalability: Definition: Evaluates the practicality of implementing the hypothesis at various scales, from laboratory to industrial applications, focusing on existing infrastructure, technology, cost, and effort required to scale the process. Scale: 1 -Not Feasible: Cannot be implemented with current technology, materials, or resources, or faces insurmountable obstacles in scale-up. 2 -Slightly Feasible: Implementation is theoretically possible but would require breakthroughs or prohibitively high investment in materials, equipment, or processes. Scaling up significantly increases the complexity and costs. 3 -Moderately Feasible: Can be implemented on a small scale (e.g., lab setting), but larger-scale production faces major challenges, such as cost, resource availability, or technological limitations. 4 -Highly Feasible: Practical to implement with current technology, materials, and methods on both small and moderate scales. Scaling up to industrial production would require some adjustments but is manageable without substantial redesign or new resources. 5 -Completely Feasible: Easily implemented and scaled from lab to industrial scale with minimal changes or additional investment. Resources, techniques, and materials are widely available, and the process is cost-effective at all scales. 6) Impact Potential: Definition: Assesses the hypothesis's potential to significantly advance the field or address critical challenges in material science. Scale: 1 -No Impact: Unlikely to contribute meaningfully to the field. 2 -Low Impact: May offer minor contributions but lacks significant influence. 3 -Moderate Impact: Has the potential to contribute positively in specific areas. 4 -High Impact: Likely to drive significant advancements or address important challenges. 5 -Transformative Impact: Capable of revolutionizing the field or providing solutions to major problems 1 -No Match: The ground truth is: Materials: Core-shell nanofibers synthesized using coaxial electrospinning. Organosilane compounds, specifically silyl esters, used as the self-healing agent. Metallic substrates (e.g., steel) for corrosion testing. 2 -Difficult to Test: Testing requires highly specialized, rare, or prohibitively expensive equipment Methods: Utilize in-situ polymerization to encapsulate the healing agent within a polyurea matrix. Rules: and resources. Apply using high-pressure spray techniques suitable for polyurea coatings. 1) Alignment with Research Objectives and Constraints: 3 -Moderately Testable: Can be tested with available techniques, but requires complex procedures Reasoning: Polyurea provides excellent chemical and abrasion resistance, ideal for offshore Definition: Assesses how directly and effectively the generated hypothesis addresses the objectives or significant resources. environments. The water-activated silane-based healing agent allows for immediate response to specified in the goal statement while adhering to all given constraints and incorporating provided key points or clues. 4 -Easily Testable: Can be tested using common laboratory equipment and straightforward damage, ensuring continuous protection.</p>
<p>Suggestion 7 exhibits high overlap in concepts, properties, and keywords.It closely mirrors the ground truth's approach by using a silane-based healing agent activated by water, embedded within a polymer matrix, and applied via spray techniques.The minor differences do not significantly detract from the overall alignment.Suggestion 6 Reasoning: Suggestion 6 shows moderate overlap in all categories.It shares the use of a silane-based healing agent and spray application methods but differs in the encapsulation technique and the polymer matrix used.The core concepts are somewhat aligned but lack the depth of similarity seen in Suggestion 7.Suggestion 1 Reasoning: Suggestion 1 has minimal overlap with the ground truth.The healing agent and encapsulation methods are different, and there is a lack of critical keyword alignment.While it shares the broad idea of a water-activated self-healing coating, the significant differences in materials and methods place it lowest in the ranking.
Suggestion 1:. . .==================================================================== Ranking from Most to Least Aligned with the Ground Truth: Suggestion 7 4 5 Alignment with Research Ob-jectives and Misaligned: Hy-pothesis does not relate to objectives Slightly Aligned: Hypothesis touches on the topic but Moderately Aligned: Ad-dresses some Highly Aligned: Addresses most objectives and Fully Aligned: Di-rectly addresses all key objectives and Reasoning: 3 Constraints and violates con-misses key objec-objectives but lacks complies with fully adheres to allstraints.tives or violatesfocus on criticalmajor constraints.constraints.some constraints.elements;meetsmost constraints.Scientific Plausi-Not Plausible: Con-Slightly Plausible:Moderately Plausi-Highly Plausible:Completely Plau-bilitytradicts fundamen-Contains significantble: ScientificallyScientifically accu-sible: Fully con-tal scientific princi-inaccuracies or un-sound but includesrate with negligiblesistent with estab-ples.supported assump-minor inaccuraciesissues.lished knowledgetions.or speculative ele-and well-supportedments.by theory.Innovation andNotInnovative:Slightly Innovative:Moderately Innova-Highly Innovative:Exceptionally In-NoveltyRestates existingProvides minimaltive: Offers someIntroduces signifi-novative: Ground-knowledge withoutnew insights ororiginal ideas orcant new ideas orbreaking ideas orintroducing newslight variations onnovel combinationsapproaches.methodologies thatideas.known concepts.of existing con-could revolutionizecepts.the field.TestabilityNot Testable: Can-Difficult to Test:ModeratelyEasilyTestable:Highly Testable:not be tested withRequiresspe-Testable:CanTestableusingRapid,cost-current techniques.cialized or pro-be tested with avail-common equipmenteffective validationhibitively expensiveable techniques butand straightforwardwith readily avail-equipment.requires complexprocedures.able resources andprocedures.techniques.
 and GPT-4o (Documentation) have knowledge cutoffs of December 2023 and October 2023 respectively.
AcknowledgementWe thank the anonymous reviewers for their constructive suggestions.We extend our gratitude to the Research Computing (RC), and Enterprise Technology at Arizona State University for providing computing resources, and access to the Chat-GPT enterprise version for experiments.This research was supported by the Engineering Research and Development Center -Information Technology Laboratory (ERDC-ITL) under Contract No. W912HZ24C0022.Any opinions, findings and conclusions or recommendations expressed in this work are those of the author(s) and do not necessarily reflect the views of the ERDC-ITL.Hypotheses Evaluated with Quality Metric Suggestion 1 Materials: Polyurethane matrix with cyanoacrylate-filled vascular-like channels.Methods: 3D print a network of channels within the polyurethane matrix using a sacrificial material like PVA.Dissolve the PVA post-formation to leave hollow channels and inject cyanoacrylate.Apply the final coating using spray-painting techniques.Reasoning: Polyurethane provides durability while cyanoacrylate offers a rapid water-activated healing mechanism.The vascular-like structure allows multiple healing events, and the method is compatible with scalable techniques like spray-painting.Evaluation:Alignment with Research Objectives and Constraints: Score: 3 (Moderately Aligned) Reasoning: The hypothesis addresses the objective of creating a self-healing coating with water-activated mechanisms.However, incorporating vascular-like channels via 3D printing may not be fully compatible with spray-painting techniques, which is a specified constraint.The use of 3D printing and sacrificial materials could complicate scalability and industrial application.Scientific Plausibility: Score: 4 (Highly Plausible) Reasoning: The use of polyurethane and cyanoacrylate is scientifically sound, and the concept of vascular networks for self-healing is established in materials science.Minor challenges may exist in integrating 3D printed channels with spray-painting, but overall, the hypothesis is plausible.Innovation and Novelty: Score: 4 (Highly Innovative) Reasoning: Combining vascular networks with spray-paintable coatings introduces a novel approach to self-healing materials.The integration of 3D printing techniques in creating self-healing coatings is a significant advancement.Testability: Score: 2 (Difficult to Test) Reasoning: Testing this hypothesis requires specialized equipment like 3D printers capable of creating vascular networks within polymers and handling sacrificial materials.This complexity makes experimental validation more challenging and resource-intensive.Feasibility and Scalability: Score: 2 (Slightly Feasible) Reasoning: Implementing vascular networks within coatings on an industrial scale is challenging.The process involves complex steps that are not easily scalable with current spray-painting technologies.Scaling up would require significant adjustments and investment.Impact Potential: Score: 4 (High Impact) Reasoning: If successfully implemented, this approach could significantly advance self-healing coatings by enabling multiple healing events and improving longevity, addressing critical challenges in corrosion protection.Suggestion 2 Materials: Polyurea-based elastomer with polyisocyanate-filled vascular-like channels.Methods: Use 3D printing to create a network of channels within the polyurea matrix using a sacrificial material.Dissolve the sacrificial material to leave the channels, then inject polyisocyanate.Apply using spray-painting techniques.Reasoning: Polyurea and polyisocyanate are effective for corrosion protection.The vascular-like structure ensures multiple healing events, and the process is scalable.Evaluation:Alignment with Research Objectives and Constraints: Score: 3 (Moderately Aligned) . . .=================================================================== Ranking from Highest to Lowest RatedBased on the evaluations, the suggestions can be ranked according to their overall scores:Suggestions 4,5,6,7,9,11,13,15,17,18,19,20 (Tied for Highest Rating)Reasoning: These suggestions scored consistently high across all criteria, fully aligning with research objectives and constraints, being scientifically plausible, highly testable, completely feasible, and having high impact potential.Suggestions 10 and 16 (Next Highest Rating) Reasoning: These scored slightly lower in testability and feasibility due to minor challenges with fluoropolymer processing but still rated highly overall.Suggestions 1, 2, 3, 8, 12, 14 (Lower Rating) Reasoning: These suggestions, while innovative, faced challenges in alignment due to the use of vascular-like channels which complicate compatibility with spray-painting techniques, resulting in lower scores for testability and feasibility.G Closeness Evaluation with BERTScoreTo evaluate the similarity between the generated hypotheses and the ground truth hypotheses, we computed BERTScore Zhang et al. (2019) with the MaterialsBERT model(Shetty et al., 2023).The calculated scores for the proposed methods are as follows:• Hypotheses Generation Without Feedback: 52.66%• Hypotheses Generation With Feedback: 60.59%• Hypotheses Generation With Knowledge Graph and Feedback: 50.30%However, these scores do not align with the evaluations conducted by LLM-based methods or human experts in our study where Hypotheses Generation With Knowledge Graph and Feedback method achieves the best scores.Moreover, interpreting these scores within the context of our task poses challenges.For example, while there is a 7.93% improvement between the first two methods, it is unclear how this translates into factors such as the alignment of the mentioned properties, or the similarity of the methods described.This lack of interpretability makes BERTScore less effective for meaningfully assessing the performance of our methods in this domain.
Claude sonnet. Anthropic, 2024</p>
<p>Kunyang Joseph M Cavanagh, Andrew Sun, Dorian Gritsevskiy, Thomas D Bagni, Teresa Bannister, Head-Gordon, arXiv:2409.02231Smileyllama: Modifying large language models for directed chemical space exploration. 2024arXiv preprint</p>
<p>A universal graph deep learning interatomic potential for the periodic table. Chi Chen, Ping Shyue, Ong, Nature Computational Science. 2112022</p>
<p>Scientific paper extractive summarization enhanced by citation graphs. Xiuying Chen, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, Xiangliang Zhang, arXiv:2212.042142022arXiv preprint</p>
<p>Accelerating materials language processing with large language models. Jaewoong Choi, Byungju Lee, Communications Materials. 51132024</p>
<p>3-d inorganic crystal structure generation and property prediction via representation learning. J Callum, Batuhan Court, Apoorv Yildirim, Jacqueline M Jain, Cole, Journal of Chemical Information and Modeling. 60102020</p>
<p>Computational screening of all stoichiometric inorganic materials. Keith T Daniel W Davies, Adam J Butler, Andrew Jackson, Jarvist M Morris, Jonathan M Frost, Aron Skelton, Walsh, Chem. 142016</p>            </div>
        </div>

    </div>
</body>
</html>
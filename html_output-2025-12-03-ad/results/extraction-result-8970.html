<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8970 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8970</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8970</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-7bd2ab0c21134bd9d389d04765998f356779dfb0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7bd2ab0c21134bd9d389d04765998f356779dfb0" target="_blank">AMR-to-text Generation with Synchronous Node Replacement Grammar</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar by leveragingynchronous node Replacement grammar and gives the state-of-the-art result.</p>
                <p><strong>Paper Abstract:</strong> This paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark, our method gives the state-of-the-art result.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8970.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8970.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SNRG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synchronous Node Replacement Grammar</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synchronous graph-to-string rewriting formalism used to learn hierarchical graph-to-text rules mapping rooted AMR fragments to target phrases; at test time a graph transducer collapses AMR subgraphs according to learned productions to generate sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AMR-to-text Generation with Synchronous Node Replacement Grammar</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Synchronous Node Replacement Grammar (SNRG)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>A synchronous NRG defines productions X_i → ((F, E), ~) where F is a rooted, connected AMR fragment (nodes labelled by concepts/NTs and edges by AMR edge labels) and E is the corresponding target string possibly containing nonterminals; a single nonterminal type X is used with subscripts to distinguish instances. Rules include induced (learned) rules, concept rules (single-node verbalizations), and graph glue rules (concatenation patterns for connected nonterminals).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) graphs — rooted, directed semantic graphs with concept nodes and labeled edges (e.g., ARG0, ARG1).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Training: extract initial phrase-to-graph-fragment rules from aligned (sentence, AMR, alignment) pairs using a phrase-to-graph-fragment extraction algorithm; generate induced rules by collapsing subgraph matches (Algorithm 1) to introduce nonterminals; add concept and glue rules. Testing: apply a graph transducer / bottom-up collapse decoder with beam search that repeatedly replaces matched AMR fragments F with their E strings (or nonterminals) according to learned productions, using a log-linear model (translation probabilities, lexicalized probabilities, LM score, reordering model for glue rules, moving-distance feature) and tuned weights (MERT).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (surface realization of AMR graphs to natural language sentences).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU (on LDC2015E86): All (full SNRG system) = 25.62 (test), 25.24 (dev). Ablation: NoInducedRule = 17.43 (test), NoConceptRule = 24.86 (test), NoMovingDistance = 24.06 (test), NoReorderModel = 25.43 (test). Rule usage on 1-best outputs: Glue 30.0%, Nonterminal 30.1%, Terminal 39.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms prior AMR-to-text methods on same dataset: JAMR-gen (tree-based spanning-tree → tree-to-string) = 23.00 BLEU, TSP-gen (graph-fragment + TSP decoding) = 22.44 BLEU. Paper argues SNRG retains hierarchical correspondences without incurring graph-to-tree information loss (a problem for Flanigan et al.'s spanning-tree approach) and models hierarchies missing from TSP-gen.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Learns hierarchical graph↔string correspondences (captures structure beyond flat fragment sequencing), avoids irreversible graph-to-tree transformation (reducing information loss and error propagation), achieves state-of-the-art BLEU on LDC2015E86, supports guaranteed derivations via concept and glue rules, includes lexicalized probabilities and reordering/moving-distance features to improve fluency and local ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Graph grammar data sparsity — many glue rules are required (≈30% of rules used are glue), induced rules can be small and sparse, requires aligned (sentence, AMR, word-to-AMR) training data, some rule probabilities (concept/glue) are manually set low (0.0001), decoding is more complex (graph matching and beam search).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported generation errors in examples: incorrect small-word choices (e.g., 'that' vs 'what') and missing prepositions (missing 'in' between entities). Ablation shows system fails badly without induced hierarchical rules (BLEU drop to ~17.4), indicating reliance on induced rules and potential brittleness when induced coverage is low.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR-to-text Generation with Synchronous Node Replacement Grammar', 'publication_date_yy_mm': '2017-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8970.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8970.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spanning-tree → tree-to-string</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-spanning-tree then tree-to-string transducer (JAMR-gen)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step pipeline that first converts an AMR graph into a spanning tree and then uses a tree-to-string transducer to map the tree to a sentence, leveraging MT tree-based techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generation from abstract meaning representation using tree transducers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Spanning-tree + Tree-to-string transducer</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert the input AMR graph into a spanning tree (a projective tree covering graph nodes/edges), then apply a tree-to-string hierarchical transducer (learned from tree–string pairs) to produce the surface string. This leverages existing hierarchical SMT/tree transducer machinery.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (converted into spanning trees before generation).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph-to-tree: derive a spanning tree from the AMR graph (graph→spanning-tree transformation). Tree-to-string: apply a learned tree-to-string transducer using tree-based hierarchical alignment methods to generate sentence text.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported BLEU in this paper for JAMR-gen on LDC2015E86 = 23.00 (test).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared in this paper: SNRG (this work) outperforms JAMR-gen by ~2.6 BLEU points (25.62 vs 23.00). Paper notes JAMR-gen suffers from error propagation and irreversible information loss due to graph→tree transformation and the projective correspondence constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages mature tree-to-string MT technology; captures hierarchical correspondences when operating on trees.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Graph→tree conversion can cause information loss since AMR encodes relations that may not be representable projectively in a single tree; error propagation from tree conversion step cannot be recovered in generation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Information loss and constrained generation due to spanning-tree conversion; projectivity constraints lead to outputs that cannot realize all graph information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR-to-text Generation with Synchronous Node Replacement Grammar', 'publication_date_yy_mm': '2017-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8970.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8970.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TSP-gen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR-to-text via graph-fragment-to-string rules cast as a Traveling Salesman Problem</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that extracts graph-fragment-to-string rules and casts finding a sequence of disjoint rule applications to cover the AMR as a Traveling Salesman Problem, ranking candidate sentences with local features and a language model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AMR-to-text generation as a traveling salesman problem</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-fragment-to-string (TSP-based sequencing)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent the AMR as a set of graph-fragment → phrase rules (flat fragments with terminal output). The conversion chooses a sequence of disjoint fragment rules to cover the graph; this sequencing and ordering problem is formulated as a TSP and solved to produce a linearized sentence candidate, scored with local features and a language model.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (represented as disconnected or disjoint fragments mapped to phrases).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Extract fragment→phrase mappings; search for an ordering/selection of fragments that covers the AMR (formulated as a TSP), then concatenate the mapped phrases according to the solved tour and rerank using LM and features.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported BLEU in this paper for TSP-gen on LDC2015E86 = 22.44 (test).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared in this paper: SNRG outperforms TSP-gen by ~3.2 BLEU points. Paper argues TSP-gen does not learn hierarchical structural correspondences between AMR graphs and strings, which SNRG does.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly operates on graph fragments; avoids committing to a spanning-tree; can leverage global optimization (TSP) to choose fragment order.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Does not capture hierarchical correspondences (no explicit hierarchical rules), potentially limiting modeling of nested or hierarchical dependencies; lower BLEU relative to SNRG in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Lacks hierarchical structural modeling; performance lower than hierarchical graph-to-string grammar approaches when measured by BLEU on benchmark dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR-to-text Generation with Synchronous Node Replacement Grammar', 'publication_date_yy_mm': '2017-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8970.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8970.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phrase→graph-fragment extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phrase-to-graph-fragment extraction algorithm (Peng et al., 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extraction algorithm used to obtain initial terminal graph-fragment↔phrase rule pairs from aligned (sentence, AMR, word–AMR alignment) training data; these initial rules are input to induced rule generation (collapsing) for SNRG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A synchronous hyperedge replacement grammar based approach for AMR parsing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Phrase-to-graph-fragment extraction</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Heuristic algorithm that extracts aligned phrase ↔ AMR-fragment pairs from aligned corpora; initial extracted rules contain only terminals (no nonterminals) and are later combined/collapsed to create induced hierarchical rules with nonterminals.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (fragments of AMR graphs corresponding to phrases).</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Given (sentence, AMR, alignment) pairs, extract initial rules mapping contiguous target phrases to connected AMR fragments using a phrase-to-graph-fragment extraction procedure; then match and collapse pairs of initial rules where one contains the other to introduce nonterminals, producing induced rules.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Used as the rule extraction/training step for SNRG-based AMR-to-text generation (training-time conversion / rule induction).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported standalone in this paper; used as the basis for induced-rule creation that yields final BLEU=25.62 for the SNRG system.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>This extraction method is the source of initial rules for SNRG; it is cited from Peng et al. (2015). No direct empirical comparison of extraction algorithms is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Provides grounded initial fragment↔phrase mappings from real aligned data, enabling later creation of hierarchical induced rules by collapsing contained fragments.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Initial rules are terminal-only and must be collapsed to obtain hierarchical nonterminal rules; quality depends on alignment quality (heuristic aligner used); may contribute to data sparsity of graph grammars.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If alignments are poor or if fragments are not found, induced hierarchical rules may be sparse, forcing reliance on glue rules or concept rules with manually set low probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR-to-text Generation with Synchronous Node Replacement Grammar', 'publication_date_yy_mm': '2017-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generation from abstract meaning representation using tree transducers <em>(Rating: 2)</em></li>
                <li>AMR-to-text generation as a traveling salesman problem <em>(Rating: 2)</em></li>
                <li>A synchronous hyperedge replacement grammar based approach for AMR parsing <em>(Rating: 2)</em></li>
                <li>Generating English from abstract meaning representations <em>(Rating: 1)</em></li>
                <li>Addressing the data sparsity issue in neural amr parsing <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8970",
    "paper_id": "paper-7bd2ab0c21134bd9d389d04765998f356779dfb0",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "SNRG",
            "name_full": "Synchronous Node Replacement Grammar",
            "brief_description": "A synchronous graph-to-string rewriting formalism used to learn hierarchical graph-to-text rules mapping rooted AMR fragments to target phrases; at test time a graph transducer collapses AMR subgraphs according to learned productions to generate sentences.",
            "citation_title": "AMR-to-text Generation with Synchronous Node Replacement Grammar",
            "mention_or_use": "use",
            "representation_name": "Synchronous Node Replacement Grammar (SNRG)",
            "representation_description": "A synchronous NRG defines productions X_i → ((F, E), ~) where F is a rooted, connected AMR fragment (nodes labelled by concepts/NTs and edges by AMR edge labels) and E is the corresponding target string possibly containing nonterminals; a single nonterminal type X is used with subscripts to distinguish instances. Rules include induced (learned) rules, concept rules (single-node verbalizations), and graph glue rules (concatenation patterns for connected nonterminals).",
            "graph_type": "Abstract Meaning Representation (AMR) graphs — rooted, directed semantic graphs with concept nodes and labeled edges (e.g., ARG0, ARG1).",
            "conversion_method": "Training: extract initial phrase-to-graph-fragment rules from aligned (sentence, AMR, alignment) pairs using a phrase-to-graph-fragment extraction algorithm; generate induced rules by collapsing subgraph matches (Algorithm 1) to introduce nonterminals; add concept and glue rules. Testing: apply a graph transducer / bottom-up collapse decoder with beam search that repeatedly replaces matched AMR fragments F with their E strings (or nonterminals) according to learned productions, using a log-linear model (translation probabilities, lexicalized probabilities, LM score, reordering model for glue rules, moving-distance feature) and tuned weights (MERT).",
            "downstream_task": "AMR-to-text generation (surface realization of AMR graphs to natural language sentences).",
            "performance_metrics": "BLEU (on LDC2015E86): All (full SNRG system) = 25.62 (test), 25.24 (dev). Ablation: NoInducedRule = 17.43 (test), NoConceptRule = 24.86 (test), NoMovingDistance = 24.06 (test), NoReorderModel = 25.43 (test). Rule usage on 1-best outputs: Glue 30.0%, Nonterminal 30.1%, Terminal 39.9%.",
            "comparison_to_others": "Outperforms prior AMR-to-text methods on same dataset: JAMR-gen (tree-based spanning-tree → tree-to-string) = 23.00 BLEU, TSP-gen (graph-fragment + TSP decoding) = 22.44 BLEU. Paper argues SNRG retains hierarchical correspondences without incurring graph-to-tree information loss (a problem for Flanigan et al.'s spanning-tree approach) and models hierarchies missing from TSP-gen.",
            "advantages": "Learns hierarchical graph↔string correspondences (captures structure beyond flat fragment sequencing), avoids irreversible graph-to-tree transformation (reducing information loss and error propagation), achieves state-of-the-art BLEU on LDC2015E86, supports guaranteed derivations via concept and glue rules, includes lexicalized probabilities and reordering/moving-distance features to improve fluency and local ordering.",
            "disadvantages": "Graph grammar data sparsity — many glue rules are required (≈30% of rules used are glue), induced rules can be small and sparse, requires aligned (sentence, AMR, word-to-AMR) training data, some rule probabilities (concept/glue) are manually set low (0.0001), decoding is more complex (graph matching and beam search).",
            "failure_cases": "Reported generation errors in examples: incorrect small-word choices (e.g., 'that' vs 'what') and missing prepositions (missing 'in' between entities). Ablation shows system fails badly without induced hierarchical rules (BLEU drop to ~17.4), indicating reliance on induced rules and potential brittleness when induced coverage is low.",
            "uuid": "e8970.0",
            "source_info": {
                "paper_title": "AMR-to-text Generation with Synchronous Node Replacement Grammar",
                "publication_date_yy_mm": "2017-02"
            }
        },
        {
            "name_short": "Spanning-tree → tree-to-string",
            "name_full": "Graph-to-spanning-tree then tree-to-string transducer (JAMR-gen)",
            "brief_description": "A two-step pipeline that first converts an AMR graph into a spanning tree and then uses a tree-to-string transducer to map the tree to a sentence, leveraging MT tree-based techniques.",
            "citation_title": "Generation from abstract meaning representation using tree transducers",
            "mention_or_use": "mention",
            "representation_name": "Spanning-tree + Tree-to-string transducer",
            "representation_description": "Convert the input AMR graph into a spanning tree (a projective tree covering graph nodes/edges), then apply a tree-to-string hierarchical transducer (learned from tree–string pairs) to produce the surface string. This leverages existing hierarchical SMT/tree transducer machinery.",
            "graph_type": "AMR graphs (converted into spanning trees before generation).",
            "conversion_method": "Graph-to-tree: derive a spanning tree from the AMR graph (graph→spanning-tree transformation). Tree-to-string: apply a learned tree-to-string transducer using tree-based hierarchical alignment methods to generate sentence text.",
            "downstream_task": "AMR-to-text generation.",
            "performance_metrics": "Reported BLEU in this paper for JAMR-gen on LDC2015E86 = 23.00 (test).",
            "comparison_to_others": "Compared in this paper: SNRG (this work) outperforms JAMR-gen by ~2.6 BLEU points (25.62 vs 23.00). Paper notes JAMR-gen suffers from error propagation and irreversible information loss due to graph→tree transformation and the projective correspondence constraint.",
            "advantages": "Leverages mature tree-to-string MT technology; captures hierarchical correspondences when operating on trees.",
            "disadvantages": "Graph→tree conversion can cause information loss since AMR encodes relations that may not be representable projectively in a single tree; error propagation from tree conversion step cannot be recovered in generation.",
            "failure_cases": "Information loss and constrained generation due to spanning-tree conversion; projectivity constraints lead to outputs that cannot realize all graph information.",
            "uuid": "e8970.1",
            "source_info": {
                "paper_title": "AMR-to-text Generation with Synchronous Node Replacement Grammar",
                "publication_date_yy_mm": "2017-02"
            }
        },
        {
            "name_short": "TSP-gen",
            "name_full": "AMR-to-text via graph-fragment-to-string rules cast as a Traveling Salesman Problem",
            "brief_description": "A method that extracts graph-fragment-to-string rules and casts finding a sequence of disjoint rule applications to cover the AMR as a Traveling Salesman Problem, ranking candidate sentences with local features and a language model.",
            "citation_title": "AMR-to-text generation as a traveling salesman problem",
            "mention_or_use": "mention",
            "representation_name": "Graph-fragment-to-string (TSP-based sequencing)",
            "representation_description": "Represent the AMR as a set of graph-fragment → phrase rules (flat fragments with terminal output). The conversion chooses a sequence of disjoint fragment rules to cover the graph; this sequencing and ordering problem is formulated as a TSP and solved to produce a linearized sentence candidate, scored with local features and a language model.",
            "graph_type": "AMR graphs (represented as disconnected or disjoint fragments mapped to phrases).",
            "conversion_method": "Extract fragment→phrase mappings; search for an ordering/selection of fragments that covers the AMR (formulated as a TSP), then concatenate the mapped phrases according to the solved tour and rerank using LM and features.",
            "downstream_task": "AMR-to-text generation.",
            "performance_metrics": "Reported BLEU in this paper for TSP-gen on LDC2015E86 = 22.44 (test).",
            "comparison_to_others": "Compared in this paper: SNRG outperforms TSP-gen by ~3.2 BLEU points. Paper argues TSP-gen does not learn hierarchical structural correspondences between AMR graphs and strings, which SNRG does.",
            "advantages": "Directly operates on graph fragments; avoids committing to a spanning-tree; can leverage global optimization (TSP) to choose fragment order.",
            "disadvantages": "Does not capture hierarchical correspondences (no explicit hierarchical rules), potentially limiting modeling of nested or hierarchical dependencies; lower BLEU relative to SNRG in experiments.",
            "failure_cases": "Lacks hierarchical structural modeling; performance lower than hierarchical graph-to-string grammar approaches when measured by BLEU on benchmark dataset.",
            "uuid": "e8970.2",
            "source_info": {
                "paper_title": "AMR-to-text Generation with Synchronous Node Replacement Grammar",
                "publication_date_yy_mm": "2017-02"
            }
        },
        {
            "name_short": "Phrase→graph-fragment extraction",
            "name_full": "Phrase-to-graph-fragment extraction algorithm (Peng et al., 2015)",
            "brief_description": "An extraction algorithm used to obtain initial terminal graph-fragment↔phrase rule pairs from aligned (sentence, AMR, word–AMR alignment) training data; these initial rules are input to induced rule generation (collapsing) for SNRG.",
            "citation_title": "A synchronous hyperedge replacement grammar based approach for AMR parsing",
            "mention_or_use": "use",
            "representation_name": "Phrase-to-graph-fragment extraction",
            "representation_description": "Heuristic algorithm that extracts aligned phrase ↔ AMR-fragment pairs from aligned corpora; initial extracted rules contain only terminals (no nonterminals) and are later combined/collapsed to create induced hierarchical rules with nonterminals.",
            "graph_type": "AMR graphs (fragments of AMR graphs corresponding to phrases).",
            "conversion_method": "Given (sentence, AMR, alignment) pairs, extract initial rules mapping contiguous target phrases to connected AMR fragments using a phrase-to-graph-fragment extraction procedure; then match and collapse pairs of initial rules where one contains the other to introduce nonterminals, producing induced rules.",
            "downstream_task": "Used as the rule extraction/training step for SNRG-based AMR-to-text generation (training-time conversion / rule induction).",
            "performance_metrics": "Not reported standalone in this paper; used as the basis for induced-rule creation that yields final BLEU=25.62 for the SNRG system.",
            "comparison_to_others": "This extraction method is the source of initial rules for SNRG; it is cited from Peng et al. (2015). No direct empirical comparison of extraction algorithms is provided in this paper.",
            "advantages": "Provides grounded initial fragment↔phrase mappings from real aligned data, enabling later creation of hierarchical induced rules by collapsing contained fragments.",
            "disadvantages": "Initial rules are terminal-only and must be collapsed to obtain hierarchical nonterminal rules; quality depends on alignment quality (heuristic aligner used); may contribute to data sparsity of graph grammars.",
            "failure_cases": "If alignments are poor or if fragments are not found, induced hierarchical rules may be sparse, forcing reliance on glue rules or concept rules with manually set low probabilities.",
            "uuid": "e8970.3",
            "source_info": {
                "paper_title": "AMR-to-text Generation with Synchronous Node Replacement Grammar",
                "publication_date_yy_mm": "2017-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generation from abstract meaning representation using tree transducers",
            "rating": 2
        },
        {
            "paper_title": "AMR-to-text generation as a traveling salesman problem",
            "rating": 2
        },
        {
            "paper_title": "A synchronous hyperedge replacement grammar based approach for AMR parsing",
            "rating": 2
        },
        {
            "paper_title": "Generating English from abstract meaning representations",
            "rating": 1
        },
        {
            "paper_title": "Addressing the data sparsity issue in neural amr parsing",
            "rating": 1
        }
    ],
    "cost": 0.01220925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AMR-to-text Generation with Synchronous Node Replacement Grammar</h1>
<p>Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang and Daniel Gildea<br>Department of Computer Science, University of Rochester, Rochester, NY 14627<br>IBM T.J. Watson Research Center, Yorktown Heights, NY 10598<br>Singapore University of Technology and Design</p>
<h4>Abstract</h4>
<p>This paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark, our method gives the state-of-the-art result.</p>
<h2>1 Introduction</h2>
<p>Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as "boy", "want-01") represent concepts, and edges (such as "ARG0", "ARG1") represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015).</p>
<p>AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Graph-to-string derivation.</p>
<p>Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer. Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string. However, it suffers from error propagation since the output is constrained given a spanning tree due to the projective correspondence between them. Information loss in the graph-to-tree transformation step cannot be recovered. Song et al. (2016) directly generate sentences using graph-fragment-to-string rules. They cast the task of finding a sequence of disjoint rules to transduce an AMR graph into a sentence as a traveling salesman problem, using local features and a language model to rank candidate sentences. However, their method does not learn hierarchical structural correspondences between AMR graphs and strings.</p>
<p>We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from a corpus of aligned AMR and sentence pairs. At test time, we apply a graph transducer to collapse input</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example deduction procedure</p>
<table>
<thead>
<tr>
<th>ID.</th>
<th>F</th>
<th>E</th>
</tr>
</thead>
<tbody>
<tr>
<td>(a)</td>
<td>(b / boy)</td>
<td>the boy</td>
</tr>
<tr>
<td>(b)</td>
<td>(w / want-01</td>
<td></td>
</tr>
<tr>
<td></td>
<td>:ARG0 (X / #X#))</td>
<td>#X# wants</td>
</tr>
<tr>
<td>(c)</td>
<td>(X / #X#</td>
<td></td>
</tr>
<tr>
<td></td>
<td>:ARG1 (g / go-01</td>
<td>#X# to go</td>
</tr>
<tr>
<td></td>
<td>:ARG0 X))</td>
<td></td>
</tr>
<tr>
<td>(d)</td>
<td>(w / want-01</td>
<td></td>
</tr>
<tr>
<td></td>
<td>:ARG0 (b / boy))</td>
<td>the boy wants</td>
</tr>
</tbody>
</table>
<p>Table 1: Example rule set</p>
<p>AMR graphs and generate output strings according to the learned grammar. Our system makes use of a log-linear model with real-valued features, tuned using MERT (Och, 2003), and beam search decoding. It gives a BLEU score of 25.62 on LDC2015E86, which is the state-of-the-art on this dataset.</p>
<h2>2 Synchronous Node Replacement Grammar</h2>
<h3>2.1 Grammar Definition</h3>
<p>A synchronous node replacement grammar (NRG) is a rewriting formalism: G = (N, Σ, Δ, P, S), where N is a finite set of nonterminals, Σ and Δ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form X_{i} → ((F, E), ∼), where X_{i} ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a corresponding target string over N ∪ Δ and ∼ denotes the alignment of nonterminal symbols between F and E. A classic NRG (Engelfriet and Rozenberg, 1997, Chapter 1) also defines C, which is an embedding mechanism defining how F is connected to the rest of the graph when replacing X_{i} with F on the graph. Here we omit defining C and allow arbitrary connections. Following Chiang</p>
<p>Data: training corpus C Result: rule instances R</p>
<p>R ← []; 2 for (Sent, AMR, ∼) in C do 3 R_{cur} ← FRAGMENTEXTRACT(Sent, AMR, ∼); 4 for r_{i} in R_{cur} do 5 R.APPEND(r_{i}) ; 6 for r_{j} in R_{cur}/{r_{i}} do 7 if r_{i}, CONTAINS(r_{j}) then 8 r_{ij} ← r_{i}, COLLAPSE(r_{j}); 9 R.APPEND(r_{ij}) ; 10 end 11 end 12 end 13 end Algorithm 1: Rule extraction</p>
<p>(2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances.</p>
<p>Figure 2 shows an example derivation process for the sentence "the boy wants to go" given the rule set in Table 1. Given the start symbol S, which is first replaced with X_{1}, rule (c) is applied to generate "X_{2} to go" and its AMR counterpart. Then rule (b) is used to generate "X_{3} wants" and its AMR counterpart from X_{2}. Finally, rule (a) is used to generate "the boy" and its AMR counterpart from X_{3}. Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013).</p>
<h3>2.2 Induced Rules</h3>
<p>There are three types of rules in our system, namely induced rules, concept rules and graph glue rules. Here we first introduce induced rules, which are obtained by a two-step procedure on a training corpus. Shown in Algorithm 1, the first step is to extract a set of initial rules from training (sentence, AMR, ∼) 2 pairs (Line 2) using the phrase-to-graph-fragment extraction algorithm of Peng et al. (2015) (Line 3). Here an initial rule</p>
<p>^{2} ∼ denotes alignment between words and AMR labels.</p>
<p>^{1}This may over generate, but does not affect our case, as in our bottom-up decoding procedure (section 3) when F is replaced with X_{i}, nodes previously connected to F are re-</p>
<p>contains only terminal symbols in both $F$ and $E$. As a next step, we match between pairs of initial rules $r_{i}$ and $r_{j}$, and generate $r_{i j}$ by collapsing $r_{i}$ with $r_{j}$, if $r_{i}$ contains $r_{j}$ (Line 6-8). Here $r_{i}$ contains $r_{j}$, if $r_{j} . F$ is a subgraph of $r_{i} . F$ and $r_{j} . E$ is a sub-phrase of $r_{i} . E$. When collapsing $r_{i}$ with $r_{j}$, we replace the corresponding subgraph in $r_{i} . F$ with a new non-terminal node, and the sub-phrase in $r_{i} . E$ with the same non-terminal. For example, we obtain rule (b) by collapsing (d) with (a) in Table 1. All initial and generated rules are stored in a rule list $R$ (Lines 5 and 9), which will be further normalized to obtain the final induced rule set.</p>
<h3>2.3 Concept Rules and Glue Rules</h3>
<p>In addition to induced rules, we adopt concept rules (Song et al., 2016) and graph glue rules to ensure existence of derivations. For a concept rule, $F$ is a single node in the input AMR graph, and $E$ is a morphological string of the node concept. A concept rule is used in case no induced rule can cover the node. We refer to the verbalization list ${ }^{3}$ and AMR guidelines ${ }^{4}$ for creating more complex concept rules. For example, one concept rule created from the verbalization list is "(k / keep-01 :ARG1 (p / peace)) ||| peacekeeping".</p>
<p>Inspired by Chiang (2005), we define graph glue rules to concatenate non-terminal nodes connected with an edge, when no induced rules can be applied. Three glue rules are defined for each type of edge label. Taking the edge label "ARG0" as an example, we create the following glue rules:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ID.</th>
<th style="text-align: center;">$F$</th>
<th style="text-align: center;">$E$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$r_{1}$</td>
<td style="text-align: center;">(X1 / #X1# :ARG0 (X2 / #X2#))</td>
<td style="text-align: center;">#X1# #X2#</td>
</tr>
<tr>
<td style="text-align: center;">$r_{2}$</td>
<td style="text-align: center;">(X1 / #X1# :ARG0 (X2 / #X2#))</td>
<td style="text-align: center;">#X2# #X1#</td>
</tr>
<tr>
<td style="text-align: center;">$r_{3}$</td>
<td style="text-align: center;">(X1 / #X1# :ARG0 X1)</td>
<td style="text-align: center;">#X1#</td>
</tr>
</tbody>
</table>
<p>where for both $r_{1}$ and $r_{2}, F$ contains two nonterminal nodes with a directed edge connecting them, and $E$ is the concatenation the two nonterminals in either the monotonic or the inverse order. For $r_{3}, F$ contains one non-terminal node with a self-pointing edge, and $E$ is the nonterminal. With concept rules and glue rules in our final rule set, it is easily guaranteed that there are legal derivations for any input AMR graph.</p>
<h2>3 Model</h2>
<p>We adopt a log-linear model for scoring search hypotheses. Given an input AMR graph, we find</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the highest scored derivation $t^{*}$ from all possible derivations $t$ :</p>
<p>$$
t^{*}=\arg \max <em i="i">{i} \exp \sum</em>(g, t)
$$} w_{i} f_{i</p>
<p>where $g$ denotes the input AMR, $f_{i}(\cdot, \cdot)$ and $w_{i}$ represent a feature and the corresponding weight, respectively. The feature set that we adopt includes phrase-to-graph and graph-to-phrase translation probabilities and their corresponding lexicalized translation probabilities (section 3.1), language model score, word count, rule count, reordering model score (section 3.2) and moving distance (section 3.3). The language model score, word count and phrase count features are adopted from SMT (Koehn et al., 2003; Chiang, 2005).</p>
<p>We perform bottom-up search to transduce input AMRs to surface strings. Each hypothesis contains the current AMR graph, translations of collapsed subgraphs, the feature vector and the current model score. Beam search is adopted, where hypotheses with the same number of collapsed edges and nodes are put into the same beam.</p>
<h3>3.1 Translation Probabilities</h3>
<p>Production rules serve as a basis for scoring hypotheses. We associate each synchronous NRG rule $n \rightarrow(\langle F, E\rangle, \sim)$ with a set of probabilities. First, phrase-to-fragment translation probabilities are defined based on maximum likelihood estimation (MLE), as shown in Equation 2, where $c_{\langle F, E\rangle}$ is the fractional count of $\langle F, E\rangle$.</p>
<p>$$
p(F \mid E)=\frac{c_{\langle F, E\rangle}}{\sum_{F^{\prime}} c_{\left\langle F^{\prime}, E\right\rangle}}
$$</p>
<p>In addition, lexicalized translation probabilities are defined as:</p>
<p>$$
p_{w}(F \mid E)=\prod_{l \in F} \sum_{w \in E} p(l \mid w)
$$</p>
<p>Here $l$ is a label (including both edge labels such as "ARG0" and concept labels such as "want-01") in the AMR fragment $F$, and $w$ is a word in the phrase $E$. Equation 3 can be regarded as a "soft" version of the lexicalized translation probabilities adopted by SMT, which picks the alignment yielding the maximum lexicalized probability for each translation rule. In addition to $p(F \mid E)$ and $p_{w}(F \mid E)$, we use features in the reverse direction, namely $p(E \mid F)$ and $p_{w}(E \mid F)$, the definitions of which are omitted as they are consistent with</p>
<p>Equations 2 and 3, respectively. The probabilities associated with concept rules and glue rules are manually set to 0.0001 .</p>
<h3>3.2 Reordering Model</h3>
<p>Although the word order is defined for induced rules, it is not the case for glue rules. We learn a reordering model that helps to decide whether the translations of the nodes should be monotonic or inverse given the directed connecting edge label. The probabilistic model using smoothed counts is defined as:</p>
<p>$$
\begin{aligned}
&amp; p(M \mid h, l, t)= \
&amp; \quad \frac{1.0+\sum_{h} \sum_{t} c(h, l, t, M)}{2.0+\sum_{o \in{M, I}} \sum_{h} \sum_{t} c(h, l, t, o)}
\end{aligned}
$$</p>
<p>$c(h, l, t, M)$ is the count of monotonic translations of head $h$ and tail $t$, connected by edge $l$.</p>
<h3>3.3 Moving Distance</h3>
<p>The moving distance feature captures the distances between the subgraph roots of two consecutive rule matches in the decoding process, which controls a bias towards collapsing nearby subgraphs consecutively.</p>
<h2>4 Experiments</h2>
<h3>4.1 Setup</h3>
<p>We use LDC2015E86 as our experimental dataset, which contains 16833 training, 1368 dev and 1371 test instances. Each instance contains a sentence, an AMR graph and the alignment generated by a heuristic aligner. Rules are extracted from the training data, and model parameters are tuned on the dev set. For tuning and testing, we filter out sentences with more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4 -gram language model (LM) on gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric. MERT is used (Och, 2003) to tune model parameters on $k$-best outputs on the devset, where $k$ is set 50 .</p>
<p>We investigate the effectiveness of rules and features by ablation tests: "NoInducedRule" does not adopt induced rules, "NoConceptRule" does not adopt concept rules, "NoMovingDistance" does not adopt the moving distance feature, and "NoReorderModel" disables the reordering model. Given an AMR graph, if NoConceptRule cannot produce a legal derivation, we concatenate</p>
<table>
<thead>
<tr>
<th style="text-align: left;">System</th>
<th style="text-align: left;">Dev</th>
<th style="text-align: left;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TSP-gen</td>
<td style="text-align: left;">21.12</td>
<td style="text-align: left;">22.44</td>
</tr>
<tr>
<td style="text-align: left;">JAMR-gen</td>
<td style="text-align: left;">23.00</td>
<td style="text-align: left;">23.00</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: left;">$\mathbf{2 5 . 2 4}$</td>
<td style="text-align: left;">$\mathbf{2 5 . 6 2}$</td>
</tr>
<tr>
<td style="text-align: left;">NoInducedRule</td>
<td style="text-align: left;">16.75</td>
<td style="text-align: left;">17.43</td>
</tr>
<tr>
<td style="text-align: left;">NoConceptRule</td>
<td style="text-align: left;">23.99</td>
<td style="text-align: left;">24.86</td>
</tr>
<tr>
<td style="text-align: left;">NoMovingDistance</td>
<td style="text-align: left;">23.48</td>
<td style="text-align: left;">24.06</td>
</tr>
<tr>
<td style="text-align: left;">NoReorderModel</td>
<td style="text-align: left;">25.09</td>
<td style="text-align: left;">25.43</td>
</tr>
</tbody>
</table>
<p>Table 2: Main results.
existing translation fragments into a final translation, and if a subgraph can not be translated, the empty string is used as the output. We also compare our method with previous works, in particular JAMR-gen (Flanigan et al., 2016) and TSP-gen (Song et al., 2016), on the same dataset.</p>
<h3>4.2 Main results</h3>
<p>The results are shown in Table 2. First, All outperforms all baselines. NoInducedRule leads to the greatest performance drop compared with All, demonstrating that induced rules play a very important role in our system. On the other hand, NoConceptRule does not lead to much performance drop. This observation is consistent with the observation of Song et al. (2016) for their TSP-based system. NoMovingDistance leads to a significant performance drop, empirically verifying the fact that the translations of nearby subgraphs are also close. Finally, NoReorderingModel does not affect the performance significantly, which can be because the most important reordering patterns are already covered by the hierarchical induced rules. Compared with TSP-gen and JAMR-gen, our final model All improves the BLEU from 22.44 and 23.00 to 25.62 , showing the advantage of our model. To our knowledge, this is the best result reported so far on the task.</p>
<h3>4.3 Grammar analysis</h3>
<p>We have shown the effectiveness of our synchronous node replacement grammar (SNRG) on the AMR-to-text generation task. Here we further analyze our grammar as it is relatively less studied than the hyperedge replacement grammar (HRG) (Drewes et al., 1997).</p>
<h2>Statistics on the whole rule set</h2>
<p>We first categorize our rule set by the number of terminals and nonterminals in the AMR fragment $F$, and show the percentages of each type in Figure 3. Each rule contains at most 1 nonterminal, as we collapse each initial rule only once. First</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Statistics on the right-hand side.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Glue</th>
<th style="text-align: center;">Nonterminal</th>
<th style="text-align: center;">Terminal</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1-best</td>
<td style="text-align: center;">$30.0 \%$</td>
<td style="text-align: center;">$30.1 \%$</td>
<td style="text-align: center;">$39.9 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Rules used for decoding.
of all, the percentage of rules containing nonterminals are much more than those without nonterminals, as we collapse each pair of initial rules (in Algorithm 1) and the results can be quadratic the number of initial rules. In addition, most rules are small containing 1 to 3 terminals, meaning that they represent small pieces of meaning and are easier to matched on a new AMR graph. Finally, there are a few large rules, which represent complex meaning.</p>
<h2>Statistics on the rules used for decoding</h2>
<p>In addition, we collect the rules that our well-tuned system used for generating the 1-best output on the testset, and categorize them into 3 types: (1) glue rules, (2) nonterminal rules, which are not glue rules but contain nonterminals on the righthand side and (3) terminal rules, whose right-hand side only contain terminals. Over the rules used on the 1-best result, more than $30 \%$ are non-terminal rules, showing that the induced rules play an important role. On the other hand, $30 \%$ are glue rules. The reason is that the data sparsity for graph grammars is more severe than string-based grammars (such as CFG), as the graph structures are more complex than strings. Finally, terminal rules take the largest percentage, while most are induced rules, but not concept rules.</p>
<h2>Rule examples</h2>
<p>Finally, we show some rules in Table 4, where $F$ and $E$ are the right-hand-side AMR fragment and phrase, respectively. For the first rule, the root of $F$ is a verb ("give-01") whose subject is a nonterminal and object is a AMR fragment "(p / person :ARG0-of (u / use-01))", which means "user". So it is easy to see that the corresponding phrase $E$ conveys the same meaning. For the second rule, "(s3 / stay-01 :accompanier (i / i))" means "stay</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">(g / give-01</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$F:$</td>
<td style="text-align: center;">:ARG0 (X1 / #X1#) <br> :ARG2 (p / person <br> :ARG0-of (u / use-01)))</td>
</tr>
<tr>
<td style="text-align: center;">$E:$</td>
<td style="text-align: center;">#X1# has given users an</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(X1 / #X1#</td>
</tr>
<tr>
<td style="text-align: center;">$F:$</td>
<td style="text-align: center;">:ARG2 (s3 / stay-01 :ARG1 X1 <br> :accompanier (i / i)))</td>
</tr>
<tr>
<td style="text-align: center;">$E:$</td>
<td style="text-align: center;">#X1# staying with me</td>
</tr>
</tbody>
</table>
<p>Table 4: Example rules.</p>
<div class="codehilite"><pre><span></span><code>(u / understand-01
    :ARG0 (y / you)
    :ARG1 (t2 / thing
        :ARG1-of (f2 / feel-01
            :ARG0 (p2 / person
            :example (p / person :wiki -
            :name (t / name :op1 &quot;TMT&quot;)
            :location (c / city :wiki &quot;Fairfax,..Virginia&quot;
            :name (f / name :op1 &quot;Fairfax&quot;))))))
</code></pre></div>

<p>:time (n / now))
Trans: now, you have to understand that people feel about such as tmt fairfax
Ref: now you understand how people like tmt in fairfax feel .</p>
<p>Table 5: Generation example.
with me", which is also covered by its phrase.</p>
<h3>4.4 Generation example</h3>
<p>Finally, we show an example in Table 5, where the top is the input AMR graph, and the bottom is the generation result. Generally, most of the meaning of the input AMR are correctly translated, such as ":example", which means "such as", and "thing", which is an abstract concept and should not be translated, while there are a few errors, such as "that" in the result should be "what", and there should be an "in" between "tmt" and "fairfax".</p>
<h2>5 Conclusion</h2>
<p>We showed that synchronous node replacement grammar is useful for AMR-to-text generation by developing a system that learns a synchronous NRG in the training time, and applies a graph transducer to collapse input AMR graphs and generate output strings according to the learned grammar at test time. Our method performs better than the previous systems, empirically proving the advantages of our graph-to-string rules.</p>
<h2>Acknowledgement</h2>
<p>This work was funded by a Google Faculty Research Award. Yue Zhang is funded by NSFC61572245 and T2MOE201301 from Singapore Ministry of Education.</p>
<h2>References</h2>
<p>Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015. Broad-coverage CCG semantic parsing with AMR. In Conference on Empirical Methods in Natural Language Processing (EMNLP-15). pages 16991710 .</p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. pages 178-186.</p>
<p>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-05). Ann Arbor, Michigan, pages 263-270.</p>
<p>Frank Drewes, Hans-Jörg Kreowski, and Annegret Habel. 1997. Hyperedge replacement, graph grammars. Handbook of Graph Grammars 1:95-162.
J. Engelfriet and G. Rozenberg. 1997. Node replacement graph grammars. In Grzegorz Rozenberg, editor, Handbook of Graph Grammars and Computing by Graph Transformation, World Scientific Publishing Co., Inc., River Edge, NJ, USA, pages 1-94.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016. Generation from abstract meaning representation using tree transducers. In Proceedings of the 2016 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-16). pages 731-739.</p>
<p>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-14). pages 1426-1436.</p>
<p>Daniel Gildea. 2003. Loosely tree-based alignment for machine translation. In Proceedings of the 41th Annual Conference of the Association for Computational Linguistics (ACL-03). Sapporo, Japan, pages $80-87$.</p>
<p>James Goodman, Andreas Vlachos, and Jason Naradowsky. 2016. Noise reduction and targeted exploration in imitation learning for abstract meaning representation parsing. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16). Berlin, Germany, pages 1-11.</p>
<p>Jonas Groschwitz, Alexander Koller, and Christoph Teichmann. 2015. Graph parsing with s-graph grammars. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL-15). Beijing, China, pages 1481-1490.</p>
<p>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of Association for Machine Translation in the Americas (AMTA2006). pages 66-73.</p>
<p>Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012. Semantics-based machine translation with hyperedge replacement grammars. In Proceedings of the International Conference on Computational Linguistics (COLING-12). pages 1359-1376.</p>
<p>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03). pages 48-54.</p>
<p>Xiang Li, Thien Huu Nguyen, Kai Cao, and Ralph Grishman. 2015. Improving event detection with abstract meaning representation. In Proceedings of the First Workshop on Computing News Storylines. Beijing, China, pages 11-15.</p>
<p>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL-06). Sydney, Australia, pages 609-616.</p>
<p>Fandong Meng, Jun Xie, Linfeng Song, Yajuan Lü, and Qun Liu. 2013. Translation with source constituency and dependency trees. In Conference on Empirical Methods in Natural Language Processing (EMNLP-13). Seattle, Washington, USA, pages 1066-1076.</p>
<p>Arindam Mitra and Chitta Baral. 2015. Addressing a question answering challenge by combining statistical methods with inductive rule learning and reasoning. In Proceedings of the National Conference on Artificial Intelligence (AAAI-16).</p>
<p>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03). Sapporo, Japan, pages 160-167.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02). pages 311-318.</p>
<p>Xiaochang Peng, Linfeng Song, and Daniel Gildea. 2015. A synchronous hyperedge replacement grammar based approach for AMR parsing. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning (CoNLL-15). pages 731-739.</p>
<p>Xiaochang Peng, Chuan Wang, Daniel Gildea, and Nianwen Xue. 2017. Addressing the data sparsity issue in neural amr parsing. In Proceedings of the</p>
<p>15th Conference of the European Chapter of the Association for Computational Linguistics (EACL-17). Valencia, Spain, pages 366-375.</p>
<p>Nima Pourdamghani, Kevin Knight, and Ulf Hermjakob. 2016. Generating English from abstract meaning representations. In International Conference on Natural Language Generation (INLG-16). Edinburgh, UK, pages 21-25.</p>
<p>Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, and Jonathan May. 2015. Parsing English into abstract meaning representation using syntaxbased machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP-15). pages 1143-1154.</p>
<p>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08). Columbus, Ohio, pages 577-585.</p>
<p>Linfeng Song, Yue Zhang, Xiaochang Peng, Zhiguo Wang, and Daniel Gildea. 2016. AMR-to-text generation as a traveling salesman problem. In Conference on Empirical Methods in Natural Language Processing (EMNLP-16). Austin, Texas, pages 2084-2089.</p>
<p>Sho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu Hirao, and Masaaki Nagata. 2016. Neural headline generation on abstract meaning representation. In Conference on Empirical Methods in Natural Language Processing (EMNLP-16). Austin, Texas, pages 1054-1059.</p>
<p>Aleš Tamchyna, Chris Quirk, and Michel Galley. 2015. A discriminative model for semanticsto-string translation. In Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation (S2MT 2015). Beijing, China, pages 3036.</p>
<p>Lucy Vanderwende, Arul Menezes, and Chris Quirk. 2015. An AMR parser for English, French, German, Spanish and Japanese and a new AMR-annotated corpus. In Proceedings of the 2015 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-15). pages 26-30.</p>
<p>Chuan Wang, Nianwen Xue, and Sameer Pradhan. 2015. A transition-based algorithm for AMR parsing. In Proceedings of the 2015 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-15). pages 366-375.</p>
<p>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational linguistics 23(3):377-403.</p>
<p>Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP-11). Edinburgh, Scotland, UK., pages 216-226.</p>
<p>Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-based statistical MT. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02). Philadelphia, Pennsylvania, USA, pages 303-310.</p>
<p>Junsheng Zhou, Feiyu Xu, Hans Uszkoreit, Weiguang QU, Ran Li, and Yanhui Gu. 2016. AMR parsing with an incremental joint model. In Conference on Empirical Methods in Natural Language Processing (EMNLP-16). Austin, Texas, pages 680-689.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3} \mathrm{http}: / /$ amr.isi.edu/download/lists/verbalization-listv1.06.txt
${ }^{4}$ https://github.com/amrisi/amr-guidelines&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
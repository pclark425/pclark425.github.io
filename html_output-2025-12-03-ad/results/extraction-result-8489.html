<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8489 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8489</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8489</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f" target="_blank">Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A framework to use the knowledge in LLMs to simplify the control problem, rather than solving it is proposed, which leads to a significant 15% improvement over SOTA for generalization to human goal specifications.</p>
                <p><strong>Paper Abstract:</strong> Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8489.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8489.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plan, Eliminate, and Track</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that leverages pretrained LLMs to (1) generate high-level sub-tasks (Plan), (2) mask irrelevant objects/receptacles via zero-shot QA (Eliminate), and (3) detect sub-task completion via zero-shot QA (Track) to simplify control for an embodied/text-game agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PET + Action Attention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A combination of the PET LLM-assisted pipeline (Plan, Eliminate, Track) feeding an Action Attention policy; Plan generates sub-tasks, Eliminate masks irrelevant observation elements, Track determines sub-task completion, and the action attention agent selects actions conditioned on the current sub-task.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>MT-NLG (530B) for Plan; Macaw-11B for Eliminate & Track; RoBERTa-large for embeddings (used by agent)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Plan: MT-NLG (530B, large industrial LLM) or smaller alternatives (GPT-Neo-2.7B/GPT-2) used for in-context subtask generation; Eliminate/Track: Macaw-11B (zero-shot QA model tuned for common-sense QA); RoBERTa-large used for dense embeddings of observations/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld (TextWorld-derived)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A text-based interactive environment that mirrors ALFRED scenes; tasks require multi-step, compositional manipulation and navigation with long textual observations and large variable action sets, stressing planning, memory, and commonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Hybrid: (a) working / episodic summary memory (history-averaged embeddings) used by the Action Attention agent; (b) short-term context window memory (last d steps, d ≤ 3) used by the Track module for completion detection.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>History H^t is computed as the average of embeddings of all previous observations (Embed(O^j)) and is provided as a task/history embedding input to transformer heads that compute query Q and keys K for each permissible action. Track uses a sliding local context of the last d textual steps appended to a Yes/No QA prompt to detect sub-task completion.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>The averaged history embedding H^t is concatenated/combined with current observation embedding, task embedding, and action embeddings as inputs to transformer 'query' and 'key' heads (M_Q and M_K); Track's short-term context is concatenated into a QA prompt fed to Macaw which outputs Yes/No probabilities to advance the progress tracker.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Full PET + Action Attention completion rates (Table 1): Template goals: 70.0% (seen), 67.5% (unseen); Human goal specs: 52.5% (seen), 60.0% (unseen). (These results use Plan+Eliminate+Track with history averaging integrated into the agent.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Action Attention without PET modules (Table 3 baseline): 25.0% (seen), 9.0% (unseen). (Note: the Action Attention policy itself still uses averaged-history embeddings; 'without memory' in the paper is not an isolated ablation that removes history embedding.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablation Table 3 compares: Action Attention (25/9), +Eliminate (25/11), +Plan & Track (35/15), +PET (Action Attention + Plan+Track+Eliminate) (52.5/27.5) on a 140-demo sampled set; Table 1 reports full-evaluation PET numbers vs GPT and BUTLER. Track QA model trade-offs reported: Macaw-11B precision 0.99 recall 0.78 (thus limits theoretical recall), Macaw-large precision 0.96 recall 0.96. Plan LLM comparison (Table 2): MT-NLG (530B) greatly outperforms smaller LLMs on human-goal sub-task generation.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>The paper's results favor sub-task-level short-term tracking (Track) combined with a compact history summary fed into the action selector: i.e., Plan + Track (subtask tracking using a small context window) + Eliminate applied on sub-tasks, with history-averaged embeddings integrated into an attention-based policy, yielded the best empirical gains. Larger LMs for planning (MT-NLG) and QA models with balanced precision/recall improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Track module does not support re-visiting or undo detection (no dynamic replanning) so if progress is undone (e.g., object moved away) the tracker may incorrectly proceed; Eliminate can mask receptacles that actually contain objects due to simulator spawn anomalies; QA model trade-offs (high precision but lower recall for Macaw-11B) limit detectable progress (recall-limited to ~0.78); no explicit ablation that removes history-averaged memory from the agent to isolate its precise contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Authors recommend using LLMs for high-level decomposition and short-term QA-based tracking rather than relying on LLMs as end-to-end actors; combine sub-task planning+tracking with targeted observation filtering (Eliminate) and a trainable attention-based policy that ingests a compact history embedding. They also recommend using larger LLMs for robust subtask generation and QA models with better recall/precision trade-offs, and future work adding sub-task-level dynamic replanning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8489.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8489.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action Attention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action Attention agent (transformer-based action scorer with history averaging)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based policy that encodes each permissible action and scores actions by computing a query from task/history/current observation and keys for each action; it represents history as the average of embeddings of past observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Action Attention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy network that (1) computes H^t as the average embedding of all previous observations, (2) embeds each permissible action individually, and (3) uses transformer heads to compute a Query (Q) from task, history and observation and Keys (K_i) per action; action probabilities = softmax(Q·K_i).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>RoBERTa-large (for embeddings) and a 12-layer transformer policy (12 heads, hidden dim 384) trained by behavior cloning</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>RoBERTa-large provides 1024-d embeddings for observations and actions; the policy transformer (12 layers, 12 heads, hidden size 384) produces Q/K vectors for attention scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Text-based benchmark with large, variable action spaces and long textual observations, requiring planning, memory, and compositional action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Working/episodic summary memory (averaged historical observation embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>H^t = average_j Embed(O^j) over past observations; H^t is used as an input along with current observation embedding and action embeddings to transformer heads M_Q and M_K to compute action scores.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>The averaged history embedding H^t is provided as part of the transformer's inputs to generate the query Q and to compute per-action keys K_i; decision uses dot-product attention between Q and K_i.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Baseline Action Attention (Table 3): completion rate 25.0% (seen), 9.0% (unseen) on sampled 140-demo set (this baseline already includes history averaging).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Table 3 shows incremental gains when augmenting Action Attention with PET modules; Action Attention alone (25/9) vs Action Attention + Plan&Track (35/15) vs full PET (52.5/27.5). There is no isolated ablation that removes the history averaging memory from Action Attention.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Within the agent, a compact history summary (average of past observation embeddings) combined with per-action encoding and attention scoring is effective when coupled with PET modules; the paper suggests that summarizing history (rather than concatenating long rollouts) is a practical memory strategy for large text observations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No ablation isolating the contribution of history averaging, and the average summary may lose fine-grained temporal information; paper notes generation-by-token policies degenerate in large action spaces, motivating this architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Use compact summary representations of history (averaging embeddings) and encode actions individually to handle variable and large action spaces; pair this with LLM-derived sub-task structure and short-term trackers for best results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8489.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8489.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Track Module</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Progress Tracker (zero-shot QA-based sub-task completion detector)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot QA model (Macaw-11B or Macaw-large) is prompted with the last d steps of textual roll-out plus a 'Did you finish s_p?' question to produce Yes/No probabilities; the tracker advances sub-task index when the model answers Yes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Track (Progress Tracker)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A short-term memory mechanism that uses a zero-shot QA LLM to judge whether the current sub-task is complete, operating on a sliding context of the last up to 3 steps of textual history and producing a binary Yes/No decision to update progress.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Macaw-11B (primary) and Macaw-large (evaluated for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Macaw-11B: an 11B-parameter QA model with strong commonsense QA performance; Macaw-large: a larger Macaw variant with different precision/recall trade-offs used in evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALFWorld</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Text-based, multi-step household tasks requiring stepwise completion detection and progress tracking for hierarchical plans.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Short-term context/window memory (sliding window of last d steps, d grows to max 3 while waiting for completion).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>The module concatenates the last d textual observations/rollout steps with the question 'Did you finish the task of s_p?' to form a prompt; the QA model returns p('Yes') and p('No') and the tracker advances if p('Yes')>p('No'). The window d increments until a max of 3 and resets to 1 when tracker advances.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Track's output controls which sub-task the agent conditions on; it therefore modulates the agent's goal-conditioned input rather than directly modifying the agent's embedding memory.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported agreement metrics: Macaw-11B precision 0.99, recall 0.78 (recall-limited theoretical performance ~78%); Macaw-large precision 0.96, recall 0.96. Ablation (Table 3) shows adding Plan&Track to Action Attention yields 35.0% (seen) and 15.0% (unseen) vs 25.0%/9.0% baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Comparison of QA models: Macaw-11B (high precision, lower recall) vs Macaw-large (more balanced precision/recall). Ablation table demonstrates Plan&Track as a combined module improves performance; Track alone is not evaluated in complete isolation.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Short window (d up to 3) context fed into a QA model with balanced precision and recall; high precision with low recall (Macaw-11B) can limit theoretical end-performance, so prefer QA models with better recall when possible.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Tracker cannot re-visit previously finished sub-tasks and does not detect if progress has been undone; QA model recall limitations (Macaw-11B) can miss sub-task completion events, limiting achievable performance; false positives/negatives in QA-based detection can cause premature or delayed progress.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Use a short sliding context (last up to 3 steps) for efficient detection; favor QA models with balanced precision/recall for tracking; combine tracking with sub-task planning so Eliminate operates on narrower sub-goals for better masking performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>Keep calm and explore: Language models for action generation in text-based games <em>(Rating: 2)</em></li>
                <li>Language models are few-shot butlers <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8489",
    "paper_id": "paper-5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "PET",
            "name_full": "Plan, Eliminate, and Track",
            "brief_description": "A framework that leverages pretrained LLMs to (1) generate high-level sub-tasks (Plan), (2) mask irrelevant objects/receptacles via zero-shot QA (Eliminate), and (3) detect sub-task completion via zero-shot QA (Track) to simplify control for an embodied/text-game agent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PET + Action Attention",
            "agent_description": "A combination of the PET LLM-assisted pipeline (Plan, Eliminate, Track) feeding an Action Attention policy; Plan generates sub-tasks, Eliminate masks irrelevant observation elements, Track determines sub-task completion, and the action attention agent selects actions conditioned on the current sub-task.",
            "llm_model_name": "MT-NLG (530B) for Plan; Macaw-11B for Eliminate & Track; RoBERTa-large for embeddings (used by agent)",
            "llm_model_description": "Plan: MT-NLG (530B, large industrial LLM) or smaller alternatives (GPT-Neo-2.7B/GPT-2) used for in-context subtask generation; Eliminate/Track: Macaw-11B (zero-shot QA model tuned for common-sense QA); RoBERTa-large used for dense embeddings of observations/actions.",
            "benchmark_name": "ALFWorld (TextWorld-derived)",
            "benchmark_description": "A text-based interactive environment that mirrors ALFRED scenes; tasks require multi-step, compositional manipulation and navigation with long textual observations and large variable action sets, stressing planning, memory, and commonsense.",
            "memory_used": true,
            "memory_type": "Hybrid: (a) working / episodic summary memory (history-averaged embeddings) used by the Action Attention agent; (b) short-term context window memory (last d steps, d ≤ 3) used by the Track module for completion detection.",
            "memory_architecture": "History H^t is computed as the average of embeddings of all previous observations (Embed(O^j)) and is provided as a task/history embedding input to transformer heads that compute query Q and keys K for each permissible action. Track uses a sliding local context of the last d textual steps appended to a Yes/No QA prompt to detect sub-task completion.",
            "memory_integration_strategy": "The averaged history embedding H^t is concatenated/combined with current observation embedding, task embedding, and action embeddings as inputs to transformer 'query' and 'key' heads (M_Q and M_K); Track's short-term context is concatenated into a QA prompt fed to Macaw which outputs Yes/No probabilities to advance the progress tracker.",
            "performance_with_memory": "Full PET + Action Attention completion rates (Table 1): Template goals: 70.0% (seen), 67.5% (unseen); Human goal specs: 52.5% (seen), 60.0% (unseen). (These results use Plan+Eliminate+Track with history averaging integrated into the agent.)",
            "performance_without_memory": "Action Attention without PET modules (Table 3 baseline): 25.0% (seen), 9.0% (unseen). (Note: the Action Attention policy itself still uses averaged-history embeddings; 'without memory' in the paper is not an isolated ablation that removes history embedding.)",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Ablation Table 3 compares: Action Attention (25/9), +Eliminate (25/11), +Plan & Track (35/15), +PET (Action Attention + Plan+Track+Eliminate) (52.5/27.5) on a 140-demo sampled set; Table 1 reports full-evaluation PET numbers vs GPT and BUTLER. Track QA model trade-offs reported: Macaw-11B precision 0.99 recall 0.78 (thus limits theoretical recall), Macaw-large precision 0.96 recall 0.96. Plan LLM comparison (Table 2): MT-NLG (530B) greatly outperforms smaller LLMs on human-goal sub-task generation.",
            "best_memory_strategy": "The paper's results favor sub-task-level short-term tracking (Track) combined with a compact history summary fed into the action selector: i.e., Plan + Track (subtask tracking using a small context window) + Eliminate applied on sub-tasks, with history-averaged embeddings integrated into an attention-based policy, yielded the best empirical gains. Larger LMs for planning (MT-NLG) and QA models with balanced precision/recall improve performance.",
            "limitations_or_failure_cases": "Track module does not support re-visiting or undo detection (no dynamic replanning) so if progress is undone (e.g., object moved away) the tracker may incorrectly proceed; Eliminate can mask receptacles that actually contain objects due to simulator spawn anomalies; QA model trade-offs (high precision but lower recall for Macaw-11B) limit detectable progress (recall-limited to ~0.78); no explicit ablation that removes history-averaged memory from the agent to isolate its precise contribution.",
            "recommendations_or_conclusions": "Authors recommend using LLMs for high-level decomposition and short-term QA-based tracking rather than relying on LLMs as end-to-end actors; combine sub-task planning+tracking with targeted observation filtering (Eliminate) and a trainable attention-based policy that ingests a compact history embedding. They also recommend using larger LLMs for robust subtask generation and QA models with better recall/precision trade-offs, and future work adding sub-task-level dynamic replanning.",
            "uuid": "e8489.0",
            "source_info": {
                "paper_title": "Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Action Attention",
            "name_full": "Action Attention agent (transformer-based action scorer with history averaging)",
            "brief_description": "A transformer-based policy that encodes each permissible action and scores actions by computing a query from task/history/current observation and keys for each action; it represents history as the average of embeddings of past observations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Action Attention",
            "agent_description": "Policy network that (1) computes H^t as the average embedding of all previous observations, (2) embeds each permissible action individually, and (3) uses transformer heads to compute a Query (Q) from task, history and observation and Keys (K_i) per action; action probabilities = softmax(Q·K_i).",
            "llm_model_name": "RoBERTa-large (for embeddings) and a 12-layer transformer policy (12 heads, hidden dim 384) trained by behavior cloning",
            "llm_model_description": "RoBERTa-large provides 1024-d embeddings for observations and actions; the policy transformer (12 layers, 12 heads, hidden size 384) produces Q/K vectors for attention scoring.",
            "benchmark_name": "ALFWorld",
            "benchmark_description": "Text-based benchmark with large, variable action spaces and long textual observations, requiring planning, memory, and compositional action sequences.",
            "memory_used": true,
            "memory_type": "Working/episodic summary memory (averaged historical observation embeddings).",
            "memory_architecture": "H^t = average_j Embed(O^j) over past observations; H^t is used as an input along with current observation embedding and action embeddings to transformer heads M_Q and M_K to compute action scores.",
            "memory_integration_strategy": "The averaged history embedding H^t is provided as part of the transformer's inputs to generate the query Q and to compute per-action keys K_i; decision uses dot-product attention between Q and K_i.",
            "performance_with_memory": "Baseline Action Attention (Table 3): completion rate 25.0% (seen), 9.0% (unseen) on sampled 140-demo set (this baseline already includes history averaging).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Table 3 shows incremental gains when augmenting Action Attention with PET modules; Action Attention alone (25/9) vs Action Attention + Plan&Track (35/15) vs full PET (52.5/27.5). There is no isolated ablation that removes the history averaging memory from Action Attention.",
            "best_memory_strategy": "Within the agent, a compact history summary (average of past observation embeddings) combined with per-action encoding and attention scoring is effective when coupled with PET modules; the paper suggests that summarizing history (rather than concatenating long rollouts) is a practical memory strategy for large text observations.",
            "limitations_or_failure_cases": "No ablation isolating the contribution of history averaging, and the average summary may lose fine-grained temporal information; paper notes generation-by-token policies degenerate in large action spaces, motivating this architecture.",
            "recommendations_or_conclusions": "Use compact summary representations of history (averaging embeddings) and encode actions individually to handle variable and large action spaces; pair this with LLM-derived sub-task structure and short-term trackers for best results.",
            "uuid": "e8489.1",
            "source_info": {
                "paper_title": "Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Track Module",
            "name_full": "Progress Tracker (zero-shot QA-based sub-task completion detector)",
            "brief_description": "A zero-shot QA model (Macaw-11B or Macaw-large) is prompted with the last d steps of textual roll-out plus a 'Did you finish s_p?' question to produce Yes/No probabilities; the tracker advances sub-task index when the model answers Yes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Track (Progress Tracker)",
            "agent_description": "A short-term memory mechanism that uses a zero-shot QA LLM to judge whether the current sub-task is complete, operating on a sliding context of the last up to 3 steps of textual history and producing a binary Yes/No decision to update progress.",
            "llm_model_name": "Macaw-11B (primary) and Macaw-large (evaluated for comparison)",
            "llm_model_description": "Macaw-11B: an 11B-parameter QA model with strong commonsense QA performance; Macaw-large: a larger Macaw variant with different precision/recall trade-offs used in evaluation.",
            "benchmark_name": "ALFWorld",
            "benchmark_description": "Text-based, multi-step household tasks requiring stepwise completion detection and progress tracking for hierarchical plans.",
            "memory_used": true,
            "memory_type": "Short-term context/window memory (sliding window of last d steps, d grows to max 3 while waiting for completion).",
            "memory_architecture": "The module concatenates the last d textual observations/rollout steps with the question 'Did you finish the task of s_p?' to form a prompt; the QA model returns p('Yes') and p('No') and the tracker advances if p('Yes')&gt;p('No'). The window d increments until a max of 3 and resets to 1 when tracker advances.",
            "memory_integration_strategy": "Track's output controls which sub-task the agent conditions on; it therefore modulates the agent's goal-conditioned input rather than directly modifying the agent's embedding memory.",
            "performance_with_memory": "Reported agreement metrics: Macaw-11B precision 0.99, recall 0.78 (recall-limited theoretical performance ~78%); Macaw-large precision 0.96, recall 0.96. Ablation (Table 3) shows adding Plan&Track to Action Attention yields 35.0% (seen) and 15.0% (unseen) vs 25.0%/9.0% baseline.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Comparison of QA models: Macaw-11B (high precision, lower recall) vs Macaw-large (more balanced precision/recall). Ablation table demonstrates Plan&Track as a combined module improves performance; Track alone is not evaluated in complete isolation.",
            "best_memory_strategy": "Short window (d up to 3) context fed into a QA model with balanced precision and recall; high precision with low recall (Macaw-11B) can limit theoretical end-performance, so prefer QA models with better recall when possible.",
            "limitations_or_failure_cases": "Tracker cannot re-visit previously finished sub-tasks and does not detect if progress has been undone; QA model recall limitations (Macaw-11B) can miss sub-task completion events, limiting achievable performance; false positives/negatives in QA-based detection can cause premature or delayed progress.",
            "recommendations_or_conclusions": "Use a short sliding context (last up to 3 steps) for efficient detection; favor QA models with balanced precision/recall for tracking; combine tracking with sub-task planning so Eliminate operates on narrower sub-goals for better masking performance.",
            "uuid": "e8489.2",
            "source_info": {
                "paper_title": "Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2
        },
        {
            "paper_title": "Keep calm and explore: Language models for action generation in text-based games",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot butlers",
            "rating": 2
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 1
        }
    ],
    "cost": 0.0140235,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Plan, Eliminate, and Track Language Models are Good Teachers for Embodied Agents.</h1>
<p>Yue Wu ${ }^{1}$ So Yeon Min ${ }^{1}$ Yonatan Bisk ${ }^{1}$ Ruslan Salakhutdinov ${ }^{1}$ Amos Azaria ${ }^{2}$ Yuanzhi Li ${ }^{13}$ Tom M. Mitchell ${ }^{1}$ Shrimai Prabhumoye ${ }^{4}$</p>
<h4>Abstract</h4>
<p>Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with nontext environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant $15 \%$ improvement over SOTA for generalization to human goal specifications.</p>
<h2>1. Introduction</h2>
<p>Humans can abstractly plan their everyday tasks without execution; for example, given the task "Make breakfast", we can roughly plan to first pick up a mug and make coffee, before grabbing eggs to scramble. Embodied agents, endowed with this capability will generalize more effectively by leveraging common-sense reasoning.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. PET framework. Plan module uses LLM to generate a high-level plan. Eliminate Module uses a QA model to mask irrelevant objects in observation. Track module uses a QA model to track the completion of sub-tasks.</p>
<p>Recent work (Huang et al., 2022a;b; Ahn et al., 2022; Yao et al., 2020) has used LLMs (Bommasani et al., 2021) for abstract planning for embodied or gaming agents. These have shown incipient success in extracting procedural world knowledge from LLMs in linguistic form with posthoc alignment to executable actions in the environment. However, they treat LLMs as the actor, and focus on adapting LLM outputs to executable actions either through fine-tuning (Micheli \&amp; Fleuret, 2021) or constraints (Ahn et al., 2022). Using LLM as the actor works for pure-text environments with limited interactions (Huang et al., 2022b; Ahn et al., 2022) (just consisting of "picking/placing" objects), but limits generalization to other modalities. In addition, the scenarios considered have been largely simplified from the real world. Ahn et al. (2022) provides all available objects and possible interactions at the start and limits tasks to the set of provided objects/interactions. Huang et al. (2022b) limits the environment to objects on a single table.</p>
<p>On the other hand, to successfully "cut some lettuce" in a real-world room, one has to "find a knife", which can be non-trivial since there can be multiple drawers or cabinets (Chaplot et al., 2020; Min et al., 2021; Blukis et al., 2021). A more realistic scenario leads to a</p>
<p>diverse, complicated set of tasks or large and changing action space. Furthermore, the text description of the observation increases as a function of the number of receptacles and objects the agent sees. Combined with growing roll-outs, the state becomes too verbose to fit into any LLM.</p>
<p>In this work, we explore alternative mechanisms to leverage the prior knowledge encoded in LLMs without impacting the trainable nature of the actor. We propose a 3-step framework (Figure 1): Plan, Eliminate, and Track (PET). Plan module simplifies complex tasks by breaking them down into sub-tasks. It uses a pretrained LLM to generate a list of sub-tasks for an input task description employing example prompts from the training set similar to Huang et al. (2022a); Ahn et al. (2022). The Eliminate module addresses the challenge of long observations. It uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. The Track module uses a zero-shot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Finally, the Action Attention agent uses a transformer-based architecture to accommodate for long roll-out and variable length action space. The agent observes the masked observation and takes an action conditioned on the current sub-task.</p>
<p>We focus on instruction following in indoor households on the AlfWorld (Shridhar et al., 2020b) interactive text environment benchmark. Our experiments and analysis demonstrate that LLMs not only remove $40 \%$ of taskirrelevant objects in observation through common-sense QA, but also generate high-level sub-tasks with $99 \%$ accuracy. In addition, multiple LLMs may be used in coordination with each other to assist the agent from different aspects.</p>
<p>Our contributions are as follows:</p>
<ol>
<li>PET: A novel framework for leveraging pretrained LLMs with embodied agents; our work shows that each of $\mathrm{P}, \mathrm{E}, \mathrm{T}$ serves a complementary role and should be simultaneously addressed to tackle control tasks.</li>
<li>An Action Attention agent that handles the changing action space for text environments.</li>
<li>A $15 \%$ improvement over SOTA for generalization to human goals via sub-task planning and tracking.</li>
</ol>
<h2>2. Related Work</h2>
<p>Language Conditioned Policies A considerable portion of prior work studies imitation learning (Tellex et al., 2011; Mei et al., 2016; Nair et al., 2022; Stepputtis et al., 2020; Jang et al., 2022; Shridhar et al., 2022;</p>
<p>Sharma et al., 2021) or reinforcement learning (Misra et al., 2017; Jiang et al., 2019; Cideron et al., 2020; Goyal et al., 2021; Nair et al., 2022; Akakzia et al., 2020) policies conditioned on natural language instruction or goal (MacMahon et al., 2006; Kollar et al., 2010). While some prior research has used pre-trained language embeddings to improve generalization to new instructions (Nair et al., 2022), they lack domain knowledge that is captured in LLMs. Our PET framework enables planning, progress tracking, and observation filtering through the use of LLMs, and is designed to be compatible with any language conditional policies above.</p>
<p>LLMs for Control LLMs have recently achieved success in high-level planning. Huang et al. (2022a) shows that pre-trained LLMs can generate plausible plans for day-to-day tasks, but the generated sub-tasks cannot be directly executed in an end-to-end control environment. Ahn et al. (2022) solves the executability issue by training an action scoring model to re-weigh LLM action choices and demonstrates success on a robot. However, LLM scores work for simple environments with actions limited to pick/place (Ahn et al., 2022), but fails with environments with more objects and diverse actions (Shridhar et al., 2020b). Song et al. (2022) uses GPT3 to generate step-by-step lowlevel commands, which are then executed by respective control policies. the work improves Ahn et al. (2022) with more action diversity and on-the-fly re-plan. In addition, all the above LLMs require few-shot demonstrations of up to 17 examples, making the length of the prompt infeasible for AlfWorld. Micheli \&amp; Fleuret (2021) fine-tuned a GPT2-medium model on expert trajectories in AlfWorld and demonstrated impressive evaluation results. However, LM fine-tuning requires a fully text-based environment, consistent expert trajectories, and a fully text-based action space. Such requirements greatly limit the generalization to other domains, and even to other forms of task specification. We show that our PET framework achieves better generalization to human goal specifications which the agents were not trained on.</p>
<h2>Hierarchical Planning with Natural Language</h2>
<p>Due to the structured nature of natural language, Andreas et al. (2017) explored associating each task description to a modular sub-policy. Later works extend the above approach by using a single conditional policy (Mei et al., 2016), or by matching sub-tasks to templates (Oh et al., 2017). Recent works have shown that LLMs are proficient high-level planners (Huang et al., 2022a; Ahn et al., 2022; Lin et al., 2022), and therefore motivates us to revisit the idea of hierarchical task plan-</p>
<p>ning with progress tracking. To our knowledge, PET is the first work combining a zero-shot subtask-level LLM planner and zero-shot LLM progress tracker with a low-level conditional sub-task policy.</p>
<p>Text Games Text-based games are complex, interactive simulations where the game state and action space are in natural lanugage. They are fertile ground for language-focused machine learning research. In addition to language understanding, successful play requires skills like memory and planning, exploration (trial and error), and common sense. The AlfWorld <em>(Shridhar et al., 2020b)</em> simulator extends a common text-based game simulator, TextWorld <em>Côté et al. (2018a)</em>, to create text-based analogs of each ALFRED scene.</p>
<p>Agents for Large Action Space <em>He et al. (2015)</em> learns representation for state and actions with two different models and computes the Q function as the inner product of the representations. While this could generalize to large action space, they only considered a small number of actions.</p>
<p><em>Fulda et al. (2017); Ahn et al. (2022)</em> explore action elimination in the setting of affordances. <em>Zahavy et al. (2018)</em> trains a model to eliminate invalid actions on Zork from external environment signals. However, the functionality depends on the existence of external elimination signal.</p>
<h2>3 Plan, Eliminate, and Track</h2>
<p>In this section, we explain our 3-step framework: Plan, Eliminate, and Track (PET). In Plan module (M<sub>P</sub>), a pre-trained LLM generates a list of sub-tasks for an input task description using samples from the training set as in-context examples. The Eliminate module (M<sub>E</sub>) uses a zero-shot QA language model to score and mask objects and receptacles that are irrelevant to the current sub-task. The Track module (M<sub>T</sub>) uses a zero-shot QA language model to determine if the current sub-task is complete and moves to the next sub-task. Note that Plan is a generative task and Eliminate and Track are classification tasks.</p>
<p>We also implement an attention-based agent (Action Attention), which scores each permissible action and is trained on imitation learning on the expert. The agent observes the masked observation and takes an action conditioned on the current sub-task.</p>
<p>Problem Setting We define the task description as T, the observation string at time step t as O<sup>t</sup>, and the list of permissible actions {a<sup>t</sup><sub>i</sub> | a<sup>t</sup><sub>i</sub> can be executed} as A<sup>t</sup>. For each observation string O<sup>t</sup>, we define the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Plan Module (Sub-task Generation). 5 full examples are chosen from the training set based on RoBERTa embedding similarity with the task query description. Then the examples are concatenated with the task query to get the prompt. Finally, we prompt the LLM to generate the desired sub-tasks.</p>
<p>receptacles and objects within the observation as r<sup>t</sup><sub>i</sub> and o<sup>t</sup><sub>i</sub> respectively. The classification between receptacles and objects is defined by the environment <em>(Shridhar et al., 2020b)</em>. For a task T, we assume there exists a list of sub-tasks S<sub>T</sub> = {s<sub>1</sub>, . . . s<sub>k</sub>} that solves T.</p>
<h3>3.1 Plan</h3>
<p>Tasks in the real world are often complex and need more than one step to be completed. Motivated by the ability of humans to plan high-level sub-tasks given a complex task, we design the Plan module (M<sub>P</sub>) to generate a list of high-level sub-tasks for a task description T.</p>
<p>Inspired by the contextual prompting techniques for planning with LLMs <em>(Huang et al., 2022a)</em>, we use an LLM as our plan module M<sub>P</sub>. For a given task description T, we compose the query question Q<sub>T</sub> as "What are the middle steps required to T?", and require M<sub>P</sub> to generate a list sub-tasks S<sub>T</sub> = {s<sub>1</sub>, . . . s<sub>k</sub>}.</p>
<p>Specifically, we select the top 5 example tasks T<sup>E</sup> from the training set based on RoBERTa <em>(Liu et al., 2019)</em> embedding similarity with the query task T. We then concatenate the example tasks with example sub-tasks in a query-answer format to build the prompt P<sub>T</sub> for M<sub>P</sub> (Fig. 2):</p>
<p>P<sub>T</sub> = concat(Q<sub>T<sup>E</sup></sub>, S<sub>T<sup>E</sup></sub>, . . . , Q<sub>T<sup>E</sup></sub>, S<sub>T<sup>E</sup></sub>, Q<sub>T</sub>)</p>
<p>An illustration of our prompt format is shown in Figure 2, where T = "heat some apple and put it in fridge", and Q<sub>T<sup>E</sup></sub> = "What are the middle steps required to put two spraybottles on toilet", S<sub>T<sup>E</sup></sub> = "take a spraybottle,</p>
<p>place the spraybottle in/on toilet, take a spraybottle, place the spraybottle in/on toilet". The expected list of sub-tasks to achieve this task $\mathcal{T}$ is $s_{1}=$ 'take an apple', $s_{2}=$ 'heat the apple', and $s_{3}=$ 'place the apple in/on fridge'
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Eliminate Module (Receptacle Masking). We use a pre-trained QA model to filter irrelevant receptacles/objects in the observation of each scene. As we can see, the original observation is too long and the receptacles shown in red are not relevant for task completion. These receptacles are filtered by the QA model making the observation shorter.</p>
<h3>3.2. Eliminate</h3>
<p>Typical Alfworld scenes can start with around 15 receptacles, each containing up to 15 objects. In some close-to-worst cases, there can be around 30 open-able receptacles (e.g. a kitchen with many cabinets and drawers), and it easily takes an agent with no prior knowledge more than 50 steps for the agent to find the desired object (repeating the process of visiting each receptacle, opening it, closing it). We observe that many receptacles and objects are irrelevant to specific tasks during both training and evaluation, and can be easily filtered with common-sense knowledge about the tasks. For example, in Fig. 3 the task is to heat some apple. By removing the irrelevant receptacles like the coffeemachine, garbagecan, or objects like knife, we could significantly shorten our observation. We therefore propose to leverage commonsense knowledge captured by large pre-trained QA models to design our Eliminate module $\mathcal{M}_{\mathbf{E}}$ to mask out irrelevant receptacles and objects.</p>
<p>For task $\mathcal{T}$, we create prompts in the format $\mathcal{P}<em o="o">{r}=$ "Your task is to: $\mathcal{T}$. Where should you go to?" for receptacles and $\mathcal{P}</em>}=$ "Your task is to: $\mathcal{T}$. Which objects will be relevant?" for objects. Using the pre-trained QA model $\mathcal{M<em o__i="o_{i">{\mathbf{E}}$ in a zero-shot manner, we compute score $\mu</em>}}=$ $\mathcal{M<em o="o">{\mathbf{E}}\left(\mathcal{P}</em>}, o_{i}\right)$ for each object $o_{i}$ and $\mu_{r_{i}}=\mathcal{M<em o="o">{\mathbf{E}}\left(\mathcal{P}</em>$ in observation at every step. $\mu$ represents the belief score of whether the common-sense QA model believes the object/receptacle is relevant to
$\mathcal{T}$. We then remove $o_{i}$ from observation if $\mu_{o_{i}}&lt;\tau_{o}$, and remove $r_{i}$ if $\mu_{r_{i}}&lt;\tau_{r}$. Threshold $\tau_{o}, \tau_{r}$ are hyperparameters.}, r_{i}\right)$ for each receptacle $r_{j</p>
<h2>Environment</h2>
<p>You are in the middle of a room. Looking quickly around you, you see ..., a garbagecan 1, a sinkbasin 1, and a toaster 1.
$&gt;$ go to sinkbasin 1
On the sinkbasin 1, you see nothing.
$&gt;$ go to diningtable 1
On the diningtable 1, you see a apple 1, a bread 3, a cup 3, and a peppershaker 2. You take apple 1 from diningtable 1.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Track Module (Progress Tracking). At every step, we take the last 3 steps of roll-out as context and append a query (about whether the current sub-task is completed) to get the prompt. A pre-trained QA model generates a Yes/No answer to the prompt. For the answer "Yes", we update the tracker to the next sub-task.</p>
<h3>3.3. Track</h3>
<p>For the agent to utilize the high-level plan, it first needs to know which sub-task to execute. A human actor typically starts from the first item and check-off the tasks one by one until completion. Therefore, similar to Section 3.2, we use a pre-trained QA model to design the Track module $\mathcal{M}<em _mathcal_T="\mathcal{T">{\mathbf{T}}$ to perform zero-shot sub-task completion detection. ${ }^{1}$
Specifically, as illustrated in Figure 4, for sub-task list $\mathcal{S}</em>\right)$. We then compose the context as the last $d$ steps of the agent observation}}=\left{s_{1}, \ldots s_{k}\right}$, we keep track of a progress tracker $p$ (initialized at 1) that indicates the sub-task the agent is currently working on $\left(s_{p</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>for the current sub-task and the question as "Did you finish the task of $s_{p}$ ?". For efficiency, we set $d:=$ $\min (d+1,3)$ at each step. Note that $d$ is reset to 1 whenever the progress tracker updates. Hence, the template $\mathcal{P}<em p="p">{a}=\operatorname{concat}\left(\mathcal{O}^{t-d}, \ldots, \mathcal{O}^{t-1},\right.$ "Did you finish the task of $\left.s</em>} ? "\right)$. We feed $\mathcal{P<em _mathcal_T="\mathcal{T">{a}$ to a pre-trained zeroshot QA model $\mathcal{M}</em>}}$ and compute the probability of tokens 'Yes' and 'No' as follows: $p_{\mathcal{M<em a="a">{\mathcal{T}}}\left({ }^{\prime \prime} Y e s^{\prime \prime} \mid \mathcal{P}</em>}\right)$ and $p_{\mathcal{M<em a="a">{\mathcal{T}}}\left({ }^{\prime \prime} N o^{\prime \prime} \mid \mathcal{P}</em>}\right)$. If $p_{\mathcal{M<em a="a">{\mathcal{T}}}\left({ }^{\prime \prime} Y e s^{\prime \prime} \mid \mathcal{P}</em>}\right)&gt;p_{\mathcal{M<em a="a">{\mathcal{T}}}\left({ }^{\prime \prime} N o^{\prime \prime} \mid \mathcal{P}</em>\right)$ then we increment the tracker $p$ to track the next subtask.</p>
<p>If the tracking ends prematurely, meaning that $p&gt;$ $\operatorname{len}\left(\mathcal{S}_{\mathcal{T}}\right)$ but the environment has not returned "done", we fall back to conditioning with $\mathcal{T}$. We study the rate of pre-mature ends in Section 4.4 in terms of precision and recall.</p>
<h3>3.4. Agent</h3>
<p>Since the number of permissible actions can vary a lot by the environment, the agent needs to handle arbitrary dimensions of action space. While Shridhar et al. (2020b) addresses this challenge by generating actions token-by-token, such a generation process leads to degenerate performance even on the training set.</p>
<p>We draw inspiration from the field of text summarization, where models are built to handle variable input lengths. See et al. (2017) generates a summary through an attention-like "pointing" mechanism that extracts the output word by word. Similarly, an attention-like "pointing" model could be used to select an action from the list of permissible actions.</p>
<p>Action Attention We are interested in learning a policy $\pi$ that outputs the optimal action among permissible actions. We eschew the long rollout/ large action space problems by (1) representing observations by averaging over history, and (2) individually encoding actions (Fig 5). In our proposed action attention framework, we first represent historical observations $H^{t}$ as the average of embeddings of all individual observations through history (Eq. 1), and $H^{A}$ as the list of embeddings of all the current permissible actions (Eq. 2). Then, in Eq. 3, we compute the query $Q$ using a transformer with a "query" head $\left(\mathcal{M}<em i="i">{\mathcal{Q}}\right)$ on task embedding $\left(H^{t}\right)$, the current observation embedding $\left(\mathcal{O}^{t}\right)$, and the list of action embeddings $\left(H^{A}\right)$. In Eq. 4 we compute the key $K</em>}$ for each action $a_{i}$ using the same transformer with a "key" head $\left(\mathcal{M<em i="i">{\mathcal{K}}\right)$ on task embedding $\left(H^{t}\right)$, the current observation embedding $\left(\mathcal{O}^{t}\right)$, and embedding of action $\left(a</em>\right)$.</p>
<p>Finally, we compute the dot-product of the query and
keys as action scores for the policy $\pi$ (Eq. 5).</p>
<p>$$
\begin{aligned}
H^{t} &amp; =\operatorname{avg}<em 1="1">{j \in[1, t-1]} \operatorname{Embed}\left(\mathcal{O}^{j}\right) \
H^{A} &amp; =\left[\operatorname{Embed}\left(a</em>\right)\right] \
Q &amp; =\mathcal{M}}^{t}\right), \ldots, \operatorname{Embed}\left(a_{n}^{t<em i="i">{\mathcal{Q}}\left(\operatorname{Embed}(\mathcal{T}), H^{t}, \operatorname{Embed}\left(\mathcal{O}^{t}\right), H^{A}\right) \
K</em>} &amp; =\mathcal{M<em i="i">{\mathcal{K}}\left(\operatorname{Embed}(\mathcal{T}), H^{t}, \operatorname{Embed}\left(\mathcal{O}^{t}\right), \operatorname{Embed}\left(a</em>\right)\right) \
\pi &amp; =\operatorname{softmax}\left(\left[Q \cdot K_{i} \mid i \in \text { all permissible actions }\right]\right)
\end{aligned}
$$}^{t</p>
<h2>4. Experiments and Results</h2>
<p>We present our experiments as follows. First, we explain the environment setup and baselines for our experiments. Then we compare PET to the baselines on different splits of the environment. Finally, we conduct ablation studies and analyze the PET framework part by part. We show that PET generalizes better to human goal specification under efficient behavior cloning training.</p>
<h3>4.1. Experimental Details</h3>
<p>AlfWorld Environment ALFWorld (Shridhar et al., 2020b) is a set of TextWorld environments (Côté et al., 2018b) that are parallels of the ALFRED embodied dataset (Shridhar et al., 2020a). ALFWorld includes 6 task types that each require solving multiple compositional sub-goals. There are 3553 training task instances ({tasktype, object, receptacle, room}), 140 in-distribution evaluation task instances (seen split tasks themselves are novel but take place in rooms seen during training) and 134 out-of-distribution evaluation task instances (unseen split - tasks take place in novels rooms). An example of the task could be: "Rinse the egg to put it in the microwave." Each training instance in AlfWorld comes with an expert, from which we collected our training demonstration.</p>
<p>Human Goal Specification The crowd-sourced human goal specifications for evaluation contain 66 unseen verbs and 189 unseen nouns (Shridhar et al., 2020b). In comparison, the template goals use only 12 ways of goal specification. In addition, the sentence structure for human goal specification is more diverse compared to the template goals. Therefore, human goal experiments are good for testing the generalization of models to out-of-distribution scenarios.</p>
<p>Pre-trained LMs. For the Plan module (sub-task generation), we experimented with the open-source GPT-Neo-2.7B (Black et al., 2021), and an industryscale LLM with 530B parameters (Smith et al., 2022).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Agent (Action Attention). Action Attention block is a transformer-based framework that computes a key K<sup>t</sup> for each permissible action and output action scores as dot-product between key and query Q from the observations.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>seen</th>
<th>unseen</th>
<th>seen</th>
<th>unseen</th>
</tr>
</thead>
<tbody>
<tr>
<td>BUTLER + DAgger* (Shridhar et al., 2020b)</td>
<td>40</td>
<td>35</td>
<td>8</td>
<td>3</td>
</tr>
<tr>
<td>BUTLER + BC (Shridhar et al., 2020b)</td>
<td>10</td>
<td>9</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT (Micheli &amp; Fleuret, 2021)</td>
<td>91</td>
<td>95</td>
<td>42</td>
<td>57</td>
</tr>
<tr>
<td>PET + Action Attention (Ours)</td>
<td>70</td>
<td>67.5</td>
<td>52.5</td>
<td>60</td>
</tr>
</tbody>
</table>
<p>Table 1. Comparison of different models in terms of completion rate per evaluation split (seen and unseen), with and without human annotated goals. PET under-performs GPT on Template goal specifications but generalizes better to human goal specifications. * We include the performance of BUTLER with DAgger for completeness. All other rows are trained without interaction with the environment, MLE for GPT and behavior cloning for BUTLER+BC and PET.</p>
<p>For the Eliminate module (receptacle/object masking), we choose Macaw-11b (Tafjord &amp; Clark, 2021), which is reported to have common sense QA performance on par with GPT3 (Brown et al., 2020) while being orders of magnitudes smaller. We use a decision threshold of 0.4 for Macaw score below which the objects are masked out. For the Track module (progress tracking), we use the same Macaw-11b model as the Eliminate module answer to Yes/No questions.</p>
<h3>Actor Model Design</h3>
<p>Our <strong>Action Attention</strong> agent (M<sub>Q</sub> and M<sub>K</sub>) is a 12-layer transformer with 12 heads and hidden dimension 384. The last layer is then fed into two linear heads to generate K and Q. For embedding of actions and observations, we use pre-trained RoBERTa-large (Liu et al., 2019) with embedding dimension 1024. For sub-task generation, we use ground-truth sub-tasks for training, and generated sub-tasks from Plan module for evaluation.</p>
<h3>Experimental Setup</h3>
<p>Unlike the original benchmark (Shridhar et al., 2020b), we experiment with models trained with behavior cloning. Although Shridhar et al. (2020b) observe that models benefit greatly from DAgger training, DAgger assumes an expert that is well-defined at all possible states, which is inefficient and impractical. In our experiments, training is 100x slower with DAgger compared to behavior cloning (3 weeks for DAgger v.s. 6 hours for Behavior Cloning). In addition, we demonstrate that our models surpass the DAgger training performance of the BUTLER (Shridhar et al., 2020b) agents trained with DAgger, even when our agent does not have the option to interact with the environment.</p>
<h3>Baselines</h3>
<p>Our first baseline is the BUTLER::BRAIN (<strong>BUTLER</strong>) agent (Shridhar et al., 2020b), which consists of an encoder, an aggregator, and a decoder. At each time step t, the encoder takes initial observation s<sup>0</sup>, current observation s<sup>t</sup>, and task string s<sub>task</sub> and generates representation r<sup>t</sup>. The recurrent aggregator combines r<sup>t</sup> with the last recurrent state h<sup>t−1</sup> to produce h<sup>t</sup>, which is then decoded into a string a<sup>t</sup> representing action. In addition, the BUTLER agent uses beam search to get out of stuck conditions in the event of a failed action. Our second baseline <strong>GPT</strong> (Micheli &amp; Fleuret, 2021) is a fine-tuned GPT2-medium on 3553 demonstrations from the AlfWorld training set. Specifically, the GPT is fined-tuned to generate each action step word-by-word to mimic the rule-based expert using the standard maximum likelihood loss.</p>
<h3>4.2. Overall Results on Template and Human Goals</h3>
<p>We compare the performance of action attention assisted by PET with BUTLER (Shridhar et al., 2020b) and fine-tuned GPT (Micheli \&amp; Fleuret, 2021) in Table 1. For human goal specifications, PET outperforms SOTA (GPT) by $25 \%$ on seen and $5 \%$ on the unseen split.</p>
<p>Although PET under-performs GPT on Template goal specifications, GPT requires fine-tuning on fully textbased expert trajectory and thus loses adaptability to different environment settings. Qualitatively, on human goal specification tasks, where the goal specifications are out-of-distribution, GPT often gets stuck repeating the same action after producing a single wrong move. On the other hand, since the Plan module of PET is not trained on the task, it generalizes to the variations for human goal specifications as shown in Section 4.5. Quantitatively, GPT suffers from a relative $50 \%$ performance drop transferring from template to human-goal specifications, whereas PET incurs only a $15 \sim 25 \%$ drop.</p>
<p>The setting closest to PET is BUTLER with behavior cloning (BUTLER + BC). Since BUTLER + BC performs poorly, we also include DAgger training results. Nevertheless, action attention assisted by PET outperforms BUTLER with DAgger by more than 2 x while being much more efficient. (Section 4.1)</p>
<h3>4.3. Ablations for Plan, Eliminate, and Track</h3>
<p>In Table 3, we analyze the contribution of each PET module by sequentially adding each component to the action attention agent on 140 training trajectories sampled from the training set. The data set size is chosen to match the size of the seen validation set, for an efficient and sparse setting. Note that we treat Plan and Track as a single module for this ablation since they cannot work separately.</p>
<p>Adding Plan and Track greatly improves the completion rate relatively by $60 \%$, which provides evidence to our hypothesis that solving some embodied tasks step-by-step reduces the complexity. We observe a relatively insignificant $3 \%$ improvement in absolute performance when adding Eliminate without sub-task tracking. On the other hand, when applying Eliminate to sub-tasks with Plan and Track, we observe more than $60 \%$ relative improvement over just Plan and Track alone. We, therefore, deduce that Plan and Track boost the performance of Eliminate during evaluation, since it is easier to remove irrelevant objects when the objective is more focused on sub-tasks.</p>
<h3>4.4. Automated Analysis of PET modules</h3>
<p>Plan Module We experiment with different LLMs such as GPT2-XL (Radford et al., 2019), GPT-Neo2.7B (Black et al., 2021), and the 530B parameter MT-NLG (Smith et al., 2022) models. Table 2 reports the generation accuracy and the RoBERTa (Liu et al., 2019) embedding cosine similarity against ground-truth sub-tasks. We observe that all LLMs achieve high accuracy on template goal specifications, where there is no variation in sentence structures. For human goal specification, MT-NLG generates subtasks similar to ground truth in terms of embedding similarity, while the other smaller models perform significantly worse.</p>
<p>Eliminate module We evaluate the zero-shot receptacle/object masking performance of Macaw on the three splits of AlfWorld. In Fig 6, we illustrate the AUC curve of the relevance score that the model assigns to the objects v.s. objects that the rule-based expert interacted with when completing each task. Since the Macaw QA model is queried in a zero-shot manner, it demonstrates consistent masking performance on all three splits of the environment, even on the unseen split. In addition, we note that object receptacle accuracy is generally lower than object accuracy because of the counter-intuitive spawning locations described in Section 4.5. In our experiments, a decision threshold of 0.4 has a recall of 0.91 and reduces the number of objects in observation by $40 \%$ on average.</p>
<p>Track module Since sub-task alignment information is not provided by the environment, we explore an alternative performance metric for the detection of the event of completion. Ideally, a sub-task tracker should record the last sub-task as "finished" if and only if the environment is "fully solved" by the expert. As an agreement measure, we report a precision of 0.99 and a recall of 0.78 for Macaw-11B and a precision of 0.96 and a recall of 0.96 for Macaw-large. The larger model (Macaw-11b) is more precise but misses more detection, therefore limiting the theoretical performance to $78 \%$. The smaller model is much less accurate according to human evaluation but does not limit the overall model performance in theory. In our experiments, we find that both models produce similar overall results, which may suggest that the overall results could be improved with LLMs doing better on both precision and recall.</p>
<h3>4.5. Qualitative Analysis</h3>
<p>Plan Module We show two types of failure examples for sub-task generation in Table 4. The first type of error is caused by generating synonyms of the ground truth, and the second type of error is caused by inaccu-</p>
<table>
<thead>
<tr>
<th></th>
<th>Template Goals</th>
<th></th>
<th>Human Goals</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>LLM</td>
<td>seen</td>
<td>unseen</td>
<td>seen</td>
<td>unseen</td>
</tr>
<tr>
<td>GPT-2 (Radford et al., 2019)</td>
<td>$94.29(0.97)$</td>
<td>$87.31(0.94)$</td>
<td>$10.07(0.62)$</td>
<td>$7.98(0.58)$</td>
</tr>
<tr>
<td>GPT-Neo-2.7B (Black et al., 2021)</td>
<td>$\mathbf{9 9 . 2 9 ( 1 . 0 0 )}$</td>
<td>$96.27(0.98)$</td>
<td>$4.70(0.82)$</td>
<td>$9.16(0.80)$</td>
</tr>
<tr>
<td>MT-NLG (Smith et al., 2022)</td>
<td>$98.57(0.99)$</td>
<td>$\mathbf{1 0 0 ( 1 . 0 0 )}$</td>
<td>$\mathbf{4 0 . 0 4 ( 0 . 9 4 )}$</td>
<td>$\mathbf{4 9 . 3 ( 0 . 9 4 )}$</td>
</tr>
</tbody>
</table>
<p>Table 2. Evaluation of different LLMs for Plan module in terms of accuracy and RoBERTa embedding cosine similarity (in brackets) against ground-truth sub-tasks, per evaluation split (seen and unseen), with and without human annotated goals. The MT-NLG with 530B parameters achieves the overall best performance on all dataset splits and greatly exceeds the performance of smaller models on hard tasks with human goal specification. In addition, MT-NLG generates sub-tasks with almost perfect embedding similarity for all tasks.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Plot of AUC scores of zero-shot relevance identification across all tasks in the Alfworld-Thor environment, with the Macow-11b model. The ground truth is obtained as receptacles/objects accessed by the rule-based expert. Top: Receptacle relevance identification. Bottom: Object relevance identification. The QA model achieves an average AUC-ROC score of 65 for receptacles and 76 on objects.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model Ablations</th>
<th style="text-align: right;">seen</th>
<th style="text-align: right;">unseen</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Action Attention</td>
<td style="text-align: right;">25</td>
<td style="text-align: right;">9</td>
</tr>
<tr>
<td style="text-align: left;">Action Attention + Eliminate</td>
<td style="text-align: right;">25</td>
<td style="text-align: right;">11</td>
</tr>
<tr>
<td style="text-align: left;">Action Attention + Plan \&amp; Track</td>
<td style="text-align: right;">35</td>
<td style="text-align: right;">15</td>
</tr>
<tr>
<td style="text-align: left;">Action Attention + PET</td>
<td style="text-align: right;">52.5</td>
<td style="text-align: right;">27.5</td>
</tr>
</tbody>
</table>
<p>Table 3. Comparison of different ablations of PET trained on a sampled set of 140 demonstrations from the training set, in terms of completion rate per evaluation split (seen and unseen). Applying Eliminate module alone has an insignificant effect on overall performance compared to Plan \&amp; Track. However, applying Eliminate module on sub-tasks together with Plan \&amp; Track results in a much more significant performance improvement.
racies in the human goal specifications. Note that our Action Attention framework uses RoBERTa (Liu et al., 2019) embedding for sub-tasks, known to be robust to synonym variations.</p>
<p>Eliminate Module We observe that the main source of elimination error occurs when the module
incorrectly masks a receptacle that contains the object of interest so the agent fails to find such receptacles. This is often because some objects in the AI2Thor simulator do not spawn according to common sense. As noted in the documentation of the environment ${ }^{2}$, objects like Apple or Egg has a chance of spawning in unexpected receptacles like GarbageCan, or TVStand. However, such generations in AI2Thor are unlikely in real deployment; thus, the "mistakes" of our Eliminate module are reasonable.</p>
<p>Track Module Experimentally, we find that subtask planning/tracking is particularly helpful for tasks that require counting procedures. As shown in Table 7?, PET breaks the task of "Place two soapbar in cabinet" into two repeating set of sub-tasks: "take soapbar $\rightarrow$ place soapbar in/on cabinet". Sub-task planning and tracking, therefore, simplify the hard problem of counting.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Human Goal Specification Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task</td>
<td style="text-align: left;">Chill a cup and place it in the cabinet.</td>
</tr>
<tr>
<td style="text-align: left;">GT</td>
<td style="text-align: left;">cool the mug $\rightarrow$ place the mug in/on coffeem-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">chine</td>
</tr>
<tr>
<td style="text-align: left;">Gen</td>
<td style="text-align: left;">chill the mug $\rightarrow$ return the mug to coffeem-</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">chine</td>
</tr>
<tr>
<td style="text-align: left;">Task</td>
<td style="text-align: left;">Take the pencil from the desk, put it on the</td>
</tr>
<tr>
<td style="text-align: left;">GT</td>
<td style="text-align: left;">other side of the desk</td>
</tr>
<tr>
<td style="text-align: left;">Gen</td>
<td style="text-align: left;">take a pencil $\rightarrow$ place the pencil in/on shelf</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">pick up the white pencil on the desk $\rightarrow$ put the</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">white pencil on another spot on the desk</td>
</tr>
</tbody>
</table>
<p>Table 4. Failure examples from the Plan module on human goal specifications (Task), ground-truth (GT) v.s. generated (Gen). In the first example, generated plan differs from the ground truth but the meaning agrees. In the second example, the generated plan largely differs from the ground truth due to the mistake in human goal specification "another side on the desk" instead of "shelf".</p>
<h2>5. Conclusion, Limitations, and Future Work</h2>
<p>In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no fine-tuning and is designed to be compatible with any goal-conditional embodied agents.</p>
<p>In our experiments, we combine PET with a novel Action Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to fit the training set tasks, it demonstrates better generalization to unseen human goal specification tasks. Finally, our ablation studies show the Plan and Track modules together improve the performance of Eliminate module to achieve the best performance.</p>
<p>Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve effectiveness.</p>
<p>One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit finished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic replanning to address this limitation or explore other ways in which LLMs can assist the learning of the policy (i.e., reading an instruction manual about the environment).</p>
<h2>References</h2>
<p>Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL https: //arxiv.org/abs/2204.01691.</p>
<p>Akakzia, A., Colas, C., Oudeyer, P.-Y., Chetouani, M., and Sigaud, O. Grounding language to autonomously-acquired skills via goal generation. arXiv preprint arXiv:2006.07185, 2020.</p>
<p>Andreas, J., Klein, D., and Levine, S. Modular multitask reinforcement learning with policy sketches. In International Conference on Machine Learning, pp. 166-175. PMLR, 2017.</p>
<p>Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/10.5281/zenodo.5297715. If you use this software, please cite it using these metadata.</p>
<p>Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for highlevel natural language instruction execution, 2021. URL https://arxiv.org/abs/2107.05612.</p>
<p>Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X. L., Li, X., Ma,</p>
<p>T., Malik, A., Manning, C. D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J. C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I., Park, J. S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Ré, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A. W., Tramèr, F., Wang, R. E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S. M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng, L., Zhou, K., and Liang, P. On the opportunities and risks of foundation models, 2021. URL https://arxiv.org/abs/2108.07258.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Chaplot, D. S., Gandhi, D., Gupta, A., and Salakhutdinov, R. Object goal navigation using goal-oriented semantic exploration, 2020. URL https://arxiv. org/abs/2007.00643.</p>
<p>Cideron, G., Seurin, M., Strub, F., and Pietquin, O. Higher: Improving instruction following with hindsight generation for experience replay. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 225-232. IEEE, 2020.</p>
<p>Côté, M.-A., Kádár, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, L. E., Adada, M., et al. Textworld: A learning environment for text-based games. In Workshop on Computer Games, pp. 41-75. Springer, 2018a.</p>
<p>Côté, M.-A., Kádár, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, L. E., Adada, M., et al. Textworld: A learning environment for text-based games. In Workshop on Computer Games, pp. 41-75. Springer, 2018b.</p>
<p>Fulda, N., Ricks, D., Murdoch, B., and Wingate, D. What can you do with a rock? affordance extraction via word embeddings. arXiv preprint arXiv:1703.03429, 2017.</p>
<p>Goyal, P., Niekum, S., and Mooney, R. Pixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards. In Conference on Robot Learning, pp. 485-497. PMLR, 2021.</p>
<p>He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., and Ostendorf, M. Deep reinforcement learning with
a natural language action space. arXiv preprint arXiv:1511.04636, 2015.</p>
<p>Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents, 2022a. URL https://arxiv.org/abs/2201.07207.</p>
<p>Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Brown, N., Jackson, T., Luu, L., Levine, S., Hausman, K., and Ichter, B. Inner monologue: Embodied reasoning through planning with language models, 2022b. URL https: //arxiv.org/abs/2207.05608.</p>
<p>Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine, S., and Finn, C. Bc-z: Zeroshot task generalization with robotic imitation learning. In Conference on Robot Learning, pp. 991-1002. PMLR, 2022.</p>
<p>Jiang, Y., Gu, S. S., Murphy, K. P., and Finn, C. Language as an abstraction for hierarchical deep reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Kollar, T., Tellex, S., Roy, D., and Roy, N. Toward understanding natural language directions. In 2010 5th ACM/IEEE International Conference on HumanRobot Interaction (HRI), pp. 259-266. IEEE, 2010.</p>
<p>Lin, B. Y., Huang, C., Liu, Q., Gu, W., Sommerer, S., and Ren, X. On grounded planning for embodied tasks with language models. arXiv preprint arXiv:2209.00465, 2022.</p>
<p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>MacMahon, M., Stankiewicz, B., and Kuipers, B. Walk the talk: Connecting language, knowledge, and action in route instructions. Def, 2(6):4, 2006.</p>
<p>Mei, H., Bansal, M., and Walter, M. R. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.</p>
<p>Micheli, V. and Fleuret, F. Language models are fewshot butlers. arXiv preprint arXiv:2104.07972, 2021.</p>
<p>Min, S. Y., Chaplot, D. S., Ravikumar, P., Bisk, Y., and Salakhutdinov, R. Film: Following instructions in language with modular methods, 2021.</p>
<p>Misra, D., Langford, J., and Artzi, Y. Mapping instructions and visual observations to actions with reinforcement learning. arXiv preprint arXiv:1704.08795, 2017.</p>
<p>Nair, S., Mitchell, E., Chen, K., Savarese, S., Finn, C., et al. Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. In Conference on Robot Learning, pp. 1303-1315. PMLR, 2022.</p>
<p>Oh, J., Singh, S., Lee, H., and Kohli, P. Zero-shot task generalization with multi-task deep reinforcement learning. In International Conference on Machine Learning, pp. 2661-2670. PMLR, 2017.</p>
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019.</p>
<p>See, A., Liu, P. J., and Manning, C. D. Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368, 2017.</p>
<p>Sharma, P., Torralba, A., and Andreas, J. Skill induction and planning with latent language. arXiv preprint arXiv:2110.01517, 2021.</p>
<p>Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R., Zettlemoyer, L., and Fox, D. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10740-10749, 2020a.</p>
<p>Shridhar, M., Yuan, X., Côté, M.-A., Bisk, Y., Trischler, A., and Hausknecht, M. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020b.</p>
<p>Shridhar, M., Manuelli, L., and Fox, D. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, pp. 894-906. PMLR, 2022.</p>
<p>Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., Liu, Z., Prabhumoye, S., Zerveas, G., Korthikanti, V., Zheng, E., Child, R., Aminabadi, R. Y., Bernauer, J., Song, X., Shoeybi, M., He, Y., Houston, M., Tiwary, S., and Catanzaro, B. Using deepspeed and megatron to train megatron-turing NLG 530b, A large-scale generative language model. CoRR, abs/2201.11990, 2022. URL https://arxiv.org/abs/2201.11990.</p>
<p>Song, C. H., Wu, J., Washington, C., Sadler, B. M., Chao, W.-L., and Su, Y. Llm-planner: Few-shot grounded planning for embodied agents with large
language models. arXiv preprint arXiv:2212.04088, 2022.</p>
<p>Stepputtis, S., Campbell, J., Phielipp, M., Lee, S., Baral, C., and Ben Amor, H. Language-conditioned imitation learning for robot manipulation tasks. $A d$ vances in Neural Information Processing Systems, 33:13139-13150, 2020.</p>
<p>Tafjord, O. and Clark, P. General-purpose question-answering with macaw. arXiv preprint arXiv:2109.02593, 2021.</p>
<p>Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S., and Roy, N. Understanding natural language commands for robotic navigation and mobile manipulation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 25, pp. $1507-1514,2011$.</p>
<p>Yao, S., Rao, R., Hausknecht, M., and Narasimhan, K. Keep calm and explore: Language models for action generation in text-based games, 2020. URL https://arxiv.org/abs/2010.02903.</p>
<p>Zahavy, T., Haroush, M., Merlis, N., Mankowitz, D. J., and Mannor, S. Learn what not to learn: Action elimination with deep reinforcement learning. Advances in neural information processing systems, 31, 2018.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ ai2thor.allenai.org/ithor/documentation/objects/objecttypes/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
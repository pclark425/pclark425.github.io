<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7706 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7706</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7706</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-257496621</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.07304v1.pdf" target="_blank">Algorithmic Ghost in the Research Shell: Large Language Models and Academic Knowledge Creation in Management Research</a></p>
                <p><strong>Paper Abstract:</strong> The paper looks at the role of large language models in academic knowledge creation based on a scoping review (2018 to January 2023) of how researchers have previously used the language model GPT to assist in the performance of academic knowledge creation tasks beyond data analysis. These tasks include writing, editing, reviewing, dataset creation and curation, which have been difficult to perform using earlier ML tools. Based on a synthesis of these papers, this study identifies pathways for a future academic research landscape that incorporates wider usage of large language models based on the current modes of adoption in published articles as a Co-Writer, Research Assistant and Respondent.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7706.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7706.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Co-Writer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT as Co-Writer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A usage mode in which a large language model (GPT) is deployed to synthesize related work, generate literature summaries, draft academic text, and expand exploration of problem spaces across disciplines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Algorithmic Ghost in the Research Shell</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Nigel Williams, Stanislav Ivanov, Dimitrios Buhalis</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GPT as Co-Writer</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The LLM is prompted (or fine-tuned on domain text) to read, summarise and synthesise collections of scholarly texts and to generate draft manuscript sections, literature-review summaries, and argument framings that researchers then edit and validate.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Full-text articles and abstracts / large collections of scholarly text (as available to the model); domain-specific corpora when fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Textual synthesis: literature summaries, draft manuscript sections, integrated reviews and conceptual synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Prompt-based generation and optional domain-specific fine-tuning; iterative prompt refinement (human-in-the-loop)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT family (e.g., GPT-3 / ChatGPT as discussed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B parameters (GPT-3, as cited in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>General large web and scholarly corpora as used by GPT; domain-specific corpora when adapted (not specified in detail)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Qualitative reports that GPT can summarise and synthesise knowledge across fields, accelerate initial literature scanning and draft creation, and expand problem-space exploration; cited study-level findings (e.g., equivalence in some tasks) are mixed and domain-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authorship attribution concerns, difficulty detecting AI-generated text, risk of fabricated or misleading content, embedding of training-data biases, and general cautions about treating outputs as human-level understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7706.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7706.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResearchAssistant</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT as Research Assistant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A usage mode where GPT supports literature search, data preparation/curation, item/scale generation, exploratory analysis design and other preparatory research tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Algorithmic Ghost in the Research Shell</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Nigel Williams, Stanislav Ivanov, Dimitrios Buhalis</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GPT as Research Assistant</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>LLMs are used to automate or assist tasks such as searching and summarising literature, cleaning and formatting unstructured data, generating candidate survey/scale items, proposing methodology alternatives, and performing exploratory analyses given prompts and few examples.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Unstructured text (papers, notes, social media posts), datasets requiring wrangling, small example sets for few-shot operations</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Cleaned/structured data, summaries, candidate measurement items, exploratory analyses and suggested protocols</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Few-shot prompting, demonstration-based examples, domain-adaptive fine-tuning in some studies, and recursive self-application (using the model on its own outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT family (examples discussed include GPT-3 / ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B parameters (GPT-3, as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Not consistently specified; uses researcher-provided unstructured datasets and domain texts; some studies adapt the model on domain text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported successful applications include automated data cleaning/formatting, generation of scale items comparable to traditional methods (Hernandez & Nie), and faster exploratory workflows; results reported qualitatively or in limited task comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Potential for incorrect inferences during data filling, relevance errors, propagation of biases from training data, need for human verification, and limits on generalisability of simulated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7706.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7706.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Respondent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT as Respondent / Simulated Participant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A usage mode employing GPT to generate synthetic participant responses, simulate group interactions, act as surrogate subjects in experiments, or provide questionnaire/interview answers for research design and testing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Algorithmic Ghost in the Research Shell</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Nigel Williams, Stanislav Ivanov, Dimitrios Buhalis</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2023</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GPT as Respondent</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Researchers prompt the LLM with experimental scenarios, interview questions or role descriptions to generate synthetic responses that can be used for piloting instruments, augmenting datasets, or evaluating system behaviours.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Prompted experimental scenarios, interview questions, survey items (no external human data required)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Synthetic respondent data: interview transcripts, survey responses, simulated group interaction narratives</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Role-based prompting, scenario specification, iterative prompt refinement; sometimes few-shot examples for style/control</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT family (GPT-3 / ChatGPT referenced in examples)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B parameters (GPT-3, as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>None required beyond the model's pretraining; some studies compare outputs to human datasets for validation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used to replicate economic experiments, generate PANAS-style affect reports, act as surrogate users in HCI evaluations, and create synthetic sensitive-topic respondents for training; results show LLMs can produce plausible, richly descriptive synthetic responses but not true human-equivalent perceptions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Synthetic responses do not equal real human perceptions or emotions; limited external validity; risk of over-generalisation if used as substitute for human data.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7706.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7706.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZeroGen / TAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ZeroGen: Zero-shot dataset generation and Tiny Task Model (TAM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses an LLM to generate synthetic datasets which are then used to train compact task-specific models (Tiny Task Models) without requiring external labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Zerogen: Efficient zero-shot learning via dataset generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Zerogen: Efficient zero-shot learning via dataset generation</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>J. Ye et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ZeroGen / Tiny Task Model (TAM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The LLM is prompted to produce a synthetic labeled dataset for a target task; that synthetic data is used to train a small model (TAM) that performs the specific analysis without external annotated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Prompts describing task plus the LLM's pretraining knowledge; no external labeled dataset required</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Synthetic labeled dataset and a trained compact task model (TAM)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Zero-shot dataset generation via task-description prompts; synthetic-label generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM (specific model not detailed in the scoping review's summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Synthetic datasets generated by the LLM; no standard external corpus specified in the scoping review</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Described as enabling creation of task-specific models without external training data (enables zero-shot learning workflows); specifics of quantitative performance not reported in the reviewed summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Risks include synthetic-data biases and domain mismatch between synthetic data and real-world distributions; details not fully enumerated in the scoping review.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7706.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7706.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExperimentMaker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experiment Maker</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tool to facilitate creation of experiments with GPT-3, enabling researchers to design and run GPT-based experimental setups more easily.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Experiment Maker: a Tool to create Experiments with GPT-3 easily</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Experiment Maker: a Tool to create Experiments with GPT-3 easily</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>P. Bellan, M. Dragoni, C. Ghidini</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Experiment Maker</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Framework/tooling that helps researchers specify, run and collect outputs from GPT-3 driven experiments (e.g., prompts, roles, experimental conditions) to study model behaviour or use models as experimental agents.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Experimental prompt specifications and condition descriptions; GPT-3 as the executor</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Model-generated experimental responses and logs for analysis</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Systematised prompt templates and experimental condition scripting for GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Not applicable; uses GPT-3 as pre-trained model and researcher-defined prompts/inputs</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Provides tooling to streamline GPT-3 experiment creation; the scoping review references it as an example of systems enabling GPT-driven experimental research.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>The review does not provide detailed empirical evaluation; general concerns about reproducibility and dependence on proprietary APIs apply.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7706.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7706.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReadReviseRepeat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Read, Revise, Repeat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-in-the-loop iterative text revision system where models generate revision suggestions and humans guide successive improvements to a text draft.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>W. Du, Z. M. Kim, V. Raheja, D. Kumar, D. Kang</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Read–Revise–Repeat (human-in-the-loop LLM revision)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>An interactive workflow where an LLM proposes edits/revisions to text, the human accepts/refines them, and the cycle repeats to improve clarity, structure or argumentation, useful for drafting and refining scholarly syntheses.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Draft manuscript text, paragraphs or outlines</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Revised text drafts and suggested edits (incrementally improved)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Human-in-the-loop iterative prompting and revision cycles</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (specific model not detailed in the scoping review summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Not specified (operates on user-provided drafts)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Demonstration of accelerated iterative text revision workflows; specific quantitative evaluations not reported in the scoping review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Dependence on human oversight to avoid introducing errors or misleading content; evaluation details limited.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7706.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7706.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ComputationalInflection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Computational Inflection for Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual/methodological proposal arguing that computational methods (including LLMs) can enable new forms of scientific discovery such as hypothesis generation and automated synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Computational Inflection for Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>A Computational Inflection for Scientific Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>T. Hope, D. Downey, O. Etzioni, D. S. Weld, E. Horvitz</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Computational Inflection for Scientific Discovery (conceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Advocates for computational tooling — including LLMs and related systems — to assist in discovery tasks (hypothesis generation, literature synthesis, experiment design), describing architectures and research directions rather than a single operational method.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Scholarly corpora, data archives, experimental logs (conceptual; not single implemented pipeline in the scoping review summary)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Hypotheses, research agendas, synthesis artifacts (conceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Not specified in detail in the scoping review; advocates integration of retrieval-augmented and structured computational pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Position/agenda-setting contribution; outlines possibilities and research directions rather than empirical results in the scoping-review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High-level proposal; practical challenges such as validation, reproducibility, and trustworthiness of machine-generated discoveries are acknowledged in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7706.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7706.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WhatIf-Engine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Language Models as What-If?-Engines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using LLMs as 'what-if' engines to explore counterfactuals and hypothetical scenarios for HCI and research design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural Language Models as What If?-Engines for HCI Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Neural Language Models as What If?-Engines for HCI Research</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>P. Hämäläinen, M. Tavast, A. Kunnari</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>What-If Engine (LLM-based hypothetical simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>LLMs are prompted to generate plausible hypothetical behaviours and counterfactual responses to support design exploration and to anticipate user/system interactions without collecting real-world data.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Prompted hypothetical scenarios and context descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Counterfactual narratives and simulated user/system behaviours</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Scenario and role prompting; few-shot examples for behaviour shaping</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (specific model not specified in scoping review summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Shown to be useful as exploratory tools for HCI research design and scenario testing; empirical validation context-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Outputs are model-derived plausibilities, not real user data; caution needed when generalising.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Zerogen: Efficient zero-shot learning via dataset generation <em>(Rating: 2)</em></li>
                <li>A Computational Inflection for Scientific Discovery <em>(Rating: 2)</em></li>
                <li>Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision <em>(Rating: 2)</em></li>
                <li>Experiment Maker: a Tool to create Experiments with GPT-3 easily <em>(Rating: 2)</em></li>
                <li>Neural Language Models as What If?-Engines for HCI Research <em>(Rating: 2)</em></li>
                <li>Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision <em>(Rating: 1)</em></li>
                <li>Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus <em>(Rating: 1)</em></li>
                <li>The AI-IP: Minimising the guesswork of personality scale item development through artificial intelligence <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7706",
    "paper_id": "paper-257496621",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "Co-Writer",
            "name_full": "GPT as Co-Writer",
            "brief_description": "A usage mode in which a large language model (GPT) is deployed to synthesize related work, generate literature summaries, draft academic text, and expand exploration of problem spaces across disciplines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Algorithmic Ghost in the Research Shell",
            "authors": "Nigel Williams, Stanislav Ivanov, Dimitrios Buhalis",
            "year": 2023,
            "method_name": "GPT as Co-Writer",
            "method_description": "The LLM is prompted (or fine-tuned on domain text) to read, summarise and synthesise collections of scholarly texts and to generate draft manuscript sections, literature-review summaries, and argument framings that researchers then edit and validate.",
            "input_type": "Full-text articles and abstracts / large collections of scholarly text (as available to the model); domain-specific corpora when fine-tuned",
            "output_type": "Textual synthesis: literature summaries, draft manuscript sections, integrated reviews and conceptual synthesis",
            "prompting_technique": "Prompt-based generation and optional domain-specific fine-tuning; iterative prompt refinement (human-in-the-loop)",
            "model_name": "GPT family (e.g., GPT-3 / ChatGPT as discussed)",
            "model_size": "175B parameters (GPT-3, as cited in the paper)",
            "datasets_used": "General large web and scholarly corpora as used by GPT; domain-specific corpora when adapted (not specified in detail)",
            "evaluation_metric": null,
            "reported_results": "Qualitative reports that GPT can summarise and synthesise knowledge across fields, accelerate initial literature scanning and draft creation, and expand problem-space exploration; cited study-level findings (e.g., equivalence in some tasks) are mixed and domain-specific.",
            "limitations": "Authorship attribution concerns, difficulty detecting AI-generated text, risk of fabricated or misleading content, embedding of training-data biases, and general cautions about treating outputs as human-level understanding.",
            "counterpoint": true,
            "uuid": "e7706.0"
        },
        {
            "name_short": "ResearchAssistant",
            "name_full": "GPT as Research Assistant",
            "brief_description": "A usage mode where GPT supports literature search, data preparation/curation, item/scale generation, exploratory analysis design and other preparatory research tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Algorithmic Ghost in the Research Shell",
            "authors": "Nigel Williams, Stanislav Ivanov, Dimitrios Buhalis",
            "year": 2023,
            "method_name": "GPT as Research Assistant",
            "method_description": "LLMs are used to automate or assist tasks such as searching and summarising literature, cleaning and formatting unstructured data, generating candidate survey/scale items, proposing methodology alternatives, and performing exploratory analyses given prompts and few examples.",
            "input_type": "Unstructured text (papers, notes, social media posts), datasets requiring wrangling, small example sets for few-shot operations",
            "output_type": "Cleaned/structured data, summaries, candidate measurement items, exploratory analyses and suggested protocols",
            "prompting_technique": "Few-shot prompting, demonstration-based examples, domain-adaptive fine-tuning in some studies, and recursive self-application (using the model on its own outputs)",
            "model_name": "GPT family (examples discussed include GPT-3 / ChatGPT)",
            "model_size": "175B parameters (GPT-3, as cited)",
            "datasets_used": "Not consistently specified; uses researcher-provided unstructured datasets and domain texts; some studies adapt the model on domain text.",
            "evaluation_metric": null,
            "reported_results": "Reported successful applications include automated data cleaning/formatting, generation of scale items comparable to traditional methods (Hernandez & Nie), and faster exploratory workflows; results reported qualitatively or in limited task comparisons.",
            "limitations": "Potential for incorrect inferences during data filling, relevance errors, propagation of biases from training data, need for human verification, and limits on generalisability of simulated outputs.",
            "counterpoint": true,
            "uuid": "e7706.1"
        },
        {
            "name_short": "Respondent",
            "name_full": "GPT as Respondent / Simulated Participant",
            "brief_description": "A usage mode employing GPT to generate synthetic participant responses, simulate group interactions, act as surrogate subjects in experiments, or provide questionnaire/interview answers for research design and testing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Algorithmic Ghost in the Research Shell",
            "authors": "Nigel Williams, Stanislav Ivanov, Dimitrios Buhalis",
            "year": 2023,
            "method_name": "GPT as Respondent",
            "method_description": "Researchers prompt the LLM with experimental scenarios, interview questions or role descriptions to generate synthetic responses that can be used for piloting instruments, augmenting datasets, or evaluating system behaviours.",
            "input_type": "Prompted experimental scenarios, interview questions, survey items (no external human data required)",
            "output_type": "Synthetic respondent data: interview transcripts, survey responses, simulated group interaction narratives",
            "prompting_technique": "Role-based prompting, scenario specification, iterative prompt refinement; sometimes few-shot examples for style/control",
            "model_name": "GPT family (GPT-3 / ChatGPT referenced in examples)",
            "model_size": "175B parameters (GPT-3, as cited)",
            "datasets_used": "None required beyond the model's pretraining; some studies compare outputs to human datasets for validation",
            "evaluation_metric": null,
            "reported_results": "Used to replicate economic experiments, generate PANAS-style affect reports, act as surrogate users in HCI evaluations, and create synthetic sensitive-topic respondents for training; results show LLMs can produce plausible, richly descriptive synthetic responses but not true human-equivalent perceptions.",
            "limitations": "Synthetic responses do not equal real human perceptions or emotions; limited external validity; risk of over-generalisation if used as substitute for human data.",
            "counterpoint": true,
            "uuid": "e7706.2"
        },
        {
            "name_short": "ZeroGen / TAM",
            "name_full": "ZeroGen: Zero-shot dataset generation and Tiny Task Model (TAM)",
            "brief_description": "A method that uses an LLM to generate synthetic datasets which are then used to train compact task-specific models (Tiny Task Models) without requiring external labeled data.",
            "citation_title": "Zerogen: Efficient zero-shot learning via dataset generation",
            "mention_or_use": "mention",
            "paper_title": "Zerogen: Efficient zero-shot learning via dataset generation",
            "authors": "J. Ye et al.",
            "year": 2022,
            "method_name": "ZeroGen / Tiny Task Model (TAM)",
            "method_description": "The LLM is prompted to produce a synthetic labeled dataset for a target task; that synthetic data is used to train a small model (TAM) that performs the specific analysis without external annotated datasets.",
            "input_type": "Prompts describing task plus the LLM's pretraining knowledge; no external labeled dataset required",
            "output_type": "Synthetic labeled dataset and a trained compact task model (TAM)",
            "prompting_technique": "Zero-shot dataset generation via task-description prompts; synthetic-label generation",
            "model_name": "LLM (specific model not detailed in the scoping review's summary)",
            "model_size": null,
            "datasets_used": "Synthetic datasets generated by the LLM; no standard external corpus specified in the scoping review",
            "evaluation_metric": null,
            "reported_results": "Described as enabling creation of task-specific models without external training data (enables zero-shot learning workflows); specifics of quantitative performance not reported in the reviewed summary.",
            "limitations": "Risks include synthetic-data biases and domain mismatch between synthetic data and real-world distributions; details not fully enumerated in the scoping review.",
            "counterpoint": null,
            "uuid": "e7706.3"
        },
        {
            "name_short": "ExperimentMaker",
            "name_full": "Experiment Maker",
            "brief_description": "A tool to facilitate creation of experiments with GPT-3, enabling researchers to design and run GPT-based experimental setups more easily.",
            "citation_title": "Experiment Maker: a Tool to create Experiments with GPT-3 easily",
            "mention_or_use": "mention",
            "paper_title": "Experiment Maker: a Tool to create Experiments with GPT-3 easily",
            "authors": "P. Bellan, M. Dragoni, C. Ghidini",
            "year": 2022,
            "method_name": "Experiment Maker",
            "method_description": "Framework/tooling that helps researchers specify, run and collect outputs from GPT-3 driven experiments (e.g., prompts, roles, experimental conditions) to study model behaviour or use models as experimental agents.",
            "input_type": "Experimental prompt specifications and condition descriptions; GPT-3 as the executor",
            "output_type": "Model-generated experimental responses and logs for analysis",
            "prompting_technique": "Systematised prompt templates and experimental condition scripting for GPT-3",
            "model_name": "GPT-3",
            "model_size": "175B parameters",
            "datasets_used": "Not applicable; uses GPT-3 as pre-trained model and researcher-defined prompts/inputs",
            "evaluation_metric": null,
            "reported_results": "Provides tooling to streamline GPT-3 experiment creation; the scoping review references it as an example of systems enabling GPT-driven experimental research.",
            "limitations": "The review does not provide detailed empirical evaluation; general concerns about reproducibility and dependence on proprietary APIs apply.",
            "counterpoint": null,
            "uuid": "e7706.4"
        },
        {
            "name_short": "ReadReviseRepeat",
            "name_full": "Read, Revise, Repeat",
            "brief_description": "A human-in-the-loop iterative text revision system where models generate revision suggestions and humans guide successive improvements to a text draft.",
            "citation_title": "Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision",
            "mention_or_use": "mention",
            "paper_title": "Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision",
            "authors": "W. Du, Z. M. Kim, V. Raheja, D. Kumar, D. Kang",
            "year": 2022,
            "method_name": "Read–Revise–Repeat (human-in-the-loop LLM revision)",
            "method_description": "An interactive workflow where an LLM proposes edits/revisions to text, the human accepts/refines them, and the cycle repeats to improve clarity, structure or argumentation, useful for drafting and refining scholarly syntheses.",
            "input_type": "Draft manuscript text, paragraphs or outlines",
            "output_type": "Revised text drafts and suggested edits (incrementally improved)",
            "prompting_technique": "Human-in-the-loop iterative prompting and revision cycles",
            "model_name": "Large language models (specific model not detailed in the scoping review summary)",
            "model_size": null,
            "datasets_used": "Not specified (operates on user-provided drafts)",
            "evaluation_metric": null,
            "reported_results": "Demonstration of accelerated iterative text revision workflows; specific quantitative evaluations not reported in the scoping review summary.",
            "limitations": "Dependence on human oversight to avoid introducing errors or misleading content; evaluation details limited.",
            "counterpoint": null,
            "uuid": "e7706.5"
        },
        {
            "name_short": "ComputationalInflection",
            "name_full": "A Computational Inflection for Scientific Discovery",
            "brief_description": "A conceptual/methodological proposal arguing that computational methods (including LLMs) can enable new forms of scientific discovery such as hypothesis generation and automated synthesis.",
            "citation_title": "A Computational Inflection for Scientific Discovery",
            "mention_or_use": "mention",
            "paper_title": "A Computational Inflection for Scientific Discovery",
            "authors": "T. Hope, D. Downey, O. Etzioni, D. S. Weld, E. Horvitz",
            "year": 2022,
            "method_name": "Computational Inflection for Scientific Discovery (conceptual)",
            "method_description": "Advocates for computational tooling — including LLMs and related systems — to assist in discovery tasks (hypothesis generation, literature synthesis, experiment design), describing architectures and research directions rather than a single operational method.",
            "input_type": "Scholarly corpora, data archives, experimental logs (conceptual; not single implemented pipeline in the scoping review summary)",
            "output_type": "Hypotheses, research agendas, synthesis artifacts (conceptual)",
            "prompting_technique": "Not specified in detail in the scoping review; advocates integration of retrieval-augmented and structured computational pipelines",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Position/agenda-setting contribution; outlines possibilities and research directions rather than empirical results in the scoping-review summary.",
            "limitations": "High-level proposal; practical challenges such as validation, reproducibility, and trustworthiness of machine-generated discoveries are acknowledged in the literature.",
            "counterpoint": null,
            "uuid": "e7706.6"
        },
        {
            "name_short": "WhatIf-Engine",
            "name_full": "Neural Language Models as What-If?-Engines",
            "brief_description": "Using LLMs as 'what-if' engines to explore counterfactuals and hypothetical scenarios for HCI and research design.",
            "citation_title": "Neural Language Models as What If?-Engines for HCI Research",
            "mention_or_use": "mention",
            "paper_title": "Neural Language Models as What If?-Engines for HCI Research",
            "authors": "P. Hämäläinen, M. Tavast, A. Kunnari",
            "year": 2022,
            "method_name": "What-If Engine (LLM-based hypothetical simulation)",
            "method_description": "LLMs are prompted to generate plausible hypothetical behaviours and counterfactual responses to support design exploration and to anticipate user/system interactions without collecting real-world data.",
            "input_type": "Prompted hypothetical scenarios and context descriptions",
            "output_type": "Counterfactual narratives and simulated user/system behaviours",
            "prompting_technique": "Scenario and role prompting; few-shot examples for behaviour shaping",
            "model_name": "LLMs (specific model not specified in scoping review summary)",
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Shown to be useful as exploratory tools for HCI research design and scenario testing; empirical validation context-dependent.",
            "limitations": "Outputs are model-derived plausibilities, not real user data; caution needed when generalising.",
            "counterpoint": null,
            "uuid": "e7706.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Zerogen: Efficient zero-shot learning via dataset generation",
            "rating": 2,
            "sanitized_title": "zerogen_efficient_zeroshot_learning_via_dataset_generation"
        },
        {
            "paper_title": "A Computational Inflection for Scientific Discovery",
            "rating": 2,
            "sanitized_title": "a_computational_inflection_for_scientific_discovery"
        },
        {
            "paper_title": "Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision",
            "rating": 2,
            "sanitized_title": "read_revise_repeat_a_system_demonstration_for_humanintheloop_iterative_text_revision"
        },
        {
            "paper_title": "Experiment Maker: a Tool to create Experiments with GPT-3 easily",
            "rating": 2,
            "sanitized_title": "experiment_maker_a_tool_to_create_experiments_with_gpt3_easily"
        },
        {
            "paper_title": "Neural Language Models as What If?-Engines for HCI Research",
            "rating": 2,
            "sanitized_title": "neural_language_models_as_what_ifengines_for_hci_research"
        },
        {
            "paper_title": "Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision",
            "rating": 1,
            "sanitized_title": "read_revise_repeat_a_system_demonstration_for_humanintheloop_iterative_text_revision"
        },
        {
            "paper_title": "Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus",
            "rating": 1,
            "sanitized_title": "large_language_models_as_simulated_economic_agents_what_can_we_learn_from_homo_silicus"
        },
        {
            "paper_title": "The AI-IP: Minimising the guesswork of personality scale item development through artificial intelligence",
            "rating": 1,
            "sanitized_title": "the_aiip_minimising_the_guesswork_of_personality_scale_item_development_through_artificial_intelligence"
        }
    ],
    "cost": 0.01607275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Algorithmic Ghost in the Research Shell</p>
<p>Nigel Williams nigel.williams@port.ac.uk 
Stanislav Ivanov stanislav.ivanov@vumk.eu@stanislavivanov.com 
Dimitrios Buhalis dbuhalis@bournemouth.ac.uk </p>
<p>Systems and People
Faculty of Business and Law
The School of Organisations
University of Portsmouth
UK</p>
<p>Professor and Vice-Rector (Research)
Varna University of Management
13A Oborishte str9000VarnaBulgaria</p>
<p>Business School
Zangador Research Institute
9010VarnaBulgaria</p>
<p>Bournemouth University
UK</p>
<p>Algorithmic Ghost in the Research Shell
1large language modelsknowledge creationmanagement researchChatGPTGPT-3
The paper looks at the role of large language models in academic knowledge creation based on a scoping review (2018 to January 2023) of how researchers have previously used the language model GPT to assist in the performance of academic knowledge creation tasks beyond data analysis. These tasks include writing, editing, reviewing, dataset creation and curation, which have been difficult to perform using earlier ML tools. Based on a synthesis of these papers, this study identifies pathways for a future academic research landscape that incorporates wider usage of large language models based on the current modes of adoption in published articles as a CoWriter, Research Assistant and Respondent. The paper concludes with a research and practice agenda for management knowledge creation based on the wider adoption of Large Language models. The paper's focus is on understanding the nature of the current usage of GPT to perform academic tasks. As such, it does not describe the challenges and problems of large language models. It does also not speculate about the extent to which they present machine intelligence or consciousness.</p>
<p>Introduction</p>
<p>Artificial Intelligence (AI) has been defined as the research and design of creating machines that simulate human intelligence to perform actions or intellectual tasks (Müller and Bostrom, 2016). This paper will define AI simply as "computer systems that perform tasks requiring cognition tasks autonomously". This is similar to earlier definitions (Russell, 2010).</p>
<p>Emerging phenomena can often be overlooked in management research as they are poorly defined with unclear conceptual concepts and limited empirical data (Yadav, 2018). Large Language models like GPT, however, have growth drivers that suggest that they are worthy of researcher attention and specifically, their impact on academic knowledge production should be identified at this early stage of adoption.</p>
<p>Previous academic research in business and management have identified the potential for machine learning analytics to change the nature of theorising in business and management reseach (Leavitt, Schabram, Hariharan &amp; Barnes, 2021). Large Language models such as GPT, however, can go further to influence the nature of academic knowledge production itself in this domain. Management research is primarily based on empirical quantitative and qualitative studies done by small teams of researchers which may be difficult to replicate (Block, Fisch, Kanwal, Lorenzen, &amp; Schulze, 2022). As a result, the domain can be influenced by tools that can simulate human created text in a manner that experimental or lab based work would not. The area therefore requires examination which is the purpose of this paper.</p>
<p>AI transformations</p>
<p>AI in the Business and management domain research has taken two main perspectives. The first is as an analytical approach: AI tools identify insights, using classification or modelling of complex dynamic data. AI analytical approaches enable the direct examination of "mixed" data, such as data collected from social media that can combine text, images and video (Al-Smadi, Jaradat, Al-Ayyoub, and Jararweh, 2017). These approaches have been used to examine the meaning of text (Martinez-Torres, and Toral, 2019), identify market segments via geographical data (Rodríguez, Semanjski, Gautama, Van de Weghe and Ochoa, 2018) and emergent visual representations from photographs (Zhang, Chen, and Li, 2019). The second stream of research examines the impact of AI on organisational activity. Research has examined the extent to which AI can be applied in service operations (Meyer, Cohen, and Nair, 2020).</p>
<p>Machine learning (ML) is the dominant approach to implementing artificial intelligence in computer systems (Ghahramani, 2015). Neural Network approaches use a combination of machine learning algorithms configured as layers of nodes, which process inputs of data or outputs from previous nodes (Pourgholamali, Kahani, Bagheri and Noorian, 2017). A subset of these approaches, Language models are combinations of neural networks trained by predicting blanked-out words in texts (Otter, Medina and Kalita, 2018 ) using a technique called Transformer, which allows for parallel training on multiple processors. Examples of such models include Google's BERT and OpenAI's GPT (Generative Pre-Trained).</p>
<p>The latter, GPT combines transformers with other machine learning models (Vig, 2019). The current iteration, GPT-3 has 175 billion parameters. GPT-3 can recognise grammar, essay structure, and writing genre based on the analysis of very large text datasets. It can be retrained on small datasets to perform tasks such as summarisation and question answering which cannot be done by statistical, unsupervised or supervised learning techniques. GPT-3 can be deployed using natural language prompts that apply the software's rich representations of language on itself to configure its internal neural networks.</p>
<p>In practice, users of GPT can create statements or prompts that describe knowledge tasks. That may include tasks such as "write an academic abstract on Y topic for X journal". This is translated by the program into required software actions that result in text generation, transformation and summarisation to produce a final output. On November 30 th 2022, an adapted version of GPT was launched via a simple to use chat interface. ChatGPT, as it is known has been trained using Reinforcement Learning with Human Feedback. ChatGPT has grown to 30 million users in two months, faster than many other digital products (https://www.nytimes.com/2023/02/03/technology/chatgpt-openai-artificialintelligence.html).</p>
<p>Researchers have begun to use GPT as not merely an analytical tool but a contributor to the academic knowledge creation process as it can assist with core tasks of research such as identifying potential academic contributions, forming and prioritising ideas (Du, Kim, Raheja, Kumar &amp; Kang, 2022). To date, researchers in education (Baidoo-Anu &amp; Owusu Ansah, 2023), medicine (Shen,Heacock, Elias, Hentel, Reig, Shih, &amp; Moy, 2023), and tourism (Carvalho &amp; Ivanov, 2023), among others, are examing the impact of the adoption of LLMs in their domains.</p>
<p>Research, however, has not yet explored the potential for AI to change the underlying practices of academic knowledge creation. The exponential growth of data processing power has led to advances in AI, including self-supervised neural models, that can learn powerful representations from large-scale unstructured data such as text without human supervision. In this manner, they can go beyond analysis by applying these representations to generate outputs in the form of text, audio, images and video (Weisz,Muller, He, &amp; Houde, 2023). For example, in 2019 Springer published the first academic book written by AI (Writer, 2019).</p>
<p>Like other types of digital products, GPT, has catalysed an online community of knowledge and practices (Van de Vrande, De Jong, Vanhaverbeke, &amp; De Rochemont, 2009). This community provides advice on how to utilize software applications in addition to offical support (Cosentino, Izquierdo, and Cabot 2017). Growth in available advice will make core tools more accessible to non-technical individuals, supporting increased adoption even if core technical functionality does not change.</p>
<p>GPT and other language models are poised to be embedded in consumer word processing and other applications (https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/). This can only ensure that the number of users, including academics, will perform knowledge-creation tasks using these tools. Improved language models are under development which are designed to overcome the limitations of existing offerings (https://www.datacamp.com/blog/what-we-know-gpt4). Combined, these drivers suggest that their impact on knowledge creation will continue to grow.</p>
<p>This paper makes an initial contribution based on a scoping review (2018 to January 2023) of how researchers have previously used the language model GPT to assist in the performance of academic knowledge creation tasks beyond the analysis of data. These tasks include writing, editing, reviewing, dataset creation and curation, which have been difficult to perform using earlier ML tools form the basis of creating research outputs, which underpin academic impact, knowledge exchange with industry and educational experiences. Based on a synthesis of these papers, we identify pathways for a future academic research landscape that incorporates wider usage of large language models based on the current modes of adoption in published articles as a CoWriter, Research Assistant and Respondent. The paper concludes with a research and practice agenda for management knowledge creation based on the wider adoption of Large Language models. The paper's focus is on understanding the nature of the current usage of GPT to perform academic tasks. As such, it does not describe the challenges and problems of large language models. It does also not speculate about the extent to which they present machine intelligence or consciousness.</p>
<p>Method</p>
<p>This research takes the form of a scoping study, a type of literature review that aims to identify sources of evidence in a research area. Unlike systematic reviews, scoping reviews do not focus on a welldefined question and tend to address broader topics in an emerging area (Brogaard, 2021). Scoping studies can provide an initial overview of an area that has not been reviewed before. Given the nature of the approach, the research questions tend to be broad and can include non-peer-reviewed articles, which may limit the robustness of the findings (Pham, Rajić, Greig, Sargeant, Papadopoulos &amp; McEwen, 2014). This scoping review was created to identify research was done that examines the use of GPT as in scholarly knowledge production beyond the analysis of data to support the creation of outputs. We identify academic tasks as belonging to the broad categories identified in Table 1. Identify the nature of gaps in existing work (insufficient research, overlooked area, emerging area in need of empirical work or synthesis) and identify potential academic contribution from the planned study. Identify potential outlet for publication. Finalise knowledge development approach Determine the approach (literature review, secondary or primary data analysis) required to create an academic contribution. Design approach protocol and perform exploratory evaluation. Perform knowledge development activity Adapt analysis protocol, and document decisions made and findings. Identify academic and industry implications of findings.</p>
<p>Create initial research communication artefact</p>
<p>Complete output (paper, presentation), and submit to outlet. Adapted from Hope, Downey, Etzioni, Weld &amp; Horvitz (2022).</p>
<p>Records were first identified using the search term GPT, GPT-2 and GPT-3 and academic tasks from table 1 using Scopus, Google Scholar and Semantic Scholar from 2018 to Jan 2023. To limit the search results, the articles had to mention academic tasks, including academic writing, literature summary, text generation, and experiment design. Abstracts were screened by two academics using the platform Rayyan.ai to remove duplicates and remove irrevelant studies. This left 182 studies to be examined in detail for eligibility and were removed if they did not focus on one or more of the categories identified in Table 1. Figure 1 provides an overview of the knowledge search and identification of the final subset of articles.</p>
<p>Records identified from Scopus, Google Scholar Semantic Scholar and Arxiv search using Publish or Perish (973) Records after duplicates removed using Rayyan.ai</p>
<p>Records Screened</p>
<p>Full Text Articles to be assessed for Eligibility</p>
<p>Records Excluded Irrelevant (409) Full Text Articles were excluded if they did not focus on academic knowledge creation (Table 1) Studies included in Synthesis (22) Table 2 4. Findings: GPT in Academic Knowledge production GPT, like other large language models have a number of impacts on academic knowledge creation. While large language models have been available for some time, previous versions required programming knowledge in order to obtain full benefit of usage. The public availability of GPT via a chat interface has enabled non-programmers to access these advanced tools, thus leading to the democratisation of academic knowledge creation. There is also an exponential increase in available advice on how to create prompts to be able to get the best possible output for a wide variety of tasks. Table 2 summarizes how GPT has been employed for academic knowledge-creation tasks. Three themes emerged from the literature: 1) GPT as a Co-Writer in which the tool was deployed to complete academic outputs (paper, presentation) to scan and evaluate current academic research, identify and prioritise possible research directions. 2) GPT as a Research Assistant determines the approach (literature review, secondary or primary data analysis) required to create academic contribution; designs approach protocol and performs exploratory evaluation; performs evaluation or analysis; documents decisions made and findings; identifies academic and industry implications of findings. 3) GPT as a Respondent in which GPT was used as a source of simulated respondents and systems.</p>
<p>GPT as a Co-Writer</p>
<p>Previous Natural Language Generation (NLG) software systems followed rule-based systems. Large Language models learn representations from large text collections, in the case of GPT, 175 billion parameters. The use of these tools can enable researchers to synthesise related work as well as expand the exploration of problem spaces to adjacent and parallel fields (Alarie and Cockfield, 2021). Many management phenomena are examined differently in different fields and even related fields. The use of GPT (Table 2) to summarise and synthesise knowledge can enable research teams at the initial stage to deploy arguments based on developments in parallel domains in business and management or broadly across the social sciences.</p>
<p>Researchers have speculated on the role of GPT as a disguised ghost-writer in academic research (Srivastava, 2022). Although some software applications already exist (e.g. https://writer.com/aicontent-detector/), text created by AI is difficult to detect, and formal response requires the coordination of multiple technological and social institutions in order to establish acceptable use of text generation (Illia, Colleoni and Zyglidopoulos, 2022). As these models become bigger, the act of Authorship Attribution (human or machine becomes more difficult (Nowak-Gruca, 2022). The introduction of ChatGPT forced many academic publishers to adopt formal policies towards AI-generated text and AI authorship of academic publications. The consensus is that GPT or other LLM cannot be a co-author and must be treated as a tool. Moreover, the use of AI to generate texts needs to be explicitly acknowledged but the human authors take full responsibility for the manuscript's content.</p>
<p>In addition to core academic texts, GPT has been used to create academic peer review reports (Bartoli and Medvet, 2020). In both cases, the team adapted the model to academic domains by training them on domain-specific text so that they can create outputs in the required format and style. The rate of knowledge production is increasing and in many domains, the number of papers that are available on a weekly or monthly basis frequently exceeds the capacity of individual researchers to meaningfully absorb. There are also potential modalities of fraud based on the creation of fake review reports (Uchendu, Le and Lee, 2022). Since reviewers are anonymous, review reports for papers can be generated without attribution, that may be used to reduce trust in the academic process. These tools also have indirect impacts. For researchers who depend on public text sources to generate outputs, such as social media these tools may pose a problem as they will be an increasing amount of fake reviews and posts on online platforms (Karanjai, 2022). In both cases we will have knowledge being created by tools that do not have a mind, worldview or perspective and are simply presenting words based on statistical inference rather than on meaning.</p>
<p>Far more dangerous are the use of these tools to create false or misleading information from external actors, which may result increased amount of academic hoaxes (Al-Khatib &amp; Teixeira da Silva, 2016). These frauds have been popular where politically motivated actors create fake articles in an attempt to show academic biases and flaws (Piedra, 2019). The ability to generate plausible text in the style of targeted journals as well as plausible data sets will increase the volume of these hoaxes and academia will have to create enhanced ways of identifying these hoaxes.</p>
<p>GPT as a research assistant</p>
<p>This stream of research (Table 2) identifies the potential for these tools to support literature search, data preparation, transformation and synthesis tasks performed by academics. For synthesis, these tools can create summaries of existing text, including examination of arguments (Alarie &amp; Cockfield, 2021;Illia, Colleoni, &amp; Zyglidopoulos, 2023). Data quality improvement by actions such as cleaning and curation are critical for computational analyses of large datasets. Attempts have been made to utilise machine learning approaches to automate this process, but they face limitations of determining relavance and can result in errors. Due to the complex nature, a significant amount of this work is done via crowdsourcing or hiring of data cleaning staff. Academics have used GPT to perform tasks that require domain knowledge on unstructured data, including cleaning, formatting and exploratory analysis (Jaimovitch-López, Ferri, Hernández-Orallo, Martínez-Plumed &amp; Ramírez-Quintana, 2022). GPT has been able to identify required elements in text, fill in data that is missing using a semantic approach, learn transformation functions and identify anomalies in multimodal data sets given a few examples from researchers. In this way, they automate data preprocessing tasks in a manner that can shape subsequent research by reducing the time to apply multiple transformation approaches which can support a greater range of analytical tasks (De Bie et al, 2022).</p>
<p>In addition to tasks on unstructured data, GPT has been used to support conventional quantitative and qualitative analyses. For the former, GPT has been used to create items for scale development by generating a large number of options which were then refined using stated researcher priorities (Hernandez &amp; Nie, 2022). Scale development is a complex research task that can be limited by the team's capacity for identifying potential items, creating appropriate descriptions, ensuring validity and identifying correlations among items. GPT outputs were found to be equivalent to those created using traditional approaches (Hernandez &amp; Nie, 2022).</p>
<p>GPT has also been used as an approach to explore research options. The tool has been used to examine potential respondent behaviour and interactions by querying it's internal text representations. In contrast to traditional data collection approaches, GPT was able to support the development of qualitative and quantitative data collection as well as enable researchers in order to explore confactual what if questions . In this way, GPT was used to improve research methodology, not simply perform analyses. Even further, in ZeroGen, GPT was used to create a subset of itself, called a tiny task model (TAM) to perform specific types of analyses when an existing machine learning model did not exist (Ye, Gao, Li, Xu, Feng, Wu, &amp; Kong, 2022). Unlike previous approaches, the model did not require external training data in order to create a tool that could analyse specific types of data. In this area, GPT has also been used recursively on itself to design large language experiments (Bellan, Dragoni, &amp; Ghidini, 2022).</p>
<p>GPT as Respondent</p>
<p>GPT has itself become a respondent to create papers. Without any additional data sources, researchers have queried GPT on perspective on issues such as climate change to identify biases or dominant perspectives in its internal language model (Leippold, 2022). GPT has also acted as a respondent to interview questions in traditional academic research (Iskender, 2023).</p>
<p>GPT has also been acted as a participant in experiments, replacing crowdsourced workers in computing and economics research (Bellan, Dragoni, and Ghidini, 2022). In the former, GPT has acted as a surrogate user in a conversational search system to ask questions of a given information system and evaluate the usefulness of the answers (Meyer, Elsweiler, Ludwig, Fernandez-Pichel &amp; Losada, 2022).</p>
<p>In the latter, GPT has been used to replicate findings from famous economic experiments (Horton, 2023).</p>
<p>While GPT does not have a worldview, it may exhibit emergent behaviour based on its training data. These idiosyncrasies are not seen as a limitation but as a benefit for researchers seeking to explore complex behaviours. In this way, GPT can provide simulated responses to questions of emotions that a rich descriptions but are entirely synthetic. These responses can be used to enrich existing datasets or to provide a basis for comparison to extend theoretical work (Ye et al., 2022). This is of particular value where few respondents are available or respondents may be unresponsive. In this mode, GPT has been used to create simulated responders for sensitive topics that allow researchers to explore these areas without causing harm (Salehi et al., 2022). These respondents need not be individuals as group interactions can also be modelled (Hamilton 2023). However, this means that the studies that are based on simulated responses and emotions do not evaluate actual human perceptions and emotions; hence, the validity of the findings of such papers will be limited to the AI domain and they should not be generalised to humans. A related stream of research identifies the biases of Chat GPT as a respondent on specific subjects. The tool has been used to answer Positive and Negative Affect Schedule (PANAS) items (Lee, Fyffe, Son, Jia and Yao, 2022).</p>
<p>Discussion and Research Agenda</p>
<p>Large language models like GPT create new capacities and constraints to Business and Management academics involved in research output creation. The above themes suggest that LLMs like GPT can increase the capacity of academic teams to perform research. Management academic researchers are increasingly required to provide knowledge that is not just rigorous but impactful (Wickert et al., 2021). Management research has been criticised for having a gap between researchers and practitioners, as managers rarely read articles in top management journals due to their theoretical nature with limited practical value (Kieser, Nicolai, &amp; Seidl, 2015). Additionally, the focus is on publishing in highly ranked/high-impact journals to increase institutional status with financial and reputational benefits. By increasing the capacity of researchers to deliver research, Large Language models may enable the field to enact its societal responsibilities by being able to generate rigorous impactful research. This benefit may be tempered by the increasing volume of outputs from researchers using simulated outputs in order to meet institutional status requirements.</p>
<p>Large Language Models and Academic Capacity Expansion</p>
<p>The use of these tools to summarise and synthesise knowledge can enable research teams at the initial stage to gain some insight into methodological and theoretical developments in parallel domains in business and management or broadly across the social sciences. By expanding the exploration of the problem space, academic contributions could be based on a broader conceptual base range. In this way academic silos can be broken down by conceptual frameworks enriched by contributions from parallel fields (Pavlik, 2023). In areas where there may a limited number of respondents, such as niche populations or difficult populations, these tools can be used to help refine data collection instruments or to generate simulated data (Salehi, Hassan, Lammerse, Sabet, Riiser, Røed &amp; Riegler, 2022). However, in the, the validity of such studies might be questionable and new modes of verification in addition to conceptual validity and triangulation must be created to examine the validity of computer generated responses.</p>
<p>Large language models provide the opportunity to create new types of outputs based on syntesis of extant research. These technologies can be applied as as a precursor to or a supplement to a systematic literature review. The rate of scholarly production is ever increasing (World Bank, n.d.) and academics may find it difficult to keep up with the body of knowledge (Johann, Raabe, &amp; Rauhut, 2022). Furthermore, articulating a distinct academic contribution may require academics to summarise and synthesise different bodies of knowledge, which can be difficult (Lindgreen, Di Benedetto, Clarke, Evald, Bjørn-Andersen &amp; Lambert, 2021). The use of these language models as initial summarisation tools in their research assistant role may be of value. Instead of narrowing down to a small number of articles for synthesis, researchers can explore broader questions based on contributions that are embedded in different types of knowledge. This can create a new type of systematic integrated review that extends the current manual approach using the summarization capabilities of these tools (Elsbach &amp; van Knippenberg, 2020).</p>
<p>The second new type of output may be based on prompt programming. Researchers have published articles that combine text, code and data (Hildebrand, Efthymiou, Busquet, Hampton, Hoffman, &amp; Novak, 2020). The way in which GPT is accessed is via prompting which may be a series of instructions that can be increasingly refined to provide feedback to the model (Zhou, Muresanu, Han, Paster, Pitis, Chan &amp; Ba, 2022). Future papers may include a structured description of prompts and responses along with a recording of text generation in real time to enable replication of research.</p>
<p>Future academic outputs may be based on entirely new methodologies facilitated by GPT. Netnography, for example, adapted the idea of ethnography to online communities and interactions (Kozinets, 2020). Language models may be used in a similar manner to query themselves in a new form of ethnography. In this case, the "respondent" is an aggregated body of statistical patterns derived from online text and the researcher is querying statistical patterns in the text to gain some insight into what common knowledge or widely held perspectives are on a given topic or area. When prompted by the authors, ChatGPT suggested such netnographic approach to be named "AI-based netnography" ot "AI-driven netnography".</p>
<p>GPT can also be prompted to act in different rules and can be used to extend existing analyses that may require the creation of narratives from a distinct population that is difficult to access (Salehi et al., 2022). In this role, they may also be a distinct form of social simulation. Existing simulation models use agent-based modelling or system dynamics to model associations among numerical variables. A large language model does not need such an abstraction and can directly simulate interactions among simulated characters that it creates (Hamilton, 2023). This may be a new way of performing social simulation activities that is not based on simplification of a problem, but on simulating scenarios based on a richer form of qualitative description.</p>
<p>Large Language Models and Academic Capacity Constraints</p>
<p>The use of these tools can increase the power of technological companies over academic knowledge production. The development of large language models is funded by very large commercial organisations and are not public goods (Bender, Gebru, McMillan-Major &amp; Shmitchell, 2021). The information provided to these tools via researcher usage help train the systems to improve knowledge creation (Pavlik, 2023). Increasing the use of these tools in academic publications allows them to become better at creating academic writing which ironically will increase their power over knowledge production. For researchers who rely on technological companies for data access, changes in the terms of service can force cancellation of planned initiatives, in drastic cases affecting an entire subdomain of research (Bruns, 2019). Further, as large language models are inscrutable, they may embed training dataset biases on outputs, indirectly shaping academic research (Horton, 2023).</p>
<p>For researchers who depend on public text sources to generate outputs such as social media these tools may pose a problem as they will be an increasing amount of fake reviews and posts on online platforms. The problem of fake data will be a major one since academics, students and public can create plausible sounding data using simple prompts for interviews or take existing data sets and modify them quite simply to create a plausible seeming data set that is then analysed and presented as if it were collected from real individuals (Tallón-Ballesteros, 2020). The increasing reliance of academics on platforms such as Mechanical Turk is another potential concern since these respondents may also use large language models in order to generate responses to be paid for surveys or experimental participation .</p>
<p>Concluding remarks</p>
<p>Large Language Models are taking academia like a storm. While there are many fears about them, there are significant benefits in terms of knowledge creation. New research methodology, increased output, better quality of the research output, and new insights and only some of the potential impacts of these technologies on academic knowledge creation. One thing to remember is that LLMs are nothing else but tools, sophisticated but yet tools. They do not have consciousness and cannot take responsibility for the written text. Therefore, they cannot be listed as co-authors of academic publications. However, they can assist in all stages of the research process, making it more effective and efficient. Hence, in the future, researchers may not go the way of horses (Brynjolffson &amp; McAfee, 2015) but rather researchers that utilise AI (in this case LLMs) will outperform researchers that do not on traditional metrics. Thus, LLMs may be a source of competitive advantage in academia and skills to use LLMs will be part of researchers' near future core competences. Research methods modules at universities will need to incorporate LLM-based research methodologies and skills in order to equip the future researchers with the necessary research skills. As the LLMs develop, so will the researchers in a co-evolution game that has no end or winner but fuzzy rules, evolution path and knowledge cocreation.</p>
<p>Table 1 :
1Categories of Academic TasksAcademic Task 
Summary 
Identify area of focus 
Scanning and evaluating current academic research </p>
<p>Table 2 :
2PapersLeippold, M. (2022). Thus spoke GPT-3: Interviewing a large-language model on climate finance. Finance Research Letters, 103617.Type 
Title and Reference 
Classification </p>
<p>Preprint </p>
<p>Srivastava, M. (2023, January 9). A Day in the 
Life of ChatGPT as a researcher: Sustainable 
and Efficient Machine Learning -A Review of 
Sparsity Techniques and Future Research 
Directions. https://doi.org/10.31219/osf.io/e9p3g </p>
<p>GPT as Cowriter </p>
<p>Preprint </p>
<p>Uchendu, A., Le, T., &amp; Lee, D. (2022). Attribution 
and Obfuscation of Neural Text Authorship: A 
Data Mining Perspective. arXiv preprint 
arXiv:2210.10488. </p>
<p>GPT as Cowriter </p>
<p>Journal article </p>
<p>GPT as Cowriter </p>
<p>Preprint </p>
<p>Liew, A., &amp; Mueller, K. (2022). Using Large 
Language Models to Generate Engaging 
Captions for Data Visualizations. arXiv preprint 
arXiv:2212.14047. </p>
<p>GPT as Cowriter </p>
<p>Preprint </p>
<p>Liyanage, V., Buscaldi, D., &amp; Nazarenko, A. 
(2022). A benchmark corpus for the detection of 
automatically generated text in academic 
publications. arXiv preprint arXiv:2202.02013. </p>
<p>GPT as Cowriter </p>
<p>Journal article </p>
<p>Nowak-Gruca, A. J. (2022). Could an Artificial 
Intelligence be a Ghostwriter?. Journal of 
Intellectual Property Rights (JIPR), 27(1), 25-37. </p>
<p>GPT as Cowriter </p>
<p>Journal article </p>
<p>Illia, L., Colleoni, E., &amp; Zyglidopoulos, S. (2023). 
Ethical implications of text generation in the age 
of artificial intelligence. Business Ethics, the 
Environment &amp; Responsibility, 32(1), 201-210. </p>
<p>GPT as Cowriter </p>
<p>Journal article </p>
<p>Alarie, B., &amp; Cockfield, A. (2021). Will machines 
replace us?: Machine-authored texts and the 
future of scholarship. Law, Technology and 
Humans, 3(2), 5-11. </p>
<p>GPT as Cowriter </p>
<p>Conference paper </p>
<p>Tallón-Ballesteros, A. J. (2020). Exploring the 
potential of GPT-2 for generating fake reviews of 
research papers. Fuzzy Systems and Data 
Mining VI: Proceedings of FSDM, 331, 390. </p>
<p>GPT as Cowriter </p>
<p>Journal article </p>
<p>Pavlik, J. V. (2023). Collaborating With ChatGPT: 
Considering the Implications of Generative 
Artificial Intelligence for Journalism and Media </p>
<p>GPT as Cowriter </p>
<p>Will machines replace us?: Machine-authored texts and the future of scholarship. Law. B Alarie, A Cockfield, Technology and Humans. 32Alarie, B., &amp; Cockfield, A. (2021). Will machines replace us?: Machine-authored texts and the future of scholarship. Law, Technology and Humans, 3(2), 5-11.</p>
<p>Stings, hoaxes and irony breach the trust inherent in scientific publishing. A Al-Khatib, J A Silva, Publishing Research Quarterly. 323Al-Khatib, A., &amp; Teixeira da Silva, J. A. (2016). Stings, hoaxes and irony breach the trust inherent in scientific publishing. Publishing Research Quarterly, 32(3), 208-219.</p>
<p>Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning. D Baidoo-Anu, L Owusu Ansah, Available at SSRN 4337484Baidoo-Anu, D., &amp; Owusu Ansah, L. (2023). Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning. Available at SSRN 4337484.</p>
<p>Experiment Maker: a Tool to create Experiments with GPT-3 easily. P Bellan, M Dragoni, C Ghidini, EKAW'22: Companion Proceedings of the 23rd International Conference on Knowledge Engineering and Knowledge Management. Bozen-Bolzano, ITBellan, P., Dragoni, M., &amp; Ghidini, C. (2022). Experiment Maker: a Tool to create Experiments with GPT-3 easily. EKAW'22: Companion Proceedings of the 23rd International Conference on Knowledge Engineering and Knowledge Management, September 26-29, 2022, Bozen- Bolzano, IT</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and TransparencyBender, E. M., Gebru, T., McMillan-Major, A., &amp; Shmitchell, S. (2021, March). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (pp. 610-623).</p>
<p>Replication studies in top management journals: An empirical investigation of prevalence, types, outcomes, and impact. J H Block, C Fisch, N Kanwal, S Lorenzen, A Schulze, Management Review Quarterly. Block, J. H., Fisch, C., Kanwal, N., Lorenzen, S., &amp; Schulze, A. (2022). Replication studies in top management journals: An empirical investigation of prevalence, types, outcomes, and impact. Management Review Quarterly, 1-26.</p>
<p>Innovative outcomes in public-private innovation partnerships: a systematic review of empirical evidence and current challenges. L Brogaard, Public Management Review. 231Brogaard, L. (2021). Innovative outcomes in public-private innovation partnerships: a systematic review of empirical evidence and current challenges. Public Management Review, 23(1), 135-157.</p>
<p>After the 'APIcalypse': Social media platforms and their fight against critical scholarly research. A Bruns, Information, Communication &amp; Society22Bruns, A. (2019). After the 'APIcalypse': Social media platforms and their fight against critical scholarly research. Information, Communication &amp; Society, 22(11), 1544-1566.</p>
<p>Will Humans Go the Way of Horses? Labor in the Second Machine Age. E Brynjolffson, A Mcafee, Foreign Affairs. 944Brynjolffson, E., &amp; McAfee, A. (2015). Will Humans Go the Way of Horses? Labor in the Second Machine Age. Foreign Affairs, 94(4), 8-14.</p>
<p>ChatGPT for tourism: applications, benefits, and risks. I Carvalho, S Ivanov, 10.1108/TR-02-2023-0088Tourism Review. in pressCarvalho, I., &amp; Ivanov, S. (2023). ChatGPT for tourism: applications, benefits, and risks. Tourism Review, https://doi.org/10.1108/TR-02-2023-0088 (in press),</p>
<p>A systematic mapping study of software development with GitHub. V Cosentino, J L C Izquierdo, J Cabot, IEEE Access. 5Cosentino, V., Izquierdo, J.L.C. and Cabot, J., 2017. A systematic mapping study of software development with GitHub. IEEE Access, 5, pp.7173-7192.</p>
<p>W Du, Z M Kim, V Raheja, D Kumar, D Kang, arXiv:2204.03685Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision. arXiv preprintDu, W., Kim, Z. M., Raheja, V., Kumar, D., &amp; Kang, D. (2022). Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision. arXiv preprint arXiv:2204.03685.</p>
<p>Creating high-impact literature reviews: An argument for 'integrative reviews. K D Elsbach, D Van Knippenberg, Journal of Management Studies. 576Elsbach, K. D., &amp; van Knippenberg, D. (2020). Creating high-impact literature reviews: An argument for 'integrative reviews'. Journal of Management Studies, 57(6), 1277-1289.</p>
<p>Probabilistic machine learning and artificial intelligence. Z Ghahramani, Nature. 5217553452Ghahramani, Z. (2015). Probabilistic machine learning and artificial intelligence. Nature, 521(7553), p.452.</p>
<p>When will AI exceed human performance?. K Grace, J Salvatier, A Dafoe, B Zhang, O Evans, arXiv:1705.08807Evidence from AI experts. arXiv preprintGrace, K., Salvatier, J., Dafoe, A., Zhang, B. and Evans, O. (2017). When will AI exceed human performance? Evidence from AI experts. arXiv preprint arXiv:1705.08807.</p>
<p>Neural Language Models as What If?-Engines for HCI Research. P Hämäläinen, M Tavast, A Kunnari, 27th International Conference on Intelligent User Interfaces. Hämäläinen, P., Tavast, M., &amp; Kunnari, A. (2022, March). Neural Language Models as What If?-Engines for HCI Research. In 27th International Conference on Intelligent User Interfaces (pp. 77-80).</p>
<p>S Hamilton, arXiv:2301.05327Blind Judgement: Agent-Based Supreme Court Modelling With GPT. arXiv preprintHamilton, S. (2023). Blind Judgement: Agent-Based Supreme Court Modelling With GPT. arXiv preprint arXiv:2301.05327.</p>
<p>The AI-IP: Minimising the guesswork of personality scale item development through artificial intelligence. I Hernandez, W Nie, 10.1111/peps.12543Personnel Psychology. Hernandez, I., &amp; Nie, W. (2022). The AI-IP: Minimising the guesswork of personality scale item development through artificial intelligence. Personnel Psychology. DOI: 10.1111/peps.12543</p>
<p>Gazing into Clever Hans machines. J Hernández-Orallo, Nature Machine Intelligence. 1Hernández-Orallo, J. (2019). Gazing into Clever Hans machines. Nature Machine Intelligence, 1, 172- 173.</p>
<p>T Hope, D Downey, O Etzioni, D S Weld, E Horvitz, arXiv:2205.02007A Computational Inflection for Scientific Discovery. arXiv preprintHope, T., Downey, D., Etzioni, O., Weld, D. S., &amp; Horvitz, E. (2022). A Computational Inflection for Scientific Discovery. arXiv preprint arXiv:2205.02007.</p>
<p>Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus. J J Horton, arXiv:2301.07543arXiv preprintHorton, J. J. (2023). Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?. arXiv preprint arXiv:2301.07543.</p>
<p>Ethical implications of text generation in the age of artificial intelligence. L Illia, E Colleoni, S Zyglidopoulos, Business Ethics, the Environment &amp; Responsibility. 321Illia, L., Colleoni, E., &amp; Zyglidopoulos, S. (2023). Ethical implications of text generation in the age of artificial intelligence. Business Ethics, the Environment &amp; Responsibility, 32(1), 201-210.</p>
<p>Holy or Unholy? Interview with Open AI's ChatGPT. A Iskender, 10.54055/ejtr.v34i.3169?fbclid=IwAR2aQHqs3wE8BLSQT1hSQ-qIr1EdRJiZnHFFRfywGGbqm6YBYOS-rerX-uYEuropean Journal of Tourism Research. 34Iskender, A. (2023). Holy or Unholy? Interview with Open AI's ChatGPT. European Journal of Tourism Research, 34, Article number 3414. https://doi.org/10.54055/ejtr.v34i.3169</p>
<p>Can language models automate data wrangling. G Jaimovitch-López, C Ferri, J Hernández-Orallo, F Martínez-Plumed, M J Ramírez-Quintana, Machine Learning. Jaimovitch-López, G., Ferri, C., Hernández-Orallo, J., Martínez-Plumed, F., &amp; Ramírez-Quintana, M. J. (2022). Can language models automate data wrangling?. Machine Learning, 1-30.</p>
<p>Under pressure: The extent and distribution of perceived pressure among scientists in Germany. D Johann, I J Raabe, H Rauhut, Research Evaluation. 313Johann, D., Raabe, I. J., &amp; Rauhut, H. (2022). Under pressure: The extent and distribution of perceived pressure among scientists in Germany, Austria, and Switzerland. Research Evaluation, 31(3), 385-409.</p>
<p>R Karanjai, 10.48550/arxiv.2301.00665Targeted Phishing Campaigns using Large Scale Language Models. arXivKaranjai, R. (2022). Targeted Phishing Campaigns using Large Scale Language Models. arXiv, available at: https://doi.org/10.48550/arxiv.2301.00665 (accessed 3 February 2023)</p>
<p>Digital doping for historians: can history, memory, and historical theory be rendered artificially intelligent? History and Theory. W Kansteiner, 10.1111/hith.12282Kansteiner, W. (2022). Digital doping for historians: can history, memory, and historical theory be rendered artificially intelligent? History and Theory.https://doi.org/10.1111/hith.12282</p>
<p>The practical relevance of management research: Turning the debate on relevance into a rigorous scientific research program. A Kieser, A Nicolai, D Seidl, Academy of Management annals. 91Kieser, A., Nicolai, A., &amp; Seidl, D. (2015). The practical relevance of management research: Turning the debate on relevance into a rigorous scientific research program. Academy of Management annals, 9(1), 143-233.</p>
<p>Ghost in the machine: On organisational theory in the age of machine learning. R Kozinets, Sage, K Leavitt, K Schabram, P Hariharan, C M Barnes, Academy of Management Review. 464Netnography: The essential guide to qualitative social media researchKozinets, R. (2020). Netnography: The essential guide to qualitative social media research. Sage. Leavitt, K., Schabram, K., Hariharan, P., &amp; Barnes, C. M. (2021). Ghost in the machine: On organisational theory in the age of machine learning. Academy of Management Review, 46(4), 750-777.</p>
<p>A Paradigm Shift from "Human Writing" to "Machine Generation" in Personality Test Development: an Application of State-of-the-Art Natural Language Processing. P Lee, S Fyffe, M Son, Z Jia, Z Yao, Journal of Business and Psychology. Lee, P., Fyffe, S., Son, M., Jia, Z., &amp; Yao, Z. (2022). A Paradigm Shift from "Human Writing" to "Machine Generation" in Personality Test Development: an Application of State-of-the-Art Natural Language Processing. Journal of Business and Psychology, 1-28.</p>
<p>Thus spoke GPT-3: Interviewing a large-language model on climate finance. M Leippold, Finance Research Letters. 103617Leippold, M. (2022). Thus spoke GPT-3: Interviewing a large-language model on climate finance. Finance Research Letters, 103617.</p>
<p>Using Large Language Models to Generate Engaging Captions for Data Visualizations. A Liew, K Mueller, arXiv:2212.14047arXiv preprintLiew, A., &amp; Mueller, K. (2022). Using Large Language Models to Generate Engaging Captions for Data Visualizations. arXiv preprint arXiv:2212.14047.</p>
<p>How to define, identify, and measure societal value. A Lindgreen, C A Di Benedetto, A H Clarke, M R Evald, N Bjørn-Andersen, D M Lambert, Industrial Marketing Management. 97Lindgreen, A., Di Benedetto, C. A., Clarke, A. H., Evald, M. R., Bjørn-Andersen, N., &amp; Lambert, D. M. (2021). How to define, identify, and measure societal value. Industrial Marketing Management, 97, A1-A13.</p>
<p>A benchmark corpus for the detection of automatically generated text in academic publications. V Liyanage, D Buscaldi, A Nazarenko, arXiv:2202.02013arXiv preprintLiyanage, V., Buscaldi, D., &amp; Nazarenko, A. (2022). A benchmark corpus for the detection of automatically generated text in academic publications. arXiv preprint arXiv:2202.02013.</p>
<p>A machine learning approach for the identification of the deceptive reviews in the hospitality sector using unique attributes and sentiment orientation. M D R Martinez-Torres, S L Toral, Tourism Management. 75Martinez-Torres, M. D. R., &amp; Toral, S. L. (2019). A machine learning approach for the identification of the deceptive reviews in the hospitality sector using unique attributes and sentiment orientation. Tourism Management, 75, 393-403.</p>
<p>From automats to algorithms: the automation of services using artificial intelligence. C Meyer, D Cohen, S Nair, Journal of Service Management. 312Meyer, C., Cohen, D., &amp; Nair, S. (2020). From automats to algorithms: the automation of services using artificial intelligence. Journal of Service Management, 31(2), 145-161.</p>
<p>Paraphrase identification and semantic text similarity analysis in Arabic news tweets using lexical, syntactic, and semantic features. A S Mohammad, Z Jaradat, A A Mahmoud, Y Jararweh, Information Processing &amp; Management. 533Mohammad, A. S., Jaradat, Z., Mahmoud, A. A., &amp; Jararweh, Y. (2017). Paraphrase identification and semantic text similarity analysis in Arabic news tweets using lexical, syntactic, and semantic features. Information Processing &amp; Management, 53(3), 640-652.</p>
<p>Future progress in artificial intelligence: A survey of expert opinion. V C Müller, N Bostrom, Fundamental issues of artificial intelligence. Müller, V. C., &amp; Bostrom, N. (2016). Future progress in artificial intelligence: A survey of expert opinion. Fundamental issues of artificial intelligence, 555-572.</p>
<p>Could an Artificial Intelligence be a Ghostwriter?. A J Nowak-Gruca, Journal of Intellectual Property Rights (JIPR). 271Nowak-Gruca, A. J. (2022). Could an Artificial Intelligence be a Ghostwriter?. Journal of Intellectual Property Rights (JIPR), 27(1), 25-37.</p>
<p>A survey of the usages of deep learning for natural language processing. D W Otter, J R Medina, J K Kalita, IEEE transactions on neural networks and learning systems. 32Otter, D. W., Medina, J. R., &amp; Kalita, J. K. (2020). A survey of the usages of deep learning for natural language processing. IEEE transactions on neural networks and learning systems, 32(2), 604- 624.</p>
<p>Collaborating With ChatGPT: Considering the Implications of Generative Artificial Intelligence for Journalism and Media Education. J V Pavlik, Journalism &amp; Mass Communication Educator. 10776958221149577Pavlik, J. V. (2023). Collaborating With ChatGPT: Considering the Implications of Generative Artificial Intelligence for Journalism and Media Education. Journalism &amp; Mass Communication Educator, 10776958221149577.</p>
<p>A scoping review of scoping reviews: advancing the approach and enhancing the consistency. M T Pham, A Rajić, J D Greig, J M Sargeant, A Papadopoulos, S A Mcewen, Research synthesis methods. 54Pham, M. T., Rajić, A., Greig, J. D., Sargeant, J. M., Papadopoulos, A., &amp; McEwen, S. A. (2014). A scoping review of scoping reviews: advancing the approach and enhancing the consistency. Research synthesis methods, 5(4), 371-385.</p>
<p>The gift of a hoax. L M Piedra, Qualitative Social Work. 182Piedra, L. M. (2019). The gift of a hoax. Qualitative Social Work, 18(2), 152-158.</p>
<p>Embedding unstructured side information in product recommendation. F Pourgholamali, M Kahani, E Bagheri, Z Noorian, Electronic Commerce Research and Applications. 25Pourgholamali, F., Kahani, M., Bagheri, E. and Noorian, Z., 2017. Embedding unstructured side information in product recommendation. Electronic Commerce Research and Applications, 25, pp.70-85.</p>
<p>Unsupervised hierarchical clustering approach for tourism market segmentation based on crowdsourced mobile phone data. J Rodríguez, I Semanjski, S Gautama, N Van De Weghe, D Ochoa, Sensors. 1892972Rodríguez, J., Semanjski, I., Gautama, S., Van de Weghe, N., &amp; Ochoa, D. (2018). Unsupervised hierarchical clustering approach for tourism market segmentation based on crowdsourced mobile phone data. Sensors, 18(9), 2972.</p>
<p>Artificial intelligence a modern approach. S J Russell, Pearson Education, IncRussell, S. J. (2010). Artificial intelligence a modern approach. Pearson Education, Inc..</p>
<p>Evaluating mixed-initiative conversational search systems via user simulation. I Sekulić, M Aliannejadi, F Crestani, Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining. the Fifteenth ACM International Conference on Web Search and Data MiningSekulić, I., Aliannejadi, M., &amp; Crestani, F. (2022, February). Evaluating mixed-initiative conversational search systems via user simulation. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (pp. 888-896).</p>
<p>Synthesising a talking child avatar to train interviewers working with maltreated children. P Salehi, S Z Hassan, M Lammerse, S S Sabet, I Riiser, R K Røed, . . Riegler, M A , Big Data and Cognitive Computing. 6262Salehi, P., Hassan, S. Z., Lammerse, M., Sabet, S. S., Riiser, I., Røed, R. K., ... &amp; Riegler, M. A. (2022). Synthesising a talking child avatar to train interviewers working with maltreated children. Big Data and Cognitive Computing, 6(2), 62.</p>
<p>ChatGPT and Other Large Language Models Are Double-edged Swords. Y Shen, L Heacock, J Elias, K D Hentel, B Reig, G Shih, L Moy, Radiology. 230163Shen, Y., Heacock, L., Elias, J., Hentel, K. D., Reig, B., Shih, G., &amp; Moy, L. (2023). ChatGPT and Other Large Language Models Are Double-edged Swords. Radiology, 230163.</p>
<p>A Day in the Life of ChatGPT as a researcher: Sustainable and Efficient Machine Learning -A Review of Sparsity Techniques and Future Research Directions. M Srivastava, 10.31219/osf.io/e9p3gSrivastava, M. (2023, January 9). A Day in the Life of ChatGPT as a researcher: Sustainable and Efficient Machine Learning -A Review of Sparsity Techniques and Future Research Directions. https://doi.org/10.31219/osf.io/e9p3g</p>
<p>Exploring the potential of GPT-2 for generating fake reviews of research papers. Fuzzy Systems and Data Mining VI: Proceedings of FSDM. A J Tallón-Ballesteros, 331390Tallón-Ballesteros, A. J. (2020). Exploring the potential of GPT-2 for generating fake reviews of research papers. Fuzzy Systems and Data Mining VI: Proceedings of FSDM, 331, 390.</p>
<p>Language Models Can Generate Human-Like Self-Reports of Emotion. M Tavast, A Kunnari, P Hämäläinen, 27th International Conference on Intelligent User Interfaces. Tavast, M., Kunnari, A., &amp; Hämäläinen, P. (2022, March). Language Models Can Generate Human- Like Self-Reports of Emotion. In 27th International Conference on Intelligent User Interfaces (pp. 69-72).</p>
<p>A Uchendu, T Le, D Lee, arXiv:2210.10488Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective. arXiv preprintUchendu, A., Le, T., &amp; Lee, D. (2022). Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective. arXiv preprint arXiv:2210.10488.</p>
<p>A multiscale visualization of attention in the transformer model. J Vig, arXiv:1906.05714arXiv preprintVig, J. (2019). A multiscale visualization of attention in the transformer model. arXiv preprint arXiv:1906.05714.</p>
<p>Open innovation in SMEs: Trends, motives and management challenges. V Van De Vrande, J P De Jong, W Vanhaverbeke, M De Rochemont, 29TechnovationVan de Vrande, V., De Jong, J. P., Vanhaverbeke, W., &amp; De Rochemont, M. (2009). Open innovation in SMEs: Trends, motives and management challenges. Technovation, 29(6-7), 423-437.</p>
<p>Toward General Design Principles for Generative AI Applications. J D Weisz, M Muller, J He, S Houde, arXiv:2301.05578arXiv preprintWeisz, J. D., Muller, M., He, J., &amp; Houde, S. (2023). Toward General Design Principles for Generative AI Applications. arXiv preprint arXiv:2301.05578.</p>
<p>Management research that makes a difference: Broadening the meaning of impact. C Wickert, C Post, J P Doh, J E Prescott, A Prencipe, Journal of Management Studies. 582Wickert, C., Post, C., Doh, J. P., Prescott, J. E., &amp; Prencipe, A. (2021). Management research that makes a difference: Broadening the meaning of impact. Journal of Management Studies, 58(2), 297-320.</p>
<p>Scientific and technical journal articles. World Bank, World Bank (n.d.) Scientific and technical journal articles 2000-2018. Retrieved from https://data.worldbank.org/indicator/IP.JRN.ARTC.SC</p>
<p>Lithium-Ion Batteries. A Machine-Generated Summary of Current Research. B Writer, SpringerChamWriter, B. (2019). Lithium-Ion Batteries. A Machine-Generated Summary of Current Research. Cham: Springer. Retrieved from https://link.springer.com/content/pdf/10.1007%2F978-3-030-16800- 1.pdf</p>
<p>Making emerging phenomena a research priority. M S Yadav, Journal of the Academy of Marketing Science. 46Yadav, M. S. (2018). Making emerging phenomena a research priority. Journal of the Academy of Marketing Science, 46, 361-365.</p>
<p>Zerogen: Efficient zero-shot learning via dataset generation. J Ye, J Gao, Q Li, H Xu, J Feng, Z Wu, . . Kong, L , arXiv:2202.07922arXiv preprintYe, J., Gao, J., Li, Q., Xu, H., Feng, J., Wu, Z., ... &amp; Kong, L. (2022). Zerogen: Efficient zero-shot learning via dataset generation. arXiv preprint arXiv:2202.07922.</p>
<p>Discovering the tourists' behaviors and perceptions in a tourism destination by analysing photos' visual content with a computer deep learning model: The case of Beijing. K Zhang, Y Chen, C Li, Tourism Management. 75Zhang, K., Chen, Y. and Li, C., 2019. Discovering the tourists' behaviors and perceptions in a tourism destination by analysing photos' visual content with a computer deep learning model: The case of Beijing. Tourism Management, 75, pp. 595-608.</p>
<p>Large language models are human-level prompt engineers. Y Zhou, A I Muresanu, Z Han, K Paster, S Pitis, H Chan, J Ba, arXiv:2211.01910arXiv preprintZhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., &amp; Ba, J. (2022). Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910.</p>            </div>
        </div>

    </div>
</body>
</html>
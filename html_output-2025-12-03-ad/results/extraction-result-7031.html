<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7031 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7031</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7031</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-259341991</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.01548v1.pdf" target="_blank">Knowledge Graph for NLG in the context of conversational agents</a></p>
                <p><strong>Paper Abstract:</strong> The use of knowledge graphs (KGs) enhances the accuracy and comprehensiveness of the responses provided by a conversational agent. While generating answers during conversations consists in generating text from these KGs, it is still regarded as a challenging task that has gained significant attention in recent years. In this document, we provide a review of different architectures used for knowledge graph-to-text generation including: Graph Neural Networks, the Graph Transformer, and linearization with seq2seq models. We discuss the advantages and limitations of each architecture and conclude that the choice of architecture will depend on the specific requirements of the task at hand. We also highlight the importance of considering constraints such as execution time and model validity, particularly in the context of conversational agents. Based on these constraints and the availability of labeled data for the domains of DAVI, we choose to use seq2seq Transformer-based models (PLMs) for the Knowledge Graph-to-Text Generation task. We aim to refine benchmark datasets of kg-to-text generation on PLMs and to explore the emotional and multilingual dimensions in our future work. Overall, this review provides insights into the different approaches for knowledge graph-to-text generation and outlines future directions for research in this area.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7031.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7031.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearization+Seq2Seq</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph linearization with sequence-to-sequence pre-trained language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert knowledge graphs into a linear token sequence (linearization of nodes/triples) and fine-tune pre-trained seq2seq Transformers (e.g., GPT, BART, T5) to generate text; favored in this paper for conversational-agent constraints due to inference speed and available tooling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearized graph sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graphs are serialized into a flat sequence of tokens (e.g., ordered nodes and/or RDF triples) which is used as text-like input to a seq2seq model; linearizations may attempt to keep topology by ordering triples or concatenating node/edge strings but are represented as plain token sequences without explicit graph pointers.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>graph-to-sequence linearization (ordered triple/node serialization) — (paper discusses general linearization; specific traversal algorithms not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG / AMR / Genwiki (as reported in review references)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (knowledge-graph verbalization, AMR-to-text in referenced works)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART, T5, GPT (pre-trained seq2seq / decoder-only LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained language models (PLMs) trained on large text corpora (e.g., BART denoising seq2seq, T5 text-to-text, GPT family language models) fine-tuned on linearized KG→text datasets; used because they generalize well from text-only pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>general NLG metrics and semantic-fidelity checks mentioned in the review (BLEU/ROUGE/METEOR commonly used in literature; semantic fidelity notions: hallucination/omission/redundancy/accuracy/ordering; also NLI-based faithfulness and FEQA referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables direct fine-tuning of strong PLMs with simple pipeline and good fluency; benefits from text pretraining and typically yields strong linguistic coherence with modest fine-tuning effort.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lossy: structural information of graphs is often lost or diluted during linearization; harder to learn explicit graph↔text alignments; can be computationally costly to linearize and to pretrain at large scale; may produce hallucinations or omissions due to missing explicit structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to GNNs/Graph-Transformers, linearization is simpler and easier to deploy (faster inference, lower engineering cost) but weaker at preserving explicit graph topology and alignment; the paper favors seq2seq linearization for conversational-agent constraints (latency, existing infra).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7031.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7031.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K-BERT: Enabling language representation with knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-based augmentation that injects KG triplets into sentence structure by expanding sentence entities into a sentence-level tree and controlling attention using a visibility matrix and masked attention (Mask-Transformer).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>K-bert: Enabling language representation with knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>K-BERT expanded sentence tree with visibility matrix</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Input sentences are augmented by expanding entities with their KG triplets into a sentence tree; a Visibility Matrix encodes which expanded tokens can attend to which others, and a modified self-attention (Mask-Transformer) uses this matrix to limit/weight attention, preserving original sentence structure while embedding KG facts.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical / token-based (augmented text with attention masks), largely lossless for local expansions</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>entity expansion into a sentence-tree (triplet-driven expansion) combined with a visibility matrix to control attention; representation remains as token sequence plus an attention-visibility mask.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>knowledge-enhanced language representation / graph-aware text encoding for downstream NLG tasks</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>K-BERT (modified Transformer with Mask-Transformer and visibility-aware attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer augmented with a knowledge-layer that expands sentences with triplets, a Seeing Layer to preserve original sentence structure, and a Mask-Transformer that modifies self-attention using the visibility matrix.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Designed to inject KG facts into PLM inputs so that models trained on textual objectives can access structured KG information without separate graph encoders; reported in literature to improve knowledge-aware representations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Introduces architectural modifications (visibility masks, expanded token sequences) and increased input size; may complicate attention computation and increase computational cost; potential scaling issues for large expansions not detailed in review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Provides a middle ground between pure linearization and graph-specific encoders: preserves more graph-local structure than naive linearization while keeping Transformer pipelines, but unlike explicit GNN/GT methods it encodes structure via attention masking rather than native graph operators.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7031.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7031.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDF-triple sequence (GTR-LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Triple encoder for sentence generation from RDF data (GTR-LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representation that encodes RDF triples (subject, predicate, object) as a structured/triple-wise input and uses a triple encoder (GTR-LSTM) to generate sentences from RDF data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GTR-LSTM: A triple encoder for sentence generation from RDF data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>RDF triple sequence (triple-wise serialization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graphs expressed as ordered RDF triples; triples are encoded (often as grouped subject-predicate-object units) by a dedicated encoder (triple encoder) before being passed to a decoder for text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (triple-ordered), structured/token-based, partially lossless if triple grouping preserved</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>serialize graph as a list/sequence of RDF triples (subject|predicate|object grouping); GTR-LSTM variant encodes triples with a specialized encoder (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>RDF datasets / WebNLG (papers on RDF-to-text commonly evaluate on WebNLG; review cites WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RDF-to-text / WebNLG-style graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GTR-LSTM (triple encoder + LSTM decoder) as cited</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A model that encodes RDF triple groups with a triple encoder and generates sentences with an LSTM-based decoder; an example of encoding graph data as grouped triple sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Keeps RDF structure explicit through grouped triples which can improve content selection and alignment between triples and output tokens compared to naïve linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires specialized encoders and may not leverage large PLM pretraining as directly as seq2seq PLMs; scalability and inference-time characteristics not reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>More structure-preserving than plain linearization; simpler than full GNN/Graph-Transformer pipelines but may be outperformed by PLM-based seq2seq approaches in linguistic fluency unless combined with PLM pretraining.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7031.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7031.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR-to-text linearized representation (AMR linearization for LM training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Abstract Meaning Representation (AMR) graphs are linearized into sequences and used as inputs to language models for AMR-to-text generation; cited works explore a language-model-first approach using PLMs for this representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-too: A language-model-first approach for amr-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearized AMR sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR graphs are converted to a flat sequence (a serialized AMR) that preserves node labels and relation markers; the sequence is provided to a language model (e.g., GPT-based) to generate natural text.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy to varying degree depending on linearization scheme</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>AMR serialization / linearization (specific traversal not specified in review; referenced works use LM-first approaches that accept serialized AMR as text input).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR corpora (AMR datasets referenced indirectly in review via AMR-to-text literature)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-first approaches (as in cited GPT-too work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Language-model-first approach: use large pre-trained generative LMs (GPT family) fine-tuned on serialized AMR→text pairs to leverage strong textual pretraining for graph-to-text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Allows reuse of strong text-only pretraining (LMs) for structured-input tasks by treating serialized graphs as text; reported to be effective in referenced work but specific gains not detailed in review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Linearization of AMR can lose explicit graph topology and relations; learning alignments between AMR structures and text can be challenging with limited parallel data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Similar trade-offs as KG linearization: better ease-of-use and fluency when leveraging PLMs but weaker explicit structural modeling compared to graph-native models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7031.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7031.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenWiki dataset linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genwiki: content-sharing text and graphs dataset for unsupervised graph-to-text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale dataset pairing text and graphs (1.3M instances) designed for unsupervised graph-to-text training and evaluation; used as source material for building or pretraining graph-to-text models using linearized representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>content-sharing graph↔text pairs (linearized for unsupervised training)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Contains text documents paired with graph-structured content (the review indicates this dataset supports unsupervised cycle-training approaches where graphs and texts can be linearized for both directions), enabling unsupervised pretraining of graph↔text mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (when linearized), dataset for unsupervised/paired training</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>graphs can be linearized into text-like sequences for use in text-to-text or cycle-training setups (specific linearization algorithms not described in review)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Genwiki</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>unsupervised graph-to-text / text-to-graph pretraining and generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset used to pretrain or train models (e.g., cycle training frameworks) in unsupervised graph↔text learning; referenced as enabling large-scale non-parallel training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides large-scale paired/weakly-paired data enabling unsupervised or semi-supervised pretraining strategies that can improve graph↔text alignment learning without heavy annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality and exact format of linearization-dependent signals are dataset-specific; review does not report concrete performance numbers or canonicalization details.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Offers scale (1.3M) compared to smaller supervised datasets (e.g., WebNLG), enabling unsupervised pretraining approaches that can complement PLM fine-tuning or structural encoder methods.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Investigating pretrained language models for graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Promoting graph awareness in linearized graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Gpt-too: A language-model-first approach for amr-to-text generation <em>(Rating: 2)</em></li>
                <li>GTR-LSTM: A triple encoder for sentence generation from RDF data <em>(Rating: 2)</em></li>
                <li>Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Jointgt: Graph-text joint representation learning for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>Kgpt: Knowledge-grounded pretraining for data-to-text generation <em>(Rating: 2)</em></li>
                <li>Text generation from knowledge graphs with graph transformers <em>(Rating: 1)</em></li>
                <li>Structural adapters in pretrained language models for amr-to-text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7031",
    "paper_id": "paper-259341991",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Linearization+Seq2Seq",
            "name_full": "Graph linearization with sequence-to-sequence pre-trained language models",
            "brief_description": "Convert knowledge graphs into a linear token sequence (linearization of nodes/triples) and fine-tune pre-trained seq2seq Transformers (e.g., GPT, BART, T5) to generate text; favored in this paper for conversational-agent constraints due to inference speed and available tooling.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Linearized graph sequence",
            "representation_description": "Graphs are serialized into a flat sequence of tokens (e.g., ordered nodes and/or RDF triples) which is used as text-like input to a seq2seq model; linearizations may attempt to keep topology by ordering triples or concatenating node/edge strings but are represented as plain token sequences without explicit graph pointers.",
            "representation_type": "sequential, token-based, lossy",
            "encoding_method": "graph-to-sequence linearization (ordered triple/node serialization) — (paper discusses general linearization; specific traversal algorithms not specified)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG / AMR / Genwiki (as reported in review references)",
            "task_name": "graph-to-text generation (knowledge-graph verbalization, AMR-to-text in referenced works)",
            "model_name": "BART, T5, GPT (pre-trained seq2seq / decoder-only LMs)",
            "model_description": "Pre-trained language models (PLMs) trained on large text corpora (e.g., BART denoising seq2seq, T5 text-to-text, GPT family language models) fine-tuned on linearized KG→text datasets; used because they generalize well from text-only pretraining.",
            "performance_metric": "general NLG metrics and semantic-fidelity checks mentioned in the review (BLEU/ROUGE/METEOR commonly used in literature; semantic fidelity notions: hallucination/omission/redundancy/accuracy/ordering; also NLI-based faithfulness and FEQA referenced)",
            "performance_value": null,
            "impact_on_training": "Enables direct fine-tuning of strong PLMs with simple pipeline and good fluency; benefits from text pretraining and typically yields strong linguistic coherence with modest fine-tuning effort.",
            "limitations": "Lossy: structural information of graphs is often lost or diluted during linearization; harder to learn explicit graph↔text alignments; can be computationally costly to linearize and to pretrain at large scale; may produce hallucinations or omissions due to missing explicit structure.",
            "comparison_with_other": "Compared to GNNs/Graph-Transformers, linearization is simpler and easier to deploy (faster inference, lower engineering cost) but weaker at preserving explicit graph topology and alignment; the paper favors seq2seq linearization for conversational-agent constraints (latency, existing infra).",
            "uuid": "e7031.0"
        },
        {
            "name_short": "K-BERT",
            "name_full": "K-BERT: Enabling language representation with knowledge graph",
            "brief_description": "A Transformer-based augmentation that injects KG triplets into sentence structure by expanding sentence entities into a sentence-level tree and controlling attention using a visibility matrix and masked attention (Mask-Transformer).",
            "citation_title": "K-bert: Enabling language representation with knowledge graph",
            "mention_or_use": "mention",
            "representation_name": "K-BERT expanded sentence tree with visibility matrix",
            "representation_description": "Input sentences are augmented by expanding entities with their KG triplets into a sentence tree; a Visibility Matrix encodes which expanded tokens can attend to which others, and a modified self-attention (Mask-Transformer) uses this matrix to limit/weight attention, preserving original sentence structure while embedding KG facts.",
            "representation_type": "hierarchical / token-based (augmented text with attention masks), largely lossless for local expansions",
            "encoding_method": "entity expansion into a sentence-tree (triplet-driven expansion) combined with a visibility matrix to control attention; representation remains as token sequence plus an attention-visibility mask.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "knowledge-enhanced language representation / graph-aware text encoding for downstream NLG tasks",
            "model_name": "K-BERT (modified Transformer with Mask-Transformer and visibility-aware attention)",
            "model_description": "Transformer augmented with a knowledge-layer that expands sentences with triplets, a Seeing Layer to preserve original sentence structure, and a Mask-Transformer that modifies self-attention using the visibility matrix.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Designed to inject KG facts into PLM inputs so that models trained on textual objectives can access structured KG information without separate graph encoders; reported in literature to improve knowledge-aware representations.",
            "limitations": "Introduces architectural modifications (visibility masks, expanded token sequences) and increased input size; may complicate attention computation and increase computational cost; potential scaling issues for large expansions not detailed in review.",
            "comparison_with_other": "Provides a middle ground between pure linearization and graph-specific encoders: preserves more graph-local structure than naive linearization while keeping Transformer pipelines, but unlike explicit GNN/GT methods it encodes structure via attention masking rather than native graph operators.",
            "uuid": "e7031.1"
        },
        {
            "name_short": "RDF-triple sequence (GTR-LSTM)",
            "name_full": "Triple encoder for sentence generation from RDF data (GTR-LSTM)",
            "brief_description": "Representation that encodes RDF triples (subject, predicate, object) as a structured/triple-wise input and uses a triple encoder (GTR-LSTM) to generate sentences from RDF data.",
            "citation_title": "GTR-LSTM: A triple encoder for sentence generation from RDF data",
            "mention_or_use": "mention",
            "representation_name": "RDF triple sequence (triple-wise serialization)",
            "representation_description": "Graphs expressed as ordered RDF triples; triples are encoded (often as grouped subject-predicate-object units) by a dedicated encoder (triple encoder) before being passed to a decoder for text generation.",
            "representation_type": "sequential (triple-ordered), structured/token-based, partially lossless if triple grouping preserved",
            "encoding_method": "serialize graph as a list/sequence of RDF triples (subject|predicate|object grouping); GTR-LSTM variant encodes triples with a specialized encoder (details in cited work).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "RDF datasets / WebNLG (papers on RDF-to-text commonly evaluate on WebNLG; review cites WebNLG)",
            "task_name": "RDF-to-text / WebNLG-style graph-to-text generation",
            "model_name": "GTR-LSTM (triple encoder + LSTM decoder) as cited",
            "model_description": "A model that encodes RDF triple groups with a triple encoder and generates sentences with an LSTM-based decoder; an example of encoding graph data as grouped triple sequences.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Keeps RDF structure explicit through grouped triples which can improve content selection and alignment between triples and output tokens compared to naïve linearization.",
            "limitations": "Requires specialized encoders and may not leverage large PLM pretraining as directly as seq2seq PLMs; scalability and inference-time characteristics not reported in the review.",
            "comparison_with_other": "More structure-preserving than plain linearization; simpler than full GNN/Graph-Transformer pipelines but may be outperformed by PLM-based seq2seq approaches in linguistic fluency unless combined with PLM pretraining.",
            "uuid": "e7031.2"
        },
        {
            "name_short": "AMR linearization",
            "name_full": "AMR-to-text linearized representation (AMR linearization for LM training)",
            "brief_description": "Abstract Meaning Representation (AMR) graphs are linearized into sequences and used as inputs to language models for AMR-to-text generation; cited works explore a language-model-first approach using PLMs for this representation.",
            "citation_title": "Gpt-too: A language-model-first approach for amr-to-text generation",
            "mention_or_use": "mention",
            "representation_name": "Linearized AMR sequence",
            "representation_description": "AMR graphs are converted to a flat sequence (a serialized AMR) that preserves node labels and relation markers; the sequence is provided to a language model (e.g., GPT-based) to generate natural text.",
            "representation_type": "sequential, token-based, lossy to varying degree depending on linearization scheme",
            "encoding_method": "AMR serialization / linearization (specific traversal not specified in review; referenced works use LM-first approaches that accept serialized AMR as text input).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR corpora (AMR datasets referenced indirectly in review via AMR-to-text literature)",
            "task_name": "AMR-to-text generation",
            "model_name": "GPT-first approaches (as in cited GPT-too work)",
            "model_description": "Language-model-first approach: use large pre-trained generative LMs (GPT family) fine-tuned on serialized AMR→text pairs to leverage strong textual pretraining for graph-to-text.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Allows reuse of strong text-only pretraining (LMs) for structured-input tasks by treating serialized graphs as text; reported to be effective in referenced work but specific gains not detailed in review.",
            "limitations": "Linearization of AMR can lose explicit graph topology and relations; learning alignments between AMR structures and text can be challenging with limited parallel data.",
            "comparison_with_other": "Similar trade-offs as KG linearization: better ease-of-use and fluency when leveraging PLMs but weaker explicit structural modeling compared to graph-native models.",
            "uuid": "e7031.3"
        },
        {
            "name_short": "GenWiki dataset linearization",
            "name_full": "Genwiki: content-sharing text and graphs dataset for unsupervised graph-to-text",
            "brief_description": "A large-scale dataset pairing text and graphs (1.3M instances) designed for unsupervised graph-to-text training and evaluation; used as source material for building or pretraining graph-to-text models using linearized representations.",
            "citation_title": "Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation",
            "mention_or_use": "mention",
            "representation_name": "content-sharing graph↔text pairs (linearized for unsupervised training)",
            "representation_description": "Contains text documents paired with graph-structured content (the review indicates this dataset supports unsupervised cycle-training approaches where graphs and texts can be linearized for both directions), enabling unsupervised pretraining of graph↔text mappings.",
            "representation_type": "sequential (when linearized), dataset for unsupervised/paired training",
            "encoding_method": "graphs can be linearized into text-like sequences for use in text-to-text or cycle-training setups (specific linearization algorithms not described in review)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Genwiki",
            "task_name": "unsupervised graph-to-text / text-to-graph pretraining and generation",
            "model_name": null,
            "model_description": "Dataset used to pretrain or train models (e.g., cycle training frameworks) in unsupervised graph↔text learning; referenced as enabling large-scale non-parallel training.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Provides large-scale paired/weakly-paired data enabling unsupervised or semi-supervised pretraining strategies that can improve graph↔text alignment learning without heavy annotation.",
            "limitations": "Quality and exact format of linearization-dependent signals are dataset-specific; review does not report concrete performance numbers or canonicalization details.",
            "comparison_with_other": "Offers scale (1.3M) compared to smaller supervised datasets (e.g., WebNLG), enabling unsupervised pretraining approaches that can complement PLM fine-tuning or structural encoder methods.",
            "uuid": "e7031.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Investigating pretrained language models for graph-to-text generation",
            "rating": 2,
            "sanitized_title": "investigating_pretrained_language_models_for_graphtotext_generation"
        },
        {
            "paper_title": "Promoting graph awareness in linearized graph-to-text generation",
            "rating": 2,
            "sanitized_title": "promoting_graph_awareness_in_linearized_graphtotext_generation"
        },
        {
            "paper_title": "Gpt-too: A language-model-first approach for amr-to-text generation",
            "rating": 2,
            "sanitized_title": "gpttoo_a_languagemodelfirst_approach_for_amrtotext_generation"
        },
        {
            "paper_title": "GTR-LSTM: A triple encoder for sentence generation from RDF data",
            "rating": 2,
            "sanitized_title": "gtrlstm_a_triple_encoder_for_sentence_generation_from_rdf_data"
        },
        {
            "paper_title": "Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation",
            "rating": 2,
            "sanitized_title": "genwiki_a_dataset_of_13_million_contentsharing_text_and_graphs_for_unsupervised_graphtotext_generation"
        },
        {
            "paper_title": "Jointgt: Graph-text joint representation learning for text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "jointgt_graphtext_joint_representation_learning_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "Kgpt: Knowledge-grounded pretraining for data-to-text generation",
            "rating": 2,
            "sanitized_title": "kgpt_knowledgegrounded_pretraining_for_datatotext_generation"
        },
        {
            "paper_title": "Text generation from knowledge graphs with graph transformers",
            "rating": 1,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Structural adapters in pretrained language models for amr-to-text generation",
            "rating": 1,
            "sanitized_title": "structural_adapters_in_pretrained_language_models_for_amrtotext_generation"
        }
    ],
    "cost": 0.012810499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Knowledge Graph for NLG in the context of conversational agents</p>
<p>Hussam Ghanem 
Massinissa Atmani 
Christophe Cruz 
Knowledge Graph for NLG in the context of conversational agents
Received: date / Accepted: dateNoname manuscript No. (will be inserted by the editor)Conversational agents · Knowledge graphs · Natural Language Generation
The use of knowledge graphs (KGs) enhances the accuracy and comprehensiveness of the responses provided by a conversational agent. While generating answers during conversations consists in generating text from these KGs, it is still regarded as a challenging task that has gained significant attention in recent years. In this document, we provide a review of different architectures used for knowledge graph-to-text generation including: Graph Neural Networks, the Graph Transformer, and linearization with seq2seq models. We discuss the advantages and limitations of each architecture and conclude that the choice of architecture will depend on the specific requirements of the task at hand. We also highlight the importance of considering constraints such as execution time and model validity, particularly in the context of conversational agents. Based on these constraints and the availability of labeled data for the domains of DAVI, we choose to use seq2seq Transformer-based models (PLMs) for the Knowledge Graph-to-Text Generation task. We aim to refine benchmark datasets of kg-to-text generation on PLMs and to explore the emotional and multilingual dimensions in our future work. Overall, this review provides insights into the different approaches for knowledge graph-to-text generation and outlines future directions for research in this area.</p>
<p>integrated with messaging platforms, mobile applications, and websites to provide instant support to customers and handle simple tasks, such as answering questions or helping with bookings. Conversational agents using knowledge graphs (KG) [9] are a type of chatbot that leverages structured data stored in a knowledge graph to generate human-like responses. The knowledge graph is a graph-based representation of entities and their relationships, providing a structured source of information for the chatbot to access and use during conversation. This enables the chatbot to provide more accurate and comprehensive answers to user's questions. The use of knowledge graphs can greatly enhance the capabilities of conversational agents and make interactions more informative and useful [57].</p>
<p>Generating answers during conversations consists in generating text from data. Data-to-text processes require algorithms that generate linguistically correct sentences for humans and express the semantics and structure of nonlinguistic data (sequence, tree, graph, etc.). In addition, the generation of textual answers requires supporting several languages. And for a better interaction with a conversational agent, the emotional context of the conversation (Common Ground) is fundamental. The aim of this work in collaboration with the company DAVI is to integrate a socio-emotional dimension into humanmachine interactions which complement the technical and "business" skills linked to professional expertise. The company DAVI is a software publisher in SaaS mode which has expertise in the fields of AI, Affective Computing, and Human Machine Interactions (HMIs). The following picture presents the composite AI of DAVI's solution including Natural Language Understanding, Emotion detection, skills modeling, Natural Language Generation, and Body Language Generation. For now, the natural language generation (NLG) of a conversational engine does not benefit from the latest technological advances in natural language processing (NLP). The Natural Language processing step is based on a manual process to define the template of the answer. This process requires a costly amount of time. Thus, the purpose of this project is to automate and reduce the burden of the generation of template-based responses (as the responses are manually written through a set of rules) in the implementation of conversa-tional agents. The template-based responses are modellized and stored in the skills' database. Regarding the emotional dimension, emotional responses are injected automatically in the answer depending the emotional analysis of the user. To automate NLG answers from Skills, knowledge graphs were identified. In NLG, Two criteria [33] are used to assess the quality of the produced answers. The first criterion is semantic consistency (Semantic Fidelity) which quantifies the fidelity of the data produced against the input data. The most common indicators are 1/ Hallucination: It is manifested by the presence of information (facts) in the generated text that is not present in the input data; 2/ Omission: It is manifested by the omission of one of the pieces of information (facts) in the generated text; 3/ Redundancy: This is manifested by the repetition of information in the generated text; 4/ Accuracy: The lack of accuracy is manifested by the modification of information such as the inversion of the subject and the direct object complement in the generated text; 5/ Ordering: It occurs when the sequence of information is different from the input data. The second criterion is linguistic coherence (Output Fluency) to evaluate the fluidity of the text and the linguistic constructions of the generated text, the segmentation of the text into different sentences, the use of anaphoric pronouns to reference entities and to have linguistically correct sentences.</p>
<p>Today, neural approaches offer performances exceeding all classical methods for linguistic coherence. However, limits are still present to maintain semantic consistency, and their performance deteriorates even more on long texts [30]. Another limitation due to the complexity of neural approaches is that text generation is non-parameterized with no control over the structure of the generated text. Thus, most of the current neural approaches arrive behind template-based approaches on the criterion of semantic consistency [31], but they are far superior to them on the criterion of linguistic consistency. This can be explained by the fact that large language models manage to capture certain syntactic and semantic properties of the language.</p>
<p>The present review is organized as follows, Section 2 presents a comprehensive overview of the current state-of-the-art approaches for knowledge graphto-text generation. In Section 3, we present the latest architectures and techniques that have been proposed in this field. Finally, Section 4 critically examines the strengths and limitations of these techniques in the context of conversational agents 2 Knowledge Graph-to-Text Generation KG-to-text generation aims at producing easy-to-understand sentences in natural language from knowledge graphs (KGs) while maintaining semantic consistency between the generated sentences and the KG triplets. Compared to the traditional text generation task (Seq2Seq), generating text from a knowledge graph is an additional challenge to guarantee the authenticity of the words in the generated sentences. The existing methods can be classified according to three categories ( Figure 3) and will be detailed later:</p>
<p>-Linearisation with Sequence-to-sequence (Seq2Seq): convert the graph to a sequence which is the fed to a sequence-to-sequence model; -Graph Neural Networks (GNNs): encode topological structures of a graph and learn the representation of an entity by the aggregation of the features of the entities and neighbors. They are not used as a standalone model and require a decoder to complete the encoder-decoder architecture; -Graph Transformer (GT): the enhanced version of the original transformer adapted to handle graphs.</p>
<p>The term "knowledge graph" has been around since 1972, but its current definition can be traced back to Google's 2012 announcement of their Knowledge Graph. This was followed by similar announcements from companies such as Airbnb, Amazon, eBay, Facebook, IBM, LinkedIn, Microsoft, and Uber, among others, leading to an increase in the adoption of knowledge graphs by various industries [4]. As a result, academic research in this field has seen a surge in recent years, with an increasing number of scientific publications on knowledge graphs [4]. These graphs utilize a graph-based data model to effectively manage, integrate, and extract valuable insights from large and diverse datasets [5].</p>
<p>Knowledge graphs, which are composed of nodes that represent different types of entities and edges that denote various types of relationships between those entities, are known as heterogeneous graphs. The integration of information from multiple sources and domains in knowledge graphs leads to an even greater degree of heterogeneity. To address this, recent research has applied heterogeneous graph embedding methods to represent knowledge graphs effectively. For example, ERNIE [40] and KnowBERT [41] employ knowledge graph embedding techniques such as TransE [42] and TuckER [58] to encode knowledge graphs.</p>
<p>Generating text and learning alignments between source entities/relationships and target tokens from scratch is a challenging task for standard language models because of the limited amount of parallel graph-text data [17,34]. To overcome this limitation, recent research has focused on developing generalist pre-trained language models for KG-to-text generation. A common approach is to linearize input graphs into text sequences and fine-tune pre-trained seq2seq Transformer models such as GPT [35], BART [10], or T5 [36] based on KG-to-text datasets [1,37]. These pre-trained language models can generate high-quality texts with a simple fine-tuning to the target task, thanks to their self-supervised pre-training on large-scale corpora of unlabeled texts. In fact, pre-trained language models outperform other models with sophisticated structures in KG-to-text generation tasks. This type of approach will be detailed in section 3.</p>
<p>According to [14], text generation tasks using KG-to-text models mainly fall under three aspects:</p>
<p>-Encoder modification: To reduce the loss of structural information in sequence encoders with graph inputs that have been linearized [24,6,32], proposals concentrate on constructing more intricate encoder structures to improve the representation of graphs, including GNNs and GTs; -Unsupervised training: These proposals consist in designing unsupervised training methods to jointly learn graph-to-text and text-to-graph con-version tasks with non-parallel graph-to-text data [39,17,18]. This makes it possible to compare the final result of the process with the input data; -Build pre-trained models: With the development of pre-trained Natural Language Generation (NLG) models such as GPT, BART, and T5, some recent work directly refines these models on graph-to-text datasets and reports significant performance [1,37,15,2].</p>
<p>Compared to existing work on pre-trained models for KG-text generation, the JoinGT model [14] uses pre-training methods to explicitly learn graphtext alignments instead of directly tuning the pre-trained models seq2seq on KG-to-text datasets.</p>
<p>Architectures</p>
<p>In this section, the different architectures used in data-to-text tasks will be presented. As the nature of the data greatly influences the choice of the architecture of the neural approaches, most works either try to adapt the inputs to the architectures of the models or propose new architectures better adapted to the types of input data. Due to the nature of sentences and the tree structure of its representations, several works have proposed to model data structures of this type to enhance performance.</p>
<p>Graph linearisation and sequence to sequence models (Seq2seq)</p>
<p>Recent years have been marked by significant achievements in the field of PLMs (Pretrained Language Models) [38,44]. Pre-trained on massive corpora, PLMs exhibit good generalization ability to solve related NLG (Natural Language Generation) downstream tasks [43]. However, most existing PLMs were trained on textual data [44,10] without ingesting any structured data input. The seq-to-seq category consists of linearizing the KG [1, 25, 24, 3] and then formulating a Seq2seq generation task using PLMs like GPT [35], BART [10] or T5 [36] with linearized KG nodes as input to generate sentences. The use of pre-trained language models (PLMs) in KG-to-text generation has shown superior performance, but still faces two major challenges: 1) loss of structural information during encoding, as existing models like BERT do not explicitly take into account the relationship between input entities; and 2) lack of explicit graph-text alignments, as complex knowledge graph structures make it difficult to learn graph-text alignments through text reconstruction-based pretraining tasks. Despite attempts to retain as much of the graph topology as possible with seq2seq methods, the Transformer-based seq2seq models' cost is not cheap (especially in the pretraining phase). Also, the computational cost of linearization can be high for large knowledge graphs. Hence, and so to better keep the graph topology, Graph Neural Networks (GNNs) have been proposed, which will be discussed in the next section.</p>
<p>Graph Neural Networks (GNNs)</p>
<p>Different approaches use different variants of GNNs architectures such as GCNs (Graph Convolutional Networks) [27] or extensions of GCNs such as Syn-GCNs [45] or DCGCNs [11]. Other approaches use the variant GATs (Graph Attention Networks) [8]. Or approaches that use the GGNNs (Gated Graph Neural Networks) variant [16,21,47]. Graph Neural Networks (GNNs) are a type of neural network that are well-suited for processing graph-structured data. In the context of knowledge graph-to-text generation, GNNs can be used to model the relationships between entities in a knowledge graph and generate text based on those relationships. Recent research on using GNNs for knowledge graph to text generation has shown promising results. Some studies have used graph convolutional networks (GCNs) [27] to encode the relationships between entities in a knowledge graph into a low-dimensional representation, or extensions of GCNs such as Syn-GCNs [45] and DCGCNs [11]. Other studies have used graph attention networks (GATs) [8] to dynamically weight the importance of different entities and relationships in the knowledge graph when generating text. Other studies have used a gating mechanism (Gated Graph Neural Networks or GGNNs) that allows for effectively controlling the flow of information between nodes in the graph, which is useful for incorporating contextual information [16,21,47]. Additionally, some researchers have combined GNNs with reinforcement learning to generate text that maximizes a reward function defined over the generated text and the knowledge graph [21] .</p>
<p>Overall, the use of GNNs for knowledge graph to text generation is an active area of research, with many recent studies exploring different architectures and training methods. The results so far suggest that GNNs can effectively capture the relationships between entities in a knowledge graph and generate high-quality text based on that information. A limitation relied to KG-to-Text generation with GNNs, is that GNNs can be computationally expensive and may struggle to handle large knowledge graphs. Additionally, their performance may degrade for graphs with complex relationships or structures. Despite these limitations, GNNs remain a promising direction for knowledge graph-to-text generation.</p>
<p>Graphs Transformers (GTs)</p>
<p>In order to benefit from the power of models based on Transformer and to be able to model tree or graph-type data structures as with GNNs, and to overcome the limitations of local neighborhood aggregation while avoiding strict structural inductive biases, recent works have proposed to adapt the Transformer architecture. As Graph Transformers are equipped with self-attention mechanisms, they can capture global context information by applying these mechanisms to the nodes of the graph.</p>
<p>According to [12], GT differs from GNNs in that it allows direct modeling of dependencies between any pair of nodes regardless of their distance in the input graph. An undesirable consequence is that it essentially treats any graph as a fully connected graph, greatly reducing the explicit structure of the graph. To maintain a structure-aware view of the graph, their proposed model introduces an explicit relationship encoding and integrates it into the pairwise attention score computation as a dynamic parameter.</p>
<p>From the GNNs pipeline, if we make several parallel heads of neighborhood aggregation and replace the sum on the neighbors by the attention mechanism, e.g. a weighted sum, we would get the Graph Attention Network (GAT). Adding normalization and MLP feed-forward, we have a Graph Transformer [46]. For the same reasons as Graph Transformer, [13] presents the K-BERT model, they introduce four components to augment the Transformer architecture and to be able to handle a graph as input. The first knowledge layer component takes a sentence and a set of triplets as input and outputs the sentence tree by expanding the sentence entities with their corresponding triplets. They also add the Seeing Layer component to preserve the original sentence structure and model the relationships of the triples by building a Visibility Matrix. Another component is the Mask-Transformer where they modify the self-attention layer to consider the visibility matrix when calculating attention.</p>
<p>The use of Graph Transformers for Knowledge graph to text generation has gained popularity in recent years due to their ability to effectively handle graph structures and capture the relationships between nodes in the graph. Additionally, Graph Transformers can handle large graphs and have the ability to model long-range dependencies between nodes in the graph. Despite the advantages, the training of Graph Transformers can be computationally expensive and the interpretability of the model remains a challenge. Overall, the use of Graph Transformers for Knowledge graph to text generation is a promising area of research and can lead to significant improvements in the generation of text from knowledge graphs.</p>
<p>Discusion</p>
<p>The Graph Neural Network (GNN), the Graph Transformer, and linearization with seq2seq models are three different architectures for knowledge graph to text generation, a task that involves generating natural language text from structured knowledge representations like knowledge graphs. GNNs are a type of deep learning model that are well-suited for processing graph-structured data. They provide a flexible and scalable way to model the graph structure and relationships, but they may not be able to efficiently handle large and complex graphs. The Graph Transformer is a specialized version of the Transformer, designed for graph-to-sequence learning tasks. It provides a more direct and efficient way to process the graph structure compared to linearization with seq2seq models, but it may require more training data and computation resources. Linearization with seq2seq models is a simpler and easier to implement approach that involves converting the knowledge graph into a linear sequence, such as a sentence, and then using a seq2seq model to generate text from the linearized representation. However, this approach can lose some of the structural information and relationships in the knowledge graph during the linearization process.</p>
<p>In summary, each of the three architectures has its own advantages and disadvantages, and the choice of architecture will depend on the specific requirements of the actual task.</p>
<p>As our project is part of a context of conversational agents, we must take into account all the consequent constraints such as the execution time (to respect the instant conversation constraint) and the validity of the model where the answer must not be incorrect or ambiguous. If most of the current models have satisfactory validity performances, the inference time of models based on GNN and GraphTransformer exceeds the limit threshold found in a fluid and natural conversation and requires a huge memory load that violates the standards of current industrialization (MLOps) of neural models.</p>
<p>In light of these elements, and with the constraint of the data labellisation for the domains of DAVI, we choose to go further with seq2seq Transformer based models (PLMs) in our Knowledge Graph-to-Text Generation. We also want to shed light on the fact that DAVI already handles such models in their pipeline and they have the knowledge and infrastructure to optimize the integration and deployment of the seq2seq models. Hence, DAVI should still remain in control of the time to market of the NLG solution.</p>
<p>Conclusion</p>
<p>In conclusion, the document discusses different data-to-text architectures and highlights their advantages and limitations in the context of graph-to-text project with DAVI. The Graph Neural Network (GNN), Graph Transformer, and seq2seq models are three approaches that have been applied to the task of generating natural language text from structured knowledge representations like knowledge graphs. Each approach has its own advantages and disadvantages, and the choice of architecture will depend on the specific requirements of the task. Considering the constraints of our project, which includes developing a conversational agent that must generate valid responses in real-time, we have decided to move forward with seq2seq Transformer-based models (PLMs) as they have satisfactory performance on validity and execution time. Additionally, DAVI already handles such models in their pipeline and can optimize their integration and deployment. Our next step will be to explore state-of-the-art approaches that take into account the emotional and multilingual dimensions to achieve the objectives of the graph-to-text project.</p>
<p>Fig. 1
1Composite AI pipeline of virtual agents at DAVI</p>
<p>Fig. 2
2Automatic text generation from the knowledge graph</p>
<p>Fig. 3
3The architecture of KG-to-text generation with the three categories of representation, a) Linearization + Seq2Seq, b) GNNs with decoder (e.g. LSTM), and c) Graph Transformer (GT)</p>
<p>Investigating pretrained language models for graph-to-text generation. L F Ribeiro, M Schmitt, H Schütze, I Gurevych, arXiv:2007.08426arXiv preprintRibeiro, L. F., Schmitt, M., Schütze, H., &amp; Gurevych, I. (2020). Investigating pretrained language models for graph-to-text generation. arXiv preprint arXiv:2007.08426.</p>
<p>Gpt-too: A language-model-first approach for amr-to-text generation. M Mager, R F Astudillo, T Naseem, M A Sultan, Y S Lee, R Florian, S Roukos, arXiv:2005.09123arXiv preprintMager, M., Astudillo, R. F., Naseem, T., Sultan, M. A., Lee, Y. S., Florian, R.,&amp; Roukos, S. (2020). Gpt-too: A language-model-first approach for amr-to-text generation. arXiv preprint arXiv:2005.09123.</p>
<p>Promoting graph awareness in linearized graph-to-text generation. A Hoyle, A Marasović, N Smith, arXiv:2012.15793arXiv preprintHoyle, A., Marasović, A.,&amp; Smith, N. (2020). Promoting graph awareness in linearized graph-to-text generation. arXiv preprint arXiv:2012.15793.</p>
<p>Knowledge graphs. A Hogan, E Blomqvist, M Cochez, C Amato, G D Melo, C Gutierrez, S Kirrane, J E Gayo, R Navigli, S Neumaier, A C Ngomo, ACM Computing Surveys (CSUR). 544Hogan A, Blomqvist E, Cochez M, d'Amato C, Melo GD, Gutierrez C, Kirrane S, Gayo JE, Navigli R, Neumaier S, Ngomo AC. Knowledge graphs. ACM Computing Surveys (CSUR). 2021 Jul 2;54(4):1-37.</p>
<p>Industryscale knowledge graphs: Lessons and challenges. N F Noy, Y Gao, A Jain, A Narayanan, A Patterson, J Taylor, ACM Queue. 17N. F. Noy, Y. Gao, A. Jain, A. Narayanan, A. Patterson, and J. Taylor. 2019. Industry- scale knowledge graphs: Lessons and challenges. ACM Queue 17, 2 (2019).</p>
<p>GTR-LSTM: A triple encoder for sentence generation from RDF data. B Distiawan, J Qi, R Zhang, W Wang, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1Distiawan, B., Qi, J., Zhang, R.,&amp; Wang, W. (2018, July). GTR-LSTM: A triple encoder for sentence generation from RDF data. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 1627-1637).</p>
<p>Structural adapters in pretrained language models for amr-to-text generation. L F Ribeiro, Y Zhang, I Gurevych, arXiv:2103.09120arXiv preprintRibeiro, L. F., Zhang, Y.,&amp; Gurevych, I. (2021). Structural adapters in pretrained lan- guage models for amr-to-text generation. arXiv preprint arXiv:2103.09120.</p>
<p>Modeling global and local node contexts for text generation from knowledge graphs. L F Ribeiro, Y Zhang, C Gardent, I Gurevych, Transactions of the Association for Computational Linguistics. 8Ribeiro, L. F., Zhang, Y., Gardent, C.,&amp; Gurevych, I. (2020). Modeling global and local node contexts for text generation from knowledge graphs. Transactions of the Association for Computational Linguistics, 8, 589-604.</p>
<p>Opendialkg: Explainable conversational reasoning with attention-based walks over knowledge graphs. S Moon, P Shah, A Kumar, R Subba, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsMoon, S., Shah, P., Kumar, A.,&amp; Subba, R. (2019, July). Opendialkg: Explainable con- versational reasoning with attention-based walks over knowledge graphs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 845-854).</p>
<p>Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, . . Zettlemoyer, L , arXiv:1910.13461arXiv preprintnot graphLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ...&amp; Zettle- moyer, L. (2019). Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461. (not graph)</p>
<p>Densely connected graph convolutional networks for graph-to-sequence learning. Z Guo, Y Zhang, Z Teng, W Lu, Transactions of the Association for Computational Linguistics. 7Guo, Z., Zhang, Y., Teng, Z.,&amp; Lu, W. (2019). Densely connected graph convolutional networks for graph-to-sequence learning. Transactions of the Association for Computa- tional Linguistics, 7, 297-312.</p>
<p>Graph transformer for graph-to-sequence learning. D Cai, W Lam, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence34Cai, D.,&amp; Lam, W. (2020, April). Graph transformer for graph-to-sequence learning. In Proceedings of the AAAI conference on artificial intelligence (Vol. 34, No. 05, pp. 7464- 7471).</p>
<p>. W Liu, P Zhou, Z Zhao, Z Wang, Q Ju, H Deng, P Wang, Liu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H.,&amp; Wang, P. (2020, April).</p>
<p>Enabling language representation with knowledge graph. K-Bert , Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34K-bert: Enabling language representation with knowledge graph. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 03, pp. 2901-2908).</p>
<p>Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. P Ke, H Ji, Y Ran, X Cui, L Wang, L Song, . . Huang, M , arXiv:2106.10502arXiv preprintKe, P., Ji, H., Ran, Y., Cui, X., Wang, L., Song, L., ...&amp; Huang, M. (2021). Jointgt: Graph-text joint representation learning for text generation from knowledge graphs. arXiv preprint arXiv:2106.10502.</p>
<p>Kgpt: Knowledge-grounded pretraining for data-to-text generation. W Chen, Y Su, X Yan, W Y Wang, arXiv:2010.02307arXiv preprintChen, W., Su, Y., Yan, X.,&amp; Wang, W. Y. (2020). Kgpt: Knowledge-grounded pre- training for data-to-text generation. arXiv preprint arXiv:2010.02307.</p>
<p>Toward subgraph guided knowledge graph question generation with graph neural networks. Y Chen, L Wu, M J Zaki, arXiv:2004.06015arXiv preprintChen, Y., Wu, L.,&amp; Zaki, M. J. (2020). Toward subgraph guided knowledge graph question generation with graph neural networks. arXiv preprint arXiv:2004.06015.</p>
<p>Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training. Q Guo, Z Jin, X Qiu, W Zhang, D Wipf, Z Zhang, arXiv:2006.04702arXiv preprintGuo, Q., Jin, Z., Qiu, X., Zhang, W., Wipf, D.,&amp; Zhang, Z. (2020). Cyclegt: Unsu- pervised graph-to-text and text-to-graph generation via cycle training. arXiv preprint arXiv:2006.04702</p>
<p>Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation. Z Jin, Q Guo, X Qiu, Z Zhang, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsJin, Z., Guo, Q., Qiu, X., &amp; Zhang, Z. (2020, December). Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation. In Proceedings of the 28th International Conference on Computational Linguistics (pp. 2398- 2409).</p>
<p>Stage-wise Fine-tuning for Graph-to-Text Generation. Q Wang, S Yavuz, V Lin, H Ji, N Rajani, arXiv:2105.08021arXiv preprintWang, Q., Yavuz, S., Lin, V., Ji, H.,&amp; Rajani, N. (2021). Stage-wise Fine-tuning for Graph-to-Text Generation. arXiv preprint arXiv:2105.08021.</p>
<p>Structural information preserving for graph-to-text generation. L Song, A Wang, J Su, Y Zhang, K Xu, Y Ge, D Yu, arXiv:2102.06749arXiv preprintSong, L., Wang, A., Su, J., Zhang, Y., Xu, K., Ge, Y.,&amp; Yu, D. (2021). Structural information preserving for graph-to-text generation. arXiv preprint arXiv:2102.06749.</p>
<p>Reinforcement learning based graph-to-sequence model for natural question generation. Y Chen, L Wu, M J Zaki, arXiv:1908.04942arXiv preprintChen, Y., Wu, L.,&amp; Zaki, M. J. (2019). Reinforcement learning based graph-to-sequence model for natural question generation. arXiv preprint arXiv:1908.04942.</p>
<p>Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. H Harkous, I Groves, A Saffari, arXiv:2004.06577arXiv preprintHarkous, H., Groves, I.,&amp; Saffari, A. (2020). Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. arXiv preprint arXiv:2004.06577.</p>
<p>Text generation from knowledge graphs with graph transformers. R Koncel-Kedziorski, D Bekal, Y Luan, M Lapata, H Hajishirzi, arXiv:1904.02342arXiv preprintKoncel-Kedziorski, R., Bekal, D., Luan, Y., Lapata, M.,&amp; Hajishirzi, H. (2019). Text generation from knowledge graphs with graph transformers. arXiv preprint arXiv:1904.02342.</p>
<p>The WebNLG challenge: Generating text from RDF data. C Gardent, A Shimorina, S Narayan, L Perez-Beltrachini, Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationGardent, C., Shimorina, A., Narayan, S.,&amp; Perez-Beltrachini, L. (2017, September). The WebNLG challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation (pp. 124-133).</p>
<p>Improving text-to-text pre-trained models for the graph-to-text task. Z Yang, A Einolghozati, H Inan, K Diedrick, A Fan, P Donmez, S Gupta, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+). the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)Yang, Z., Einolghozati, A., Inan, H., Diedrick, K., Fan, A., Donmez, P.,&amp; Gupta, S. (2020). Improving text-to-text pre-trained models for the graph-to-text task. In Proceed- ings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+) (pp. 107-116).</p>
<p>Attention is all you need. Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, 30&amp; Polosukhin, IVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ...&amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, arXiv:1609.02907arXiv preprintKipf, T. N.,&amp; Welling, M. (2016). Semi-supervised classification with graph convolu- tional networks. arXiv preprint arXiv:1609.02907.</p>
<p>A new model for learning in graph domains. M Gori, G Monfardini, F Scarselli, Proceedings. 2005 IEEE International Joint Conference on Neural Networks. 2005 IEEE International Joint Conference on Neural NetworksIEEE2M. Gori, G. Monfardini, and F. Scarselli, "A new model for learning in graph domains," in Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., vol. 2, pp. 729-734, IEEE, 2005.</p>
<p>The graph neural network model. F Scarselli, M Gori, A C Tsoi, M Hagenbuchner, G Monfardini, IEEE Transactions on Neural Networks. 201F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, "The graph neural network model," IEEE Transactions on Neural Networks, vol. 20, no. 1, pp. 61-80, 2008.</p>
<p>Data-to-text generation with content selection and planning. Ratish Puduppully, Li Dong, Mirella Lapata, Ratish Puduppully, Li Dong, and Mirella Lapata. 2018. Data-to-text generation with content selection and planning.</p>
<p>E2e nlg challenge: Neural models vs. templates. Yevgeniy Puzikov, Iryna Gurevych, Yevgeniy Puzikov and Iryna Gurevych. 2018. E2e nlg challenge: Neural models vs. templates.</p>
<p>A Moryossef, Y Goldberg, I Dagan, arXiv:1904.03396Step-by-step: Separating planning from realization in neural data-to-text generation. arXiv preprintMoryossef, A., Goldberg, Y.,&amp; Dagan, I. (2019). Step-by-step: Separating planning from realization in neural data-to-text generation. arXiv preprint arXiv:1904.03396.</p>
<p>Neural datato-text generation: A comparison between pipeline and end-to-end architectures. T C Ferreira, C Van Der Lee, E Van Miltenburg, E Krahmer, arXiv:1908.09022arXiv preprintFerreira, T. C., van der Lee, C., Van Miltenburg, E.,&amp; Krahmer, E. (2019). Neural data- to-text generation: A comparison between pipeline and end-to-end architectures. arXiv preprint arXiv:1908.09022.</p>
<p>Partially-aligned data-to-text generation with distant supervision. Z Fu, B Shi, W Lam, L Bing, Z Liu, arXiv:2010.01268arXiv preprintFu, Z., Shi, B., Lam, W., Bing, L.,&amp; Liu, Z. (2020). Partially-aligned data-to-text gen- eration with distant supervision. arXiv preprint arXiv:2010.01268.</p>
<p>. A Radford, K Narasimhan, T Salimans, I Sutskever, Improving language understanding by generative pre-trainingRadford, A., Narasimhan, K., Salimans, T.,&amp; Sutskever, I. (2018). Improving language understanding by generative pre-training.</p>
<p>. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ...</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. P J Liu, J. Mach. Learn. Res. 21140&amp; Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.</p>
<p>Text-to-text pre-training for data-to-text tasks. M Kale, A Rastogi, arXiv:2005.10433arXiv preprintKale, M.,&amp; Rastogi, A. (2020). Text-to-text pre-training for data-to-text tasks. arXiv preprint arXiv:2005.10433.</p>
<p>J Devlin, M W Chang, K Lee, K Toutanova, arXiv:1810.04805Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin, J., Chang, M. W., Lee, K.,&amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>An unsupervised joint system for text generation from knowledge graphs and semantic parsing. M Schmitt, S Sharifzadeh, V Tresp, H Schütze, arXiv:1904.09447arXiv preprintSchmitt, M., Sharifzadeh, S., Tresp, V., &amp; Schütze, H. (2019). An unsupervised joint system for text generation from knowledge graphs and semantic parsing. arXiv preprint arXiv:1904.09447.</p>
<p>Z Zhang, X Han, Z Liu, X Jiang, M Sun, Q Liu, arXiv:1905.07129ERNIE: Enhanced language representation with informative entities. arXiv preprintZhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., &amp; Liu, Q. (2019). ERNIE: Enhanced language representation with informative entities. arXiv preprint arXiv:1905.07129.</p>
<p>M E Peters, M Neumann, I V Logan, R L Schwartz, R Joshi, V Singh, S Smith, N A , arXiv:1909.04164Knowledge enhanced contextual word representations. arXiv preprintPeters, M. E., Neumann, M., Logan IV, R. L., Schwartz, R., Joshi, V., Singh, S., &amp; Smith, N. A. (2019). Knowledge enhanced contextual word representations. arXiv preprint arXiv:1909.04164.</p>
<p>Translating embeddings for modeling multi-relational data. A Bordes, N Usunier, A Garcia-Duran, J Weston, O Yakhnenko, Advances in neural information processing systems. 26Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., &amp; Yakhnenko, O. (2013). Trans- lating embeddings for modeling multi-relational data. Advances in neural information processing systems, 26.</p>
<p>Pretrained language models for text generation: A survey. J Li, T Tang, W X Zhao, J R Wen, arXiv:2105.10311arXiv preprintLi, J., Tang, T., Zhao, W. X.,&amp; Wen, J. R. (2021). Pretrained language models for text generation: A survey. arXiv preprint arXiv:2105.10311.</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 189Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,&amp; Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.</p>
<p>A simple and accurate syntaxagnostic neural model for dependency-based semantic role labeling. D Marcheggiani, A Frolov, I Titov, arXiv:1701.02593arXiv preprintMarcheggiani, D., Frolov, A.,&amp; Titov, I. (2017). A simple and accurate syntax- agnostic neural model for dependency-based semantic role labeling. arXiv preprint arXiv:1701.02593.</p>
<p>Transformers are Graph Neural Networks. The Gradient. K Chaitanya, Joshi, Chaitanya K.Joshi. Transformers are Graph Neural Networks. The Gradient.</p>
<p>Y Li, D Tarlow, M Brockschmidt, R Zemel, arXiv:1511.05493Gated graph sequence neural networks. arXiv preprintLi, Y., Tarlow, D., Brockschmidt, M.,&amp; Zemel, R. (2015). Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493.</p>
<p>A generalization of transformer networks to graphs. V P Dwivedi, X Bresson, arXiv:2012.09699arXiv preprintDwivedi, V. P.,&amp; Bresson, X. (2020). A generalization of transformer networks to graphs. arXiv preprint arXiv:2012.09699.</p>
<p>Evaluating semantic accuracy of data-to-text generation with natural language inference. O Dušek, Z Kasner, arXiv:2011.10819arXiv preprintDušek, O.,&amp; Kasner, Z. (2020). Evaluating semantic accuracy of data-to-text generation with natural language inference. arXiv preprint arXiv:2011.10819.</p>
<p>Sticking to the facts: Confident decoding for faithful data-to-text generation. R Tian, S Narayan, T Sellam, A P Parikh, arXiv:1910.08684arXiv preprintTian, R., Narayan, S., Sellam, T.,&amp; Parikh, A. P. (2019). Sticking to the facts: Confident decoding for faithful data-to-text generation. arXiv preprint arXiv:1910.08684.</p>
<p>On hallucination and predictive uncertainty in conditional language generation. Y Xiao, W Y Wang, arXiv:2103.15025arXiv preprintXiao, Y.,&amp; Wang, W. Y. (2021). On hallucination and predictive uncertainty in condi- tional language generation. arXiv preprint arXiv:2103.15025.</p>
<p>Challenges in data-todocument generation. Sam Wiseman, Stuart Shieber, Alexander Rush, 10.18653/v1/D17-1239Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsSam Wiseman , Stuart Shieber , and Alexander Rush . 2017 . Challenges in data-to- document generation . In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253 -2263 ,Copenhagen, Denmark. Association for Computational Linguistics .https://doi.org/10.18653/v1/D17-1239</p>
<p>FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. E Durmus, H He, M Diab, arXiv:2005.03754arXiv preprintDurmus, E., He, H.,&amp; Diab, M. (2020). FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. arXiv preprint arXiv:2005.03754.</p>
<p>Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data. Katja Filippova, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 864ś870. the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 864ś870Katja Filippova. 2020. Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 864ś870.</p>
<p>Conversational agents: Theory and applications. M Wahde, M Virgolin, Deep Learning, Intelligent Control and Evolutionary Computation. 2Wahde, M.,&amp; Virgolin, M. (2022). Conversational agents: Theory and applications. In HANDBOOK ON COMPUTER LEARNING AND INTELLIGENCE: Volume 2: Deep Learning, Intelligent Control and Evolutionary Computation (pp. 497-544).</p>
<p>A survey on conversational agents/chatbots classification and design techniques. S Hussain, O Ameri Sianaki, N Ababneh, WAINA-2019) 33Web, Artificial Intelligence and Network Applications: Proceedings of the Workshops of the 33rd International Conference on Advanced Information Networking and Applications. Hussain, S., Ameri Sianaki, O.,&amp; Ababneh, N. (2019). A survey on conversational agents/chatbots classification and design techniques. In Web, Artificial Intelligence and Network Applications: Proceedings of the Workshops of the 33rd International Conference on Advanced Information Networking and Applications (WAINA-2019) 33 (pp. 946-956).</p>
<p>KBot: A Knowledge graph based chatBot for natural language understanding over linked data. A Ait-Mlouk, L Jiang, IEEE Access. 8Ait-Mlouk, A.,&amp; Jiang, L. (2020). KBot: A Knowledge graph based chatBot for natural language understanding over linked data. IEEE Access, 8, 149220-149230.</p>
<p>Tucker: Tensor factorization for knowledge graph completion. I Balažević, C Allen, T M Hospedales, arXiv:1901.09590arXiv preprintBalažević, I., Allen, C., &amp; Hospedales, T. M. (2019). Tucker: Tensor factorization for knowledge graph completion. arXiv preprint arXiv:1901.09590.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9213 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9213</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9213</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-272593159</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.07054v1.pdf" target="_blank">Native vs Non-Native Language Prompting: A Comparative Analysis</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown remarkable abilities in different fields, including standard Natural Language Processing (NLP) tasks. To elicit knowledge from LLMs, prompts play a key role, consisting of natural language instructions. Most open and closed source LLMs are trained on available labeled and unlabeled resources--digital content such as text, images, audio, and videos. Hence, these models have better knowledge for high-resourced languages but struggle with low-resourced languages. Since prompts play a crucial role in understanding their capabilities, the language used for prompts remains an important research question. Although there has been significant research in this area, it is still limited, and less has been explored for medium to low-resourced languages. In this study, we investigate different prompting strategies (native vs. non-native) on 11 different NLP tasks associated with 12 different Arabic datasets (9.7K data points). In total, we conducted 197 experiments involving 3 LLMs, 12 datasets, and 3 prompting strategies. Our findings suggest that, on average, the non-native prompt performs the best, followed by mixed and native prompts.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9213.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9213.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptLangFormatEffect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Prompt Language and Format on LLM Performance (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregated finding across 11 Arabic social/news NLP tasks and three LLMs showing how prompt language (native Arabic vs non-native English vs mixed) and presentation format (zero-shot vs few-shot) affect classification performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o / Llama-3.1-8b-Instruct / Jais-13b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate over 11 Arabic social/news NLP tasks (hate speech, spam, propaganda, check-worthiness, factuality, claim detection, harmful content, attention-worthiness, subjectivity, adult content, offensive language)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary and multi-class classification tasks on Arabic tweets, news, and transcripts drawn from 11 datasets; metrics reported include accuracy, macro-F1, micro-F1, weighted-F1 depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot (instruction only) and few-shot (3-shot in-context learning). Three prompt-language structures: Native = Arabic instruction + Arabic labels; Non-native = English instruction + English labels; Mixed = Arabic instruction + English labels.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>The three prompt-language structures are compared against each other within both zero-shot and few-shot setups.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On average across tasks and models, non-native (English) prompts produced the best performance, followed by mixed prompts, with native (Arabic) prompts performing worst (metrics aggregated across tasks; specific metric-by-task values reported in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Aggregate ordering reported (non-native > mixed > native). Paper also reports a case where Llama-3.1-8b-Instruct in few-shot produced ~+13% improvement for mixed prompts relative to the other formats (see Llama-specific entry).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that LLMs (even Arabic-centric ones) inherit substantial English-dominant training signal, so English instructions/labels align better with model capabilities; mixed prompts can combine native input context with English label signals to help English-centric LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>198 experimental setups: 11 datasets, 3 models (GPT-4o, Llama-3.1-8b-Instruct, Jais-13b-chat), 3 prompt structures, zero-shot and 3-shot few-shot. Temperature set to 0. Few-shot example selection: 3-shot using MMR on Sentence-Transformer multilingual embeddings. Post-processing mapping functions per (model,prompt,dataset). Evaluated with accuracy, macro/micro/weighted F1 as appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Native vs Non-Native Language Prompting: A Comparative Analysis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9213.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9213.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o_PromptLang</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o performance across prompt-language formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-4o on 11 Arabic classification tasks comparing native (Arabic), non-native (English), and mixed prompts in zero-shot and 3-shot few-shot setups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate over 11 Arabic social/news NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above: classification tasks on Arabic social/news datasets with task-specific metrics (Acc, Ma-F1, Mi-F1, W-F1).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot and few-shot (3-shot) with three prompt language structures: Native (Arabic instruction + Arabic labels), Non-native (English instruction + English labels), Mixed (Arabic instruction + English labels).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared native vs non-native vs mixed, and zero-shot vs few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GPT-4o yielded the best overall performance among evaluated models; few-shot non-native prompts achieved the highest performance for GPT-4o, with mixed second and native worst (specific metric-by-task values are reported in the paper tables). In some individual tasks GPT-4o outperformed reported SOTA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Differences are small for GPT-4o compared to other models, suggesting GPT-4o is robust across prompt languages due to stronger multilingual/contextual understanding; still benefits from English (dominant training language).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot used 3 examples chosen by MMR; temperature=0. During evaluation GPT-4o triggered Azure OpenAI content-management policy on 25 out of 1,000 sample inputs for some datasets; those instances were assigned a random label to continue evaluation (reported as 25 policy-related errors out of 1,000 in affected sets).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Native vs Non-Native Language Prompting: A Comparative Analysis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9213.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9213.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3.1-8b_Instruct_PromptLang</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-8b-Instruct performance across prompt-language formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of Llama-3.1-8b-Instruct on the 11 Arabic tasks comparing native, non-native and mixed prompt structures under zero-shot and few-shot setups.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8b</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate over 11 Arabic social/news NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification tasks on Arabic social/news datasets with task-specific metrics (Acc, Ma-F1, Mi-F1, W-F1).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot and few-shot (3-shot) using three prompt-language structures: Native (Arabic instruction + Arabic labels), Non-native (English instruction + English labels), Mixed (Arabic instruction + English labels).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Native vs Non-native vs Mixed; zero-shot vs few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>In the few-shot setting, Llama-3.1-8b-Instruct performed best with Mixed prompts; in zero-shot it performed best with Non-native (English) prompts; Native (Arabic) prompts yielded the worst results in both scenarios. (Per-task metrics are in paper tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+13% (few-shot) — paper reports that for one observed aggregate comparison Llama-3.1-8b-Instruct's mixed prompt performance was approximately 13% better than non-native and native prompts in the few-shot setup.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Using English labels improves alignment with English-centric Llama instruction tuning; mixed prompts can leverage Arabic input together with English label formulation to give stronger signals in few-shot ICL for this English-dominant model.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>3-shot few-shot using MMR-selected examples from training set embeddings; temperature=0; post-processing functions created per (model,prompt,dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Native vs Non-Native Language Prompting: A Comparative Analysis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9213.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9213.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jais13b_chat_PromptLang</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Jais-13b-chat (Arabic-centric) performance across prompt-language formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of Jais-13b-chat (Arabic-centric, instruction-tuned) on 11 Arabic tasks comparing prompt-language structures in zero-shot and few-shot setups; surprising preference for non-native prompts observed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Jais-13b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13b</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate over 11 Arabic social/news NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same 11 classification tasks (Arabic datasets) with typical classification metrics per task.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot and few-shot (3-shot) with Native (Arabic instr+labels), Non-native (English instr+labels), Mixed (Arabic instr + English labels).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Native vs Non-native vs Mixed; zero-shot vs few-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Contrary to expectations for an Arabic-centric model, Jais often performed best with Non-native (English) prompts and worst with Native (Arabic) prompts across many tasks. Few-shot improved performance in many cases but also showed problematic behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Despite being Arabic-centric, Jais training contains a large proportion (59%) English data and many instruction-tuning signals translated from English, causing it to respond better to English-formatted instructions and labels.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Observed error modes: in few-shot with Arabic prompts, Jais sometimes outputs classifications of the example shots rather than the target input, returns explanatory text that complicates label parsing, hallucinates irrelevant outputs, and often returns one class for many samples; temperature=0; 3-shot MMR selection used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Native vs Non-Native Language Prompting: A Comparative Analysis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9213.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9213.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ShotEffect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Few-shot (3-shot) vs Zero-shot Presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Findings on the effect of providing 3-shot in-context examples versus zero-shot instruction-only prompts across models and prompt-language structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o / Llama-3.1-8b-Instruct / Jais-13b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate over 11 Arabic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification tasks; metrics per-task vary (Acc, Ma-F1, Mi-F1, W-F1).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot (instruction only) vs Few-shot (3-shot in-context examples selected via MMR).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot vs Few-shot in each prompt-language structure (native, non-native, mixed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Overall, few-shot prompting shows improved performance compared to zero-shot for most model/task combinations; however some experiments show declines or no improvement depending on the relevance of retrieved shots.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Few-shot gains depend strongly on quality/diversity/relevance of retrieved examples; when few-shots are similar by text but not label, the model may be misled causing performance degradation. Authors note few-shot is beneficial when small training data available and examples are well-selected.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot: 3-shot examples chosen via MMR on multilingual sentence-transformer embeddings; temperature=0; authors observe inconsistent few-shot improvements in some cases due to mismatched labels among nearest examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Native vs Non-Native Language Prompting: A Comparative Analysis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9213.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9213.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formatting_and_Postproc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Label Formatting and Output Post-processing Effects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observations about how explicit label formatting in the prompt and model adherence to label-only outputs affect evaluation and post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o / Llama-3.1-8b-Instruct / Jais-13b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Evaluation across the 11 classification tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Label-prediction tasks where prompts include explicit label lists; evaluation requires mapping raw LLM outputs to canonical labels.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompts included explicit label lists formatted according to prompt language; post-processing function f maps raw model output to desired label y'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Llama-3.1-8b-Instruct and GPT-4o adhered strictly to returning labels as prompted, simplifying post-processing; Jais often returned explanations or extraneous text, complicating mapping and reducing effective performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Model instruction-following behavior interacts with label formatting; models more strictly following instructions produce cleaner outputs easier to post-process. Divergence (explanations, extra text) increases evaluation noise and lowers effective measured performance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Authors built per-(model,prompt,dataset) post-processing functions; prompts contained comma-separated labels in language specified by prompt; temperature=0 to reduce variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Native vs Non-Native Language Prompting: A Comparative Analysis', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Democratizing llms for lowresource languages by leveraging their english dominant abilities with linguistically-diverse prompts <em>(Rating: 2)</em></li>
                <li>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing <em>(Rating: 2)</em></li>
                <li>Understanding and mitigating language confusion in llms <em>(Rating: 2)</em></li>
                <li>A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9213",
    "paper_id": "paper-272593159",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "PromptLangFormatEffect",
            "name_full": "Effect of Prompt Language and Format on LLM Performance (aggregate)",
            "brief_description": "Aggregated finding across 11 Arabic social/news NLP tasks and three LLMs showing how prompt language (native Arabic vs non-native English vs mixed) and presentation format (zero-shot vs few-shot) affect classification performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o / Llama-3.1-8b-Instruct / Jais-13b-chat",
            "model_size": null,
            "task_name": "Aggregate over 11 Arabic social/news NLP tasks (hate speech, spam, propaganda, check-worthiness, factuality, claim detection, harmful content, attention-worthiness, subjectivity, adult content, offensive language)",
            "task_description": "Binary and multi-class classification tasks on Arabic tweets, news, and transcripts drawn from 11 datasets; metrics reported include accuracy, macro-F1, micro-F1, weighted-F1 depending on task.",
            "presentation_format": "Zero-shot (instruction only) and few-shot (3-shot in-context learning). Three prompt-language structures: Native = Arabic instruction + Arabic labels; Non-native = English instruction + English labels; Mixed = Arabic instruction + English labels.",
            "comparison_format": "The three prompt-language structures are compared against each other within both zero-shot and few-shot setups.",
            "performance": "On average across tasks and models, non-native (English) prompts produced the best performance, followed by mixed prompts, with native (Arabic) prompts performing worst (metrics aggregated across tasks; specific metric-by-task values reported in paper tables).",
            "performance_comparison": null,
            "format_effect_size": "Aggregate ordering reported (non-native &gt; mixed &gt; native). Paper also reports a case where Llama-3.1-8b-Instruct in few-shot produced ~+13% improvement for mixed prompts relative to the other formats (see Llama-specific entry).",
            "explanation_or_hypothesis": "Authors hypothesize that LLMs (even Arabic-centric ones) inherit substantial English-dominant training signal, so English instructions/labels align better with model capabilities; mixed prompts can combine native input context with English label signals to help English-centric LLMs.",
            "null_or_negative_result": false,
            "experimental_details": "198 experimental setups: 11 datasets, 3 models (GPT-4o, Llama-3.1-8b-Instruct, Jais-13b-chat), 3 prompt structures, zero-shot and 3-shot few-shot. Temperature set to 0. Few-shot example selection: 3-shot using MMR on Sentence-Transformer multilingual embeddings. Post-processing mapping functions per (model,prompt,dataset). Evaluated with accuracy, macro/micro/weighted F1 as appropriate.",
            "uuid": "e9213.0",
            "source_info": {
                "paper_title": "Native vs Non-Native Language Prompting: A Comparative Analysis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-4o_PromptLang",
            "name_full": "GPT-4o performance across prompt-language formats",
            "brief_description": "Evaluation of GPT-4o on 11 Arabic classification tasks comparing native (Arabic), non-native (English), and mixed prompts in zero-shot and 3-shot few-shot setups.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_size": null,
            "task_name": "Aggregate over 11 Arabic social/news NLP tasks",
            "task_description": "As above: classification tasks on Arabic social/news datasets with task-specific metrics (Acc, Ma-F1, Mi-F1, W-F1).",
            "presentation_format": "Zero-shot and few-shot (3-shot) with three prompt language structures: Native (Arabic instruction + Arabic labels), Non-native (English instruction + English labels), Mixed (Arabic instruction + English labels).",
            "comparison_format": "Compared native vs non-native vs mixed, and zero-shot vs few-shot.",
            "performance": "GPT-4o yielded the best overall performance among evaluated models; few-shot non-native prompts achieved the highest performance for GPT-4o, with mixed second and native worst (specific metric-by-task values are reported in the paper tables). In some individual tasks GPT-4o outperformed reported SOTA.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Differences are small for GPT-4o compared to other models, suggesting GPT-4o is robust across prompt languages due to stronger multilingual/contextual understanding; still benefits from English (dominant training language).",
            "null_or_negative_result": false,
            "experimental_details": "Few-shot used 3 examples chosen by MMR; temperature=0. During evaluation GPT-4o triggered Azure OpenAI content-management policy on 25 out of 1,000 sample inputs for some datasets; those instances were assigned a random label to continue evaluation (reported as 25 policy-related errors out of 1,000 in affected sets).",
            "uuid": "e9213.1",
            "source_info": {
                "paper_title": "Native vs Non-Native Language Prompting: A Comparative Analysis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Llama3.1-8b_Instruct_PromptLang",
            "name_full": "Llama-3.1-8b-Instruct performance across prompt-language formats",
            "brief_description": "Evaluation of Llama-3.1-8b-Instruct on the 11 Arabic tasks comparing native, non-native and mixed prompt structures under zero-shot and few-shot setups.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8b-Instruct",
            "model_size": "8b",
            "task_name": "Aggregate over 11 Arabic social/news NLP tasks",
            "task_description": "Classification tasks on Arabic social/news datasets with task-specific metrics (Acc, Ma-F1, Mi-F1, W-F1).",
            "presentation_format": "Zero-shot and few-shot (3-shot) using three prompt-language structures: Native (Arabic instruction + Arabic labels), Non-native (English instruction + English labels), Mixed (Arabic instruction + English labels).",
            "comparison_format": "Native vs Non-native vs Mixed; zero-shot vs few-shot.",
            "performance": "In the few-shot setting, Llama-3.1-8b-Instruct performed best with Mixed prompts; in zero-shot it performed best with Non-native (English) prompts; Native (Arabic) prompts yielded the worst results in both scenarios. (Per-task metrics are in paper tables.)",
            "performance_comparison": null,
            "format_effect_size": "+13% (few-shot) — paper reports that for one observed aggregate comparison Llama-3.1-8b-Instruct's mixed prompt performance was approximately 13% better than non-native and native prompts in the few-shot setup.",
            "explanation_or_hypothesis": "Using English labels improves alignment with English-centric Llama instruction tuning; mixed prompts can leverage Arabic input together with English label formulation to give stronger signals in few-shot ICL for this English-dominant model.",
            "null_or_negative_result": false,
            "experimental_details": "3-shot few-shot using MMR-selected examples from training set embeddings; temperature=0; post-processing functions created per (model,prompt,dataset).",
            "uuid": "e9213.2",
            "source_info": {
                "paper_title": "Native vs Non-Native Language Prompting: A Comparative Analysis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Jais13b_chat_PromptLang",
            "name_full": "Jais-13b-chat (Arabic-centric) performance across prompt-language formats",
            "brief_description": "Evaluation of Jais-13b-chat (Arabic-centric, instruction-tuned) on 11 Arabic tasks comparing prompt-language structures in zero-shot and few-shot setups; surprising preference for non-native prompts observed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Jais-13b-chat",
            "model_size": "13b",
            "task_name": "Aggregate over 11 Arabic social/news NLP tasks",
            "task_description": "Same 11 classification tasks (Arabic datasets) with typical classification metrics per task.",
            "presentation_format": "Zero-shot and few-shot (3-shot) with Native (Arabic instr+labels), Non-native (English instr+labels), Mixed (Arabic instr + English labels).",
            "comparison_format": "Native vs Non-native vs Mixed; zero-shot vs few-shot.",
            "performance": "Contrary to expectations for an Arabic-centric model, Jais often performed best with Non-native (English) prompts and worst with Native (Arabic) prompts across many tasks. Few-shot improved performance in many cases but also showed problematic behaviors.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Despite being Arabic-centric, Jais training contains a large proportion (59%) English data and many instruction-tuning signals translated from English, causing it to respond better to English-formatted instructions and labels.",
            "null_or_negative_result": false,
            "experimental_details": "Observed error modes: in few-shot with Arabic prompts, Jais sometimes outputs classifications of the example shots rather than the target input, returns explanatory text that complicates label parsing, hallucinates irrelevant outputs, and often returns one class for many samples; temperature=0; 3-shot MMR selection used.",
            "uuid": "e9213.3",
            "source_info": {
                "paper_title": "Native vs Non-Native Language Prompting: A Comparative Analysis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ShotEffect",
            "name_full": "Effect of Few-shot (3-shot) vs Zero-shot Presentation",
            "brief_description": "Findings on the effect of providing 3-shot in-context examples versus zero-shot instruction-only prompts across models and prompt-language structures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o / Llama-3.1-8b-Instruct / Jais-13b-chat",
            "model_size": null,
            "task_name": "Aggregate over 11 Arabic tasks",
            "task_description": "Classification tasks; metrics per-task vary (Acc, Ma-F1, Mi-F1, W-F1).",
            "presentation_format": "Zero-shot (instruction only) vs Few-shot (3-shot in-context examples selected via MMR).",
            "comparison_format": "Zero-shot vs Few-shot in each prompt-language structure (native, non-native, mixed).",
            "performance": "Overall, few-shot prompting shows improved performance compared to zero-shot for most model/task combinations; however some experiments show declines or no improvement depending on the relevance of retrieved shots.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Few-shot gains depend strongly on quality/diversity/relevance of retrieved examples; when few-shots are similar by text but not label, the model may be misled causing performance degradation. Authors note few-shot is beneficial when small training data available and examples are well-selected.",
            "null_or_negative_result": true,
            "experimental_details": "Few-shot: 3-shot examples chosen via MMR on multilingual sentence-transformer embeddings; temperature=0; authors observe inconsistent few-shot improvements in some cases due to mismatched labels among nearest examples.",
            "uuid": "e9213.4",
            "source_info": {
                "paper_title": "Native vs Non-Native Language Prompting: A Comparative Analysis",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Formatting_and_Postproc",
            "name_full": "Prompt Label Formatting and Output Post-processing Effects",
            "brief_description": "Observations about how explicit label formatting in the prompt and model adherence to label-only outputs affect evaluation and post-processing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o / Llama-3.1-8b-Instruct / Jais-13b-chat",
            "model_size": null,
            "task_name": "Evaluation across the 11 classification tasks",
            "task_description": "Label-prediction tasks where prompts include explicit label lists; evaluation requires mapping raw LLM outputs to canonical labels.",
            "presentation_format": "Prompts included explicit label lists formatted according to prompt language; post-processing function f maps raw model output to desired label y'.",
            "comparison_format": null,
            "performance": "Llama-3.1-8b-Instruct and GPT-4o adhered strictly to returning labels as prompted, simplifying post-processing; Jais often returned explanations or extraneous text, complicating mapping and reducing effective performance.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Model instruction-following behavior interacts with label formatting; models more strictly following instructions produce cleaner outputs easier to post-process. Divergence (explanations, extra text) increases evaluation noise and lowers effective measured performance.",
            "null_or_negative_result": null,
            "experimental_details": "Authors built per-(model,prompt,dataset) post-processing functions; prompts contained comma-separated labels in language specified by prompt; temperature=0 to reduce variability.",
            "uuid": "e9213.5",
            "source_info": {
                "paper_title": "Native vs Non-Native Language Prompting: A Comparative Analysis",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Democratizing llms for lowresource languages by leveraging their english dominant abilities with linguistically-diverse prompts",
            "rating": 2,
            "sanitized_title": "democratizing_llms_for_lowresource_languages_by_leveraging_their_english_dominant_abilities_with_linguisticallydiverse_prompts"
        },
        {
            "paper_title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "rating": 2,
            "sanitized_title": "pretrain_prompt_and_predict_a_systematic_survey_of_prompting_methods_in_natural_language_processing"
        },
        {
            "paper_title": "Understanding and mitigating language confusion in llms",
            "rating": 2,
            "sanitized_title": "understanding_and_mitigating_language_confusion_in_llms"
        },
        {
            "paper_title": "A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity",
            "rating": 1,
            "sanitized_title": "a_multitask_multilingual_multimodal_evaluation_of_chatgpt_on_reasoning_hallucination_and_interactivity"
        }
    ],
    "cost": 0.012940749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Native vs Non-Native Language Prompting: A Comparative Analysis
6 Oct 2024</p>
<p>Mohamed Bayan Kmainasi 
Qatar University
Qatar</p>
<p>Rakif Khan 
University of Doha for Science and Technology
Qatar</p>
<p>Ali Ezzat Shahroor 
Liverpool John Moores University
Qatar</p>
<p>Boushra Bendou 
Carnegie Mellon University in Qatar
Qatar</p>
<p>Maram Hasanain 
Qatar Computing Research Institute
Qatar</p>
<p>Firoj Alam fialam@hbku.edu.qa 
Qatar Computing Research Institute
Qatar</p>
<p>Native vs Non-Native Language Prompting: A Comparative Analysis
6 Oct 2024A46B0CAB9164E823E52453352B82FF41arXiv:2409.07054v2[cs.CL]LLMsPromptingSocial MediaArabicNLPArabicLLM
Large language models (LLMs) have shown remarkable abilities in different fields, including standard Natural Language Processing (NLP) tasks.To elicit knowledge from LLMs, prompts play a key role, consisting of natural language instructions.Most open and closed source LLMs are trained on available labeled and unlabeled resources-digital content such as text, images, audio, and videos.Hence, these models have better knowledge for high-resourced languages but struggle with lowresourced languages.Since prompts play a crucial role in understanding their capabilities, the language used for prompts remains an important research question.Although there has been significant research in this area, it is still limited, and less has been explored for medium to lowresourced languages.In this study, we investigate different prompting strategies (native vs. non-native) on 11 different NLP tasks associated with 11 different Arabic datasets (9.7K data points).In total, we conducted 198 experiments involving 3 open and closed LLMs (including an Arabic-centric model), and 3 prompting strategies.Our findings suggest that, on average, the non-native prompt performs the best, followed by mixed and native prompts.All prompts will be made available to the community through the LLMeBench 1 framework.</p>
<p>Introduction</p>
<p>Recent advancements in LLMs have reshaped the spectrum of solving downstream NLP tasks.Prompt engineering plays a crucial role in solving the downstream task at hand.It is a process of creating instructions, providing context, and asking the model to solve the task or extract knowledge [25].Traditionally, supervised models solved a task by taking an input x and predicting an output y as P (y|x), whereas in the prompt-based approach, a prompt function f prompt (•) is applied to modify the input x into a prompt x ′ = f prompt (x).The final output of the LLM is then predicted from x ′ .</p>
<p>The careful design of a prompt is crucial to understand the capabilities of LLMs and solve diverse language and reasoning tasks.A prompt is made up of various elements such as instructions, context, input, and output, which together steer the model to generate the desired responses.Enhancing the effectiveness of a prompt is the objective of methods such as Chain-of-Thought (CoT) prompting [42], which leverages the power of consecutive prompts that build on each other.Another effective approach is automatic prompting, which generates prompts based on a learned distribution over prompting strategies [43,39].</p>
<p>To understand the capabilities of LLMs for solving downstream NLP tasks, there have been large-scale efforts focusing on multitask, multimodal, and multilingual evaluation [6], language-specific benchmarking with different learning setups [1], and focusing on English with a large number of tasks [23].There have also been efforts focusing on different prompting strategies, such as translating input into a non-native language (English) and providing a comparison [2,17].[27] investigates the ability of LLMs to generate the user's desired languages.Currently, the language of prompts is mainly dominated by English.What is lacking in the current literature is studies of the effect of prompt structures comprised of native and non-native language elements. 2In Figure 1, we highlight different structures of prompts that we examine in this work.Our study investigates the problem across eleven NLP tasks associated with eleven Arabic datasets.The selection of tasks and datasets was driven by the necessity to comprehend the models' capabilities within social and news media contexts.</p>
<p>Our findings are as follows: (i) Few-shot prompting shows improved performance, corroborating previous findings [1], and this could be an ideal setup for any task with a small number of training datasets available to accommodate few-shot prompts; (ii) Across different prompt setups, the non-native prompt outperforms others, while mixed prompt shows promising results in the few-shot setup, with Llama 3.1-8b-Instruct being 13% better than non-native and native prompts; (iii) For a new task where no training data is available, the zero-shot setup is the ideal solution, and based on our findings, non-native prompts perform better across all models; (iv) GPT-4o outperforms all models, and is the most robust model across all prompt setups.</p>
<p>The remainder of the paper is organized as follows: Section 2 presents some related work.Section 3 describes the datasets used for this study.Section 4 discusses the experimental details.We present the results in section 5. Finally, we provide concluding remarks in section 6.</p>
<p>Related Work</p>
<p>In this section, we first provide a brief overview of prompt-based approaches, then discuss the work that focused on mono-and multilingual prompting for NLP tasks.Following that, we discuss work related to social media analysis.</p>
<p>Prompt and Techniques</p>
<p>LLMs have demonstrated impressive capabilities in addressing a wide range of language and reasoning tasks.By carefully designing prompts, it is possible to guide LLMs toward producing more refined responses.As a result, prompt engineering has emerged as a specialized field focused on developing and optimizing prompts to enhance the performance of language models.It provides an intuitive and natural interface, enabling seamless human interaction with LLMs.However, LLMs are highly sensitive to the prompt interaction process-even small modifications can lead to entirely different responses.Therefore, it is important to develop prompts that are robust and consistently produce high-quality response.</p>
<p>A prompt can contain one or more components, which are often constructed as a template.Such components include: Instruction, which describes the task to be performed by the model; Context, which provides additional information to guide the model's response; Input, the content for which the solution is requested; and Output indicator, which guides the model in restricting and formatting its response.Often a role, commonly known as system prompt is also provided to the LLM.For prompting techniques, there are many approaches, here, we focus on most notable ones such as zero-shot and few-shot learning.</p>
<p>Zero-shot learning involves prompting LLMs without providing any specific prior training on the task or data domain.In this approach, the model uses its pre-existing knowledge to generate responses based solely on the prompt [8].</p>
<p>Few-shot learning, or in-context learning (ICL), involves prompting LLMs with a limited number of example inputs and outputs to improve performance.The effectiveness of ICL relies heavily on the quality and diversity of the examples used.Brown et al. (2020) show that large models like GPT-3 can effectively handle a wide range of tasks through few-shot learning, using minimal examples to produce relevant and insightful responses [8].</p>
<p>Native vs. Non-Native Language Prompting</p>
<p>Understanding how LLMs respond to prompts in different languages is crucial for evaluating their generalization and reasoning capabilities.Nguyen et al. (2023) examined the use of linguistically diverse prompts to leverage LLMs' strengths in multilingual contexts, especially for low-resource languages.Their results indicated that while LLMs perform well in English-dominant tasks, further research is needed for zero-shot setups in low-resource languages like Arabic [34].Recent studies also highlight the importance of linguistic diversity in evaluating LLMs' performance across different languages and cultural contexts [23].Further analysis by [24] revealed that language models often exhibit varying degrees of bias and performance discrepancies when switching from high-resource languages like English to low-resource languages.</p>
<p>News and Social Media Analysis</p>
<p>Social media platforms empower us in several ways, from disseminating to consuming information.They are valuable for supporting citizen journalism and increasing public awareness, among other uses.While this has been a significantly positive development by enabling free speech, it has also been accompanied by the spread of harm and hostility [7,19].To analyze content on social media, there has been a decade of research focused on identifying fake news [36], disinformation [3], fact-checking [16], and offensive, hateful and harmful content [28,13].Since the emergence of LLMs there has been effort to benchmark LLMs for social media datasets [18,1].</p>
<p>Our study contributes to the field of social and news media content analysis by exploring how prompts can be designed to detect various types of information.Specifically, we focus on how LLMs can be effectively prompted in both native and non-native languages.</p>
<p>Datasets</p>
<p>In this section, we discuss the tasks and datasets selected for this study.Our choice was inspired by analyses of social and news media, with a particular focus on Arabic content.The study includes 11 tasks associated with 11 different datasets, covering a variety of domains and text content types, such as tweets, news articles, and transcripts.</p>
<p>Hate Speech Detection: Hate speech is "language used to express hatred toward a targeted group or intended to be derogatory, humiliating, or insulting to its members" [11].We utilized the OSACT 2020 dataset [30], which comprises a collection of tweets labeled as either hate speech or not hate speech.</p>
<p>Adult Content Detection:</p>
<p>The task involves detecting and identifying whether the textual content contains sensitive or adult material.We used a dataset of tweets, where the authors collected tweets from Twitter accounts that post adult content [31].The tweets are manually annotated as either adult or not adult.</p>
<p>Spam Detection: Spam detection is another critical challenge, as such content can frequently mislead and frustrate users [15].Spam content on social media includes ads, malicious content, and low-quality content.For Arabic spam detection, we used the dataset discussed in [29], which contains a collection of tweets manually labeled as ads and not-ads.Subjectivity Identification: A sentence is deemed subjective when it is influenced by personal feelings, tastes, or opinions, rather than objective facts.Otherwise, the sentence is considered objective [40].We used a dataset from the CLEF CheckThat!lab [14].</p>
<p>Propaganda Detection: Propaganda can be defined as a form of communication aimed at influencing people's opinions or actions toward a specific goal, using well-defined rhetorical and psychological techniques [12].For this task, we used a dataset comprising tweet, each labeled with various propaganda techniques [4].</p>
<p>Check-worthiness Detection: Check-worthiness detection is a crucial component of fact-checking systems [33].Its goal is to streamline the manual factchecking process by prioritizing claims that are most important for fact-checkers to verify.We utilized the Arabic subset of the dataset released for Task 1A (Arabic) of the CLEF2022 CheckThat lab, which contains tweets labeled as either check-worthy or not check-worthy [32].</p>
<p>Factuality Detection: Manual fact-checking is reliable, however, it doesn't scale well with the vast amount of online information.Therefore, automatic factchecking systems are essential to assist human fact-checkers [33].We experiments with the ANS dataset developed by [21] including a collection of true and false claims, sourced from Arabic News Texts corpus.</p>
<p>Claim Detection: This is the first step for mitigating misinformation and disinformation.A factual (verifiable) claim is a statement that can be verified using accurate information such as statistics [22].We utilized the Arabic subset of the dataset released as part of the CLEF2022 CheckThat Lab, specifically CT-CWT-22-Claim [32].</p>
<p>Harmful Content Detection: We adopted the task proposed in [5,32].Research on harmful content detection also encompasses identifying offensive language, hate speech, cyberbullying, violence, as well as racist, misogynistic, and sexist content [3].For this task, we used the dataset proposed in [32].</p>
<p>Attention-worthiness Detection: On social media, people often tweet to blame authorities, provide advice, and/or call for action.It is important for policymakers to respond to these posts.This task aims to categorize such information based on whether it requires attention and which kind of it is needed.We utilized a subset of the dataset from Task 1D of the CLEF2022 CheckThat Lab [32].</p>
<p>New Test Set</p>
<p>Each dataset is publicly available in train, development, and test splits, with the exception of a few that contain only train and test sets.As shown in Table 1, the original test sets are relatively large, totaling ∼48K instances.Since our experiments involve using commercial models like GPT-4o and hosting open models such as Llama-3.1-8b,both scenarios incur costs and computational time.Therefore, we created a new test set by sampling from the original test sets.Specifically, we sampled 1,000 instances from each dataset containing more than 1,000 instances.We employed stratified sampling, such that the new test set maintains the original class label distribution present in the full testing sets [26].Such a sampling approach is a reasonable choice, as reported in a previous study [20].</p>
<p>Datasets Stats</p>
<p>In Table 1, we report the distribution of the datasets associated with various tasks, which includes the number of instances in the training set, the original test set, and newly created test set.Note that we are not reporting development set as we have not used them for this study.We used the training set to select samples for few-shot learning.</p>
<p>Experiments</p>
<p>In this section, we discuss the experimental details, which include models, different prompt structures (a main focus of this study), zero-and few-shot prompt-ing, model parameters, post-processing of the model's output, and evaluation metrics.</p>
<p>Models</p>
<p>For the experiments, we used both commercial and open-sourced models including GPT-4o [35], Llama-3.1-8b-Instruct[41] 3 , and Jais-13b-chat [38].The choice of these models is driven by their distinct strengths and suitability for multilingual and Arabic-centric applications.GPT-4o and Llama-3.1-8b-Instruct are state-of-the-art multilingual models where English is the dominant language; however, due to their extensive training on diverse and large-scale datasets, they exhibit exceptional performance across various languages, including Arabic.On the other hand, Jais-13b-chat is an Arabic-centric model specifically designed and trained to handle the nuances and complexities of the Arabic language.However, it should be noted that a large part (59%) of its training dataset contains English, and most of its instruction tuning is translated from English.Therefore, it inherits significant knowledge from English.</p>
<p>Prompt Formulation</p>
<p>For our study, we defined three different prompts to compare native versus nonnative prompt structures.We used Arabic as the native language since the input is in Arabic, and English as the non-native language.Formally, let I a and I e represent the native and non-native instructions, respectively.The input is denoted as x.The output labels within the instructions for the native and non-native languages are L I a and L I e , respectively.Finally, the output labels are denoted as y a and y e .The three different prompt structures are defined as follows: (i) Native: I a + x + L I a , (ii) Non-native: I e + x + L I e , (iii) Mixed: I a + x + L I e .In Figure 1, we present examples of three different prompts, which demonstrate the three formulations mentioned above.Based on a prompt structure, the prompting to the LLMs was to obtain a label l as a response from the l ∈ L, where L = {l 1 , l 2 , . . ., l n }.The number of labels n and label set L are dataset dependent.Note that the instructions, input and output are task and dataset dependent.We place L I a in a comma separated format in the prompt.</p>
<p>Prompting Techniques</p>
<p>For this study, we used widely used prompting techniques such as zero-shot and few-shot, as discussed below.For both techniques, we used the three prompt formulations discussed in the previous section.Zero-shot For the zero-shot experiments, only prompt is provided without any additional contextual information.We designed prompts with instructions in natural language that describe the task and specify the expected labels list.The prompt design was inspired by prior work [1,27].</p>
<p>Few-shot For the few-shot example selection, we used the maximal marginal relevance (MMR) method to construct example sets that are both relevant and diverse [9].The MMR method calculates the similarity between a test example and the example pool (e.g., training set) and selects a specified number of examples (shots).We applied MMR on top of embeddings generated by multilingual sentence-transformers [37].Our experiments were conducted using 3-shot examples.</p>
<p>Model Parameters and Post Processing</p>
<p>Reproducibility is a major concern for LLMs.To ensure reproducibility, we set the temperature to zero for all experiments and crafted the prompts with concise instructions.We used the LLMeBench framework for the experiments [10].</p>
<p>Most often, the output of LLMs includes additional information beyond the desired output.To address this problem, a post-processing function f (•) is necessary.This function maps the raw output of the LLM, denoted as L y , to the desired cleaned output y ′ .The mapping can be formally defined as: y ′ = f (L y ), where f (•) represents the post-processing operation applied to the LLM output L y to obtain the refined output y ′ .</p>
<p>For each LLM, prompt, prompting technique, and dataset, we designed a specific post-processing function.This resulted 198 experimental setups.Given that designing these configurations is a time-consuming process, we aim to make these resources publicly available for the research community.</p>
<p>Evaluation Measures</p>
<p>We evaluate all models' predictions using typical classification metrics including weighted-, macro-, micro-F1 and accuracy.Metrics are task and dataset specific and are reported in the current SOTA [1].</p>
<p>Results and Discussion</p>
<p>Overall Results</p>
<p>In Figures 2 and 3, we report the average performance for zero-shot and few-shot prompting, respectively.Each figure presents results for all models using three different prompts: Native, Non-native, and Mixed.</p>
<p>In Tables 2 and 3, we provide detailed results including random baseline and current SOTA performance for each dataset for the two learning setups.The random baseline is computed by randomly assigning a label to each instance in each dataset from the label set of the corresponding dataset.On average, non-native prompts performs better across zero and few-shot setup, followed by mixed and native prompts.</p>
<p>Compared to the random baseline, all models outperform it, except for Jais in some setups.In certain cases, GPT-4o outperforms the SOTA results; however, overall, the results of LLMs are still far from SOTA.</p>
<p>GPT-4o Performance</p>
<p>Across different models GPT-4o performs the best with the few-shot technique, achieving the highest performance with non-native prompts, followed closely by mixed prompts, and lastly native prompts.In the zero-shot scenario, the effect of prompt structures on performance is similar to the few-shot scenario: nonnative prompts give the best results, followed by mixed, and then native.While there are differences in performance based on the prompt structures of the instructions, these differences are minimal in GPT-4o, demonstrating its capability to understand context across different languages.The higher performance with non-native prompts suggests that the model has a stronger capability of the dominant language (English) it was trained on.</p>
<p>Table 2. Performance across different tasks, models, and prompting techniques in zero-shot setup.Rand: random baseline, SOTA: current state-of-the-art reported in [1]; GPT: GPT-4o; Llama: Llama-3.1-8b-Instruct;Jais: Jais-13b-chat.Acc: Accuracy, Ma-F1: macro F1, Mi-F1: micro F1, W-F1: weighted F1.Best results are boldfaced, second best underlined.</p>
<p>Llama-3.1-8b-Instruct Performance</p>
<p>Similar to GPT-4o, Llama-3.1-8b-Instructexhibits a significant increase in performance in few-shot scenarios compared to zero-shot.In the few-shot setting, Llama performs best with mixed prompts, while in the zero-shot setting, it performs best with English prompts.Arabic prompts yield the worst results in both scenarios.This suggests that using English labels can enhance performance in English-centric LLMs, and that the language of the prompt plays a crucial role in helping the model understand the context better.</p>
<p>The overall results suggest a notable improvement in few-shot learning.However, some experiments exhibit contradictory outcomes, where performance either declines or stays the same with few-shot learning.This inconsistency can be attributed to the relevance of the retrieved few-shots.Few-shots are selected based on text similarity, which does not necessarily guarantee that they share the same label.Consequently, the model may face difficulties in generalizing effectively.Moreover, few-shot learning might not always offer the model enough additional information to enhance its performance.</p>
<p>Jais-13b-chat Performance</p>
<p>Despite being Jais an Arabic-centric LLM, it shows the best results with nonnative prompts.It demonstrated superior performance in few-shot learning, which implies that few-shot learning is effective with Jais as with the other models.However, surprisingly Jais performed the worst with native prompts across most of the tasks.</p>
<p>The average results for few-shot learning were highest with non-native prompts, followed by mixed prompts, and lowest with native prompts.This pattern was also consistent in zero-shot scenarios.We observed that Jais understood the context better when the instructions were non-native, resulting in more reasonable outputs.In contrast, the most irrelevant results emerged from the native prompts.This could be due to the influence of a higher proportion of English training data ingested by the model.</p>
<p>Error Analysis</p>
<p>A common issue with Jais in few-shot learning using Arabic prompts is that it sometimes mistakenly classifies the few-shot samples instead of the input sample.For example, it might output phrases like "The classification of the first tweet is • • • " or "The overall classification for all examples, • • • " rather than addressing the new input.Additionally, the model occasionally hallucinates, producing irrelevant results.Another notable problem is that a significant portion of the responses includes phrases like "it goes against our use case policy" or "I am not able to predict," indicating an inability to process the input correctly.Furthermore, in some datasets, Jais frequently returns only one class for the majority of samples, which does not accurately reflect the actual label distribution, highlighting a potential issue with its generalization capabilities, explaining the low performance results for Jais.</p>
<p>One issue observed with GPT-4o was that out of 1,000 sample inputs, only 25 resulted in an error due to the prompt triggering Azure OpenAI's content management policy, leading to a "ResponsibleAIPolicyViolation" error.To address this issue, we mitigated the impact by assigning a random label to these instances, ensuring the continuation of the evaluation process.This issue occurred on few datasets.Therefore, the effect on the overall performance is very minimal.</p>
<p>Llama-8b-3.1 and GPT-4o consistently return responses that match the labels as explicitly prompted, adhering strictly to the instructions to return only the labels.These labels are formatted according to the language specified in the instructions.Conversely, Jais-13b often diverges from this behavior.Despite being prompted to return only the label, Jais-13b frequently includes explanations or additional information, complicating the post-processing step.</p>
<p>Conclusion and Future Work</p>
<p>In this study, we investigate different prompt structures (i.e., native, non-native, and mixed) to understand their significance in eliciting the desired output (labels for downstream NLP tasks) from various commercial and open-sourced models.Our experiments consist of 198 experimental setups, featuring 11 different social and news media datasets, 3 different models, and 3 prompt structures with zero-and few-shot prompting techniques.Our findings suggest that, overall, non-native prompts perform better, followed by mixed prompts, while native prompts significantly underperform, even with the Arabic-centric Jais model.Future work includes fine-tuning LLMs with instruction-following datasets in native language to improve models' ability to handle native users' instructions.</p>
<p>Fig. 1 .
1
Fig. 1.The three prompting techniques tested in this work: native, non-native, and mixed language.</p>
<p>Fig. 2 .
2
Fig.2.Zero-shot average results.Random: random baseline.SOTA: state-of-the-art reported in[1].</p>
<p>Fig. 3 .
3
Fig. 3. 3-shot average results.Random: random baseline.SOTA: state-of-the-art reported in [1].</p>
<p>Table 1 .
1
Data distribution across various tasks and datasets.Test (Orig.):original test set.Test: sampled test sets.
TaskDatasetTrain Test (Orig.) TestAdult ContentASAD33,68910,000 1,000Attentionworthiness CT-CWT-223,6211,186 1,000CheckworthinessCT-CWT-222,748682 682ClaimCT-CWT-223,6311,248 1,000FactualityANS3,185456 456HarmfulCT-CWT-223,6241,201 1,000Hate SpeechOSACT20207,0002,000 1,000Offensive Language OffensEval2020 7,0002,000 1,000PropagandaWANLP22504323 323SpamASAD94,68028,383 1,000SubjectivityThatiAR1,185297 297Total160,86747,776 8,758</p>
<p>Table 3 .
3
Performance across different tasks, models, and prompting techniques in 3shot setup.
TaskMetric Rand SOTA GPT Llama Jais GPT Llama Jais GPT Llama JaisNativeMixedNon-NativeAdult ContentMa-F1 0.43 0.89 0.76 0.41 0.60 0.74 0.40 0.64 0.73 0.60 0.58Attentionworthiness W-F10.15 0.21 0.29 0.18 0.24 0.29 0.14 0.21 0.33 0.20 0.22CheckworthinessF10.47 0.63 0.56 0.31 0.37 0.57 0.22 0.24 0.55 0.25 0.50Claim DetectionAcc0.52 0.57 0.59 0.66 0.54 0.59 0.45 0.49 0.69 0.56 0.64FactualityMa-F1 0.51 0.71 0.68 0.43 0.21 0.67 0.47 0.46 0.68 0.45 0.53Harmful ContentF10.27 0.56 0.57 0.28 0.31 0.52 0.32 0.28 0.56 0.45 0.35Hate SpeechMa-F1 0.37 0.82 0.68 0.50 0.08 0.68 0.60 0.18 0.70 0.49 0.40Offensive Language Ma-F1 0.46 0.91 0.87 0.49 0.56 0.87 0.72 0.69 0.87 0.70 0.62PropagandaMi-F10.14 0.65 0.44 0.23 0.13 0.50 0.43 0.09 0.44 0.36 0.17SpamMa-F1 0.41 0.99 0.86 0.74 0.67 0.82 0.72 0.31 0.88 0.79 0.74SubjectivityMa-F1 0.50 0.73 0.74 0.70 0.52 0.76 0.61 0.54 0.82 0.69 0.56TaskMetric Rand SOTA GPT Llama Jais GPT Llama Jais GPT Llama JaisNativeMixedNon-NativeAdult ContentMa-F1 0.43 0.89 0.83 0.57 0.53 0.80 0.70 0.61 0.82 0.63 0.55Attentionworthiness W-F10.15 0.21 0.41 0.17 0.19 0.38 0.42 0.21 0.38 0.26 0.14CheckworthinessF10.47 0.63 0.60 0.41 0.21 0.58 0.50 0.40 0.58 0.53 0.36Claim DetectionAcc0.52 0.57 0.61 0.70 0.50 0.62 0.66 0.49 0.71 0.58 0.58FactualityMa-F1 0.51 0.71 0.53 0.60 0.37 0.64 0.64 0.43 0.68 0.45 0.40Harmful ContentF10.27 0.56 0.59 0.30 0.34 0.61 0.41 0.28 0.60 0.40 0.29Hate SpeechMa-F1 0.37 0.82 0.65 0.65 0.33 0.66 0.60 0.41 0.66 0.60 0.46Offensive Language Ma-F1 0.46 0.91 0.81 0.74 0.58 0.82 0.77 0.45 0.88 0.71 0.71PropagandaMi-F10.14 0.65 0.55 0.29 0.14 0.55 0.44 0.10 0.52 0.45 0.21SpamMa-F1 0.41 0.99 0.92 0.58 0.26 0.93 0.82 0.25 0.89 0.62 0.53SubjectivityMa-F1 0.50 0.73 0.79 0.73 0.41 0.80 0.53 0.42 0.81 0.46 0.56
Note that we use the term 'native' to refer to the language of the user input. In our case, Arabic is the native language of the data tested.
https://ai.meta.com/blog/meta-llama-3-1/
AcknowledgmentsThe work of M. Hasanain is supported by NPRP 14C-0916-210015 from the Qatar National Research Fund, part of Qatar Research Development and Innovation Council (QRDI).⋆ The contribution was made while the author was interning at the Qatar Computing Research Institute. 1 https://llmebench.qcri.org/
LAraBench: Benchmarking Arabic AI with large language models. A Abdelali, H Mubarak, S Chowdhury, M Hasanain, B Mousi, S Boughorbel, S Abdaljalil, Y El Kheir, D Izham, F Dalvi, M Hawasly, N Nazar, Y Elshahawy, A Ali, N Durrani, N Milic-Frayling, F Alam, Proceedings of the 18th Conference of the European Chapter. Long Papers. Y Graham, M Purver, the 18th Conference of the European ChapterSt. Julian's, MaltaAssociation for Computational LinguisticsMar 20241</p>
<p>MEGA: Multilingual evaluation of generative AI. K Ahuja, H Diddee, R Hada, M Ochieng, K Ramesh, P Jain, A Nambi, T Ganu, S Segal, M Ahmed, K Bali, S Sitaram, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDec 2023</p>
<p>A survey on multimodal disinformation detection. F Alam, S Cresci, T Chakraborty, F Silvestri, D Dimitrov, G D S Martino, S Shaar, H Firooz, P Nakov, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of KoreaOct 202222</p>
<p>F Alam, H Mubarak, W Zaghouani, G Da San Martino, P Nakov, Overview of the WANLP 2022 shared task on propaganda detection in Arabic. Dec 2022</p>
<p>Fighting the COVID-19 infodemic: Modeling the perspective of journalists, fact-checkers, social media platforms, policy makers, and the society. F Alam, S Shaar, F Dalvi, H Sajjad, A Nikolov, H Mubarak, G Da San Martino, A Abdelali, N Durrani, K Darwish, A Al-Homaid, W Zaghouani, T Caselli, G Danoe, F Stolk, B Bruntink, P Nakov, Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational LinguisticsNov 2021</p>
<p>A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. Y Bang, S Cahyawijaya, N Lee, W Dai, D Su, B Wilie, H Lovenia, Z Ji, T Yu, W Chung, V Do, Q Xu, Y Fung, P , Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. Long Papers. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterIndonesiaAssociation for Computational LinguisticsNov 20231</p>
<p>S Brooke, Condescending, Rude, Assholes": Framing gender and hostility on Stack Overflow. WALO2019</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems. 2020</p>
<p>The use of mmr, diversity-based reranking for reordering documents and producing summaries. J Carbonell, J Goldstein, Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval. the 21st annual international ACM SIGIR conference on Research and development in information retrieval1998</p>
<p>LLMeBench: A flexible framework for accelerating LLMs benchmarking. F Dalvi, M Hasanain, S Boughorbel, B Mousi, S Abdaljalil, N Nazar, A Abdelali, S A Chowdhury, H Mubarak, A Ali, M Hawasly, N Durrani, F Alam, Proceedings of the 18th Conference of the European Chapter. N Aletras, O De Clercq, the 18th Conference of the European ChapterMaltaAssociation for Computational LinguisticsMar 2024St. Julians</p>
<p>Automated hate speech detection and the problem of offensive language. T Davidson, D Warmsley, M Macy, I Weber, Proceedings of the International AAAI Conference on Web and Social Media. AAAI '17. the International AAAI Conference on Web and Social Media. AAAI '17201711</p>
<p>Detecting propaganda techniques in memes. D Dimitrov, B Bin Ali, S Shaar, F Alam, F Silvestri, H Firooz, P Nakov, G Da San Martino, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAug 20211</p>
<p>A survey on automatic detection of hate speech in text. P Fortuna, S Nunes, ACM Computing Surveys (CSUR). 5142018</p>
<p>A Galassi, F Ruggeri, A B C No, F Alam, T Caselli, M Kutlu, J M Struss, F Antici, M Hasanain, J Köhler, K Korre, F Leistra, A Muti, M Siegel, M D Turkmen, M Wiegand, W Zaghouani, Overview of the CLEF-2023. </p>
<p>Working Notes of CLEF 2023-Conference and Labs of the Evaluation Forum. CLEF '2023. Thessaloniki, Greece2023CheckThat! lab task 2 on subjectivity in news articles</p>
<p>Towards online spam filtering in social networks. H Gao, Y Chen, K Lee, D Palsetia, A N Choudhary, Network and Distributed System Security Symposium. 201212</p>
<p>A survey on automated fact-checking. Z Guo, M Schlichtkrull, A Vlachos, Transactions of the Association for Computational Linguistics. 102022</p>
<p>W Jiao, W Wang, J T Huang, X Wang, S Shi, Z Tu, arXiv:2301.08745Is chatgpt a good translator? yes with gpt-4 as the engine. 2023arXiv preprint</p>
<p>Y Jin, M Choi, G Verma, J Wang, S Kumar, arXiv:2402.14154Mm-soc: Benchmarking multimodal large language models in social media platforms. 2024arXiv preprint</p>
<p>Automated identification of verbally abusive behaviors in online discussions. S Joksimovic, R S Baker, J Ocumpaugh, J M L Andres, I Tot, E Y Wang, S Dawson, 2019WALO</p>
<p>GPTAraEval: A comprehensive evaluation of chatgpt on arabic nlp. M T I Khondaker, A Waheed, M Abdul-Mageed, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Stance prediction and claim verification: An Arabic perspective. J Khouja, Proceedings of the Third Workshop on Fact Extraction and VERification (FEVER). the Third Workshop on Fact Extraction and VERification (FEVER)Association for Computational LinguisticsJul 2020</p>
<p>Towards automated factchecking: Developing an annotation schema and benchmark for consistent automated claim detection. L Konstantinovskiy, O Price, M Babakar, A Zubiaga, CoRR abs/1809.081932018</p>
<p>Holistic evaluation of language models. P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, Transactions on Machine Learning Research. </p>
<p>Gpt detectors are biased against non-native english writers. W Liang, M Yuksekgonul, Y Mao, E Wu, J Zou, Patterns. 472023</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, ACM Computing Surveys. 5592023</p>
<p>A survey of data partitioning and sampling methods to support big data analysis. M S Mahmud, J Z Huang, S Salloum, T Z Emara, K Sadatdiynov, Big Data Mining and Analytics. 322020</p>
<p>Understanding and mitigating language confusion in llms. K Marchisio, W Y Ko, A Bérard, T Dehaze, S Ruder, arXiv:2406.200522024arXiv preprint</p>
<p>Detecting and identifying the reasons for deleted tweets before they are posted. H Mubarak, S Abdaljalil, A Nassar, F Alam, Frontiers in Artificial Intelligence. 612197672023</p>
<p>Spam detection on Arabic twitter. H Mubarak, A Abdelali, S Hassan, K Darwish, Social Informatics: 12th International Conference. Proceedings. SocInfo; Pisa, ItalySpringer2020. October 6-9, 2020. 202012</p>
<p>Overview of OSACT4 Arabic offensive language detection shared task. H Mubarak, K Darwish, W Magdy, T Elsayed, H Al-Khalifa, Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools. the 4th Workshop on Open-Source Arabic Corpora and Processing ToolsMarseille, FranceMay 2020with a Shared Task on Offensive Language Detection</p>
<p>Adult content detection on Arabic twitter: Analysis and experiments. H Mubarak, S Hassan, A Abdelali, Proceedings of the Sixth Arabic Natural Language Processing Workshop. the Sixth Arabic Natural Language Processing Workshop2021</p>
<p>Overview of the CLEF-2022 CheckThat! lab task 1 on identifying relevant claims in tweets. P Nakov, A Barrón-Cedeño, G Da San Martino, F Alam, R Míguez, T Caselli, M Kutlu, W Zaghouani, C Li, S Shaar, H Mubarak, A Nikolov, Y S Kartal, J Beltrán, Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum. 20222022</p>
<p>Automated fact-checking for assisting human fact-checkers. P Nakov, D Corney, M Hasanain, F Alam, T Elsayed, A Barrón-Cedeño, P Papotti, S Shaar, G Da San Martino, Proceedings of the 30th International Joint Conference on Artificial Intelligence. the 30th International Joint Conference on Artificial Intelligence202121</p>
<p>Democratizing llms for lowresource languages by leveraging their english dominant abilities with linguisticallydiverse prompts. X P Nguyen, S M Aljunied, S Joty, L Bing, arXiv:2306.113722023arXiv preprint</p>
<p>. OpenAI: GPT-4 technical report. Tech. rep. 2023</p>
<p>A survey on natural language processing for fake news detection. R Oshikawa, J Qian, W Y Wang, Proceedings of the Twelfth Language Resources and Evaluation Conference. the Twelfth Language Resources and Evaluation Conference2020</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERT-networks. N Reimers, I Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong; China2019EMNLP-IJCNLP '19</p>
<p>Jais and jais-chat: Arabic-centric foundation and instruction-tuned open generative large language models. N Sengupta, S K Sahu, B Jia, S Katipomu, H Li, F Koto, O M Afzal, S Kamboj, O Pandit, R Pal, arXiv:2308.161492023arXiv preprint</p>
<p>AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. T Shin, Y Razeghi, I V Logan, R L Wallace, E Singh, S , Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing2020</p>
<p>R Suwaileh, M Hasanain, F Hubail, W Zaghouani, F Alam, arXiv:2406.05559ThatiAR: Subjectivity detection in arabic news sentences. 2024</p>
<p>The Llama 3 herd of models. L Team, arXiv2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems. the 36th International Conference on Neural Information Processing Systems2022</p>
<p>Large language models are human-level prompt engineers. Y Zhou, A I Muresanu, Z Han, K Paster, S Pitis, H Chan, J Ba, NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022</p>            </div>
        </div>

    </div>
</body>
</html>